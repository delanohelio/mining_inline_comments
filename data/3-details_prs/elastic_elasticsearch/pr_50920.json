{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzYyMTQ3MzY4", "number": 50920, "title": "Implement dangling indices API", "bodyText": "Part of #48366. Implement an API for managing dangling indices.\nI've been staring at this code for a while so I'd really appreciate some feedback. Note that I haven't (yet) implemented wildcard restores, or written API documentation.", "createdAt": "2020-01-13T14:29:17Z", "url": "https://github.com/elastic/elasticsearch/pull/50920", "merged": true, "mergeCommit": {"oid": "ebe89518795211eeba01b21c65d9396702441d0a"}, "closed": true, "closedAt": "2020-06-16T14:19:18Z", "author": {"login": "pugnascotia"}, "timelineItems": {"totalCount": 119, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABccZ2eCgH2gAyMzYyMTQ3MzY4OjY0NzE4YzVmNTc1NzIzOTNkMWMyODQyYjNlYzkyZDc5MzhkNTBlMTU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcr191YAFqTQzMTU1MzAzNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "64718c5f57572393d1c2842b3ec92d7938d50e15", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/64718c5f57572393d1c2842b3ec92d7938d50e15", "committedDate": "2020-04-29T15:03:05Z", "message": "More tweaks"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a38d9270ef6a53c397137200acf1b991d43b6a50", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/a38d9270ef6a53c397137200acf1b991d43b6a50", "committedDate": "2020-04-29T15:04:42Z", "message": "Merge remote-tracking branch 'upstream/master' into 48366-dangling-indices-api"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "04cca440ec638dd6197b1c42805150938272656c", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/04cca440ec638dd6197b1c42805150938272656c", "committedDate": "2020-04-29T15:19:12Z", "message": "Fix imports"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1216423ce4faba13f58b72f0eac1ba142a6925ae", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/1216423ce4faba13f58b72f0eac1ba142a6925ae", "committedDate": "2020-04-29T20:41:27Z", "message": "Merge remote-tracking branch 'upstream/master' into 48366-dangling-indices-api"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ec98df75be0f0adb8751a6e6ed0b8d80f88158e9", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/ec98df75be0f0adb8751a6e6ed0b8d80f88158e9", "committedDate": "2020-05-15T10:47:34Z", "message": "Merge remote-tracking branch 'upstream/master' into 48366-dangling-indices-api"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "53611f1a6b1ac3bb8ff106eb586b58ee795a836e", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/53611f1a6b1ac3bb8ff106eb586b58ee795a836e", "committedDate": "2020-06-01T07:28:02Z", "message": "Merge branch 'master' into 48366-dangling-indices-api"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIyNTUzMzkz", "url": "https://github.com/elastic/elasticsearch/pull/50920#pullrequestreview-422553393", "createdAt": "2020-06-02T10:40:25Z", "commit": {"oid": "53611f1a6b1ac3bb8ff106eb586b58ee795a836e"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxMDo0MDoyNlrOGdr-Qg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxMTowMTowMlrOGdsluA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzc4MjMzOA==", "bodyText": "I think this is now quadratic in the graveyard size, which defaults to 500, so that's 500x500=250000 checks in a well-established cluster. I think it'd be more efficient to compare the graveyards for equality and then use IndexGraveyardDiff to find the differences.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r433782338", "createdAt": "2020-06-02T10:40:26Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/cluster/ClusterChangedEvent.java", "diffHunk": "@@ -256,18 +256,37 @@ public boolean isNewCluster() {\n         if (metadataChanged() == false || isNewCluster()) {\n             return Collections.emptyList();\n         }\n-        List<Index> deleted = null;\n-        for (ObjectCursor<IndexMetadata> cursor : previousState.metadata().indices().values()) {\n+        Set<Index> deleted = null;\n+        final Metadata previousMetadata = previousState.metadata();\n+        final Metadata currentMetadata = state.metadata();\n+\n+        for (ObjectCursor<IndexMetadata> cursor : previousMetadata.indices().values()) {\n             IndexMetadata index = cursor.value;\n-            IndexMetadata current = state.metadata().index(index.getIndex());\n+            IndexMetadata current = currentMetadata.index(index.getIndex());\n             if (current == null) {\n                 if (deleted == null) {\n-                    deleted = new ArrayList<>();\n+                    deleted = new HashSet<>();\n                 }\n                 deleted.add(index.getIndex());\n             }\n         }\n-        return deleted == null ? Collections.<Index>emptyList() : deleted;\n+\n+        // Look for new entries in the index graveyard, where there's no corresponding index in the\n+        // previous metadata. This indicates that a dangling index has been explicitly deleted, so\n+        // each node should make sure to delete any related data.\n+        for (IndexGraveyard.Tombstone tombstone : currentMetadata.indexGraveyard().getTombstones()) {\n+            final Index index = tombstone.getIndex();\n+            final boolean isNewTombstone = previousMetadata.hasIndex(index) == false\n+                && previousMetadata.indexGraveyard().containsIndex(index) == false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "53611f1a6b1ac3bb8ff106eb586b58ee795a836e"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzc4Mjg1NQ==", "bodyText": "I don't think this should be a BaseNodesRequest, we don't broadcast it across nodes.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r433782855", "createdAt": "2020-06-02T10:41:30Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/import_index/ImportDanglingIndexRequest.java", "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling.import_index;\n+\n+import org.elasticsearch.action.support.nodes.BaseNodesRequest;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+\n+import java.io.IOException;\n+import java.util.Locale;\n+import java.util.Objects;\n+\n+/**\n+ * Represents a request to import a particular dangling index, specified\n+ * by its UUID. The {@link #acceptDataLoss} flag must also be\n+ * explicitly set to true, or later validation will fail.\n+ */\n+public class ImportDanglingIndexRequest extends BaseNodesRequest<ImportDanglingIndexRequest> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "53611f1a6b1ac3bb8ff106eb586b58ee795a836e"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzc4Mzg4OA==", "bodyText": "Nit: can we use random UUIDs instead?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r433783888", "createdAt": "2020-06-02T10:43:43Z", "author": {"login": "DaveCTurner"}, "path": "server/src/test/java/org/elasticsearch/action/admin/indices/dangling/list/ListDanglingIndicesResponseTests.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling.list;\n+\n+import org.elasticsearch.action.admin.indices.dangling.DanglingIndexInfo;\n+import org.elasticsearch.action.admin.indices.dangling.list.ListDanglingIndicesResponse.AggregatedDanglingIndexInfo;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.test.ESTestCase;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import static java.util.Collections.emptyList;\n+import static org.elasticsearch.action.admin.indices.dangling.list.ListDanglingIndicesResponse.resultsByIndexUUID;\n+import static org.hamcrest.Matchers.containsInAnyOrder;\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+public class ListDanglingIndicesResponseTests extends ESTestCase {\n+\n+    /**\n+     * Checks that {@link ListDanglingIndicesResponse#resultsByIndexUUID(List)} handles the\n+     * basic base of empty input.\n+     */\n+    public void testResultsByIndexUUIDWithEmptyListReturnsEmptyMap() {\n+        assertThat(resultsByIndexUUID(emptyList()), empty());\n+    }\n+\n+    /**\n+     * Checks that <code>resultsByIndexUUID(List)</code> can aggregate a single dangling index\n+     * on a single node.\n+     */\n+    public void testResultsByIndexUUIDCanAggregateASingleResponse() {\n+        final DiscoveryNode node = mock(DiscoveryNode.class);\n+        when(node.getId()).thenReturn(\"some-node-id\");\n+\n+        final var danglingIndexInfo = List.of(new DanglingIndexInfo(\"some-node-id\", \"some-index\", \"deadb33f\", 123456L));\n+        final var nodes = List.of(new NodeListDanglingIndicesResponse(node, danglingIndexInfo));\n+\n+        final var aggregated = new ArrayList<>(resultsByIndexUUID(nodes));\n+        assertThat(aggregated, hasSize(1));\n+\n+        final var expected = new AggregatedDanglingIndexInfo(\"deadb33f\", \"some-index\", 123456L);\n+        expected.getNodeIds().add(\"some-node-id\");\n+        assertThat(aggregated.get(0), equalTo(expected));\n+    }\n+\n+    /**\n+     * Checks that <code>resultsByIndexUUID(List)</code> can aggregate a single dangling index\n+     * across multiple nodes.\n+     */\n+    public void testResultsByIndexUUIDCanAggregateAcrossMultipleNodes() {\n+        final DiscoveryNode node1 = mock(DiscoveryNode.class);\n+        final DiscoveryNode node2 = mock(DiscoveryNode.class);\n+        when(node1.getId()).thenReturn(\"node-id-1\");\n+        when(node2.getId()).thenReturn(\"node-id-2\");\n+\n+        final var danglingIndexInfo1 = List.of(new DanglingIndexInfo(\"node-id-1\", \"some-index\", \"deadb33f\", 123456L));\n+        final var danglingIndexInfo2 = List.of(new DanglingIndexInfo(\"node-id-2\", \"some-index\", \"deadb33f\", 123456L));\n+        final var nodes = List.of(\n+            new NodeListDanglingIndicesResponse(node1, danglingIndexInfo1),\n+            new NodeListDanglingIndicesResponse(node2, danglingIndexInfo2)\n+        );\n+\n+        final var aggregated = new ArrayList<>(resultsByIndexUUID(nodes));\n+        assertThat(aggregated, hasSize(1));\n+\n+        final var expected = new AggregatedDanglingIndexInfo(\"deadb33f\", \"some-index\", 123456L);\n+        expected.getNodeIds().add(\"node-id-1\");\n+        expected.getNodeIds().add(\"node-id-2\");\n+        assertThat(aggregated.get(0), equalTo(expected));\n+    }\n+\n+    /**\n+     * Checks that <code>resultsByIndexUUID(List)</code> can aggregate multiple dangling indices\n+     * on a single node.\n+     */\n+    public void testResultsByIndexUUIDCanAggregateMultipleIndicesOnOneNode() {\n+        final DiscoveryNode node1 = mock(DiscoveryNode.class);\n+        when(node1.getId()).thenReturn(\"node-id-1\");\n+\n+        final var danglingIndexInfo = List.of(\n+            new DanglingIndexInfo(\"node-id-1\", \"some-index\", \"deadb33f\", 123456L),\n+            new DanglingIndexInfo(\"node-id-1\", \"some-other-index\", \"cafebabe\", 7891011L)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "53611f1a6b1ac3bb8ff106eb586b58ee795a836e"}, "originalPosition": 104}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzc5MDM4NA==", "bodyText": "I'm concerned that this warning will appear if you delete a dangling index and then immediately list the remaining dangling indices, because the async deletion hasn't gone through yet. But then I'm not sure why we don't see this already with async deletion. Could you investigate that?\nI'm wondering if we really need a warning here at all. If there's a tombstone then the index was certainly deleted, so it's something else's problem and we should be ignoring it here. The problem is if we never get around to cleaning it up until the tombstone expires then it'll become dangling again, but I don't think that's our concern here.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r433790384", "createdAt": "2020-06-02T10:57:03Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java", "diffHunk": "@@ -134,38 +152,51 @@ void cleanupAllocatedDangledIndices(Metadata metadata) {\n      * to the currently tracked dangling indices.\n      */\n     void findNewAndAddDanglingIndices(final Metadata metadata) {\n-        danglingIndices.putAll(findNewDanglingIndices(metadata));\n+        final IndexGraveyard graveyard = metadata.indexGraveyard();\n+\n+        // If a tombstone is created for a dangling index, we need to make sure that the\n+        // index is no longer considered dangling.\n+        for (Index key : danglingIndices.keySet()) {\n+            if (graveyard.containsIndex(key)) {\n+                danglingIndices.remove(key);\n+            }\n+        }\n+\n+        danglingIndices.putAll(findNewDanglingIndices(danglingIndices, metadata));\n     }\n \n     /**\n      * Finds new dangling indices by iterating over the indices and trying to find indices\n-     * that have state on disk, but are not part of the provided meta data, or not detected\n+     * that have state on disk, but are not part of the provided metadata, or not detected\n      * as dangled already.\n      */\n-    Map<Index, IndexMetadata> findNewDanglingIndices(final Metadata metadata) {\n+    public Map<Index, IndexMetadata> findNewDanglingIndices(Map<Index, IndexMetadata> existingDanglingIndices, final Metadata metadata) {\n         final Set<String> excludeIndexPathIds = new HashSet<>(metadata.indices().size() + danglingIndices.size());\n         for (ObjectCursor<IndexMetadata> cursor : metadata.indices().values()) {\n             excludeIndexPathIds.add(cursor.value.getIndex().getUUID());\n         }\n-        excludeIndexPathIds.addAll(danglingIndices.keySet().stream().map(Index::getUUID).collect(Collectors.toList()));\n+        for (Index index : existingDanglingIndices.keySet()) {\n+            excludeIndexPathIds.add(index.getUUID());\n+        }\n         try {\n             final List<IndexMetadata> indexMetadataList = metaStateService.loadIndicesStates(excludeIndexPathIds::contains);\n             Map<Index, IndexMetadata> newIndices = new HashMap<>(indexMetadataList.size());\n             final IndexGraveyard graveyard = metadata.indexGraveyard();\n+\n             for (IndexMetadata indexMetadata : indexMetadataList) {\n-                if (metadata.hasIndex(indexMetadata.getIndex().getName())) {\n-                    logger.warn(\"[{}] can not be imported as a dangling index, as index with same name already exists in cluster metadata\",\n-                        indexMetadata.getIndex());\n-                } else if (graveyard.containsIndex(indexMetadata.getIndex())) {\n-                    logger.warn(\"[{}] can not be imported as a dangling index, as an index with the same name and UUID exist in the \" +\n-                                \"index tombstones.  This situation is likely caused by copying over the data directory for an index \" +\n-                                \"that was previously deleted.\", indexMetadata.getIndex());\n+                Index index = indexMetadata.getIndex();\n+                // Although deleting a dangling index through the API adds a tombstone to the graveyard, that process results in the\n+                // dangling index files being deleted, so we don't expect to encounter a dangling index and a tombstone here when\n+                // everything is working normally.\n+                if (graveyard.containsIndex(index)) {\n+                    logger.warn(\"[{}] cannot be imported as a dangling index, as an index with the same name and UUID exist in the \"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "53611f1a6b1ac3bb8ff106eb586b58ee795a836e"}, "originalPosition": 148}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzc5MjIwOQ==", "bodyText": "I don't think this is what happens when auto-import is enabled as it is here; instead we update the danglingIndices field and pass that in for subsequent calls.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r433792209", "createdAt": "2020-06-02T11:00:35Z", "author": {"login": "DaveCTurner"}, "path": "server/src/test/java/org/elasticsearch/gateway/DanglingIndicesStateTests.java", "diffHunk": "@@ -124,7 +154,7 @@ public void testDanglingProcessing() throws Exception {\n             // check that several runs when not in the metadata still keep the dangled index around\n             int numberOfChecks = randomIntBetween(1, 10);\n             for (int i = 0; i < numberOfChecks; i++) {\n-                Map<Index, IndexMetadata> newDanglingIndices = danglingState.findNewDanglingIndices(metadata);\n+                Map<Index, IndexMetadata> newDanglingIndices = danglingState.findNewDanglingIndices(emptyMap(), metadata);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "53611f1a6b1ac3bb8ff106eb586b58ee795a836e"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzc5MjQ0MA==", "bodyText": "Similarly here, we should be using the previous result rather than emptyMap().", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r433792440", "createdAt": "2020-06-02T11:01:02Z", "author": {"login": "DaveCTurner"}, "path": "server/src/test/java/org/elasticsearch/gateway/DanglingIndicesStateTests.java", "diffHunk": "@@ -142,7 +172,7 @@ public void testDanglingProcessing() throws Exception {\n \n             // check that several runs when in the metadata, but not cleaned yet, still keeps dangled\n             for (int i = 0; i < numberOfChecks; i++) {\n-                Map<Index, IndexMetadata> newDanglingIndices = danglingState.findNewDanglingIndices(metadata);\n+                Map<Index, IndexMetadata> newDanglingIndices = danglingState.findNewDanglingIndices(emptyMap(), metadata);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "53611f1a6b1ac3bb8ff106eb586b58ee795a836e"}, "originalPosition": 90}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIyNTk1NDky", "url": "https://github.com/elastic/elasticsearch/pull/50920#pullrequestreview-422595492", "createdAt": "2020-06-02T11:45:53Z", "commit": {"oid": "53611f1a6b1ac3bb8ff106eb586b58ee795a836e"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxMTo0NTo1NFrOGdt7ZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxMTo1Nzo1NFrOGduT1A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzgxNDM3Mw==", "bodyText": "I'm not a fan of iterating over a map and then concurrently deleting entries. This only works because it's a concurrent map. I would prefer the use of danglingIndices.keySet().removeIf(...)", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r433814373", "createdAt": "2020-06-02T11:45:54Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java", "diffHunk": "@@ -134,38 +152,51 @@ void cleanupAllocatedDangledIndices(Metadata metadata) {\n      * to the currently tracked dangling indices.\n      */\n     void findNewAndAddDanglingIndices(final Metadata metadata) {\n-        danglingIndices.putAll(findNewDanglingIndices(metadata));\n+        final IndexGraveyard graveyard = metadata.indexGraveyard();\n+\n+        // If a tombstone is created for a dangling index, we need to make sure that the\n+        // index is no longer considered dangling.\n+        for (Index key : danglingIndices.keySet()) {\n+            if (graveyard.containsIndex(key)) {\n+                danglingIndices.remove(key);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "53611f1a6b1ac3bb8ff106eb586b58ee795a836e"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzgxNzc0Mg==", "bodyText": "What do you mean by async deletion here, David? Deletion is normally synchronous on the CS applier thread (i.e. the one that runs IndicesClusterStateService, which runs before findNewDanglingIndices here).", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r433817742", "createdAt": "2020-06-02T11:52:17Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java", "diffHunk": "@@ -134,38 +152,51 @@ void cleanupAllocatedDangledIndices(Metadata metadata) {\n      * to the currently tracked dangling indices.\n      */\n     void findNewAndAddDanglingIndices(final Metadata metadata) {\n-        danglingIndices.putAll(findNewDanglingIndices(metadata));\n+        final IndexGraveyard graveyard = metadata.indexGraveyard();\n+\n+        // If a tombstone is created for a dangling index, we need to make sure that the\n+        // index is no longer considered dangling.\n+        for (Index key : danglingIndices.keySet()) {\n+            if (graveyard.containsIndex(key)) {\n+                danglingIndices.remove(key);\n+            }\n+        }\n+\n+        danglingIndices.putAll(findNewDanglingIndices(danglingIndices, metadata));\n     }\n \n     /**\n      * Finds new dangling indices by iterating over the indices and trying to find indices\n-     * that have state on disk, but are not part of the provided meta data, or not detected\n+     * that have state on disk, but are not part of the provided metadata, or not detected\n      * as dangled already.\n      */\n-    Map<Index, IndexMetadata> findNewDanglingIndices(final Metadata metadata) {\n+    public Map<Index, IndexMetadata> findNewDanglingIndices(Map<Index, IndexMetadata> existingDanglingIndices, final Metadata metadata) {\n         final Set<String> excludeIndexPathIds = new HashSet<>(metadata.indices().size() + danglingIndices.size());\n         for (ObjectCursor<IndexMetadata> cursor : metadata.indices().values()) {\n             excludeIndexPathIds.add(cursor.value.getIndex().getUUID());\n         }\n-        excludeIndexPathIds.addAll(danglingIndices.keySet().stream().map(Index::getUUID).collect(Collectors.toList()));\n+        for (Index index : existingDanglingIndices.keySet()) {\n+            excludeIndexPathIds.add(index.getUUID());\n+        }\n         try {\n             final List<IndexMetadata> indexMetadataList = metaStateService.loadIndicesStates(excludeIndexPathIds::contains);\n             Map<Index, IndexMetadata> newIndices = new HashMap<>(indexMetadataList.size());\n             final IndexGraveyard graveyard = metadata.indexGraveyard();\n+\n             for (IndexMetadata indexMetadata : indexMetadataList) {\n-                if (metadata.hasIndex(indexMetadata.getIndex().getName())) {\n-                    logger.warn(\"[{}] can not be imported as a dangling index, as index with same name already exists in cluster metadata\",\n-                        indexMetadata.getIndex());\n-                } else if (graveyard.containsIndex(indexMetadata.getIndex())) {\n-                    logger.warn(\"[{}] can not be imported as a dangling index, as an index with the same name and UUID exist in the \" +\n-                                \"index tombstones.  This situation is likely caused by copying over the data directory for an index \" +\n-                                \"that was previously deleted.\", indexMetadata.getIndex());\n+                Index index = indexMetadata.getIndex();\n+                // Although deleting a dangling index through the API adds a tombstone to the graveyard, that process results in the\n+                // dangling index files being deleted, so we don't expect to encounter a dangling index and a tombstone here when\n+                // everything is working normally.\n+                if (graveyard.containsIndex(index)) {\n+                    logger.warn(\"[{}] cannot be imported as a dangling index, as an index with the same name and UUID exist in the \"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzc5MDM4NA=="}, "originalCommit": {"oid": "53611f1a6b1ac3bb8ff106eb586b58ee795a836e"}, "originalPosition": 148}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzgyMDYyOA==", "bodyText": "do we need to check that there is not already a tombstone for this index? Otherwise we might be spamming the graveyard with the same entry, which purges other perhaps more important entries to keep around.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r433820628", "createdAt": "2020-06-02T11:57:54Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/delete/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling.delete;\n+\n+import com.carrotsearch.hppc.cursors.ObjectObjectCursor;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.admin.indices.dangling.DanglingIndexInfo;\n+import org.elasticsearch.action.admin.indices.dangling.list.ListDanglingIndicesAction;\n+import org.elasticsearch.action.admin.indices.dangling.list.ListDanglingIndicesRequest;\n+import org.elasticsearch.action.admin.indices.dangling.list.ListDanglingIndicesResponse;\n+import org.elasticsearch.action.admin.indices.dangling.list.NodeListDanglingIndicesResponse;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.master.AcknowledgedResponse;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.client.node.NodeClient;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.Metadata;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.Index;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, AcknowledgedResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+    private final NodeClient nodeClient;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings,\n+        NodeClient nodeClient\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+        this.nodeClient = nodeClient;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.GENERIC;\n+    }\n+\n+    @Override\n+    protected AcknowledgedResponse read(StreamInput in) throws IOException {\n+        return new AcknowledgedResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest deleteRequest,\n+        ClusterState state,\n+        ActionListener<AcknowledgedResponse> deleteListener\n+    ) throws Exception {\n+        findDanglingIndex(deleteRequest.getIndexUUID(), new ActionListener<>() {\n+            @Override\n+            public void onResponse(Index indexToDelete) {\n+                // This flag is checked at this point so that we always check that the supplied index ID\n+                // does correspond to a dangling index.\n+                if (deleteRequest.isAcceptDataLoss() == false) {\n+                    deleteListener.onFailure(new IllegalArgumentException(\"accept_data_loss must be set to true\"));\n+                    return;\n+                }\n+\n+                String indexName = indexToDelete.getName();\n+                String indexUUID = indexToDelete.getUUID();\n+\n+                final ActionListener<AcknowledgedResponse> clusterStateUpdatedListener = new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(AcknowledgedResponse response) {\n+                        deleteListener.onResponse(response);\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        logger.debug(\"Failed to delete dangling index [\" + indexName + \"] [\" + indexUUID + \"]\", e);\n+                        deleteListener.onFailure(e);\n+                    }\n+                };\n+\n+                final String taskSource = \"delete-dangling-index [\" + indexName + \"] [\" + indexUUID + \"]\";\n+\n+                clusterService.submitStateUpdateTask(\n+                    taskSource,\n+                    new AckedClusterStateUpdateTask<>(deleteRequest, clusterStateUpdatedListener) {\n+\n+                        @Override\n+                        protected AcknowledgedResponse newResponse(boolean acknowledged) {\n+                            return new AcknowledgedResponse(acknowledged);\n+                        }\n+\n+                        @Override\n+                        public ClusterState execute(final ClusterState currentState) {\n+                            return deleteDanglingIndex(currentState, indexToDelete);\n+                        }\n+                    }\n+                );\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.debug(\"Failed to find dangling index [\" + deleteRequest.getIndexUUID() + \"]\", e);\n+                deleteListener.onFailure(e);\n+            }\n+        });\n+    }\n+\n+    private ClusterState deleteDanglingIndex(ClusterState currentState, Index indexToDelete) {\n+        final Metadata metaData = currentState.getMetadata();\n+\n+        for (ObjectObjectCursor<String, IndexMetadata> each : metaData.indices()) {\n+            if (indexToDelete.getUUID().equals(each.value.getIndexUUID())) {\n+                throw new IllegalArgumentException(\n+                    \"Refusing to delete dangling index \"\n+                        + indexToDelete\n+                        + \" as an index with UUID [\"\n+                        + indexToDelete.getUUID()\n+                        + \"] already exists in the cluster state\"\n+                );\n+            }\n+        }\n+\n+        Metadata.Builder metaDataBuilder = Metadata.builder(metaData);\n+\n+        final IndexGraveyard newGraveyard = IndexGraveyard.builder(metaDataBuilder.indexGraveyard())\n+            .addTombstone(indexToDelete)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "53611f1a6b1ac3bb8ff106eb586b58ee795a836e"}, "originalPosition": 179}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f80e6dace57a906e48a5eb8ecf40a9b691b9161b", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/f80e6dace57a906e48a5eb8ecf40a9b691b9161b", "committedDate": "2020-06-08T10:31:32Z", "message": "Merge remote-tracking branch 'upstream/master' into 48366-dangling-indices-api"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f8db53fb5751a448138aa13bb764d22e796a1652", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/f8db53fb5751a448138aa13bb764d22e796a1652", "committedDate": "2020-06-08T10:55:41Z", "message": "Use IndexGraveyardDiff to compare graveyards"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9043ae88c0e06973e8501ae46edb16f0ea2d6a90", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/9043ae88c0e06973e8501ae46edb16f0ea2d6a90", "committedDate": "2020-06-08T13:46:30Z", "message": "Change class hierarchy of ImportDanglingIndexRequest\n\nBy extending AcknowledgedRequest, it becomes possible to replace\nImportDanglingIndexResponse with AcknowledgedResponse."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2195c953e4be0803c62f9ced21afe74939f08b4b", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/2195c953e4be0803c62f9ced21afe74939f08b4b", "committedDate": "2020-06-08T13:50:13Z", "message": "Replace test strings with random UUIDs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "637c3c39f6dc23fcb188896ef6e8361c43b8b562", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/637c3c39f6dc23fcb188896ef6e8361c43b8b562", "committedDate": "2020-06-08T15:29:31Z", "message": "Guard against duplicate tombstones for dangling indices"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8e2c9fc914f3e625e7151abc5c34ae86d7eeab9f", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/8e2c9fc914f3e625e7151abc5c34ae86d7eeab9f", "committedDate": "2020-06-08T15:29:52Z", "message": "Tweaking"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a8a68fa6c66171f1e2a4524f9aae4bac1f8dbfd2", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/a8a68fa6c66171f1e2a4524f9aae4bac1f8dbfd2", "committedDate": "2020-06-09T10:52:20Z", "message": "Remove unnecessary log line"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "593e07c5d0aa9b075465260c242414ebad11c7d4", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/593e07c5d0aa9b075465260c242414ebad11c7d4", "committedDate": "2020-06-09T11:13:24Z", "message": "Test fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "56f66179a7fb5e7bea44dfe8d4126cc8cd5cb2c9", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/56f66179a7fb5e7bea44dfe8d4126cc8cd5cb2c9", "committedDate": "2020-06-10T12:24:40Z", "message": "Merge remote-tracking branch 'upstream/master' into 48366-dangling-indices-api"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMxNTE0NzI3", "url": "https://github.com/elastic/elasticsearch/pull/50920#pullrequestreview-431514727", "createdAt": "2020-06-16T13:42:04Z", "commit": {"oid": "56f66179a7fb5e7bea44dfe8d4126cc8cd5cb2c9"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMxNTUzMDM3", "url": "https://github.com/elastic/elasticsearch/pull/50920#pullrequestreview-431553037", "createdAt": "2020-06-16T14:17:20Z", "commit": {"oid": "56f66179a7fb5e7bea44dfe8d4126cc8cd5cb2c9"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cf467369948148d56b5058d4e2f7e5394206458e", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/cf467369948148d56b5058d4e2f7e5394206458e", "committedDate": "2019-12-09T12:16:31Z", "message": "Add an API to fetch dangling indices"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "befcfa42811b1bb16662e6b5f97ee3adfcb402b7", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/befcfa42811b1bb16662e6b5f97ee3adfcb402b7", "committedDate": "2019-12-10T19:51:17Z", "message": "Refactor list dangling indices to call all nodes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "747908e00884a73ca1167fb09fd2c0d4bcfebba4", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/747908e00884a73ca1167fb09fd2c0d4bcfebba4", "committedDate": "2019-12-12T12:47:14Z", "message": "WIP - working on restore API"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0d6fb6def4c277a3aebf521ab3cc1b71931a4d7e", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/0d6fb6def4c277a3aebf521ab3cc1b71931a4d7e", "committedDate": "2019-12-12T14:41:03Z", "message": "Merge remote-tracking branch 'upstream/master' into 48366-dangling-indices-api"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4f63acc7c4349bf390adfdf293bfde838b6ae80a", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/4f63acc7c4349bf390adfdf293bfde838b6ae80a", "committedDate": "2019-12-13T13:12:54Z", "message": "Wow, restores seem to work"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "48437419da3c46ccfafe5f3784d7447aa45596bc", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/48437419da3c46ccfafe5f3784d7447aa45596bc", "committedDate": "2019-12-16T09:25:42Z", "message": "Use a better REST model for restores"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b335d44aa839d47ac0b448880e07bee65b001ecc", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/b335d44aa839d47ac0b448880e07bee65b001ecc", "committedDate": "2019-12-18T09:50:21Z", "message": "Test fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ac76ed4aacc6b4bced086064b6ce4febb9a440e4", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/ac76ed4aacc6b4bced086064b6ce4febb9a440e4", "committedDate": "2019-12-20T10:18:16Z", "message": "WIP - working on a delete endpoint"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7419339bb8260e3ede516a8b65bbb1c456ca295d", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/7419339bb8260e3ede516a8b65bbb1c456ca295d", "committedDate": "2019-12-20T16:35:14Z", "message": "Delete seems to work. Wow."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a52023665ae6e6e97890de43df20823eca5444d5", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/a52023665ae6e6e97890de43df20823eca5444d5", "committedDate": "2020-01-07T13:00:16Z", "message": "Merge remote-tracking branch 'upstream/master' into 48366-dangling-indices-api"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "df9bf0299dbbdb455cd63cd26050a89299e39f7c", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/df9bf0299dbbdb455cd63cd26050a89299e39f7c", "committedDate": "2020-01-09T10:10:03Z", "message": "Polishing"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bbef246c30c839dbb94f7fdbcff1b3fe97045b54", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/bbef246c30c839dbb94f7fdbcff1b3fe97045b54", "committedDate": "2020-01-09T11:22:03Z", "message": "Check data loss flag after validating an index is dangling"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "515dd9b1d48fafce5eb5005cc994eccf33fa87fe", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/515dd9b1d48fafce5eb5005cc994eccf33fa87fe", "committedDate": "2020-01-09T11:22:41Z", "message": "Merge remote-tracking branch 'upstream/master' into 48366-dangling-indices-api"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2234641b7eff8ba93d012c8307469992ed46ab57", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/2234641b7eff8ba93d012c8307469992ed46ab57", "committedDate": "2020-01-09T16:14:51Z", "message": "Check data loss flag when restoring dangling indices"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "969a87a4b132b08b1da5066096ae12e221bde02e", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/969a87a4b132b08b1da5066096ae12e221bde02e", "committedDate": "2020-01-13T09:26:49Z", "message": "Ensure nodes are removed before deleting indices"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9847ed1c83c29af42b3846cf20dfdba72c63be73", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/9847ed1c83c29af42b3846cf20dfdba72c63be73", "committedDate": "2020-01-13T11:46:34Z", "message": "Merge remote-tracking branch 'upstream/master' into 48366-dangling-indices-api"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b9b9b9746c89894e6ca27f1cdfa1c16e3e828946", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/b9b9b9746c89894e6ca27f1cdfa1c16e3e828946", "committedDate": "2020-01-13T14:15:14Z", "message": "Add lots of Javadoc"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "eaf940f3ff9904380bcb58ec8e8fc621016e244b", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/eaf940f3ff9904380bcb58ec8e8fc621016e244b", "committedDate": "2020-01-13T14:18:09Z", "message": "Add missing license info"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "aadc7cc6175d0aea252c115dee4a9312b2655ca0", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/aadc7cc6175d0aea252c115dee4a9312b2655ca0", "committedDate": "2020-01-13T14:23:27Z", "message": "Checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/a0aa2366a228038d72442514bb80496174ee6d50", "committedDate": "2020-01-13T14:39:16Z", "message": "Merge remote-tracking branch 'upstream/master' into 48366-dangling-indices-api"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQyNTQ4ODUz", "url": "https://github.com/elastic/elasticsearch/pull/50920#pullrequestreview-342548853", "createdAt": "2020-01-14T13:58:05Z", "commit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 24, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxMzo1ODowNVrOFdYUcg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNTowMDoxOFrOFdagQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM1MTQ3NA==", "bodyText": "unrelated to this PR, please revert", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366351474", "createdAt": "2020-01-14T13:58:05Z", "author": {"login": "ywelsch"}, "path": ".eclipseformat.xml", "diffHunk": "@@ -62,7 +62,7 @@\n         <setting id=\"org.eclipse.jdt.core.formatter.insert_space_after_opening_angle_bracket_in_type_arguments\" value=\"do not insert\"/>\n         <setting id=\"org.eclipse.jdt.core.formatter.insert_new_line_after_annotation_on_method\" value=\"insert\"/>\n         <setting id=\"org.eclipse.jdt.core.formatter.insert_space_after_opening_paren_in_switch\" value=\"do not insert\"/>\n-        <setting id=\"org.eclipse.jdt.core.formatter.alignment_for_parameterized_type_references\" value=\"0\"/>\n+        <setting id=\"org.eclipse.jdt.core.formatter.alignment_for_parameterized_type_references\" value=\"48\"/>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM1MTkzMw==", "bodyText": "should this be dangling_indices.delete?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366351933", "createdAt": "2020-01-14T13:58:55Z", "author": {"login": "ywelsch"}, "path": "rest-api-spec/src/main/resources/rest-api-spec/api/dangling_indices.delete.json", "diffHunk": "@@ -0,0 +1,34 @@\n+{\n+  \"dangling_indices.restore\": {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 2}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM1MjE5OQ==", "bodyText": "indexUUID", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366352199", "createdAt": "2020-01-14T13:59:21Z", "author": {"login": "ywelsch"}, "path": "rest-api-spec/src/main/resources/rest-api-spec/api/dangling_indices.delete.json", "diffHunk": "@@ -0,0 +1,34 @@\n+{\n+  \"dangling_indices.restore\": {\n+    \"documentation\": {\n+      \"description\": \"Deletes the specified dangling index\"\n+    },\n+    \"stability\": \"experimental\",\n+    \"url\": {\n+      \"paths\": [\n+        {\n+          \"path\": \"/_dangling/{indexUuid}\",\n+          \"methods\": [\n+            \"DELETE\"\n+          ],\n+          \"parts\": {\n+            \"indexUuid\": {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM1MjYyOA==", "bodyText": "why is a body required for deletion?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366352628", "createdAt": "2020-01-14T14:00:11Z", "author": {"login": "ywelsch"}, "path": "rest-api-spec/src/main/resources/rest-api-spec/api/dangling_indices.delete.json", "diffHunk": "@@ -0,0 +1,34 @@\n+{\n+  \"dangling_indices.restore\": {\n+    \"documentation\": {\n+      \"description\": \"Deletes the specified dangling index\"\n+    },\n+    \"stability\": \"experimental\",\n+    \"url\": {\n+      \"paths\": [\n+        {\n+          \"path\": \"/_dangling/{indexUuid}\",\n+          \"methods\": [\n+            \"DELETE\"\n+          ],\n+          \"parts\": {\n+            \"indexUuid\": {\n+              \"type\": \"string\",\n+              \"description\": \"The UUID of the dangling index\"\n+            }\n+          }\n+        }\n+      ]\n+    },\n+    \"params\": {\n+      \"accept_data_loss\": {\n+        \"type\": \"boolean\",\n+        \"description\": \"Must be set to true in order to delete the dangling index\"\n+      }\n+    },\n+    \"body\": {\n+      \"description\": \"Supplies options to the request\",\n+      \"required\": true", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM1MzQ1Ng==", "bodyText": "Perhaps use the Import terminology here instead of Restore? That's how we have been describing the reintroduction of dangling indices into the cluster.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366353456", "createdAt": "2020-01-14T14:01:44Z", "author": {"login": "ywelsch"}, "path": "rest-api-spec/src/main/resources/rest-api-spec/api/dangling_indices.restore.json", "diffHunk": "@@ -0,0 +1,34 @@\n+{\n+  \"dangling_indices.restore\": {\n+    \"documentation\": {\n+      \"description\": \"Restores the specified dangling index\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM1NDI3Nw==", "bodyText": "why is a body required?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366354277", "createdAt": "2020-01-14T14:03:19Z", "author": {"login": "ywelsch"}, "path": "rest-api-spec/src/main/resources/rest-api-spec/api/dangling_indices.restore.json", "diffHunk": "@@ -0,0 +1,34 @@\n+{\n+  \"dangling_indices.restore\": {\n+    \"documentation\": {\n+      \"description\": \"Restores the specified dangling index\"\n+    },\n+    \"stability\": \"experimental\",\n+    \"url\": {\n+      \"paths\": [\n+        {\n+          \"path\": \"/_dangling/{indexUuid}\",\n+          \"methods\": [\n+            \"POST\"\n+          ],\n+          \"parts\": {\n+            \"indexUuid\": {\n+              \"type\": \"string\",\n+              \"description\": \"The UUID of the dangling index\"\n+            }\n+          }\n+        }\n+      ]\n+    },\n+    \"params\": {\n+      \"accept_data_loss\": {\n+        \"type\": \"boolean\",\n+        \"description\": \"Must be set to true in order to restore the dangling index\"\n+      }\n+    },\n+    \"body\": {\n+      \"description\": \"Supplies options to the request\",\n+      \"required\": true", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM1Nzc1OQ==", "bodyText": "let's also add the index.creation_date here, i.e. IndexMetaData.getCreationDate().", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366357759", "createdAt": "2020-01-14T14:09:57Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DanglingIndexInfo.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.support.nodes.BaseNodeResponse;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ToXContentObject;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Contains information about a dangling index, i.e. an index that Elasticsearch has found\n+ * on-disk but is not present in the cluster state.\n+ */\n+public class DanglingIndexInfo extends BaseNodeResponse implements ToXContentObject {\n+    private String indexName;\n+    private String indexUUID;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM1ODk2NQ==", "bodyText": "Should we also output node name here?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366358965", "createdAt": "2020-01-14T14:11:58Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DanglingIndexInfo.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.support.nodes.BaseNodeResponse;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ToXContentObject;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Contains information about a dangling index, i.e. an index that Elasticsearch has found\n+ * on-disk but is not present in the cluster state.\n+ */\n+public class DanglingIndexInfo extends BaseNodeResponse implements ToXContentObject {\n+    private String indexName;\n+    private String indexUUID;\n+\n+    public DanglingIndexInfo(DiscoveryNode node, String indexName, String indexUUID) {\n+        super(node);\n+        this.indexName = indexName;\n+        this.indexUUID = indexUUID;\n+    }\n+\n+    public DanglingIndexInfo(StreamInput in) throws IOException {\n+        super(in);\n+        this.indexName = in.readString();\n+        this.indexUUID = in.readString();\n+    }\n+\n+    public String getIndexName() {\n+        return indexName;\n+    }\n+\n+    public String getIndexUUID() {\n+        return indexUUID;\n+    }\n+\n+    public String getNodeId() {\n+        return this.getNode().getId();\n+    }\n+\n+    @Override\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        builder.startObject();\n+        builder.field(\"nodeId\", this.getNodeId());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM2MDU2Mw==", "bodyText": "would be more convenient as a URL parameter. Deleting/Importing dangling indices should not require a body.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366360563", "createdAt": "2020-01-14T14:14:32Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DeleteDanglingIndexRequest.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionRequestValidationException;\n+import org.elasticsearch.action.support.master.MasterNodeRequest;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+\n+/**\n+ * Represents a request to delete a particular dangling index, specified by its UUID. The {@link #acceptDataLoss}\n+ * flag must also be explicitly set to true, or later validation will fail.\n+ */\n+public class DeleteDanglingIndexRequest extends MasterNodeRequest<DeleteDanglingIndexRequest> {\n+    private String indexUuid;\n+    private boolean acceptDataLoss = false;\n+\n+    public DeleteDanglingIndexRequest(StreamInput in) throws IOException {\n+        super(in);\n+        this.indexUuid = in.readString();\n+        this.acceptDataLoss = in.readBoolean();\n+    }\n+\n+    public DeleteDanglingIndexRequest() {\n+        super();\n+    }\n+\n+    public DeleteDanglingIndexRequest(String indexUuid, boolean acceptDataLoss) {\n+        super();\n+        this.indexUuid = indexUuid;\n+        this.acceptDataLoss = acceptDataLoss;\n+    }\n+\n+    @Override\n+    public ActionRequestValidationException validate() {\n+        if (this.indexUuid == null) {\n+            ActionRequestValidationException e = new ActionRequestValidationException();\n+            e.addValidationError(\"No index ID specified\");\n+            return e;\n+        }\n+\n+        return null;\n+    }\n+\n+    public String getIndexUuid() {\n+        return indexUuid;\n+    }\n+\n+    public void setIndexUuid(String indexUuid) {\n+        this.indexUuid = indexUuid;\n+    }\n+\n+    public boolean isAcceptDataLoss() {\n+        return acceptDataLoss;\n+    }\n+\n+    public void setAcceptDataLoss(boolean acceptDataLoss) {\n+        this.acceptDataLoss = acceptDataLoss;\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return \"delete dangling index\";\n+    }\n+\n+    public void source(Map<String, Object> source) {\n+        source.forEach((name, value) -> {\n+            if (\"accept_data_loss\".equals(name)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM2MTIzMw==", "bodyText": "why the extra \"status\" field?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366361233", "createdAt": "2020-01-14T14:15:49Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DeleteDanglingIndexResponse.java", "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionResponse;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.StatusToXContentObject;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.rest.RestStatus;\n+\n+import java.io.IOException;\n+\n+public class DeleteDanglingIndexResponse extends ActionResponse implements StatusToXContentObject {\n+    public DeleteDanglingIndexResponse() {\n+    }\n+\n+    public DeleteDanglingIndexResponse(StreamInput in) {\n+    }\n+\n+    @Override\n+    public RestStatus status() {\n+        return RestStatus.ACCEPTED;\n+    }\n+\n+    @Override\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        return builder.startObject().field(\"status\", \"ok\").endObject();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM2MTU1OQ==", "bodyText": "call super here?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366361559", "createdAt": "2020-01-14T14:16:23Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DeleteDanglingIndexResponse.java", "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionResponse;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.StatusToXContentObject;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.rest.RestStatus;\n+\n+import java.io.IOException;\n+\n+public class DeleteDanglingIndexResponse extends ActionResponse implements StatusToXContentObject {\n+    public DeleteDanglingIndexResponse() {\n+    }\n+\n+    public DeleteDanglingIndexResponse(StreamInput in) {\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM2MTYzMA==", "bodyText": "call super here?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366361630", "createdAt": "2020-01-14T14:16:30Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DeleteDanglingIndexResponse.java", "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionResponse;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.StatusToXContentObject;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.rest.RestStatus;\n+\n+import java.io.IOException;\n+\n+public class DeleteDanglingIndexResponse extends ActionResponse implements StatusToXContentObject {\n+    public DeleteDanglingIndexResponse() {\n+    }\n+\n+    public DeleteDanglingIndexResponse(StreamInput in) {\n+    }\n+\n+    @Override\n+    public RestStatus status() {\n+        return RestStatus.ACCEPTED;\n+    }\n+\n+    @Override\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        return builder.startObject().field(\"status\", \"ok\").endObject();\n+    }\n+\n+    @Override\n+    public void writeTo(StreamOutput out) throws IOException {\n+        // no fields to write", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM2MzM1NQ==", "bodyText": "might be convenient later to allow selecting subset of nodes here\nPerhaps use (String[]) null for now like the other usages?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366363355", "createdAt": "2020-01-14T14:19:40Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ListDanglingIndicesRequest.java", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionRequestValidationException;\n+import org.elasticsearch.action.support.nodes.BaseNodesRequest;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+\n+import java.io.IOException;\n+\n+public class ListDanglingIndicesRequest extends BaseNodesRequest<ListDanglingIndicesRequest> {\n+    public ListDanglingIndicesRequest(StreamInput in) throws IOException {\n+        super(in);\n+    }\n+\n+    public ListDanglingIndicesRequest() {\n+        super(new String[0]);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM2NDg0Nw==", "bodyText": "Why SERVICE_UNAVAILABLE? Should this not just be a 500?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366364847", "createdAt": "2020-01-14T14:22:14Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ListDanglingIndicesResponse.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.nodes.BaseNodesResponse;\n+import org.elasticsearch.cluster.ClusterName;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.StatusToXContentObject;\n+import org.elasticsearch.common.xcontent.XContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.rest.RestStatus;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * Models a response to a {@link ListDanglingIndicesRequest}. A list request queries every node in the\n+ * cluster and aggregates their responses. When the aggregated response is converted to {@link XContent},\n+ * information for each dangling index is presented under the \"dangling_indices\" key. If any nodes\n+ * in the cluster failed to answer, the details are presented under the \"failed_nodes\" key.\n+ */\n+public class ListDanglingIndicesResponse extends BaseNodesResponse<NodeDanglingIndicesResponse> implements StatusToXContentObject {\n+\n+    public ListDanglingIndicesResponse(StreamInput in) throws IOException {\n+        super(in);\n+    }\n+\n+    public ListDanglingIndicesResponse(\n+        ClusterName clusterName,\n+        List<NodeDanglingIndicesResponse> nodes,\n+        List<FailedNodeException> failures\n+    ) {\n+        super(clusterName, nodes, failures);\n+    }\n+\n+    @Override\n+    public RestStatus status() {\n+        return this.hasFailures() ? RestStatus.SERVICE_UNAVAILABLE : RestStatus.OK;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM2NjA0OA==", "bodyText": "The full IndexMetaData might be large, and collecting all of these on a coordinator node might lead to a lot of data transfer and make that node blow up. Let's just return what's needed, index uuid and name and creation date.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366366048", "createdAt": "2020-01-14T14:24:12Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/NodeDanglingIndicesResponse.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.support.nodes.BaseNodeResponse;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Used when querying every node in the cluster for dangling indices, in response to a list request.\n+ */\n+public class NodeDanglingIndicesResponse extends BaseNodeResponse {\n+    private final List<IndexMetaData> indexMetaData;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM2NjM3MA==", "bodyText": "use in.readList", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366366370", "createdAt": "2020-01-14T14:24:51Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/NodeDanglingIndicesResponse.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.support.nodes.BaseNodeResponse;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Used when querying every node in the cluster for dangling indices, in response to a list request.\n+ */\n+public class NodeDanglingIndicesResponse extends BaseNodeResponse {\n+    private final List<IndexMetaData> indexMetaData;\n+\n+    public List<IndexMetaData> getDanglingIndices() {\n+        return this.indexMetaData;\n+    }\n+\n+    public NodeDanglingIndicesResponse(DiscoveryNode node, List<IndexMetaData> indexMetaData) {\n+        super(node);\n+        this.indexMetaData = indexMetaData;\n+    }\n+\n+    protected NodeDanglingIndicesResponse(StreamInput in) throws IOException {\n+        super(in);\n+\n+        final int size = in.readInt();\n+        this.indexMetaData = new ArrayList<>(size);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM2NjQ4Mg==", "bodyText": "use out.writeList", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366366482", "createdAt": "2020-01-14T14:25:03Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/NodeDanglingIndicesResponse.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.support.nodes.BaseNodeResponse;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Used when querying every node in the cluster for dangling indices, in response to a list request.\n+ */\n+public class NodeDanglingIndicesResponse extends BaseNodeResponse {\n+    private final List<IndexMetaData> indexMetaData;\n+\n+    public List<IndexMetaData> getDanglingIndices() {\n+        return this.indexMetaData;\n+    }\n+\n+    public NodeDanglingIndicesResponse(DiscoveryNode node, List<IndexMetaData> indexMetaData) {\n+        super(node);\n+        this.indexMetaData = indexMetaData;\n+    }\n+\n+    protected NodeDanglingIndicesResponse(StreamInput in) throws IOException {\n+        super(in);\n+\n+        final int size = in.readInt();\n+        this.indexMetaData = new ArrayList<>(size);\n+\n+        for (int i = 0; i < size; i++) {\n+            this.indexMetaData.add(IndexMetaData.readFrom(in));\n+        }\n+    }\n+\n+    @Override\n+    public void writeTo(StreamOutput out) throws IOException {\n+        super.writeTo(out);\n+\n+        out.writeInt(this.indexMetaData.size());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM2NzUyNQ==", "bodyText": "similar to accept_data_loss flag, I wonder if the node id could just be URL parameter. In this case, it's less clear if we want to add other parameters.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366367525", "createdAt": "2020-01-14T14:26:58Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/RestoreDanglingIndexRequest.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionRequestValidationException;\n+import org.elasticsearch.action.support.nodes.BaseNodesRequest;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+\n+/**\n+ * Represents a request to restore a particular dangling index, specified\n+ * by its UUID and optionally the node ID, if the dangling index exists on\n+ * more than one node. The {@link #acceptDataLoss} flag must also be\n+ * explicitly set to true, or later validation will fail.\n+ */\n+public class RestoreDanglingIndexRequest extends BaseNodesRequest<RestoreDanglingIndexRequest> {\n+    private String indexUuid;\n+    private boolean acceptDataLoss;\n+    private String nodeId;\n+\n+    public RestoreDanglingIndexRequest(StreamInput in) throws IOException {\n+        super(in);\n+        this.indexUuid = in.readString();\n+        this.acceptDataLoss = in.readBoolean();\n+        this.nodeId = in.readOptionalString();\n+    }\n+\n+    public RestoreDanglingIndexRequest() {\n+        super(new String[0]);\n+    }\n+\n+    public RestoreDanglingIndexRequest(String indexUuid, boolean acceptDataLoss) {\n+        this();\n+        this.indexUuid = indexUuid;\n+        this.acceptDataLoss = acceptDataLoss;\n+    }\n+\n+    @Override\n+    public ActionRequestValidationException validate() {\n+        if (this.indexUuid == null || this.indexUuid.isEmpty()) {\n+            ActionRequestValidationException e = new ActionRequestValidationException();\n+            e.addValidationError(\"No index UUID specified\");\n+            return e;\n+        }\n+\n+        return null;\n+    }\n+\n+    public String getIndexUuid() {\n+        return indexUuid;\n+    }\n+\n+    public void setIndexUuid(String indexUuid) {\n+        this.indexUuid = indexUuid;\n+    }\n+\n+    public String getNodeId() {\n+        return nodeId;\n+    }\n+\n+    public void setNodeId(String nodeId) {\n+        this.nodeId = nodeId;\n+    }\n+\n+    public boolean isAcceptDataLoss() {\n+        return acceptDataLoss;\n+    }\n+\n+    public void setAcceptDataLoss(boolean acceptDataLoss) {\n+        this.acceptDataLoss = acceptDataLoss;\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return \"restore dangling index\";\n+    }\n+\n+    public void source(Map<String, Object> source) {\n+        source.forEach((name, value) -> {\n+            switch (name) {\n+                case \"accept_data_loss\":\n+                    if (value instanceof Boolean) {\n+                        this.acceptDataLoss = (boolean) value;\n+                    } else {\n+                        throw new IllegalArgumentException(\"malformed accept_data_loss\");\n+                    }\n+                    break;\n+\n+                case \"node_id\":", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM2ODgwOQ==", "bodyText": "this will lead to a lot of duplicated entries, if many nodes contain the same dangling index. I wonder if we should aggregate this somehow?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366368809", "createdAt": "2020-01-14T14:29:18Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ListDanglingIndicesResponse.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.nodes.BaseNodesResponse;\n+import org.elasticsearch.cluster.ClusterName;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.StatusToXContentObject;\n+import org.elasticsearch.common.xcontent.XContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.rest.RestStatus;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * Models a response to a {@link ListDanglingIndicesRequest}. A list request queries every node in the\n+ * cluster and aggregates their responses. When the aggregated response is converted to {@link XContent},\n+ * information for each dangling index is presented under the \"dangling_indices\" key. If any nodes\n+ * in the cluster failed to answer, the details are presented under the \"failed_nodes\" key.\n+ */\n+public class ListDanglingIndicesResponse extends BaseNodesResponse<NodeDanglingIndicesResponse> implements StatusToXContentObject {\n+\n+    public ListDanglingIndicesResponse(StreamInput in) throws IOException {\n+        super(in);\n+    }\n+\n+    public ListDanglingIndicesResponse(\n+        ClusterName clusterName,\n+        List<NodeDanglingIndicesResponse> nodes,\n+        List<FailedNodeException> failures\n+    ) {\n+        super(clusterName, nodes, failures);\n+    }\n+\n+    @Override\n+    public RestStatus status() {\n+        return this.hasFailures() ? RestStatus.SERVICE_UNAVAILABLE : RestStatus.OK;\n+    }\n+\n+    @Override\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        builder.startObject();\n+\n+        builder.startArray(\"dangling_indices\");\n+        for (NodeDanglingIndicesResponse nodeResponse : this.getNodes()) {\n+            for (IndexMetaData indexMetaData : nodeResponse.getDanglingIndices()) {\n+                DanglingIndexInfo danglingIndexInfo = new DanglingIndexInfo(\n+                    nodeResponse.getNode(),\n+                    indexMetaData.getIndex().getName(),\n+                    indexMetaData.getIndexUUID()\n+                );\n+                danglingIndexInfo.toXContent(builder, params);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM3MDc5Ng==", "bodyText": "Why tie this into the graveyard? I was wondering if we should just reach out to all the nodes and delete the on-disk index folder, after checking that the index with the given UUID does not exist in the cluster state. The advantage is that this would not need to involve the master. @DaveCTurner thoughts?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366370796", "createdAt": "2020-01-14T14:32:39Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.admin.indices.delete.DeleteIndexRequest;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ack.ClusterStateUpdateResponse;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Priority;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM3MjAyNw==", "bodyText": "Isn't the request already validated earlier?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366372027", "createdAt": "2020-01-14T14:34:46Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.admin.indices.delete.DeleteIndexRequest;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ack.ClusterStateUpdateResponse;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Priority;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, DeleteDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.WRITE;\n+    }\n+\n+    @Override\n+    protected DeleteDanglingIndexResponse read(StreamInput in) throws IOException {\n+        return new DeleteDanglingIndexResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest request,\n+        ClusterState state,\n+        ActionListener<DeleteDanglingIndexResponse> listener\n+    ) throws Exception {\n+        IndexMetaData indexMetaDataToDelete;\n+        try {\n+            indexMetaDataToDelete = getIndexMetaDataToDelete(request);\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+            return;\n+        }\n+        final String indexName = indexMetaDataToDelete.getIndex().getName();\n+\n+        final ActionListener<ClusterStateUpdateResponse> actionListener = new ActionListener<>() {\n+            @Override\n+            public void onResponse(ClusterStateUpdateResponse clusterStateUpdateResponse) {\n+                listener.onResponse(new DeleteDanglingIndexResponse());\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.warn(\"Failed to delete dangling index [{}]\" + indexName, e);\n+                listener.onFailure(e);\n+            }\n+        };\n+\n+        // This flag is checked at this point so that we always check that the supplied index ID\n+        // does correspond to a dangling index.\n+        if (request.isAcceptDataLoss() == false) {\n+            throw new IllegalArgumentException(\"accept_data_loss must be set to true\");\n+        }\n+\n+        this.clusterService.submitStateUpdateTask(\n+            \"delete-dangling-index \" + indexName,\n+            new AckedClusterStateUpdateTask<>(Priority.NORMAL, new DeleteIndexRequest(), actionListener) {\n+\n+                @Override\n+                protected ClusterStateUpdateResponse newResponse(boolean acknowledged) {\n+                    return new ClusterStateUpdateResponse(acknowledged);\n+                }\n+\n+                @Override\n+                public ClusterState execute(final ClusterState currentState) {\n+                    return deleteDanglingIndex(currentState, indexMetaDataToDelete);\n+                }\n+            }\n+        );\n+    }\n+\n+    private ClusterState deleteDanglingIndex(ClusterState currentState, IndexMetaData indexMetaDataToDelete) {\n+        final MetaData meta = currentState.metaData();\n+\n+        MetaData.Builder metaDataBuilder = MetaData.builder(meta);\n+\n+        final IndexGraveyard.Builder graveyardBuilder = IndexGraveyard.builder(metaDataBuilder.indexGraveyard());\n+\n+        final IndexGraveyard newGraveyard = graveyardBuilder.addTombstone(indexMetaDataToDelete.getIndex()).build(settings);\n+        metaDataBuilder.indexGraveyard(newGraveyard);\n+\n+        return ClusterState.builder(currentState).metaData(metaDataBuilder.build()).build();\n+    }\n+\n+    @Override\n+    protected ClusterBlockException checkBlock(DeleteDanglingIndexRequest request, ClusterState state) {\n+        return null;\n+    }\n+\n+    private IndexMetaData getIndexMetaDataToDelete(DeleteDanglingIndexRequest request) {\n+        String indexUuid = request.getIndexUuid();\n+\n+        if (indexUuid == null || indexUuid.isEmpty()) {\n+            throw new IllegalArgumentException(\"No index UUID specified in request\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 168}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM4MTU4OQ==", "bodyText": "why the write thread pool? That's usually used for indexing", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366381589", "createdAt": "2020-01-14T14:51:22Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.admin.indices.delete.DeleteIndexRequest;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ack.ClusterStateUpdateResponse;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Priority;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, DeleteDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.WRITE;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM4MTk1Nw==", "bodyText": "ouch, why do a blocking call here?\nWhy query the full list of dangling indices just to delete one?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366381957", "createdAt": "2020-01-14T14:51:56Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.admin.indices.delete.DeleteIndexRequest;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ack.ClusterStateUpdateResponse;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Priority;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, DeleteDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.WRITE;\n+    }\n+\n+    @Override\n+    protected DeleteDanglingIndexResponse read(StreamInput in) throws IOException {\n+        return new DeleteDanglingIndexResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest request,\n+        ClusterState state,\n+        ActionListener<DeleteDanglingIndexResponse> listener\n+    ) throws Exception {\n+        IndexMetaData indexMetaDataToDelete;\n+        try {\n+            indexMetaDataToDelete = getIndexMetaDataToDelete(request);\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+            return;\n+        }\n+        final String indexName = indexMetaDataToDelete.getIndex().getName();\n+\n+        final ActionListener<ClusterStateUpdateResponse> actionListener = new ActionListener<>() {\n+            @Override\n+            public void onResponse(ClusterStateUpdateResponse clusterStateUpdateResponse) {\n+                listener.onResponse(new DeleteDanglingIndexResponse());\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.warn(\"Failed to delete dangling index [{}]\" + indexName, e);\n+                listener.onFailure(e);\n+            }\n+        };\n+\n+        // This flag is checked at this point so that we always check that the supplied index ID\n+        // does correspond to a dangling index.\n+        if (request.isAcceptDataLoss() == false) {\n+            throw new IllegalArgumentException(\"accept_data_loss must be set to true\");\n+        }\n+\n+        this.clusterService.submitStateUpdateTask(\n+            \"delete-dangling-index \" + indexName,\n+            new AckedClusterStateUpdateTask<>(Priority.NORMAL, new DeleteIndexRequest(), actionListener) {\n+\n+                @Override\n+                protected ClusterStateUpdateResponse newResponse(boolean acknowledged) {\n+                    return new ClusterStateUpdateResponse(acknowledged);\n+                }\n+\n+                @Override\n+                public ClusterState execute(final ClusterState currentState) {\n+                    return deleteDanglingIndex(currentState, indexMetaDataToDelete);\n+                }\n+            }\n+        );\n+    }\n+\n+    private ClusterState deleteDanglingIndex(ClusterState currentState, IndexMetaData indexMetaDataToDelete) {\n+        final MetaData meta = currentState.metaData();\n+\n+        MetaData.Builder metaDataBuilder = MetaData.builder(meta);\n+\n+        final IndexGraveyard.Builder graveyardBuilder = IndexGraveyard.builder(metaDataBuilder.indexGraveyard());\n+\n+        final IndexGraveyard newGraveyard = graveyardBuilder.addTombstone(indexMetaDataToDelete.getIndex()).build(settings);\n+        metaDataBuilder.indexGraveyard(newGraveyard);\n+\n+        return ClusterState.builder(currentState).metaData(metaDataBuilder.build()).build();\n+    }\n+\n+    @Override\n+    protected ClusterBlockException checkBlock(DeleteDanglingIndexRequest request, ClusterState state) {\n+        return null;\n+    }\n+\n+    private IndexMetaData getIndexMetaDataToDelete(DeleteDanglingIndexRequest request) {\n+        String indexUuid = request.getIndexUuid();\n+\n+        if (indexUuid == null || indexUuid.isEmpty()) {\n+            throw new IllegalArgumentException(\"No index UUID specified in request\");\n+        }\n+\n+        List<IndexMetaData> matchingMetaData = new ArrayList<>();\n+\n+        final List<NodeDanglingIndicesResponse> nodes = fetchDanglingIndices().actionGet().getNodes();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 173}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM4NzI2NQ==", "bodyText": "This means that dangling indices scanning is still done on every cluster state update. One of the benefits of this PR was that this now would only needed to be done on-demand, i.e. when a user requests the list of dangling indices, not on every CS update.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366387265", "createdAt": "2020-01-14T15:00:18Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java", "diffHunk": "@@ -65,24 +69,25 @@\n \n     private final NodeEnvironment nodeEnv;\n     private final MetaStateService metaStateService;\n-    private final LocalAllocateDangledIndices allocateDangledIndices;\n+    private final LocalAllocateDangledIndices danglingIndicesAllocator;\n     private final boolean isAutoImportDanglingIndicesEnabled;\n \n     private final Map<Index, IndexMetaData> danglingIndices = ConcurrentCollections.newConcurrentMap();\n \n     @Inject\n     public DanglingIndicesState(NodeEnvironment nodeEnv, MetaStateService metaStateService,\n-                                LocalAllocateDangledIndices allocateDangledIndices, ClusterService clusterService) {\n+                                LocalAllocateDangledIndices danglingIndicesAllocator, ClusterService clusterService) {\n         this.nodeEnv = nodeEnv;\n         this.metaStateService = metaStateService;\n-        this.allocateDangledIndices = allocateDangledIndices;\n+        this.danglingIndicesAllocator = danglingIndicesAllocator;\n \n         this.isAutoImportDanglingIndicesEnabled = AUTO_IMPORT_DANGLING_INDICES_SETTING.get(clusterService.getSettings());\n \n-        if (this.isAutoImportDanglingIndicesEnabled) {\n-            clusterService.addListener(this);\n-        } else {\n-            logger.warn(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey() + \" is disabled, dangling indices will not be detected or imported\");\n+        clusterService.addListener(this);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 47}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "65ef49c2477b57881ff7a9b6094a4f50248ab7b8", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/65ef49c2477b57881ff7a9b6094a4f50248ab7b8", "committedDate": "2020-01-21T14:03:53Z", "message": "Merge remote-tracking branch 'upstream/master' into 48366-dangling-indices-api"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8b27586f19fc5af3b2423aa3af74bfb441dd4b92", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/8b27586f19fc5af3b2423aa3af74bfb441dd4b92", "committedDate": "2020-01-21T14:26:36Z", "message": "Rename restore to import, uppercase UUID refs\n\nRefer to \"importing\" dangling indices rather than \"restoring\" them. Also\nkeeping UUID uppercased in various places, instead of camel-cased."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a68b1fa77face32e6a2bc4facc7af4e10bca3034", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/a68b1fa77face32e6a2bc4facc7af4e10bca3034", "committedDate": "2020-01-21T14:31:39Z", "message": "Add node_name and creation_date fields to DanglingIndexInfo"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a956490470f4ddd47fb87c1a995727e3b8167d40", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/a956490470f4ddd47fb87c1a995727e3b8167d40", "committedDate": "2020-01-21T14:50:49Z", "message": "Convert danling index API request bodies to URL params"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "07638ed35ca9f38e56186792e2dd821158347eff", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/07638ed35ca9f38e56186792e2dd821158347eff", "committedDate": "2020-01-21T15:20:24Z", "message": "Cleanups"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/820f07e77f57307b839c3cf4356ed56db2d5a796", "committedDate": "2020-01-21T15:51:20Z", "message": "Improve NodeDanglingIndicesResponse serialisation"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ1OTc3NDky", "url": "https://github.com/elastic/elasticsearch/pull/50920#pullrequestreview-345977492", "createdAt": "2020-01-21T15:42:37Z", "commit": {"oid": "07638ed35ca9f38e56186792e2dd821158347eff"}, "state": "COMMENTED", "comments": {"totalCount": 28, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNTo0MjozN1rOFf-tcw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNzoxMDowOFrOFgB85w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTA3NzYxOQ==", "bodyText": "I think we can be less tentative about this API, unless you know of some specific future changes needed to move it out of an experimental state (in which case, what are they?).", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369077619", "createdAt": "2020-01-21T15:42:37Z", "author": {"login": "DaveCTurner"}, "path": "rest-api-spec/src/main/resources/rest-api-spec/api/dangling_indices.delete.json", "diffHunk": "@@ -0,0 +1,30 @@\n+{\n+  \"dangling_indices.delete\": {\n+    \"documentation\": {\n+      \"description\": \"Deletes the specified dangling index\"\n+    },\n+    \"stability\": \"experimental\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07638ed35ca9f38e56186792e2dd821158347eff"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTA3NzgyMw==", "bodyText": "I think we can be less tentative about this API, unless you know of some specific future changes needed to move it out of an experimental state (in which case, what are they?).", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369077823", "createdAt": "2020-01-21T15:42:57Z", "author": {"login": "DaveCTurner"}, "path": "rest-api-spec/src/main/resources/rest-api-spec/api/dangling_indices.import.json", "diffHunk": "@@ -0,0 +1,34 @@\n+{\n+  \"dangling_indices.import\": {\n+    \"documentation\": {\n+      \"description\": \"Imports the specified dangling index\"\n+    },\n+    \"stability\": \"experimental\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07638ed35ca9f38e56186792e2dd821158347eff"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTA3ODIwNA==", "bodyText": "I think we can be less tentative about this API, unless you know of some specific future changes needed to move it out of an experimental state (in which case, what are they?).", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369078204", "createdAt": "2020-01-21T15:43:33Z", "author": {"login": "DaveCTurner"}, "path": "rest-api-spec/src/main/resources/rest-api-spec/api/dangling_indices.list.json", "diffHunk": "@@ -0,0 +1,19 @@\n+{\n+  \"dangling_indices.list\":{\n+    \"documentation\":{\n+      \"description\":\"Returns all dangling indices.\"\n+    },\n+    \"stability\":\"experimental\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07638ed35ca9f38e56186792e2dd821158347eff"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTA4MjAwNQ==", "bodyText": "StreamInput#readString and StreamOutput#writeString do not deal in nulls, so request that's this invalid can't even be (de)serialised. Could we drop the default constructor and the setters, make the fields final, use Objects.requireNonNull(indexUUID) in the 2-argument constructor, and avoid dealing with nulls at all?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369082005", "createdAt": "2020-01-21T15:49:28Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DeleteDanglingIndexRequest.java", "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionRequestValidationException;\n+import org.elasticsearch.action.support.master.MasterNodeRequest;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Represents a request to delete a particular dangling index, specified by its UUID. The {@link #acceptDataLoss}\n+ * flag must also be explicitly set to true, or later validation will fail.\n+ */\n+public class DeleteDanglingIndexRequest extends MasterNodeRequest<DeleteDanglingIndexRequest> {\n+    private String indexUUID;\n+    private boolean acceptDataLoss = false;\n+\n+    public DeleteDanglingIndexRequest(StreamInput in) throws IOException {\n+        super(in);\n+        this.indexUUID = in.readString();\n+        this.acceptDataLoss = in.readBoolean();\n+    }\n+\n+    public DeleteDanglingIndexRequest() {\n+        super();\n+    }\n+\n+    public DeleteDanglingIndexRequest(String indexUUID, boolean acceptDataLoss) {\n+        super();\n+        this.indexUUID = indexUUID;\n+        this.acceptDataLoss = acceptDataLoss;\n+    }\n+\n+    @Override\n+    public ActionRequestValidationException validate() {\n+        if (this.indexUUID == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07638ed35ca9f38e56186792e2dd821158347eff"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTA4Mzk2Mw==", "bodyText": "could we include the field values here? An IDE-generated toString() method is normally fine.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369083963", "createdAt": "2020-01-21T15:52:33Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DeleteDanglingIndexRequest.java", "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionRequestValidationException;\n+import org.elasticsearch.action.support.master.MasterNodeRequest;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Represents a request to delete a particular dangling index, specified by its UUID. The {@link #acceptDataLoss}\n+ * flag must also be explicitly set to true, or later validation will fail.\n+ */\n+public class DeleteDanglingIndexRequest extends MasterNodeRequest<DeleteDanglingIndexRequest> {\n+    private String indexUUID;\n+    private boolean acceptDataLoss = false;\n+\n+    public DeleteDanglingIndexRequest(StreamInput in) throws IOException {\n+        super(in);\n+        this.indexUUID = in.readString();\n+        this.acceptDataLoss = in.readBoolean();\n+    }\n+\n+    public DeleteDanglingIndexRequest() {\n+        super();\n+    }\n+\n+    public DeleteDanglingIndexRequest(String indexUUID, boolean acceptDataLoss) {\n+        super();\n+        this.indexUUID = indexUUID;\n+        this.acceptDataLoss = acceptDataLoss;\n+    }\n+\n+    @Override\n+    public ActionRequestValidationException validate() {\n+        if (this.indexUUID == null) {\n+            ActionRequestValidationException e = new ActionRequestValidationException();\n+            e.addValidationError(\"No index UUID specified\");\n+            return e;\n+        }\n+\n+        // acceptDataLoss is validated later in the transport action, so that the API call can\n+        // be made to check that the UUID exists.\n+\n+        return null;\n+    }\n+\n+    public String getIndexUUID() {\n+        return indexUUID;\n+    }\n+\n+    public void setIndexUUID(String indexUUID) {\n+        this.indexUUID = indexUUID;\n+    }\n+\n+    public boolean isAcceptDataLoss() {\n+        return acceptDataLoss;\n+    }\n+\n+    public void setAcceptDataLoss(boolean acceptDataLoss) {\n+        this.acceptDataLoss = acceptDataLoss;\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return \"delete dangling index\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07638ed35ca9f38e56186792e2dd821158347eff"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTA4NjYzNA==", "bodyText": "there is no super method of this - all of the base classes are abstract AFAICT.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369086634", "createdAt": "2020-01-21T15:56:34Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DeleteDanglingIndexResponse.java", "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionResponse;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.StatusToXContentObject;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.rest.RestStatus;\n+\n+import java.io.IOException;\n+\n+public class DeleteDanglingIndexResponse extends ActionResponse implements StatusToXContentObject {\n+    public DeleteDanglingIndexResponse() {\n+    }\n+\n+    public DeleteDanglingIndexResponse(StreamInput in) {\n+    }\n+\n+    @Override\n+    public RestStatus status() {\n+        return RestStatus.ACCEPTED;\n+    }\n+\n+    @Override\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        return builder.startObject().field(\"status\", \"ok\").endObject();\n+    }\n+\n+    @Override\n+    public void writeTo(StreamOutput out) throws IOException {\n+        // no fields to write", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM2MTYzMA=="}, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTA4OTY4MQ==", "bodyText": "Serialization doesn't work if indexUUID is null, so let's exclude this at construction time. I think we can lose the setters and the default constructor and the non-final fields here as well. I also think we can survive without checking this.indexUUID.isEmpty() - if you pass an empty UUID here we'll just fail to find a matching index later on, right?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369089681", "createdAt": "2020-01-21T16:01:20Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ImportDanglingIndexRequest.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionRequestValidationException;\n+import org.elasticsearch.action.support.nodes.BaseNodesRequest;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Represents a request to import a particular dangling index, specified\n+ * by its UUID and optionally the node ID, if the dangling index exists on\n+ * more than one node. The {@link #acceptDataLoss} flag must also be\n+ * explicitly set to true, or later validation will fail.\n+ */\n+public class ImportDanglingIndexRequest extends BaseNodesRequest<ImportDanglingIndexRequest> {\n+    private String indexUUID;\n+    private boolean acceptDataLoss;\n+    private String nodeId;\n+\n+    public ImportDanglingIndexRequest(StreamInput in) throws IOException {\n+        super(in);\n+        this.indexUUID = in.readString();\n+        this.acceptDataLoss = in.readBoolean();\n+        this.nodeId = in.readOptionalString();\n+    }\n+\n+    public ImportDanglingIndexRequest() {\n+        super(new String[0]);\n+    }\n+\n+    public ImportDanglingIndexRequest(String indexUUID, boolean acceptDataLoss) {\n+        this();\n+        this.indexUUID = indexUUID;\n+        this.acceptDataLoss = acceptDataLoss;\n+    }\n+\n+    @Override\n+    public ActionRequestValidationException validate() {\n+        if (this.indexUUID == null || this.indexUUID.isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTA5MDkxNg==", "bodyText": "Could we add the values of the fields too? Whatever your IDE auto-generates for a toString() is probably fine.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369090916", "createdAt": "2020-01-21T16:03:22Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ImportDanglingIndexRequest.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionRequestValidationException;\n+import org.elasticsearch.action.support.nodes.BaseNodesRequest;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Represents a request to import a particular dangling index, specified\n+ * by its UUID and optionally the node ID, if the dangling index exists on\n+ * more than one node. The {@link #acceptDataLoss} flag must also be\n+ * explicitly set to true, or later validation will fail.\n+ */\n+public class ImportDanglingIndexRequest extends BaseNodesRequest<ImportDanglingIndexRequest> {\n+    private String indexUUID;\n+    private boolean acceptDataLoss;\n+    private String nodeId;\n+\n+    public ImportDanglingIndexRequest(StreamInput in) throws IOException {\n+        super(in);\n+        this.indexUUID = in.readString();\n+        this.acceptDataLoss = in.readBoolean();\n+        this.nodeId = in.readOptionalString();\n+    }\n+\n+    public ImportDanglingIndexRequest() {\n+        super(new String[0]);\n+    }\n+\n+    public ImportDanglingIndexRequest(String indexUUID, boolean acceptDataLoss) {\n+        this();\n+        this.indexUUID = indexUUID;\n+        this.acceptDataLoss = acceptDataLoss;\n+    }\n+\n+    @Override\n+    public ActionRequestValidationException validate() {\n+        if (this.indexUUID == null || this.indexUUID.isEmpty()) {\n+            ActionRequestValidationException e = new ActionRequestValidationException();\n+            e.addValidationError(\"No index UUID specified\");\n+            return e;\n+        }\n+\n+        // acceptDataLoss is validated later in the transport action, so that the API call can\n+        // be made to check that the UUID exists.\n+\n+        return null;\n+    }\n+\n+    public String getIndexUUID() {\n+        return indexUUID;\n+    }\n+\n+    public void setIndexUUID(String indexUUID) {\n+        this.indexUUID = indexUUID;\n+    }\n+\n+    public String getNodeId() {\n+        return nodeId;\n+    }\n+\n+    public void setNodeId(String nodeId) {\n+        this.nodeId = nodeId;\n+    }\n+\n+    public boolean isAcceptDataLoss() {\n+        return acceptDataLoss;\n+    }\n+\n+    public void setAcceptDataLoss(boolean acceptDataLoss) {\n+        this.acceptDataLoss = acceptDataLoss;\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return \"import dangling index\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTA5MjM0NQ==", "bodyText": "acknowledged: true would be more consistent with the other APIs I think.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369092345", "createdAt": "2020-01-21T16:05:43Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ImportDanglingIndexResponse.java", "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionResponse;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.StatusToXContentObject;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.rest.RestStatus;\n+\n+import java.io.IOException;\n+\n+public class ImportDanglingIndexResponse extends ActionResponse implements StatusToXContentObject {\n+    public ImportDanglingIndexResponse() {\n+    }\n+\n+    public ImportDanglingIndexResponse(StreamInput in) {\n+    }\n+\n+    @Override\n+    public RestStatus status() {\n+        return RestStatus.ACCEPTED;\n+    }\n+\n+    @Override\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        return builder.startObject().field(\"status\", \"ok\").endObject();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTA5MjUzMA==", "bodyText": "I think this needs a super(in); too.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369092530", "createdAt": "2020-01-21T16:06:04Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ImportDanglingIndexResponse.java", "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionResponse;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.StatusToXContentObject;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.rest.RestStatus;\n+\n+import java.io.IOException;\n+\n+public class ImportDanglingIndexResponse extends ActionResponse implements StatusToXContentObject {\n+    public ImportDanglingIndexResponse() {\n+    }\n+\n+    public ImportDanglingIndexResponse(StreamInput in) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTA5NTUwNA==", "bodyText": "I think I'd prefer this to a null here:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    super((String[]) null);\n          \n          \n            \n                    super(Strings.EMPTY_ARRAY);\n          \n      \n    \n    \n  \n\nnulls get converted to empty arrays when serialised, and this is a little trappy.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369095504", "createdAt": "2020-01-21T16:10:57Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ListDanglingIndicesRequest.java", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionRequestValidationException;\n+import org.elasticsearch.action.support.nodes.BaseNodesRequest;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+\n+import java.io.IOException;\n+\n+public class ListDanglingIndicesRequest extends BaseNodesRequest<ListDanglingIndicesRequest> {\n+    public ListDanglingIndicesRequest(StreamInput in) throws IOException {\n+        super(in);\n+    }\n+\n+    public ListDanglingIndicesRequest() {\n+        super((String[]) null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTA5NjczOQ==", "bodyText": "I don't think we need to override this, the base class already has a no-op validate() method.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369096739", "createdAt": "2020-01-21T16:13:03Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ListDanglingIndicesRequest.java", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionRequestValidationException;\n+import org.elasticsearch.action.support.nodes.BaseNodesRequest;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+\n+import java.io.IOException;\n+\n+public class ListDanglingIndicesRequest extends BaseNodesRequest<ListDanglingIndicesRequest> {\n+    public ListDanglingIndicesRequest(StreamInput in) throws IOException {\n+        super(in);\n+    }\n+\n+    public ListDanglingIndicesRequest() {\n+        super((String[]) null);\n+    }\n+\n+    @Override\n+    public ActionRequestValidationException validate() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTA5ODMyMQ==", "bodyText": "Could we use the class name here? I think something that looks a bit too English might catch someone out if it ever appears in a log message or similar.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369098321", "createdAt": "2020-01-21T16:15:45Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ListDanglingIndicesRequest.java", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionRequestValidationException;\n+import org.elasticsearch.action.support.nodes.BaseNodesRequest;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+\n+import java.io.IOException;\n+\n+public class ListDanglingIndicesRequest extends BaseNodesRequest<ListDanglingIndicesRequest> {\n+    public ListDanglingIndicesRequest(StreamInput in) throws IOException {\n+        super(in);\n+    }\n+\n+    public ListDanglingIndicesRequest() {\n+        super((String[]) null);\n+    }\n+\n+    @Override\n+    public ActionRequestValidationException validate() {\n+        return null;\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return \"list dangling indices\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTEwMjE0Ng==", "bodyText": "Yes, I think that's overkill, or at least inconsistent with other APIs. HTTP doesn't have a good way to indicate with a status code that something only partly worked, so we prefer 200 OK to indicate that the coordinating node did its thing successfully, and if it encountered node-level failures then they are already included in the response by RestActions#buildNodesHeader.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369102146", "createdAt": "2020-01-21T16:21:54Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ListDanglingIndicesResponse.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.nodes.BaseNodesResponse;\n+import org.elasticsearch.cluster.ClusterName;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.StatusToXContentObject;\n+import org.elasticsearch.common.xcontent.XContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.rest.RestStatus;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * Models a response to a {@link ListDanglingIndicesRequest}. A list request queries every node in the\n+ * cluster and aggregates their responses. When the aggregated response is converted to {@link XContent},\n+ * information for each dangling index is presented under the \"dangling_indices\" key. If any nodes\n+ * in the cluster failed to answer, the details are presented under the \"failed_nodes\" key.\n+ */\n+public class ListDanglingIndicesResponse extends BaseNodesResponse<NodeDanglingIndicesResponse> implements StatusToXContentObject {\n+\n+    public ListDanglingIndicesResponse(StreamInput in) throws IOException {\n+        super(in);\n+    }\n+\n+    public ListDanglingIndicesResponse(\n+        ClusterName clusterName,\n+        List<NodeDanglingIndicesResponse> nodes,\n+        List<FailedNodeException> failures\n+    ) {\n+        super(clusterName, nodes, failures);\n+    }\n+\n+    @Override\n+    public RestStatus status() {\n+        return this.hasFailures() ? RestStatus.SERVICE_UNAVAILABLE : RestStatus.OK;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM2NDg0Nw=="}, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTEwMzMzOQ==", "bodyText": "++\nAs a user, I am likely to care more about the indices than the nodes on which they live, so I think it would make more sense to aggregate this by index UUID and list the nodes under each index. Note that the index name and creation date are immutable for a given index (UUID) so we only need include them once.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369103339", "createdAt": "2020-01-21T16:23:49Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ListDanglingIndicesResponse.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.nodes.BaseNodesResponse;\n+import org.elasticsearch.cluster.ClusterName;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.StatusToXContentObject;\n+import org.elasticsearch.common.xcontent.XContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.rest.RestStatus;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * Models a response to a {@link ListDanglingIndicesRequest}. A list request queries every node in the\n+ * cluster and aggregates their responses. When the aggregated response is converted to {@link XContent},\n+ * information for each dangling index is presented under the \"dangling_indices\" key. If any nodes\n+ * in the cluster failed to answer, the details are presented under the \"failed_nodes\" key.\n+ */\n+public class ListDanglingIndicesResponse extends BaseNodesResponse<NodeDanglingIndicesResponse> implements StatusToXContentObject {\n+\n+    public ListDanglingIndicesResponse(StreamInput in) throws IOException {\n+        super(in);\n+    }\n+\n+    public ListDanglingIndicesResponse(\n+        ClusterName clusterName,\n+        List<NodeDanglingIndicesResponse> nodes,\n+        List<FailedNodeException> failures\n+    ) {\n+        super(clusterName, nodes, failures);\n+    }\n+\n+    @Override\n+    public RestStatus status() {\n+        return this.hasFailures() ? RestStatus.SERVICE_UNAVAILABLE : RestStatus.OK;\n+    }\n+\n+    @Override\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        builder.startObject();\n+\n+        builder.startArray(\"dangling_indices\");\n+        for (NodeDanglingIndicesResponse nodeResponse : this.getNodes()) {\n+            for (IndexMetaData indexMetaData : nodeResponse.getDanglingIndices()) {\n+                DanglingIndexInfo danglingIndexInfo = new DanglingIndexInfo(\n+                    nodeResponse.getNode(),\n+                    indexMetaData.getIndex().getName(),\n+                    indexMetaData.getIndexUUID()\n+                );\n+                danglingIndexInfo.toXContent(builder, params);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM2ODgwOQ=="}, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTEwMzgwNw==", "bodyText": "I think we already include this automatically thanks to RestActions#buildNodesHeader", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369103807", "createdAt": "2020-01-21T16:24:33Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ListDanglingIndicesResponse.java", "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.nodes.BaseNodesResponse;\n+import org.elasticsearch.cluster.ClusterName;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.StatusToXContentObject;\n+import org.elasticsearch.common.xcontent.XContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.rest.RestStatus;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * Models a response to a {@link ListDanglingIndicesRequest}. A list request queries every node in the\n+ * cluster and aggregates their responses. When the aggregated response is converted to {@link XContent},\n+ * information for each dangling index is presented under the \"dangling_indices\" key. If any nodes\n+ * in the cluster failed to answer, the details are presented under the \"failed_nodes\" key.\n+ */\n+public class ListDanglingIndicesResponse extends BaseNodesResponse<NodeDanglingIndicesResponse> implements StatusToXContentObject {\n+\n+    public ListDanglingIndicesResponse(StreamInput in) throws IOException {\n+        super(in);\n+    }\n+\n+    public ListDanglingIndicesResponse(\n+        ClusterName clusterName,\n+        List<NodeDanglingIndicesResponse> nodes,\n+        List<FailedNodeException> failures\n+    ) {\n+        super(clusterName, nodes, failures);\n+    }\n+\n+    @Override\n+    public RestStatus status() {\n+        return this.hasFailures() ? RestStatus.SERVICE_UNAVAILABLE : RestStatus.OK;\n+    }\n+\n+    @Override\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        builder.startObject();\n+\n+        builder.startArray(\"dangling_indices\");\n+        for (NodeDanglingIndicesResponse nodeResponse : this.getNodes()) {\n+            for (IndexMetaData indexMetaData : nodeResponse.getDanglingIndices()) {\n+                DanglingIndexInfo danglingIndexInfo = new DanglingIndexInfo(\n+                    nodeResponse.getNode(),\n+                    indexMetaData.getIndex().getName(),\n+                    indexMetaData.getIndexUUID(),\n+                    indexMetaData.getCreationDate()\n+                );\n+                danglingIndexInfo.toXContent(builder, params);\n+            }\n+        }\n+        builder.endArray();\n+\n+        if (this.hasFailures()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTExMDQ3NA==", "bodyText": "Inventing a new DeleteIndexRequest() seems inappropriate here. I think we should make DeleteDanglingIndexRequest implement AckedRequest and use request instead.\nAlso you can drop the Priority argument, since NORMAL is the default.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369110474", "createdAt": "2020-01-21T16:35:19Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,221 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.admin.indices.delete.DeleteIndexRequest;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ack.ClusterStateUpdateResponse;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Priority;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, DeleteDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.GENERIC;\n+    }\n+\n+    @Override\n+    protected DeleteDanglingIndexResponse read(StreamInput in) throws IOException {\n+        return new DeleteDanglingIndexResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest request,\n+        ClusterState state,\n+        ActionListener<DeleteDanglingIndexResponse> listener\n+    ) throws Exception {\n+        IndexMetaData indexMetaDataToDelete;\n+        try {\n+            indexMetaDataToDelete = getIndexMetaDataToDelete(request);\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+            return;\n+        }\n+        final String indexName = indexMetaDataToDelete.getIndex().getName();\n+\n+        final ActionListener<ClusterStateUpdateResponse> actionListener = new ActionListener<>() {\n+            @Override\n+            public void onResponse(ClusterStateUpdateResponse clusterStateUpdateResponse) {\n+                listener.onResponse(new DeleteDanglingIndexResponse());\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.warn(\"Failed to delete dangling index [{}]\" + indexName, e);\n+                listener.onFailure(e);\n+            }\n+        };\n+\n+        // This flag is checked at this point so that we always check that the supplied index ID\n+        // does correspond to a dangling index.\n+        if (request.isAcceptDataLoss() == false) {\n+            throw new IllegalArgumentException(\"accept_data_loss must be set to true\");\n+        }\n+\n+        this.clusterService.submitStateUpdateTask(\n+            \"delete-dangling-index \" + indexName,\n+            new AckedClusterStateUpdateTask<>(Priority.NORMAL, new DeleteIndexRequest(), actionListener) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTExMTk1Nw==", "bodyText": "Since we're using an AckedClusterStateUpdateTask we should pass clusterStateUpdateResponse.isAcknowledged() back to the caller. Alternatively, if we don't care about whether all the nodes acked the request or not then we can just use a plain ClusterStateUpdateTask below.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369111957", "createdAt": "2020-01-21T16:37:45Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,221 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.admin.indices.delete.DeleteIndexRequest;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ack.ClusterStateUpdateResponse;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Priority;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, DeleteDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.GENERIC;\n+    }\n+\n+    @Override\n+    protected DeleteDanglingIndexResponse read(StreamInput in) throws IOException {\n+        return new DeleteDanglingIndexResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest request,\n+        ClusterState state,\n+        ActionListener<DeleteDanglingIndexResponse> listener\n+    ) throws Exception {\n+        IndexMetaData indexMetaDataToDelete;\n+        try {\n+            indexMetaDataToDelete = getIndexMetaDataToDelete(request);\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+            return;\n+        }\n+        final String indexName = indexMetaDataToDelete.getIndex().getName();\n+\n+        final ActionListener<ClusterStateUpdateResponse> actionListener = new ActionListener<>() {\n+            @Override\n+            public void onResponse(ClusterStateUpdateResponse clusterStateUpdateResponse) {\n+                listener.onResponse(new DeleteDanglingIndexResponse());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTExMjk2NQ==", "bodyText": "This looks to be parametric in the response type. Can we use DeleteDanglingIndexResponse instead of ClusterStateUpdateResponse?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369112965", "createdAt": "2020-01-21T16:39:26Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,221 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.admin.indices.delete.DeleteIndexRequest;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ack.ClusterStateUpdateResponse;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Priority;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, DeleteDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.GENERIC;\n+    }\n+\n+    @Override\n+    protected DeleteDanglingIndexResponse read(StreamInput in) throws IOException {\n+        return new DeleteDanglingIndexResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest request,\n+        ClusterState state,\n+        ActionListener<DeleteDanglingIndexResponse> listener\n+    ) throws Exception {\n+        IndexMetaData indexMetaDataToDelete;\n+        try {\n+            indexMetaDataToDelete = getIndexMetaDataToDelete(request);\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+            return;\n+        }\n+        final String indexName = indexMetaDataToDelete.getIndex().getName();\n+\n+        final ActionListener<ClusterStateUpdateResponse> actionListener = new ActionListener<>() {\n+            @Override\n+            public void onResponse(ClusterStateUpdateResponse clusterStateUpdateResponse) {\n+                listener.onResponse(new DeleteDanglingIndexResponse());\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.warn(\"Failed to delete dangling index [{}]\" + indexName, e);\n+                listener.onFailure(e);\n+            }\n+        };\n+\n+        // This flag is checked at this point so that we always check that the supplied index ID\n+        // does correspond to a dangling index.\n+        if (request.isAcceptDataLoss() == false) {\n+            throw new IllegalArgumentException(\"accept_data_loss must be set to true\");\n+        }\n+\n+        this.clusterService.submitStateUpdateTask(\n+            \"delete-dangling-index \" + indexName,\n+            new AckedClusterStateUpdateTask<>(Priority.NORMAL, new DeleteIndexRequest(), actionListener) {\n+\n+                @Override\n+                protected ClusterStateUpdateResponse newResponse(boolean acknowledged) {\n+                    return new ClusterStateUpdateResponse(acknowledged);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 135}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTExMzU3OQ==", "bodyText": "I think we need only return the Index here, not the full IndexMetadata.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369113579", "createdAt": "2020-01-21T16:40:35Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,221 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.admin.indices.delete.DeleteIndexRequest;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ack.ClusterStateUpdateResponse;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Priority;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, DeleteDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.GENERIC;\n+    }\n+\n+    @Override\n+    protected DeleteDanglingIndexResponse read(StreamInput in) throws IOException {\n+        return new DeleteDanglingIndexResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest request,\n+        ClusterState state,\n+        ActionListener<DeleteDanglingIndexResponse> listener\n+    ) throws Exception {\n+        IndexMetaData indexMetaDataToDelete;\n+        try {\n+            indexMetaDataToDelete = getIndexMetaDataToDelete(request);\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+            return;\n+        }\n+        final String indexName = indexMetaDataToDelete.getIndex().getName();\n+\n+        final ActionListener<ClusterStateUpdateResponse> actionListener = new ActionListener<>() {\n+            @Override\n+            public void onResponse(ClusterStateUpdateResponse clusterStateUpdateResponse) {\n+                listener.onResponse(new DeleteDanglingIndexResponse());\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.warn(\"Failed to delete dangling index [{}]\" + indexName, e);\n+                listener.onFailure(e);\n+            }\n+        };\n+\n+        // This flag is checked at this point so that we always check that the supplied index ID\n+        // does correspond to a dangling index.\n+        if (request.isAcceptDataLoss() == false) {\n+            throw new IllegalArgumentException(\"accept_data_loss must be set to true\");\n+        }\n+\n+        this.clusterService.submitStateUpdateTask(\n+            \"delete-dangling-index \" + indexName,\n+            new AckedClusterStateUpdateTask<>(Priority.NORMAL, new DeleteIndexRequest(), actionListener) {\n+\n+                @Override\n+                protected ClusterStateUpdateResponse newResponse(boolean acknowledged) {\n+                    return new ClusterStateUpdateResponse(acknowledged);\n+                }\n+\n+                @Override\n+                public ClusterState execute(final ClusterState currentState) {\n+                    return deleteDanglingIndex(currentState, indexMetaDataToDelete);\n+                }\n+            }\n+        );\n+    }\n+\n+    private ClusterState deleteDanglingIndex(ClusterState currentState, IndexMetaData indexMetaDataToDelete) {\n+        final MetaData meta = currentState.metaData();\n+\n+        MetaData.Builder metaDataBuilder = MetaData.builder(meta);\n+\n+        final IndexGraveyard.Builder graveyardBuilder = IndexGraveyard.builder(metaDataBuilder.indexGraveyard());\n+\n+        final IndexGraveyard newGraveyard = graveyardBuilder.addTombstone(indexMetaDataToDelete.getIndex()).build(settings);\n+        metaDataBuilder.indexGraveyard(newGraveyard);\n+\n+        return ClusterState.builder(currentState).metaData(metaDataBuilder.build()).build();\n+    }\n+\n+    @Override\n+    protected ClusterBlockException checkBlock(DeleteDanglingIndexRequest request, ClusterState state) {\n+        return null;\n+    }\n+\n+    private IndexMetaData getIndexMetaDataToDelete(DeleteDanglingIndexRequest request) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 164}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTExNTU3OQ==", "bodyText": "Rather than returning an ActionFuture here, pass in an ActionListener<...> and avoid blocking a thread on the response.\nMoreover I think it would be better to use a different action that focusses only on the index UUID we're interested in here, or else adapt the listing action to allow it to focus on a single UUID. Listing everything just to delete the one thing seems like it might cause trouble.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369115579", "createdAt": "2020-01-21T16:43:54Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,221 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.admin.indices.delete.DeleteIndexRequest;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ack.ClusterStateUpdateResponse;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Priority;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, DeleteDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.GENERIC;\n+    }\n+\n+    @Override\n+    protected DeleteDanglingIndexResponse read(StreamInput in) throws IOException {\n+        return new DeleteDanglingIndexResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest request,\n+        ClusterState state,\n+        ActionListener<DeleteDanglingIndexResponse> listener\n+    ) throws Exception {\n+        IndexMetaData indexMetaDataToDelete;\n+        try {\n+            indexMetaDataToDelete = getIndexMetaDataToDelete(request);\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+            return;\n+        }\n+        final String indexName = indexMetaDataToDelete.getIndex().getName();\n+\n+        final ActionListener<ClusterStateUpdateResponse> actionListener = new ActionListener<>() {\n+            @Override\n+            public void onResponse(ClusterStateUpdateResponse clusterStateUpdateResponse) {\n+                listener.onResponse(new DeleteDanglingIndexResponse());\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.warn(\"Failed to delete dangling index [{}]\" + indexName, e);\n+                listener.onFailure(e);\n+            }\n+        };\n+\n+        // This flag is checked at this point so that we always check that the supplied index ID\n+        // does correspond to a dangling index.\n+        if (request.isAcceptDataLoss() == false) {\n+            throw new IllegalArgumentException(\"accept_data_loss must be set to true\");\n+        }\n+\n+        this.clusterService.submitStateUpdateTask(\n+            \"delete-dangling-index \" + indexName,\n+            new AckedClusterStateUpdateTask<>(Priority.NORMAL, new DeleteIndexRequest(), actionListener) {\n+\n+                @Override\n+                protected ClusterStateUpdateResponse newResponse(boolean acknowledged) {\n+                    return new ClusterStateUpdateResponse(acknowledged);\n+                }\n+\n+                @Override\n+                public ClusterState execute(final ClusterState currentState) {\n+                    return deleteDanglingIndex(currentState, indexMetaDataToDelete);\n+                }\n+            }\n+        );\n+    }\n+\n+    private ClusterState deleteDanglingIndex(ClusterState currentState, IndexMetaData indexMetaDataToDelete) {\n+        final MetaData meta = currentState.metaData();\n+\n+        MetaData.Builder metaDataBuilder = MetaData.builder(meta);\n+\n+        final IndexGraveyard.Builder graveyardBuilder = IndexGraveyard.builder(metaDataBuilder.indexGraveyard());\n+\n+        final IndexGraveyard newGraveyard = graveyardBuilder.addTombstone(indexMetaDataToDelete.getIndex()).build(settings);\n+        metaDataBuilder.indexGraveyard(newGraveyard);\n+\n+        return ClusterState.builder(currentState).metaData(metaDataBuilder.build()).build();\n+    }\n+\n+    @Override\n+    protected ClusterBlockException checkBlock(DeleteDanglingIndexRequest request, ClusterState state) {\n+        return null;\n+    }\n+\n+    private IndexMetaData getIndexMetaDataToDelete(DeleteDanglingIndexRequest request) {\n+        String indexUUID = request.getIndexUUID();\n+\n+        List<IndexMetaData> matchingMetaData = new ArrayList<>();\n+\n+        final List<NodeDanglingIndicesResponse> nodes = fetchDanglingIndices().actionGet().getNodes();\n+\n+        for (NodeDanglingIndicesResponse response : nodes) {\n+            for (IndexMetaData danglingIndex : response.getDanglingIndices()) {\n+                if (danglingIndex.getIndexUUID().equals(indexUUID)) {\n+                    matchingMetaData.add(danglingIndex);\n+                }\n+            }\n+        }\n+\n+        if (matchingMetaData.isEmpty()) {\n+            throw new IllegalArgumentException(\"No dangling index found for UUID [\" + indexUUID + \"]\");\n+        }\n+\n+        // Although we could find metadata for the same index on multiple nodes, we return the first\n+        // metadata here because only the index part goes into the graveyard, which is basically the\n+        // name and UUID.\n+        return matchingMetaData.get(0);\n+    }\n+\n+    private ActionFuture<ListDanglingIndicesResponse> fetchDanglingIndices() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 189}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTExNjIxOA==", "bodyText": "We pass the exception back to the caller, so I think we need log this at no higher than DEBUG.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369116218", "createdAt": "2020-01-21T16:44:58Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,221 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.admin.indices.delete.DeleteIndexRequest;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ack.ClusterStateUpdateResponse;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Priority;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, DeleteDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.GENERIC;\n+    }\n+\n+    @Override\n+    protected DeleteDanglingIndexResponse read(StreamInput in) throws IOException {\n+        return new DeleteDanglingIndexResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest request,\n+        ClusterState state,\n+        ActionListener<DeleteDanglingIndexResponse> listener\n+    ) throws Exception {\n+        IndexMetaData indexMetaDataToDelete;\n+        try {\n+            indexMetaDataToDelete = getIndexMetaDataToDelete(request);\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+            return;\n+        }\n+        final String indexName = indexMetaDataToDelete.getIndex().getName();\n+\n+        final ActionListener<ClusterStateUpdateResponse> actionListener = new ActionListener<>() {\n+            @Override\n+            public void onResponse(ClusterStateUpdateResponse clusterStateUpdateResponse) {\n+                listener.onResponse(new DeleteDanglingIndexResponse());\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.warn(\"Failed to delete dangling index [{}]\" + indexName, e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 118}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTExODg5Mw==", "bodyText": "I think we might as well let exceptions be thrown, since this method is called from within ActionRunnable.wrap which routes exceptions to the listener in any case.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369118893", "createdAt": "2020-01-21T16:49:24Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,221 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.admin.indices.delete.DeleteIndexRequest;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ack.ClusterStateUpdateResponse;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Priority;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, DeleteDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.GENERIC;\n+    }\n+\n+    @Override\n+    protected DeleteDanglingIndexResponse read(StreamInput in) throws IOException {\n+        return new DeleteDanglingIndexResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest request,\n+        ClusterState state,\n+        ActionListener<DeleteDanglingIndexResponse> listener\n+    ) throws Exception {\n+        IndexMetaData indexMetaDataToDelete;\n+        try {\n+            indexMetaDataToDelete = getIndexMetaDataToDelete(request);\n+        } catch (Exception e) {\n+            listener.onFailure(e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTEyMjY0MA==", "bodyText": "Similarly here, we should be doing this in an async fashion with an ActionListener rather than blocking, and should only focus on the index in question.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369122640", "createdAt": "2020-01-21T16:55:44Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportImportDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.gateway.LocalAllocateDangledIndices;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Implements the import of a dangling index. When handling a {@link ImportDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then calls {@link LocalAllocateDangledIndices}\n+ * to perform the actual allocation.\n+ */\n+public class TransportImportDanglingIndexAction extends HandledTransportAction<ImportDanglingIndexRequest, ImportDanglingIndexResponse> {\n+\n+    private final TransportService transportService;\n+    private final LocalAllocateDangledIndices danglingIndexAllocator;\n+\n+    @Inject\n+    public TransportImportDanglingIndexAction(\n+        ActionFilters actionFilters,\n+        TransportService transportService,\n+        LocalAllocateDangledIndices danglingIndexAllocator\n+    ) {\n+        super(ImportDanglingIndexAction.NAME, transportService, actionFilters, ImportDanglingIndexRequest::new);\n+        this.transportService = transportService;\n+        this.danglingIndexAllocator = danglingIndexAllocator;\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, ImportDanglingIndexRequest request, ActionListener<ImportDanglingIndexResponse> listener) {\n+        IndexMetaData indexMetaDataToImport;\n+        try {\n+            indexMetaDataToImport = getIndexMetaDataToImport(request);\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+            return;\n+        }\n+\n+        // This flag is checked at this point so that we always check that the supplied index UUID\n+        // does correspond to a dangling index.\n+        if (request.isAcceptDataLoss() == false) {\n+            throw new IllegalArgumentException(\"accept_data_loss must be set to true\");\n+        }\n+\n+        this.danglingIndexAllocator.allocateDangled(List.of(indexMetaDataToImport), new ActionListener<>() {\n+            @Override\n+            public void onResponse(LocalAllocateDangledIndices.AllocateDangledResponse allocateDangledResponse) {\n+                listener.onResponse(new ImportDanglingIndexResponse());\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                listener.onFailure(e);\n+            }\n+        });\n+    }\n+\n+    private IndexMetaData getIndexMetaDataToImport(ImportDanglingIndexRequest request) {\n+        String indexUUID = request.getIndexUUID();\n+        String nodeId = request.getNodeId();\n+\n+        List<IndexMetaData> matchingMetaData = new ArrayList<>();\n+\n+        for (NodeDanglingIndicesResponse response : fetchDanglingIndices().actionGet().getNodes()) {\n+            for (IndexMetaData danglingIndex : response.getDanglingIndices()) {\n+                if (danglingIndex.getIndexUUID().equals(indexUUID) && (nodeId == null || response.getNode().getId().equals(nodeId))) {\n+                    matchingMetaData.add(danglingIndex);\n+                }\n+            }\n+        }\n+\n+        if (matchingMetaData.isEmpty()) {\n+            throw new IllegalArgumentException(\"No dangling index found for UUID [\" + indexUUID + \"]\");\n+        }\n+\n+        if (matchingMetaData.size() > 1) {\n+            throw new IllegalArgumentException(\n+                \"Multiple nodes contain dangling index [\" + indexUUID + \"]. \" + \"Specify a node ID to import a specific dangling index.\"\n+            );\n+        }\n+\n+        return matchingMetaData.get(0);\n+    }\n+\n+    private ActionFuture<ListDanglingIndicesResponse> fetchDanglingIndices() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 118}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTEyNDc2MQ==", "bodyText": "I think it will prove helpful to list the nodes (at least their names and IDs) in this message.\nAlso I think it would be better to automatically select the node holding the index metadata with the highest version rather than asking the user to specify a node ID.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369124761", "createdAt": "2020-01-21T16:59:08Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportImportDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.gateway.LocalAllocateDangledIndices;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Implements the import of a dangling index. When handling a {@link ImportDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then calls {@link LocalAllocateDangledIndices}\n+ * to perform the actual allocation.\n+ */\n+public class TransportImportDanglingIndexAction extends HandledTransportAction<ImportDanglingIndexRequest, ImportDanglingIndexResponse> {\n+\n+    private final TransportService transportService;\n+    private final LocalAllocateDangledIndices danglingIndexAllocator;\n+\n+    @Inject\n+    public TransportImportDanglingIndexAction(\n+        ActionFilters actionFilters,\n+        TransportService transportService,\n+        LocalAllocateDangledIndices danglingIndexAllocator\n+    ) {\n+        super(ImportDanglingIndexAction.NAME, transportService, actionFilters, ImportDanglingIndexRequest::new);\n+        this.transportService = transportService;\n+        this.danglingIndexAllocator = danglingIndexAllocator;\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, ImportDanglingIndexRequest request, ActionListener<ImportDanglingIndexResponse> listener) {\n+        IndexMetaData indexMetaDataToImport;\n+        try {\n+            indexMetaDataToImport = getIndexMetaDataToImport(request);\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+            return;\n+        }\n+\n+        // This flag is checked at this point so that we always check that the supplied index UUID\n+        // does correspond to a dangling index.\n+        if (request.isAcceptDataLoss() == false) {\n+            throw new IllegalArgumentException(\"accept_data_loss must be set to true\");\n+        }\n+\n+        this.danglingIndexAllocator.allocateDangled(List.of(indexMetaDataToImport), new ActionListener<>() {\n+            @Override\n+            public void onResponse(LocalAllocateDangledIndices.AllocateDangledResponse allocateDangledResponse) {\n+                listener.onResponse(new ImportDanglingIndexResponse());\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                listener.onFailure(e);\n+            }\n+        });\n+    }\n+\n+    private IndexMetaData getIndexMetaDataToImport(ImportDanglingIndexRequest request) {\n+        String indexUUID = request.getIndexUUID();\n+        String nodeId = request.getNodeId();\n+\n+        List<IndexMetaData> matchingMetaData = new ArrayList<>();\n+\n+        for (NodeDanglingIndicesResponse response : fetchDanglingIndices().actionGet().getNodes()) {\n+            for (IndexMetaData danglingIndex : response.getDanglingIndices()) {\n+                if (danglingIndex.getIndexUUID().equals(indexUUID) && (nodeId == null || response.getNode().getId().equals(nodeId))) {\n+                    matchingMetaData.add(danglingIndex);\n+                }\n+            }\n+        }\n+\n+        if (matchingMetaData.isEmpty()) {\n+            throw new IllegalArgumentException(\"No dangling index found for UUID [\" + indexUUID + \"]\");\n+        }\n+\n+        if (matchingMetaData.size() > 1) {\n+            throw new IllegalArgumentException(\n+                \"Multiple nodes contain dangling index [\" + indexUUID + \"]. \" + \"Specify a node ID to import a specific dangling index.\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTEyNzMzNA==", "bodyText": "\ud83d\udcaf for package-level javadocs \ud83d\ude00", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369127334", "createdAt": "2020-01-21T17:03:32Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/package-info.java", "diffHunk": "@@ -0,0 +1,37 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+/**\n+ * Dangling indices are indices that exist on disk on one or more nodes but\n+ * which do not currently exist in the cluster state. They arise in a\n+ * number of situations, such as:\n+ *\n+ * <ul>\n+ * <li>A user overflows the index graveyard by deleting more than 500 indices while a node is offline and then the node rejoins the\n+ * cluster</li>\n+ * <li>A node (unsafely) moves from one cluster to another, perhaps because the original cluster lost all its master nodes</li>\n+ * <li>A user (unsafely) meddles with the contents of the data path, maybe restoring an old index folder from a backup</li>\n+ * <li>A disk partially fails and the user has no replicas and no snapshots and wants to (unsafely) recover whatever they can</li>\n+ * <li>A cluster loses all master nodes and those are (unsafely) restored from backup, but the backup does not contain the index</li>\n+ * </ul>\n+ *\n+ * <p>The classes in this package form an API for managing dangling\n+ * indices, allowing them to be listed, imported or deleted.\n+ */\n+package org.elasticsearch.action.admin.indices.dangling;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTEzMDU2Nw==", "bodyText": "Still final?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369130567", "createdAt": "2020-01-21T17:09:50Z", "author": {"login": "DaveCTurner"}, "path": "server/src/test/java/org/elasticsearch/gateway/DanglingIndicesStateTests.java", "diffHunk": "@@ -191,27 +192,49 @@ public void testDanglingIndicesStripAliases() throws Exception {\n         }\n     }\n \n-    public void testDanglingIndicesAreNotAllocatedWhenDisabled() throws Exception {\n+    /**\n+     * Check that when auto-imports are disabled, then dangling indices are still detected.\n+     */\n+    public void testDanglingIndicesAreDetectedButNotAllocatedWhenDisabled() throws Exception {\n         try (NodeEnvironment env = newNodeEnvironment()) {\n             MetaStateService metaStateService = new MetaStateService(env, xContentRegistry());\n             LocalAllocateDangledIndices localAllocateDangledIndices = mock(LocalAllocateDangledIndices.class);\n-\n             final Settings allocateSettings = Settings.builder().put(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey(), false).build();\n \n             final ClusterService clusterServiceMock = mock(ClusterService.class);\n             when(clusterServiceMock.getSettings()).thenReturn(allocateSettings);\n \n-            final DanglingIndicesState danglingIndicesState = new DanglingIndicesState(\n+            DanglingIndicesState danglingIndicesState = new DanglingIndicesState(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTEzMDcyNw==", "bodyText": "nit: unnecessary whitespace change", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369130727", "createdAt": "2020-01-21T17:10:08Z", "author": {"login": "DaveCTurner"}, "path": "server/src/test/java/org/elasticsearch/gateway/DanglingIndicesStateTests.java", "diffHunk": "@@ -191,27 +192,49 @@ public void testDanglingIndicesStripAliases() throws Exception {\n         }\n     }\n \n-    public void testDanglingIndicesAreNotAllocatedWhenDisabled() throws Exception {\n+    /**\n+     * Check that when auto-imports are disabled, then dangling indices are still detected.\n+     */\n+    public void testDanglingIndicesAreDetectedButNotAllocatedWhenDisabled() throws Exception {\n         try (NodeEnvironment env = newNodeEnvironment()) {\n             MetaStateService metaStateService = new MetaStateService(env, xContentRegistry());\n             LocalAllocateDangledIndices localAllocateDangledIndices = mock(LocalAllocateDangledIndices.class);\n-\n             final Settings allocateSettings = Settings.builder().put(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey(), false).build();\n \n             final ClusterService clusterServiceMock = mock(ClusterService.class);\n             when(clusterServiceMock.getSettings()).thenReturn(allocateSettings);\n \n-            final DanglingIndicesState danglingIndicesState = new DanglingIndicesState(\n+            DanglingIndicesState danglingIndicesState = new DanglingIndicesState(\n                 env,\n                 metaStateService,\n-                localAllocateDangledIndices,\n-                clusterServiceMock\n+                localAllocateDangledIndices, clusterServiceMock", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 32}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "553b8388f18ed48c05239f644bb410ca2f4aa4b0", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/553b8388f18ed48c05239f644bb410ca2f4aa4b0", "committedDate": "2020-01-22T12:10:13Z", "message": "Mark APIs as stable"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "96389abbb551386f7ced9ac5f708ccb31efff6c0", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/96389abbb551386f7ced9ac5f708ccb31efff6c0", "committedDate": "2020-01-22T12:17:38Z", "message": "Make DeleteDanglingIndexRequest immutable"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3268744e4a7009f555a2c980f8c46792f22447a6", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/3268744e4a7009f555a2c980f8c46792f22447a6", "committedDate": "2020-01-22T12:25:22Z", "message": "Make ImportDanglingIndexRequest immutable"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "72165897748f14a9422e1d84c35858a1559084e1", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/72165897748f14a9422e1d84c35858a1559084e1", "committedDate": "2020-01-22T12:26:07Z", "message": "Remove redundant validation"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fab56093eeda3294c002b4c5e3a951bc3cc80b2c", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/fab56093eeda3294c002b4c5e3a951bc3cc80b2c", "committedDate": "2020-01-22T12:27:08Z", "message": "Replace hand-rolled toString() with generated"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "df3ad067cd798951e2fbc7a50d748a32c078f8e5", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/df3ad067cd798951e2fbc7a50d748a32c078f8e5", "committedDate": "2020-01-22T15:11:57Z", "message": "Review feedback"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1f623520817a3730e6385f9c86627394a01dcd61", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/1f623520817a3730e6385f9c86627394a01dcd61", "committedDate": "2020-01-23T13:55:59Z", "message": "Aggregate list dangling indices rest response by ID"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c55227fc8799493c0ebe3ba84c09584138b75464", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/c55227fc8799493c0ebe3ba84c09584138b75464", "committedDate": "2020-01-23T14:58:10Z", "message": "Remove blocking call from delete danling index"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "21645765defcec85f90a982982ccc5eb908bfac7", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/21645765defcec85f90a982982ccc5eb908bfac7", "committedDate": "2020-01-23T15:56:31Z", "message": "Remove blocking call from import danlging index"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1e74fc3cc71712ea3273e62e365dc87041c6f80c", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/1e74fc3cc71712ea3273e62e365dc87041c6f80c", "committedDate": "2020-01-24T11:14:51Z", "message": "Pick the latest metadata when importing a dangling index"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "de586e2ae977129a5cd91113c520fbf1ea6c4e97", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/de586e2ae977129a5cd91113c520fbf1ea6c4e97", "committedDate": "2020-01-24T11:50:48Z", "message": "Tweaks"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fedd909042c31312b0b3b9dfb52c5403363f7a50", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/fedd909042c31312b0b3b9dfb52c5403363f7a50", "committedDate": "2020-01-27T15:46:18Z", "message": "Refactor to use async requests\n\nRefactor inter-node requests to be async, and introduce a find action so\nthat the import and delete dangling index commands don't have to pull\ntoo much data over the network."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "743de045223b0a2151cad37bb702240a4f35d942", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/743de045223b0a2151cad37bb702240a4f35d942", "committedDate": "2020-01-28T11:26:04Z", "message": "Merge remote-tracking branch 'upstream/master' into 48366-dangling-indices-api"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9c00ab3e33de2f194a5b415ce00c31a13c807419", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/9c00ab3e33de2f194a5b415ce00c31a13c807419", "committedDate": "2020-01-28T14:22:03Z", "message": "Fix unstable test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8ad19775ebdf426016b85c421445d8225ffe05fa", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/8ad19775ebdf426016b85c421445d8225ffe05fa", "committedDate": "2020-01-28T14:29:24Z", "message": "Nest restartRandomDataNode calls and delete restartRandomDataNodes(int,RestartCallback)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8069ba97c74aba97a6868944b9f26c823814f726", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/8069ba97c74aba97a6868944b9f26c823814f726", "committedDate": "2020-01-28T14:46:15Z", "message": "Polishing"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c73f20ff6d1fc399a4710d19e2324d855e388725", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/c73f20ff6d1fc399a4710d19e2324d855e388725", "committedDate": "2020-01-30T16:07:53Z", "message": "Log level tweak"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e5216679f8922af054704b8a03715b1f31812a86", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/e5216679f8922af054704b8a03715b1f31812a86", "committedDate": "2020-01-31T15:04:03Z", "message": "Fix the very broken dangling indices REST API"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9aabc86311e0eed64a636d7166b8f295e3b44b9f", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/9aabc86311e0eed64a636d7166b8f295e3b44b9f", "committedDate": "2020-02-03T17:37:24Z", "message": "Write REST tests for dangling indices"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5fb15d62939a7747deb6b8e40629b8194ed2a2e1", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/5fb15d62939a7747deb6b8e40629b8194ed2a2e1", "committedDate": "2020-02-04T10:29:58Z", "message": "JavaDoc"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "81d40b36a8f13c023dd448136aa2996cafedf028", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/81d40b36a8f13c023dd448136aa2996cafedf028", "committedDate": "2020-02-13T14:40:55Z", "message": "Merge remote-tracking branch 'upstream/master' into 48366-dangling-indices-api"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b5390661871278df5d0209a6f89ae4c1166f32f3", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/b5390661871278df5d0209a6f89ae4c1166f32f3", "committedDate": "2020-02-13T16:05:08Z", "message": "Updates after merge, move REST IT\n\nChanges in master required updates to the new REST controllers. Also\nmove `DanglingIndicesRestIT` to the `qa:smoke-test-http` project,\nbecause it's a slightly better home than the `modules:transport-netty4`\nproject."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a9487f2192fc5cce911a08d8b17a5d66c669883d", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/a9487f2192fc5cce911a08d8b17a5d66c669883d", "committedDate": "2020-02-13T17:20:35Z", "message": "Checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f78f09db16ca863a318dce1ed40e4094f5e808c8", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/f78f09db16ca863a318dce1ed40e4094f5e808c8", "committedDate": "2020-02-13T17:45:03Z", "message": "Satify :server:loggerUsageCheck"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c3569e45a7ab6590eb19133edf7adc1c2c3b5fe8", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/c3569e45a7ab6590eb19133edf7adc1c2c3b5fe8", "committedDate": "2020-02-13T20:46:14Z", "message": "Test fixes"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY1NTc3MDE0", "url": "https://github.com/elastic/elasticsearch/pull/50920#pullrequestreview-365577014", "createdAt": "2020-02-27T10:40:28Z", "commit": {"oid": "c3569e45a7ab6590eb19133edf7adc1c2c3b5fe8"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxMDo0MDo0NFrOFvNTSA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxMDo1Mzo0MFrOFvNvNA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTA0NTMyMA==", "bodyText": "Maybe NodeListDanglingIndicesResponse?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385045320", "createdAt": "2020-02-27T10:40:44Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/NodeDanglingIndicesResponse.java", "diffHunk": "@@ -0,0 +1,55 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.support.nodes.BaseNodeResponse;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * Used when querying every node in the cluster for dangling indices, in response to a list request.\n+ */\n+public class NodeDanglingIndicesResponse extends BaseNodeResponse {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3569e45a7ab6590eb19133edf7adc1c2c3b5fe8"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTA0NjA1Nw==", "bodyText": "Oh, fun, we can have more than one version of the index metadata if the user has done something really weird involving multiple data paths. I think that's worth a comment.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385046057", "createdAt": "2020-02-27T10:42:02Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/NodeFindDanglingIndexResponse.java", "diffHunk": "@@ -0,0 +1,56 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.support.nodes.BaseNodeResponse;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * Used when querying every node in the cluster for a specific dangling index.\n+ */\n+public class NodeFindDanglingIndexResponse extends BaseNodeResponse {\n+    private final List<IndexMetaData> danglingIndexMetaData;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3569e45a7ab6590eb19133edf7adc1c2c3b5fe8"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTA0NjQ0NQ==", "bodyText": "Maybe NodeListDanglingIndicesRequest?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385046445", "createdAt": "2020-02-27T10:42:46Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/NodeDanglingIndicesRequest.java", "diffHunk": "@@ -0,0 +1,38 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.transport.TransportRequest;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Used when querying every node in the cluster for dangling indices, in response to a list request.\n+ */\n+public class NodeDanglingIndicesRequest extends TransportRequest {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3569e45a7ab6590eb19133edf7adc1c2c3b5fe8"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTA0Nzk1MA==", "bodyText": "Should we execute this action with the NodeClient rather than going via the transport service?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385047950", "createdAt": "2020-02-27T10:45:29Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.Index;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+import static org.elasticsearch.common.util.CollectionUtils.map;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, DeleteDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.GENERIC;\n+    }\n+\n+    @Override\n+    protected DeleteDanglingIndexResponse read(StreamInput in) throws IOException {\n+        return new DeleteDanglingIndexResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest deleteRequest,\n+        ClusterState state,\n+        ActionListener<DeleteDanglingIndexResponse> deleteListener\n+    ) throws Exception {\n+        findDanglingIndex(deleteRequest.getIndexUUID(), new ActionListener<>() {\n+\n+            @Override\n+            public void onResponse(Index indexToDelete) {\n+                // This flag is checked at this point so that we always check that the supplied index ID\n+                // does correspond to a dangling index.\n+                if (deleteRequest.isAcceptDataLoss() == false) {\n+                    throw new IllegalArgumentException(\"accept_data_loss must be set to true\");\n+                }\n+\n+                String indexName = indexToDelete.getName();\n+\n+                final ActionListener<DeleteDanglingIndexResponse> clusterStateUpdatedListener = new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(DeleteDanglingIndexResponse response) {\n+                        deleteListener.onResponse(response);\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        logger.debug(\"Failed to delete dangling index [{}]\" + indexName, e);\n+                        deleteListener.onFailure(e);\n+                    }\n+                };\n+\n+                clusterService.submitStateUpdateTask(\n+                    \"delete-dangling-index \" + indexName,\n+                    new AckedClusterStateUpdateTask<>(deleteRequest, clusterStateUpdatedListener) {\n+\n+                        @Override\n+                        protected DeleteDanglingIndexResponse newResponse(boolean acknowledged) {\n+                            return new DeleteDanglingIndexResponse(acknowledged);\n+                        }\n+\n+                        @Override\n+                        public ClusterState execute(final ClusterState currentState) {\n+                            return deleteDanglingIndex(currentState, indexToDelete);\n+                        }\n+                    }\n+                );\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.debug(\"Failed to list dangling indices\", e);\n+                deleteListener.onFailure(e);\n+            }\n+        });\n+    }\n+\n+    private ClusterState deleteDanglingIndex(ClusterState currentState, Index indexToDelete) {\n+        final MetaData meta = currentState.metaData();\n+\n+        MetaData.Builder metaDataBuilder = MetaData.builder(meta);\n+\n+        final IndexGraveyard.Builder graveyardBuilder = IndexGraveyard.builder(metaDataBuilder.indexGraveyard());\n+\n+        final IndexGraveyard newGraveyard = graveyardBuilder.addTombstone(indexToDelete).build(settings);\n+        metaDataBuilder.indexGraveyard(newGraveyard);\n+\n+        return ClusterState.builder(currentState).metaData(metaDataBuilder.build()).build();\n+    }\n+\n+    @Override\n+    protected ClusterBlockException checkBlock(DeleteDanglingIndexRequest request, ClusterState state) {\n+        return null;\n+    }\n+\n+    private void findDanglingIndex(String indexUUID, ActionListener<Index> listener) {\n+        this.transportService.sendRequest(\n+            this.transportService.getLocalNode(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3569e45a7ab6590eb19133edf7adc1c2c3b5fe8"}, "originalPosition": 170}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTA0OTI3OQ==", "bodyText": "I think this should use a format string: \"Metadata versions {} found for UUID [{}], selecting the highest\"\nAlso since it's only a DEBUG message, let's always log it.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385049279", "createdAt": "2020-02-27T10:47:47Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportImportDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.gateway.LocalAllocateDangledIndices;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.List;\n+\n+import static org.elasticsearch.common.util.CollectionUtils.map;\n+\n+/**\n+ * Implements the import of a dangling index. When handling a {@link ImportDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then calls {@link LocalAllocateDangledIndices}\n+ * to perform the actual allocation.\n+ */\n+public class TransportImportDanglingIndexAction extends HandledTransportAction<ImportDanglingIndexRequest, ImportDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportImportDanglingIndexAction.class);\n+\n+    private final TransportService transportService;\n+    private final LocalAllocateDangledIndices danglingIndexAllocator;\n+\n+    @Inject\n+    public TransportImportDanglingIndexAction(\n+        ActionFilters actionFilters,\n+        TransportService transportService,\n+        LocalAllocateDangledIndices danglingIndexAllocator\n+    ) {\n+        super(ImportDanglingIndexAction.NAME, transportService, actionFilters, ImportDanglingIndexRequest::new);\n+        this.transportService = transportService;\n+        this.danglingIndexAllocator = danglingIndexAllocator;\n+    }\n+\n+    @Override\n+    protected void doExecute(\n+        Task task,\n+        ImportDanglingIndexRequest importRequest,\n+        ActionListener<ImportDanglingIndexResponse> importListener\n+    ) {\n+        findDanglingIndex(importRequest, new ActionListener<>() {\n+            @Override\n+            public void onResponse(IndexMetaData indexMetaDataToImport) {\n+                // This flag is checked at this point so that we always check that the supplied index UUID\n+                // does correspond to a dangling index.\n+                if (importRequest.isAcceptDataLoss() == false) {\n+                    throw new IllegalArgumentException(\"accept_data_loss must be set to true\");\n+                }\n+\n+                danglingIndexAllocator.allocateDangled(List.of(indexMetaDataToImport), new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(LocalAllocateDangledIndices.AllocateDangledResponse allocateDangledResponse) {\n+                        importListener.onResponse(new ImportDanglingIndexResponse());\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        logger.debug(\"Failed to import dangling index [\" + indexMetaDataToImport.getIndexUUID() + \"]\", e);\n+                        importListener.onFailure(e);\n+                    }\n+                });\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.debug(\"Failed to find dangling index [\" + importRequest.getIndexUUID() + \"]\", e);\n+                importListener.onFailure(e);\n+            }\n+        });\n+    }\n+\n+    private void findDanglingIndex(ImportDanglingIndexRequest request, ActionListener<IndexMetaData> listener) {\n+        final String indexUUID = request.getIndexUUID();\n+\n+        this.transportService.sendRequest(\n+            this.transportService.getLocalNode(),\n+            FindDanglingIndexAction.NAME,\n+            new FindDanglingIndexRequest(indexUUID),\n+            new TransportResponseHandler<FindDanglingIndexResponse>() {\n+\n+                @Override\n+                public void handleResponse(FindDanglingIndexResponse response) {\n+                    if (response.hasFailures()) {\n+                        for (FailedNodeException failure : response.failures()) {\n+                            logger.error(\"Failed to query \" + failure.nodeId(), failure);\n+                        }\n+\n+                        listener.onFailure(\n+                            new ElasticsearchException(\"Failed to query nodes: \" + map(response.failures(), FailedNodeException::nodeId))\n+                        );\n+                        return;\n+                    }\n+\n+                    final List<IndexMetaData> metaDataSortedByVersion = new ArrayList<>();\n+                    for (NodeFindDanglingIndexResponse each : response.getNodes()) {\n+                        metaDataSortedByVersion.addAll(each.getDanglingIndexMetaData());\n+                    }\n+                    metaDataSortedByVersion.sort(Comparator.comparingLong(IndexMetaData::getVersion));\n+\n+                    if (metaDataSortedByVersion.isEmpty()) {\n+                        listener.onFailure(new IllegalArgumentException(\"No dangling index found for UUID [\" + indexUUID + \"]\"));\n+                        return;\n+                    }\n+\n+                    if (metaDataSortedByVersion.size() > 1) {\n+                        logger.debug(\n+                            \"Metadata versions \"\n+                                + map(metaDataSortedByVersion, IndexMetaData::getVersion)\n+                                + \" found for UUID [\"\n+                                + indexUUID\n+                                + \"], selecting the highest\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3569e45a7ab6590eb19133edf7adc1c2c3b5fe8"}, "originalPosition": 144}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTA1MTcxNw==", "bodyText": "This still needs addressing. I agree that we can hunt for dangling indices on demand for the List action, and also load just the specific metadata needed for the Find action. The DanglingIndicesState is effectively a cache, but I don't think we need to cache anything.\nWe could follow-up with something that checks for dangling indices at startup or periodically if we feel it's needed.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385051717", "createdAt": "2020-02-27T10:52:14Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java", "diffHunk": "@@ -65,24 +69,25 @@\n \n     private final NodeEnvironment nodeEnv;\n     private final MetaStateService metaStateService;\n-    private final LocalAllocateDangledIndices allocateDangledIndices;\n+    private final LocalAllocateDangledIndices danglingIndicesAllocator;\n     private final boolean isAutoImportDanglingIndicesEnabled;\n \n     private final Map<Index, IndexMetaData> danglingIndices = ConcurrentCollections.newConcurrentMap();\n \n     @Inject\n     public DanglingIndicesState(NodeEnvironment nodeEnv, MetaStateService metaStateService,\n-                                LocalAllocateDangledIndices allocateDangledIndices, ClusterService clusterService) {\n+                                LocalAllocateDangledIndices danglingIndicesAllocator, ClusterService clusterService) {\n         this.nodeEnv = nodeEnv;\n         this.metaStateService = metaStateService;\n-        this.allocateDangledIndices = allocateDangledIndices;\n+        this.danglingIndicesAllocator = danglingIndicesAllocator;\n \n         this.isAutoImportDanglingIndicesEnabled = AUTO_IMPORT_DANGLING_INDICES_SETTING.get(clusterService.getSettings());\n \n-        if (this.isAutoImportDanglingIndicesEnabled) {\n-            clusterService.addListener(this);\n-        } else {\n-            logger.warn(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey() + \" is disabled, dangling indices will not be detected or imported\");\n+        clusterService.addListener(this);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM4NzI2NQ=="}, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTA1MjQ2OA==", "bodyText": "Maybe hint that they can now be manually managed?\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            + \" is disabled, dangling indices will not be automatically imported\");\n          \n          \n            \n                            + \" is disabled, dangling indices will not be automatically detected or imported and must be managed manually\");", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385052468", "createdAt": "2020-02-27T10:53:40Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java", "diffHunk": "@@ -65,24 +69,25 @@\n \n     private final NodeEnvironment nodeEnv;\n     private final MetaStateService metaStateService;\n-    private final LocalAllocateDangledIndices allocateDangledIndices;\n+    private final LocalAllocateDangledIndices danglingIndicesAllocator;\n     private final boolean isAutoImportDanglingIndicesEnabled;\n \n     private final Map<Index, IndexMetaData> danglingIndices = ConcurrentCollections.newConcurrentMap();\n \n     @Inject\n     public DanglingIndicesState(NodeEnvironment nodeEnv, MetaStateService metaStateService,\n-                                LocalAllocateDangledIndices allocateDangledIndices, ClusterService clusterService) {\n+                                LocalAllocateDangledIndices danglingIndicesAllocator, ClusterService clusterService) {\n         this.nodeEnv = nodeEnv;\n         this.metaStateService = metaStateService;\n-        this.allocateDangledIndices = allocateDangledIndices;\n+        this.danglingIndicesAllocator = danglingIndicesAllocator;\n \n         this.isAutoImportDanglingIndicesEnabled = AUTO_IMPORT_DANGLING_INDICES_SETTING.get(clusterService.getSettings());\n \n-        if (this.isAutoImportDanglingIndicesEnabled) {\n-            clusterService.addListener(this);\n-        } else {\n-            logger.warn(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey() + \" is disabled, dangling indices will not be detected or imported\");\n+        clusterService.addListener(this);\n+\n+        if (this.isAutoImportDanglingIndicesEnabled == false) {\n+            logger.warn(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey()\n+                + \" is disabled, dangling indices will not be automatically imported\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3569e45a7ab6590eb19133edf7adc1c2c3b5fe8"}, "originalPosition": 51}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fc4a6202f3549b5a860971db6f798d43841e76ef", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/fc4a6202f3549b5a860971db6f798d43841e76ef", "committedDate": "2020-02-27T11:18:37Z", "message": "Merge remote-tracking branch 'upstream/master' into 48366-dangling-indices-api"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8a4321b7771ac1d4ba460a2afa127cb49bed26bf", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/8a4321b7771ac1d4ba460a2afa127cb49bed26bf", "committedDate": "2020-02-27T11:26:32Z", "message": "Rename NodeDanglingIndicesResponse to NodeListDanglingIndicesResponse"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "16b1f5a69d3988805a766a0e3cffd6393763e778", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/16b1f5a69d3988805a766a0e3cffd6393763e778", "committedDate": "2020-02-27T11:33:43Z", "message": "Add a comment in NodeFindDanglingIndexResponse"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "01350afb37d7249fea804075de895c87b588d385", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/01350afb37d7249fea804075de895c87b588d385", "committedDate": "2020-02-27T11:34:17Z", "message": "Rename NodeDanglingIndicesRequest to NodeListDanglingIndicesRequest"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "466eb8c456093c607017d3f34855cb7e8a07b42f", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/466eb8c456093c607017d3f34855cb7e8a07b42f", "committedDate": "2020-02-27T11:51:00Z", "message": "Logging tweaks"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0c413ea5f9d1157861d777c41748612eae39a62a", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/0c413ea5f9d1157861d777c41748612eae39a62a", "committedDate": "2020-02-27T13:11:13Z", "message": "Use NodeClient in TransportDeleteDanglingIndexAction"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "83b4fc5ea0ce9f0cd2c8e7cb8eab4f3203b1f6e5", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/83b4fc5ea0ce9f0cd2c8e7cb8eab4f3203b1f6e5", "committedDate": "2020-02-27T13:24:25Z", "message": "Use NodeClient in TransportImportDanglingIndexAction"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b95bc58a8d95e8ad8ad31b2ad64fd267064d2ff2", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/b95bc58a8d95e8ad8ad31b2ad64fd267064d2ff2", "committedDate": "2020-02-27T15:09:08Z", "message": "Only poll dangling indices if setting is enabled\n\n`gateway.auto_import_dangling_indices` used to only control whether\ndangling indices would be automatically imported when found. This commit\nexpands this setting's role so that when it's disabled,\nDanglingIndicesState won't listen for cluster state changes at all.\nInsetad, the new `/_dangling` API must be used."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c", "committedDate": "2020-02-27T16:07:34Z", "message": "Checkstyle"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY1ODAyMTky", "url": "https://github.com/elastic/elasticsearch/pull/50920#pullrequestreview-365802192", "createdAt": "2020-02-27T16:24:33Z", "commit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 30, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNjoyNDozM1rOFvX6Ig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzo1ODoxM1rOFvbX8g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTIxOTEwNg==", "bodyText": "I think we can rely on the framework to manage our cluster more, ideally enough that this annotation isn't needed. We should be able to start our own data node in each test, ensure it has at least one shard of the test index, and then trigger the dangling index by restarting it. Since we'd be reusing the same cluster we'd need to set SETTING_MAX_TOMBSTONES to 1 throughout and then delete two indices, but I think that's better than taking this much control over the cluster.\nMight be worth extracting a method containing the setup that gets us into a state with a dangling index, since I think that's the same for all the tests.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385219106", "createdAt": "2020-02-27T16:24:33Z", "author": {"login": "DaveCTurner"}, "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http;\n+\n+import org.elasticsearch.client.Request;\n+import org.elasticsearch.client.Response;\n+import org.elasticsearch.client.RestClient;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.XContentTestUtils;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.elasticsearch.cluster.metadata.IndexGraveyard.SETTING_MAX_TOMBSTONES;\n+import static org.elasticsearch.gateway.DanglingIndicesState.AUTO_IMPORT_DANGLING_INDICES_SETTING;\n+import static org.elasticsearch.rest.RestStatus.ACCEPTED;\n+import static org.elasticsearch.rest.RestStatus.OK;\n+import static org.elasticsearch.test.XContentTestUtils.createJsonMapView;\n+import static org.hamcrest.Matchers.emptyString;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.hamcrest.Matchers.not;\n+\n+/**\n+ * This class tests the dangling indices REST API.  These tests are here\n+ * today so they have access to a proper REST client. They cannot be in\n+ * :server:integTest since the REST client needs a proper transport\n+ * implementation, and they cannot be REST tests today since they need to\n+ * restart nodes. Really, though, this test should live elsewhere.\n+ *\n+ * @see org.elasticsearch.action.admin.indices.dangling\n+ */\n+@ClusterScope(numDataNodes = 0, scope = ESIntegTestCase.Scope.TEST, autoManageMasterNodes = false)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTIyMzQ5MQ==", "bodyText": "Suggest importing this statically to be consistent with the other two settings.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385223491", "createdAt": "2020-02-27T16:30:50Z", "author": {"login": "DaveCTurner"}, "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http;\n+\n+import org.elasticsearch.client.Request;\n+import org.elasticsearch.client.Response;\n+import org.elasticsearch.client.RestClient;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.XContentTestUtils;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.elasticsearch.cluster.metadata.IndexGraveyard.SETTING_MAX_TOMBSTONES;\n+import static org.elasticsearch.gateway.DanglingIndicesState.AUTO_IMPORT_DANGLING_INDICES_SETTING;\n+import static org.elasticsearch.rest.RestStatus.ACCEPTED;\n+import static org.elasticsearch.rest.RestStatus.OK;\n+import static org.elasticsearch.test.XContentTestUtils.createJsonMapView;\n+import static org.hamcrest.Matchers.emptyString;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.hamcrest.Matchers.not;\n+\n+/**\n+ * This class tests the dangling indices REST API.  These tests are here\n+ * today so they have access to a proper REST client. They cannot be in\n+ * :server:integTest since the REST client needs a proper transport\n+ * implementation, and they cannot be REST tests today since they need to\n+ * restart nodes. Really, though, this test should live elsewhere.\n+ *\n+ * @see org.elasticsearch.action.admin.indices.dangling\n+ */\n+@ClusterScope(numDataNodes = 0, scope = ESIntegTestCase.Scope.TEST, autoManageMasterNodes = false)\n+public class DanglingIndicesRestIT extends HttpSmokeTestCase {\n+    private static final String INDEX_NAME = \"test-idx-1\";\n+    private static final String OTHER_INDEX_NAME = INDEX_NAME + \"-other\";\n+\n+    private Settings buildSettings(int maxTombstones) {\n+        return Settings.builder()\n+            // Limit the indices kept in the graveyard. This can be set to zero, so that\n+            // when we delete an index, it's definitely considered to be dangling.\n+            .put(SETTING_MAX_TOMBSTONES.getKey(), maxTombstones)\n+            .put(IndicesService.WRITE_DANGLING_INDICES_INFO_SETTING.getKey(), true)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTIyMzk5Nw==", "bodyText": "Now that we list dangling indices on demand, can we drop the assertBusy here?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385223997", "createdAt": "2020-02-27T16:31:38Z", "author": {"login": "DaveCTurner"}, "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http;\n+\n+import org.elasticsearch.client.Request;\n+import org.elasticsearch.client.Response;\n+import org.elasticsearch.client.RestClient;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.XContentTestUtils;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.elasticsearch.cluster.metadata.IndexGraveyard.SETTING_MAX_TOMBSTONES;\n+import static org.elasticsearch.gateway.DanglingIndicesState.AUTO_IMPORT_DANGLING_INDICES_SETTING;\n+import static org.elasticsearch.rest.RestStatus.ACCEPTED;\n+import static org.elasticsearch.rest.RestStatus.OK;\n+import static org.elasticsearch.test.XContentTestUtils.createJsonMapView;\n+import static org.hamcrest.Matchers.emptyString;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.hamcrest.Matchers.not;\n+\n+/**\n+ * This class tests the dangling indices REST API.  These tests are here\n+ * today so they have access to a proper REST client. They cannot be in\n+ * :server:integTest since the REST client needs a proper transport\n+ * implementation, and they cannot be REST tests today since they need to\n+ * restart nodes. Really, though, this test should live elsewhere.\n+ *\n+ * @see org.elasticsearch.action.admin.indices.dangling\n+ */\n+@ClusterScope(numDataNodes = 0, scope = ESIntegTestCase.Scope.TEST, autoManageMasterNodes = false)\n+public class DanglingIndicesRestIT extends HttpSmokeTestCase {\n+    private static final String INDEX_NAME = \"test-idx-1\";\n+    private static final String OTHER_INDEX_NAME = INDEX_NAME + \"-other\";\n+\n+    private Settings buildSettings(int maxTombstones) {\n+        return Settings.builder()\n+            // Limit the indices kept in the graveyard. This can be set to zero, so that\n+            // when we delete an index, it's definitely considered to be dangling.\n+            .put(SETTING_MAX_TOMBSTONES.getKey(), maxTombstones)\n+            .put(IndicesService.WRITE_DANGLING_INDICES_INFO_SETTING.getKey(), true)\n+            .put(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey(), false)\n+            .build();\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed via the REST API.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        ensureStableCluster(3);\n+        createIndices(INDEX_NAME);\n+\n+        final AtomicReference<String> stoppedNodeName = new AtomicReference<>();\n+        final RestClient restClient = getRestClient();\n+\n+        // Restart node, deleting the index in its absence, so that there is a dangling index to recover\n+        internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {\n+\n+            @Override\n+            public Settings onNodeStopped(String nodeName) throws Exception {\n+                ensureClusterSizeConsistency();\n+                stoppedNodeName.set(nodeName);\n+                deleteIndex(INDEX_NAME);\n+                return super.onNodeStopped(nodeName);\n+            }\n+        });\n+\n+        ensureStableCluster(3);\n+\n+        final String stoppedNodeId = mapNodeNameToId(stoppedNodeName.get());\n+\n+        assertBusy(() -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTIyNDA3NQ==", "bodyText": "Now that we list dangling indices on demand, can we drop the assertBusy here?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385224075", "createdAt": "2020-02-27T16:31:47Z", "author": {"login": "DaveCTurner"}, "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http;\n+\n+import org.elasticsearch.client.Request;\n+import org.elasticsearch.client.Response;\n+import org.elasticsearch.client.RestClient;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.XContentTestUtils;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.elasticsearch.cluster.metadata.IndexGraveyard.SETTING_MAX_TOMBSTONES;\n+import static org.elasticsearch.gateway.DanglingIndicesState.AUTO_IMPORT_DANGLING_INDICES_SETTING;\n+import static org.elasticsearch.rest.RestStatus.ACCEPTED;\n+import static org.elasticsearch.rest.RestStatus.OK;\n+import static org.elasticsearch.test.XContentTestUtils.createJsonMapView;\n+import static org.hamcrest.Matchers.emptyString;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.hamcrest.Matchers.not;\n+\n+/**\n+ * This class tests the dangling indices REST API.  These tests are here\n+ * today so they have access to a proper REST client. They cannot be in\n+ * :server:integTest since the REST client needs a proper transport\n+ * implementation, and they cannot be REST tests today since they need to\n+ * restart nodes. Really, though, this test should live elsewhere.\n+ *\n+ * @see org.elasticsearch.action.admin.indices.dangling\n+ */\n+@ClusterScope(numDataNodes = 0, scope = ESIntegTestCase.Scope.TEST, autoManageMasterNodes = false)\n+public class DanglingIndicesRestIT extends HttpSmokeTestCase {\n+    private static final String INDEX_NAME = \"test-idx-1\";\n+    private static final String OTHER_INDEX_NAME = INDEX_NAME + \"-other\";\n+\n+    private Settings buildSettings(int maxTombstones) {\n+        return Settings.builder()\n+            // Limit the indices kept in the graveyard. This can be set to zero, so that\n+            // when we delete an index, it's definitely considered to be dangling.\n+            .put(SETTING_MAX_TOMBSTONES.getKey(), maxTombstones)\n+            .put(IndicesService.WRITE_DANGLING_INDICES_INFO_SETTING.getKey(), true)\n+            .put(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey(), false)\n+            .build();\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed via the REST API.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        ensureStableCluster(3);\n+        createIndices(INDEX_NAME);\n+\n+        final AtomicReference<String> stoppedNodeName = new AtomicReference<>();\n+        final RestClient restClient = getRestClient();\n+\n+        // Restart node, deleting the index in its absence, so that there is a dangling index to recover\n+        internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {\n+\n+            @Override\n+            public Settings onNodeStopped(String nodeName) throws Exception {\n+                ensureClusterSizeConsistency();\n+                stoppedNodeName.set(nodeName);\n+                deleteIndex(INDEX_NAME);\n+                return super.onNodeStopped(nodeName);\n+            }\n+        });\n+\n+        ensureStableCluster(3);\n+\n+        final String stoppedNodeId = mapNodeNameToId(stoppedNodeName.get());\n+\n+        assertBusy(() -> {\n+            final Response listResponse = restClient.performRequest(new Request(\"GET\", \"/_dangling\"));\n+            assertOK(listResponse);\n+\n+            final XContentTestUtils.JsonMapView mapView = createJsonMapView(listResponse.getEntity().getContent());\n+\n+            assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+            assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+            assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+            List<Object> indices = mapView.get(\"dangling_indices\");\n+            assertThat(indices, hasSize(1));\n+\n+            assertThat(mapView.get(\"dangling_indices.0.index_name\"), equalTo(INDEX_NAME));\n+            assertThat(mapView.get(\"dangling_indices.0.index_uuid\"), not(emptyString()));\n+            assertThat(mapView.get(\"dangling_indices.0.creation_date_millis\"), instanceOf(Long.class));\n+            assertThat(mapView.get(\"dangling_indices.0.node_ids.0\"), equalTo(stoppedNodeId));\n+        });\n+    }\n+\n+    /**\n+     * Check that dangling indices can be imported.\n+     */\n+    public void testDanglingIndicesCanBeImported() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        ensureStableCluster(3);\n+        createIndices(INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+        final AtomicReference<String> stoppedNodeName = new AtomicReference<>();\n+\n+        // Restart a node, deleting the index in its absence, so that there is a dangling index to recover\n+        internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {\n+\n+            @Override\n+            public Settings onNodeStopped(String nodeName) throws Exception {\n+                ensureClusterSizeConsistency();\n+                stoppedNodeName.set(nodeName);\n+                deleteIndex(INDEX_NAME);\n+                return super.onNodeStopped(nodeName);\n+            }\n+        });\n+\n+        final AtomicReference<String> danglingIndexUUID = new AtomicReference<>();\n+\n+        // Wait for the dangling index to be noticed\n+        assertBusy(() -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 150}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTIyNDEyMg==", "bodyText": "Now that we list dangling indices on demand, can we drop the assertBusy here?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385224122", "createdAt": "2020-02-27T16:31:51Z", "author": {"login": "DaveCTurner"}, "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http;\n+\n+import org.elasticsearch.client.Request;\n+import org.elasticsearch.client.Response;\n+import org.elasticsearch.client.RestClient;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.XContentTestUtils;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.elasticsearch.cluster.metadata.IndexGraveyard.SETTING_MAX_TOMBSTONES;\n+import static org.elasticsearch.gateway.DanglingIndicesState.AUTO_IMPORT_DANGLING_INDICES_SETTING;\n+import static org.elasticsearch.rest.RestStatus.ACCEPTED;\n+import static org.elasticsearch.rest.RestStatus.OK;\n+import static org.elasticsearch.test.XContentTestUtils.createJsonMapView;\n+import static org.hamcrest.Matchers.emptyString;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.hamcrest.Matchers.not;\n+\n+/**\n+ * This class tests the dangling indices REST API.  These tests are here\n+ * today so they have access to a proper REST client. They cannot be in\n+ * :server:integTest since the REST client needs a proper transport\n+ * implementation, and they cannot be REST tests today since they need to\n+ * restart nodes. Really, though, this test should live elsewhere.\n+ *\n+ * @see org.elasticsearch.action.admin.indices.dangling\n+ */\n+@ClusterScope(numDataNodes = 0, scope = ESIntegTestCase.Scope.TEST, autoManageMasterNodes = false)\n+public class DanglingIndicesRestIT extends HttpSmokeTestCase {\n+    private static final String INDEX_NAME = \"test-idx-1\";\n+    private static final String OTHER_INDEX_NAME = INDEX_NAME + \"-other\";\n+\n+    private Settings buildSettings(int maxTombstones) {\n+        return Settings.builder()\n+            // Limit the indices kept in the graveyard. This can be set to zero, so that\n+            // when we delete an index, it's definitely considered to be dangling.\n+            .put(SETTING_MAX_TOMBSTONES.getKey(), maxTombstones)\n+            .put(IndicesService.WRITE_DANGLING_INDICES_INFO_SETTING.getKey(), true)\n+            .put(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey(), false)\n+            .build();\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed via the REST API.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        ensureStableCluster(3);\n+        createIndices(INDEX_NAME);\n+\n+        final AtomicReference<String> stoppedNodeName = new AtomicReference<>();\n+        final RestClient restClient = getRestClient();\n+\n+        // Restart node, deleting the index in its absence, so that there is a dangling index to recover\n+        internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {\n+\n+            @Override\n+            public Settings onNodeStopped(String nodeName) throws Exception {\n+                ensureClusterSizeConsistency();\n+                stoppedNodeName.set(nodeName);\n+                deleteIndex(INDEX_NAME);\n+                return super.onNodeStopped(nodeName);\n+            }\n+        });\n+\n+        ensureStableCluster(3);\n+\n+        final String stoppedNodeId = mapNodeNameToId(stoppedNodeName.get());\n+\n+        assertBusy(() -> {\n+            final Response listResponse = restClient.performRequest(new Request(\"GET\", \"/_dangling\"));\n+            assertOK(listResponse);\n+\n+            final XContentTestUtils.JsonMapView mapView = createJsonMapView(listResponse.getEntity().getContent());\n+\n+            assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+            assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+            assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+            List<Object> indices = mapView.get(\"dangling_indices\");\n+            assertThat(indices, hasSize(1));\n+\n+            assertThat(mapView.get(\"dangling_indices.0.index_name\"), equalTo(INDEX_NAME));\n+            assertThat(mapView.get(\"dangling_indices.0.index_uuid\"), not(emptyString()));\n+            assertThat(mapView.get(\"dangling_indices.0.creation_date_millis\"), instanceOf(Long.class));\n+            assertThat(mapView.get(\"dangling_indices.0.node_ids.0\"), equalTo(stoppedNodeId));\n+        });\n+    }\n+\n+    /**\n+     * Check that dangling indices can be imported.\n+     */\n+    public void testDanglingIndicesCanBeImported() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        ensureStableCluster(3);\n+        createIndices(INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+        final AtomicReference<String> stoppedNodeName = new AtomicReference<>();\n+\n+        // Restart a node, deleting the index in its absence, so that there is a dangling index to recover\n+        internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {\n+\n+            @Override\n+            public Settings onNodeStopped(String nodeName) throws Exception {\n+                ensureClusterSizeConsistency();\n+                stoppedNodeName.set(nodeName);\n+                deleteIndex(INDEX_NAME);\n+                return super.onNodeStopped(nodeName);\n+            }\n+        });\n+\n+        final AtomicReference<String> danglingIndexUUID = new AtomicReference<>();\n+\n+        // Wait for the dangling index to be noticed\n+        assertBusy(() -> {\n+            final List<String> danglingIndexIds = listDanglingIndexIds();\n+            assertThat(danglingIndexIds, hasSize(1));\n+            danglingIndexUUID.set(danglingIndexIds.get(0));\n+        });\n+\n+        final Request importRequest = new Request(\"POST\", \"/_dangling/\" + danglingIndexUUID.get());\n+        importRequest.addParameter(\"accept_data_loss\", \"true\");\n+        final Response importResponse = restClient.performRequest(importRequest);\n+        assertThat(importResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        assertBusy(() -> assertTrue(\"Expected dangling index \" + INDEX_NAME + \" to be recovered\", indexExists(INDEX_NAME)));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be deleted. Since this requires that\n+     * we add an entry to the index graveyard, the graveyard size must be\n+     * greater than 1. To test deletes, we set the index graveyard size to\n+     * 1, then create two indices and delete them both while one node in\n+     * the cluster is stopped. The deletion of the second pushes the deletion\n+     * of the first out of the graveyard. When the stopped node is resumed,\n+     * only the second index will be found into the graveyard and the the\n+     * other will be considered dangling, and can therefore be listed and\n+     * deleted through the API\n+     */\n+    public void testDanglingIndicesCanBeDeleted() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(1));\n+\n+        ensureStableCluster(3);\n+        createIndices(INDEX_NAME, OTHER_INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+        final AtomicReference<String> stoppedNodeName = new AtomicReference<>();\n+\n+        // Restart a node, deleting the index in its absence, so that there is a dangling index to recover\n+        internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {\n+\n+            @Override\n+            public Settings onNodeStopped(String nodeName) throws Exception {\n+                ensureClusterSizeConsistency();\n+                stoppedNodeName.set(nodeName);\n+                deleteIndex(INDEX_NAME);\n+                deleteIndex(OTHER_INDEX_NAME);\n+                return super.onNodeStopped(nodeName);\n+            }\n+        });\n+\n+        final AtomicReference<String> danglingIndexUUID = new AtomicReference<>();\n+\n+        // Wait for the dangling index to be noticed\n+        assertBusy(() -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 201}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTIzNDQzMA==", "bodyText": "This assertBusy is suspicious. I think the index should exist before the import request returns?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385234430", "createdAt": "2020-02-27T16:47:46Z", "author": {"login": "DaveCTurner"}, "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http;\n+\n+import org.elasticsearch.client.Request;\n+import org.elasticsearch.client.Response;\n+import org.elasticsearch.client.RestClient;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.XContentTestUtils;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.elasticsearch.cluster.metadata.IndexGraveyard.SETTING_MAX_TOMBSTONES;\n+import static org.elasticsearch.gateway.DanglingIndicesState.AUTO_IMPORT_DANGLING_INDICES_SETTING;\n+import static org.elasticsearch.rest.RestStatus.ACCEPTED;\n+import static org.elasticsearch.rest.RestStatus.OK;\n+import static org.elasticsearch.test.XContentTestUtils.createJsonMapView;\n+import static org.hamcrest.Matchers.emptyString;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.hamcrest.Matchers.not;\n+\n+/**\n+ * This class tests the dangling indices REST API.  These tests are here\n+ * today so they have access to a proper REST client. They cannot be in\n+ * :server:integTest since the REST client needs a proper transport\n+ * implementation, and they cannot be REST tests today since they need to\n+ * restart nodes. Really, though, this test should live elsewhere.\n+ *\n+ * @see org.elasticsearch.action.admin.indices.dangling\n+ */\n+@ClusterScope(numDataNodes = 0, scope = ESIntegTestCase.Scope.TEST, autoManageMasterNodes = false)\n+public class DanglingIndicesRestIT extends HttpSmokeTestCase {\n+    private static final String INDEX_NAME = \"test-idx-1\";\n+    private static final String OTHER_INDEX_NAME = INDEX_NAME + \"-other\";\n+\n+    private Settings buildSettings(int maxTombstones) {\n+        return Settings.builder()\n+            // Limit the indices kept in the graveyard. This can be set to zero, so that\n+            // when we delete an index, it's definitely considered to be dangling.\n+            .put(SETTING_MAX_TOMBSTONES.getKey(), maxTombstones)\n+            .put(IndicesService.WRITE_DANGLING_INDICES_INFO_SETTING.getKey(), true)\n+            .put(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey(), false)\n+            .build();\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed via the REST API.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        ensureStableCluster(3);\n+        createIndices(INDEX_NAME);\n+\n+        final AtomicReference<String> stoppedNodeName = new AtomicReference<>();\n+        final RestClient restClient = getRestClient();\n+\n+        // Restart node, deleting the index in its absence, so that there is a dangling index to recover\n+        internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {\n+\n+            @Override\n+            public Settings onNodeStopped(String nodeName) throws Exception {\n+                ensureClusterSizeConsistency();\n+                stoppedNodeName.set(nodeName);\n+                deleteIndex(INDEX_NAME);\n+                return super.onNodeStopped(nodeName);\n+            }\n+        });\n+\n+        ensureStableCluster(3);\n+\n+        final String stoppedNodeId = mapNodeNameToId(stoppedNodeName.get());\n+\n+        assertBusy(() -> {\n+            final Response listResponse = restClient.performRequest(new Request(\"GET\", \"/_dangling\"));\n+            assertOK(listResponse);\n+\n+            final XContentTestUtils.JsonMapView mapView = createJsonMapView(listResponse.getEntity().getContent());\n+\n+            assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+            assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+            assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+            List<Object> indices = mapView.get(\"dangling_indices\");\n+            assertThat(indices, hasSize(1));\n+\n+            assertThat(mapView.get(\"dangling_indices.0.index_name\"), equalTo(INDEX_NAME));\n+            assertThat(mapView.get(\"dangling_indices.0.index_uuid\"), not(emptyString()));\n+            assertThat(mapView.get(\"dangling_indices.0.creation_date_millis\"), instanceOf(Long.class));\n+            assertThat(mapView.get(\"dangling_indices.0.node_ids.0\"), equalTo(stoppedNodeId));\n+        });\n+    }\n+\n+    /**\n+     * Check that dangling indices can be imported.\n+     */\n+    public void testDanglingIndicesCanBeImported() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        ensureStableCluster(3);\n+        createIndices(INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+        final AtomicReference<String> stoppedNodeName = new AtomicReference<>();\n+\n+        // Restart a node, deleting the index in its absence, so that there is a dangling index to recover\n+        internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {\n+\n+            @Override\n+            public Settings onNodeStopped(String nodeName) throws Exception {\n+                ensureClusterSizeConsistency();\n+                stoppedNodeName.set(nodeName);\n+                deleteIndex(INDEX_NAME);\n+                return super.onNodeStopped(nodeName);\n+            }\n+        });\n+\n+        final AtomicReference<String> danglingIndexUUID = new AtomicReference<>();\n+\n+        // Wait for the dangling index to be noticed\n+        assertBusy(() -> {\n+            final List<String> danglingIndexIds = listDanglingIndexIds();\n+            assertThat(danglingIndexIds, hasSize(1));\n+            danglingIndexUUID.set(danglingIndexIds.get(0));\n+        });\n+\n+        final Request importRequest = new Request(\"POST\", \"/_dangling/\" + danglingIndexUUID.get());\n+        importRequest.addParameter(\"accept_data_loss\", \"true\");\n+        final Response importResponse = restClient.performRequest(importRequest);\n+        assertThat(importResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        assertBusy(() -> assertTrue(\"Expected dangling index \" + INDEX_NAME + \" to be recovered\", indexExists(INDEX_NAME)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 161}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTIzNTM4Nw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            stoppedNodeId = elements[0];\n          \n          \n            \n                            return elements[0];\n          \n      \n    \n    \n  \n\nThen I think we can drop stoppedNodeId.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385235387", "createdAt": "2020-02-27T16:49:14Z", "author": {"login": "DaveCTurner"}, "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http;\n+\n+import org.elasticsearch.client.Request;\n+import org.elasticsearch.client.Response;\n+import org.elasticsearch.client.RestClient;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.XContentTestUtils;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.elasticsearch.cluster.metadata.IndexGraveyard.SETTING_MAX_TOMBSTONES;\n+import static org.elasticsearch.gateway.DanglingIndicesState.AUTO_IMPORT_DANGLING_INDICES_SETTING;\n+import static org.elasticsearch.rest.RestStatus.ACCEPTED;\n+import static org.elasticsearch.rest.RestStatus.OK;\n+import static org.elasticsearch.test.XContentTestUtils.createJsonMapView;\n+import static org.hamcrest.Matchers.emptyString;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.hamcrest.Matchers.not;\n+\n+/**\n+ * This class tests the dangling indices REST API.  These tests are here\n+ * today so they have access to a proper REST client. They cannot be in\n+ * :server:integTest since the REST client needs a proper transport\n+ * implementation, and they cannot be REST tests today since they need to\n+ * restart nodes. Really, though, this test should live elsewhere.\n+ *\n+ * @see org.elasticsearch.action.admin.indices.dangling\n+ */\n+@ClusterScope(numDataNodes = 0, scope = ESIntegTestCase.Scope.TEST, autoManageMasterNodes = false)\n+public class DanglingIndicesRestIT extends HttpSmokeTestCase {\n+    private static final String INDEX_NAME = \"test-idx-1\";\n+    private static final String OTHER_INDEX_NAME = INDEX_NAME + \"-other\";\n+\n+    private Settings buildSettings(int maxTombstones) {\n+        return Settings.builder()\n+            // Limit the indices kept in the graveyard. This can be set to zero, so that\n+            // when we delete an index, it's definitely considered to be dangling.\n+            .put(SETTING_MAX_TOMBSTONES.getKey(), maxTombstones)\n+            .put(IndicesService.WRITE_DANGLING_INDICES_INFO_SETTING.getKey(), true)\n+            .put(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey(), false)\n+            .build();\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed via the REST API.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        ensureStableCluster(3);\n+        createIndices(INDEX_NAME);\n+\n+        final AtomicReference<String> stoppedNodeName = new AtomicReference<>();\n+        final RestClient restClient = getRestClient();\n+\n+        // Restart node, deleting the index in its absence, so that there is a dangling index to recover\n+        internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {\n+\n+            @Override\n+            public Settings onNodeStopped(String nodeName) throws Exception {\n+                ensureClusterSizeConsistency();\n+                stoppedNodeName.set(nodeName);\n+                deleteIndex(INDEX_NAME);\n+                return super.onNodeStopped(nodeName);\n+            }\n+        });\n+\n+        ensureStableCluster(3);\n+\n+        final String stoppedNodeId = mapNodeNameToId(stoppedNodeName.get());\n+\n+        assertBusy(() -> {\n+            final Response listResponse = restClient.performRequest(new Request(\"GET\", \"/_dangling\"));\n+            assertOK(listResponse);\n+\n+            final XContentTestUtils.JsonMapView mapView = createJsonMapView(listResponse.getEntity().getContent());\n+\n+            assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+            assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+            assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+            List<Object> indices = mapView.get(\"dangling_indices\");\n+            assertThat(indices, hasSize(1));\n+\n+            assertThat(mapView.get(\"dangling_indices.0.index_name\"), equalTo(INDEX_NAME));\n+            assertThat(mapView.get(\"dangling_indices.0.index_uuid\"), not(emptyString()));\n+            assertThat(mapView.get(\"dangling_indices.0.creation_date_millis\"), instanceOf(Long.class));\n+            assertThat(mapView.get(\"dangling_indices.0.node_ids.0\"), equalTo(stoppedNodeId));\n+        });\n+    }\n+\n+    /**\n+     * Check that dangling indices can be imported.\n+     */\n+    public void testDanglingIndicesCanBeImported() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        ensureStableCluster(3);\n+        createIndices(INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+        final AtomicReference<String> stoppedNodeName = new AtomicReference<>();\n+\n+        // Restart a node, deleting the index in its absence, so that there is a dangling index to recover\n+        internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {\n+\n+            @Override\n+            public Settings onNodeStopped(String nodeName) throws Exception {\n+                ensureClusterSizeConsistency();\n+                stoppedNodeName.set(nodeName);\n+                deleteIndex(INDEX_NAME);\n+                return super.onNodeStopped(nodeName);\n+            }\n+        });\n+\n+        final AtomicReference<String> danglingIndexUUID = new AtomicReference<>();\n+\n+        // Wait for the dangling index to be noticed\n+        assertBusy(() -> {\n+            final List<String> danglingIndexIds = listDanglingIndexIds();\n+            assertThat(danglingIndexIds, hasSize(1));\n+            danglingIndexUUID.set(danglingIndexIds.get(0));\n+        });\n+\n+        final Request importRequest = new Request(\"POST\", \"/_dangling/\" + danglingIndexUUID.get());\n+        importRequest.addParameter(\"accept_data_loss\", \"true\");\n+        final Response importResponse = restClient.performRequest(importRequest);\n+        assertThat(importResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        assertBusy(() -> assertTrue(\"Expected dangling index \" + INDEX_NAME + \" to be recovered\", indexExists(INDEX_NAME)));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be deleted. Since this requires that\n+     * we add an entry to the index graveyard, the graveyard size must be\n+     * greater than 1. To test deletes, we set the index graveyard size to\n+     * 1, then create two indices and delete them both while one node in\n+     * the cluster is stopped. The deletion of the second pushes the deletion\n+     * of the first out of the graveyard. When the stopped node is resumed,\n+     * only the second index will be found into the graveyard and the the\n+     * other will be considered dangling, and can therefore be listed and\n+     * deleted through the API\n+     */\n+    public void testDanglingIndicesCanBeDeleted() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(1));\n+\n+        ensureStableCluster(3);\n+        createIndices(INDEX_NAME, OTHER_INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+        final AtomicReference<String> stoppedNodeName = new AtomicReference<>();\n+\n+        // Restart a node, deleting the index in its absence, so that there is a dangling index to recover\n+        internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {\n+\n+            @Override\n+            public Settings onNodeStopped(String nodeName) throws Exception {\n+                ensureClusterSizeConsistency();\n+                stoppedNodeName.set(nodeName);\n+                deleteIndex(INDEX_NAME);\n+                deleteIndex(OTHER_INDEX_NAME);\n+                return super.onNodeStopped(nodeName);\n+            }\n+        });\n+\n+        final AtomicReference<String> danglingIndexUUID = new AtomicReference<>();\n+\n+        // Wait for the dangling index to be noticed\n+        assertBusy(() -> {\n+            final List<String> danglingIndexIds = listDanglingIndexIds();\n+            assertThat(danglingIndexIds, hasSize(1));\n+            danglingIndexUUID.set(danglingIndexIds.get(0));\n+        });\n+\n+        final Request deleteRequest = new Request(\"DELETE\", \"/_dangling/\" + danglingIndexUUID.get());\n+        deleteRequest.addParameter(\"accept_data_loss\", \"true\");\n+        final Response deleteResponse = restClient.performRequest(deleteRequest);\n+        assertThat(deleteResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        assertBusy(() -> assertThat(\"Expected dangling index to be deleted\", listDanglingIndexIds(), hasSize(0)));\n+    }\n+\n+    private List<String> listDanglingIndexIds() throws IOException {\n+        final Response response = getRestClient().performRequest(new Request(\"GET\", \"/_dangling\"));\n+        assertOK(response);\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(response.getEntity().getContent());\n+\n+        assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+        List<Object> indices = mapView.get(\"dangling_indices\");\n+\n+        List<String> danglingIndexIds = new ArrayList<>();\n+\n+        for (int i = 0; i < indices.size(); i++) {\n+            danglingIndexIds.add(mapView.get(\"dangling_indices.\" + i + \".index_uuid\"));\n+        }\n+\n+        return danglingIndexIds;\n+    }\n+\n+    private void assertOK(Response response) {\n+        assertThat(response.getStatusLine().getStatusCode(), equalTo(OK.getStatus()));\n+    }\n+\n+    /**\n+     * Given a node name, finds the corresponding node ID.\n+     */\n+    private String mapNodeNameToId(String nodeName) throws IOException {\n+        String stoppedNodeId = null;\n+\n+        final Response catResponse = getRestClient().performRequest(new Request(\"GET\", \"/_cat/nodes?full_id&h=id,name\"));\n+        assertOK(catResponse);\n+\n+        for (String nodeLine : Streams.readAllLines(catResponse.getEntity().getContent())) {\n+            String[] elements = nodeLine.split(\" \");\n+            if (elements[1].equals(nodeName)) {\n+                stoppedNodeId = elements[0];", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 252}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTIzNzM1Ng==", "bodyText": "I think we need a test that we really do pick the metadata with the greatest version number.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385237356", "createdAt": "2020-02-27T16:52:05Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportImportDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.client.node.NodeClient;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.util.CollectionUtils;\n+import org.elasticsearch.gateway.LocalAllocateDangledIndices;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.List;\n+\n+/**\n+ * Implements the import of a dangling index. When handling a {@link ImportDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then calls {@link LocalAllocateDangledIndices}\n+ * to perform the actual allocation.\n+ */\n+public class TransportImportDanglingIndexAction extends HandledTransportAction<ImportDanglingIndexRequest, ImportDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportImportDanglingIndexAction.class);\n+\n+    private final LocalAllocateDangledIndices danglingIndexAllocator;\n+    private final NodeClient nodeClient;\n+\n+    @Inject\n+    public TransportImportDanglingIndexAction(\n+        ActionFilters actionFilters,\n+        TransportService transportService,\n+        LocalAllocateDangledIndices danglingIndexAllocator,\n+        NodeClient nodeClient\n+    ) {\n+        super(ImportDanglingIndexAction.NAME, transportService, actionFilters, ImportDanglingIndexRequest::new);\n+        this.danglingIndexAllocator = danglingIndexAllocator;\n+        this.nodeClient = nodeClient;\n+    }\n+\n+    @Override\n+    protected void doExecute(\n+        Task task,\n+        ImportDanglingIndexRequest importRequest,\n+        ActionListener<ImportDanglingIndexResponse> importListener\n+    ) {\n+        findDanglingIndex(importRequest, new ActionListener<>() {\n+            @Override\n+            public void onResponse(IndexMetaData indexMetaDataToImport) {\n+                // This flag is checked at this point so that we always check that the supplied index UUID\n+                // does correspond to a dangling index.\n+                if (importRequest.isAcceptDataLoss() == false) {\n+                    importListener.onFailure(new IllegalArgumentException(\"accept_data_loss must be set to true\"));\n+                    return;\n+                }\n+\n+                danglingIndexAllocator.allocateDangled(List.of(indexMetaDataToImport), new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(LocalAllocateDangledIndices.AllocateDangledResponse allocateDangledResponse) {\n+                        importListener.onResponse(new ImportDanglingIndexResponse());\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        logger.debug(\"Failed to import dangling index [\" + indexMetaDataToImport.getIndexUUID() + \"]\", e);\n+                        importListener.onFailure(e);\n+                    }\n+                });\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.debug(\"Failed to find dangling index [\" + importRequest.getIndexUUID() + \"]\", e);\n+                importListener.onFailure(e);\n+            }\n+        });\n+    }\n+\n+    private void findDanglingIndex(ImportDanglingIndexRequest request, ActionListener<IndexMetaData> listener) {\n+        final String indexUUID = request.getIndexUUID();\n+\n+        this.nodeClient.execute(FindDanglingIndexAction.INSTANCE, new FindDanglingIndexRequest(indexUUID), new ActionListener<>() {\n+            @Override\n+            public void onResponse(FindDanglingIndexResponse response) {\n+                if (response.hasFailures()) {\n+                    for (FailedNodeException failure : response.failures()) {\n+                        logger.error(\"Failed to query \" + failure.nodeId(), failure);\n+                    }\n+\n+                    listener.onFailure(\n+                        new ElasticsearchException(\n+                            \"Failed to query nodes: \" + CollectionUtils.map(response.failures(), FailedNodeException::nodeId)\n+                        )\n+                    );\n+                    return;\n+                }\n+\n+                final List<IndexMetaData> metaDataSortedByVersion = new ArrayList<>();\n+                for (NodeFindDanglingIndexResponse each : response.getNodes()) {\n+                    metaDataSortedByVersion.addAll(each.getDanglingIndexMetaData());\n+                }\n+                metaDataSortedByVersion.sort(Comparator.comparingLong(IndexMetaData::getVersion));\n+\n+                if (metaDataSortedByVersion.isEmpty()) {\n+                    listener.onFailure(new IllegalArgumentException(\"No dangling index found for UUID [\" + indexUUID + \"]\"));\n+                    return;\n+                }\n+\n+                logger.debug(\n+                    \"Metadata versions {}  found for UUID [{}], selecting the highest\",\n+                    CollectionUtils.map(metaDataSortedByVersion, IndexMetaData::getVersion),\n+                    indexUUID\n+                );\n+\n+                listener.onResponse(metaDataSortedByVersion.get(metaDataSortedByVersion.size() - 1));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTIzOTU4Mg==", "bodyText": "Can we get the index UUID from before and then assert that it matches the one we get here?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385239582", "createdAt": "2020-02-27T16:55:35Z", "author": {"login": "DaveCTurner"}, "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http;\n+\n+import org.elasticsearch.client.Request;\n+import org.elasticsearch.client.Response;\n+import org.elasticsearch.client.RestClient;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.XContentTestUtils;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.elasticsearch.cluster.metadata.IndexGraveyard.SETTING_MAX_TOMBSTONES;\n+import static org.elasticsearch.gateway.DanglingIndicesState.AUTO_IMPORT_DANGLING_INDICES_SETTING;\n+import static org.elasticsearch.rest.RestStatus.ACCEPTED;\n+import static org.elasticsearch.rest.RestStatus.OK;\n+import static org.elasticsearch.test.XContentTestUtils.createJsonMapView;\n+import static org.hamcrest.Matchers.emptyString;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.hamcrest.Matchers.not;\n+\n+/**\n+ * This class tests the dangling indices REST API.  These tests are here\n+ * today so they have access to a proper REST client. They cannot be in\n+ * :server:integTest since the REST client needs a proper transport\n+ * implementation, and they cannot be REST tests today since they need to\n+ * restart nodes. Really, though, this test should live elsewhere.\n+ *\n+ * @see org.elasticsearch.action.admin.indices.dangling\n+ */\n+@ClusterScope(numDataNodes = 0, scope = ESIntegTestCase.Scope.TEST, autoManageMasterNodes = false)\n+public class DanglingIndicesRestIT extends HttpSmokeTestCase {\n+    private static final String INDEX_NAME = \"test-idx-1\";\n+    private static final String OTHER_INDEX_NAME = INDEX_NAME + \"-other\";\n+\n+    private Settings buildSettings(int maxTombstones) {\n+        return Settings.builder()\n+            // Limit the indices kept in the graveyard. This can be set to zero, so that\n+            // when we delete an index, it's definitely considered to be dangling.\n+            .put(SETTING_MAX_TOMBSTONES.getKey(), maxTombstones)\n+            .put(IndicesService.WRITE_DANGLING_INDICES_INFO_SETTING.getKey(), true)\n+            .put(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey(), false)\n+            .build();\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed via the REST API.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        ensureStableCluster(3);\n+        createIndices(INDEX_NAME);\n+\n+        final AtomicReference<String> stoppedNodeName = new AtomicReference<>();\n+        final RestClient restClient = getRestClient();\n+\n+        // Restart node, deleting the index in its absence, so that there is a dangling index to recover\n+        internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {\n+\n+            @Override\n+            public Settings onNodeStopped(String nodeName) throws Exception {\n+                ensureClusterSizeConsistency();\n+                stoppedNodeName.set(nodeName);\n+                deleteIndex(INDEX_NAME);\n+                return super.onNodeStopped(nodeName);\n+            }\n+        });\n+\n+        ensureStableCluster(3);\n+\n+        final String stoppedNodeId = mapNodeNameToId(stoppedNodeName.get());\n+\n+        assertBusy(() -> {\n+            final Response listResponse = restClient.performRequest(new Request(\"GET\", \"/_dangling\"));\n+            assertOK(listResponse);\n+\n+            final XContentTestUtils.JsonMapView mapView = createJsonMapView(listResponse.getEntity().getContent());\n+\n+            assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+            assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+            assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+            List<Object> indices = mapView.get(\"dangling_indices\");\n+            assertThat(indices, hasSize(1));\n+\n+            assertThat(mapView.get(\"dangling_indices.0.index_name\"), equalTo(INDEX_NAME));\n+            assertThat(mapView.get(\"dangling_indices.0.index_uuid\"), not(emptyString()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI0MTEzMQ==", "bodyText": "I have a slight preference for creationDateMillis here, since it's just a bare long and in some places that means nanoseconds.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385241131", "createdAt": "2020-02-27T16:58:00Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DanglingIndexInfo.java", "diffHunk": "@@ -0,0 +1,89 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.support.nodes.BaseNodeResponse;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ToXContentObject;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Contains information about a dangling index, i.e. an index that Elasticsearch has found\n+ * on-disk but is not present in the cluster state.\n+ */\n+public class DanglingIndexInfo extends BaseNodeResponse implements ToXContentObject {\n+    private String indexName;\n+    private String indexUUID;\n+    private long creationDate;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI0MTcwNw==", "bodyText": "I think we can drop the this. qualifier in a bunch of places in this class?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385241707", "createdAt": "2020-02-27T16:58:54Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DanglingIndexInfo.java", "diffHunk": "@@ -0,0 +1,89 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.support.nodes.BaseNodeResponse;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ToXContentObject;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Contains information about a dangling index, i.e. an index that Elasticsearch has found\n+ * on-disk but is not present in the cluster state.\n+ */\n+public class DanglingIndexInfo extends BaseNodeResponse implements ToXContentObject {\n+    private String indexName;\n+    private String indexUUID;\n+    private long creationDate;\n+\n+    public DanglingIndexInfo(DiscoveryNode node, String indexName, String indexUUID, long creationDate) {\n+        super(node);\n+        this.indexName = indexName;\n+        this.indexUUID = indexUUID;\n+        this.creationDate = creationDate;\n+    }\n+\n+    public DanglingIndexInfo(StreamInput in) throws IOException {\n+        super(in);\n+        this.indexName = in.readString();\n+        this.indexUUID = in.readString();\n+        this.creationDate = in.readLong();\n+    }\n+\n+    public String getIndexName() {\n+        return indexName;\n+    }\n+\n+    public String getIndexUUID() {\n+        return indexUUID;\n+    }\n+\n+    public String getNodeId() {\n+        return this.getNode().getId();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI0NDcyNw==", "bodyText": "I think we should not hardcode this timeout - it's inevitable that we find a cluster where the default isn't long enough, so we should accept a ?timeout parameter in line with other APIs.\nSimilarly I think should accept a ?master_timeout parameter in line with other APIs.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385244727", "createdAt": "2020-02-27T17:03:50Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DeleteDanglingIndexRequest.java", "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionRequestValidationException;\n+import org.elasticsearch.action.support.master.AcknowledgedRequest;\n+import org.elasticsearch.action.support.master.MasterNodeRequest;\n+import org.elasticsearch.cluster.ack.AckedRequest;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.unit.TimeValue;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Represents a request to delete a particular dangling index, specified by its UUID. The {@link #acceptDataLoss}\n+ * flag must also be explicitly set to true, or later validation will fail.\n+ */\n+public class DeleteDanglingIndexRequest extends MasterNodeRequest<DeleteDanglingIndexRequest> implements AckedRequest {\n+    private final String indexUUID;\n+    private final boolean acceptDataLoss;\n+\n+    public DeleteDanglingIndexRequest(StreamInput in) throws IOException {\n+        super(in);\n+        this.indexUUID = in.readString();\n+        this.acceptDataLoss = in.readBoolean();\n+    }\n+\n+    public DeleteDanglingIndexRequest(String indexUUID, boolean acceptDataLoss) {\n+        super();\n+        this.indexUUID = Strings.requireNonEmpty(indexUUID, \"indexUUID cannot be null or empty\");\n+        this.acceptDataLoss = acceptDataLoss;\n+    }\n+\n+    @Override\n+    public ActionRequestValidationException validate() {\n+        return null;\n+    }\n+\n+    public String getIndexUUID() {\n+        return indexUUID;\n+    }\n+\n+    public boolean isAcceptDataLoss() {\n+        return acceptDataLoss;\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return \"DeleteDanglingIndexRequest{\" + \"indexUUID='\" + indexUUID + \"', acceptDataLoss=\" + acceptDataLoss + '}';\n+    }\n+\n+    @Override\n+    public void writeTo(StreamOutput out) throws IOException {\n+        super.writeTo(out);\n+        out.writeString(this.indexUUID);\n+        out.writeBoolean(this.acceptDataLoss);\n+    }\n+\n+    @Override\n+    public TimeValue ackTimeout() {\n+        return AcknowledgedRequest.DEFAULT_ACK_TIMEOUT;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI0NTUzNg==", "bodyText": "Can we use org.elasticsearch.action.support.master.AcknowledgedResponse instead of a specialised class here?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385245536", "createdAt": "2020-02-27T17:05:21Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DeleteDanglingIndexResponse.java", "diffHunk": "@@ -0,0 +1,61 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionResponse;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.StatusToXContentObject;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.rest.RestStatus;\n+\n+import java.io.IOException;\n+\n+public class DeleteDanglingIndexResponse extends ActionResponse implements StatusToXContentObject {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI0OTk4Mg==", "bodyText": "Can we drop this acknowledged field? I don't think we wait for all nodes to ack the import, in which case we shouldn't really return true here.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385249982", "createdAt": "2020-02-27T17:12:15Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ImportDanglingIndexResponse.java", "diffHunk": "@@ -0,0 +1,53 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionResponse;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.StatusToXContentObject;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.rest.RestStatus;\n+\n+import java.io.IOException;\n+\n+public class ImportDanglingIndexResponse extends ActionResponse implements StatusToXContentObject {\n+    public ImportDanglingIndexResponse() {\n+    }\n+\n+    public ImportDanglingIndexResponse(StreamInput in) throws IOException {\n+        super(in);\n+    }\n+\n+    @Override\n+    public RestStatus status() {\n+        return RestStatus.ACCEPTED;\n+    }\n+\n+    @Override\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        return builder.startObject().field(\"acknowledged\", true).endObject();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI1MzAzOQ==", "bodyText": "use byIndexUUID.computeIfAbsent or byIndexUUID.putIfAbsent?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385253039", "createdAt": "2020-02-27T17:17:41Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ListDanglingIndicesResponse.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.nodes.BaseNodesResponse;\n+import org.elasticsearch.cluster.ClusterName;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.StatusToXContentObject;\n+import org.elasticsearch.common.xcontent.XContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.rest.RestStatus;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Models a response to a {@link ListDanglingIndicesRequest}. A list request queries every node in the\n+ * cluster and aggregates their responses. When the aggregated response is converted to {@link XContent},\n+ * information for each dangling index is presented under the \"dangling_indices\" key. If any nodes\n+ * in the cluster failed to answer, the details are presented under the \"_nodes.failures\" key.\n+ */\n+public class ListDanglingIndicesResponse extends BaseNodesResponse<NodeListDanglingIndicesResponse> implements StatusToXContentObject {\n+\n+    public ListDanglingIndicesResponse(StreamInput in) throws IOException {\n+        super(in);\n+    }\n+\n+    public ListDanglingIndicesResponse(\n+        ClusterName clusterName,\n+        List<NodeListDanglingIndicesResponse> nodes,\n+        List<FailedNodeException> failures\n+    ) {\n+        super(clusterName, nodes, failures);\n+    }\n+\n+    @Override\n+    public RestStatus status() {\n+        return this.hasFailures() ? RestStatus.INTERNAL_SERVER_ERROR : RestStatus.OK;\n+    }\n+\n+    private Collection<AggregatedDanglingIndexInfo> resultsByIndexUUID() {\n+        Map<String, AggregatedDanglingIndexInfo> byIndexUUID = new HashMap<>();\n+\n+        for (NodeListDanglingIndicesResponse nodeResponse : this.getNodes()) {\n+            for (DanglingIndexInfo info : nodeResponse.getDanglingIndices()) {\n+                final String indexUUID = info.getIndexUUID();\n+\n+                if (byIndexUUID.containsKey(indexUUID) == false) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI1NzI3Mg==", "bodyText": "Does this need extends BaseNodeResponse? I think the actual per-node response is NodeListDanglingIndicesResponse.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385257272", "createdAt": "2020-02-27T17:25:06Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DanglingIndexInfo.java", "diffHunk": "@@ -0,0 +1,89 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.support.nodes.BaseNodeResponse;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ToXContentObject;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Contains information about a dangling index, i.e. an index that Elasticsearch has found\n+ * on-disk but is not present in the cluster state.\n+ */\n+public class DanglingIndexInfo extends BaseNodeResponse implements ToXContentObject {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI2MDAwNg==", "bodyText": "Simple though it is, I think this aggregation logic deserves a unit test.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385260006", "createdAt": "2020-02-27T17:28:39Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ListDanglingIndicesResponse.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.nodes.BaseNodesResponse;\n+import org.elasticsearch.cluster.ClusterName;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.StatusToXContentObject;\n+import org.elasticsearch.common.xcontent.XContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.rest.RestStatus;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Models a response to a {@link ListDanglingIndicesRequest}. A list request queries every node in the\n+ * cluster and aggregates their responses. When the aggregated response is converted to {@link XContent},\n+ * information for each dangling index is presented under the \"dangling_indices\" key. If any nodes\n+ * in the cluster failed to answer, the details are presented under the \"_nodes.failures\" key.\n+ */\n+public class ListDanglingIndicesResponse extends BaseNodesResponse<NodeListDanglingIndicesResponse> implements StatusToXContentObject {\n+\n+    public ListDanglingIndicesResponse(StreamInput in) throws IOException {\n+        super(in);\n+    }\n+\n+    public ListDanglingIndicesResponse(\n+        ClusterName clusterName,\n+        List<NodeListDanglingIndicesResponse> nodes,\n+        List<FailedNodeException> failures\n+    ) {\n+        super(clusterName, nodes, failures);\n+    }\n+\n+    @Override\n+    public RestStatus status() {\n+        return this.hasFailures() ? RestStatus.INTERNAL_SERVER_ERROR : RestStatus.OK;\n+    }\n+\n+    private Collection<AggregatedDanglingIndexInfo> resultsByIndexUUID() {\n+        Map<String, AggregatedDanglingIndexInfo> byIndexUUID = new HashMap<>();\n+\n+        for (NodeListDanglingIndicesResponse nodeResponse : this.getNodes()) {\n+            for (DanglingIndexInfo info : nodeResponse.getDanglingIndices()) {\n+                final String indexUUID = info.getIndexUUID();\n+\n+                if (byIndexUUID.containsKey(indexUUID) == false) {\n+                    AggregatedDanglingIndexInfo aggregatedInfo = new AggregatedDanglingIndexInfo(\n+                        indexUUID,\n+                        info.getIndexName(),\n+                        info.getCreationDate()\n+                    );\n+\n+                    byIndexUUID.put(indexUUID, aggregatedInfo);\n+                }\n+\n+                byIndexUUID.get(indexUUID).getNodeIds().add(nodeResponse.getNode().getId());\n+            }\n+        }\n+\n+        return byIndexUUID.values();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI2MDgxNg==", "bodyText": "Probably a holdover from a previous iteration, but here we're not listing all indices we're finding one of them.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385260816", "createdAt": "2020-02-27T17:29:59Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.client.node.NodeClient;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.Index;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, DeleteDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+    private final NodeClient nodeClient;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings,\n+        NodeClient nodeClient\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+        this.nodeClient = nodeClient;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.GENERIC;\n+    }\n+\n+    @Override\n+    protected DeleteDanglingIndexResponse read(StreamInput in) throws IOException {\n+        return new DeleteDanglingIndexResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest deleteRequest,\n+        ClusterState state,\n+        ActionListener<DeleteDanglingIndexResponse> deleteListener\n+    ) throws Exception {\n+        findDanglingIndex(deleteRequest.getIndexUUID(), new ActionListener<>() {\n+\n+            @Override\n+            public void onResponse(Index indexToDelete) {\n+                // This flag is checked at this point so that we always check that the supplied index ID\n+                // does correspond to a dangling index.\n+                if (deleteRequest.isAcceptDataLoss() == false) {\n+                    deleteListener.onFailure(new IllegalArgumentException(\"accept_data_loss must be set to true\"));\n+                    return;\n+                }\n+\n+                String indexName = indexToDelete.getName();\n+\n+                final ActionListener<DeleteDanglingIndexResponse> clusterStateUpdatedListener = new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(DeleteDanglingIndexResponse response) {\n+                        deleteListener.onResponse(response);\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        logger.debug(\"Failed to delete dangling index [{}]\" + indexName, e);\n+                        deleteListener.onFailure(e);\n+                    }\n+                };\n+\n+                clusterService.submitStateUpdateTask(\n+                    \"delete-dangling-index \" + indexName,\n+                    new AckedClusterStateUpdateTask<>(deleteRequest, clusterStateUpdatedListener) {\n+\n+                        @Override\n+                        protected DeleteDanglingIndexResponse newResponse(boolean acknowledged) {\n+                            return new DeleteDanglingIndexResponse(acknowledged);\n+                        }\n+\n+                        @Override\n+                        public ClusterState execute(final ClusterState currentState) {\n+                            return deleteDanglingIndex(currentState, indexToDelete);\n+                        }\n+                    }\n+                );\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.debug(\"Failed to list dangling indices\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 146}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI2MTYyMw==", "bodyText": "Optional nit: maybe inline a bunch of these local variables.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385261623", "createdAt": "2020-02-27T17:31:23Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.client.node.NodeClient;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.Index;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, DeleteDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+    private final NodeClient nodeClient;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings,\n+        NodeClient nodeClient\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+        this.nodeClient = nodeClient;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.GENERIC;\n+    }\n+\n+    @Override\n+    protected DeleteDanglingIndexResponse read(StreamInput in) throws IOException {\n+        return new DeleteDanglingIndexResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest deleteRequest,\n+        ClusterState state,\n+        ActionListener<DeleteDanglingIndexResponse> deleteListener\n+    ) throws Exception {\n+        findDanglingIndex(deleteRequest.getIndexUUID(), new ActionListener<>() {\n+\n+            @Override\n+            public void onResponse(Index indexToDelete) {\n+                // This flag is checked at this point so that we always check that the supplied index ID\n+                // does correspond to a dangling index.\n+                if (deleteRequest.isAcceptDataLoss() == false) {\n+                    deleteListener.onFailure(new IllegalArgumentException(\"accept_data_loss must be set to true\"));\n+                    return;\n+                }\n+\n+                String indexName = indexToDelete.getName();\n+\n+                final ActionListener<DeleteDanglingIndexResponse> clusterStateUpdatedListener = new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(DeleteDanglingIndexResponse response) {\n+                        deleteListener.onResponse(response);\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        logger.debug(\"Failed to delete dangling index [{}]\" + indexName, e);\n+                        deleteListener.onFailure(e);\n+                    }\n+                };\n+\n+                clusterService.submitStateUpdateTask(\n+                    \"delete-dangling-index \" + indexName,\n+                    new AckedClusterStateUpdateTask<>(deleteRequest, clusterStateUpdatedListener) {\n+\n+                        @Override\n+                        protected DeleteDanglingIndexResponse newResponse(boolean acknowledged) {\n+                            return new DeleteDanglingIndexResponse(acknowledged);\n+                        }\n+\n+                        @Override\n+                        public ClusterState execute(final ClusterState currentState) {\n+                            return deleteDanglingIndex(currentState, indexToDelete);\n+                        }\n+                    }\n+                );\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.debug(\"Failed to list dangling indices\", e);\n+                deleteListener.onFailure(e);\n+            }\n+        });\n+    }\n+\n+    private ClusterState deleteDanglingIndex(ClusterState currentState, Index indexToDelete) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 152}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI2MzE3MQ==", "bodyText": "I think it might avoid some confusion to include the index UUID and put the whole thing in square brackets too:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                \"delete-dangling-index \" + indexName,\n          \n          \n            \n                                \"delete-dangling-index \" + indexToDelete,", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385263171", "createdAt": "2020-02-27T17:34:18Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.client.node.NodeClient;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.Index;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, DeleteDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+    private final NodeClient nodeClient;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings,\n+        NodeClient nodeClient\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+        this.nodeClient = nodeClient;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.GENERIC;\n+    }\n+\n+    @Override\n+    protected DeleteDanglingIndexResponse read(StreamInput in) throws IOException {\n+        return new DeleteDanglingIndexResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest deleteRequest,\n+        ClusterState state,\n+        ActionListener<DeleteDanglingIndexResponse> deleteListener\n+    ) throws Exception {\n+        findDanglingIndex(deleteRequest.getIndexUUID(), new ActionListener<>() {\n+\n+            @Override\n+            public void onResponse(Index indexToDelete) {\n+                // This flag is checked at this point so that we always check that the supplied index ID\n+                // does correspond to a dangling index.\n+                if (deleteRequest.isAcceptDataLoss() == false) {\n+                    deleteListener.onFailure(new IllegalArgumentException(\"accept_data_loss must be set to true\"));\n+                    return;\n+                }\n+\n+                String indexName = indexToDelete.getName();\n+\n+                final ActionListener<DeleteDanglingIndexResponse> clusterStateUpdatedListener = new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(DeleteDanglingIndexResponse response) {\n+                        deleteListener.onResponse(response);\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        logger.debug(\"Failed to delete dangling index [{}]\" + indexName, e);\n+                        deleteListener.onFailure(e);\n+                    }\n+                };\n+\n+                clusterService.submitStateUpdateTask(\n+                    \"delete-dangling-index \" + indexName,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 128}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI2Mzc5OQ==", "bodyText": "Worth a test that we bail out on one or more failures here, with appropriate logging.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385263799", "createdAt": "2020-02-27T17:35:34Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.client.node.NodeClient;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.Index;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, DeleteDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+    private final NodeClient nodeClient;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings,\n+        NodeClient nodeClient\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+        this.nodeClient = nodeClient;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.GENERIC;\n+    }\n+\n+    @Override\n+    protected DeleteDanglingIndexResponse read(StreamInput in) throws IOException {\n+        return new DeleteDanglingIndexResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest deleteRequest,\n+        ClusterState state,\n+        ActionListener<DeleteDanglingIndexResponse> deleteListener\n+    ) throws Exception {\n+        findDanglingIndex(deleteRequest.getIndexUUID(), new ActionListener<>() {\n+\n+            @Override\n+            public void onResponse(Index indexToDelete) {\n+                // This flag is checked at this point so that we always check that the supplied index ID\n+                // does correspond to a dangling index.\n+                if (deleteRequest.isAcceptDataLoss() == false) {\n+                    deleteListener.onFailure(new IllegalArgumentException(\"accept_data_loss must be set to true\"));\n+                    return;\n+                }\n+\n+                String indexName = indexToDelete.getName();\n+\n+                final ActionListener<DeleteDanglingIndexResponse> clusterStateUpdatedListener = new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(DeleteDanglingIndexResponse response) {\n+                        deleteListener.onResponse(response);\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        logger.debug(\"Failed to delete dangling index [{}]\" + indexName, e);\n+                        deleteListener.onFailure(e);\n+                    }\n+                };\n+\n+                clusterService.submitStateUpdateTask(\n+                    \"delete-dangling-index \" + indexName,\n+                    new AckedClusterStateUpdateTask<>(deleteRequest, clusterStateUpdatedListener) {\n+\n+                        @Override\n+                        protected DeleteDanglingIndexResponse newResponse(boolean acknowledged) {\n+                            return new DeleteDanglingIndexResponse(acknowledged);\n+                        }\n+\n+                        @Override\n+                        public ClusterState execute(final ClusterState currentState) {\n+                            return deleteDanglingIndex(currentState, indexToDelete);\n+                        }\n+                    }\n+                );\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.debug(\"Failed to list dangling indices\", e);\n+                deleteListener.onFailure(e);\n+            }\n+        });\n+    }\n+\n+    private ClusterState deleteDanglingIndex(ClusterState currentState, Index indexToDelete) {\n+        final MetaData meta = currentState.metaData();\n+\n+        MetaData.Builder metaDataBuilder = MetaData.builder(meta);\n+\n+        final IndexGraveyard.Builder graveyardBuilder = IndexGraveyard.builder(metaDataBuilder.indexGraveyard());\n+\n+        final IndexGraveyard newGraveyard = graveyardBuilder.addTombstone(indexToDelete).build(settings);\n+        metaDataBuilder.indexGraveyard(newGraveyard);\n+\n+        return ClusterState.builder(currentState).metaData(metaDataBuilder.build()).build();\n+    }\n+\n+    @Override\n+    protected ClusterBlockException checkBlock(DeleteDanglingIndexRequest request, ClusterState state) {\n+        return null;\n+    }\n+\n+    private void findDanglingIndex(String indexUUID, ActionListener<Index> listener) {\n+        this.nodeClient.execute(FindDanglingIndexAction.INSTANCE, new FindDanglingIndexRequest(indexUUID), new ActionListener<>() {\n+            @Override\n+            public void onResponse(FindDanglingIndexResponse response) {\n+                if (response.hasFailures()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 174}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI2ODg0Mg==", "bodyText": "This is almost the same as what we do in TransportImportDanglingIndexAction except:\n\nwe expose any errors better here\nwe return the first Index here and the latest IndexMetaData in the import action.\n\nI think we could reasonably extract this common code, bringing in the error handling from here and the find-the-latest-IndexMetaData from the other place. It makes little difference if we use the first or the latest Index here.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385268842", "createdAt": "2020-02-27T17:45:06Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.client.node.NodeClient;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.Index;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, DeleteDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+    private final NodeClient nodeClient;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings,\n+        NodeClient nodeClient\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+        this.nodeClient = nodeClient;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.GENERIC;\n+    }\n+\n+    @Override\n+    protected DeleteDanglingIndexResponse read(StreamInput in) throws IOException {\n+        return new DeleteDanglingIndexResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest deleteRequest,\n+        ClusterState state,\n+        ActionListener<DeleteDanglingIndexResponse> deleteListener\n+    ) throws Exception {\n+        findDanglingIndex(deleteRequest.getIndexUUID(), new ActionListener<>() {\n+\n+            @Override\n+            public void onResponse(Index indexToDelete) {\n+                // This flag is checked at this point so that we always check that the supplied index ID\n+                // does correspond to a dangling index.\n+                if (deleteRequest.isAcceptDataLoss() == false) {\n+                    deleteListener.onFailure(new IllegalArgumentException(\"accept_data_loss must be set to true\"));\n+                    return;\n+                }\n+\n+                String indexName = indexToDelete.getName();\n+\n+                final ActionListener<DeleteDanglingIndexResponse> clusterStateUpdatedListener = new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(DeleteDanglingIndexResponse response) {\n+                        deleteListener.onResponse(response);\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        logger.debug(\"Failed to delete dangling index [{}]\" + indexName, e);\n+                        deleteListener.onFailure(e);\n+                    }\n+                };\n+\n+                clusterService.submitStateUpdateTask(\n+                    \"delete-dangling-index \" + indexName,\n+                    new AckedClusterStateUpdateTask<>(deleteRequest, clusterStateUpdatedListener) {\n+\n+                        @Override\n+                        protected DeleteDanglingIndexResponse newResponse(boolean acknowledged) {\n+                            return new DeleteDanglingIndexResponse(acknowledged);\n+                        }\n+\n+                        @Override\n+                        public ClusterState execute(final ClusterState currentState) {\n+                            return deleteDanglingIndex(currentState, indexToDelete);\n+                        }\n+                    }\n+                );\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.debug(\"Failed to list dangling indices\", e);\n+                deleteListener.onFailure(e);\n+            }\n+        });\n+    }\n+\n+    private ClusterState deleteDanglingIndex(ClusterState currentState, Index indexToDelete) {\n+        final MetaData meta = currentState.metaData();\n+\n+        MetaData.Builder metaDataBuilder = MetaData.builder(meta);\n+\n+        final IndexGraveyard.Builder graveyardBuilder = IndexGraveyard.builder(metaDataBuilder.indexGraveyard());\n+\n+        final IndexGraveyard newGraveyard = graveyardBuilder.addTombstone(indexToDelete).build(settings);\n+        metaDataBuilder.indexGraveyard(newGraveyard);\n+\n+        return ClusterState.builder(currentState).metaData(metaDataBuilder.build()).build();\n+    }\n+\n+    @Override\n+    protected ClusterBlockException checkBlock(DeleteDanglingIndexRequest request, ClusterState state) {\n+        return null;\n+    }\n+\n+    private void findDanglingIndex(String indexUUID, ActionListener<Index> listener) {\n+        this.nodeClient.execute(FindDanglingIndexAction.INSTANCE, new FindDanglingIndexRequest(indexUUID), new ActionListener<>() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 171}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI2OTU4Ng==", "bodyText": "See earlier comment about re-using the error handling from TransportDeleteDanglingIndexAction.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385269586", "createdAt": "2020-02-27T17:46:32Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportImportDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.client.node.NodeClient;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.util.CollectionUtils;\n+import org.elasticsearch.gateway.LocalAllocateDangledIndices;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.List;\n+\n+/**\n+ * Implements the import of a dangling index. When handling a {@link ImportDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then calls {@link LocalAllocateDangledIndices}\n+ * to perform the actual allocation.\n+ */\n+public class TransportImportDanglingIndexAction extends HandledTransportAction<ImportDanglingIndexRequest, ImportDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportImportDanglingIndexAction.class);\n+\n+    private final LocalAllocateDangledIndices danglingIndexAllocator;\n+    private final NodeClient nodeClient;\n+\n+    @Inject\n+    public TransportImportDanglingIndexAction(\n+        ActionFilters actionFilters,\n+        TransportService transportService,\n+        LocalAllocateDangledIndices danglingIndexAllocator,\n+        NodeClient nodeClient\n+    ) {\n+        super(ImportDanglingIndexAction.NAME, transportService, actionFilters, ImportDanglingIndexRequest::new);\n+        this.danglingIndexAllocator = danglingIndexAllocator;\n+        this.nodeClient = nodeClient;\n+    }\n+\n+    @Override\n+    protected void doExecute(\n+        Task task,\n+        ImportDanglingIndexRequest importRequest,\n+        ActionListener<ImportDanglingIndexResponse> importListener\n+    ) {\n+        findDanglingIndex(importRequest, new ActionListener<>() {\n+            @Override\n+            public void onResponse(IndexMetaData indexMetaDataToImport) {\n+                // This flag is checked at this point so that we always check that the supplied index UUID\n+                // does correspond to a dangling index.\n+                if (importRequest.isAcceptDataLoss() == false) {\n+                    importListener.onFailure(new IllegalArgumentException(\"accept_data_loss must be set to true\"));\n+                    return;\n+                }\n+\n+                danglingIndexAllocator.allocateDangled(List.of(indexMetaDataToImport), new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(LocalAllocateDangledIndices.AllocateDangledResponse allocateDangledResponse) {\n+                        importListener.onResponse(new ImportDanglingIndexResponse());\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        logger.debug(\"Failed to import dangling index [\" + indexMetaDataToImport.getIndexUUID() + \"]\", e);\n+                        importListener.onFailure(e);\n+                    }\n+                });\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.debug(\"Failed to find dangling index [\" + importRequest.getIndexUUID() + \"]\", e);\n+                importListener.onFailure(e);\n+            }\n+        });\n+    }\n+\n+    private void findDanglingIndex(ImportDanglingIndexRequest request, ActionListener<IndexMetaData> listener) {\n+        final String indexUUID = request.getIndexUUID();\n+\n+        this.nodeClient.execute(FindDanglingIndexAction.INSTANCE, new FindDanglingIndexRequest(indexUUID), new ActionListener<>() {\n+            @Override\n+            public void onResponse(FindDanglingIndexResponse response) {\n+                if (response.hasFailures()) {\n+                    for (FailedNodeException failure : response.failures()) {\n+                        logger.error(\"Failed to query \" + failure.nodeId(), failure);\n+                    }\n+\n+                    listener.onFailure(\n+                        new ElasticsearchException(\n+                            \"Failed to query nodes: \" + CollectionUtils.map(response.failures(), FailedNodeException::nodeId)\n+                        )\n+                    );\n+                    return;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 118}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI3MDg2Mw==", "bodyText": "I think we can treat this action as internal and not expose it in the client. If we are going to expose it, it should be called findDanglingIndex.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385270863", "createdAt": "2020-02-27T17:49:00Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/client/ClusterAdminClient.java", "diffHunk": "@@ -718,4 +726,44 @@\n      * Get a script from the cluster state\n      */\n     ActionFuture<GetStoredScriptResponse> getStoredScript(GetStoredScriptRequest request);\n+\n+    /**\n+     * List dangling indices on all nodes.\n+     */\n+    void listDanglingIndices(ListDanglingIndicesRequest request, ActionListener<ListDanglingIndicesResponse> listener);\n+\n+    /**\n+     * List dangling indices on all nodes.\n+     */\n+    ActionFuture<ListDanglingIndicesResponse> listDanglingIndices(ListDanglingIndicesRequest request);\n+\n+    /**\n+     * Find dangling indices on all nodes.\n+     */\n+    void findDanglingIndices(FindDanglingIndexRequest request, ActionListener<FindDanglingIndexResponse> listener);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI3MTA0Mg==", "bodyText": "I think we can treat this action as internal and not expose it in the client. If we are going to expose it, it should be called findDanglingIndex.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385271042", "createdAt": "2020-02-27T17:49:19Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/client/ClusterAdminClient.java", "diffHunk": "@@ -718,4 +726,44 @@\n      * Get a script from the cluster state\n      */\n     ActionFuture<GetStoredScriptResponse> getStoredScript(GetStoredScriptRequest request);\n+\n+    /**\n+     * List dangling indices on all nodes.\n+     */\n+    void listDanglingIndices(ListDanglingIndicesRequest request, ActionListener<ListDanglingIndicesResponse> listener);\n+\n+    /**\n+     * List dangling indices on all nodes.\n+     */\n+    ActionFuture<ListDanglingIndicesResponse> listDanglingIndices(ListDanglingIndicesRequest request);\n+\n+    /**\n+     * Find dangling indices on all nodes.\n+     */\n+    void findDanglingIndices(FindDanglingIndexRequest request, ActionListener<FindDanglingIndexResponse> listener);\n+\n+    /**\n+     * Find dangling indices on all nodes.\n+     */\n+    ActionFuture<FindDanglingIndexResponse> findDanglingIndices(FindDanglingIndexRequest request);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI3MjQ0Nw==", "bodyText": "I don't think we need this - Objects.requireNonNull is enough IMO, an empty UUID string is kinda valid (although it doesn't match anything).", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385272447", "createdAt": "2020-02-27T17:51:50Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/common/Strings.java", "diffHunk": "@@ -853,4 +853,41 @@ public static String padStart(String s, int minimumLength, char c) {\n             return sb.toString();\n         }\n     }\n+\n+    /**\n+     * Checks that the specified String reference is:\n+     *\n+     * <ul>\n+     * <li>not {@code null} and throws a customized {@link NullPointerException} if it is.\n+     * <li>not empty (i.e. has zero length) and throws an  {@link IllegalArgumentException} if it is.\n+     * </ul>\n+     *\n+     * This method is designed primarily for doing parameter validation in methods and\n+     * constructors with multiple parameters, as demonstrated below:\n+     *\n+     * <blockquote><pre>\n+     * public Foo(String bar, String baz) {\n+     *     this.bar = Strings.requireNonEmpty(bar, \"bar must not be null or empty\");\n+     *     this.baz = Strings.requireNonEmpty(baz, \"baz must not be null or empty\");\n+     * }\n+     * </pre></blockquote>\n+     *\n+     * @param obj     the String reference to check for nullity or zero-length\n+     * @param message detail message to be used in the event that an exception is thrown\n+     * @return {@code obj} if not {@code null} or zero-length\n+     * @throws NullPointerException if {@code obj} is {@code null}\n+     * @throws IllegalArgumentException if {@code obj.isEmpty()} returns {@code true}\n+     */\n+    public static String requireNonEmpty(String obj, String message) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI3MzY4MA==", "bodyText": "If we're here then we're answering a query from the user about dangling indices, so I think we don't need to log anything additional.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385273680", "createdAt": "2020-02-27T17:54:07Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java", "diffHunk": "@@ -161,11 +192,21 @@ void findNewAndAddDanglingIndices(final MetaData metaData) {\n                                 \"index tombstones.  This situation is likely caused by copying over the data directory for an index \" +\n                                 \"that was previously deleted.\", indexMetaData.getIndex());\n                 } else {\n-                    logger.info(\"[{}] dangling index exists on local file system, but not in cluster metadata, \" +\n-                                \"auto import to cluster state\", indexMetaData.getIndex());\n+                    if (this.isAutoImportDanglingIndicesEnabled) {\n+                        logger.info(\n+                            \"[{}] dangling index exists on local file system, but not in cluster metadata, auto import to cluster state\",\n+                            indexMetaData.getIndex()\n+                        );\n+                    } else {\n+                        logger.info(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 141}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI3NDMxNA==", "bodyText": "I think this condition is constant (we assert this.isAutoImportDanglingIndicesEnabled in the only production caller of this method.)", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385274314", "createdAt": "2020-02-27T17:55:22Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java", "diffHunk": "@@ -188,15 +229,21 @@ private IndexMetaData stripAliases(IndexMetaData indexMetaData) {\n     }\n \n     /**\n-     * Allocates the provided list of the dangled indices by sending them to the master node\n-     * for allocation.\n+     * Allocates the detected list of dangling indices by sending them to the master node\n+     * for allocation, provided auto-import is enabled via the\n+     * {@link #AUTO_IMPORT_DANGLING_INDICES_SETTING} setting.\n      */\n     void allocateDanglingIndices() {\n+        if (this.isAutoImportDanglingIndicesEnabled == false) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 164}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI3NDU5NA==", "bodyText": "Suggest reverting this -- one fewer file in the stats :)", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385274594", "createdAt": "2020-02-27T17:55:55Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/gateway/LocalAllocateDangledIndices.java", "diffHunk": "@@ -147,7 +147,8 @@ public ClusterState execute(ClusterState currentState) {\n                             logger.warn(() -> new ParameterizedMessage(\"found dangled index [{}] on node [{}]. This index cannot be \" +\n                                 \"upgraded to the latest version, adding as closed\", indexMetaData.getIndex(), request.fromNode), ex);\n                             upgradedIndexMetaData = IndexMetaData.builder(indexMetaData).state(IndexMetaData.State.CLOSE)\n-                                .version(indexMetaData.getVersion() + 1).build();\n+                                .version(indexMetaData.getVersion() + 1)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI3NTg5MA==", "bodyText": "I'd sorta prefer not to add these simple utility methods. They're not saving that much noise in their callers.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385275890", "createdAt": "2020-02-27T17:58:13Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/common/util/CollectionUtils.java", "diffHunk": "@@ -348,4 +350,80 @@ public static int sortAndDedup(final BytesRefArray bytes, final int[] indices) {\n \n         return result;\n     }\n+\n+    /**\n+     * Utility method that removes some of the boilerplate around streams and mapping. Instead of writing e.g.\n+     *\n+     * <pre>\n+     * List&lt;String&gt; ids = indexMetaData\n+     *   .stream()\n+     *   .map(each -&gt; each.getIndexUUID())\n+     *   .collect(Collectors.toList());\n+     * </pre>\n+     *\n+     * You can instead write:\n+     *\n+     * <pre>\n+     * List&lt;String&gt; ids = map(indexMetaData, each -&gt; each.getIndexUUID());\n+     * </pre>\n+     *\n+     * @param list the list to map\n+     * @param mapper a mapping function to apply to each element of the list\n+     * @param <T1> the type that the supplied list contains\n+     * @param <T2> the type that the returned list contains\n+     * @return a list containing the result of applying the <code>mapper</code> function to each element in <code>list</code>.\n+     */\n+    public static <T1, T2> List<T2> map(List<T1> list, Function<T1, T2> mapper) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 45}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY2MzYyODgx", "url": "https://github.com/elastic/elasticsearch/pull/50920#pullrequestreview-366362881", "createdAt": "2020-02-28T12:29:07Z", "commit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yOFQxMjoyOTowN1rOFvzX-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yOFQxMjoyOTowN1rOFvzX-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTY2OTExNA==", "bodyText": "Can we avoid sending the full index metadata here? This object is potentially very big, and we are only interested in two small fields from that object.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385669114", "createdAt": "2020-02-28T12:29:07Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/NodeFindDanglingIndexResponse.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.support.nodes.BaseNodeResponse;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * Used when querying every node in the cluster for a specific dangling index.\n+ */\n+public class NodeFindDanglingIndexResponse extends BaseNodeResponse {\n+    /**\n+     * A node could report several dangling indices. This class will contain them all.\n+     * A single node could even multiple different index versions for the same index\n+     * UUID if the situation is really crazy, though perhaps this is more likely\n+     * when collating responses from different nodes.\n+     */\n+    private final List<IndexMetaData> danglingIndexMetaData;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 41}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY2MzY0MjEz", "url": "https://github.com/elastic/elasticsearch/pull/50920#pullrequestreview-366364213", "createdAt": "2020-02-28T12:31:44Z", "commit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yOFQxMjozMTo0NFrOFvzcNQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yOFQxMjo0NDowMVrOFvzwfw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTY3MDE5Nw==", "bodyText": "do we need to double-check here that the index does not exist as proper index in the cluster state?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385670197", "createdAt": "2020-02-28T12:31:44Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.client.node.NodeClient;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.Index;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, DeleteDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+    private final NodeClient nodeClient;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings,\n+        NodeClient nodeClient\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+        this.nodeClient = nodeClient;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.GENERIC;\n+    }\n+\n+    @Override\n+    protected DeleteDanglingIndexResponse read(StreamInput in) throws IOException {\n+        return new DeleteDanglingIndexResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest deleteRequest,\n+        ClusterState state,\n+        ActionListener<DeleteDanglingIndexResponse> deleteListener\n+    ) throws Exception {\n+        findDanglingIndex(deleteRequest.getIndexUUID(), new ActionListener<>() {\n+\n+            @Override\n+            public void onResponse(Index indexToDelete) {\n+                // This flag is checked at this point so that we always check that the supplied index ID\n+                // does correspond to a dangling index.\n+                if (deleteRequest.isAcceptDataLoss() == false) {\n+                    deleteListener.onFailure(new IllegalArgumentException(\"accept_data_loss must be set to true\"));\n+                    return;\n+                }\n+\n+                String indexName = indexToDelete.getName();\n+\n+                final ActionListener<DeleteDanglingIndexResponse> clusterStateUpdatedListener = new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(DeleteDanglingIndexResponse response) {\n+                        deleteListener.onResponse(response);\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        logger.debug(\"Failed to delete dangling index [{}]\" + indexName, e);\n+                        deleteListener.onFailure(e);\n+                    }\n+                };\n+\n+                clusterService.submitStateUpdateTask(\n+                    \"delete-dangling-index \" + indexName,\n+                    new AckedClusterStateUpdateTask<>(deleteRequest, clusterStateUpdatedListener) {\n+\n+                        @Override\n+                        protected DeleteDanglingIndexResponse newResponse(boolean acknowledged) {\n+                            return new DeleteDanglingIndexResponse(acknowledged);\n+                        }\n+\n+                        @Override\n+                        public ClusterState execute(final ClusterState currentState) {\n+                            return deleteDanglingIndex(currentState, indexToDelete);\n+                        }\n+                    }\n+                );\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.debug(\"Failed to list dangling indices\", e);\n+                deleteListener.onFailure(e);\n+            }\n+        });\n+    }\n+\n+    private ClusterState deleteDanglingIndex(ClusterState currentState, Index indexToDelete) {\n+        final MetaData meta = currentState.metaData();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 153}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTY3NTM5MQ==", "bodyText": "By just adding the index to the graveyard, how is that making the nodes actually delete the index when applying the CS update? I might be missing something here but can't find the logic for it.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385675391", "createdAt": "2020-02-28T12:44:01Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.client.node.NodeClient;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.Index;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, DeleteDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+    private final NodeClient nodeClient;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings,\n+        NodeClient nodeClient\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+        this.nodeClient = nodeClient;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.GENERIC;\n+    }\n+\n+    @Override\n+    protected DeleteDanglingIndexResponse read(StreamInput in) throws IOException {\n+        return new DeleteDanglingIndexResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest deleteRequest,\n+        ClusterState state,\n+        ActionListener<DeleteDanglingIndexResponse> deleteListener\n+    ) throws Exception {\n+        findDanglingIndex(deleteRequest.getIndexUUID(), new ActionListener<>() {\n+\n+            @Override\n+            public void onResponse(Index indexToDelete) {\n+                // This flag is checked at this point so that we always check that the supplied index ID\n+                // does correspond to a dangling index.\n+                if (deleteRequest.isAcceptDataLoss() == false) {\n+                    deleteListener.onFailure(new IllegalArgumentException(\"accept_data_loss must be set to true\"));\n+                    return;\n+                }\n+\n+                String indexName = indexToDelete.getName();\n+\n+                final ActionListener<DeleteDanglingIndexResponse> clusterStateUpdatedListener = new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(DeleteDanglingIndexResponse response) {\n+                        deleteListener.onResponse(response);\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        logger.debug(\"Failed to delete dangling index [{}]\" + indexName, e);\n+                        deleteListener.onFailure(e);\n+                    }\n+                };\n+\n+                clusterService.submitStateUpdateTask(\n+                    \"delete-dangling-index \" + indexName,\n+                    new AckedClusterStateUpdateTask<>(deleteRequest, clusterStateUpdatedListener) {\n+\n+                        @Override\n+                        protected DeleteDanglingIndexResponse newResponse(boolean acknowledged) {\n+                            return new DeleteDanglingIndexResponse(acknowledged);\n+                        }\n+\n+                        @Override\n+                        public ClusterState execute(final ClusterState currentState) {\n+                            return deleteDanglingIndex(currentState, indexToDelete);\n+                        }\n+                    }\n+                );\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.debug(\"Failed to list dangling indices\", e);\n+                deleteListener.onFailure(e);\n+            }\n+        });\n+    }\n+\n+    private ClusterState deleteDanglingIndex(ClusterState currentState, Index indexToDelete) {\n+        final MetaData meta = currentState.metaData();\n+\n+        MetaData.Builder metaDataBuilder = MetaData.builder(meta);\n+\n+        final IndexGraveyard.Builder graveyardBuilder = IndexGraveyard.builder(metaDataBuilder.indexGraveyard());\n+\n+        final IndexGraveyard newGraveyard = graveyardBuilder.addTombstone(indexToDelete).build(settings);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 159}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "47468de1c6c7ceb12911e97bf551cbe58c4b6bed", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/47468de1c6c7ceb12911e97bf551cbe58c4b6bed", "committedDate": "2020-02-28T14:27:54Z", "message": "Test refactoring"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d979b971122c0f9ac380d760f748f9322e99ef00", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/d979b971122c0f9ac380d760f748f9322e99ef00", "committedDate": "2020-02-28T14:54:18Z", "message": "Static import"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dcb24746faa2098cc02ab4282bf833e6d91cc71d", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/dcb24746faa2098cc02ab4282bf833e6d91cc71d", "committedDate": "2020-02-28T15:20:47Z", "message": "More test refactoring"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "79b8df486c7c61304b842a0a1dab2375d09787ce", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/79b8df486c7c61304b842a0a1dab2375d09787ce", "committedDate": "2020-02-28T15:25:23Z", "message": "Expect dangling index allocation to be immediate"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "80021f9951949921afd47318dded9acb66076faa", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/80021f9951949921afd47318dded9acb66076faa", "committedDate": "2020-02-28T16:00:00Z", "message": "Check dangling index UUID in REST test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a0b29bdc82e72c0131f34ff3e6cbaba758fbb1f4", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/a0b29bdc82e72c0131f34ff3e6cbaba758fbb1f4", "committedDate": "2020-02-28T16:11:00Z", "message": "Rename variables"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "33f027db082f03301eb7aa05b79f965283c36b93", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/33f027db082f03301eb7aa05b79f965283c36b93", "committedDate": "2020-02-28T16:50:18Z", "message": "Use AcknowledgedResponse where possible"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1b09baf04c2f9788d437523c2fcd2619a2c58984", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/1b09baf04c2f9788d437523c2fcd2619a2c58984", "committedDate": "2020-02-28T20:48:43Z", "message": "Simplify DanglingIndexInfo"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ed427c09e0da97fce1e84d6df0eea24692039157", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/ed427c09e0da97fce1e84d6df0eea24692039157", "committedDate": "2020-02-28T21:03:04Z", "message": "Tweaks"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6ab6a6413d8a1da283dd6198a2853b4986ef8642", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/6ab6a6413d8a1da283dd6198a2853b4986ef8642", "committedDate": "2020-03-01T14:03:34Z", "message": "Don't pass around IndexMetaData for deletes\n\nRefactor dangling index classes into sub-packages. Rework the find\naction to return a list of `DanglingIndexInfo`. Introduce a new find\naction that still returns `IndexMetaData` since importing a dangling\nindex requires this."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "02ec404241b0fe223316a7c011a1b94ee3b09f13", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/02ec404241b0fe223316a7c011a1b94ee3b09f13", "committedDate": "2020-03-01T14:22:26Z", "message": "Add optional filter to list requests\n\nMake it possible to filter list requests by index UUID, which means that\nthe find actions can be consolidated once again."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d4a083636263db8a819bedaa97ec7bfbda37e6e4", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/d4a083636263db8a819bedaa97ec7bfbda37e6e4", "committedDate": "2020-03-01T14:36:34Z", "message": "Add check for index UUID in cluster state\n\nBefore performing a dangling index delete, first check that the dangling\nindex UUID doesn't exist in the cluster state."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cfc7772560595a75f7aa8ed899c80a6e7acd584f", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/cfc7772560595a75f7aa8ed899c80a6e7acd584f", "committedDate": "2020-03-02T09:39:48Z", "message": "Support timeout params in import and delete commands"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e358ca51769891f398cea3daa9d4a9c9ea43680d", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/e358ca51769891f398cea3daa9d4a9c9ea43680d", "committedDate": "2020-03-02T13:28:21Z", "message": "Change import response type again\n\nUse a tailored response type for importing a danging index, but include\n\"accepted\" in the response payload rather than \"acknowledged\".\n\nAlso add unit tests for the list action's aggregation logic."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5a0f0ef1e5b64e3e1e4f8ee94d475fe661199124", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/5a0f0ef1e5b64e3e1e4f8ee94d475fe661199124", "committedDate": "2020-03-02T14:00:13Z", "message": "Tweaks"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0e7903274971f0cdcaabd981bf32a25d097168e0", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/0e7903274971f0cdcaabd981bf32a25d097168e0", "committedDate": "2020-03-02T14:00:30Z", "message": "Remove find dangling index from cluster admin client"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dc38fbcf2cd2cc0b4d87cbb41fc9e61965720eba", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/dc38fbcf2cd2cc0b4d87cbb41fc9e61965720eba", "committedDate": "2020-03-03T12:20:13Z", "message": "Checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d36ea4f517e5dadfce60b708c0a08e2b07956928", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/d36ea4f517e5dadfce60b708c0a08e2b07956928", "committedDate": "2020-03-03T14:04:08Z", "message": "Fix DanglingIndicesStateTests\n\nReplace a test, which was checking allocation behaviour when dangling\nindices were detected, with a test that checks that no cluster state\nlistener is registered when the detection setting is disabled."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ef7e6b16c8ecb894449feb7855ea807d0421ae6b", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/ef7e6b16c8ecb894449feb7855ea807d0421ae6b", "committedDate": "2020-03-23T14:06:51Z", "message": "Merge remote-tracking branch 'upstream/master' into 48366-dangling-indices-api"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "da2e7cc57ba2447bda8b0cc5b8177c2032fb16b7", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/da2e7cc57ba2447bda8b0cc5b8177c2032fb16b7", "committedDate": "2020-03-23T14:18:18Z", "message": "Fix checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5ce7f09b55fa5c518531347a0e73b13755f0d5cc", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/5ce7f09b55fa5c518531347a0e73b13755f0d5cc", "committedDate": "2020-04-08T13:06:44Z", "message": "Merge remote-tracking branch 'upstream/master' into 48366-dangling-indices-api"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk1NDMyMTYz", "url": "https://github.com/elastic/elasticsearch/pull/50920#pullrequestreview-395432163", "createdAt": "2020-04-17T13:01:47Z", "commit": {"oid": "5ce7f09b55fa5c518531347a0e73b13755f0d5cc"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QxMzowMTo0N1rOGHNA7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QxMzo1NzowM1rOGHPFMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDIwNjQ0Nw==", "bodyText": "Still outstanding -- let's remove this and just use Objects.requireNonNull.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r410206447", "createdAt": "2020-04-17T13:01:47Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/common/Strings.java", "diffHunk": "@@ -853,4 +853,41 @@ public static String padStart(String s, int minimumLength, char c) {\n             return sb.toString();\n         }\n     }\n+\n+    /**\n+     * Checks that the specified String reference is:\n+     *\n+     * <ul>\n+     * <li>not {@code null} and throws a customized {@link NullPointerException} if it is.\n+     * <li>not empty (i.e. has zero length) and throws an  {@link IllegalArgumentException} if it is.\n+     * </ul>\n+     *\n+     * This method is designed primarily for doing parameter validation in methods and\n+     * constructors with multiple parameters, as demonstrated below:\n+     *\n+     * <blockquote><pre>\n+     * public Foo(String bar, String baz) {\n+     *     this.bar = Strings.requireNonEmpty(bar, \"bar must not be null or empty\");\n+     *     this.baz = Strings.requireNonEmpty(baz, \"baz must not be null or empty\");\n+     * }\n+     * </pre></blockquote>\n+     *\n+     * @param obj     the String reference to check for nullity or zero-length\n+     * @param message detail message to be used in the event that an exception is thrown\n+     * @return {@code obj} if not {@code null} or zero-length\n+     * @throws NullPointerException if {@code obj} is {@code null}\n+     * @throws IllegalArgumentException if {@code obj.isEmpty()} returns {@code true}\n+     */\n+    public static String requireNonEmpty(String obj, String message) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI3MjQ0Nw=="}, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDIwNjY2Mw==", "bodyText": "This is still outstanding -- let's remove these and inline them at their few callers.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r410206663", "createdAt": "2020-04-17T13:02:11Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/common/util/CollectionUtils.java", "diffHunk": "@@ -348,4 +350,80 @@ public static int sortAndDedup(final BytesRefArray bytes, final int[] indices) {\n \n         return result;\n     }\n+\n+    /**\n+     * Utility method that removes some of the boilerplate around streams and mapping. Instead of writing e.g.\n+     *\n+     * <pre>\n+     * List&lt;String&gt; ids = indexMetaData\n+     *   .stream()\n+     *   .map(each -&gt; each.getIndexUUID())\n+     *   .collect(Collectors.toList());\n+     * </pre>\n+     *\n+     * You can instead write:\n+     *\n+     * <pre>\n+     * List&lt;String&gt; ids = map(indexMetaData, each -&gt; each.getIndexUUID());\n+     * </pre>\n+     *\n+     * @param list the list to map\n+     * @param mapper a mapping function to apply to each element of the list\n+     * @param <T1> the type that the supplied list contains\n+     * @param <T2> the type that the returned list contains\n+     * @return a list containing the result of applying the <code>mapper</code> function to each element in <code>list</code>.\n+     */\n+    public static <T1, T2> List<T2> map(List<T1> list, Function<T1, T2> mapper) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI3NTg5MA=="}, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDIwNjk3NQ==", "bodyText": "This is still outstanding -- let's not log things here.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r410206975", "createdAt": "2020-04-17T13:02:46Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java", "diffHunk": "@@ -161,11 +192,21 @@ void findNewAndAddDanglingIndices(final MetaData metaData) {\n                                 \"index tombstones.  This situation is likely caused by copying over the data directory for an index \" +\n                                 \"that was previously deleted.\", indexMetaData.getIndex());\n                 } else {\n-                    logger.info(\"[{}] dangling index exists on local file system, but not in cluster metadata, \" +\n-                                \"auto import to cluster state\", indexMetaData.getIndex());\n+                    if (this.isAutoImportDanglingIndicesEnabled) {\n+                        logger.info(\n+                            \"[{}] dangling index exists on local file system, but not in cluster metadata, auto import to cluster state\",\n+                            indexMetaData.getIndex()\n+                        );\n+                    } else {\n+                        logger.info(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI3MzY4MA=="}, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 141}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDIyMDMzNA==", "bodyText": "I think we also need to set index.routing.allocation.total_shards_per_node: 1 to ensure that every node gets a copy of the shard.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r410220334", "createdAt": "2020-04-17T13:26:00Z", "author": {"login": "DaveCTurner"}, "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "diffHunk": "@@ -0,0 +1,291 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http;\n+\n+import org.elasticsearch.client.Request;\n+import org.elasticsearch.client.Response;\n+import org.elasticsearch.client.RestClient;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.XContentTestUtils;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.elasticsearch.cluster.metadata.IndexGraveyard.SETTING_MAX_TOMBSTONES;\n+import static org.elasticsearch.gateway.DanglingIndicesState.AUTO_IMPORT_DANGLING_INDICES_SETTING;\n+import static org.elasticsearch.indices.IndicesService.WRITE_DANGLING_INDICES_INFO_SETTING;\n+import static org.elasticsearch.rest.RestStatus.ACCEPTED;\n+import static org.elasticsearch.rest.RestStatus.OK;\n+import static org.elasticsearch.test.XContentTestUtils.createJsonMapView;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+\n+/**\n+ * This class tests the dangling indices REST API.  These tests are here\n+ * today so they have access to a proper REST client. They cannot be in\n+ * :server:integTest since the REST client needs a proper transport\n+ * implementation, and they cannot be REST tests today since they need to\n+ * restart nodes. Really, though, this test should live elsewhere.\n+ *\n+ * @see org.elasticsearch.action.admin.indices.dangling\n+ */\n+@ClusterScope(numDataNodes = 0, scope = ESIntegTestCase.Scope.TEST, autoManageMasterNodes = false)\n+public class DanglingIndicesRestIT extends HttpSmokeTestCase {\n+    private static final String INDEX_NAME = \"test-idx-1\";\n+    private static final String OTHER_INDEX_NAME = INDEX_NAME + \"-other\";\n+\n+    private Settings buildSettings(int maxTombstones) {\n+        return Settings.builder()\n+            // Limit the indices kept in the graveyard. This can be set to zero, so that\n+            // when we delete an index, it's definitely considered to be dangling.\n+            .put(SETTING_MAX_TOMBSTONES.getKey(), maxTombstones)\n+            .put(WRITE_DANGLING_INDICES_INFO_SETTING.getKey(), true)\n+            .put(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey(), false)\n+            .build();\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed via the REST API.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        final DanglingIndexDetails danglingIndexDetails = createDanglingIndices(INDEX_NAME);\n+        final String stoppedNodeId = mapNodeNameToId(danglingIndexDetails.stoppedNodeName);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final Response listResponse = restClient.performRequest(new Request(\"GET\", \"/_dangling\"));\n+        assertOK(listResponse);\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(listResponse.getEntity().getContent());\n+\n+        assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+        List<Object> indices = mapView.get(\"dangling_indices\");\n+        assertThat(indices, hasSize(1));\n+\n+        assertThat(mapView.get(\"dangling_indices.0.index_name\"), equalTo(INDEX_NAME));\n+        assertThat(mapView.get(\"dangling_indices.0.index_uuid\"), equalTo(danglingIndexDetails.indexToUUID.get(INDEX_NAME)));\n+        assertThat(mapView.get(\"dangling_indices.0.creation_date_millis\"), instanceOf(Long.class));\n+        assertThat(mapView.get(\"dangling_indices.0.node_ids.0\"), equalTo(stoppedNodeId));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be imported.\n+     */\n+    public void testDanglingIndicesCanBeImported() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        createDanglingIndices(INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final List<String> danglingIndexIds = listDanglingIndexIds();\n+        assertThat(danglingIndexIds, hasSize(1));\n+\n+        final Request importRequest = new Request(\"POST\", \"/_dangling/\" + danglingIndexIds.get(0));\n+        importRequest.addParameter(\"accept_data_loss\", \"true\");\n+        // Ensure this parameter is accepted\n+        importRequest.addParameter(\"timeout\", \"20s\");\n+        final Response importResponse = restClient.performRequest(importRequest);\n+        assertThat(importResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(importResponse.getEntity().getContent());\n+        assertThat(mapView.get(\"accepted\"), equalTo(true));\n+\n+        assertTrue(\"Expected dangling index \" + INDEX_NAME + \" to be recovered\", indexExists(INDEX_NAME));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be deleted. Since this requires that\n+     * we add an entry to the index graveyard, the graveyard size must be\n+     * greater than 1. To test deletes, we set the index graveyard size to\n+     * 1, then create two indices and delete them both while one node in\n+     * the cluster is stopped. The deletion of the second pushes the deletion\n+     * of the first out of the graveyard. When the stopped node is resumed,\n+     * only the second index will be found into the graveyard and the the\n+     * other will be considered dangling, and can therefore be listed and\n+     * deleted through the API\n+     */\n+    public void testDanglingIndicesCanBeDeleted() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(1));\n+\n+        createDanglingIndices(INDEX_NAME, OTHER_INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final List<String> danglingIndexIds = listDanglingIndexIds();\n+        assertThat(danglingIndexIds, hasSize(1));\n+\n+        final Request deleteRequest = new Request(\"DELETE\", \"/_dangling/\" + danglingIndexIds.get(0));\n+        deleteRequest.addParameter(\"accept_data_loss\", \"true\");\n+        // Ensure these parameters is accepted\n+        deleteRequest.addParameter(\"timeout\", \"20s\");\n+        deleteRequest.addParameter(\"master_timeout\", \"20s\");\n+        final Response deleteResponse = restClient.performRequest(deleteRequest);\n+        assertThat(deleteResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        assertBusy(() -> assertThat(\"Expected dangling index to be deleted\", listDanglingIndexIds(), hasSize(0)));\n+    }\n+\n+    private List<String> listDanglingIndexIds() throws IOException {\n+        final Response response = getRestClient().performRequest(new Request(\"GET\", \"/_dangling\"));\n+        assertOK(response);\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(response.getEntity().getContent());\n+\n+        assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+        List<Object> indices = mapView.get(\"dangling_indices\");\n+\n+        List<String> danglingIndexIds = new ArrayList<>();\n+\n+        for (int i = 0; i < indices.size(); i++) {\n+            danglingIndexIds.add(mapView.get(\"dangling_indices.\" + i + \".index_uuid\"));\n+        }\n+\n+        return danglingIndexIds;\n+    }\n+\n+    private void assertOK(Response response) {\n+        assertThat(response.getStatusLine().getStatusCode(), equalTo(OK.getStatus()));\n+    }\n+\n+    /**\n+     * Given a node name, finds the corresponding node ID.\n+     */\n+    private String mapNodeNameToId(String nodeName) throws IOException {\n+        final Response catResponse = getRestClient().performRequest(new Request(\"GET\", \"/_cat/nodes?full_id&h=id,name\"));\n+        assertOK(catResponse);\n+\n+        for (String nodeLine : Streams.readAllLines(catResponse.getEntity().getContent())) {\n+            String[] elements = nodeLine.split(\" \");\n+            if (elements[1].equals(nodeName)) {\n+                return elements[0];\n+            }\n+        }\n+\n+        throw new AssertionError(\"Failed to map node name [\" + nodeName + \"] to node ID\");\n+    }\n+\n+    /**\n+     * Helper that creates one or more indices, and importantly,\n+     * checks that they are green before proceeding. This is important\n+     * because the tests in this class stop and restart nodes, assuming\n+     * that each index has a primary or replica shard on every node, and if\n+     * a node is stopped prematurely, this assumption is broken.\n+     *\n+     * @return a mapping from each createad index name to its UUID\n+     */\n+    private Map<String, String> createIndices(String... indices) throws IOException {\n+        assert indices.length > 0;\n+        for (String index : indices) {\n+            String indexSettings = \"{\"\n+                + \"  \\\"settings\\\": {\"\n+                + \"    \\\"index\\\": {\"\n+                + \"      \\\"number_of_shards\\\": 1,\"\n+                + \"      \\\"number_of_replicas\\\": 2\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5ce7f09b55fa5c518531347a0e73b13755f0d5cc"}, "originalPosition": 222}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDIyMTE2Nw==", "bodyText": "Are you sure that the dangling index metadata was written to disk by this point? I think it happens asynchronously now.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r410221167", "createdAt": "2020-04-17T13:27:22Z", "author": {"login": "DaveCTurner"}, "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "diffHunk": "@@ -0,0 +1,291 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http;\n+\n+import org.elasticsearch.client.Request;\n+import org.elasticsearch.client.Response;\n+import org.elasticsearch.client.RestClient;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.XContentTestUtils;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.elasticsearch.cluster.metadata.IndexGraveyard.SETTING_MAX_TOMBSTONES;\n+import static org.elasticsearch.gateway.DanglingIndicesState.AUTO_IMPORT_DANGLING_INDICES_SETTING;\n+import static org.elasticsearch.indices.IndicesService.WRITE_DANGLING_INDICES_INFO_SETTING;\n+import static org.elasticsearch.rest.RestStatus.ACCEPTED;\n+import static org.elasticsearch.rest.RestStatus.OK;\n+import static org.elasticsearch.test.XContentTestUtils.createJsonMapView;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+\n+/**\n+ * This class tests the dangling indices REST API.  These tests are here\n+ * today so they have access to a proper REST client. They cannot be in\n+ * :server:integTest since the REST client needs a proper transport\n+ * implementation, and they cannot be REST tests today since they need to\n+ * restart nodes. Really, though, this test should live elsewhere.\n+ *\n+ * @see org.elasticsearch.action.admin.indices.dangling\n+ */\n+@ClusterScope(numDataNodes = 0, scope = ESIntegTestCase.Scope.TEST, autoManageMasterNodes = false)\n+public class DanglingIndicesRestIT extends HttpSmokeTestCase {\n+    private static final String INDEX_NAME = \"test-idx-1\";\n+    private static final String OTHER_INDEX_NAME = INDEX_NAME + \"-other\";\n+\n+    private Settings buildSettings(int maxTombstones) {\n+        return Settings.builder()\n+            // Limit the indices kept in the graveyard. This can be set to zero, so that\n+            // when we delete an index, it's definitely considered to be dangling.\n+            .put(SETTING_MAX_TOMBSTONES.getKey(), maxTombstones)\n+            .put(WRITE_DANGLING_INDICES_INFO_SETTING.getKey(), true)\n+            .put(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey(), false)\n+            .build();\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed via the REST API.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        final DanglingIndexDetails danglingIndexDetails = createDanglingIndices(INDEX_NAME);\n+        final String stoppedNodeId = mapNodeNameToId(danglingIndexDetails.stoppedNodeName);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final Response listResponse = restClient.performRequest(new Request(\"GET\", \"/_dangling\"));\n+        assertOK(listResponse);\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(listResponse.getEntity().getContent());\n+\n+        assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+        List<Object> indices = mapView.get(\"dangling_indices\");\n+        assertThat(indices, hasSize(1));\n+\n+        assertThat(mapView.get(\"dangling_indices.0.index_name\"), equalTo(INDEX_NAME));\n+        assertThat(mapView.get(\"dangling_indices.0.index_uuid\"), equalTo(danglingIndexDetails.indexToUUID.get(INDEX_NAME)));\n+        assertThat(mapView.get(\"dangling_indices.0.creation_date_millis\"), instanceOf(Long.class));\n+        assertThat(mapView.get(\"dangling_indices.0.node_ids.0\"), equalTo(stoppedNodeId));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be imported.\n+     */\n+    public void testDanglingIndicesCanBeImported() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        createDanglingIndices(INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final List<String> danglingIndexIds = listDanglingIndexIds();\n+        assertThat(danglingIndexIds, hasSize(1));\n+\n+        final Request importRequest = new Request(\"POST\", \"/_dangling/\" + danglingIndexIds.get(0));\n+        importRequest.addParameter(\"accept_data_loss\", \"true\");\n+        // Ensure this parameter is accepted\n+        importRequest.addParameter(\"timeout\", \"20s\");\n+        final Response importResponse = restClient.performRequest(importRequest);\n+        assertThat(importResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(importResponse.getEntity().getContent());\n+        assertThat(mapView.get(\"accepted\"), equalTo(true));\n+\n+        assertTrue(\"Expected dangling index \" + INDEX_NAME + \" to be recovered\", indexExists(INDEX_NAME));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be deleted. Since this requires that\n+     * we add an entry to the index graveyard, the graveyard size must be\n+     * greater than 1. To test deletes, we set the index graveyard size to\n+     * 1, then create two indices and delete them both while one node in\n+     * the cluster is stopped. The deletion of the second pushes the deletion\n+     * of the first out of the graveyard. When the stopped node is resumed,\n+     * only the second index will be found into the graveyard and the the\n+     * other will be considered dangling, and can therefore be listed and\n+     * deleted through the API\n+     */\n+    public void testDanglingIndicesCanBeDeleted() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(1));\n+\n+        createDanglingIndices(INDEX_NAME, OTHER_INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final List<String> danglingIndexIds = listDanglingIndexIds();\n+        assertThat(danglingIndexIds, hasSize(1));\n+\n+        final Request deleteRequest = new Request(\"DELETE\", \"/_dangling/\" + danglingIndexIds.get(0));\n+        deleteRequest.addParameter(\"accept_data_loss\", \"true\");\n+        // Ensure these parameters is accepted\n+        deleteRequest.addParameter(\"timeout\", \"20s\");\n+        deleteRequest.addParameter(\"master_timeout\", \"20s\");\n+        final Response deleteResponse = restClient.performRequest(deleteRequest);\n+        assertThat(deleteResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        assertBusy(() -> assertThat(\"Expected dangling index to be deleted\", listDanglingIndexIds(), hasSize(0)));\n+    }\n+\n+    private List<String> listDanglingIndexIds() throws IOException {\n+        final Response response = getRestClient().performRequest(new Request(\"GET\", \"/_dangling\"));\n+        assertOK(response);\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(response.getEntity().getContent());\n+\n+        assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+        List<Object> indices = mapView.get(\"dangling_indices\");\n+\n+        List<String> danglingIndexIds = new ArrayList<>();\n+\n+        for (int i = 0; i < indices.size(); i++) {\n+            danglingIndexIds.add(mapView.get(\"dangling_indices.\" + i + \".index_uuid\"));\n+        }\n+\n+        return danglingIndexIds;\n+    }\n+\n+    private void assertOK(Response response) {\n+        assertThat(response.getStatusLine().getStatusCode(), equalTo(OK.getStatus()));\n+    }\n+\n+    /**\n+     * Given a node name, finds the corresponding node ID.\n+     */\n+    private String mapNodeNameToId(String nodeName) throws IOException {\n+        final Response catResponse = getRestClient().performRequest(new Request(\"GET\", \"/_cat/nodes?full_id&h=id,name\"));\n+        assertOK(catResponse);\n+\n+        for (String nodeLine : Streams.readAllLines(catResponse.getEntity().getContent())) {\n+            String[] elements = nodeLine.split(\" \");\n+            if (elements[1].equals(nodeName)) {\n+                return elements[0];\n+            }\n+        }\n+\n+        throw new AssertionError(\"Failed to map node name [\" + nodeName + \"] to node ID\");\n+    }\n+\n+    /**\n+     * Helper that creates one or more indices, and importantly,\n+     * checks that they are green before proceeding. This is important\n+     * because the tests in this class stop and restart nodes, assuming\n+     * that each index has a primary or replica shard on every node, and if\n+     * a node is stopped prematurely, this assumption is broken.\n+     *\n+     * @return a mapping from each createad index name to its UUID\n+     */\n+    private Map<String, String> createIndices(String... indices) throws IOException {\n+        assert indices.length > 0;\n+        for (String index : indices) {\n+            String indexSettings = \"{\"\n+                + \"  \\\"settings\\\": {\"\n+                + \"    \\\"index\\\": {\"\n+                + \"      \\\"number_of_shards\\\": 1,\"\n+                + \"      \\\"number_of_replicas\\\": 2\"\n+                + \"    }\"\n+                + \"  }\"\n+                + \"}\";\n+            Request request = new Request(\"PUT\", \"/\" + index);\n+            request.setJsonEntity(indexSettings);\n+            final Response response = getRestClient().performRequest(request);\n+            assertOK(response);\n+        }\n+        ensureGreen(indices);\n+\n+        final Response catResponse = getRestClient().performRequest(new Request(\"GET\", \"/_cat/indices?h=index,uuid\"));\n+        assertOK(catResponse);\n+\n+        final Map<String, String> createdIndexIDs = new HashMap<>();\n+\n+        final List<String> indicesAsList = Arrays.asList(indices);\n+\n+        for (String indexLine : Streams.readAllLines(catResponse.getEntity().getContent())) {\n+            String[] elements = indexLine.split(\" +\");\n+            if (indicesAsList.contains(elements[0])) {\n+                createdIndexIDs.put(elements[0], elements[1]);\n+            }\n+        }\n+\n+        assertThat(\"Expected to find as many index UUIDs as created indices\", createdIndexIDs.size(), equalTo(indices.length));\n+\n+        return createdIndexIDs;\n+    }\n+\n+    private void deleteIndex(String indexName) throws IOException {\n+        Response deleteResponse = getRestClient().performRequest(new Request(\"DELETE\", \"/\" + indexName));\n+        assertOK(deleteResponse);\n+    }\n+\n+    private DanglingIndexDetails createDanglingIndices(String... indices) throws Exception {\n+        ensureStableCluster(3);\n+        final Map<String, String> indexToUUID = createIndices(indices);\n+\n+        final AtomicReference<String> stoppedNodeName = new AtomicReference<>();\n+\n+        // Restart node, deleting the index in its absence, so that there is a dangling index to recover\n+        internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5ce7f09b55fa5c518531347a0e73b13755f0d5cc"}, "originalPosition": 264}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDI0MDMwNw==", "bodyText": "We can demonstrate that the dangling index was not properly deleted by adding this:\n        createIndex(\"additional\");\n        deleteIndex(\"additional\");\n        assertThat(listDanglingIndexIds(), empty());\n\nThe creation/deletion of this extra index pushes the tombstone out of the graveyard which resurrects it.\n(gruesome nomenclature isn't it? \ud83d\ude01)", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r410240307", "createdAt": "2020-04-17T13:57:03Z", "author": {"login": "DaveCTurner"}, "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "diffHunk": "@@ -0,0 +1,291 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http;\n+\n+import org.elasticsearch.client.Request;\n+import org.elasticsearch.client.Response;\n+import org.elasticsearch.client.RestClient;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.XContentTestUtils;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.elasticsearch.cluster.metadata.IndexGraveyard.SETTING_MAX_TOMBSTONES;\n+import static org.elasticsearch.gateway.DanglingIndicesState.AUTO_IMPORT_DANGLING_INDICES_SETTING;\n+import static org.elasticsearch.indices.IndicesService.WRITE_DANGLING_INDICES_INFO_SETTING;\n+import static org.elasticsearch.rest.RestStatus.ACCEPTED;\n+import static org.elasticsearch.rest.RestStatus.OK;\n+import static org.elasticsearch.test.XContentTestUtils.createJsonMapView;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+\n+/**\n+ * This class tests the dangling indices REST API.  These tests are here\n+ * today so they have access to a proper REST client. They cannot be in\n+ * :server:integTest since the REST client needs a proper transport\n+ * implementation, and they cannot be REST tests today since they need to\n+ * restart nodes. Really, though, this test should live elsewhere.\n+ *\n+ * @see org.elasticsearch.action.admin.indices.dangling\n+ */\n+@ClusterScope(numDataNodes = 0, scope = ESIntegTestCase.Scope.TEST, autoManageMasterNodes = false)\n+public class DanglingIndicesRestIT extends HttpSmokeTestCase {\n+    private static final String INDEX_NAME = \"test-idx-1\";\n+    private static final String OTHER_INDEX_NAME = INDEX_NAME + \"-other\";\n+\n+    private Settings buildSettings(int maxTombstones) {\n+        return Settings.builder()\n+            // Limit the indices kept in the graveyard. This can be set to zero, so that\n+            // when we delete an index, it's definitely considered to be dangling.\n+            .put(SETTING_MAX_TOMBSTONES.getKey(), maxTombstones)\n+            .put(WRITE_DANGLING_INDICES_INFO_SETTING.getKey(), true)\n+            .put(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey(), false)\n+            .build();\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed via the REST API.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        final DanglingIndexDetails danglingIndexDetails = createDanglingIndices(INDEX_NAME);\n+        final String stoppedNodeId = mapNodeNameToId(danglingIndexDetails.stoppedNodeName);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final Response listResponse = restClient.performRequest(new Request(\"GET\", \"/_dangling\"));\n+        assertOK(listResponse);\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(listResponse.getEntity().getContent());\n+\n+        assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+        List<Object> indices = mapView.get(\"dangling_indices\");\n+        assertThat(indices, hasSize(1));\n+\n+        assertThat(mapView.get(\"dangling_indices.0.index_name\"), equalTo(INDEX_NAME));\n+        assertThat(mapView.get(\"dangling_indices.0.index_uuid\"), equalTo(danglingIndexDetails.indexToUUID.get(INDEX_NAME)));\n+        assertThat(mapView.get(\"dangling_indices.0.creation_date_millis\"), instanceOf(Long.class));\n+        assertThat(mapView.get(\"dangling_indices.0.node_ids.0\"), equalTo(stoppedNodeId));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be imported.\n+     */\n+    public void testDanglingIndicesCanBeImported() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        createDanglingIndices(INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final List<String> danglingIndexIds = listDanglingIndexIds();\n+        assertThat(danglingIndexIds, hasSize(1));\n+\n+        final Request importRequest = new Request(\"POST\", \"/_dangling/\" + danglingIndexIds.get(0));\n+        importRequest.addParameter(\"accept_data_loss\", \"true\");\n+        // Ensure this parameter is accepted\n+        importRequest.addParameter(\"timeout\", \"20s\");\n+        final Response importResponse = restClient.performRequest(importRequest);\n+        assertThat(importResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(importResponse.getEntity().getContent());\n+        assertThat(mapView.get(\"accepted\"), equalTo(true));\n+\n+        assertTrue(\"Expected dangling index \" + INDEX_NAME + \" to be recovered\", indexExists(INDEX_NAME));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be deleted. Since this requires that\n+     * we add an entry to the index graveyard, the graveyard size must be\n+     * greater than 1. To test deletes, we set the index graveyard size to\n+     * 1, then create two indices and delete them both while one node in\n+     * the cluster is stopped. The deletion of the second pushes the deletion\n+     * of the first out of the graveyard. When the stopped node is resumed,\n+     * only the second index will be found into the graveyard and the the\n+     * other will be considered dangling, and can therefore be listed and\n+     * deleted through the API\n+     */\n+    public void testDanglingIndicesCanBeDeleted() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(1));\n+\n+        createDanglingIndices(INDEX_NAME, OTHER_INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final List<String> danglingIndexIds = listDanglingIndexIds();\n+        assertThat(danglingIndexIds, hasSize(1));\n+\n+        final Request deleteRequest = new Request(\"DELETE\", \"/_dangling/\" + danglingIndexIds.get(0));\n+        deleteRequest.addParameter(\"accept_data_loss\", \"true\");\n+        // Ensure these parameters is accepted\n+        deleteRequest.addParameter(\"timeout\", \"20s\");\n+        deleteRequest.addParameter(\"master_timeout\", \"20s\");\n+        final Response deleteResponse = restClient.performRequest(deleteRequest);\n+        assertThat(deleteResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        assertBusy(() -> assertThat(\"Expected dangling index to be deleted\", listDanglingIndexIds(), hasSize(0)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5ce7f09b55fa5c518531347a0e73b13755f0d5cc"}, "originalPosition": 161}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "30f1b37fac3da5311289f09e8937720a13d4bb9e", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/30f1b37fac3da5311289f09e8937720a13d4bb9e", "committedDate": "2020-04-20T13:05:53Z", "message": "Address review feedback"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e520f234e1b68586fbc46a033bd12ef3cda2a392", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/e520f234e1b68586fbc46a033bd12ef3cda2a392", "committedDate": "2020-04-20T13:19:15Z", "message": "Merge remote-tracking branch 'upstream/master' into 48366-dangling-indices-api"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "23acbf8e011c4d48298ec92f30dc562f648ba32a", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/23acbf8e011c4d48298ec92f30dc562f648ba32a", "committedDate": "2020-04-20T13:31:54Z", "message": "Checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d02c5ce31130f9eff92cc037eabcdadc1c378561", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/d02c5ce31130f9eff92cc037eabcdadc1c378561", "committedDate": "2020-04-21T14:00:33Z", "message": "Dangling index deletes should remove data from disk\n\nAttempt to ensure that when a dangling index is explicitly deleted, the\ndata on disk is also removed. While the end-to-end REST test now passes,\nI'm really not sure about the changes to the server."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "da9886b0a64e75db43b76e35b83111b5e33da816", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/da9886b0a64e75db43b76e35b83111b5e33da816", "committedDate": "2020-04-23T10:16:37Z", "message": "Checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b26f4c4ab48248cdae983544acef729857ea965d", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/b26f4c4ab48248cdae983544acef729857ea965d", "committedDate": "2020-04-23T10:16:50Z", "message": "Merge remote-tracking branch 'upstream/master' into 48366-dangling-indices-api"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2158d54786063585265402bfa9f20f69e4585d81", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/2158d54786063585265402bfa9f20f69e4585d81", "committedDate": "2020-04-23T10:39:46Z", "message": "Fix REST API validation failures"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b94efdf802bf2b9d8bf1a2c4abd422e0eefcb35c", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/b94efdf802bf2b9d8bf1a2c4abd422e0eefcb35c", "committedDate": "2020-04-27T12:41:56Z", "message": "Review feedback"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a2ce053cf3111ba6b637778f08231c71289f0504", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/a2ce053cf3111ba6b637778f08231c71289f0504", "committedDate": "2020-04-27T12:42:38Z", "message": "Merge remote-tracking branch 'upstream/master' into 48366-dangling-indices-api"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAxODE5Nzc4", "url": "https://github.com/elastic/elasticsearch/pull/50920#pullrequestreview-401819778", "createdAt": "2020-04-28T13:12:45Z", "commit": {"oid": "a2ce053cf3111ba6b637778f08231c71289f0504"}, "state": "COMMENTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQxMzoxMjo0NlrOGNTTsA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQxMzo0NTozOVrOGNUzoA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjYwMTAwOA==", "bodyText": "I see that you added an assertBusy() on GatewayMetaState#allPendingAsyncStatesWritten below. In fact I think we need to wait for that before we restart the node, not after.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r416601008", "createdAt": "2020-04-28T13:12:46Z", "author": {"login": "DaveCTurner"}, "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "diffHunk": "@@ -0,0 +1,291 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http;\n+\n+import org.elasticsearch.client.Request;\n+import org.elasticsearch.client.Response;\n+import org.elasticsearch.client.RestClient;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.XContentTestUtils;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.elasticsearch.cluster.metadata.IndexGraveyard.SETTING_MAX_TOMBSTONES;\n+import static org.elasticsearch.gateway.DanglingIndicesState.AUTO_IMPORT_DANGLING_INDICES_SETTING;\n+import static org.elasticsearch.indices.IndicesService.WRITE_DANGLING_INDICES_INFO_SETTING;\n+import static org.elasticsearch.rest.RestStatus.ACCEPTED;\n+import static org.elasticsearch.rest.RestStatus.OK;\n+import static org.elasticsearch.test.XContentTestUtils.createJsonMapView;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+\n+/**\n+ * This class tests the dangling indices REST API.  These tests are here\n+ * today so they have access to a proper REST client. They cannot be in\n+ * :server:integTest since the REST client needs a proper transport\n+ * implementation, and they cannot be REST tests today since they need to\n+ * restart nodes. Really, though, this test should live elsewhere.\n+ *\n+ * @see org.elasticsearch.action.admin.indices.dangling\n+ */\n+@ClusterScope(numDataNodes = 0, scope = ESIntegTestCase.Scope.TEST, autoManageMasterNodes = false)\n+public class DanglingIndicesRestIT extends HttpSmokeTestCase {\n+    private static final String INDEX_NAME = \"test-idx-1\";\n+    private static final String OTHER_INDEX_NAME = INDEX_NAME + \"-other\";\n+\n+    private Settings buildSettings(int maxTombstones) {\n+        return Settings.builder()\n+            // Limit the indices kept in the graveyard. This can be set to zero, so that\n+            // when we delete an index, it's definitely considered to be dangling.\n+            .put(SETTING_MAX_TOMBSTONES.getKey(), maxTombstones)\n+            .put(WRITE_DANGLING_INDICES_INFO_SETTING.getKey(), true)\n+            .put(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey(), false)\n+            .build();\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed via the REST API.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        final DanglingIndexDetails danglingIndexDetails = createDanglingIndices(INDEX_NAME);\n+        final String stoppedNodeId = mapNodeNameToId(danglingIndexDetails.stoppedNodeName);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final Response listResponse = restClient.performRequest(new Request(\"GET\", \"/_dangling\"));\n+        assertOK(listResponse);\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(listResponse.getEntity().getContent());\n+\n+        assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+        List<Object> indices = mapView.get(\"dangling_indices\");\n+        assertThat(indices, hasSize(1));\n+\n+        assertThat(mapView.get(\"dangling_indices.0.index_name\"), equalTo(INDEX_NAME));\n+        assertThat(mapView.get(\"dangling_indices.0.index_uuid\"), equalTo(danglingIndexDetails.indexToUUID.get(INDEX_NAME)));\n+        assertThat(mapView.get(\"dangling_indices.0.creation_date_millis\"), instanceOf(Long.class));\n+        assertThat(mapView.get(\"dangling_indices.0.node_ids.0\"), equalTo(stoppedNodeId));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be imported.\n+     */\n+    public void testDanglingIndicesCanBeImported() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        createDanglingIndices(INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final List<String> danglingIndexIds = listDanglingIndexIds();\n+        assertThat(danglingIndexIds, hasSize(1));\n+\n+        final Request importRequest = new Request(\"POST\", \"/_dangling/\" + danglingIndexIds.get(0));\n+        importRequest.addParameter(\"accept_data_loss\", \"true\");\n+        // Ensure this parameter is accepted\n+        importRequest.addParameter(\"timeout\", \"20s\");\n+        final Response importResponse = restClient.performRequest(importRequest);\n+        assertThat(importResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(importResponse.getEntity().getContent());\n+        assertThat(mapView.get(\"accepted\"), equalTo(true));\n+\n+        assertTrue(\"Expected dangling index \" + INDEX_NAME + \" to be recovered\", indexExists(INDEX_NAME));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be deleted. Since this requires that\n+     * we add an entry to the index graveyard, the graveyard size must be\n+     * greater than 1. To test deletes, we set the index graveyard size to\n+     * 1, then create two indices and delete them both while one node in\n+     * the cluster is stopped. The deletion of the second pushes the deletion\n+     * of the first out of the graveyard. When the stopped node is resumed,\n+     * only the second index will be found into the graveyard and the the\n+     * other will be considered dangling, and can therefore be listed and\n+     * deleted through the API\n+     */\n+    public void testDanglingIndicesCanBeDeleted() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(1));\n+\n+        createDanglingIndices(INDEX_NAME, OTHER_INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final List<String> danglingIndexIds = listDanglingIndexIds();\n+        assertThat(danglingIndexIds, hasSize(1));\n+\n+        final Request deleteRequest = new Request(\"DELETE\", \"/_dangling/\" + danglingIndexIds.get(0));\n+        deleteRequest.addParameter(\"accept_data_loss\", \"true\");\n+        // Ensure these parameters is accepted\n+        deleteRequest.addParameter(\"timeout\", \"20s\");\n+        deleteRequest.addParameter(\"master_timeout\", \"20s\");\n+        final Response deleteResponse = restClient.performRequest(deleteRequest);\n+        assertThat(deleteResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        assertBusy(() -> assertThat(\"Expected dangling index to be deleted\", listDanglingIndexIds(), hasSize(0)));\n+    }\n+\n+    private List<String> listDanglingIndexIds() throws IOException {\n+        final Response response = getRestClient().performRequest(new Request(\"GET\", \"/_dangling\"));\n+        assertOK(response);\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(response.getEntity().getContent());\n+\n+        assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+        List<Object> indices = mapView.get(\"dangling_indices\");\n+\n+        List<String> danglingIndexIds = new ArrayList<>();\n+\n+        for (int i = 0; i < indices.size(); i++) {\n+            danglingIndexIds.add(mapView.get(\"dangling_indices.\" + i + \".index_uuid\"));\n+        }\n+\n+        return danglingIndexIds;\n+    }\n+\n+    private void assertOK(Response response) {\n+        assertThat(response.getStatusLine().getStatusCode(), equalTo(OK.getStatus()));\n+    }\n+\n+    /**\n+     * Given a node name, finds the corresponding node ID.\n+     */\n+    private String mapNodeNameToId(String nodeName) throws IOException {\n+        final Response catResponse = getRestClient().performRequest(new Request(\"GET\", \"/_cat/nodes?full_id&h=id,name\"));\n+        assertOK(catResponse);\n+\n+        for (String nodeLine : Streams.readAllLines(catResponse.getEntity().getContent())) {\n+            String[] elements = nodeLine.split(\" \");\n+            if (elements[1].equals(nodeName)) {\n+                return elements[0];\n+            }\n+        }\n+\n+        throw new AssertionError(\"Failed to map node name [\" + nodeName + \"] to node ID\");\n+    }\n+\n+    /**\n+     * Helper that creates one or more indices, and importantly,\n+     * checks that they are green before proceeding. This is important\n+     * because the tests in this class stop and restart nodes, assuming\n+     * that each index has a primary or replica shard on every node, and if\n+     * a node is stopped prematurely, this assumption is broken.\n+     *\n+     * @return a mapping from each createad index name to its UUID\n+     */\n+    private Map<String, String> createIndices(String... indices) throws IOException {\n+        assert indices.length > 0;\n+        for (String index : indices) {\n+            String indexSettings = \"{\"\n+                + \"  \\\"settings\\\": {\"\n+                + \"    \\\"index\\\": {\"\n+                + \"      \\\"number_of_shards\\\": 1,\"\n+                + \"      \\\"number_of_replicas\\\": 2\"\n+                + \"    }\"\n+                + \"  }\"\n+                + \"}\";\n+            Request request = new Request(\"PUT\", \"/\" + index);\n+            request.setJsonEntity(indexSettings);\n+            final Response response = getRestClient().performRequest(request);\n+            assertOK(response);\n+        }\n+        ensureGreen(indices);\n+\n+        final Response catResponse = getRestClient().performRequest(new Request(\"GET\", \"/_cat/indices?h=index,uuid\"));\n+        assertOK(catResponse);\n+\n+        final Map<String, String> createdIndexIDs = new HashMap<>();\n+\n+        final List<String> indicesAsList = Arrays.asList(indices);\n+\n+        for (String indexLine : Streams.readAllLines(catResponse.getEntity().getContent())) {\n+            String[] elements = indexLine.split(\" +\");\n+            if (indicesAsList.contains(elements[0])) {\n+                createdIndexIDs.put(elements[0], elements[1]);\n+            }\n+        }\n+\n+        assertThat(\"Expected to find as many index UUIDs as created indices\", createdIndexIDs.size(), equalTo(indices.length));\n+\n+        return createdIndexIDs;\n+    }\n+\n+    private void deleteIndex(String indexName) throws IOException {\n+        Response deleteResponse = getRestClient().performRequest(new Request(\"DELETE\", \"/\" + indexName));\n+        assertOK(deleteResponse);\n+    }\n+\n+    private DanglingIndexDetails createDanglingIndices(String... indices) throws Exception {\n+        ensureStableCluster(3);\n+        final Map<String, String> indexToUUID = createIndices(indices);\n+\n+        final AtomicReference<String> stoppedNodeName = new AtomicReference<>();\n+\n+        // Restart node, deleting the index in its absence, so that there is a dangling index to recover\n+        internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDIyMTE2Nw=="}, "originalCommit": {"oid": "5ce7f09b55fa5c518531347a0e73b13755f0d5cc"}, "originalPosition": 264}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjYwMTcxOQ==", "bodyText": "Typo\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * @return a mapping from each createad index name to its UUID\n          \n          \n            \n                 * @return a mapping from each created index name to its UUID", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r416601719", "createdAt": "2020-04-28T13:13:47Z", "author": {"login": "DaveCTurner"}, "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "diffHunk": "@@ -0,0 +1,312 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http;\n+\n+import org.elasticsearch.client.Request;\n+import org.elasticsearch.client.Response;\n+import org.elasticsearch.client.RestClient;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.gateway.GatewayMetaState;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.XContentTestUtils;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.elasticsearch.cluster.metadata.IndexGraveyard.SETTING_MAX_TOMBSTONES;\n+import static org.elasticsearch.gateway.DanglingIndicesState.AUTO_IMPORT_DANGLING_INDICES_SETTING;\n+import static org.elasticsearch.indices.IndicesService.WRITE_DANGLING_INDICES_INFO_SETTING;\n+import static org.elasticsearch.rest.RestStatus.ACCEPTED;\n+import static org.elasticsearch.rest.RestStatus.OK;\n+import static org.elasticsearch.test.XContentTestUtils.createJsonMapView;\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.hamcrest.Matchers.is;\n+\n+/**\n+ * This class tests the dangling indices REST API.  These tests are here\n+ * today so they have access to a proper REST client. They cannot be in\n+ * :server:integTest since the REST client needs a proper transport\n+ * implementation, and they cannot be REST tests today since they need to\n+ * restart nodes. Really, though, this test should live elsewhere.\n+ *\n+ * @see org.elasticsearch.action.admin.indices.dangling\n+ */\n+@ClusterScope(numDataNodes = 0, scope = ESIntegTestCase.Scope.TEST, autoManageMasterNodes = false)\n+public class DanglingIndicesRestIT extends HttpSmokeTestCase {\n+    private static final String INDEX_NAME = \"test-idx-1\";\n+    private static final String OTHER_INDEX_NAME = INDEX_NAME + \"-other\";\n+\n+    private Settings buildSettings(int maxTombstones) {\n+        return Settings.builder()\n+            // Limit the indices kept in the graveyard. This can be set to zero, so that\n+            // when we delete an index, it's definitely considered to be dangling.\n+            .put(SETTING_MAX_TOMBSTONES.getKey(), maxTombstones)\n+            .put(WRITE_DANGLING_INDICES_INFO_SETTING.getKey(), true)\n+            .put(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey(), false)\n+            .build();\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed via the REST API.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        final DanglingIndexDetails danglingIndexDetails = createDanglingIndices(INDEX_NAME);\n+        final String stoppedNodeId = mapNodeNameToId(danglingIndexDetails.stoppedNodeName);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final Response listResponse = restClient.performRequest(new Request(\"GET\", \"/_dangling\"));\n+        assertOK(listResponse);\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(listResponse.getEntity().getContent());\n+\n+        assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+        List<Object> indices = mapView.get(\"dangling_indices\");\n+        assertThat(indices, hasSize(1));\n+\n+        assertThat(mapView.get(\"dangling_indices.0.index_name\"), equalTo(INDEX_NAME));\n+        assertThat(mapView.get(\"dangling_indices.0.index_uuid\"), equalTo(danglingIndexDetails.indexToUUID.get(INDEX_NAME)));\n+        assertThat(mapView.get(\"dangling_indices.0.creation_date_millis\"), instanceOf(Long.class));\n+        assertThat(mapView.get(\"dangling_indices.0.node_ids.0\"), equalTo(stoppedNodeId));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be imported.\n+     */\n+    public void testDanglingIndicesCanBeImported() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        createDanglingIndices(INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final List<String> danglingIndexIds = listDanglingIndexIds();\n+        assertThat(danglingIndexIds, hasSize(1));\n+\n+        final Request importRequest = new Request(\"POST\", \"/_dangling/\" + danglingIndexIds.get(0));\n+        importRequest.addParameter(\"accept_data_loss\", \"true\");\n+        // Ensure this parameter is accepted\n+        importRequest.addParameter(\"timeout\", \"20s\");\n+        final Response importResponse = restClient.performRequest(importRequest);\n+        assertThat(importResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(importResponse.getEntity().getContent());\n+        assertThat(mapView.get(\"accepted\"), equalTo(true));\n+\n+        assertTrue(\"Expected dangling index \" + INDEX_NAME + \" to be recovered\", indexExists(INDEX_NAME));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be deleted. Since this requires that\n+     * we add an entry to the index graveyard, the graveyard size must be\n+     * greater than 1. To test deletes, we set the index graveyard size to\n+     * 1, then create two indices and delete them both while one node in\n+     * the cluster is stopped. The deletion of the second pushes the deletion\n+     * of the first out of the graveyard. When the stopped node is resumed,\n+     * only the second index will be found into the graveyard and the the\n+     * other will be considered dangling, and can therefore be listed and\n+     * deleted through the API\n+     */\n+    public void testDanglingIndicesCanBeDeleted() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(1));\n+\n+        createDanglingIndices(INDEX_NAME, OTHER_INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final List<String> danglingIndexIds = listDanglingIndexIds();\n+        assertThat(danglingIndexIds, hasSize(1));\n+\n+        final Request deleteRequest = new Request(\"DELETE\", \"/_dangling/\" + danglingIndexIds.get(0));\n+        deleteRequest.addParameter(\"accept_data_loss\", \"true\");\n+        // Ensure these parameters is accepted\n+        deleteRequest.addParameter(\"timeout\", \"20s\");\n+        deleteRequest.addParameter(\"master_timeout\", \"20s\");\n+        final Response deleteResponse = restClient.performRequest(deleteRequest);\n+        assertThat(deleteResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        assertBusy(() -> assertThat(\"Expected dangling index to be deleted\", listDanglingIndexIds(), hasSize(0)));\n+\n+        // The dangling index that we deleted ought to have been removed from disk. Check by\n+        // creating and deleting another index, which creates a new tombstone entry, which should\n+        // not cause the deleted dangling index to be considered \"live\" again, just because its\n+        // tombstone has been pushed out of the graveyard.\n+        createIndex(\"additional\");\n+        deleteIndex(\"additional\");\n+        assertThat(listDanglingIndexIds(), is(empty()));\n+    }\n+\n+    private List<String> listDanglingIndexIds() throws IOException {\n+        final Response response = getRestClient().performRequest(new Request(\"GET\", \"/_dangling\"));\n+        assertOK(response);\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(response.getEntity().getContent());\n+\n+        assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+        List<Object> indices = mapView.get(\"dangling_indices\");\n+\n+        List<String> danglingIndexIds = new ArrayList<>();\n+\n+        for (int i = 0; i < indices.size(); i++) {\n+            danglingIndexIds.add(mapView.get(\"dangling_indices.\" + i + \".index_uuid\"));\n+        }\n+\n+        return danglingIndexIds;\n+    }\n+\n+    private void assertOK(Response response) {\n+        assertThat(response.getStatusLine().getStatusCode(), equalTo(OK.getStatus()));\n+    }\n+\n+    /**\n+     * Given a node name, finds the corresponding node ID.\n+     */\n+    private String mapNodeNameToId(String nodeName) throws IOException {\n+        final Response catResponse = getRestClient().performRequest(new Request(\"GET\", \"/_cat/nodes?full_id&h=id,name\"));\n+        assertOK(catResponse);\n+\n+        for (String nodeLine : Streams.readAllLines(catResponse.getEntity().getContent())) {\n+            String[] elements = nodeLine.split(\" \");\n+            if (elements[1].equals(nodeName)) {\n+                return elements[0];\n+            }\n+        }\n+\n+        throw new AssertionError(\"Failed to map node name [\" + nodeName + \"] to node ID\");\n+    }\n+\n+    /**\n+     * Helper that creates one or more indices, and importantly,\n+     * checks that they are green before proceeding. This is important\n+     * because the tests in this class stop and restart nodes, assuming\n+     * that each index has a primary or replica shard on every node, and if\n+     * a node is stopped prematurely, this assumption is broken.\n+     *\n+     * @return a mapping from each createad index name to its UUID", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a2ce053cf3111ba6b637778f08231c71289f0504"}, "originalPosition": 224}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjYxNDI3Mw==", "bodyText": "Could we expectThrows(IllegalArgumentException.class, ...) here instead of this?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r416614273", "createdAt": "2020-04-28T13:30:51Z", "author": {"login": "DaveCTurner"}, "path": "server/src/test/java/org/elasticsearch/indices/recovery/DanglingIndicesIT.java", "diffHunk": "@@ -126,29 +146,291 @@ public Settings onNodeStopped(String nodeName) throws Exception {\n      * Check that when dangling indices are not written, then they cannot be recovered into the cluster.\n      */\n     public void testDanglingIndicesAreNotRecoveredWhenNotWritten() throws Exception {\n-        internalCluster().startNodes(3, buildSettings(false, true));\n+        internalCluster().startNodes(3, buildSettings(0, false, true));\n \n-        createIndex(INDEX_NAME, Settings.builder().put(\"number_of_replicas\", 2).build());\n-        ensureGreen(INDEX_NAME);\n-        internalCluster().getInstances(IndicesService.class).forEach(\n-            indicesService -> assertTrue(indicesService.allPendingDanglingIndicesWritten()));\n+        createDanglingIndices(INDEX_NAME);\n \n-        // Restart node, deleting the index in its absence, so that there is a dangling index to recover\n+        // Since index recovery is async, we can't prove index recovery will never occur, just that it doesn't occur within some reasonable\n+        // amount of time\n+        assertFalse(\n+            \"Did not expect dangling index \" + INDEX_NAME + \" to be recovered\",\n+            waitUntil(() -> indexExists(INDEX_NAME), 1, TimeUnit.SECONDS)\n+        );\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().startNodes(3, buildSettings(0, true, false));\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME);\n+\n+        final ListDanglingIndicesResponse response = client().admin()\n+            .cluster()\n+            .listDanglingIndices(new ListDanglingIndicesRequest())\n+            .actionGet();\n+        assertThat(response.status(), equalTo(RestStatus.OK));\n+\n+        final List<NodeListDanglingIndicesResponse> nodeResponses = response.getNodes();\n+        assertThat(\"Didn't get responses from all nodes\", nodeResponses, hasSize(3));\n+\n+        for (NodeListDanglingIndicesResponse nodeResponse : nodeResponses) {\n+            if (nodeResponse.getNode().getName().equals(stoppedNodeName)) {\n+                assertThat(\"Expected node that was stopped to have one dangling index\", nodeResponse.getDanglingIndices(), hasSize(1));\n+\n+                final DanglingIndexInfo danglingIndexInfo = nodeResponse.getDanglingIndices().get(0);\n+                assertThat(danglingIndexInfo.getIndexName(), equalTo(INDEX_NAME));\n+            } else {\n+                assertThat(\"Expected node that was never stopped to have no dangling indices\", nodeResponse.getDanglingIndices(), empty());\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Check that dangling indices can be imported.\n+     */\n+    public void testDanglingIndicesCanBeImported() throws Exception {\n+        internalCluster().startNodes(3, buildSettings(0, true, false));\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME);\n+\n+        final String danglingIndexUUID = findDanglingIndexForNode(stoppedNodeName, INDEX_NAME);\n+\n+        final ImportDanglingIndexRequest request = new ImportDanglingIndexRequest(danglingIndexUUID, true);\n+\n+        client().admin().cluster().importDanglingIndex(request).actionGet();\n+\n+        assertTrue(\"Expected dangling index \" + INDEX_NAME + \" to be recovered\", indexExists(INDEX_NAME));\n+    }\n+\n+    /**\n+     * Check that the when sending an import-dangling-indices request, the specified UUIDs are validated as\n+     * being dangling.\n+     */\n+    public void testDanglingIndicesMustExistToBeImported() {\n+        internalCluster().startNodes(1, buildSettings(0, true, false));\n+\n+        final ImportDanglingIndexRequest request = new ImportDanglingIndexRequest(\"NonExistentUUID\", true);\n+\n+        boolean noExceptionThrown = false;\n+        try {\n+            client().admin().cluster().importDanglingIndex(request).actionGet();\n+            noExceptionThrown = true;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a2ce053cf3111ba6b637778f08231c71289f0504"}, "originalPosition": 219}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjYxNDc2Nw==", "bodyText": "Could we expectThrows() here?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r416614767", "createdAt": "2020-04-28T13:31:28Z", "author": {"login": "DaveCTurner"}, "path": "server/src/test/java/org/elasticsearch/indices/recovery/DanglingIndicesIT.java", "diffHunk": "@@ -126,29 +146,291 @@ public Settings onNodeStopped(String nodeName) throws Exception {\n      * Check that when dangling indices are not written, then they cannot be recovered into the cluster.\n      */\n     public void testDanglingIndicesAreNotRecoveredWhenNotWritten() throws Exception {\n-        internalCluster().startNodes(3, buildSettings(false, true));\n+        internalCluster().startNodes(3, buildSettings(0, false, true));\n \n-        createIndex(INDEX_NAME, Settings.builder().put(\"number_of_replicas\", 2).build());\n-        ensureGreen(INDEX_NAME);\n-        internalCluster().getInstances(IndicesService.class).forEach(\n-            indicesService -> assertTrue(indicesService.allPendingDanglingIndicesWritten()));\n+        createDanglingIndices(INDEX_NAME);\n \n-        // Restart node, deleting the index in its absence, so that there is a dangling index to recover\n+        // Since index recovery is async, we can't prove index recovery will never occur, just that it doesn't occur within some reasonable\n+        // amount of time\n+        assertFalse(\n+            \"Did not expect dangling index \" + INDEX_NAME + \" to be recovered\",\n+            waitUntil(() -> indexExists(INDEX_NAME), 1, TimeUnit.SECONDS)\n+        );\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().startNodes(3, buildSettings(0, true, false));\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME);\n+\n+        final ListDanglingIndicesResponse response = client().admin()\n+            .cluster()\n+            .listDanglingIndices(new ListDanglingIndicesRequest())\n+            .actionGet();\n+        assertThat(response.status(), equalTo(RestStatus.OK));\n+\n+        final List<NodeListDanglingIndicesResponse> nodeResponses = response.getNodes();\n+        assertThat(\"Didn't get responses from all nodes\", nodeResponses, hasSize(3));\n+\n+        for (NodeListDanglingIndicesResponse nodeResponse : nodeResponses) {\n+            if (nodeResponse.getNode().getName().equals(stoppedNodeName)) {\n+                assertThat(\"Expected node that was stopped to have one dangling index\", nodeResponse.getDanglingIndices(), hasSize(1));\n+\n+                final DanglingIndexInfo danglingIndexInfo = nodeResponse.getDanglingIndices().get(0);\n+                assertThat(danglingIndexInfo.getIndexName(), equalTo(INDEX_NAME));\n+            } else {\n+                assertThat(\"Expected node that was never stopped to have no dangling indices\", nodeResponse.getDanglingIndices(), empty());\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Check that dangling indices can be imported.\n+     */\n+    public void testDanglingIndicesCanBeImported() throws Exception {\n+        internalCluster().startNodes(3, buildSettings(0, true, false));\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME);\n+\n+        final String danglingIndexUUID = findDanglingIndexForNode(stoppedNodeName, INDEX_NAME);\n+\n+        final ImportDanglingIndexRequest request = new ImportDanglingIndexRequest(danglingIndexUUID, true);\n+\n+        client().admin().cluster().importDanglingIndex(request).actionGet();\n+\n+        assertTrue(\"Expected dangling index \" + INDEX_NAME + \" to be recovered\", indexExists(INDEX_NAME));\n+    }\n+\n+    /**\n+     * Check that the when sending an import-dangling-indices request, the specified UUIDs are validated as\n+     * being dangling.\n+     */\n+    public void testDanglingIndicesMustExistToBeImported() {\n+        internalCluster().startNodes(1, buildSettings(0, true, false));\n+\n+        final ImportDanglingIndexRequest request = new ImportDanglingIndexRequest(\"NonExistentUUID\", true);\n+\n+        boolean noExceptionThrown = false;\n+        try {\n+            client().admin().cluster().importDanglingIndex(request).actionGet();\n+            noExceptionThrown = true;\n+        } catch (Exception e) {\n+            assertThat(e, instanceOf(IllegalArgumentException.class));\n+            assertThat(e.getMessage(), containsString(\"No dangling index found for UUID [NonExistentUUID]\"));\n+        }\n+\n+        assertFalse(\"No exception thrown\", noExceptionThrown);\n+    }\n+\n+    /**\n+     * Check that a dangling index can only be imported if \"accept_data_loss\" is set to true.\n+     */\n+    public void testMustAcceptDataLossToImportDanglingIndex() throws Exception {\n+        internalCluster().startNodes(3, buildSettings(0, true, false));\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME);\n+        final String danglingIndexUUID = findDanglingIndexForNode(stoppedNodeName, INDEX_NAME);\n+\n+        final ImportDanglingIndexRequest request = new ImportDanglingIndexRequest(danglingIndexUUID, false);\n+\n+        Exception caughtException = null;\n+\n+        try {\n+            client().admin().cluster().importDanglingIndex(request).actionGet();\n+        } catch (Exception e) {\n+            caughtException = e;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a2ce053cf3111ba6b637778f08231c71289f0504"}, "originalPosition": 244}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjYxNTgwMQ==", "bodyText": "WDYT about making the same fix to this test as the REST test to verify that the index really did get deleted?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r416615801", "createdAt": "2020-04-28T13:32:47Z", "author": {"login": "DaveCTurner"}, "path": "server/src/test/java/org/elasticsearch/indices/recovery/DanglingIndicesIT.java", "diffHunk": "@@ -126,29 +146,291 @@ public Settings onNodeStopped(String nodeName) throws Exception {\n      * Check that when dangling indices are not written, then they cannot be recovered into the cluster.\n      */\n     public void testDanglingIndicesAreNotRecoveredWhenNotWritten() throws Exception {\n-        internalCluster().startNodes(3, buildSettings(false, true));\n+        internalCluster().startNodes(3, buildSettings(0, false, true));\n \n-        createIndex(INDEX_NAME, Settings.builder().put(\"number_of_replicas\", 2).build());\n-        ensureGreen(INDEX_NAME);\n-        internalCluster().getInstances(IndicesService.class).forEach(\n-            indicesService -> assertTrue(indicesService.allPendingDanglingIndicesWritten()));\n+        createDanglingIndices(INDEX_NAME);\n \n-        // Restart node, deleting the index in its absence, so that there is a dangling index to recover\n+        // Since index recovery is async, we can't prove index recovery will never occur, just that it doesn't occur within some reasonable\n+        // amount of time\n+        assertFalse(\n+            \"Did not expect dangling index \" + INDEX_NAME + \" to be recovered\",\n+            waitUntil(() -> indexExists(INDEX_NAME), 1, TimeUnit.SECONDS)\n+        );\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().startNodes(3, buildSettings(0, true, false));\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME);\n+\n+        final ListDanglingIndicesResponse response = client().admin()\n+            .cluster()\n+            .listDanglingIndices(new ListDanglingIndicesRequest())\n+            .actionGet();\n+        assertThat(response.status(), equalTo(RestStatus.OK));\n+\n+        final List<NodeListDanglingIndicesResponse> nodeResponses = response.getNodes();\n+        assertThat(\"Didn't get responses from all nodes\", nodeResponses, hasSize(3));\n+\n+        for (NodeListDanglingIndicesResponse nodeResponse : nodeResponses) {\n+            if (nodeResponse.getNode().getName().equals(stoppedNodeName)) {\n+                assertThat(\"Expected node that was stopped to have one dangling index\", nodeResponse.getDanglingIndices(), hasSize(1));\n+\n+                final DanglingIndexInfo danglingIndexInfo = nodeResponse.getDanglingIndices().get(0);\n+                assertThat(danglingIndexInfo.getIndexName(), equalTo(INDEX_NAME));\n+            } else {\n+                assertThat(\"Expected node that was never stopped to have no dangling indices\", nodeResponse.getDanglingIndices(), empty());\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Check that dangling indices can be imported.\n+     */\n+    public void testDanglingIndicesCanBeImported() throws Exception {\n+        internalCluster().startNodes(3, buildSettings(0, true, false));\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME);\n+\n+        final String danglingIndexUUID = findDanglingIndexForNode(stoppedNodeName, INDEX_NAME);\n+\n+        final ImportDanglingIndexRequest request = new ImportDanglingIndexRequest(danglingIndexUUID, true);\n+\n+        client().admin().cluster().importDanglingIndex(request).actionGet();\n+\n+        assertTrue(\"Expected dangling index \" + INDEX_NAME + \" to be recovered\", indexExists(INDEX_NAME));\n+    }\n+\n+    /**\n+     * Check that the when sending an import-dangling-indices request, the specified UUIDs are validated as\n+     * being dangling.\n+     */\n+    public void testDanglingIndicesMustExistToBeImported() {\n+        internalCluster().startNodes(1, buildSettings(0, true, false));\n+\n+        final ImportDanglingIndexRequest request = new ImportDanglingIndexRequest(\"NonExistentUUID\", true);\n+\n+        boolean noExceptionThrown = false;\n+        try {\n+            client().admin().cluster().importDanglingIndex(request).actionGet();\n+            noExceptionThrown = true;\n+        } catch (Exception e) {\n+            assertThat(e, instanceOf(IllegalArgumentException.class));\n+            assertThat(e.getMessage(), containsString(\"No dangling index found for UUID [NonExistentUUID]\"));\n+        }\n+\n+        assertFalse(\"No exception thrown\", noExceptionThrown);\n+    }\n+\n+    /**\n+     * Check that a dangling index can only be imported if \"accept_data_loss\" is set to true.\n+     */\n+    public void testMustAcceptDataLossToImportDanglingIndex() throws Exception {\n+        internalCluster().startNodes(3, buildSettings(0, true, false));\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME);\n+        final String danglingIndexUUID = findDanglingIndexForNode(stoppedNodeName, INDEX_NAME);\n+\n+        final ImportDanglingIndexRequest request = new ImportDanglingIndexRequest(danglingIndexUUID, false);\n+\n+        Exception caughtException = null;\n+\n+        try {\n+            client().admin().cluster().importDanglingIndex(request).actionGet();\n+        } catch (Exception e) {\n+            caughtException = e;\n+        }\n+\n+        assertNotNull(\"No exception thrown\", caughtException);\n+        assertThat(caughtException.getMessage(), containsString(\"accept_data_loss must be set to true\"));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be deleted. Since this requires that\n+     * we add an entry to the index graveyard, the graveyard size must be\n+     * greater than 1. To test deletes, we set the index graveyard size to\n+     * 1, then create two indices and delete them both while one node in\n+     * the cluster is stopped. The deletion of the second pushes the deletion\n+     * of the first out of the graveyard. When the stopped node is resumed,\n+     * only the second index will be found into the graveyard and the the\n+     * other will be considered dangling, and can therefore be listed and\n+     * deleted through the API\n+     */\n+    public void testDanglingIndexCanBeDeleted() throws Exception {\n+        final Settings settings = buildSettings(1, true, false);\n+        internalCluster().startNodes(3, settings);\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME, OTHER_INDEX_NAME);\n+        final String danglingIndexUUID = findDanglingIndexForNode(stoppedNodeName, INDEX_NAME);\n+\n+        client().admin().cluster().deleteDanglingIndex(new DeleteDanglingIndexRequest(danglingIndexUUID, true)).actionGet();\n+\n+        assertThat(listDanglingIndices(), is(empty()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a2ce053cf3111ba6b637778f08231c71289f0504"}, "originalPosition": 271}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjYxNjE0Mw==", "bodyText": "Also here -- WDYT about making the same fix to this test as the REST test to verify that the index really did get deleted?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r416616143", "createdAt": "2020-04-28T13:33:18Z", "author": {"login": "DaveCTurner"}, "path": "server/src/test/java/org/elasticsearch/indices/recovery/DanglingIndicesIT.java", "diffHunk": "@@ -126,29 +146,291 @@ public Settings onNodeStopped(String nodeName) throws Exception {\n      * Check that when dangling indices are not written, then they cannot be recovered into the cluster.\n      */\n     public void testDanglingIndicesAreNotRecoveredWhenNotWritten() throws Exception {\n-        internalCluster().startNodes(3, buildSettings(false, true));\n+        internalCluster().startNodes(3, buildSettings(0, false, true));\n \n-        createIndex(INDEX_NAME, Settings.builder().put(\"number_of_replicas\", 2).build());\n-        ensureGreen(INDEX_NAME);\n-        internalCluster().getInstances(IndicesService.class).forEach(\n-            indicesService -> assertTrue(indicesService.allPendingDanglingIndicesWritten()));\n+        createDanglingIndices(INDEX_NAME);\n \n-        // Restart node, deleting the index in its absence, so that there is a dangling index to recover\n+        // Since index recovery is async, we can't prove index recovery will never occur, just that it doesn't occur within some reasonable\n+        // amount of time\n+        assertFalse(\n+            \"Did not expect dangling index \" + INDEX_NAME + \" to be recovered\",\n+            waitUntil(() -> indexExists(INDEX_NAME), 1, TimeUnit.SECONDS)\n+        );\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().startNodes(3, buildSettings(0, true, false));\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME);\n+\n+        final ListDanglingIndicesResponse response = client().admin()\n+            .cluster()\n+            .listDanglingIndices(new ListDanglingIndicesRequest())\n+            .actionGet();\n+        assertThat(response.status(), equalTo(RestStatus.OK));\n+\n+        final List<NodeListDanglingIndicesResponse> nodeResponses = response.getNodes();\n+        assertThat(\"Didn't get responses from all nodes\", nodeResponses, hasSize(3));\n+\n+        for (NodeListDanglingIndicesResponse nodeResponse : nodeResponses) {\n+            if (nodeResponse.getNode().getName().equals(stoppedNodeName)) {\n+                assertThat(\"Expected node that was stopped to have one dangling index\", nodeResponse.getDanglingIndices(), hasSize(1));\n+\n+                final DanglingIndexInfo danglingIndexInfo = nodeResponse.getDanglingIndices().get(0);\n+                assertThat(danglingIndexInfo.getIndexName(), equalTo(INDEX_NAME));\n+            } else {\n+                assertThat(\"Expected node that was never stopped to have no dangling indices\", nodeResponse.getDanglingIndices(), empty());\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Check that dangling indices can be imported.\n+     */\n+    public void testDanglingIndicesCanBeImported() throws Exception {\n+        internalCluster().startNodes(3, buildSettings(0, true, false));\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME);\n+\n+        final String danglingIndexUUID = findDanglingIndexForNode(stoppedNodeName, INDEX_NAME);\n+\n+        final ImportDanglingIndexRequest request = new ImportDanglingIndexRequest(danglingIndexUUID, true);\n+\n+        client().admin().cluster().importDanglingIndex(request).actionGet();\n+\n+        assertTrue(\"Expected dangling index \" + INDEX_NAME + \" to be recovered\", indexExists(INDEX_NAME));\n+    }\n+\n+    /**\n+     * Check that the when sending an import-dangling-indices request, the specified UUIDs are validated as\n+     * being dangling.\n+     */\n+    public void testDanglingIndicesMustExistToBeImported() {\n+        internalCluster().startNodes(1, buildSettings(0, true, false));\n+\n+        final ImportDanglingIndexRequest request = new ImportDanglingIndexRequest(\"NonExistentUUID\", true);\n+\n+        boolean noExceptionThrown = false;\n+        try {\n+            client().admin().cluster().importDanglingIndex(request).actionGet();\n+            noExceptionThrown = true;\n+        } catch (Exception e) {\n+            assertThat(e, instanceOf(IllegalArgumentException.class));\n+            assertThat(e.getMessage(), containsString(\"No dangling index found for UUID [NonExistentUUID]\"));\n+        }\n+\n+        assertFalse(\"No exception thrown\", noExceptionThrown);\n+    }\n+\n+    /**\n+     * Check that a dangling index can only be imported if \"accept_data_loss\" is set to true.\n+     */\n+    public void testMustAcceptDataLossToImportDanglingIndex() throws Exception {\n+        internalCluster().startNodes(3, buildSettings(0, true, false));\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME);\n+        final String danglingIndexUUID = findDanglingIndexForNode(stoppedNodeName, INDEX_NAME);\n+\n+        final ImportDanglingIndexRequest request = new ImportDanglingIndexRequest(danglingIndexUUID, false);\n+\n+        Exception caughtException = null;\n+\n+        try {\n+            client().admin().cluster().importDanglingIndex(request).actionGet();\n+        } catch (Exception e) {\n+            caughtException = e;\n+        }\n+\n+        assertNotNull(\"No exception thrown\", caughtException);\n+        assertThat(caughtException.getMessage(), containsString(\"accept_data_loss must be set to true\"));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be deleted. Since this requires that\n+     * we add an entry to the index graveyard, the graveyard size must be\n+     * greater than 1. To test deletes, we set the index graveyard size to\n+     * 1, then create two indices and delete them both while one node in\n+     * the cluster is stopped. The deletion of the second pushes the deletion\n+     * of the first out of the graveyard. When the stopped node is resumed,\n+     * only the second index will be found into the graveyard and the the\n+     * other will be considered dangling, and can therefore be listed and\n+     * deleted through the API\n+     */\n+    public void testDanglingIndexCanBeDeleted() throws Exception {\n+        final Settings settings = buildSettings(1, true, false);\n+        internalCluster().startNodes(3, settings);\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME, OTHER_INDEX_NAME);\n+        final String danglingIndexUUID = findDanglingIndexForNode(stoppedNodeName, INDEX_NAME);\n+\n+        client().admin().cluster().deleteDanglingIndex(new DeleteDanglingIndexRequest(danglingIndexUUID, true)).actionGet();\n+\n+        assertThat(listDanglingIndices(), is(empty()));\n+    }\n+\n+    /**\n+     * Check that when a index is found to be dangling on more than one node, it can be deleted.\n+     */\n+    public void testDanglingIndexOverMultipleNodesCanBeDeleted() throws Exception {\n+        final Settings settings = buildSettings(1, true, false);\n+        internalCluster().startNodes(3, settings);\n+\n+        createIndices(INDEX_NAME, OTHER_INDEX_NAME);\n+\n+        // Restart 2 nodes, deleting the indices in their absence, so that there is a dangling index to recover\n         internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {\n \n             @Override\n             public Settings onNodeStopped(String nodeName) throws Exception {\n-                internalCluster().validateClusterFormed();\n-                assertAcked(client().admin().indices().prepareDelete(INDEX_NAME));\n+                internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {\n+\n+                    @Override\n+                    public Settings onNodeStopped(String nodeName) throws Exception {\n+                        internalCluster().validateClusterFormed();\n+                        assertAcked(client().admin().indices().prepareDelete(INDEX_NAME));\n+                        assertAcked(client().admin().indices().prepareDelete(OTHER_INDEX_NAME));\n+                        return super.onNodeStopped(nodeName);\n+                    }\n+                });\n+\n                 return super.onNodeStopped(nodeName);\n             }\n         });\n \n-        // Since index recovery is async, we can't prove index recovery will never occur, just that it doesn't occur within some reasonable\n-        // amount of time\n-        assertFalse(\n-            \"Did not expect dangling index \" + INDEX_NAME + \" to be recovered\",\n-            waitUntil(() -> indexExists(INDEX_NAME), 1, TimeUnit.SECONDS)\n+        final AtomicReference<List<DanglingIndexInfo>> danglingIndices = new AtomicReference<>();\n+\n+        final List<DanglingIndexInfo> results = listDanglingIndices();\n+\n+        // Both the stopped nodes should have found a dangling index.\n+        assertThat(results, hasSize(2));\n+        danglingIndices.set(results);\n+\n+        // Try to delete the index - this request should succeed\n+        client().admin()\n+            .cluster()\n+            .deleteDanglingIndex(new DeleteDanglingIndexRequest(danglingIndices.get().get(0).getIndexUUID(), true))\n+            .actionGet();\n+\n+        assertBusy(() -> assertThat(listDanglingIndices(), empty()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a2ce053cf3111ba6b637778f08231c71289f0504"}, "originalPosition": 324}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjYxNjQyOA==", "bodyText": "Suggest expectThrows here.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r416616428", "createdAt": "2020-04-28T13:33:41Z", "author": {"login": "DaveCTurner"}, "path": "server/src/test/java/org/elasticsearch/indices/recovery/DanglingIndicesIT.java", "diffHunk": "@@ -126,29 +146,291 @@ public Settings onNodeStopped(String nodeName) throws Exception {\n      * Check that when dangling indices are not written, then they cannot be recovered into the cluster.\n      */\n     public void testDanglingIndicesAreNotRecoveredWhenNotWritten() throws Exception {\n-        internalCluster().startNodes(3, buildSettings(false, true));\n+        internalCluster().startNodes(3, buildSettings(0, false, true));\n \n-        createIndex(INDEX_NAME, Settings.builder().put(\"number_of_replicas\", 2).build());\n-        ensureGreen(INDEX_NAME);\n-        internalCluster().getInstances(IndicesService.class).forEach(\n-            indicesService -> assertTrue(indicesService.allPendingDanglingIndicesWritten()));\n+        createDanglingIndices(INDEX_NAME);\n \n-        // Restart node, deleting the index in its absence, so that there is a dangling index to recover\n+        // Since index recovery is async, we can't prove index recovery will never occur, just that it doesn't occur within some reasonable\n+        // amount of time\n+        assertFalse(\n+            \"Did not expect dangling index \" + INDEX_NAME + \" to be recovered\",\n+            waitUntil(() -> indexExists(INDEX_NAME), 1, TimeUnit.SECONDS)\n+        );\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().startNodes(3, buildSettings(0, true, false));\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME);\n+\n+        final ListDanglingIndicesResponse response = client().admin()\n+            .cluster()\n+            .listDanglingIndices(new ListDanglingIndicesRequest())\n+            .actionGet();\n+        assertThat(response.status(), equalTo(RestStatus.OK));\n+\n+        final List<NodeListDanglingIndicesResponse> nodeResponses = response.getNodes();\n+        assertThat(\"Didn't get responses from all nodes\", nodeResponses, hasSize(3));\n+\n+        for (NodeListDanglingIndicesResponse nodeResponse : nodeResponses) {\n+            if (nodeResponse.getNode().getName().equals(stoppedNodeName)) {\n+                assertThat(\"Expected node that was stopped to have one dangling index\", nodeResponse.getDanglingIndices(), hasSize(1));\n+\n+                final DanglingIndexInfo danglingIndexInfo = nodeResponse.getDanglingIndices().get(0);\n+                assertThat(danglingIndexInfo.getIndexName(), equalTo(INDEX_NAME));\n+            } else {\n+                assertThat(\"Expected node that was never stopped to have no dangling indices\", nodeResponse.getDanglingIndices(), empty());\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Check that dangling indices can be imported.\n+     */\n+    public void testDanglingIndicesCanBeImported() throws Exception {\n+        internalCluster().startNodes(3, buildSettings(0, true, false));\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME);\n+\n+        final String danglingIndexUUID = findDanglingIndexForNode(stoppedNodeName, INDEX_NAME);\n+\n+        final ImportDanglingIndexRequest request = new ImportDanglingIndexRequest(danglingIndexUUID, true);\n+\n+        client().admin().cluster().importDanglingIndex(request).actionGet();\n+\n+        assertTrue(\"Expected dangling index \" + INDEX_NAME + \" to be recovered\", indexExists(INDEX_NAME));\n+    }\n+\n+    /**\n+     * Check that the when sending an import-dangling-indices request, the specified UUIDs are validated as\n+     * being dangling.\n+     */\n+    public void testDanglingIndicesMustExistToBeImported() {\n+        internalCluster().startNodes(1, buildSettings(0, true, false));\n+\n+        final ImportDanglingIndexRequest request = new ImportDanglingIndexRequest(\"NonExistentUUID\", true);\n+\n+        boolean noExceptionThrown = false;\n+        try {\n+            client().admin().cluster().importDanglingIndex(request).actionGet();\n+            noExceptionThrown = true;\n+        } catch (Exception e) {\n+            assertThat(e, instanceOf(IllegalArgumentException.class));\n+            assertThat(e.getMessage(), containsString(\"No dangling index found for UUID [NonExistentUUID]\"));\n+        }\n+\n+        assertFalse(\"No exception thrown\", noExceptionThrown);\n+    }\n+\n+    /**\n+     * Check that a dangling index can only be imported if \"accept_data_loss\" is set to true.\n+     */\n+    public void testMustAcceptDataLossToImportDanglingIndex() throws Exception {\n+        internalCluster().startNodes(3, buildSettings(0, true, false));\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME);\n+        final String danglingIndexUUID = findDanglingIndexForNode(stoppedNodeName, INDEX_NAME);\n+\n+        final ImportDanglingIndexRequest request = new ImportDanglingIndexRequest(danglingIndexUUID, false);\n+\n+        Exception caughtException = null;\n+\n+        try {\n+            client().admin().cluster().importDanglingIndex(request).actionGet();\n+        } catch (Exception e) {\n+            caughtException = e;\n+        }\n+\n+        assertNotNull(\"No exception thrown\", caughtException);\n+        assertThat(caughtException.getMessage(), containsString(\"accept_data_loss must be set to true\"));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be deleted. Since this requires that\n+     * we add an entry to the index graveyard, the graveyard size must be\n+     * greater than 1. To test deletes, we set the index graveyard size to\n+     * 1, then create two indices and delete them both while one node in\n+     * the cluster is stopped. The deletion of the second pushes the deletion\n+     * of the first out of the graveyard. When the stopped node is resumed,\n+     * only the second index will be found into the graveyard and the the\n+     * other will be considered dangling, and can therefore be listed and\n+     * deleted through the API\n+     */\n+    public void testDanglingIndexCanBeDeleted() throws Exception {\n+        final Settings settings = buildSettings(1, true, false);\n+        internalCluster().startNodes(3, settings);\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME, OTHER_INDEX_NAME);\n+        final String danglingIndexUUID = findDanglingIndexForNode(stoppedNodeName, INDEX_NAME);\n+\n+        client().admin().cluster().deleteDanglingIndex(new DeleteDanglingIndexRequest(danglingIndexUUID, true)).actionGet();\n+\n+        assertThat(listDanglingIndices(), is(empty()));\n+    }\n+\n+    /**\n+     * Check that when a index is found to be dangling on more than one node, it can be deleted.\n+     */\n+    public void testDanglingIndexOverMultipleNodesCanBeDeleted() throws Exception {\n+        final Settings settings = buildSettings(1, true, false);\n+        internalCluster().startNodes(3, settings);\n+\n+        createIndices(INDEX_NAME, OTHER_INDEX_NAME);\n+\n+        // Restart 2 nodes, deleting the indices in their absence, so that there is a dangling index to recover\n         internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {\n \n             @Override\n             public Settings onNodeStopped(String nodeName) throws Exception {\n-                internalCluster().validateClusterFormed();\n-                assertAcked(client().admin().indices().prepareDelete(INDEX_NAME));\n+                internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {\n+\n+                    @Override\n+                    public Settings onNodeStopped(String nodeName) throws Exception {\n+                        internalCluster().validateClusterFormed();\n+                        assertAcked(client().admin().indices().prepareDelete(INDEX_NAME));\n+                        assertAcked(client().admin().indices().prepareDelete(OTHER_INDEX_NAME));\n+                        return super.onNodeStopped(nodeName);\n+                    }\n+                });\n+\n                 return super.onNodeStopped(nodeName);\n             }\n         });\n \n-        // Since index recovery is async, we can't prove index recovery will never occur, just that it doesn't occur within some reasonable\n-        // amount of time\n-        assertFalse(\n-            \"Did not expect dangling index \" + INDEX_NAME + \" to be recovered\",\n-            waitUntil(() -> indexExists(INDEX_NAME), 1, TimeUnit.SECONDS)\n+        final AtomicReference<List<DanglingIndexInfo>> danglingIndices = new AtomicReference<>();\n+\n+        final List<DanglingIndexInfo> results = listDanglingIndices();\n+\n+        // Both the stopped nodes should have found a dangling index.\n+        assertThat(results, hasSize(2));\n+        danglingIndices.set(results);\n+\n+        // Try to delete the index - this request should succeed\n+        client().admin()\n+            .cluster()\n+            .deleteDanglingIndex(new DeleteDanglingIndexRequest(danglingIndices.get().get(0).getIndexUUID(), true))\n+            .actionGet();\n+\n+        assertBusy(() -> assertThat(listDanglingIndices(), empty()));\n+    }\n+\n+    /**\n+     * Check that when deleting a dangling index, it is required that the \"accept_data_loss\" flag is set.\n+     */\n+    public void testDeleteDanglingIndicesRequiresDataLossFlagToBeTrue() throws Exception {\n+        final Settings settings = buildSettings(1, true, false);\n+        internalCluster().startNodes(3, settings);\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME, OTHER_INDEX_NAME);\n+        final String danglingIndexUUID = findDanglingIndexForNode(stoppedNodeName, INDEX_NAME);\n+\n+        Exception caughtException = null;\n+\n+        try {\n+            client().admin().cluster().deleteDanglingIndex(new DeleteDanglingIndexRequest(danglingIndexUUID, false)).actionGet();\n+        } catch (Exception e) {\n+            caughtException = e;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a2ce053cf3111ba6b637778f08231c71289f0504"}, "originalPosition": 342}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjYxOTAwOQ==", "bodyText": "Should probably have a unit test that passes something other than emptyMap() to this new parameter.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r416619009", "createdAt": "2020-04-28T13:37:03Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java", "diffHunk": "@@ -134,24 +153,37 @@ void cleanupAllocatedDangledIndices(Metadata metadata) {\n      * to the currently tracked dangling indices.\n      */\n     void findNewAndAddDanglingIndices(final Metadata metadata) {\n-        danglingIndices.putAll(findNewDanglingIndices(metadata));\n+        final IndexGraveyard graveyard = metadata.indexGraveyard();\n+\n+        // If a tombstone is created for a dangling index, we need to make sure that the\n+        // index is no longer considered dangling.\n+        for (Index key : danglingIndices.keySet()) {\n+            if (graveyard.containsIndex(key)) {\n+                danglingIndices.remove(key);\n+            }\n+        }\n+\n+        danglingIndices.putAll(findNewDanglingIndices(danglingIndices, metadata));\n     }\n \n     /**\n      * Finds new dangling indices by iterating over the indices and trying to find indices\n      * that have state on disk, but are not part of the provided meta data, or not detected\n      * as dangled already.\n      */\n-    Map<Index, IndexMetadata> findNewDanglingIndices(final Metadata metadata) {\n+    public Map<Index, IndexMetadata> findNewDanglingIndices(Map<Index, IndexMetadata> existingDanglingIndices, final Metadata metadata) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a2ce053cf3111ba6b637778f08231c71289f0504"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjYyNDg4OQ==", "bodyText": "Should we also test that this works if auto-import is enabled? You can make an un-importable dangling index by deleting it and creating another with the same name while the node is away.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r416624889", "createdAt": "2020-04-28T13:44:45Z", "author": {"login": "DaveCTurner"}, "path": "server/src/test/java/org/elasticsearch/indices/recovery/DanglingIndicesIT.java", "diffHunk": "@@ -126,29 +146,291 @@ public Settings onNodeStopped(String nodeName) throws Exception {\n      * Check that when dangling indices are not written, then they cannot be recovered into the cluster.\n      */\n     public void testDanglingIndicesAreNotRecoveredWhenNotWritten() throws Exception {\n-        internalCluster().startNodes(3, buildSettings(false, true));\n+        internalCluster().startNodes(3, buildSettings(0, false, true));\n \n-        createIndex(INDEX_NAME, Settings.builder().put(\"number_of_replicas\", 2).build());\n-        ensureGreen(INDEX_NAME);\n-        internalCluster().getInstances(IndicesService.class).forEach(\n-            indicesService -> assertTrue(indicesService.allPendingDanglingIndicesWritten()));\n+        createDanglingIndices(INDEX_NAME);\n \n-        // Restart node, deleting the index in its absence, so that there is a dangling index to recover\n+        // Since index recovery is async, we can't prove index recovery will never occur, just that it doesn't occur within some reasonable\n+        // amount of time\n+        assertFalse(\n+            \"Did not expect dangling index \" + INDEX_NAME + \" to be recovered\",\n+            waitUntil(() -> indexExists(INDEX_NAME), 1, TimeUnit.SECONDS)\n+        );\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a2ce053cf3111ba6b637778f08231c71289f0504"}, "originalPosition": 164}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjYyNTU2OA==", "bodyText": "Should we also check that this works if auto-import is enabled?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r416625568", "createdAt": "2020-04-28T13:45:39Z", "author": {"login": "DaveCTurner"}, "path": "server/src/test/java/org/elasticsearch/indices/recovery/DanglingIndicesIT.java", "diffHunk": "@@ -126,29 +146,291 @@ public Settings onNodeStopped(String nodeName) throws Exception {\n      * Check that when dangling indices are not written, then they cannot be recovered into the cluster.\n      */\n     public void testDanglingIndicesAreNotRecoveredWhenNotWritten() throws Exception {\n-        internalCluster().startNodes(3, buildSettings(false, true));\n+        internalCluster().startNodes(3, buildSettings(0, false, true));\n \n-        createIndex(INDEX_NAME, Settings.builder().put(\"number_of_replicas\", 2).build());\n-        ensureGreen(INDEX_NAME);\n-        internalCluster().getInstances(IndicesService.class).forEach(\n-            indicesService -> assertTrue(indicesService.allPendingDanglingIndicesWritten()));\n+        createDanglingIndices(INDEX_NAME);\n \n-        // Restart node, deleting the index in its absence, so that there is a dangling index to recover\n+        // Since index recovery is async, we can't prove index recovery will never occur, just that it doesn't occur within some reasonable\n+        // amount of time\n+        assertFalse(\n+            \"Did not expect dangling index \" + INDEX_NAME + \" to be recovered\",\n+            waitUntil(() -> indexExists(INDEX_NAME), 1, TimeUnit.SECONDS)\n+        );\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().startNodes(3, buildSettings(0, true, false));\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME);\n+\n+        final ListDanglingIndicesResponse response = client().admin()\n+            .cluster()\n+            .listDanglingIndices(new ListDanglingIndicesRequest())\n+            .actionGet();\n+        assertThat(response.status(), equalTo(RestStatus.OK));\n+\n+        final List<NodeListDanglingIndicesResponse> nodeResponses = response.getNodes();\n+        assertThat(\"Didn't get responses from all nodes\", nodeResponses, hasSize(3));\n+\n+        for (NodeListDanglingIndicesResponse nodeResponse : nodeResponses) {\n+            if (nodeResponse.getNode().getName().equals(stoppedNodeName)) {\n+                assertThat(\"Expected node that was stopped to have one dangling index\", nodeResponse.getDanglingIndices(), hasSize(1));\n+\n+                final DanglingIndexInfo danglingIndexInfo = nodeResponse.getDanglingIndices().get(0);\n+                assertThat(danglingIndexInfo.getIndexName(), equalTo(INDEX_NAME));\n+            } else {\n+                assertThat(\"Expected node that was never stopped to have no dangling indices\", nodeResponse.getDanglingIndices(), empty());\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Check that dangling indices can be imported.\n+     */\n+    public void testDanglingIndicesCanBeImported() throws Exception {\n+        internalCluster().startNodes(3, buildSettings(0, true, false));\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME);\n+\n+        final String danglingIndexUUID = findDanglingIndexForNode(stoppedNodeName, INDEX_NAME);\n+\n+        final ImportDanglingIndexRequest request = new ImportDanglingIndexRequest(danglingIndexUUID, true);\n+\n+        client().admin().cluster().importDanglingIndex(request).actionGet();\n+\n+        assertTrue(\"Expected dangling index \" + INDEX_NAME + \" to be recovered\", indexExists(INDEX_NAME));\n+    }\n+\n+    /**\n+     * Check that the when sending an import-dangling-indices request, the specified UUIDs are validated as\n+     * being dangling.\n+     */\n+    public void testDanglingIndicesMustExistToBeImported() {\n+        internalCluster().startNodes(1, buildSettings(0, true, false));\n+\n+        final ImportDanglingIndexRequest request = new ImportDanglingIndexRequest(\"NonExistentUUID\", true);\n+\n+        boolean noExceptionThrown = false;\n+        try {\n+            client().admin().cluster().importDanglingIndex(request).actionGet();\n+            noExceptionThrown = true;\n+        } catch (Exception e) {\n+            assertThat(e, instanceOf(IllegalArgumentException.class));\n+            assertThat(e.getMessage(), containsString(\"No dangling index found for UUID [NonExistentUUID]\"));\n+        }\n+\n+        assertFalse(\"No exception thrown\", noExceptionThrown);\n+    }\n+\n+    /**\n+     * Check that a dangling index can only be imported if \"accept_data_loss\" is set to true.\n+     */\n+    public void testMustAcceptDataLossToImportDanglingIndex() throws Exception {\n+        internalCluster().startNodes(3, buildSettings(0, true, false));\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME);\n+        final String danglingIndexUUID = findDanglingIndexForNode(stoppedNodeName, INDEX_NAME);\n+\n+        final ImportDanglingIndexRequest request = new ImportDanglingIndexRequest(danglingIndexUUID, false);\n+\n+        Exception caughtException = null;\n+\n+        try {\n+            client().admin().cluster().importDanglingIndex(request).actionGet();\n+        } catch (Exception e) {\n+            caughtException = e;\n+        }\n+\n+        assertNotNull(\"No exception thrown\", caughtException);\n+        assertThat(caughtException.getMessage(), containsString(\"accept_data_loss must be set to true\"));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be deleted. Since this requires that\n+     * we add an entry to the index graveyard, the graveyard size must be\n+     * greater than 1. To test deletes, we set the index graveyard size to\n+     * 1, then create two indices and delete them both while one node in\n+     * the cluster is stopped. The deletion of the second pushes the deletion\n+     * of the first out of the graveyard. When the stopped node is resumed,\n+     * only the second index will be found into the graveyard and the the\n+     * other will be considered dangling, and can therefore be listed and\n+     * deleted through the API\n+     */\n+    public void testDanglingIndexCanBeDeleted() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a2ce053cf3111ba6b637778f08231c71289f0504"}, "originalPosition": 262}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6becf55126c5fde45f9a068ce0552ba72b0f8cd2", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/6becf55126c5fde45f9a068ce0552ba72b0f8cd2", "committedDate": "2020-04-29T14:49:32Z", "message": "Address review feedback"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7e6c1bd91208a2a3585746f5409d2756815fe60b", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/7e6c1bd91208a2a3585746f5409d2756815fe60b", "committedDate": "2020-04-29T14:54:21Z", "message": "More tweaks"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3187, "cost": 1, "resetAt": "2021-10-28T18:54:27Z"}}}