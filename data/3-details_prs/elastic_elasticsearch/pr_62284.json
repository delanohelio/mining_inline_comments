{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDg2Mjc1NjIx", "number": 62284, "title": "Make SLM Run Snapshot Deletes in Parallel", "bodyText": "Now that we support parallel snapshot and delete operations, there\nis no reason for SLM to wait for any snapshots to finish before starting\na delete. Deletes themselves can also be executed in parallel and in arbitrary\norder.\nThis commit makes use of parallel snapshot operations by removing all waits on\nother operations to finish. I tried to keep the change-set limited here so the\nfollowing follow ups were not included in this PR:\n\nUse snapshot multi-delete instead of looping over individual deletes\n\nThis isn't super important now as the deletes are internally batched into\none or two deletes anyway but requires a number of changes with the way\nstats are tracked currently)\n\n\nDeprecate and remove the time bound setting for the retention run.\n\nThis setting doesn't really make sense when all deletes run in parallel.\nFor one, there is no actual point at which to check for stopping the delete operation\nany longer. Also, multiple deletes in parallel tend to (in most cases) run about as\nfast as a single delete internally making the limit irrelevant sine we would always try\nand run at least one delete.\n\n\n\nRelates #59655", "createdAt": "2020-09-14T01:26:16Z", "url": "https://github.com/elastic/elasticsearch/pull/62284", "merged": true, "mergeCommit": {"oid": "fc97dcc6b11a31596208ebc4bcc7a74bb3a4e029"}, "closed": true, "closedAt": "2020-10-14T06:34:55Z", "author": {"login": "original-brownbear"}, "timelineItems": {"totalCount": 17, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdIovkBgH2gAyNDg2Mjc1NjIxOjY2ZmVmYzNhODYzY2RiNDRmMTdlZTE5MWM0ZDI2NzU0MmRlOWRkNmU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdSWH7zgH2gAyNDg2Mjc1NjIxOjZiN2U3ODc5MTU2ZmU5NjY0MDE4YWI1Mjc2MzFiMmRhNDQ4YzkxY2Q=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "66fefc3a863cdb44f17ee191c4d267542de9dd6e", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/66fefc3a863cdb44f17ee191c4d267542de9dd6e", "committedDate": "2020-09-14T01:16:47Z", "message": "Make SLM Run Snapshot Deletes in Parallel\n\nNow that we support parallel snapshot and delete operations, there\nis no reason for SLM to wait for any snapshots to finish before starting\na delete. Deletes themselves can also be executed in parallel and in arbitrary\norder.\nThis commit makes use of parallel snapshot operations by removing all waits on\nother operations to finish. I tried to keep the changecount limited here so the\nfollowing follow ups were not included in this PR:\n* Use snapshot multi-delete instead of looping over individual deletes\n  * This isn't super important now as the deletes are internally batched into\none or two deletes anyway but requires a number of changes with the way\nstats are tracked currently)\n* Deprecate and remove the time bound setting for the retention run.\n  * This setting doesn't really make sense when all deletes run in parallel.\nFor one, there is no actual point at which to check for stopping the delete operation\nany longer. Also, multiple deletes in parallel tend to (in most cases) run about as\nfast as a single delete internally making the limit irrelevant sine we would always try\nand run at least one delete.\n\nRelates #59655"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d5974ca318fae59c41f9706b0756637ac557b558", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/d5974ca318fae59c41f9706b0756637ac557b558", "committedDate": "2020-09-14T01:27:21Z", "message": "formatting"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fd36d0507c3533ce4e8faf809bedbceaf071aed6", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/fd36d0507c3533ce4e8faf809bedbceaf071aed6", "committedDate": "2020-09-14T01:27:56Z", "message": "formatting"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg3NDI2MTcx", "url": "https://github.com/elastic/elasticsearch/pull/62284#pullrequestreview-487426171", "createdAt": "2020-09-14T05:33:44Z", "commit": {"oid": "fd36d0507c3533ce4e8faf809bedbceaf071aed6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwNTozMzo0NFrOHREcvg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwNTozMzo0NFrOHREcvg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzY2MDczNA==", "bodyText": "Technically repo cleanup still isn't a concurrent operation and neither is deleting a snapshot that is currently being restored.\nI don't think these two cases are worth keeping this complexity around though. Both cases are kind of fringe and also the current way of checking for the cluster state not having anything conflicting in-progress isn't bullet proof to begin with (lots of possible races between checking the CS and actually starting the delete). => I think this is the reasonable simplification.", "url": "https://github.com/elastic/elasticsearch/pull/62284#discussion_r487660734", "createdAt": "2020-09-14T05:33:44Z", "author": {"login": "original-brownbear"}, "path": "x-pack/plugin/ilm/src/main/java/org/elasticsearch/xpack/slm/SnapshotRetentionTask.java", "diffHunk": "@@ -414,111 +343,22 @@ void deleteSnapshots(Map<String, List<SnapshotInfo>> snapshotsToDelete,\n     void deleteSnapshot(String slmPolicy, String repo, SnapshotId snapshot, SnapshotLifecycleStats slmStats,\n                         ActionListener<AcknowledgedResponse> listener) {\n         logger.info(\"[{}] snapshot retention deleting snapshot [{}]\", repo, snapshot);\n-        CountDownLatch latch = new CountDownLatch(1);\n-        client.admin().cluster().prepareDeleteSnapshot(repo, snapshot.getName())\n-            .execute(new LatchedActionListener<>(ActionListener.wrap(acknowledgedResponse -> {\n-                    if (acknowledgedResponse.isAcknowledged()) {\n-                        logger.debug(\"[{}] snapshot [{}] deleted successfully\", repo, snapshot);\n-                    } else {\n-                        logger.warn(\"[{}] snapshot [{}] delete issued but the request was not acknowledged\", repo, snapshot);\n-                    }\n+        client.admin().cluster().prepareDeleteSnapshot(repo, snapshot.getName()).execute(ActionListener.wrap(acknowledgedResponse -> {\n                     slmStats.snapshotDeleted(slmPolicy);\n                     listener.onResponse(acknowledgedResponse);\n                 },\n                 e -> {\n-                    logger.warn(new ParameterizedMessage(\"[{}] failed to delete snapshot [{}] for retention\",\n-                        repo, snapshot), e);\n-                    slmStats.snapshotDeleteFailure(slmPolicy);\n-                    listener.onFailure(e);\n-                }), latch));\n-        try {\n-            // Deletes cannot occur simultaneously, so wait for this\n-            // deletion to complete before attempting the next one\n-            latch.await();\n-        } catch (InterruptedException e) {\n-            logger.error(new ParameterizedMessage(\"[{}] deletion of snapshot [{}] interrupted\",\n-                repo, snapshot), e);\n-            listener.onFailure(e);\n-            slmStats.snapshotDeleteFailure(slmPolicy);\n-        }\n+                    try {\n+                        logger.warn(new ParameterizedMessage(\"[{}] failed to delete snapshot [{}] for retention\",\n+                                repo, snapshot), e);\n+                        slmStats.snapshotDeleteFailure(slmPolicy);\n+                    } finally {\n+                        listener.onFailure(e);\n+                    }\n+                }));\n     }\n \n     void updateStateWithStats(SnapshotLifecycleStats newStats) {\n         clusterService.submitStateUpdateTask(\"update_slm_stats\", new UpdateSnapshotLifecycleStatsTask(newStats));\n     }\n-\n-    public static boolean okayToDeleteSnapshots(ClusterState state) {\n-        // Cannot delete during a snapshot\n-        if (state.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY).entries().size() > 0) {\n-            logger.trace(\"deletion cannot proceed as there are snapshots in progress\");\n-            return false;\n-        }\n-\n-        // Cannot delete during an existing delete\n-        if (state.custom(SnapshotDeletionsInProgress.TYPE, SnapshotDeletionsInProgress.EMPTY).hasDeletionsInProgress()) {\n-            logger.trace(\"deletion cannot proceed as there are snapshot deletions in progress\");\n-            return false;\n-        }\n-\n-        // Cannot delete while a repository is being cleaned\n-        if (state.custom(RepositoryCleanupInProgress.TYPE, RepositoryCleanupInProgress.EMPTY).hasCleanupInProgress()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fd36d0507c3533ce4e8faf809bedbceaf071aed6"}, "originalPosition": 432}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg3NDI2Mjk4", "url": "https://github.com/elastic/elasticsearch/pull/62284#pullrequestreview-487426298", "createdAt": "2020-09-14T05:34:10Z", "commit": {"oid": "fd36d0507c3533ce4e8faf809bedbceaf071aed6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwNTozNDoxMFrOHREdNQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwNTozNDoxMFrOHREdNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzY2MDg1Mw==", "bodyText": "These tests seemed irrelevant with the removal of the time bound on delete runs.", "url": "https://github.com/elastic/elasticsearch/pull/62284#discussion_r487660853", "createdAt": "2020-09-14T05:34:10Z", "author": {"login": "original-brownbear"}, "path": "x-pack/plugin/ilm/src/test/java/org/elasticsearch/xpack/slm/SnapshotRetentionTaskTests.java", "diffHunk": "@@ -239,154 +223,6 @@ private void retentionTaskTest(final boolean deletionSuccess) throws Exception {\n         }\n     }\n \n-    public void testSuccessfulTimeBoundedDeletion() throws Exception {\n-        timeBoundedDeletion(true);\n-    }\n-\n-    public void testFailureTimeBoundedDeletion() throws Exception {\n-        timeBoundedDeletion(false);\n-    }\n-\n-    private void timeBoundedDeletion(final boolean deletionSuccess) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fd36d0507c3533ce4e8faf809bedbceaf071aed6"}, "originalPosition": 71}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg3NDI2OTc3", "url": "https://github.com/elastic/elasticsearch/pull/62284#pullrequestreview-487426977", "createdAt": "2020-09-14T05:36:23Z", "commit": {"oid": "fd36d0507c3533ce4e8faf809bedbceaf071aed6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwNTozNjoyNFrOHREfZA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwNTozNjoyNFrOHREfZA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzY2MTQxMg==", "bodyText": "This flag seems unnecessary with concurrent snapshots. If (unlikely case now anyway) two delete runs collide and re-submit the same snapshot(s) for deletion the delete functionality will de-duplicate those deletes => we don't save anything by having this guard IMO and it's just more complexity (especially with the necessary changes in here making all the APIs async)", "url": "https://github.com/elastic/elasticsearch/pull/62284#discussion_r487661412", "createdAt": "2020-09-14T05:36:24Z", "author": {"login": "original-brownbear"}, "path": "x-pack/plugin/ilm/src/main/java/org/elasticsearch/xpack/slm/SnapshotRetentionTask.java", "diffHunk": "@@ -65,21 +54,18 @@\n public class SnapshotRetentionTask implements SchedulerEngine.Listener {\n \n     private static final Logger logger = LogManager.getLogger(SnapshotRetentionTask.class);\n-    private static final AtomicBoolean running = new AtomicBoolean(false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fd36d0507c3533ce4e8faf809bedbceaf071aed6"}, "originalPosition": 43}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg3NDI3NTI1", "url": "https://github.com/elastic/elasticsearch/pull/62284#pullrequestreview-487427525", "createdAt": "2020-09-14T05:38:07Z", "commit": {"oid": "fd36d0507c3533ce4e8faf809bedbceaf071aed6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwNTozODowN1rOHREhLw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwNTozODowN1rOHREhLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzY2MTg3MQ==", "bodyText": "All the other changes that make methods async by adding ActionListeners come from removing the blocking here. I tried to not make any logical changes outside of moving to async and to keep those changes purely mechanical.", "url": "https://github.com/elastic/elasticsearch/pull/62284#discussion_r487661871", "createdAt": "2020-09-14T05:38:07Z", "author": {"login": "original-brownbear"}, "path": "x-pack/plugin/ilm/src/main/java/org/elasticsearch/xpack/slm/SnapshotRetentionTask.java", "diffHunk": "@@ -290,117 +264,72 @@ static String getPolicyId(SnapshotInfo snapshotInfo) {\n                 \" to have a policy in its metadata, but it did not\"));\n     }\n \n-    /**\n-     * Maybe delete the given snapshots. If a snapshot is currently running according to the cluster\n-     * state, this waits (using a {@link ClusterStateObserver} until a cluster state with no running\n-     * snapshots before executing the blocking\n-     * {@link #deleteSnapshots(Map, TimeValue, SnapshotLifecycleStats)} request. At most, we wait\n-     * for the maximum allowed deletion time before timing out waiting for a state with no\n-     * running snapshots.\n-     *\n-     * It's possible the task may still run into a SnapshotInProgressException, if a snapshot is\n-     * started between the state retrieved here and the actual deletion. Since is is expected to be\n-     * a rare case, no special handling is present.\n-     */\n-    private void maybeDeleteSnapshots(Map<String, List<SnapshotInfo>> snapshotsToDelete,\n-                                      TimeValue maximumTime,\n-                                      SnapshotLifecycleStats slmStats) {\n+    void deleteSnapshots(Map<String, List<SnapshotInfo>> snapshotsToDelete,\n+                         SnapshotLifecycleStats slmStats,\n+                         ActionListener<Void> listener) {\n         int count = snapshotsToDelete.values().stream().mapToInt(List::size).sum();\n         if (count == 0) {\n             logger.debug(\"no snapshots are eligible for deletion\");\n             return;\n         }\n \n-        ClusterState state = clusterService.state();\n-        if (okayToDeleteSnapshots(state)) {\n-            logger.trace(\"there are no snapshots currently running, proceeding with snapshot deletion of [{}]\",\n-                formatSnapshots(snapshotsToDelete));\n-            deleteSnapshots(snapshotsToDelete, maximumTime, slmStats);\n-        } else {\n-            logger.debug(\"a snapshot is currently running, rescheduling SLM retention for after snapshot has completed\");\n-            ClusterStateObserver observer = new ClusterStateObserver(clusterService, maximumTime, logger, threadPool.getThreadContext());\n-            CountDownLatch latch = new CountDownLatch(1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fd36d0507c3533ce4e8faf809bedbceaf071aed6"}, "originalPosition": 237}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg3NDI3ODA2", "url": "https://github.com/elastic/elasticsearch/pull/62284#pullrequestreview-487427806", "createdAt": "2020-09-14T05:38:54Z", "commit": {"oid": "fd36d0507c3533ce4e8faf809bedbceaf071aed6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwNTozODo1NFrOHREiBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwNTozODo1NFrOHREiBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzY2MjA4NA==", "bodyText": "This is irrelevant now, the loop over the deletes doesn't block so we will never break the time limit practically.", "url": "https://github.com/elastic/elasticsearch/pull/62284#discussion_r487662084", "createdAt": "2020-09-14T05:38:54Z", "author": {"login": "original-brownbear"}, "path": "x-pack/plugin/ilm/src/main/java/org/elasticsearch/xpack/slm/SnapshotRetentionTask.java", "diffHunk": "@@ -290,117 +264,72 @@ static String getPolicyId(SnapshotInfo snapshotInfo) {\n                 \" to have a policy in its metadata, but it did not\"));\n     }\n \n-    /**\n-     * Maybe delete the given snapshots. If a snapshot is currently running according to the cluster\n-     * state, this waits (using a {@link ClusterStateObserver} until a cluster state with no running\n-     * snapshots before executing the blocking\n-     * {@link #deleteSnapshots(Map, TimeValue, SnapshotLifecycleStats)} request. At most, we wait\n-     * for the maximum allowed deletion time before timing out waiting for a state with no\n-     * running snapshots.\n-     *\n-     * It's possible the task may still run into a SnapshotInProgressException, if a snapshot is\n-     * started between the state retrieved here and the actual deletion. Since is is expected to be\n-     * a rare case, no special handling is present.\n-     */\n-    private void maybeDeleteSnapshots(Map<String, List<SnapshotInfo>> snapshotsToDelete,\n-                                      TimeValue maximumTime,\n-                                      SnapshotLifecycleStats slmStats) {\n+    void deleteSnapshots(Map<String, List<SnapshotInfo>> snapshotsToDelete,\n+                         SnapshotLifecycleStats slmStats,\n+                         ActionListener<Void> listener) {\n         int count = snapshotsToDelete.values().stream().mapToInt(List::size).sum();\n         if (count == 0) {\n             logger.debug(\"no snapshots are eligible for deletion\");\n             return;\n         }\n \n-        ClusterState state = clusterService.state();\n-        if (okayToDeleteSnapshots(state)) {\n-            logger.trace(\"there are no snapshots currently running, proceeding with snapshot deletion of [{}]\",\n-                formatSnapshots(snapshotsToDelete));\n-            deleteSnapshots(snapshotsToDelete, maximumTime, slmStats);\n-        } else {\n-            logger.debug(\"a snapshot is currently running, rescheduling SLM retention for after snapshot has completed\");\n-            ClusterStateObserver observer = new ClusterStateObserver(clusterService, maximumTime, logger, threadPool.getThreadContext());\n-            CountDownLatch latch = new CountDownLatch(1);\n-            observer.waitForNextChange(\n-                new NoSnapshotRunningListener(observer,\n-                    newState -> threadPool.executor(ThreadPool.Names.MANAGEMENT).execute(() -> {\n-                        try {\n-                            logger.trace(\"received cluster state without running snapshots, proceeding with snapshot deletion of [{}]\",\n-                                formatSnapshots(snapshotsToDelete));\n-                            deleteSnapshots(snapshotsToDelete, maximumTime, slmStats);\n-                        } finally {\n-                            latch.countDown();\n-                        }\n-                    }),\n-                    e -> {\n-                        latch.countDown();\n-                        throw new ElasticsearchException(e);\n-                    }));\n-            try {\n-                logger.trace(\"waiting for snapshot deletion to complete\");\n-                // Wait until we find a cluster state not running a snapshot operation.\n-                // If we can't find one within a day, give up and throw an error.\n-                latch.await(1, TimeUnit.DAYS);\n-                logger.trace(\"deletion complete\");\n-            } catch (InterruptedException e) {\n-                throw new ElasticsearchException(e);\n-            }\n-        }\n-    }\n-\n-    void deleteSnapshots(Map<String, List<SnapshotInfo>> snapshotsToDelete,\n-                         TimeValue maximumTime,\n-                         SnapshotLifecycleStats slmStats) {\n-        int count = snapshotsToDelete.values().stream().mapToInt(List::size).sum();\n-\n         logger.info(\"starting snapshot retention deletion for [{}] snapshots\", count);\n         long startTime = nowNanoSupplier.getAsLong();\n-        final AtomicInteger deleted =  new AtomicInteger(0);\n+        final AtomicInteger deleted = new AtomicInteger(0);\n         final AtomicInteger failed = new AtomicInteger(0);\n+        final GroupedActionListener<Void> allDeletesListener =\n+                new GroupedActionListener<>(ActionListener.runAfter(ActionListener.map(listener, v -> null),\n+                        () -> {\n+                            TimeValue totalElapsedTime = TimeValue.timeValueNanos(nowNanoSupplier.getAsLong() - startTime);\n+                            logger.debug(\"total elapsed time for deletion of [{}] snapshots: {}\", deleted, totalElapsedTime);\n+                            slmStats.deletionTime(totalElapsedTime);\n+                        }), snapshotsToDelete.size());\n         for (Map.Entry<String, List<SnapshotInfo>> entry : snapshotsToDelete.entrySet()) {\n             String repo = entry.getKey();\n             List<SnapshotInfo> snapshots = entry.getValue();\n-            for (SnapshotInfo info : snapshots) {\n-                final String policyId = getPolicyId(info);\n-                final long deleteStartTime = nowNanoSupplier.getAsLong();\n-                // TODO: Use snapshot multi-delete instead of this loop if all nodes in the cluster support it\n-                //       i.e are newer or equal to SnapshotsService#MULTI_DELETE_VERSION\n-                deleteSnapshot(policyId, repo, info.snapshotId(), slmStats, ActionListener.wrap(acknowledgedResponse -> {\n-                    deleted.incrementAndGet();\n-                    assert acknowledgedResponse.isAcknowledged();\n-                    historyStore.putAsync(SnapshotHistoryItem.deletionSuccessRecord(Instant.now().toEpochMilli(),\n-                            info.snapshotId().getName(), policyId, repo));\n-                }, e -> {\n-                    failed.incrementAndGet();\n-                    try {\n-                        final SnapshotHistoryItem result = SnapshotHistoryItem.deletionFailureRecord(Instant.now().toEpochMilli(),\n-                            info.snapshotId().getName(), policyId, repo, e);\n-                        historyStore.putAsync(result);\n-                    } catch (IOException ex) {\n-                        // This shouldn't happen unless there's an issue with serializing the original exception\n-                        logger.error(new ParameterizedMessage(\n-                            \"failed to record snapshot deletion failure for snapshot lifecycle policy [{}]\",\n-                            policyId), ex);\n-                    }\n-                }));\n-                // Check whether we have exceeded the maximum time allowed to spend deleting\n-                // snapshots, if we have, short-circuit the rest of the deletions\n-                long finishTime = nowNanoSupplier.getAsLong();\n-                TimeValue deletionTime = TimeValue.timeValueNanos(finishTime - deleteStartTime);\n-                logger.debug(\"elapsed time for deletion of [{}] snapshot: {}\", info.snapshotId(), deletionTime);\n-                TimeValue totalDeletionTime = TimeValue.timeValueNanos(finishTime - startTime);\n-                if (totalDeletionTime.compareTo(maximumTime) > 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fd36d0507c3533ce4e8faf809bedbceaf071aed6"}, "originalPosition": 314}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "30b61b5a3ffec9673e19e347ad7899c0864f5d80", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/30b61b5a3ffec9673e19e347ad7899c0864f5d80", "committedDate": "2020-09-14T08:04:49Z", "message": "Merge remote-tracking branch 'elastic/master' into slm-start-deleting-in-parallel"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk3ODg0NDkz", "url": "https://github.com/elastic/elasticsearch/pull/62284#pullrequestreview-497884493", "createdAt": "2020-09-28T20:48:34Z", "commit": {"oid": "30b61b5a3ffec9673e19e347ad7899c0864f5d80"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQyMDo0ODozNVrOHZPEOA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQyMDo1NzowOFrOHZPVcg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjIyMzI4OA==", "bodyText": "This now needs to call the listener onResponse handler", "url": "https://github.com/elastic/elasticsearch/pull/62284#discussion_r496223288", "createdAt": "2020-09-28T20:48:35Z", "author": {"login": "dakrone"}, "path": "x-pack/plugin/ilm/src/main/java/org/elasticsearch/xpack/slm/SnapshotRetentionTask.java", "diffHunk": "@@ -290,117 +264,72 @@ static String getPolicyId(SnapshotInfo snapshotInfo) {\n                 \" to have a policy in its metadata, but it did not\"));\n     }\n \n-    /**\n-     * Maybe delete the given snapshots. If a snapshot is currently running according to the cluster\n-     * state, this waits (using a {@link ClusterStateObserver} until a cluster state with no running\n-     * snapshots before executing the blocking\n-     * {@link #deleteSnapshots(Map, TimeValue, SnapshotLifecycleStats)} request. At most, we wait\n-     * for the maximum allowed deletion time before timing out waiting for a state with no\n-     * running snapshots.\n-     *\n-     * It's possible the task may still run into a SnapshotInProgressException, if a snapshot is\n-     * started between the state retrieved here and the actual deletion. Since is is expected to be\n-     * a rare case, no special handling is present.\n-     */\n-    private void maybeDeleteSnapshots(Map<String, List<SnapshotInfo>> snapshotsToDelete,\n-                                      TimeValue maximumTime,\n-                                      SnapshotLifecycleStats slmStats) {\n+    void deleteSnapshots(Map<String, List<SnapshotInfo>> snapshotsToDelete,\n+                         SnapshotLifecycleStats slmStats,\n+                         ActionListener<Void> listener) {\n         int count = snapshotsToDelete.values().stream().mapToInt(List::size).sum();\n         if (count == 0) {\n             logger.debug(\"no snapshots are eligible for deletion\");\n             return;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "30b61b5a3ffec9673e19e347ad7899c0864f5d80"}, "originalPosition": 226}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjIyNDAyMg==", "bodyText": "This comment is out of date now", "url": "https://github.com/elastic/elasticsearch/pull/62284#discussion_r496224022", "createdAt": "2020-09-28T20:50:06Z", "author": {"login": "dakrone"}, "path": "x-pack/plugin/ilm/src/main/java/org/elasticsearch/xpack/slm/SnapshotRetentionTask.java", "diffHunk": "@@ -290,117 +264,72 @@ static String getPolicyId(SnapshotInfo snapshotInfo) {\n                 \" to have a policy in its metadata, but it did not\"));\n     }\n \n-    /**\n-     * Maybe delete the given snapshots. If a snapshot is currently running according to the cluster\n-     * state, this waits (using a {@link ClusterStateObserver} until a cluster state with no running\n-     * snapshots before executing the blocking\n-     * {@link #deleteSnapshots(Map, TimeValue, SnapshotLifecycleStats)} request. At most, we wait\n-     * for the maximum allowed deletion time before timing out waiting for a state with no\n-     * running snapshots.\n-     *\n-     * It's possible the task may still run into a SnapshotInProgressException, if a snapshot is\n-     * started between the state retrieved here and the actual deletion. Since is is expected to be\n-     * a rare case, no special handling is present.\n-     */\n-    private void maybeDeleteSnapshots(Map<String, List<SnapshotInfo>> snapshotsToDelete,\n-                                      TimeValue maximumTime,\n-                                      SnapshotLifecycleStats slmStats) {\n+    void deleteSnapshots(Map<String, List<SnapshotInfo>> snapshotsToDelete,\n+                         SnapshotLifecycleStats slmStats,\n+                         ActionListener<Void> listener) {\n         int count = snapshotsToDelete.values().stream().mapToInt(List::size).sum();\n         if (count == 0) {\n             logger.debug(\"no snapshots are eligible for deletion\");\n             return;\n         }\n \n-        ClusterState state = clusterService.state();\n-        if (okayToDeleteSnapshots(state)) {\n-            logger.trace(\"there are no snapshots currently running, proceeding with snapshot deletion of [{}]\",\n-                formatSnapshots(snapshotsToDelete));\n-            deleteSnapshots(snapshotsToDelete, maximumTime, slmStats);\n-        } else {\n-            logger.debug(\"a snapshot is currently running, rescheduling SLM retention for after snapshot has completed\");\n-            ClusterStateObserver observer = new ClusterStateObserver(clusterService, maximumTime, logger, threadPool.getThreadContext());\n-            CountDownLatch latch = new CountDownLatch(1);\n-            observer.waitForNextChange(\n-                new NoSnapshotRunningListener(observer,\n-                    newState -> threadPool.executor(ThreadPool.Names.MANAGEMENT).execute(() -> {\n-                        try {\n-                            logger.trace(\"received cluster state without running snapshots, proceeding with snapshot deletion of [{}]\",\n-                                formatSnapshots(snapshotsToDelete));\n-                            deleteSnapshots(snapshotsToDelete, maximumTime, slmStats);\n-                        } finally {\n-                            latch.countDown();\n-                        }\n-                    }),\n-                    e -> {\n-                        latch.countDown();\n-                        throw new ElasticsearchException(e);\n-                    }));\n-            try {\n-                logger.trace(\"waiting for snapshot deletion to complete\");\n-                // Wait until we find a cluster state not running a snapshot operation.\n-                // If we can't find one within a day, give up and throw an error.\n-                latch.await(1, TimeUnit.DAYS);\n-                logger.trace(\"deletion complete\");\n-            } catch (InterruptedException e) {\n-                throw new ElasticsearchException(e);\n-            }\n-        }\n-    }\n-\n-    void deleteSnapshots(Map<String, List<SnapshotInfo>> snapshotsToDelete,\n-                         TimeValue maximumTime,\n-                         SnapshotLifecycleStats slmStats) {\n-        int count = snapshotsToDelete.values().stream().mapToInt(List::size).sum();\n-\n         logger.info(\"starting snapshot retention deletion for [{}] snapshots\", count);\n         long startTime = nowNanoSupplier.getAsLong();\n-        final AtomicInteger deleted =  new AtomicInteger(0);\n+        final AtomicInteger deleted = new AtomicInteger(0);\n         final AtomicInteger failed = new AtomicInteger(0);\n+        final GroupedActionListener<Void> allDeletesListener =\n+                new GroupedActionListener<>(ActionListener.runAfter(ActionListener.map(listener, v -> null),\n+                        () -> {\n+                            TimeValue totalElapsedTime = TimeValue.timeValueNanos(nowNanoSupplier.getAsLong() - startTime);\n+                            logger.debug(\"total elapsed time for deletion of [{}] snapshots: {}\", deleted, totalElapsedTime);\n+                            slmStats.deletionTime(totalElapsedTime);\n+                        }), snapshotsToDelete.size());\n         for (Map.Entry<String, List<SnapshotInfo>> entry : snapshotsToDelete.entrySet()) {\n             String repo = entry.getKey();\n             List<SnapshotInfo> snapshots = entry.getValue();\n-            for (SnapshotInfo info : snapshots) {\n-                final String policyId = getPolicyId(info);\n-                final long deleteStartTime = nowNanoSupplier.getAsLong();\n-                // TODO: Use snapshot multi-delete instead of this loop if all nodes in the cluster support it\n-                //       i.e are newer or equal to SnapshotsService#MULTI_DELETE_VERSION\n-                deleteSnapshot(policyId, repo, info.snapshotId(), slmStats, ActionListener.wrap(acknowledgedResponse -> {\n-                    deleted.incrementAndGet();\n-                    assert acknowledgedResponse.isAcknowledged();\n-                    historyStore.putAsync(SnapshotHistoryItem.deletionSuccessRecord(Instant.now().toEpochMilli(),\n-                            info.snapshotId().getName(), policyId, repo));\n-                }, e -> {\n-                    failed.incrementAndGet();\n-                    try {\n-                        final SnapshotHistoryItem result = SnapshotHistoryItem.deletionFailureRecord(Instant.now().toEpochMilli(),\n-                            info.snapshotId().getName(), policyId, repo, e);\n-                        historyStore.putAsync(result);\n-                    } catch (IOException ex) {\n-                        // This shouldn't happen unless there's an issue with serializing the original exception\n-                        logger.error(new ParameterizedMessage(\n-                            \"failed to record snapshot deletion failure for snapshot lifecycle policy [{}]\",\n-                            policyId), ex);\n-                    }\n-                }));\n-                // Check whether we have exceeded the maximum time allowed to spend deleting\n-                // snapshots, if we have, short-circuit the rest of the deletions\n-                long finishTime = nowNanoSupplier.getAsLong();\n-                TimeValue deletionTime = TimeValue.timeValueNanos(finishTime - deleteStartTime);\n-                logger.debug(\"elapsed time for deletion of [{}] snapshot: {}\", info.snapshotId(), deletionTime);\n-                TimeValue totalDeletionTime = TimeValue.timeValueNanos(finishTime - startTime);\n-                if (totalDeletionTime.compareTo(maximumTime) > 0) {\n-                    logger.info(\"maximum snapshot retention deletion time reached, time spent: [{}],\" +\n-                            \" maximum allowed time: [{}], deleted [{}] out of [{}] snapshots scheduled for deletion, failed to delete [{}]\",\n-                        totalDeletionTime, maximumTime, deleted, count, failed);\n-                    slmStats.deletionTime(totalDeletionTime);\n-                    slmStats.retentionTimedOut();\n-                    return;\n-                }\n-            }\n+            deleteSnapshots(slmStats, deleted, failed, repo, snapshots, allDeletesListener);\n+        }\n+    }\n+\n+    private void deleteSnapshots(SnapshotLifecycleStats slmStats, AtomicInteger deleted, AtomicInteger failed, String repo,\n+                                 List<SnapshotInfo> snapshots, ActionListener<Void> listener) {\n+\n+        final ActionListener<Void> allDeletesListener =\n+                new GroupedActionListener<>(ActionListener.map(listener, v -> null), snapshots.size());\n+        for (SnapshotInfo info : snapshots) {\n+            final String policyId = getPolicyId(info);\n+            final long deleteStartTime = nowNanoSupplier.getAsLong();\n+            // TODO: Use snapshot multi-delete instead of this loop if all nodes in the cluster support it\n+            //       i.e are newer or equal to SnapshotsService#MULTI_DELETE_VERSION\n+            deleteSnapshot(policyId, repo, info.snapshotId(), slmStats, ActionListener.runAfter(\n+                    ActionListener.wrap(acknowledgedResponse -> {\n+                        deleted.incrementAndGet();\n+                        assert acknowledgedResponse.isAcknowledged();\n+                        historyStore.putAsync(SnapshotHistoryItem.deletionSuccessRecord(Instant.now().toEpochMilli(),\n+                                info.snapshotId().getName(), policyId, repo));\n+                        allDeletesListener.onResponse(null);\n+                    }, e -> {\n+                        failed.incrementAndGet();\n+                        try {\n+                            final SnapshotHistoryItem result = SnapshotHistoryItem.deletionFailureRecord(Instant.now().toEpochMilli(),\n+                                    info.snapshotId().getName(), policyId, repo, e);\n+                            historyStore.putAsync(result);\n+                        } catch (IOException ex) {\n+                            // This shouldn't happen unless there's an issue with serializing the original exception\n+                            logger.error(new ParameterizedMessage(\n+                                    \"failed to record snapshot deletion failure for snapshot lifecycle policy [{}]\",\n+                                    policyId), ex);\n+                        } finally {\n+                            allDeletesListener.onFailure(e);\n+                        }\n+                    }), () -> {\n+                        // Check whether we have exceeded the maximum time allowed to spend deleting\n+                        // snapshots, if we have, short-circuit the rest of the deletions", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "30b61b5a3ffec9673e19e347ad7899c0864f5d80"}, "originalPosition": 360}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjIyNzY5OA==", "bodyText": "One concern I have here is stats counting, if SLM retention is super aggressive and deletes the same snapshot 15 times (let's assume it's spending forever deleting the snapshot) then it tells the client that it's succeeded 15 times right? that means we'd increment the deleted stats incorrectly for the actual number of snapshots that have been deleted", "url": "https://github.com/elastic/elasticsearch/pull/62284#discussion_r496227698", "createdAt": "2020-09-28T20:57:08Z", "author": {"login": "dakrone"}, "path": "x-pack/plugin/ilm/src/main/java/org/elasticsearch/xpack/slm/SnapshotRetentionTask.java", "diffHunk": "@@ -65,21 +54,18 @@\n public class SnapshotRetentionTask implements SchedulerEngine.Listener {\n \n     private static final Logger logger = LogManager.getLogger(SnapshotRetentionTask.class);\n-    private static final AtomicBoolean running = new AtomicBoolean(false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzY2MTQxMg=="}, "originalCommit": {"oid": "fd36d0507c3533ce4e8faf809bedbceaf071aed6"}, "originalPosition": 43}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c1c7108c34697f7723366cac7469ab0f471aaef5", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/c1c7108c34697f7723366cac7469ab0f471aaef5", "committedDate": "2020-09-29T11:07:07Z", "message": "Merge remote-tracking branch 'elastic/master' into slm-start-deleting-in-parallel"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "83dbef89b26f4242cee800a4fc1e13a9195e40da", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/83dbef89b26f4242cee800a4fc1e13a9195e40da", "committedDate": "2020-09-29T11:43:29Z", "message": "CR: comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk4OTQzMDUy", "url": "https://github.com/elastic/elasticsearch/pull/62284#pullrequestreview-498943052", "createdAt": "2020-09-29T22:22:06Z", "commit": {"oid": "83dbef89b26f4242cee800a4fc1e13a9195e40da"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMjoyMjowNlrOHaEQdA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMjoyODoxN1rOHaEZ_Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA5NDc3Mg==", "bodyText": "Can you move this up to the top before all the methods? It's a little strange to have a private class var randomly in the middle of the class", "url": "https://github.com/elastic/elasticsearch/pull/62284#discussion_r497094772", "createdAt": "2020-09-29T22:22:06Z", "author": {"login": "dakrone"}, "path": "x-pack/plugin/ilm/src/main/java/org/elasticsearch/xpack/slm/SnapshotRetentionTask.java", "diffHunk": "@@ -290,117 +265,81 @@ static String getPolicyId(SnapshotInfo snapshotInfo) {\n                 \" to have a policy in its metadata, but it did not\"));\n     }\n \n-    /**\n-     * Maybe delete the given snapshots. If a snapshot is currently running according to the cluster\n-     * state, this waits (using a {@link ClusterStateObserver} until a cluster state with no running\n-     * snapshots before executing the blocking\n-     * {@link #deleteSnapshots(Map, TimeValue, SnapshotLifecycleStats)} request. At most, we wait\n-     * for the maximum allowed deletion time before timing out waiting for a state with no\n-     * running snapshots.\n-     *\n-     * It's possible the task may still run into a SnapshotInProgressException, if a snapshot is\n-     * started between the state retrieved here and the actual deletion. Since is is expected to be\n-     * a rare case, no special handling is present.\n-     */\n-    private void maybeDeleteSnapshots(Map<String, List<SnapshotInfo>> snapshotsToDelete,\n-                                      TimeValue maximumTime,\n-                                      SnapshotLifecycleStats slmStats) {\n+    void deleteSnapshots(Map<String, List<SnapshotInfo>> snapshotsToDelete,\n+                         SnapshotLifecycleStats slmStats,\n+                         ActionListener<Void> listener) {\n         int count = snapshotsToDelete.values().stream().mapToInt(List::size).sum();\n         if (count == 0) {\n+            listener.onResponse(null);\n             logger.debug(\"no snapshots are eligible for deletion\");\n             return;\n         }\n \n-        ClusterState state = clusterService.state();\n-        if (okayToDeleteSnapshots(state)) {\n-            logger.trace(\"there are no snapshots currently running, proceeding with snapshot deletion of [{}]\",\n-                formatSnapshots(snapshotsToDelete));\n-            deleteSnapshots(snapshotsToDelete, maximumTime, slmStats);\n-        } else {\n-            logger.debug(\"a snapshot is currently running, rescheduling SLM retention for after snapshot has completed\");\n-            ClusterStateObserver observer = new ClusterStateObserver(clusterService, maximumTime, logger, threadPool.getThreadContext());\n-            CountDownLatch latch = new CountDownLatch(1);\n-            observer.waitForNextChange(\n-                new NoSnapshotRunningListener(observer,\n-                    newState -> threadPool.executor(ThreadPool.Names.MANAGEMENT).execute(() -> {\n-                        try {\n-                            logger.trace(\"received cluster state without running snapshots, proceeding with snapshot deletion of [{}]\",\n-                                formatSnapshots(snapshotsToDelete));\n-                            deleteSnapshots(snapshotsToDelete, maximumTime, slmStats);\n-                        } finally {\n-                            latch.countDown();\n-                        }\n-                    }),\n-                    e -> {\n-                        latch.countDown();\n-                        throw new ElasticsearchException(e);\n-                    }));\n-            try {\n-                logger.trace(\"waiting for snapshot deletion to complete\");\n-                // Wait until we find a cluster state not running a snapshot operation.\n-                // If we can't find one within a day, give up and throw an error.\n-                latch.await(1, TimeUnit.DAYS);\n-                logger.trace(\"deletion complete\");\n-            } catch (InterruptedException e) {\n-                throw new ElasticsearchException(e);\n-            }\n-        }\n-    }\n-\n-    void deleteSnapshots(Map<String, List<SnapshotInfo>> snapshotsToDelete,\n-                         TimeValue maximumTime,\n-                         SnapshotLifecycleStats slmStats) {\n-        int count = snapshotsToDelete.values().stream().mapToInt(List::size).sum();\n-\n         logger.info(\"starting snapshot retention deletion for [{}] snapshots\", count);\n         long startTime = nowNanoSupplier.getAsLong();\n-        final AtomicInteger deleted =  new AtomicInteger(0);\n+        final AtomicInteger deleted = new AtomicInteger(0);\n         final AtomicInteger failed = new AtomicInteger(0);\n+        final GroupedActionListener<Void> allDeletesListener =\n+                new GroupedActionListener<>(ActionListener.runAfter(ActionListener.map(listener, v -> null),\n+                        () -> {\n+                            TimeValue totalElapsedTime = TimeValue.timeValueNanos(nowNanoSupplier.getAsLong() - startTime);\n+                            logger.debug(\"total elapsed time for deletion of [{}] snapshots: {}\", deleted, totalElapsedTime);\n+                            slmStats.deletionTime(totalElapsedTime);\n+                        }), snapshotsToDelete.size());\n         for (Map.Entry<String, List<SnapshotInfo>> entry : snapshotsToDelete.entrySet()) {\n             String repo = entry.getKey();\n             List<SnapshotInfo> snapshots = entry.getValue();\n-            for (SnapshotInfo info : snapshots) {\n-                final String policyId = getPolicyId(info);\n-                final long deleteStartTime = nowNanoSupplier.getAsLong();\n-                // TODO: Use snapshot multi-delete instead of this loop if all nodes in the cluster support it\n-                //       i.e are newer or equal to SnapshotsService#MULTI_DELETE_VERSION\n-                deleteSnapshot(policyId, repo, info.snapshotId(), slmStats, ActionListener.wrap(acknowledgedResponse -> {\n-                    deleted.incrementAndGet();\n-                    assert acknowledgedResponse.isAcknowledged();\n-                    historyStore.putAsync(SnapshotHistoryItem.deletionSuccessRecord(Instant.now().toEpochMilli(),\n-                            info.snapshotId().getName(), policyId, repo));\n-                }, e -> {\n-                    failed.incrementAndGet();\n-                    try {\n-                        final SnapshotHistoryItem result = SnapshotHistoryItem.deletionFailureRecord(Instant.now().toEpochMilli(),\n-                            info.snapshotId().getName(), policyId, repo, e);\n-                        historyStore.putAsync(result);\n-                    } catch (IOException ex) {\n-                        // This shouldn't happen unless there's an issue with serializing the original exception\n-                        logger.error(new ParameterizedMessage(\n-                            \"failed to record snapshot deletion failure for snapshot lifecycle policy [{}]\",\n-                            policyId), ex);\n-                    }\n-                }));\n-                // Check whether we have exceeded the maximum time allowed to spend deleting\n-                // snapshots, if we have, short-circuit the rest of the deletions\n-                long finishTime = nowNanoSupplier.getAsLong();\n-                TimeValue deletionTime = TimeValue.timeValueNanos(finishTime - deleteStartTime);\n-                logger.debug(\"elapsed time for deletion of [{}] snapshot: {}\", info.snapshotId(), deletionTime);\n-                TimeValue totalDeletionTime = TimeValue.timeValueNanos(finishTime - startTime);\n-                if (totalDeletionTime.compareTo(maximumTime) > 0) {\n-                    logger.info(\"maximum snapshot retention deletion time reached, time spent: [{}],\" +\n-                            \" maximum allowed time: [{}], deleted [{}] out of [{}] snapshots scheduled for deletion, failed to delete [{}]\",\n-                        totalDeletionTime, maximumTime, deleted, count, failed);\n-                    slmStats.deletionTime(totalDeletionTime);\n-                    slmStats.retentionTimedOut();\n-                    return;\n-                }\n+            deleteSnapshots(slmStats, deleted, failed, repo, snapshots, allDeletesListener);\n+        }\n+    }\n+\n+    /**\n+     * Set of all currently deleting {@link SnapshotId} used to prevent starting multiple deletes for the same snapshot.\n+     */\n+    private final Set<SnapshotId> runningDeletions = Collections.synchronizedSet(new HashSet<>());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "83dbef89b26f4242cee800a4fc1e13a9195e40da"}, "originalPosition": 335}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA5NzIxMw==", "bodyText": "I think we leak memory (it would be very rare) if the next two lines (after the if) were to throw an exception.\nFor example, if getPolicyId(info) were to throw an exception (possible since it has an orElseThrow(...) clause) we'd never remove the id from the deletions, and we'd never be able to remove the snapshot because we'd always think we were currently removing it.\nNot sure what the best way to handle it is, but maybe we can just wrap the following lines to remove it otherwise? I still feel a little uneasy about deleteSnapshot potentially not calling the listener (for example, if its implementation were changed and it threw an exception rather than calling listener.onFailure(...))", "url": "https://github.com/elastic/elasticsearch/pull/62284#discussion_r497097213", "createdAt": "2020-09-29T22:28:17Z", "author": {"login": "dakrone"}, "path": "x-pack/plugin/ilm/src/main/java/org/elasticsearch/xpack/slm/SnapshotRetentionTask.java", "diffHunk": "@@ -290,117 +265,81 @@ static String getPolicyId(SnapshotInfo snapshotInfo) {\n                 \" to have a policy in its metadata, but it did not\"));\n     }\n \n-    /**\n-     * Maybe delete the given snapshots. If a snapshot is currently running according to the cluster\n-     * state, this waits (using a {@link ClusterStateObserver} until a cluster state with no running\n-     * snapshots before executing the blocking\n-     * {@link #deleteSnapshots(Map, TimeValue, SnapshotLifecycleStats)} request. At most, we wait\n-     * for the maximum allowed deletion time before timing out waiting for a state with no\n-     * running snapshots.\n-     *\n-     * It's possible the task may still run into a SnapshotInProgressException, if a snapshot is\n-     * started between the state retrieved here and the actual deletion. Since is is expected to be\n-     * a rare case, no special handling is present.\n-     */\n-    private void maybeDeleteSnapshots(Map<String, List<SnapshotInfo>> snapshotsToDelete,\n-                                      TimeValue maximumTime,\n-                                      SnapshotLifecycleStats slmStats) {\n+    void deleteSnapshots(Map<String, List<SnapshotInfo>> snapshotsToDelete,\n+                         SnapshotLifecycleStats slmStats,\n+                         ActionListener<Void> listener) {\n         int count = snapshotsToDelete.values().stream().mapToInt(List::size).sum();\n         if (count == 0) {\n+            listener.onResponse(null);\n             logger.debug(\"no snapshots are eligible for deletion\");\n             return;\n         }\n \n-        ClusterState state = clusterService.state();\n-        if (okayToDeleteSnapshots(state)) {\n-            logger.trace(\"there are no snapshots currently running, proceeding with snapshot deletion of [{}]\",\n-                formatSnapshots(snapshotsToDelete));\n-            deleteSnapshots(snapshotsToDelete, maximumTime, slmStats);\n-        } else {\n-            logger.debug(\"a snapshot is currently running, rescheduling SLM retention for after snapshot has completed\");\n-            ClusterStateObserver observer = new ClusterStateObserver(clusterService, maximumTime, logger, threadPool.getThreadContext());\n-            CountDownLatch latch = new CountDownLatch(1);\n-            observer.waitForNextChange(\n-                new NoSnapshotRunningListener(observer,\n-                    newState -> threadPool.executor(ThreadPool.Names.MANAGEMENT).execute(() -> {\n-                        try {\n-                            logger.trace(\"received cluster state without running snapshots, proceeding with snapshot deletion of [{}]\",\n-                                formatSnapshots(snapshotsToDelete));\n-                            deleteSnapshots(snapshotsToDelete, maximumTime, slmStats);\n-                        } finally {\n-                            latch.countDown();\n-                        }\n-                    }),\n-                    e -> {\n-                        latch.countDown();\n-                        throw new ElasticsearchException(e);\n-                    }));\n-            try {\n-                logger.trace(\"waiting for snapshot deletion to complete\");\n-                // Wait until we find a cluster state not running a snapshot operation.\n-                // If we can't find one within a day, give up and throw an error.\n-                latch.await(1, TimeUnit.DAYS);\n-                logger.trace(\"deletion complete\");\n-            } catch (InterruptedException e) {\n-                throw new ElasticsearchException(e);\n-            }\n-        }\n-    }\n-\n-    void deleteSnapshots(Map<String, List<SnapshotInfo>> snapshotsToDelete,\n-                         TimeValue maximumTime,\n-                         SnapshotLifecycleStats slmStats) {\n-        int count = snapshotsToDelete.values().stream().mapToInt(List::size).sum();\n-\n         logger.info(\"starting snapshot retention deletion for [{}] snapshots\", count);\n         long startTime = nowNanoSupplier.getAsLong();\n-        final AtomicInteger deleted =  new AtomicInteger(0);\n+        final AtomicInteger deleted = new AtomicInteger(0);\n         final AtomicInteger failed = new AtomicInteger(0);\n+        final GroupedActionListener<Void> allDeletesListener =\n+                new GroupedActionListener<>(ActionListener.runAfter(ActionListener.map(listener, v -> null),\n+                        () -> {\n+                            TimeValue totalElapsedTime = TimeValue.timeValueNanos(nowNanoSupplier.getAsLong() - startTime);\n+                            logger.debug(\"total elapsed time for deletion of [{}] snapshots: {}\", deleted, totalElapsedTime);\n+                            slmStats.deletionTime(totalElapsedTime);\n+                        }), snapshotsToDelete.size());\n         for (Map.Entry<String, List<SnapshotInfo>> entry : snapshotsToDelete.entrySet()) {\n             String repo = entry.getKey();\n             List<SnapshotInfo> snapshots = entry.getValue();\n-            for (SnapshotInfo info : snapshots) {\n-                final String policyId = getPolicyId(info);\n-                final long deleteStartTime = nowNanoSupplier.getAsLong();\n-                // TODO: Use snapshot multi-delete instead of this loop if all nodes in the cluster support it\n-                //       i.e are newer or equal to SnapshotsService#MULTI_DELETE_VERSION\n-                deleteSnapshot(policyId, repo, info.snapshotId(), slmStats, ActionListener.wrap(acknowledgedResponse -> {\n-                    deleted.incrementAndGet();\n-                    assert acknowledgedResponse.isAcknowledged();\n-                    historyStore.putAsync(SnapshotHistoryItem.deletionSuccessRecord(Instant.now().toEpochMilli(),\n-                            info.snapshotId().getName(), policyId, repo));\n-                }, e -> {\n-                    failed.incrementAndGet();\n-                    try {\n-                        final SnapshotHistoryItem result = SnapshotHistoryItem.deletionFailureRecord(Instant.now().toEpochMilli(),\n-                            info.snapshotId().getName(), policyId, repo, e);\n-                        historyStore.putAsync(result);\n-                    } catch (IOException ex) {\n-                        // This shouldn't happen unless there's an issue with serializing the original exception\n-                        logger.error(new ParameterizedMessage(\n-                            \"failed to record snapshot deletion failure for snapshot lifecycle policy [{}]\",\n-                            policyId), ex);\n-                    }\n-                }));\n-                // Check whether we have exceeded the maximum time allowed to spend deleting\n-                // snapshots, if we have, short-circuit the rest of the deletions\n-                long finishTime = nowNanoSupplier.getAsLong();\n-                TimeValue deletionTime = TimeValue.timeValueNanos(finishTime - deleteStartTime);\n-                logger.debug(\"elapsed time for deletion of [{}] snapshot: {}\", info.snapshotId(), deletionTime);\n-                TimeValue totalDeletionTime = TimeValue.timeValueNanos(finishTime - startTime);\n-                if (totalDeletionTime.compareTo(maximumTime) > 0) {\n-                    logger.info(\"maximum snapshot retention deletion time reached, time spent: [{}],\" +\n-                            \" maximum allowed time: [{}], deleted [{}] out of [{}] snapshots scheduled for deletion, failed to delete [{}]\",\n-                        totalDeletionTime, maximumTime, deleted, count, failed);\n-                    slmStats.deletionTime(totalDeletionTime);\n-                    slmStats.retentionTimedOut();\n-                    return;\n-                }\n+            deleteSnapshots(slmStats, deleted, failed, repo, snapshots, allDeletesListener);\n+        }\n+    }\n+\n+    /**\n+     * Set of all currently deleting {@link SnapshotId} used to prevent starting multiple deletes for the same snapshot.\n+     */\n+    private final Set<SnapshotId> runningDeletions = Collections.synchronizedSet(new HashSet<>());\n+\n+    private void deleteSnapshots(SnapshotLifecycleStats slmStats, AtomicInteger deleted, AtomicInteger failed, String repo,\n+                                 List<SnapshotInfo> snapshots, ActionListener<Void> listener) {\n+\n+        final ActionListener<Void> allDeletesListener =\n+                new GroupedActionListener<>(ActionListener.map(listener, v -> null), snapshots.size());\n+        for (SnapshotInfo info : snapshots) {\n+            if (runningDeletions.add(info.snapshotId()) == false) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "83dbef89b26f4242cee800a4fc1e13a9195e40da"}, "originalPosition": 343}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "712a974bb27ab8b57ee1d4a013892861348a6208", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/712a974bb27ab8b57ee1d4a013892861348a6208", "committedDate": "2020-09-30T17:00:18Z", "message": "Merge remote-tracking branch 'elastic/master' into slm-start-deleting-in-parallel"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "41797fa4102f7c370c204dc30d9d810281e7236c", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/41797fa4102f7c370c204dc30d9d810281e7236c", "committedDate": "2020-09-30T17:19:05Z", "message": "CR: don't leak listener"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA3ODI2ODg2", "url": "https://github.com/elastic/elasticsearch/pull/62284#pullrequestreview-507826886", "createdAt": "2020-10-13T21:03:23Z", "commit": {"oid": "41797fa4102f7c370c204dc30d9d810281e7236c"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6b7e7879156fe9664018ab527631b2da448c91cd", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/6b7e7879156fe9664018ab527631b2da448c91cd", "committedDate": "2020-10-14T05:14:27Z", "message": "Merge remote-tracking branch 'elastic/master' into slm-start-deleting-in-parallel"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4557, "cost": 1, "resetAt": "2021-10-28T19:08:13Z"}}}