{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDgzODk5NjMx", "number": 62230, "title": "Rationalise fetch phase exceptions", "bodyText": "We have a special FetchPhaseExecutionException which contains some useful\ninformation about which shard and doc a fetch phase has failed in.  However, this\nis not used in many places - currently only the ExplainPhase and the highlighters\nthrow one, and the FetchPhase itself catches IOExceptions and just passes them\nto the ExceptionsHelper with no extra context.\nThis commit changes FetchPhase to throw FetchPhaseExecutionException if it\nencounters problems in any of its subphases, and removes the special handling\nfrom the explain and highlight phases.  It also removes the need to pass shard ids\naround when building HitContext objects.", "createdAt": "2020-09-10T14:04:38Z", "url": "https://github.com/elastic/elasticsearch/pull/62230", "merged": true, "mergeCommit": {"oid": "2661f96114e006c67e17f11741432b3db495fc19"}, "closed": true, "closedAt": "2020-09-15T08:07:36Z", "author": {"login": "romseygeek"}, "timelineItems": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdHhLwmgH2gAyNDgzODk5NjMxOjc5NjZmMWU0YzJiYWVkY2EwZjUyNTI3NDVmNTg3NmNkZjhmZGZhYzU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdI1ysRAFqTQ4Nzk0NjMyMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "7966f1e4c2baedca0f5252745f5876cdf8fdfac5", "author": {"user": {"login": "romseygeek", "name": "Alan Woodward"}}, "url": "https://github.com/elastic/elasticsearch/commit/7966f1e4c2baedca0f5252745f5876cdf8fdfac5", "committedDate": "2020-09-10T13:54:25Z", "message": "Rationalise fetch phase exceptions"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg2MTQ2MDg1", "url": "https://github.com/elastic/elasticsearch/pull/62230#pullrequestreview-486146085", "createdAt": "2020-09-10T17:14:18Z", "commit": {"oid": "7966f1e4c2baedca0f5252745f5876cdf8fdfac5"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxNzoxNDoxOVrOHP96SA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxNzoyNToxOFrOHP-Tvg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjUwNTAzMg==", "bodyText": "Double-checking that none of this logic changed, you just removed the try/ catch? The git diff is probably just unintuitive.", "url": "https://github.com/elastic/elasticsearch/pull/62230#discussion_r486505032", "createdAt": "2020-09-10T17:14:19Z", "author": {"login": "jtibshirani"}, "path": "server/src/main/java/org/elasticsearch/search/fetch/subphase/highlight/FastVectorHighlighter.java", "diffHunk": "@@ -87,126 +86,119 @@ public HighlightField highlight(FieldHighlightContext fieldContext) {\n             hitContext.cache().put(CACHE_KEY, new HighlighterEntry());\n         }\n         HighlighterEntry cache = (HighlighterEntry) hitContext.cache().get(CACHE_KEY);\n-\n-        try {\n-            FieldHighlightEntry entry = cache.fields.get(fieldType);\n-            if (entry == null) {\n-                FragListBuilder fragListBuilder;\n-                BaseFragmentsBuilder fragmentsBuilder;\n-\n-                final BoundaryScanner boundaryScanner = getBoundaryScanner(field);\n-                if (field.fieldOptions().numberOfFragments() == 0) {\n-                    fragListBuilder = new SingleFragListBuilder();\n-\n+        FieldHighlightEntry entry = cache.fields.get(fieldType);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7966f1e4c2baedca0f5252745f5876cdf8fdfac5"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjUxMTU1MA==", "bodyText": "Does this change the behavior? I'm not sure if we previously swallowed InvalidTokenOffsetsException.", "url": "https://github.com/elastic/elasticsearch/pull/62230#discussion_r486511550", "createdAt": "2020-09-10T17:25:18Z", "author": {"login": "jtibshirani"}, "path": "server/src/main/java/org/elasticsearch/search/fetch/subphase/highlight/PlainHighlighter.java", "diffHunk": "@@ -111,56 +109,42 @@ public HighlightField highlight(FieldHighlightContext fieldContext) {\n         }\n         final int maxAnalyzedOffset = context.getIndexSettings().getHighlightMaxAnalyzedOffset();\n \n-        try {\n-            textsToHighlight = HighlightUtils.loadFieldValues(fieldType, hitContext, fieldContext.forceSource);\n+        textsToHighlight = HighlightUtils.loadFieldValues(fieldType, hitContext, fieldContext.forceSource);\n \n-            for (Object textToHighlight : textsToHighlight) {\n-                String text = convertFieldValue(fieldType, textToHighlight);\n-                int textLength = text.length();\n-                if (keywordIgnoreAbove != null  && textLength > keywordIgnoreAbove) {\n-                    continue; // skip highlighting keyword terms that were ignored during indexing\n-                }\n-                if (textLength > maxAnalyzedOffset) {\n-                    throw new IllegalArgumentException(\n-                        \"The length of [\" + fieldContext.fieldName + \"] field of [\" + hitContext.hit().getId() +\n-                            \"] doc of [\" + context.index().getName() + \"] index \" +\n-                            \"has exceeded [\" + maxAnalyzedOffset + \"] - maximum allowed to be analyzed for highlighting. \" +\n-                            \"This maximum can be set by changing the [\" + IndexSettings.MAX_ANALYZED_OFFSET_SETTING.getKey() +\n-                            \"] index level setting. \" + \"For large texts, indexing with offsets or term vectors, and highlighting \" +\n-                            \"with unified or fvh highlighter is recommended!\");\n-                }\n+        for (Object textToHighlight : textsToHighlight) {\n+            String text = convertFieldValue(fieldType, textToHighlight);\n+            int textLength = text.length();\n+            if (keywordIgnoreAbove != null && textLength > keywordIgnoreAbove) {\n+                continue; // skip highlighting keyword terms that were ignored during indexing\n+            }\n+            if (textLength > maxAnalyzedOffset) {\n+                throw new IllegalArgumentException(\n+                    \"The length of [\" + fieldContext.fieldName + \"] field of [\" + hitContext.hit().getId() +\n+                        \"] doc of [\" + context.index().getName() + \"] index \" +\n+                        \"has exceeded [\" + maxAnalyzedOffset + \"] - maximum allowed to be analyzed for highlighting. \" +\n+                        \"This maximum can be set by changing the [\" + IndexSettings.MAX_ANALYZED_OFFSET_SETTING.getKey() +\n+                        \"] index level setting. \" + \"For large texts, indexing with offsets or term vectors, and highlighting \" +\n+                        \"with unified or fvh highlighter is recommended!\");\n+            }\n \n-                try (TokenStream tokenStream = analyzer.tokenStream(fieldType.name(), text)) {\n-                    if (!tokenStream.hasAttribute(CharTermAttribute.class) || !tokenStream.hasAttribute(OffsetAttribute.class)) {\n-                        // can't perform highlighting if the stream has no terms (binary token stream) or no offsets\n-                        continue;\n-                    }\n-                    TextFragment[] bestTextFragments = entry.getBestTextFragments(tokenStream, text, false, numberOfFragments);\n-                    for (TextFragment bestTextFragment : bestTextFragments) {\n-                        if (bestTextFragment != null && bestTextFragment.getScore() > 0) {\n-                            fragsList.add(bestTextFragment);\n-                        }\n+            try (TokenStream tokenStream = analyzer.tokenStream(fieldType.name(), text)) {\n+                if (!tokenStream.hasAttribute(CharTermAttribute.class) || !tokenStream.hasAttribute(OffsetAttribute.class)) {\n+                    // can't perform highlighting if the stream has no terms (binary token stream) or no offsets\n+                    continue;\n+                }\n+                TextFragment[] bestTextFragments = entry.getBestTextFragments(tokenStream, text, false, numberOfFragments);\n+                for (TextFragment bestTextFragment : bestTextFragments) {\n+                    if (bestTextFragment != null && bestTextFragment.getScore() > 0) {\n+                        fragsList.add(bestTextFragment);\n                     }\n                 }\n-            }\n-        } catch (Exception e) {\n-            if (ExceptionsHelper.unwrap(e, BytesRefHash.MaxBytesLengthExceededException.class) != null) {\n-                // this can happen if for example a field is not_analyzed and ignore_above option is set.\n-                // the field will be ignored when indexing but the huge term is still in the source and\n-                // the plain highlighter will parse the source and try to analyze it.\n-                return null;\n-            } else {\n-                throw new FetchPhaseExecutionException(fieldContext.shardTarget,\n-                    \"Failed to highlight field [\" + fieldContext.fieldName + \"]\", e);\n+            } catch (InvalidTokenOffsetsException | BytesRefHash.MaxBytesLengthExceededException e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7966f1e4c2baedca0f5252745f5876cdf8fdfac5"}, "originalPosition": 106}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "898b2d00044805bf9eb5ceb03610e6095b92c908", "author": {"user": {"login": "romseygeek", "name": "Alan Woodward"}}, "url": "https://github.com/elastic/elasticsearch/commit/898b2d00044805bf9eb5ceb03610e6095b92c908", "committedDate": "2020-09-12T15:54:42Z", "message": "feedback"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f2d567420dc07b1b7b5dd0893a65f4f1b3140e0f", "author": {"user": {"login": "elasticmachine", "name": "Elastic Machine"}}, "url": "https://github.com/elastic/elasticsearch/commit/f2d567420dc07b1b7b5dd0893a65f4f1b3140e0f", "committedDate": "2020-09-13T10:49:12Z", "message": "Merge branch 'master' into fetch/exceptions"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "25051501b4327146d84397d8189429028e444018", "author": {"user": {"login": "elasticmachine", "name": "Elastic Machine"}}, "url": "https://github.com/elastic/elasticsearch/commit/25051501b4327146d84397d8189429028e444018", "committedDate": "2020-09-14T09:17:35Z", "message": "Merge branch 'master' into fetch/exceptions"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg3OTE5Njc4", "url": "https://github.com/elastic/elasticsearch/pull/62230#pullrequestreview-487919678", "createdAt": "2020-09-14T15:57:33Z", "commit": {"oid": "25051501b4327146d84397d8189429028e444018"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQxNTo1NzozM1rOHRb6OQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQxNTo1NzozM1rOHRb6OQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODA0NTExMw==", "bodyText": "Sorry if I'm missing something, why does InvalidTokenOffsetsException need special treatment?\nAlso maybe we can keep the comment explaining why we're skipping BytesRefHash.MaxBytesLengthExceededException?", "url": "https://github.com/elastic/elasticsearch/pull/62230#discussion_r488045113", "createdAt": "2020-09-14T15:57:33Z", "author": {"login": "jtibshirani"}, "path": "server/src/main/java/org/elasticsearch/search/fetch/subphase/highlight/PlainHighlighter.java", "diffHunk": "@@ -111,56 +109,44 @@ public HighlightField highlight(FieldHighlightContext fieldContext) {\n         }\n         final int maxAnalyzedOffset = context.getIndexSettings().getHighlightMaxAnalyzedOffset();\n \n-        try {\n-            textsToHighlight = HighlightUtils.loadFieldValues(fieldType, hitContext, fieldContext.forceSource);\n+        textsToHighlight = HighlightUtils.loadFieldValues(fieldType, hitContext, fieldContext.forceSource);\n \n-            for (Object textToHighlight : textsToHighlight) {\n-                String text = convertFieldValue(fieldType, textToHighlight);\n-                int textLength = text.length();\n-                if (keywordIgnoreAbove != null  && textLength > keywordIgnoreAbove) {\n-                    continue; // skip highlighting keyword terms that were ignored during indexing\n-                }\n-                if (textLength > maxAnalyzedOffset) {\n-                    throw new IllegalArgumentException(\n-                        \"The length of [\" + fieldContext.fieldName + \"] field of [\" + hitContext.hit().getId() +\n-                            \"] doc of [\" + context.index().getName() + \"] index \" +\n-                            \"has exceeded [\" + maxAnalyzedOffset + \"] - maximum allowed to be analyzed for highlighting. \" +\n-                            \"This maximum can be set by changing the [\" + IndexSettings.MAX_ANALYZED_OFFSET_SETTING.getKey() +\n-                            \"] index level setting. \" + \"For large texts, indexing with offsets or term vectors, and highlighting \" +\n-                            \"with unified or fvh highlighter is recommended!\");\n-                }\n+        for (Object textToHighlight : textsToHighlight) {\n+            String text = convertFieldValue(fieldType, textToHighlight);\n+            int textLength = text.length();\n+            if (keywordIgnoreAbove != null && textLength > keywordIgnoreAbove) {\n+                continue; // skip highlighting keyword terms that were ignored during indexing\n+            }\n+            if (textLength > maxAnalyzedOffset) {\n+                throw new IllegalArgumentException(\n+                    \"The length of [\" + fieldContext.fieldName + \"] field of [\" + hitContext.hit().getId() +\n+                        \"] doc of [\" + context.index().getName() + \"] index \" +\n+                        \"has exceeded [\" + maxAnalyzedOffset + \"] - maximum allowed to be analyzed for highlighting. \" +\n+                        \"This maximum can be set by changing the [\" + IndexSettings.MAX_ANALYZED_OFFSET_SETTING.getKey() +\n+                        \"] index level setting. \" + \"For large texts, indexing with offsets or term vectors, and highlighting \" +\n+                        \"with unified or fvh highlighter is recommended!\");\n+            }\n \n-                try (TokenStream tokenStream = analyzer.tokenStream(fieldType.name(), text)) {\n-                    if (!tokenStream.hasAttribute(CharTermAttribute.class) || !tokenStream.hasAttribute(OffsetAttribute.class)) {\n-                        // can't perform highlighting if the stream has no terms (binary token stream) or no offsets\n-                        continue;\n-                    }\n-                    TextFragment[] bestTextFragments = entry.getBestTextFragments(tokenStream, text, false, numberOfFragments);\n-                    for (TextFragment bestTextFragment : bestTextFragments) {\n-                        if (bestTextFragment != null && bestTextFragment.getScore() > 0) {\n-                            fragsList.add(bestTextFragment);\n-                        }\n+            try (TokenStream tokenStream = analyzer.tokenStream(fieldType.name(), text)) {\n+                if (!tokenStream.hasAttribute(CharTermAttribute.class) || !tokenStream.hasAttribute(OffsetAttribute.class)) {\n+                    // can't perform highlighting if the stream has no terms (binary token stream) or no offsets\n+                    continue;\n+                }\n+                TextFragment[] bestTextFragments = entry.getBestTextFragments(tokenStream, text, false, numberOfFragments);\n+                for (TextFragment bestTextFragment : bestTextFragments) {\n+                    if (bestTextFragment != null && bestTextFragment.getScore() > 0) {\n+                        fragsList.add(bestTextFragment);\n                     }\n                 }\n-            }\n-        } catch (Exception e) {\n-            if (ExceptionsHelper.unwrap(e, BytesRefHash.MaxBytesLengthExceededException.class) != null) {\n-                // this can happen if for example a field is not_analyzed and ignore_above option is set.\n-                // the field will be ignored when indexing but the huge term is still in the source and\n-                // the plain highlighter will parse the source and try to analyze it.\n-                return null;\n-            } else {\n-                throw new FetchPhaseExecutionException(fieldContext.shardTarget,\n-                    \"Failed to highlight field [\" + fieldContext.fieldName + \"]\", e);\n+            } catch (BytesRefHash.MaxBytesLengthExceededException e) {\n+                // ignore and continue to the next value", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "25051501b4327146d84397d8189429028e444018"}, "originalPosition": 107}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "35363d15b0cef8dbf3e5c8535e16ff6bb7a426c3", "author": {"user": {"login": "romseygeek", "name": "Alan Woodward"}}, "url": "https://github.com/elastic/elasticsearch/commit/35363d15b0cef8dbf3e5c8535e16ff6bb7a426c3", "committedDate": "2020-09-14T16:21:17Z", "message": "Merge remote-tracking branch 'origin/master' into fetch/exceptions"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "33992c50edd742474a70e34e3530ac82672422cc", "author": {"user": {"login": "romseygeek", "name": "Alan Woodward"}}, "url": "https://github.com/elastic/elasticsearch/commit/33992c50edd742474a70e34e3530ac82672422cc", "committedDate": "2020-09-14T16:26:12Z", "message": "add comment back"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "67cb9b0b353ae652eb8b25e54bd84a509fc78d9e", "author": {"user": {"login": "romseygeek", "name": "Alan Woodward"}}, "url": "https://github.com/elastic/elasticsearch/commit/67cb9b0b353ae652eb8b25e54bd84a509fc78d9e", "committedDate": "2020-09-14T16:26:24Z", "message": "Merge remote-tracking branch 'romseygeek/fetch/exceptions' into fetch/exceptions"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg3OTQ2MzIw", "url": "https://github.com/elastic/elasticsearch/pull/62230#pullrequestreview-487946320", "createdAt": "2020-09-14T16:28:55Z", "commit": {"oid": "25051501b4327146d84397d8189429028e444018"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4694, "cost": 1, "resetAt": "2021-10-28T19:08:13Z"}}}