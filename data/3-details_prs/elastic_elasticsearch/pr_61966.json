{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDc5NDQ0ODMy", "number": 61966, "title": "Write deprecation logs to a data stream", "bodyText": "Backport of #58924.\nCloses #46106. Introduce a mechanism for writing deprecation logs to a data stream as well as to disk.\nTest by running ./gradlew run and then:\ncurl -u elastic-admin:elastic-password http://localhost:9200/_flush/synced?pretty\ncurl -u elastic-admin:elastic-password http://localhost:9200/logs-deprecation-elasticsearch/_search?pretty\n\nThis implementation reworks deprecation logging to rely on log4j by introducing filters and appenders, and a custom log level. The existing deprecation X-Pack plugin simply configures an extra appender to capture messages to an index. A \"component\" class manages most of the interface between the cluster and the appender.\nThis approach can be extended for other types of logging, e.g. security by introducing further custom log levels.", "createdAt": "2020-09-04T08:34:55Z", "url": "https://github.com/elastic/elasticsearch/pull/61966", "merged": true, "mergeCommit": {"oid": "b7fd7cf1546a755dda759976f725c15675a7fa34"}, "closed": true, "closedAt": "2020-09-09T11:16:28Z", "author": {"login": "pugnascotia"}, "timelineItems": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdFS-eLgH2gAyNDc5NDQ0ODMyOjQ0MWFlZjFiZmY0N2Q1NWQzOTI5MGU5ZTg5YTJmMGU3ODE4NTkwNDQ=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdHJbiqAH2gAyNDc5NDQ0ODMyOjUwZWIyOGFjMDdlYzE1ZTVmY2NmMzRlMDg4YzZiZTgzNzMwNGYzYmY=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "441aef1bff47d55d39290e9e89a2f0e781859044", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/441aef1bff47d55d39290e9e89a2f0e781859044", "committedDate": "2020-09-03T16:13:23Z", "message": "Write deprecation logs to a data stream\n\nBackport of #61484.\n\nCloses #46106. Implement a new log4j appender for deprecation logging, in\norder to write logs to a dedicated data stream. This is controlled by a new\nsetting, `cluster.deprecation_indexing.enabled`."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgyNTg3ODYx", "url": "https://github.com/elastic/elasticsearch/pull/61966#pullrequestreview-482587861", "createdAt": "2020-09-04T11:52:36Z", "commit": {"oid": "441aef1bff47d55d39290e9e89a2f0e781859044"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNFQxMTo1MjozNlrOHNKp0g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNFQxMTo1MjozNlrOHNKp0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzU2ODA4Mg==", "bodyText": "Can you add this to the body of the afterBulk() method to introspect if there are bulk item failures?\nlong items = request.numberOfActions();\nif (logger.isTraceEnabled()) {\n   logger.trace(\"indexed [{}] deprecation documents into [{}]\", items,\n   Arrays.stream(response.getItems())\n      .map(BulkItemResponse::getIndex)\n      .distinct()\n      .collect(Collectors.joining(\",\")));\n}\n\nif (response.hasFailures()) {\n   Map<String, String> failures = Arrays.stream(response.getItems())\n      .filter(BulkItemResponse::isFailed)\n      .collect(Collectors.toMap(BulkItemResponse::getId, BulkItemResponse::getFailureMessage));\n   logger.error(\"failures: [{}]\", failures);\n}", "url": "https://github.com/elastic/elasticsearch/pull/61966#discussion_r483568082", "createdAt": "2020-09-04T11:52:36Z", "author": {"login": "martijnvg"}, "path": "x-pack/plugin/deprecation/src/main/java/org/elasticsearch/xpack/deprecation/logging/DeprecationIndexingComponent.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.deprecation.logging;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.core.LoggerContext;\n+import org.apache.logging.log4j.core.config.Configuration;\n+import org.elasticsearch.action.bulk.BackoffPolicy;\n+import org.elasticsearch.action.bulk.BulkProcessor;\n+import org.elasticsearch.action.bulk.BulkRequest;\n+import org.elasticsearch.action.bulk.BulkResponse;\n+import org.elasticsearch.action.index.IndexRequest;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.ClusterChangedEvent;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ClusterStateListener;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.logging.ESJsonLayout;\n+import org.elasticsearch.common.logging.Loggers;\n+import org.elasticsearch.common.logging.RateLimitingFilter;\n+import org.elasticsearch.common.settings.Setting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.common.util.concurrent.EsExecutors;\n+import org.elasticsearch.xpack.core.ClientHelper;\n+\n+import java.util.function.Consumer;\n+\n+/**\n+ * This component manages the construction and lifecycle of the {@link DeprecationIndexingAppender}.\n+ * It also starts and stops the appender\n+ */\n+public class DeprecationIndexingComponent extends AbstractLifecycleComponent implements ClusterStateListener {\n+    private static final Logger logger = LogManager.getLogger(DeprecationIndexingComponent.class);\n+\n+    public static final Setting<Boolean> WRITE_DEPRECATION_LOGS_TO_INDEX = Setting.boolSetting(\n+        \"cluster.deprecation_indexing.enabled\",\n+        false,\n+        Setting.Property.NodeScope,\n+        Setting.Property.Dynamic\n+    );\n+\n+    private final DeprecationIndexingAppender appender;\n+    private final BulkProcessor processor;\n+    private final RateLimitingFilter filter;\n+\n+    public DeprecationIndexingComponent(Client client, Settings settings) {\n+        this.processor = getBulkProcessor(new OriginSettingClient(client, ClientHelper.DEPRECATION_ORIGIN), settings);\n+        final Consumer<IndexRequest> consumer = this.processor::add;\n+\n+        final LoggerContext context = (LoggerContext) LogManager.getContext(false);\n+        final Configuration configuration = context.getConfiguration();\n+\n+        final ESJsonLayout ecsLayout = ESJsonLayout.newBuilder()\n+            .setType(\"deprecation\")\n+            // This matches the additional fields in the DeprecatedMessage class\n+            .setESMessageFields(\"x-opaque-id,data_stream.type,data_stream.datatype,data_stream.namespace,ecs.version\")\n+            .setConfiguration(configuration)\n+            .build();\n+\n+        this.filter = new RateLimitingFilter();\n+        this.appender = new DeprecationIndexingAppender(\"deprecation_indexing_appender\", filter, ecsLayout, consumer);\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        this.appender.start();\n+        Loggers.addAppender(LogManager.getLogger(\"org.elasticsearch.deprecation\"), this.appender);\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        Loggers.removeAppender(LogManager.getLogger(\"org.elasticsearch.deprecation\"), this.appender);\n+        this.appender.stop();\n+    }\n+\n+    @Override\n+    protected void doClose() {\n+        this.processor.close();\n+    }\n+\n+    /**\n+     * Listens for changes to the cluster state, in order to know whether to toggle indexing\n+     * and to set the cluster UUID and node ID. These can't be set in the constructor because\n+     * the initial cluster state won't be set yet.\n+     *\n+     * @param event the cluster state event to process\n+     */\n+    @Override\n+    public void clusterChanged(ClusterChangedEvent event) {\n+        final ClusterState state = event.state();\n+        final boolean newEnabled = WRITE_DEPRECATION_LOGS_TO_INDEX.get(state.getMetadata().settings());\n+        if (appender.isEnabled() != newEnabled) {\n+            // We've flipped from disabled to enabled. Make sure we start with a clean cache of\n+            // previously-seen keys, otherwise we won't index anything.\n+            if (newEnabled) {\n+                this.filter.reset();\n+            }\n+            appender.setEnabled(newEnabled);\n+        }\n+    }\n+\n+    /**\n+     * Constructs a bulk processor for writing documents\n+     *\n+     * @param client   the client to use\n+     * @param settings the settings to use\n+     * @return an initialised bulk processor\n+     */\n+    private BulkProcessor getBulkProcessor(Client client, Settings settings) {\n+        final OriginSettingClient originSettingClient = new OriginSettingClient(client, ClientHelper.DEPRECATION_ORIGIN);\n+        final BulkProcessor.Listener listener = new DeprecationBulkListener();\n+\n+        // This configuration disables the size count and size thresholds,\n+        // and instead uses a scheduled flush only. This means that calling\n+        // processor.add() will not block the calling thread.\n+        return BulkProcessor.builder(originSettingClient::bulk, listener)\n+            .setBackoffPolicy(BackoffPolicy.exponentialBackoff(TimeValue.timeValueMillis(1000), 3))\n+            .setConcurrentRequests(Math.max(2, EsExecutors.allocatedProcessors(settings)))\n+            .setBulkActions(-1)\n+            .setBulkSize(new ByteSizeValue(-1, ByteSizeUnit.BYTES))\n+            .setFlushInterval(TimeValue.timeValueSeconds(5))\n+            .build();\n+    }\n+\n+    private static class DeprecationBulkListener implements BulkProcessor.Listener {\n+        @Override\n+        public void beforeBulk(long executionId, BulkRequest request) {}\n+\n+        @Override\n+        public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "441aef1bff47d55d39290e9e89a2f0e781859044"}, "originalPosition": 139}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9cc1c2cda199b9e569ed850f33136a243b11a8ef", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/9cc1c2cda199b9e569ed850f33136a243b11a8ef", "committedDate": "2020-09-04T14:48:07Z", "message": "Fixes to write documents in ECS"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2b1db5656ad611ad685e3a73f5ff1d157a4febc7", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/2b1db5656ad611ad685e3a73f5ff1d157a4febc7", "committedDate": "2020-09-04T15:03:15Z", "message": "License fix"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgzMzE0NTEx", "url": "https://github.com/elastic/elasticsearch/pull/61966#pullrequestreview-483314511", "createdAt": "2020-09-07T07:38:08Z", "commit": {"oid": "2b1db5656ad611ad685e3a73f5ff1d157a4febc7"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "89f09eda99aca340ae24eb1e39b492c6f90c03bc", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/89f09eda99aca340ae24eb1e39b492c6f90c03bc", "committedDate": "2020-09-09T09:24:55Z", "message": "Logger fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "50eb28ac07ec15e5fccf34e088c6be837304f3bf", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/50eb28ac07ec15e5fccf34e088c6be837304f3bf", "committedDate": "2020-09-09T10:13:56Z", "message": "Merge remote-tracking branch 'upstream/7.x' into 46106-index-deprecation-logs-7x"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4859, "cost": 1, "resetAt": "2021-10-28T19:08:13Z"}}}