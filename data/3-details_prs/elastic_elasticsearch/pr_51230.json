{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzY0OTIwNzA4", "number": 51230, "title": "Optimize sequential reads in SearchableSnapshotIndexInput", "bodyText": "Today SearchableSnapshotIndexInput translates each readBytesInternal call\nto one or more calls to readBlob on the underlying repository. We make a lot\nof small readBytesInternal calls since they are used to fill a small\nin-memory buffer. Calls to readBlob are expensive: blob storage providers\nlike AWS S3 charge money per API call.\nA common usage pattern is to take a brand-new IndexInput, seek to a\nparticular location, and then sequentially read a substantial amount of data\nand stream it to disk.\nThis commit optimizes the implementation for that specific usage pattern.\nRather than calling readBlob each time the internal buffer needs filling we\ninstead request a (potentially much larger) range of the blob and consume the\nresponse bit-by-bit as needed by a sequentially-reading client.", "createdAt": "2020-01-20T16:37:26Z", "url": "https://github.com/elastic/elasticsearch/pull/51230", "merged": true, "mergeCommit": {"oid": "30b5553c60b95f7413593e1d93eac063c04dfb10"}, "closed": true, "closedAt": "2020-02-04T17:08:51Z", "author": {"login": "DaveCTurner"}, "timelineItems": {"totalCount": 23, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABb8PDh6gH2gAyMzY0OTIwNzA4OmEyNjVjMzAyZjczYmM5NDBmZjY2YTAwZWQ3YWQ3NThkM2FmNjg1NmE=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcBDrLYgH2gAyMzY0OTIwNzA4OmY5ZDRkZWY0OWQ3OTc2MDExNDEzZDA4ZTE2ZGU3MDIzYjI3OTY3MGM=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "a265c302f73bc940ff66a00ed7ad758d3af6856a", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/a265c302f73bc940ff66a00ed7ad758d3af6856a", "committedDate": "2020-01-20T16:22:49Z", "message": "Optimize sequential reads in SearchableSnapshotIndexInput\n\nToday `SearchableSnapshotIndexInput` translates each `readBytesInternal` call\nto one or more calls to `readBlob` on the underlying repository. We make a lot\nof small `readBytesInternal` calls since they are used to fill a small\nin-memory buffer. Calls to `readBlob` are expensive: blob storage providers\nlike AWS S3 charge money per API call.\n\nA common usage pattern is to take a brand-new `IndexInput`, seek to a\nparticular location, and then sequentially read a substantial amount of data\nand stream it to disk.\n\nThis commit optimizes the implementation for that specific usage pattern.\nRather than calling `readBlob` each time the internal buffer needs filling we\ninstead request a (potentially much larger) range of the blob and consume the\nresponse bit-by-bit as needed by a sequentially-reading client."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b8a425b253124bb1ba8f367bb5d9839a523b9904", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/b8a425b253124bb1ba8f367bb5d9839a523b9904", "committedDate": "2020-01-20T16:45:06Z", "message": "Post-PR-opening blues"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6b22442d48cc1afa065482f9ef4a6aae721a6896", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/6b22442d48cc1afa065482f9ef4a6aae721a6896", "committedDate": "2020-01-20T17:09:26Z", "message": "Introduce constant to clarify that clones and slices are not optimized"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "855b661c37381a7f79e171aad17798ce1f9bdc5e", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/855b661c37381a7f79e171aad17798ce1f9bdc5e", "committedDate": "2020-01-21T10:26:40Z", "message": "Merge branch 'feature/searchable-snapshots' into 2020-01-20-searchable-snapshot-readahead"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/4e0cfcf442ccc0ca75fbd476594320ef6cdcba57", "committedDate": "2020-01-21T10:27:42Z", "message": "Fix merge conflict"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ3MTY2NTg2", "url": "https://github.com/elastic/elasticsearch/pull/51230#pullrequestreview-347166586", "createdAt": "2020-01-23T09:45:02Z", "commit": {"oid": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57"}, "state": "COMMENTED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwOTo0NTowMlrOFg4D-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QxMDozNToyN1rOFg5mGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAxNzI3Mw==", "bodyText": "perhaps use new ByteSizeValue(32, ByteSizeUnit.MB).getBytes().\nIt auto-documents the value :)", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370017273", "createdAt": "2020-01-23T09:45:02Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -36,6 +37,7 @@\n \n     private final BlobStoreIndexShardSnapshot snapshot;\n     private final BlobContainer blobContainer;\n+    private static final long BLOB_STORE_SEQUENTIAL_READ_SIZE = 1L<<25; // 32MB", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAxODU4Mw==", "bodyText": "I wonder if choosing the BLOB_STORE_SEQUENTIAL_READ_SIZE should be left to the underlying blob store implementation. For a shared file-system, it might not make sense to have BLOB_STORE_SEQUENTIAL_READ_SIZE at all (but should use Long.MAX_VALUE instead), as it requires opening the files multiple times.", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370018583", "createdAt": "2020-01-23T09:47:29Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -68,7 +70,8 @@ public long fileLength(final String name) throws IOException {\n     @Override\n     public IndexInput openInput(final String name, final IOContext context) throws IOException {\n         ensureOpen();\n-        return new SearchableSnapshotIndexInput(blobContainer, fileInfo(name));\n+        return new SearchableSnapshotIndexInput(blobContainer, fileInfo(name), BLOB_STORE_SEQUENTIAL_READ_SIZE,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAzMTY3Ng==", "bodyText": "something missing here", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370031676", "createdAt": "2020-01-23T10:13:01Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,34 +105,126 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        final long currentSequentialReadSize = sequentialReadSize;\n+        if (currentSequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            final StreamForSequentialReads streamForSequentialReads = streamForSequentialReadsRef.get();\n+            if (streamForSequentialReads == null) {\n+                // start a new sequential read\n+                if (tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                    return;\n+                }\n+            } else if (streamForSequentialReads.part == part && streamForSequentialReads.pos == pos) {\n+                // continuing a sequential read that we started previously\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                int read = streamForSequentialReads.inputStream.read(b, offset, length);\n+                assert read <= length : read + \" vs \" + length;\n+                streamForSequentialReads.pos += read;\n+                position += read;\n+                pos += read;\n+                offset += read;\n+                length -= read;\n+\n+                if (streamForSequentialReads.isFullyRead()) {\n+                    if (streamForSequentialReadsRef.compareAndSet(streamForSequentialReads, null) != false) {\n+                        streamForSequentialReads.close();\n+                    } else {\n+                        // something happened concurrently, defensively stop optimizing\n+                        sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+                    }\n+\n+                    if (length == 0) {\n+                        // the current stream contained precisely enough data for this read, so we're good.\n+                        return;\n+                    } else {\n+                        // the current stream didn't contain enough data for this read, so we must read more\n+                        if (sequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION\n+                            && tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                            return;\n+                        }\n+                    }\n+                } else {\n+                    // the current stream contained enough data for this read and more besides, so we leave it alone.\n+                    assert length == 0 : length + \" remaining\";\n+                    return;\n+                }\n+            } else {\n+                // not a sequential read, so stop optimizing for this usage pattern and fall through to the unoptimized behaviour\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+                IOUtils.close(streamForSequentialReadsRef.getAndSet(null));\n+            }\n+        }\n+\n+        // read part of a blob directly; the code above falls through to this case where there is no optimization possible\n         try (InputStream inputStream = blobContainer.readBlob(fileInfo.partName(part), pos, length)) {\n-            int read = inputStream.read(b, offset, length);\n-            assert read == length;\n+            final int read = inputStream.read(b, offset, length);\n+            assert read == length : read + \" vs \" + length;\n             position += read;\n         }\n     }\n \n+    /**\n+     * If appropriate, open a new stream for sequential reading and satisfy the given read using it. Returns whether this happened or not;\n+     * if it did not happen then nothing was read, and the caller should perform the read directly.\n+     */\n+    private boolean tryReadAndKeepStreamOpen(int part, long pos, byte[] b, int offset, int length, long currentSequentialReadSize)\n+        throws IOException {\n+\n+        assert streamForSequentialReadsRef.get() == null : \"should only be called when a new stream is needed\";\n+        assert currentSequentialReadSize > 0L : \"should not be called if \";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57"}, "originalPosition": 141}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAzNTUxNA==", "bodyText": "should we put this logic into StreamForSequentialReads? Perhaps that class could enforce that only sequential reads are possible from the stream (and offer a method to say isSequentialReadPossible), with the logic in this class here just trying to call these methods.", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370035514", "createdAt": "2020-01-23T10:21:02Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,34 +105,126 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        final long currentSequentialReadSize = sequentialReadSize;\n+        if (currentSequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            final StreamForSequentialReads streamForSequentialReads = streamForSequentialReadsRef.get();\n+            if (streamForSequentialReads == null) {\n+                // start a new sequential read\n+                if (tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                    return;\n+                }\n+            } else if (streamForSequentialReads.part == part && streamForSequentialReads.pos == pos) {\n+                // continuing a sequential read that we started previously\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                int read = streamForSequentialReads.inputStream.read(b, offset, length);\n+                assert read <= length : read + \" vs \" + length;\n+                streamForSequentialReads.pos += read;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAzNjI0MQ==", "bodyText": "just leave off the != false", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370036241", "createdAt": "2020-01-23T10:22:38Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,34 +105,126 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        final long currentSequentialReadSize = sequentialReadSize;\n+        if (currentSequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            final StreamForSequentialReads streamForSequentialReads = streamForSequentialReadsRef.get();\n+            if (streamForSequentialReads == null) {\n+                // start a new sequential read\n+                if (tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                    return;\n+                }\n+            } else if (streamForSequentialReads.part == part && streamForSequentialReads.pos == pos) {\n+                // continuing a sequential read that we started previously\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                int read = streamForSequentialReads.inputStream.read(b, offset, length);\n+                assert read <= length : read + \" vs \" + length;\n+                streamForSequentialReads.pos += read;\n+                position += read;\n+                pos += read;\n+                offset += read;\n+                length -= read;\n+\n+                if (streamForSequentialReads.isFullyRead()) {\n+                    if (streamForSequentialReadsRef.compareAndSet(streamForSequentialReads, null) != false) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAzNzMwNw==", "bodyText": "Do we ever expect concurrent activity here? If that was the case, our bookkeeping logic (position, pos, offset, length) would not work at all?", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370037307", "createdAt": "2020-01-23T10:24:51Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,34 +105,126 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        final long currentSequentialReadSize = sequentialReadSize;\n+        if (currentSequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            final StreamForSequentialReads streamForSequentialReads = streamForSequentialReadsRef.get();\n+            if (streamForSequentialReads == null) {\n+                // start a new sequential read\n+                if (tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                    return;\n+                }\n+            } else if (streamForSequentialReads.part == part && streamForSequentialReads.pos == pos) {\n+                // continuing a sequential read that we started previously\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                int read = streamForSequentialReads.inputStream.read(b, offset, length);\n+                assert read <= length : read + \" vs \" + length;\n+                streamForSequentialReads.pos += read;\n+                position += read;\n+                pos += read;\n+                offset += read;\n+                length -= read;\n+\n+                if (streamForSequentialReads.isFullyRead()) {\n+                    if (streamForSequentialReadsRef.compareAndSet(streamForSequentialReads, null) != false) {\n+                        streamForSequentialReads.close();\n+                    } else {\n+                        // something happened concurrently, defensively stop optimizing\n+                        sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAzODgwMA==", "bodyText": "maybe put everything above this into a readOptimized() method that returns a boolean (denoting whether it read or not). This will allow having so many explicit returns in the above code (and the deliberate fall-through logic).", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370038800", "createdAt": "2020-01-23T10:27:55Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,34 +105,126 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        final long currentSequentialReadSize = sequentialReadSize;\n+        if (currentSequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            final StreamForSequentialReads streamForSequentialReads = streamForSequentialReadsRef.get();\n+            if (streamForSequentialReads == null) {\n+                // start a new sequential read\n+                if (tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                    return;\n+                }\n+            } else if (streamForSequentialReads.part == part && streamForSequentialReads.pos == pos) {\n+                // continuing a sequential read that we started previously\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                int read = streamForSequentialReads.inputStream.read(b, offset, length);\n+                assert read <= length : read + \" vs \" + length;\n+                streamForSequentialReads.pos += read;\n+                position += read;\n+                pos += read;\n+                offset += read;\n+                length -= read;\n+\n+                if (streamForSequentialReads.isFullyRead()) {\n+                    if (streamForSequentialReadsRef.compareAndSet(streamForSequentialReads, null) != false) {\n+                        streamForSequentialReads.close();\n+                    } else {\n+                        // something happened concurrently, defensively stop optimizing\n+                        sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+                    }\n+\n+                    if (length == 0) {\n+                        // the current stream contained precisely enough data for this read, so we're good.\n+                        return;\n+                    } else {\n+                        // the current stream didn't contain enough data for this read, so we must read more\n+                        if (sequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION\n+                            && tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                            return;\n+                        }\n+                    }\n+                } else {\n+                    // the current stream contained enough data for this read and more besides, so we leave it alone.\n+                    assert length == 0 : length + \" remaining\";\n+                    return;\n+                }\n+            } else {\n+                // not a sequential read, so stop optimizing for this usage pattern and fall through to the unoptimized behaviour\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+                IOUtils.close(streamForSequentialReadsRef.getAndSet(null));\n+            }\n+        }\n+\n+        // read part of a blob directly; the code above falls through to this case where there is no optimization possible", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57"}, "originalPosition": 123}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAzOTM2NA==", "bodyText": "I guess this is to handle some kind of concurrency. I don't really understand the concurrency here though.", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370039364", "createdAt": "2020-01-23T10:28:58Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,34 +105,126 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        final long currentSequentialReadSize = sequentialReadSize;\n+        if (currentSequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            final StreamForSequentialReads streamForSequentialReads = streamForSequentialReadsRef.get();\n+            if (streamForSequentialReads == null) {\n+                // start a new sequential read\n+                if (tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                    return;\n+                }\n+            } else if (streamForSequentialReads.part == part && streamForSequentialReads.pos == pos) {\n+                // continuing a sequential read that we started previously\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                int read = streamForSequentialReads.inputStream.read(b, offset, length);\n+                assert read <= length : read + \" vs \" + length;\n+                streamForSequentialReads.pos += read;\n+                position += read;\n+                pos += read;\n+                offset += read;\n+                length -= read;\n+\n+                if (streamForSequentialReads.isFullyRead()) {\n+                    if (streamForSequentialReadsRef.compareAndSet(streamForSequentialReads, null) != false) {\n+                        streamForSequentialReads.close();\n+                    } else {\n+                        // something happened concurrently, defensively stop optimizing\n+                        sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+                    }\n+\n+                    if (length == 0) {\n+                        // the current stream contained precisely enough data for this read, so we're good.\n+                        return;\n+                    } else {\n+                        // the current stream didn't contain enough data for this read, so we must read more\n+                        if (sequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDA0MjM5Mw==", "bodyText": "Should we not use this existing open stream as much as possible? We might not be able to read the full bytes from this stream, but perhaps we can use it to read everything up to streamLength, and subsequently request a new stream for the rest? This might avoid redownloading data in case where the buffer size is not a proper divisor of sequentialReadSize?", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370042393", "createdAt": "2020-01-23T10:35:27Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,34 +105,126 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        final long currentSequentialReadSize = sequentialReadSize;\n+        if (currentSequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            final StreamForSequentialReads streamForSequentialReads = streamForSequentialReadsRef.get();\n+            if (streamForSequentialReads == null) {\n+                // start a new sequential read\n+                if (tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                    return;\n+                }\n+            } else if (streamForSequentialReads.part == part && streamForSequentialReads.pos == pos) {\n+                // continuing a sequential read that we started previously\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                int read = streamForSequentialReads.inputStream.read(b, offset, length);\n+                assert read <= length : read + \" vs \" + length;\n+                streamForSequentialReads.pos += read;\n+                position += read;\n+                pos += read;\n+                offset += read;\n+                length -= read;\n+\n+                if (streamForSequentialReads.isFullyRead()) {\n+                    if (streamForSequentialReadsRef.compareAndSet(streamForSequentialReads, null) != false) {\n+                        streamForSequentialReads.close();\n+                    } else {\n+                        // something happened concurrently, defensively stop optimizing\n+                        sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+                    }\n+\n+                    if (length == 0) {\n+                        // the current stream contained precisely enough data for this read, so we're good.\n+                        return;\n+                    } else {\n+                        // the current stream didn't contain enough data for this read, so we must read more\n+                        if (sequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION\n+                            && tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                            return;\n+                        }\n+                    }\n+                } else {\n+                    // the current stream contained enough data for this read and more besides, so we leave it alone.\n+                    assert length == 0 : length + \" remaining\";\n+                    return;\n+                }\n+            } else {\n+                // not a sequential read, so stop optimizing for this usage pattern and fall through to the unoptimized behaviour\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+                IOUtils.close(streamForSequentialReadsRef.getAndSet(null));\n+            }\n+        }\n+\n+        // read part of a blob directly; the code above falls through to this case where there is no optimization possible\n         try (InputStream inputStream = blobContainer.readBlob(fileInfo.partName(part), pos, length)) {\n-            int read = inputStream.read(b, offset, length);\n-            assert read == length;\n+            final int read = inputStream.read(b, offset, length);\n+            assert read == length : read + \" vs \" + length;\n             position += read;\n         }\n     }\n \n+    /**\n+     * If appropriate, open a new stream for sequential reading and satisfy the given read using it. Returns whether this happened or not;\n+     * if it did not happen then nothing was read, and the caller should perform the read directly.\n+     */\n+    private boolean tryReadAndKeepStreamOpen(int part, long pos, byte[] b, int offset, int length, long currentSequentialReadSize)\n+        throws IOException {\n+\n+        assert streamForSequentialReadsRef.get() == null : \"should only be called when a new stream is needed\";\n+        assert currentSequentialReadSize > 0L : \"should not be called if \";\n+\n+        final long streamLength = Math.min(currentSequentialReadSize, fileInfo.partBytes(part) - pos);\n+        if (length < streamLength) {\n+            // if we open a stream of length streamLength then it will not be completely consumed by this read, so it is worthwhile to open\n+            // it and keep it open for future reads\n+            final InputStream inputStream = blobContainer.readBlob(fileInfo.partName(part), pos, streamLength);\n+            final StreamForSequentialReads newStreamForSequentialReads\n+                = new StreamForSequentialReads(inputStream, part, pos, streamLength);\n+            if (streamForSequentialReadsRef.compareAndSet(null, newStreamForSequentialReads) == false) {\n+                // something happened concurrently, defensively stop optimizing and fall through to the unoptimized behaviour\n+                this.sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+                inputStream.close();\n+                return false;\n+            }\n+\n+            final int read = newStreamForSequentialReads.inputStream.read(b, offset, length);\n+            assert read == length : read + \" vs \" + length;\n+            position += read;\n+            newStreamForSequentialReads.pos += read;\n+            assert newStreamForSequentialReads.isFullyRead() == false;\n+            return true;\n+        } else {\n+            // streamLength <= length so this single read will consume the entire stream, so there is no need to keep hold of it, so we can\n+            // tell the caller to read the data directly", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57"}, "originalPosition": 165}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9359e70c6e9f19ee918e967d85868527293abf58", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/9359e70c6e9f19ee918e967d85868527293abf58", "committedDate": "2020-01-23T11:39:37Z", "message": "Sequential read size depends on container"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "78087ab4e1c08c13002abfa5a37a1606cd4d7f98", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/78087ab4e1c08c13002abfa5a37a1606cd4d7f98", "committedDate": "2020-01-23T11:40:47Z", "message": "Fix partial assertion message"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "33a1362ce45b035158f1243a5935548c8a5e782a", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/33a1362ce45b035158f1243a5935548c8a5e782a", "committedDate": "2020-01-23T11:41:23Z", "message": "(!= false) == (== true)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "84d12b606f06c3d37e2dd9bb24549356079d913a", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/84d12b606f06c3d37e2dd9bb24549356079d913a", "committedDate": "2020-01-23T11:47:26Z", "message": "Adjust behaviour now that the only concurrent thing we support is closing"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1d692930391c32e0ca4f2bf0fd6d0e6fbbce7ec6", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/1d692930391c32e0ca4f2bf0fd6d0e6fbbce7ec6", "committedDate": "2020-01-23T12:37:11Z", "message": "readOptimized()"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "184c9b13764642baa3a6daf9ccabce54e4ef24d5", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/184c9b13764642baa3a6daf9ccabce54e4ef24d5", "committedDate": "2020-01-23T12:51:57Z", "message": "More refactoring into StreamForSequentialReads etc."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "70b44c3667d6a515d2a9da07c56767d8d48179bb", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/70b44c3667d6a515d2a9da07c56767d8d48179bb", "committedDate": "2020-01-23T12:53:26Z", "message": "Merge branch 'feature/searchable-snapshots' into 2020-01-20-searchable-snapshot-readahead"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f4094c7eca2756d9d6b1efb7ea04af1b9cd6a843", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/f4094c7eca2756d9d6b1efb7ea04af1b9cd6a843", "committedDate": "2020-01-23T13:57:35Z", "message": "Imports"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4a88bb4c8060d5f93cb351d9ad208e3c6a9b5f6a", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/4a88bb4c8060d5f93cb351d9ad208e3c6a9b5f6a", "committedDate": "2020-02-03T16:20:46Z", "message": "Merge branch 'feature/searchable-snapshots' into 2020-01-20-searchable-snapshot-readahead"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "eeab774cc90806282b81ac163e5373558973d361", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/eeab774cc90806282b81ac163e5373558973d361", "committedDate": "2020-02-03T16:46:41Z", "message": "No need to handle concurrent closing"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a92ced530401cece9969728d48d17b1196f1ec0b", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/a92ced530401cece9969728d48d17b1196f1ec0b", "committedDate": "2020-02-03T17:03:08Z", "message": "Merge branch 'feature/searchable-snapshots' into 2020-01-20-searchable-snapshot-readahead"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUzMDA5MTY3", "url": "https://github.com/elastic/elasticsearch/pull/51230#pullrequestreview-353009167", "createdAt": "2020-02-04T14:19:05Z", "commit": {"oid": "a92ced530401cece9969728d48d17b1196f1ec0b"}, "state": "APPROVED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxNDoxOTowNlrOFlVovw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxNDoyNTozMVrOFlV3Fw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDY5NjEyNw==", "bodyText": "Maybe update the class javadoc to explain how/why we use this?", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r374696127", "createdAt": "2020-02-04T14:19:06Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -41,20 +44,31 @@\n     private final long length;\n \n     private long position;\n-    private boolean closed;\n+    private volatile boolean closed;\n \n-    public SearchableSnapshotIndexInput(final BlobContainer blobContainer, final FileInfo fileInfo) {\n-        this(\"SearchableSnapshotIndexInput(\" + fileInfo.physicalName() + \")\", blobContainer, fileInfo, 0L, 0L, fileInfo.length());\n+    // optimisation for the case where we perform a single seek, then read a large block of data sequentially, then close the input\n+    @Nullable // if not currently reading sequentially\n+    private StreamForSequentialReads streamForSequentialReads;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a92ced530401cece9969728d48d17b1196f1ec0b"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDY5NzIyOA==", "bodyText": "We're closing + nullify the streamForSequentialReads many times, maybe it deserves its own closeSequentialStream() method?", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r374697228", "createdAt": "2020-02-04T14:21:05Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,12 +107,87 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n-        try (InputStream inputStream = blobContainer.readBlob(fileInfo.partName(part), pos, length)) {\n-            int read = inputStream.read(b, offset, length);\n-            assert read == length;\n-            position += read;\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        int optimizedReadSize = readOptimized(part, pos, b, offset, length);\n+        assert optimizedReadSize <= length;\n+        position += optimizedReadSize;\n+\n+        if (optimizedReadSize < length) {\n+            // we did not read everything in an optimized fashion, so read the remainder directly\n+            try (InputStream inputStream\n+                     = blobContainer.readBlob(fileInfo.partName(part), pos + optimizedReadSize, length - optimizedReadSize)) {\n+                final int directReadSize = inputStream.read(b, offset + optimizedReadSize, length - optimizedReadSize);\n+                assert optimizedReadSize + directReadSize == length : optimizedReadSize + \" and \" + directReadSize + \" vs \" + length;\n+                position += directReadSize;\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Attempt to satisfy this read in an optimized fashion using {@code streamForSequentialReadsRef}.\n+     * @return the number of bytes read\n+     */\n+    private int readOptimized(int part, long pos, byte[] b, int offset, int length) throws IOException {\n+        if (sequentialReadSize == NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            return 0;\n+        }\n+\n+        int read = 0;\n+        if (streamForSequentialReads == null) {\n+            // starting a new sequential read\n+            read = readFromNewSequentialStream(part, pos, b, offset, length);\n+        } else if (streamForSequentialReads.canContinueSequentialRead(part, pos)) {\n+            // continuing a sequential read that we started previously\n+            read = streamForSequentialReads.read(b, offset, length);\n+            if (streamForSequentialReads.isFullyRead()) {\n+                // the current stream was exhausted by this read, so it should be closed\n+                streamForSequentialReads.close();\n+                streamForSequentialReads = null;\n+            } else {\n+                // the current stream contained enough data for this read and more besides, so we leave it in place\n+                assert read == length : length + \" remaining\";\n+            }\n+\n+            if (read < length) {\n+                // the current stream didn't contain enough data for this read, so we must read more\n+                read += readFromNewSequentialStream(part, pos + read, b, offset + read, length - read);\n+            }\n+        } else {\n+            // not a sequential read, so stop optimizing for this usage pattern and fall through to the unoptimized behaviour\n+            assert streamForSequentialReads.isFullyRead() == false;\n+            sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+            IOUtils.close(streamForSequentialReads);\n+            streamForSequentialReads = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a92ced530401cece9969728d48d17b1196f1ec0b"}, "originalPosition": 125}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDY5ODQ0MA==", "bodyText": "The method signature can fit on a single line", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r374698440", "createdAt": "2020-02-04T14:23:09Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,12 +107,87 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n-        try (InputStream inputStream = blobContainer.readBlob(fileInfo.partName(part), pos, length)) {\n-            int read = inputStream.read(b, offset, length);\n-            assert read == length;\n-            position += read;\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        int optimizedReadSize = readOptimized(part, pos, b, offset, length);\n+        assert optimizedReadSize <= length;\n+        position += optimizedReadSize;\n+\n+        if (optimizedReadSize < length) {\n+            // we did not read everything in an optimized fashion, so read the remainder directly\n+            try (InputStream inputStream\n+                     = blobContainer.readBlob(fileInfo.partName(part), pos + optimizedReadSize, length - optimizedReadSize)) {\n+                final int directReadSize = inputStream.read(b, offset + optimizedReadSize, length - optimizedReadSize);\n+                assert optimizedReadSize + directReadSize == length : optimizedReadSize + \" and \" + directReadSize + \" vs \" + length;\n+                position += directReadSize;\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Attempt to satisfy this read in an optimized fashion using {@code streamForSequentialReadsRef}.\n+     * @return the number of bytes read\n+     */\n+    private int readOptimized(int part, long pos, byte[] b, int offset, int length) throws IOException {\n+        if (sequentialReadSize == NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            return 0;\n+        }\n+\n+        int read = 0;\n+        if (streamForSequentialReads == null) {\n+            // starting a new sequential read\n+            read = readFromNewSequentialStream(part, pos, b, offset, length);\n+        } else if (streamForSequentialReads.canContinueSequentialRead(part, pos)) {\n+            // continuing a sequential read that we started previously\n+            read = streamForSequentialReads.read(b, offset, length);\n+            if (streamForSequentialReads.isFullyRead()) {\n+                // the current stream was exhausted by this read, so it should be closed\n+                streamForSequentialReads.close();\n+                streamForSequentialReads = null;\n+            } else {\n+                // the current stream contained enough data for this read and more besides, so we leave it in place\n+                assert read == length : length + \" remaining\";\n+            }\n+\n+            if (read < length) {\n+                // the current stream didn't contain enough data for this read, so we must read more\n+                read += readFromNewSequentialStream(part, pos + read, b, offset + read, length - read);\n+            }\n+        } else {\n+            // not a sequential read, so stop optimizing for this usage pattern and fall through to the unoptimized behaviour\n+            assert streamForSequentialReads.isFullyRead() == false;\n+            sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+            IOUtils.close(streamForSequentialReads);\n+            streamForSequentialReads = null;\n         }\n+        return read;\n+    }\n+\n+    /**\n+     * If appropriate, open a new stream for sequential reading and satisfy the given read using it.\n+     * @return the number of bytes read; if a new stream wasn't opened then nothing was read so the caller should perform the read directly.\n+     */\n+    private int readFromNewSequentialStream(int part, long pos, byte[] b, int offset, int length)\n+        throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a92ced530401cece9969728d48d17b1196f1ec0b"}, "originalPosition": 135}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDY5OTQ4NA==", "bodyText": "Maybe nullify in a finally block, just in case", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r374699484", "createdAt": "2020-02-04T14:24:56Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -108,19 +197,24 @@ protected void seekInternal(long pos) throws IOException {\n         } else if (pos < 0L) {\n             throw new IOException(\"Seeking to negative position [\" + pos + \"] for \" + toString());\n         }\n-        this.position = offset + pos;\n+        if (position != offset + pos) {\n+            position = offset + pos;\n+            IOUtils.close(streamForSequentialReads);\n+            streamForSequentialReads = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a92ced530401cece9969728d48d17b1196f1ec0b"}, "originalPosition": 167}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDY5OTc5OQ==", "bodyText": "Maybe add a small word on why we can't read optimized for clones?", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r374699799", "createdAt": "2020-02-04T14:25:31Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -108,19 +197,24 @@ protected void seekInternal(long pos) throws IOException {\n         } else if (pos < 0L) {\n             throw new IOException(\"Seeking to negative position [\" + pos + \"] for \" + toString());\n         }\n-        this.position = offset + pos;\n+        if (position != offset + pos) {\n+            position = offset + pos;\n+            IOUtils.close(streamForSequentialReads);\n+            streamForSequentialReads = null;\n+        }\n     }\n \n     @Override\n     public BufferedIndexInput clone() {\n-        return new SearchableSnapshotIndexInput(\"clone(\" + this + \")\", blobContainer, fileInfo, position, offset, length);\n+        return new SearchableSnapshotIndexInput(\"clone(\" + this + \")\", blobContainer, fileInfo, position, offset, length,\n+            NO_SEQUENTIAL_READ_OPTIMIZATION, getBufferSize());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a92ced530401cece9969728d48d17b1196f1ec0b"}, "originalPosition": 175}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c9cf7bcd11d91a676f60d3a6bdb4c9b44858c368", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/c9cf7bcd11d91a676f60d3a6bdb4c9b44858c368", "committedDate": "2020-02-04T15:41:18Z", "message": "Extract closeStreamForSequentialReads and use finally"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "51d2af507dde139718a13d865cb2522b3f0f6a8e", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/51d2af507dde139718a13d865cb2522b3f0f6a8e", "committedDate": "2020-02-04T15:41:35Z", "message": "Whitespace"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2fee6b851ac4482dd2ec43c46c87debdbeb71113", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/2fee6b851ac4482dd2ec43c46c87debdbeb71113", "committedDate": "2020-02-04T15:45:09Z", "message": "Comment on why slices/clones are not optimized"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f18251a597595b441b09ac46432623163a25caf0", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/f18251a597595b441b09ac46432623163a25caf0", "committedDate": "2020-02-04T15:54:06Z", "message": "Add Javadoc"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f9d4def49d7976011413d08e16de7023b279670c", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/f9d4def49d7976011413d08e16de7023b279670c", "committedDate": "2020-02-04T15:56:53Z", "message": "typo"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3045, "cost": 1, "resetAt": "2021-10-28T18:54:27Z"}}}