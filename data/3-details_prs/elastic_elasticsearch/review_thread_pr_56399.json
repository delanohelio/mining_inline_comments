{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE1MDIwMDgy", "number": 56399, "reviewThreads": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQxMjoxMToyOFrOD6nmUg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQxNzo1NTo0OFrOD7w0eQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyNzkyNzg2OnYy", "diffSide": "RIGHT", "path": "docs/reference/aggregations/pipeline/normalize-aggregation.asciidoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQxMjoxMToyOFrOGSjbNA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwMDowNTo1OFrOGTxkgg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjEwNzk1Ng==", "bodyText": "This'll want [role=\"xpack\"] or so it'll get the little xpack bug.", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r422107956", "createdAt": "2020-05-08T12:11:28Z", "author": {"login": "nik9000"}, "path": "docs/reference/aggregations/pipeline/normalize-aggregation.asciidoc", "diffHunk": "@@ -0,0 +1,119 @@\n+[[search-aggregations-pipeline-normalize-aggregation]]", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5427899ada2304388686d28ce8f3c1cd435dee88"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzM4ODI5MA==", "bodyText": "added role!", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r423388290", "createdAt": "2020-05-12T00:05:58Z", "author": {"login": "talevy"}, "path": "docs/reference/aggregations/pipeline/normalize-aggregation.asciidoc", "diffHunk": "@@ -0,0 +1,119 @@\n+[[search-aggregations-pipeline-normalize-aggregation]]", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjEwNzk1Ng=="}, "originalCommit": {"oid": "5427899ada2304388686d28ce8f3c1cd435dee88"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyNzkyOTc1OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineAggregationBuilder.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQxMjoxMjowOVrOGSjcOw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQxMjo0ODo1MlrOGUE6rw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjEwODIxOQ==", "bodyText": "Check out InstantiatingObjectParser!", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r422108219", "createdAt": "2020-05-08T12:12:09Z", "author": {"login": "nik9000"}, "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineAggregationBuilder.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.BucketMetricsParser;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.function.Function;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.constructorArg;\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregator.Parser.FORMAT;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.Mean;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.Percent;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.RescaleZeroToOne;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.RescaleZeroToOneHundred;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.Softmax;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.ZScore;\n+\n+public class NormalizePipelineAggregationBuilder extends AbstractPipelineAggregationBuilder<NormalizePipelineAggregationBuilder> {\n+    public static final String NAME = \"normalize\";\n+    static final ParseField NORMALIZER_FIELD = new ParseField(\"normalizer\");\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public static final ConstructingObjectParser<NormalizePipelineAggregationBuilder, String> PARSER = new ConstructingObjectParser<>(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5427899ada2304388686d28ce8f3c1cd435dee88"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzM4ODcxMw==", "bodyText": "so, I tried changing that parser to work here, but I think it deserves its own change. The InstantiatingObjectParser does not expose the Context in such a way that more constructor arguments can be passed in. I believe this can change, but I'd rather not do that here", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r423388713", "createdAt": "2020-05-12T00:07:32Z", "author": {"login": "talevy"}, "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineAggregationBuilder.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.BucketMetricsParser;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.function.Function;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.constructorArg;\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregator.Parser.FORMAT;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.Mean;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.Percent;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.RescaleZeroToOne;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.RescaleZeroToOneHundred;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.Softmax;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.ZScore;\n+\n+public class NormalizePipelineAggregationBuilder extends AbstractPipelineAggregationBuilder<NormalizePipelineAggregationBuilder> {\n+    public static final String NAME = \"normalize\";\n+    static final ParseField NORMALIZER_FIELD = new ParseField(\"normalizer\");\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public static final ConstructingObjectParser<NormalizePipelineAggregationBuilder, String> PARSER = new ConstructingObjectParser<>(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjEwODIxOQ=="}, "originalCommit": {"oid": "5427899ada2304388686d28ce8f3c1cd435dee88"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzcwNTI2Mw==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r423705263", "createdAt": "2020-05-12T12:48:52Z", "author": {"login": "nik9000"}, "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineAggregationBuilder.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.BucketMetricsParser;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.function.Function;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.constructorArg;\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregator.Parser.FORMAT;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.Mean;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.Percent;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.RescaleZeroToOne;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.RescaleZeroToOneHundred;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.Softmax;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.ZScore;\n+\n+public class NormalizePipelineAggregationBuilder extends AbstractPipelineAggregationBuilder<NormalizePipelineAggregationBuilder> {\n+    public static final String NAME = \"normalize\";\n+    static final ParseField NORMALIZER_FIELD = new ParseField(\"normalizer\");\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public static final ConstructingObjectParser<NormalizePipelineAggregationBuilder, String> PARSER = new ConstructingObjectParser<>(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjEwODIxOQ=="}, "originalCommit": {"oid": "5427899ada2304388686d28ce8f3c1cd435dee88"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyNzkzMzQzOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineAggregationBuilder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQxMjoxMzozNFrOGSjeTQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQxMjoxMzozNFrOGSjeTQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjEwODc0OQ==", "bodyText": "The normalizer should probably be in hashCode and equals, right?", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r422108749", "createdAt": "2020-05-08T12:13:34Z", "author": {"login": "nik9000"}, "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineAggregationBuilder.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.BucketMetricsParser;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.function.Function;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.constructorArg;\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregator.Parser.FORMAT;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.Mean;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.Percent;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.RescaleZeroToOne;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.RescaleZeroToOneHundred;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.Softmax;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.ZScore;\n+\n+public class NormalizePipelineAggregationBuilder extends AbstractPipelineAggregationBuilder<NormalizePipelineAggregationBuilder> {\n+    public static final String NAME = \"normalize\";\n+    static final ParseField NORMALIZER_FIELD = new ParseField(\"normalizer\");\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public static final ConstructingObjectParser<NormalizePipelineAggregationBuilder, String> PARSER = new ConstructingObjectParser<>(\n+        NAME, false, (args, name) -> new NormalizePipelineAggregationBuilder(name, (String) args[0],\n+        (String) args[1], (List<String>) args[2]));\n+\n+    static {\n+        PARSER.declareString(optionalConstructorArg(), FORMAT);\n+        PARSER.declareString(constructorArg(), NORMALIZER_FIELD);\n+        PARSER.declareStringArray(constructorArg(), BUCKETS_PATH_FIELD);\n+    }\n+\n+    static final Map<String, Function<List<Double>, NormalizePipelineNormalizer>> NAME_MAP = Map.of(\n+        RescaleZeroToOne.NAME, RescaleZeroToOne::new,\n+        RescaleZeroToOneHundred.NAME, RescaleZeroToOneHundred::new,\n+        Mean.NAME, Mean::new,\n+        ZScore.NAME, ZScore::new,\n+        Percent.NAME, Percent::new,\n+        Softmax.NAME, Softmax::new\n+    );\n+\n+    static String validateNormalizerName(String name) {\n+        if (NAME_MAP.containsKey(name)) {\n+            return name;\n+        }\n+\n+        throw new IllegalArgumentException(\"invalid normalizer [\" + name + \"]\");\n+    }\n+\n+    private final String format;\n+    private final String normalizer;\n+\n+\n+    NormalizePipelineAggregationBuilder(String name, String format, String normalizer, List<String> bucketsPath) {\n+        super(name, NAME, bucketsPath.toArray(new String[0]));\n+        this.format = format;\n+        this.normalizer = validateNormalizerName(normalizer);\n+    }\n+\n+    NormalizePipelineAggregationBuilder(String name, String format, String normalizer, String bucketsPath) {\n+        super(name, NAME, new String[] { bucketsPath });\n+        this.format = format;\n+        this.normalizer = validateNormalizerName(normalizer);\n+    }\n+\n+    /**\n+     * Read from a stream.\n+     */\n+    public NormalizePipelineAggregationBuilder(StreamInput in) throws IOException {\n+        super(in, NAME);\n+        format = in.readOptionalString();\n+        normalizer = in.readString();\n+    }\n+\n+    @Override\n+    protected final void doWriteTo(StreamOutput out) throws IOException {\n+        out.writeOptionalString(format);\n+        out.writeString(normalizer);\n+    }\n+\n+    /**\n+     * Gets the format to use on the output of this aggregation.\n+     */\n+    public String format() {\n+        return format;\n+    }\n+\n+    protected DocValueFormat formatter() {\n+        if (format != null) {\n+            return new DocValueFormat.Decimal(format);\n+        } else {\n+            return DocValueFormat.RAW;\n+        }\n+    }\n+\n+    @Override\n+    protected PipelineAggregator createInternal(Map<String, Object> metadata) {\n+        return new NormalizePipelineAggregator(name, bucketsPaths, formatter(), NAME_MAP.get(normalizer), metadata);\n+    }\n+\n+    @Override\n+    protected void validate(ValidationContext context) {\n+        if (bucketsPaths.length != 1) {\n+            context.addBucketPathValidationError(\"must contain a single entry for aggregation [\" + name + \"]\");\n+        }\n+        context.validateParentAggSequentiallyOrdered(NAME, name);\n+    }\n+\n+    @Override\n+    protected final XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {\n+        if (format != null) {\n+            builder.field(BucketMetricsParser.FORMAT.getPreferredName(), format);\n+        }\n+        builder.field(NORMALIZER_FIELD.getPreferredName(), normalizer);\n+        return builder;\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+        return Objects.hash(super.hashCode(), format);\n+    }\n+\n+    @Override\n+    public boolean equals(Object obj) {\n+        if (this == obj) return true;\n+        if (obj == null || getClass() != obj.getClass()) return false;\n+        if (super.equals(obj) == false) return false;\n+        NormalizePipelineAggregationBuilder other = (NormalizePipelineAggregationBuilder) obj;\n+        return Objects.equals(format, other.format);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5427899ada2304388686d28ce8f3c1cd435dee88"}, "originalPosition": 146}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyNzkzODY4OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineAggregator.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQxMjoxNTozNFrOGSjhhg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQxMjo0OTowNlrOGUE7RA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjEwOTU3NA==", "bodyText": "bucket.getAggregations().copyResults() does this without so much boiler plate.", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r422109574", "createdAt": "2020-05-08T12:15:34Z", "author": {"login": "nik9000"}, "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineAggregator.java", "diffHunk": "@@ -0,0 +1,78 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.aggregations.InternalAggregation;\n+import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContext;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;\n+import org.elasticsearch.search.aggregations.bucket.MultiBucketsAggregation.Bucket;\n+import org.elasticsearch.search.aggregations.bucket.histogram.HistogramFactory;\n+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;\n+import org.elasticsearch.search.aggregations.pipeline.InternalSimpleValue;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+\n+import static org.elasticsearch.search.aggregations.pipeline.BucketHelpers.resolveBucketValue;\n+\n+public class NormalizePipelineAggregator extends PipelineAggregator {\n+    private final DocValueFormat formatter;\n+    private final Function<List<Double>, NormalizePipelineNormalizer> normalizerSupplier;\n+\n+    NormalizePipelineAggregator(String name, String[] bucketsPaths, DocValueFormat formatter,\n+                                Function<List<Double>, NormalizePipelineNormalizer> normalizerSupplier,\n+                                Map<String, Object> metadata) {\n+        super(name, bucketsPaths, metadata);\n+        this.formatter = formatter;\n+        this.normalizerSupplier = normalizerSupplier;\n+    }\n+\n+    @Override\n+    public InternalAggregation reduce(InternalAggregation aggregation, ReduceContext reduceContext) {\n+        InternalMultiBucketAggregation<? extends InternalMultiBucketAggregation, ? extends InternalMultiBucketAggregation.InternalBucket>\n+                histo = (InternalMultiBucketAggregation<? extends InternalMultiBucketAggregation, ? extends\n+                InternalMultiBucketAggregation.InternalBucket>) aggregation;\n+        List<? extends InternalMultiBucketAggregation.InternalBucket> buckets = histo.getBuckets();\n+        HistogramFactory factory = (HistogramFactory) histo;\n+        List<Bucket> newBuckets = new ArrayList<>(buckets.size());\n+\n+        List<Double> values = buckets.stream().map(bucket -> resolveBucketValue(histo, bucket, bucketsPaths()[0], GapPolicy.SKIP))\n+            .collect(Collectors.toList());\n+\n+        NormalizePipelineNormalizer normalizer = normalizerSupplier.apply(values);\n+\n+        for (int i = 0; i < buckets.size(); i++) {\n+            InternalMultiBucketAggregation.InternalBucket bucket = buckets.get(i);\n+            Double thisBucketValue = values.get(i);\n+\n+            final double normalizedBucketValue;\n+\n+            // Only account for finite values\n+            if (thisBucketValue.isNaN()) {\n+                normalizedBucketValue = Double.NaN;\n+            } else {\n+                normalizedBucketValue = normalizer.normalize(thisBucketValue);\n+            }\n+\n+            List<InternalAggregation> aggs = StreamSupport.stream(bucket.getAggregations().spliterator(), false)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5427899ada2304388686d28ce8f3c1cd435dee88"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzM4ODkzOQ==", "bodyText": "unfortunately, that method does not work in this context. I think a more dedicated cleanup for this boilerplate can be tackled outside of this PR", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r423388939", "createdAt": "2020-05-12T00:08:12Z", "author": {"login": "talevy"}, "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineAggregator.java", "diffHunk": "@@ -0,0 +1,78 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.aggregations.InternalAggregation;\n+import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContext;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;\n+import org.elasticsearch.search.aggregations.bucket.MultiBucketsAggregation.Bucket;\n+import org.elasticsearch.search.aggregations.bucket.histogram.HistogramFactory;\n+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;\n+import org.elasticsearch.search.aggregations.pipeline.InternalSimpleValue;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+\n+import static org.elasticsearch.search.aggregations.pipeline.BucketHelpers.resolveBucketValue;\n+\n+public class NormalizePipelineAggregator extends PipelineAggregator {\n+    private final DocValueFormat formatter;\n+    private final Function<List<Double>, NormalizePipelineNormalizer> normalizerSupplier;\n+\n+    NormalizePipelineAggregator(String name, String[] bucketsPaths, DocValueFormat formatter,\n+                                Function<List<Double>, NormalizePipelineNormalizer> normalizerSupplier,\n+                                Map<String, Object> metadata) {\n+        super(name, bucketsPaths, metadata);\n+        this.formatter = formatter;\n+        this.normalizerSupplier = normalizerSupplier;\n+    }\n+\n+    @Override\n+    public InternalAggregation reduce(InternalAggregation aggregation, ReduceContext reduceContext) {\n+        InternalMultiBucketAggregation<? extends InternalMultiBucketAggregation, ? extends InternalMultiBucketAggregation.InternalBucket>\n+                histo = (InternalMultiBucketAggregation<? extends InternalMultiBucketAggregation, ? extends\n+                InternalMultiBucketAggregation.InternalBucket>) aggregation;\n+        List<? extends InternalMultiBucketAggregation.InternalBucket> buckets = histo.getBuckets();\n+        HistogramFactory factory = (HistogramFactory) histo;\n+        List<Bucket> newBuckets = new ArrayList<>(buckets.size());\n+\n+        List<Double> values = buckets.stream().map(bucket -> resolveBucketValue(histo, bucket, bucketsPaths()[0], GapPolicy.SKIP))\n+            .collect(Collectors.toList());\n+\n+        NormalizePipelineNormalizer normalizer = normalizerSupplier.apply(values);\n+\n+        for (int i = 0; i < buckets.size(); i++) {\n+            InternalMultiBucketAggregation.InternalBucket bucket = buckets.get(i);\n+            Double thisBucketValue = values.get(i);\n+\n+            final double normalizedBucketValue;\n+\n+            // Only account for finite values\n+            if (thisBucketValue.isNaN()) {\n+                normalizedBucketValue = Double.NaN;\n+            } else {\n+                normalizedBucketValue = normalizer.normalize(thisBucketValue);\n+            }\n+\n+            List<InternalAggregation> aggs = StreamSupport.stream(bucket.getAggregations().spliterator(), false)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjEwOTU3NA=="}, "originalCommit": {"oid": "5427899ada2304388686d28ce8f3c1cd435dee88"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzcwNTQxMg==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r423705412", "createdAt": "2020-05-12T12:49:06Z", "author": {"login": "nik9000"}, "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineAggregator.java", "diffHunk": "@@ -0,0 +1,78 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.aggregations.InternalAggregation;\n+import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContext;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;\n+import org.elasticsearch.search.aggregations.bucket.MultiBucketsAggregation.Bucket;\n+import org.elasticsearch.search.aggregations.bucket.histogram.HistogramFactory;\n+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;\n+import org.elasticsearch.search.aggregations.pipeline.InternalSimpleValue;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+\n+import static org.elasticsearch.search.aggregations.pipeline.BucketHelpers.resolveBucketValue;\n+\n+public class NormalizePipelineAggregator extends PipelineAggregator {\n+    private final DocValueFormat formatter;\n+    private final Function<List<Double>, NormalizePipelineNormalizer> normalizerSupplier;\n+\n+    NormalizePipelineAggregator(String name, String[] bucketsPaths, DocValueFormat formatter,\n+                                Function<List<Double>, NormalizePipelineNormalizer> normalizerSupplier,\n+                                Map<String, Object> metadata) {\n+        super(name, bucketsPaths, metadata);\n+        this.formatter = formatter;\n+        this.normalizerSupplier = normalizerSupplier;\n+    }\n+\n+    @Override\n+    public InternalAggregation reduce(InternalAggregation aggregation, ReduceContext reduceContext) {\n+        InternalMultiBucketAggregation<? extends InternalMultiBucketAggregation, ? extends InternalMultiBucketAggregation.InternalBucket>\n+                histo = (InternalMultiBucketAggregation<? extends InternalMultiBucketAggregation, ? extends\n+                InternalMultiBucketAggregation.InternalBucket>) aggregation;\n+        List<? extends InternalMultiBucketAggregation.InternalBucket> buckets = histo.getBuckets();\n+        HistogramFactory factory = (HistogramFactory) histo;\n+        List<Bucket> newBuckets = new ArrayList<>(buckets.size());\n+\n+        List<Double> values = buckets.stream().map(bucket -> resolveBucketValue(histo, bucket, bucketsPaths()[0], GapPolicy.SKIP))\n+            .collect(Collectors.toList());\n+\n+        NormalizePipelineNormalizer normalizer = normalizerSupplier.apply(values);\n+\n+        for (int i = 0; i < buckets.size(); i++) {\n+            InternalMultiBucketAggregation.InternalBucket bucket = buckets.get(i);\n+            Double thisBucketValue = values.get(i);\n+\n+            final double normalizedBucketValue;\n+\n+            // Only account for finite values\n+            if (thisBucketValue.isNaN()) {\n+                normalizedBucketValue = Double.NaN;\n+            } else {\n+                normalizedBucketValue = normalizer.normalize(thisBucketValue);\n+            }\n+\n+            List<InternalAggregation> aggs = StreamSupport.stream(bucket.getAggregations().spliterator(), false)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjEwOTU3NA=="}, "originalCommit": {"oid": "5427899ada2304388686d28ce8f3c1cd435dee88"}, "originalPosition": 68}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyNzk0Mjg3OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineNormalizer.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQxMjoxNzoxOFrOGSjkAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQxMjoxODoyNVrOGSjllg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjExMDIwOQ==", "bodyText": "Could this take a DoubleStream instead?", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r422110209", "createdAt": "2020-05-08T12:17:18Z", "author": {"login": "nik9000"}, "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineNormalizer.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+\n+import java.util.List;\n+\n+abstract class NormalizePipelineNormalizer {\n+\n+    static class RescaleZeroToOne extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"rescale_0_1\";\n+\n+        RescaleZeroToOne(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return (value - min) / (max - min);\n+        }\n+    }\n+\n+    static class RescaleZeroToOneHundred extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"rescale_0_100\";\n+\n+        RescaleZeroToOneHundred(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return 100 * (value - min) / (max - min);\n+        }\n+    }\n+\n+    static class Mean extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"mean\";\n+\n+        Mean(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return (value - mean) / (max - min);\n+        }\n+    }\n+\n+    static class Percent extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"percent\";\n+\n+        Percent(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return value / sum;\n+        }\n+    }\n+\n+    static class ZScore extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"z-score\";\n+\n+        private final double stdev;\n+\n+        ZScore(List<Double> values) {\n+            super(values);\n+            double variance = 0.0;\n+            for (Double value : values) {\n+                if (value.isNaN() == false) {\n+                    variance += Math.pow(value - mean, 2);\n+                }\n+            }\n+            this.stdev = Math.sqrt(variance / count);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return (value - mean) / stdev;\n+        }\n+    }\n+\n+    static class Softmax extends NormalizePipelineNormalizer {\n+        static final String NAME = \"softmax\";\n+\n+        private double sumExp;\n+\n+        Softmax(List<Double> values) {\n+            double sumExp = 0.0;\n+            for (Double value :  values) {\n+                if (value.isNaN() == false) {\n+                    sumExp += Math.exp(value);\n+                }\n+            }\n+\n+            this.sumExp = sumExp;\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return Math.exp(value) / sumExp;\n+        }\n+    }\n+\n+    abstract double normalize(double value);\n+\n+    abstract static class SinglePassSimpleStatisticsNormalizer extends NormalizePipelineNormalizer {\n+        protected final double max;\n+        protected final double min;\n+        protected final double sum;\n+        protected final double mean;\n+        protected final int count;\n+\n+        SinglePassSimpleStatisticsNormalizer(List<Double> values) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5427899ada2304388686d28ce8f3c1cd435dee88"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjExMDQ5MA==", "bodyText": "You'd only get a single pass but you wouldn't need to make Double objects which is kind of nice.", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r422110490", "createdAt": "2020-05-08T12:18:08Z", "author": {"login": "nik9000"}, "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineNormalizer.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+\n+import java.util.List;\n+\n+abstract class NormalizePipelineNormalizer {\n+\n+    static class RescaleZeroToOne extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"rescale_0_1\";\n+\n+        RescaleZeroToOne(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return (value - min) / (max - min);\n+        }\n+    }\n+\n+    static class RescaleZeroToOneHundred extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"rescale_0_100\";\n+\n+        RescaleZeroToOneHundred(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return 100 * (value - min) / (max - min);\n+        }\n+    }\n+\n+    static class Mean extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"mean\";\n+\n+        Mean(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return (value - mean) / (max - min);\n+        }\n+    }\n+\n+    static class Percent extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"percent\";\n+\n+        Percent(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return value / sum;\n+        }\n+    }\n+\n+    static class ZScore extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"z-score\";\n+\n+        private final double stdev;\n+\n+        ZScore(List<Double> values) {\n+            super(values);\n+            double variance = 0.0;\n+            for (Double value : values) {\n+                if (value.isNaN() == false) {\n+                    variance += Math.pow(value - mean, 2);\n+                }\n+            }\n+            this.stdev = Math.sqrt(variance / count);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return (value - mean) / stdev;\n+        }\n+    }\n+\n+    static class Softmax extends NormalizePipelineNormalizer {\n+        static final String NAME = \"softmax\";\n+\n+        private double sumExp;\n+\n+        Softmax(List<Double> values) {\n+            double sumExp = 0.0;\n+            for (Double value :  values) {\n+                if (value.isNaN() == false) {\n+                    sumExp += Math.exp(value);\n+                }\n+            }\n+\n+            this.sumExp = sumExp;\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return Math.exp(value) / sumExp;\n+        }\n+    }\n+\n+    abstract double normalize(double value);\n+\n+    abstract static class SinglePassSimpleStatisticsNormalizer extends NormalizePipelineNormalizer {\n+        protected final double max;\n+        protected final double min;\n+        protected final double sum;\n+        protected final double mean;\n+        protected final int count;\n+\n+        SinglePassSimpleStatisticsNormalizer(List<Double> values) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjExMDIwOQ=="}, "originalCommit": {"oid": "5427899ada2304388686d28ce8f3c1cd435dee88"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjExMDYxNA==", "bodyText": "Not that pipeline aggs are all that efficient here, but I feel compelled to save autoboxing when I can.", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r422110614", "createdAt": "2020-05-08T12:18:25Z", "author": {"login": "nik9000"}, "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineNormalizer.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+\n+import java.util.List;\n+\n+abstract class NormalizePipelineNormalizer {\n+\n+    static class RescaleZeroToOne extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"rescale_0_1\";\n+\n+        RescaleZeroToOne(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return (value - min) / (max - min);\n+        }\n+    }\n+\n+    static class RescaleZeroToOneHundred extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"rescale_0_100\";\n+\n+        RescaleZeroToOneHundred(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return 100 * (value - min) / (max - min);\n+        }\n+    }\n+\n+    static class Mean extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"mean\";\n+\n+        Mean(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return (value - mean) / (max - min);\n+        }\n+    }\n+\n+    static class Percent extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"percent\";\n+\n+        Percent(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return value / sum;\n+        }\n+    }\n+\n+    static class ZScore extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"z-score\";\n+\n+        private final double stdev;\n+\n+        ZScore(List<Double> values) {\n+            super(values);\n+            double variance = 0.0;\n+            for (Double value : values) {\n+                if (value.isNaN() == false) {\n+                    variance += Math.pow(value - mean, 2);\n+                }\n+            }\n+            this.stdev = Math.sqrt(variance / count);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return (value - mean) / stdev;\n+        }\n+    }\n+\n+    static class Softmax extends NormalizePipelineNormalizer {\n+        static final String NAME = \"softmax\";\n+\n+        private double sumExp;\n+\n+        Softmax(List<Double> values) {\n+            double sumExp = 0.0;\n+            for (Double value :  values) {\n+                if (value.isNaN() == false) {\n+                    sumExp += Math.exp(value);\n+                }\n+            }\n+\n+            this.sumExp = sumExp;\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return Math.exp(value) / sumExp;\n+        }\n+    }\n+\n+    abstract double normalize(double value);\n+\n+    abstract static class SinglePassSimpleStatisticsNormalizer extends NormalizePipelineNormalizer {\n+        protected final double max;\n+        protected final double min;\n+        protected final double sum;\n+        protected final double mean;\n+        protected final int count;\n+\n+        SinglePassSimpleStatisticsNormalizer(List<Double> values) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjExMDIwOQ=="}, "originalCommit": {"oid": "5427899ada2304388686d28ce8f3c1cd435dee88"}, "originalPosition": 119}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyNzk0OTIwOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineNormalizer.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQxMjoxOTo1MFrOGSjnzw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQxMjoxOTo1MFrOGSjnzw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjExMTE4Mw==", "bodyText": "I wonder if we could have these normalizers all implement DoubleUnaryOperator instead of making an abstract class for them.", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r422111183", "createdAt": "2020-05-08T12:19:50Z", "author": {"login": "nik9000"}, "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineNormalizer.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+\n+import java.util.List;\n+\n+abstract class NormalizePipelineNormalizer {\n+\n+    static class RescaleZeroToOne extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"rescale_0_1\";\n+\n+        RescaleZeroToOne(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return (value - min) / (max - min);\n+        }\n+    }\n+\n+    static class RescaleZeroToOneHundred extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"rescale_0_100\";\n+\n+        RescaleZeroToOneHundred(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return 100 * (value - min) / (max - min);\n+        }\n+    }\n+\n+    static class Mean extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"mean\";\n+\n+        Mean(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return (value - mean) / (max - min);\n+        }\n+    }\n+\n+    static class Percent extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"percent\";\n+\n+        Percent(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return value / sum;\n+        }\n+    }\n+\n+    static class ZScore extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"z-score\";\n+\n+        private final double stdev;\n+\n+        ZScore(List<Double> values) {\n+            super(values);\n+            double variance = 0.0;\n+            for (Double value : values) {\n+                if (value.isNaN() == false) {\n+                    variance += Math.pow(value - mean, 2);\n+                }\n+            }\n+            this.stdev = Math.sqrt(variance / count);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return (value - mean) / stdev;\n+        }\n+    }\n+\n+    static class Softmax extends NormalizePipelineNormalizer {\n+        static final String NAME = \"softmax\";\n+\n+        private double sumExp;\n+\n+        Softmax(List<Double> values) {\n+            double sumExp = 0.0;\n+            for (Double value :  values) {\n+                if (value.isNaN() == false) {\n+                    sumExp += Math.exp(value);\n+                }\n+            }\n+\n+            this.sumExp = sumExp;\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return Math.exp(value) / sumExp;\n+        }\n+    }\n+\n+    abstract double normalize(double value);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5427899ada2304388686d28ce8f3c1cd435dee88"}, "originalPosition": 110}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyNzk1MzU3OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/src/test/resources/rest-api-spec/test/analytics/normalize.yml", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQxMjoyMTozMlrOGSjqcA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQxMjoyMTozMlrOGSjqcA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjExMTg1Ng==", "bodyText": "Do we need this bit? I don't remember needing it for top_metrics.", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r422111856", "createdAt": "2020-05-08T12:21:32Z", "author": {"login": "nik9000"}, "path": "x-pack/plugin/src/test/resources/rest-api-spec/test/analytics/normalize.yml", "diffHunk": "@@ -0,0 +1,86 @@\n+setup:\n+  - skip:\n+      features: headers\n+  - do:\n+      indices.create:\n+        index: foo\n+        body:\n+          mappings:\n+            properties:\n+              timestamp:\n+                type: date\n+              user:\n+                type: keyword\n+\n+\n+  - do:\n+      headers:\n+        Authorization: \"Basic eF9wYWNrX3Jlc3RfdXNlcjp4LXBhY2stdGVzdC1wYXNzd29yZA==\" # run as x_pack_rest_user, i.e. the test setup superuser", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5427899ada2304388686d28ce8f3c1cd435dee88"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyNzk1NjQ4OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/src/test/resources/rest-api-spec/test/analytics/normalize.yml", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQxMjoyMjoyOVrOGSjsFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQxMjoyMjoyOVrOGSjsFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjExMjI3Nw==", "bodyText": "You'll switch this to normalizer, right?", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r422112277", "createdAt": "2020-05-08T12:22:29Z", "author": {"login": "nik9000"}, "path": "x-pack/plugin/src/test/resources/rest-api-spec/test/analytics/normalize.yml", "diffHunk": "@@ -0,0 +1,86 @@\n+setup:\n+  - skip:\n+      features: headers\n+  - do:\n+      indices.create:\n+        index: foo\n+        body:\n+          mappings:\n+            properties:\n+              timestamp:\n+                type: date\n+              user:\n+                type: keyword\n+\n+\n+  - do:\n+      headers:\n+        Authorization: \"Basic eF9wYWNrX3Jlc3RfdXNlcjp4LXBhY2stdGVzdC1wYXNzd29yZA==\" # run as x_pack_rest_user, i.e. the test setup superuser\n+      bulk:\n+        refresh: true\n+        body:\n+          - index:\n+              _index: \"foo\"\n+          - timestamp: \"2017-01-01T05:00:00Z\"\n+            user: \"a\"\n+\n+          - index:\n+              _index: \"foo\"\n+          - timestamp: \"2017-01-01T05:00:00Z\"\n+            user: \"b\"\n+\n+          - index:\n+              _index: \"foo\"\n+          - timestamp: \"2017-01-01T05:00:00Z\"\n+            user: \"c\"\n+\n+          - index:\n+              _index: \"foo\"\n+          - timestamp: \"2017-01-02T05:00:00Z\"\n+            user: \"a\"\n+\n+          - index:\n+              _index: \"foo\"\n+          - timestamp: \"2017-01-02T05:00:00Z\"\n+            user: \"b\"\n+\n+          - index:\n+              _index: \"foo\"\n+          - timestamp: \"2017-01-03T05:00:00Z\"\n+            user: \"d\"\n+\n+---\n+\"Basic Search\":\n+\n+  - do:\n+      search:\n+        index: \"foo\"\n+        body:\n+          size: 0\n+          aggs:\n+            histo:\n+              date_histogram:\n+                field: \"timestamp\"\n+                calendar_interval: \"day\"\n+              aggs:\n+                sum_users:\n+                  sum:\n+                    field: \"user\"\n+                total_users:\n+                  cumulative_cardinality:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5427899ada2304388686d28ce8f3c1cd435dee88"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzNzEyMTQ5OnYy", "diffSide": "RIGHT", "path": "docs/reference/aggregations/pipeline/normalize-aggregation.asciidoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwNTozMDo0MlrOGT2mxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QyMjo1NTozMFrOGVGUsQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ3MDc4OA==", "bodyText": "I still need to update docs to discuss all the different normalizers", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r423470788", "createdAt": "2020-05-12T05:30:42Z", "author": {"login": "talevy"}, "path": "docs/reference/aggregations/pipeline/normalize-aggregation.asciidoc", "diffHunk": "@@ -0,0 +1,121 @@\n+[role=\"xpack\"]", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9718e01a400b15db41b59bb17d9ee924d5ee0ec1"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDc3Njg4MQ==", "bodyText": "I've added more docs", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r424776881", "createdAt": "2020-05-13T22:55:30Z", "author": {"login": "talevy"}, "path": "docs/reference/aggregations/pipeline/normalize-aggregation.asciidoc", "diffHunk": "@@ -0,0 +1,121 @@\n+[role=\"xpack\"]", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ3MDc4OA=="}, "originalCommit": {"oid": "9718e01a400b15db41b59bb17d9ee924d5ee0ec1"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzOTg3NTM1OnYy", "diffSide": "RIGHT", "path": "docs/reference/aggregations/pipeline/normalize-aggregation.asciidoc", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQxNzo0MzoyMlrOGUR5Rw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQxOToyMzo1N1rOGUViqA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkxNzg5NQ==", "bodyText": "Should we make a note somewhere that this pipeline always uses a skip gap policy?", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r423917895", "createdAt": "2020-05-12T17:43:22Z", "author": {"login": "polyfractal"}, "path": "docs/reference/aggregations/pipeline/normalize-aggregation.asciidoc", "diffHunk": "@@ -0,0 +1,121 @@\n+[role=\"xpack\"]\n+[testenv=\"basic\"]\n+[[search-aggregations-pipeline-normalize-aggregation]]\n+=== Normalize Aggregation\n+\n+A parent pipeline aggregation which calculates the specific normalized/rescaled value for a specific bucket value.\n+\n+==== Syntax\n+\n+A `normalize` aggregation looks like this in isolation:\n+\n+[source,js]\n+--------------------------------------------------\n+{\n+    \"normalize\": {\n+        \"buckets_path\": \"normalized\",\n+        \"normalizer\": \"percent_of_sum\"\n+    }\n+}\n+--------------------------------------------------\n+// NOTCONSOLE\n+\n+[[normalizer_pipeline-params]]", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9c64e3613621bc3ca9e5187ce605e39ff4009167"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzk3NzY0MA==", "bodyText": "good call!", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r423977640", "createdAt": "2020-05-12T19:23:57Z", "author": {"login": "talevy"}, "path": "docs/reference/aggregations/pipeline/normalize-aggregation.asciidoc", "diffHunk": "@@ -0,0 +1,121 @@\n+[role=\"xpack\"]\n+[testenv=\"basic\"]\n+[[search-aggregations-pipeline-normalize-aggregation]]\n+=== Normalize Aggregation\n+\n+A parent pipeline aggregation which calculates the specific normalized/rescaled value for a specific bucket value.\n+\n+==== Syntax\n+\n+A `normalize` aggregation looks like this in isolation:\n+\n+[source,js]\n+--------------------------------------------------\n+{\n+    \"normalize\": {\n+        \"buckets_path\": \"normalized\",\n+        \"normalizer\": \"percent_of_sum\"\n+    }\n+}\n+--------------------------------------------------\n+// NOTCONSOLE\n+\n+[[normalizer_pipeline-params]]", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkxNzg5NQ=="}, "originalCommit": {"oid": "9c64e3613621bc3ca9e5187ce605e39ff4009167"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzOTg4Mzg4OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineAggregationBuilder.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQxNzo0NTo0MFrOGUR-4g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QyMjo1NToyMVrOGVGUhA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkxOTMzMA==", "bodyText": "Fine with normalizer, but wanted to also suggest method as a potential param name.  No strong opinion though :)", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r423919330", "createdAt": "2020-05-12T17:45:40Z", "author": {"login": "polyfractal"}, "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineAggregationBuilder.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.BucketMetricsParser;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.function.DoubleUnaryOperator;\n+import java.util.function.Function;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.constructorArg;\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregator.Parser.FORMAT;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.Mean;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.Percent;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.RescaleZeroToOne;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.RescaleZeroToOneHundred;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.Softmax;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.ZScore;\n+\n+public class NormalizePipelineAggregationBuilder extends AbstractPipelineAggregationBuilder<NormalizePipelineAggregationBuilder> {\n+    public static final String NAME = \"normalize\";\n+    static final ParseField NORMALIZER_FIELD = new ParseField(\"normalizer\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9c64e3613621bc3ca9e5187ce605e39ff4009167"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzk3ODA0OQ==", "bodyText": "I was wishy washy on the naming here as well, and decided not to fret, but I too have leaned towards method earlier, so I am happy to do so here. especially given the overloading of the term across the stack.", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r423978049", "createdAt": "2020-05-12T19:24:40Z", "author": {"login": "talevy"}, "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineAggregationBuilder.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.BucketMetricsParser;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.function.DoubleUnaryOperator;\n+import java.util.function.Function;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.constructorArg;\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregator.Parser.FORMAT;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.Mean;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.Percent;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.RescaleZeroToOne;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.RescaleZeroToOneHundred;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.Softmax;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.ZScore;\n+\n+public class NormalizePipelineAggregationBuilder extends AbstractPipelineAggregationBuilder<NormalizePipelineAggregationBuilder> {\n+    public static final String NAME = \"normalize\";\n+    static final ParseField NORMALIZER_FIELD = new ParseField(\"normalizer\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkxOTMzMA=="}, "originalCommit": {"oid": "9c64e3613621bc3ca9e5187ce605e39ff4009167"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDc3NjgzNg==", "bodyText": "I've updated the naming to be method", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r424776836", "createdAt": "2020-05-13T22:55:21Z", "author": {"login": "talevy"}, "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineAggregationBuilder.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.BucketMetricsParser;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.function.DoubleUnaryOperator;\n+import java.util.function.Function;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.constructorArg;\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregator.Parser.FORMAT;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.Mean;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.Percent;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.RescaleZeroToOne;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.RescaleZeroToOneHundred;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.Softmax;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.ZScore;\n+\n+public class NormalizePipelineAggregationBuilder extends AbstractPipelineAggregationBuilder<NormalizePipelineAggregationBuilder> {\n+    public static final String NAME = \"normalize\";\n+    static final ParseField NORMALIZER_FIELD = new ParseField(\"normalizer\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkxOTMzMA=="}, "originalCommit": {"oid": "9c64e3613621bc3ca9e5187ce605e39ff4009167"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzOTkwMDYwOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineAggregationBuilder.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQxNzo1MDoxMlrOGUSJkg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QyMjo1NToxMFrOGVGUJQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkyMjA2Ng==", "bodyText": "Should we also check context.validateHasParent() to make sure this isn't at the top level?", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r423922066", "createdAt": "2020-05-12T17:50:12Z", "author": {"login": "polyfractal"}, "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineAggregationBuilder.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.BucketMetricsParser;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.function.DoubleUnaryOperator;\n+import java.util.function.Function;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.constructorArg;\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregator.Parser.FORMAT;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.Mean;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.Percent;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.RescaleZeroToOne;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.RescaleZeroToOneHundred;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.Softmax;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.ZScore;\n+\n+public class NormalizePipelineAggregationBuilder extends AbstractPipelineAggregationBuilder<NormalizePipelineAggregationBuilder> {\n+    public static final String NAME = \"normalize\";\n+    static final ParseField NORMALIZER_FIELD = new ParseField(\"normalizer\");\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public static final ConstructingObjectParser<NormalizePipelineAggregationBuilder, String> PARSER = new ConstructingObjectParser<>(\n+        NAME, false, (args, name) -> new NormalizePipelineAggregationBuilder(name, (String) args[0],\n+        (String) args[1], (List<String>) args[2]));\n+\n+    static {\n+        PARSER.declareString(optionalConstructorArg(), FORMAT);\n+        PARSER.declareString(constructorArg(), NORMALIZER_FIELD);\n+        PARSER.declareStringArray(constructorArg(), BUCKETS_PATH_FIELD);\n+    }\n+\n+    static final Map<String, Function<double[], DoubleUnaryOperator>> NAME_MAP = Map.of(\n+        RescaleZeroToOne.NAME, RescaleZeroToOne::new,\n+        RescaleZeroToOneHundred.NAME, RescaleZeroToOneHundred::new,\n+        Mean.NAME, Mean::new,\n+        ZScore.NAME, ZScore::new,\n+        Percent.NAME, Percent::new,\n+        Softmax.NAME, Softmax::new\n+    );\n+\n+    static String validateNormalizerName(String name) {\n+        if (NAME_MAP.containsKey(name)) {\n+            return name;\n+        }\n+\n+        throw new IllegalArgumentException(\"invalid normalizer [\" + name + \"]\");\n+    }\n+\n+    private final String format;\n+    private final String normalizer;\n+\n+\n+    public NormalizePipelineAggregationBuilder(String name, String format, String normalizer, List<String> bucketsPath) {\n+        super(name, NAME, bucketsPath.toArray(new String[0]));\n+        this.format = format;\n+        this.normalizer = validateNormalizerName(normalizer);\n+    }\n+\n+    /**\n+     * Read from a stream.\n+     */\n+    public NormalizePipelineAggregationBuilder(StreamInput in) throws IOException {\n+        super(in, NAME);\n+        format = in.readOptionalString();\n+        normalizer = in.readString();\n+    }\n+\n+    @Override\n+    protected final void doWriteTo(StreamOutput out) throws IOException {\n+        out.writeOptionalString(format);\n+        out.writeString(normalizer);\n+    }\n+\n+    /**\n+     * Gets the format to use on the output of this aggregation.\n+     */\n+    public String format() {\n+        return format;\n+    }\n+\n+    protected DocValueFormat formatter() {\n+        if (format != null) {\n+            return new DocValueFormat.Decimal(format);\n+        } else {\n+            return DocValueFormat.RAW;\n+        }\n+    }\n+\n+    @Override\n+    protected PipelineAggregator createInternal(Map<String, Object> metadata) {\n+        return new NormalizePipelineAggregator(name, bucketsPaths, formatter(), NAME_MAP.get(normalizer), metadata);\n+    }\n+\n+    @Override\n+    protected void validate(ValidationContext context) {\n+        if (bucketsPaths.length != 1) {\n+            context.addBucketPathValidationError(\"must contain a single entry for aggregation [\" + name + \"]\");\n+        }\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9c64e3613621bc3ca9e5187ce605e39ff4009167"}, "originalPosition": 118}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzk3ODIyOQ==", "bodyText": "ah, yes. I wasn't aware of this. thanks for bringing it up", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r423978229", "createdAt": "2020-05-12T19:25:00Z", "author": {"login": "talevy"}, "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineAggregationBuilder.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.BucketMetricsParser;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.function.DoubleUnaryOperator;\n+import java.util.function.Function;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.constructorArg;\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregator.Parser.FORMAT;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.Mean;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.Percent;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.RescaleZeroToOne;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.RescaleZeroToOneHundred;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.Softmax;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.ZScore;\n+\n+public class NormalizePipelineAggregationBuilder extends AbstractPipelineAggregationBuilder<NormalizePipelineAggregationBuilder> {\n+    public static final String NAME = \"normalize\";\n+    static final ParseField NORMALIZER_FIELD = new ParseField(\"normalizer\");\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public static final ConstructingObjectParser<NormalizePipelineAggregationBuilder, String> PARSER = new ConstructingObjectParser<>(\n+        NAME, false, (args, name) -> new NormalizePipelineAggregationBuilder(name, (String) args[0],\n+        (String) args[1], (List<String>) args[2]));\n+\n+    static {\n+        PARSER.declareString(optionalConstructorArg(), FORMAT);\n+        PARSER.declareString(constructorArg(), NORMALIZER_FIELD);\n+        PARSER.declareStringArray(constructorArg(), BUCKETS_PATH_FIELD);\n+    }\n+\n+    static final Map<String, Function<double[], DoubleUnaryOperator>> NAME_MAP = Map.of(\n+        RescaleZeroToOne.NAME, RescaleZeroToOne::new,\n+        RescaleZeroToOneHundred.NAME, RescaleZeroToOneHundred::new,\n+        Mean.NAME, Mean::new,\n+        ZScore.NAME, ZScore::new,\n+        Percent.NAME, Percent::new,\n+        Softmax.NAME, Softmax::new\n+    );\n+\n+    static String validateNormalizerName(String name) {\n+        if (NAME_MAP.containsKey(name)) {\n+            return name;\n+        }\n+\n+        throw new IllegalArgumentException(\"invalid normalizer [\" + name + \"]\");\n+    }\n+\n+    private final String format;\n+    private final String normalizer;\n+\n+\n+    public NormalizePipelineAggregationBuilder(String name, String format, String normalizer, List<String> bucketsPath) {\n+        super(name, NAME, bucketsPath.toArray(new String[0]));\n+        this.format = format;\n+        this.normalizer = validateNormalizerName(normalizer);\n+    }\n+\n+    /**\n+     * Read from a stream.\n+     */\n+    public NormalizePipelineAggregationBuilder(StreamInput in) throws IOException {\n+        super(in, NAME);\n+        format = in.readOptionalString();\n+        normalizer = in.readString();\n+    }\n+\n+    @Override\n+    protected final void doWriteTo(StreamOutput out) throws IOException {\n+        out.writeOptionalString(format);\n+        out.writeString(normalizer);\n+    }\n+\n+    /**\n+     * Gets the format to use on the output of this aggregation.\n+     */\n+    public String format() {\n+        return format;\n+    }\n+\n+    protected DocValueFormat formatter() {\n+        if (format != null) {\n+            return new DocValueFormat.Decimal(format);\n+        } else {\n+            return DocValueFormat.RAW;\n+        }\n+    }\n+\n+    @Override\n+    protected PipelineAggregator createInternal(Map<String, Object> metadata) {\n+        return new NormalizePipelineAggregator(name, bucketsPaths, formatter(), NAME_MAP.get(normalizer), metadata);\n+    }\n+\n+    @Override\n+    protected void validate(ValidationContext context) {\n+        if (bucketsPaths.length != 1) {\n+            context.addBucketPathValidationError(\"must contain a single entry for aggregation [\" + name + \"]\");\n+        }\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkyMjA2Ng=="}, "originalCommit": {"oid": "9c64e3613621bc3ca9e5187ce605e39ff4009167"}, "originalPosition": 118}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDc3Njc0MQ==", "bodyText": "added a check and a test for this!", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r424776741", "createdAt": "2020-05-13T22:55:10Z", "author": {"login": "talevy"}, "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineAggregationBuilder.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.BucketMetricsParser;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.function.DoubleUnaryOperator;\n+import java.util.function.Function;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.constructorArg;\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregator.Parser.FORMAT;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.Mean;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.Percent;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.RescaleZeroToOne;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.RescaleZeroToOneHundred;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.Softmax;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.ZScore;\n+\n+public class NormalizePipelineAggregationBuilder extends AbstractPipelineAggregationBuilder<NormalizePipelineAggregationBuilder> {\n+    public static final String NAME = \"normalize\";\n+    static final ParseField NORMALIZER_FIELD = new ParseField(\"normalizer\");\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public static final ConstructingObjectParser<NormalizePipelineAggregationBuilder, String> PARSER = new ConstructingObjectParser<>(\n+        NAME, false, (args, name) -> new NormalizePipelineAggregationBuilder(name, (String) args[0],\n+        (String) args[1], (List<String>) args[2]));\n+\n+    static {\n+        PARSER.declareString(optionalConstructorArg(), FORMAT);\n+        PARSER.declareString(constructorArg(), NORMALIZER_FIELD);\n+        PARSER.declareStringArray(constructorArg(), BUCKETS_PATH_FIELD);\n+    }\n+\n+    static final Map<String, Function<double[], DoubleUnaryOperator>> NAME_MAP = Map.of(\n+        RescaleZeroToOne.NAME, RescaleZeroToOne::new,\n+        RescaleZeroToOneHundred.NAME, RescaleZeroToOneHundred::new,\n+        Mean.NAME, Mean::new,\n+        ZScore.NAME, ZScore::new,\n+        Percent.NAME, Percent::new,\n+        Softmax.NAME, Softmax::new\n+    );\n+\n+    static String validateNormalizerName(String name) {\n+        if (NAME_MAP.containsKey(name)) {\n+            return name;\n+        }\n+\n+        throw new IllegalArgumentException(\"invalid normalizer [\" + name + \"]\");\n+    }\n+\n+    private final String format;\n+    private final String normalizer;\n+\n+\n+    public NormalizePipelineAggregationBuilder(String name, String format, String normalizer, List<String> bucketsPath) {\n+        super(name, NAME, bucketsPath.toArray(new String[0]));\n+        this.format = format;\n+        this.normalizer = validateNormalizerName(normalizer);\n+    }\n+\n+    /**\n+     * Read from a stream.\n+     */\n+    public NormalizePipelineAggregationBuilder(StreamInput in) throws IOException {\n+        super(in, NAME);\n+        format = in.readOptionalString();\n+        normalizer = in.readString();\n+    }\n+\n+    @Override\n+    protected final void doWriteTo(StreamOutput out) throws IOException {\n+        out.writeOptionalString(format);\n+        out.writeString(normalizer);\n+    }\n+\n+    /**\n+     * Gets the format to use on the output of this aggregation.\n+     */\n+    public String format() {\n+        return format;\n+    }\n+\n+    protected DocValueFormat formatter() {\n+        if (format != null) {\n+            return new DocValueFormat.Decimal(format);\n+        } else {\n+            return DocValueFormat.RAW;\n+        }\n+    }\n+\n+    @Override\n+    protected PipelineAggregator createInternal(Map<String, Object> metadata) {\n+        return new NormalizePipelineAggregator(name, bucketsPaths, formatter(), NAME_MAP.get(normalizer), metadata);\n+    }\n+\n+    @Override\n+    protected void validate(ValidationContext context) {\n+        if (bucketsPaths.length != 1) {\n+            context.addBucketPathValidationError(\"must contain a single entry for aggregation [\" + name + \"]\");\n+        }\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkyMjA2Ng=="}, "originalCommit": {"oid": "9c64e3613621bc3ca9e5187ce605e39ff4009167"}, "originalPosition": 118}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzOTkyNDQxOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineAggregator.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQxNzo1NTo0OFrOGUSYXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QyMjo1NDo1OVrOGVGT5A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkyNTg1Mg==", "bodyText": "Do we know if this works with a terms agg as the parent?  It feels like it should (e.g. it doesn't require any specific ordering of the buckets, unlike something like a moving avg which needs an ordering).\nIf we think it should work with terms we should tweak this to not use a HistogramFactory directly.  BucketScriptPipelineAggregator has an example of how to generically build buckets from any InternalMultiBucketAggregation (the internal agg can create buckets too, not just the factory).", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r423925852", "createdAt": "2020-05-12T17:55:48Z", "author": {"login": "polyfractal"}, "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineAggregator.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.aggregations.InternalAggregation;\n+import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContext;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;\n+import org.elasticsearch.search.aggregations.bucket.MultiBucketsAggregation.Bucket;\n+import org.elasticsearch.search.aggregations.bucket.histogram.HistogramFactory;\n+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;\n+import org.elasticsearch.search.aggregations.pipeline.InternalSimpleValue;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.DoubleUnaryOperator;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+\n+import static org.elasticsearch.search.aggregations.pipeline.BucketHelpers.resolveBucketValue;\n+\n+public class NormalizePipelineAggregator extends PipelineAggregator {\n+    private final DocValueFormat formatter;\n+    private final Function<double[], DoubleUnaryOperator> normalizerSupplier;\n+\n+    NormalizePipelineAggregator(String name, String[] bucketsPaths, DocValueFormat formatter,\n+                                Function<double[], DoubleUnaryOperator> normalizerSupplier,\n+                                Map<String, Object> metadata) {\n+        super(name, bucketsPaths, metadata);\n+        this.formatter = formatter;\n+        this.normalizerSupplier = normalizerSupplier;\n+    }\n+\n+    @Override\n+    public InternalAggregation reduce(InternalAggregation aggregation, ReduceContext reduceContext) {\n+        InternalMultiBucketAggregation<? extends InternalMultiBucketAggregation, ? extends InternalMultiBucketAggregation.InternalBucket>\n+                histo = (InternalMultiBucketAggregation<? extends InternalMultiBucketAggregation, ? extends\n+                InternalMultiBucketAggregation.InternalBucket>) aggregation;\n+        List<? extends InternalMultiBucketAggregation.InternalBucket> buckets = histo.getBuckets();\n+        HistogramFactory factory = (HistogramFactory) histo;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9c64e3613621bc3ca9e5187ce605e39ff4009167"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzk3ODg2MQ==", "bodyText": "thanks! I was slightly loose in my interpretation of the HistogramFactory's comment\n\n/** Implemented by histogram aggregations and used by pipeline aggregations to insert buckets. */\n\nWill look at how BucketScript does things and add a test for terms agg!", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r423978861", "createdAt": "2020-05-12T19:26:11Z", "author": {"login": "talevy"}, "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineAggregator.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.aggregations.InternalAggregation;\n+import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContext;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;\n+import org.elasticsearch.search.aggregations.bucket.MultiBucketsAggregation.Bucket;\n+import org.elasticsearch.search.aggregations.bucket.histogram.HistogramFactory;\n+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;\n+import org.elasticsearch.search.aggregations.pipeline.InternalSimpleValue;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.DoubleUnaryOperator;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+\n+import static org.elasticsearch.search.aggregations.pipeline.BucketHelpers.resolveBucketValue;\n+\n+public class NormalizePipelineAggregator extends PipelineAggregator {\n+    private final DocValueFormat formatter;\n+    private final Function<double[], DoubleUnaryOperator> normalizerSupplier;\n+\n+    NormalizePipelineAggregator(String name, String[] bucketsPaths, DocValueFormat formatter,\n+                                Function<double[], DoubleUnaryOperator> normalizerSupplier,\n+                                Map<String, Object> metadata) {\n+        super(name, bucketsPaths, metadata);\n+        this.formatter = formatter;\n+        this.normalizerSupplier = normalizerSupplier;\n+    }\n+\n+    @Override\n+    public InternalAggregation reduce(InternalAggregation aggregation, ReduceContext reduceContext) {\n+        InternalMultiBucketAggregation<? extends InternalMultiBucketAggregation, ? extends InternalMultiBucketAggregation.InternalBucket>\n+                histo = (InternalMultiBucketAggregation<? extends InternalMultiBucketAggregation, ? extends\n+                InternalMultiBucketAggregation.InternalBucket>) aggregation;\n+        List<? extends InternalMultiBucketAggregation.InternalBucket> buckets = histo.getBuckets();\n+        HistogramFactory factory = (HistogramFactory) histo;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkyNTg1Mg=="}, "originalCommit": {"oid": "9c64e3613621bc3ca9e5187ce605e39ff4009167"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDc3NTUwOQ==", "bodyText": "Yikes! I'm sorry I didn't notice this one!", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r424775509", "createdAt": "2020-05-13T22:51:48Z", "author": {"login": "nik9000"}, "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineAggregator.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.aggregations.InternalAggregation;\n+import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContext;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;\n+import org.elasticsearch.search.aggregations.bucket.MultiBucketsAggregation.Bucket;\n+import org.elasticsearch.search.aggregations.bucket.histogram.HistogramFactory;\n+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;\n+import org.elasticsearch.search.aggregations.pipeline.InternalSimpleValue;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.DoubleUnaryOperator;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+\n+import static org.elasticsearch.search.aggregations.pipeline.BucketHelpers.resolveBucketValue;\n+\n+public class NormalizePipelineAggregator extends PipelineAggregator {\n+    private final DocValueFormat formatter;\n+    private final Function<double[], DoubleUnaryOperator> normalizerSupplier;\n+\n+    NormalizePipelineAggregator(String name, String[] bucketsPaths, DocValueFormat formatter,\n+                                Function<double[], DoubleUnaryOperator> normalizerSupplier,\n+                                Map<String, Object> metadata) {\n+        super(name, bucketsPaths, metadata);\n+        this.formatter = formatter;\n+        this.normalizerSupplier = normalizerSupplier;\n+    }\n+\n+    @Override\n+    public InternalAggregation reduce(InternalAggregation aggregation, ReduceContext reduceContext) {\n+        InternalMultiBucketAggregation<? extends InternalMultiBucketAggregation, ? extends InternalMultiBucketAggregation.InternalBucket>\n+                histo = (InternalMultiBucketAggregation<? extends InternalMultiBucketAggregation, ? extends\n+                InternalMultiBucketAggregation.InternalBucket>) aggregation;\n+        List<? extends InternalMultiBucketAggregation.InternalBucket> buckets = histo.getBuckets();\n+        HistogramFactory factory = (HistogramFactory) histo;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkyNTg1Mg=="}, "originalCommit": {"oid": "9c64e3613621bc3ca9e5187ce605e39ff4009167"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDc3NjY3Ng==", "bodyText": "thanks, I've updated to include a test for terms and use a more generic way to make new buckets", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r424776676", "createdAt": "2020-05-13T22:54:59Z", "author": {"login": "talevy"}, "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineAggregator.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.aggregations.InternalAggregation;\n+import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContext;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;\n+import org.elasticsearch.search.aggregations.bucket.MultiBucketsAggregation.Bucket;\n+import org.elasticsearch.search.aggregations.bucket.histogram.HistogramFactory;\n+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;\n+import org.elasticsearch.search.aggregations.pipeline.InternalSimpleValue;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.DoubleUnaryOperator;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+\n+import static org.elasticsearch.search.aggregations.pipeline.BucketHelpers.resolveBucketValue;\n+\n+public class NormalizePipelineAggregator extends PipelineAggregator {\n+    private final DocValueFormat formatter;\n+    private final Function<double[], DoubleUnaryOperator> normalizerSupplier;\n+\n+    NormalizePipelineAggregator(String name, String[] bucketsPaths, DocValueFormat formatter,\n+                                Function<double[], DoubleUnaryOperator> normalizerSupplier,\n+                                Map<String, Object> metadata) {\n+        super(name, bucketsPaths, metadata);\n+        this.formatter = formatter;\n+        this.normalizerSupplier = normalizerSupplier;\n+    }\n+\n+    @Override\n+    public InternalAggregation reduce(InternalAggregation aggregation, ReduceContext reduceContext) {\n+        InternalMultiBucketAggregation<? extends InternalMultiBucketAggregation, ? extends InternalMultiBucketAggregation.InternalBucket>\n+                histo = (InternalMultiBucketAggregation<? extends InternalMultiBucketAggregation, ? extends\n+                InternalMultiBucketAggregation.InternalBucket>) aggregation;\n+        List<? extends InternalMultiBucketAggregation.InternalBucket> buckets = histo.getBuckets();\n+        HistogramFactory factory = (HistogramFactory) histo;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkyNTg1Mg=="}, "originalCommit": {"oid": "9c64e3613621bc3ca9e5187ce605e39ff4009167"}, "originalPosition": 48}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 646, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}