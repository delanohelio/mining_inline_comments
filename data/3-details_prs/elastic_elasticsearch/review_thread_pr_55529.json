{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDA2NjgxOTQx", "number": 55529, "reviewThreads": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQxNDozNjo1N1rOD0R7Sw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQxNDozNjo1N1rOD0R7Sw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2MTQ2MjUxOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/action/TransportMlInfoAction.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQxNDozNjo1N1rOGJJQrg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQxNTozNDoyNVrOGJMMEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjI0MjA5NA==", "bodyText": "It might be nice to indicate that there is room available for larger jobs if they increased their MAX_MODEL_MEMORY_LIMIT setting.\nBut, in the scenarios where the user could take action, it seems to me that they SHOULD already know the native memory available.", "url": "https://github.com/elastic/elasticsearch/pull/55529#discussion_r412242094", "createdAt": "2020-04-21T14:36:57Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/action/TransportMlInfoAction.java", "diffHunk": "@@ -106,11 +111,50 @@ private ByteSizeValue defaultModelMemoryLimit() {\n         return anomalyDetectorsDefaults;\n     }\n \n+    static ByteSizeValue calculateCurrentEffectiveMaxModelMemoryLimit(int maxMachineMemoryPercent, DiscoveryNodes nodes) {\n+\n+        long maxMlMemory = -1;\n+\n+        for (DiscoveryNode node : nodes) {\n+\n+            Map<String, String> nodeAttributes = node.getAttributes();\n+            String machineMemoryStr = nodeAttributes.get(MachineLearning.MACHINE_MEMORY_NODE_ATTR);\n+            if (machineMemoryStr == null) {\n+                continue;\n+            }\n+            long machineMemory;\n+            try {\n+                machineMemory = Long.parseLong(machineMemoryStr);\n+            } catch (NumberFormatException e) {\n+                continue;\n+            }\n+            maxMlMemory = Math.max(maxMlMemory, machineMemory * maxMachineMemoryPercent / 100);\n+        }\n+\n+        if (maxMlMemory <= 0) {\n+            // This implies there are currently no ML nodes in the cluster, so we\n+            // have no idea what the effective limit would be if one were added\n+            return null;\n+        }\n+\n+        maxMlMemory -= Math.max(Job.PROCESS_MEMORY_OVERHEAD.getBytes(), DataFrameAnalyticsConfig.PROCESS_MEMORY_OVERHEAD.getBytes());\n+        maxMlMemory -= MachineLearning.NATIVE_EXECUTABLE_CODE_OVERHEAD.getBytes();\n+        return new ByteSizeValue(Math.max(0L, maxMlMemory) / 1024 / 1024, ByteSizeUnit.MB);\n+    }\n+\n     private Map<String, Object> limits() {\n         Map<String, Object> limits = new HashMap<>();\n+        ByteSizeValue currentEffectiveMaxModelMemoryLimit = calculateCurrentEffectiveMaxModelMemoryLimit(\n+            clusterService.getClusterSettings().get(MachineLearning.MAX_MACHINE_MEMORY_PERCENT), clusterService.state().getNodes());\n         ByteSizeValue maxModelMemoryLimit = clusterService.getClusterSettings().get(MachineLearningField.MAX_MODEL_MEMORY_LIMIT);\n         if (maxModelMemoryLimit != null && maxModelMemoryLimit.getBytes() > 0) {\n-            limits.put(\"max_model_memory_limit\", maxModelMemoryLimit);\n+            limits.put(\"max_model_memory_limit\", maxModelMemoryLimit.getStringRep());\n+            if (currentEffectiveMaxModelMemoryLimit == null || currentEffectiveMaxModelMemoryLimit.compareTo(maxModelMemoryLimit) > 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d4db68f89d63b7164e4f4eca845fe1de88f2274"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjI5MDA2Nw==", "bodyText": "The main scenario where MAX_MODEL_MEMORY_LIMIT is in Cloud, where it's controlled by the Cloud environment.\nThe other scenario where we envisage it being used is when an administrator wants to lower powered users from using all the resources with a single job.\nIn both cases, the user seeing the effect of the restriction wouldn't have the power to increase the limit.  It's extremely unlikely there would be a scenario where the user being affected by the limit had the power to change it.  Superusers who are using ML and have complete control of their hardware probably don't have the setting set at all.\nIn the event that both the hard maximum and effective maximum constrain the size of a job the UI should report the hard maximum.\nFor Elastic Cloud there is the desire for the UI to suggest upgrading to more powerful nodes if limits are hit, as that's just a case of a few clicks in the Cloud console (and paying more).  But I think this endpoint still provides enough information to facilitate that because within the Cloud environment we're already setting a hard maximum limit.", "url": "https://github.com/elastic/elasticsearch/pull/55529#discussion_r412290067", "createdAt": "2020-04-21T15:34:25Z", "author": {"login": "droberts195"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/action/TransportMlInfoAction.java", "diffHunk": "@@ -106,11 +111,50 @@ private ByteSizeValue defaultModelMemoryLimit() {\n         return anomalyDetectorsDefaults;\n     }\n \n+    static ByteSizeValue calculateCurrentEffectiveMaxModelMemoryLimit(int maxMachineMemoryPercent, DiscoveryNodes nodes) {\n+\n+        long maxMlMemory = -1;\n+\n+        for (DiscoveryNode node : nodes) {\n+\n+            Map<String, String> nodeAttributes = node.getAttributes();\n+            String machineMemoryStr = nodeAttributes.get(MachineLearning.MACHINE_MEMORY_NODE_ATTR);\n+            if (machineMemoryStr == null) {\n+                continue;\n+            }\n+            long machineMemory;\n+            try {\n+                machineMemory = Long.parseLong(machineMemoryStr);\n+            } catch (NumberFormatException e) {\n+                continue;\n+            }\n+            maxMlMemory = Math.max(maxMlMemory, machineMemory * maxMachineMemoryPercent / 100);\n+        }\n+\n+        if (maxMlMemory <= 0) {\n+            // This implies there are currently no ML nodes in the cluster, so we\n+            // have no idea what the effective limit would be if one were added\n+            return null;\n+        }\n+\n+        maxMlMemory -= Math.max(Job.PROCESS_MEMORY_OVERHEAD.getBytes(), DataFrameAnalyticsConfig.PROCESS_MEMORY_OVERHEAD.getBytes());\n+        maxMlMemory -= MachineLearning.NATIVE_EXECUTABLE_CODE_OVERHEAD.getBytes();\n+        return new ByteSizeValue(Math.max(0L, maxMlMemory) / 1024 / 1024, ByteSizeUnit.MB);\n+    }\n+\n     private Map<String, Object> limits() {\n         Map<String, Object> limits = new HashMap<>();\n+        ByteSizeValue currentEffectiveMaxModelMemoryLimit = calculateCurrentEffectiveMaxModelMemoryLimit(\n+            clusterService.getClusterSettings().get(MachineLearning.MAX_MACHINE_MEMORY_PERCENT), clusterService.state().getNodes());\n         ByteSizeValue maxModelMemoryLimit = clusterService.getClusterSettings().get(MachineLearningField.MAX_MODEL_MEMORY_LIMIT);\n         if (maxModelMemoryLimit != null && maxModelMemoryLimit.getBytes() > 0) {\n-            limits.put(\"max_model_memory_limit\", maxModelMemoryLimit);\n+            limits.put(\"max_model_memory_limit\", maxModelMemoryLimit.getStringRep());\n+            if (currentEffectiveMaxModelMemoryLimit == null || currentEffectiveMaxModelMemoryLimit.compareTo(maxModelMemoryLimit) > 0) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjI0MjA5NA=="}, "originalCommit": {"oid": "1d4db68f89d63b7164e4f4eca845fe1de88f2274"}, "originalPosition": 65}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2771, "cost": 1, "resetAt": "2021-11-12T13:16:51Z"}}}