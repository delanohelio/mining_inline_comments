{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE4OTk3ODY0", "number": 56860, "title": "Add max_token_length setting to the CharGroupTokenizer", "bodyText": "Adds max_token_length option to the CharGroupTokenizer.\nUpdates documentation as well to reflect the changes.\nSolves #56676", "createdAt": "2020-05-16T17:24:44Z", "url": "https://github.com/elastic/elasticsearch/pull/56860", "merged": true, "mergeCommit": {"oid": "da31b4b83d656a7e40cad803d3ca72977a4f56ad"}, "closed": true, "closedAt": "2020-05-20T12:15:58Z", "author": {"login": "ADBalici"}, "timelineItems": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABch5j2oAH2gAyNDE4OTk3ODY0OjNjZThiMTk0NmU1NmZlZjM5YTg3ZTE5YjE5NzYzMmZlZTI5MzczYzI=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcjGQ6UAH2gAyNDE4OTk3ODY0OjQzMWZkMWFhN2M3YTI3YmVlZWE2ZGJhM2I1Mzg1OTM5NTMwMjIwYTU=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "3ce8b1946e56fef39a87e19b197632fee29373c2", "author": {"user": {"login": "ADBalici", "name": "Andrei Balici"}}, "url": "https://github.com/elastic/elasticsearch/commit/3ce8b1946e56fef39a87e19b197632fee29373c2", "committedDate": "2020-05-16T16:49:20Z", "message": "add failing test case"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9fa42dd312fa985e2418dc14301705a26d63d122", "author": {"user": {"login": "ADBalici", "name": "Andrei Balici"}}, "url": "https://github.com/elastic/elasticsearch/commit/9fa42dd312fa985e2418dc14301705a26d63d122", "committedDate": "2020-05-16T16:55:25Z", "message": "fix code; test passes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8cf5258fd99ecec4867dbfeb7e2090a1e905cb0b", "author": {"user": {"login": "ADBalici", "name": "Andrei Balici"}}, "url": "https://github.com/elastic/elasticsearch/commit/8cf5258fd99ecec4867dbfeb7e2090a1e905cb0b", "committedDate": "2020-05-16T17:12:43Z", "message": "add more tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "89cbcc7ab21918ca0e10461305b57e34f56aa554", "author": {"user": {"login": "ADBalici", "name": "Andrei Balici"}}, "url": "https://github.com/elastic/elasticsearch/commit/89cbcc7ab21918ca0e10461305b57e34f56aa554", "committedDate": "2020-05-16T17:16:34Z", "message": "updated documentation"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE0NjMzMjgy", "url": "https://github.com/elastic/elasticsearch/pull/56860#pullrequestreview-414633282", "createdAt": "2020-05-19T16:43:19Z", "commit": {"oid": "89cbcc7ab21918ca0e10461305b57e34f56aa554"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNjo0MzoxOVrOGXpQWw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNjo0MzoxOVrOGXpQWw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ0NjM2Mw==", "bodyText": "Super small nitpick: can you change to use the default from CharTokenizer.DEFAULT_MAX_WORD_LEN here? Its the same value (255) as StandardAnalyzer.DEFAULT_MAX_TOKEN_LENGTH currently, just so we tie the default to the one thats used in the class we actually use.", "url": "https://github.com/elastic/elasticsearch/pull/56860#discussion_r427446363", "createdAt": "2020-05-19T16:43:19Z", "author": {"login": "cbuescher"}, "path": "modules/analysis-common/src/main/java/org/elasticsearch/analysis/common/CharGroupTokenizerFactory.java", "diffHunk": "@@ -41,6 +46,8 @@\n     public CharGroupTokenizerFactory(IndexSettings indexSettings, Environment environment, String name, Settings settings) {\n         super(indexSettings, settings, name);\n \n+        maxTokenLength = settings.getAsInt(MAX_TOKEN_LENGTH, StandardAnalyzer.DEFAULT_MAX_TOKEN_LENGTH);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "89cbcc7ab21918ca0e10461305b57e34f56aa554"}, "originalPosition": 25}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE0NjUxNjIy", "url": "https://github.com/elastic/elasticsearch/pull/56860#pullrequestreview-414651622", "createdAt": "2020-05-19T17:06:23Z", "commit": {"oid": "89cbcc7ab21918ca0e10461305b57e34f56aa554"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNzowNjoyM1rOGXqKKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNzowNjoyM1rOGXqKKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ2MTE2Mw==", "bodyText": "nit: we have a 140 chars line length limit that gets checked via \"checkstyle\" in CI and it complains about this line being too long. Simply splitting this is fine.", "url": "https://github.com/elastic/elasticsearch/pull/56860#discussion_r427461163", "createdAt": "2020-05-19T17:06:23Z", "author": {"login": "cbuescher"}, "path": "modules/analysis-common/src/test/java/org/elasticsearch/analysis/common/CharGroupTokenizerFactoryTests.java", "diffHunk": "@@ -61,6 +64,52 @@ public void testParseTokenChars() {\n         }\n     }\n \n+    public void testMaxTokenLength() throws IOException {\n+        final Index index = new Index(\"test\", \"_na_\");\n+        final Settings indexSettings = newAnalysisSettingsBuilder().build();\n+        IndexSettings indexProperties = IndexSettingsModule.newIndexSettings(index, indexSettings);\n+        final String name = \"cg\";\n+\n+        String[] conf = new String[] {\"-\"};\n+\n+        final Settings defaultLengthSettings = newAnalysisSettingsBuilder()\n+            .putList(\"tokenize_on_chars\", conf)\n+            .build();\n+        CharTokenizer tokenizer = (CharTokenizer) new CharGroupTokenizerFactory(indexProperties, null, name, defaultLengthSettings).create();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "89cbcc7ab21918ca0e10461305b57e34f56aa554"}, "originalPosition": 39}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3609c1dcd27678efb38e40c4cf22b78f5ef408f8", "author": {"user": {"login": "ADBalici", "name": "Andrei Balici"}}, "url": "https://github.com/elastic/elasticsearch/commit/3609c1dcd27678efb38e40c4cf22b78f5ef408f8", "committedDate": "2020-05-19T20:25:42Z", "message": "small refactoring"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "48366f3ff0df906aca2b2b208b71f6c2909219ed", "author": {"user": {"login": "cbuescher", "name": "Christoph B\u00fcscher"}}, "url": "https://github.com/elastic/elasticsearch/commit/48366f3ff0df906aca2b2b208b71f6c2909219ed", "committedDate": "2020-05-20T08:04:59Z", "message": "Removing unused import"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "431fd1aa7c7a27beeea6dba3b5385939530220a5", "author": {"user": {"login": "elasticmachine", "name": "Elastic Machine"}}, "url": "https://github.com/elastic/elasticsearch/commit/431fd1aa7c7a27beeea6dba3b5385939530220a5", "committedDate": "2020-05-20T10:11:20Z", "message": "Merge branch 'master' into enhancement/max-char-limit-char-group-tokenizer"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4812, "cost": 1, "resetAt": "2021-10-28T19:08:13Z"}}}