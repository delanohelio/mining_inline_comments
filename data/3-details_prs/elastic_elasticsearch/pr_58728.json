{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQxOTQwNDIx", "number": 58728, "title": "Allow read operations to be executed without waiting for full range to be written in cache", "bodyText": "This pull request changes CacheFile and CachedBlobContainerIndexInput so that the read operations made by these classes are now progressively executed and do not wait for full range to be written in cache. It relies on the change introduced in #58477 and it is the last change extracted from #58164.\nRelates #58164", "createdAt": "2020-06-30T10:39:54Z", "url": "https://github.com/elastic/elasticsearch/pull/58728", "merged": true, "mergeCommit": {"oid": "045bdcafc62e2ee4f274b80e9ab11fe3c6c95848"}, "closed": true, "closedAt": "2020-07-01T12:18:43Z", "author": {"login": "tlrx"}, "timelineItems": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcwTOlBAH2gAyNDQxOTQwNDIxOjM1OTg1ZTFhNzczODNmYzliNzMwYjVhMWE0NTkyMDBhYzY0YjJmY2I=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcwoycigFqTQ0MDc5NDIyOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "35985e1a77383fc9b730b5a1a459200ac64b2fcb", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/35985e1a77383fc9b730b5a1a459200ac64b2fcb", "committedDate": "2020-06-30T10:38:34Z", "message": "Use progress listener in CacheFile"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "28091b888998d690d9cf926aa0e80362f75cb590", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/28091b888998d690d9cf926aa0e80362f75cb590", "committedDate": "2020-06-30T12:03:02Z", "message": "Merge branch 'master' into use-progressable-listener-in-cache-file"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQwMDAzOTcz", "url": "https://github.com/elastic/elasticsearch/pull/58728#pullrequestreview-440003973", "createdAt": "2020-06-30T13:18:41Z", "commit": {"oid": "28091b888998d690d9cf926aa0e80362f75cb590"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMzoxODo0MlrOGq74hQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMzoxODo0MlrOGq74hQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzY3NDUwMQ==", "bodyText": "Could also be named prewarmExecutor()", "url": "https://github.com/elastic/elasticsearch/pull/58728#discussion_r447674501", "createdAt": "2020-06-30T13:18:42Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -317,6 +318,14 @@ public CacheFile getCacheFile(CacheKey cacheKey, long fileLength) throws Excepti\n         return cacheService.get(cacheKey, fileLength, cacheDir);\n     }\n \n+    public Executor executor() {\n+        return threadPool.executor(SearchableSnapshotsConstants.SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME);\n+    }\n+\n+    public Executor directExecutor() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "28091b888998d690d9cf926aa0e80362f75cb590"}, "originalPosition": 16}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQwMDA0MzM5", "url": "https://github.com/elastic/elasticsearch/pull/58728#pullrequestreview-440004339", "createdAt": "2020-06-30T13:19:06Z", "commit": {"oid": "28091b888998d690d9cf926aa0e80362f75cb590"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMzoxOTowN1rOGq75pQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMzoxOTowN1rOGq75pQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzY3NDc4OQ==", "bodyText": "This is already verified by the SparseFileTracker itself", "url": "https://github.com/elastic/elasticsearch/pull/58728#discussion_r447674789", "createdAt": "2020-06-30T13:19:07Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CacheFile.java", "diffHunk": "@@ -259,38 +262,68 @@ private void ensureOpen() {\n         }\n     }\n \n-    CompletableFuture<Integer> fetchRange(\n-        long start,\n-        long end,\n-        CheckedBiFunction<Long, Long, Integer, IOException> onRangeAvailable,\n-        CheckedBiConsumer<Long, Long, IOException> onRangeMissing\n+    @FunctionalInterface\n+    interface CacheReader {\n+        int read(FileChannel channel) throws IOException;\n+    }\n+\n+    @FunctionalInterface\n+    interface CacheWriter {\n+        void write(FileChannel channel, long from, long to, Consumer<Long> progressUpdater) throws IOException;\n+    }\n+\n+    CompletableFuture<Integer> fetchAsync(\n+        final Tuple<Long, Long> rangeToWrite,\n+        final Tuple<Long, Long> rangeToRead,\n+        final CacheReader reader,\n+        final CacheWriter writer,\n+        final Executor executor\n     ) {\n         final CompletableFuture<Integer> future = new CompletableFuture<>();\n         try {\n-            if (start < 0 || start > tracker.getLength() || start > end || end > tracker.getLength()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "28091b888998d690d9cf926aa0e80362f75cb590"}, "originalPosition": 60}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQwMDA1NDE3", "url": "https://github.com/elastic/elasticsearch/pull/58728#pullrequestreview-440005417", "createdAt": "2020-06-30T13:20:16Z", "commit": {"oid": "28091b888998d690d9cf926aa0e80362f75cb590"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMzoyMDoxN1rOGq79DA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMzoyMDoxN1rOGq79DA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzY3NTY2MA==", "bodyText": "This was previously embedded in the readCacheFile() method but feels wrong to me; the readCacheFile() should expect a ByteBuffer with an appropriate limit.", "url": "https://github.com/elastic/elasticsearch/pull/58728#discussion_r447675660", "createdAt": "2020-06-30T13:20:17Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -141,13 +143,22 @@ protected void readInternal(ByteBuffer b) throws IOException {\n             try {\n                 final CacheFile cacheFile = getCacheFileSafe();\n                 try (Releasable ignored = cacheFile.fileLock()) {\n-                    final Tuple<Long, Long> range = computeRange(pos);\n-                    bytesRead = cacheFile.fetchRange(\n-                        range.v1(),\n-                        range.v2(),\n-                        (start, end) -> readCacheFile(cacheFile.getChannel(), end, pos, b, len),\n-                        (start, end) -> writeCacheFile(cacheFile.getChannel(), start, end)\n-                    ).get();\n+                    final Tuple<Long, Long> rangeToWrite = computeRange(pos);\n+                    final Tuple<Long, Long> rangeToRead = Tuple.tuple(pos, Math.min(pos + len, rangeToWrite.v2()));\n+\n+                    bytesRead = cacheFile.fetchAsync(rangeToWrite, rangeToRead, (channel) -> {\n+                        final int read;\n+                        if ((rangeToRead.v2() - rangeToRead.v1()) < b.remaining()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "28091b888998d690d9cf926aa0e80362f75cb590"}, "originalPosition": 32}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQwMDA3ODgw", "url": "https://github.com/elastic/elasticsearch/pull/58728#pullrequestreview-440007880", "createdAt": "2020-06-30T13:22:57Z", "commit": {"oid": "28091b888998d690d9cf926aa0e80362f75cb590"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMzoyMjo1N1rOGq8Exw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMzoyMjo1N1rOGq8Exw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzY3NzYzOQ==", "bodyText": "Maybe it deserves some comment: we don't care about reading the cached data for prewarming, it just need to cache the range. This is why it got passed an empty range to read. This way if the range is already (or about to be) written by a concurrent search it returns immediately and process the next small range. If the range is not available then the cache data will be written by this prewarming task.", "url": "https://github.com/elastic/elasticsearch/pull/58728#discussion_r447677639", "createdAt": "2020-06-30T13:22:57Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -224,31 +235,33 @@ public void prefetchPart(final int part) throws IOException {\n                     while (remainingBytes > 0L) {\n                         assert totalBytesRead + remainingBytes == rangeLength;\n                         final int bytesRead = readSafe(input, copyBuffer, rangeStart, rangeEnd, remainingBytes, cacheFileReference);\n+\n                         final long readStart = rangeStart + totalBytesRead;\n-                        cacheFile.fetchRange(readStart, readStart + bytesRead, (start, end) -> {\n-                            logger.trace(\n-                                \"prefetchPart: range [{}-{}] of file [{}] is available in cache\",\n-                                start,\n-                                end,\n-                                fileInfo.physicalName()\n-                            );\n-                            return Math.toIntExact(end - start);\n-                        }, (start, end) -> {\n-                            final ByteBuffer byteBuffer = ByteBuffer.wrap(\n-                                copyBuffer,\n-                                Math.toIntExact(start - readStart),\n-                                Math.toIntExact(end - start)\n-                            );\n-                            final int writtenBytes = positionalWrite(fc, start, byteBuffer);\n-                            logger.trace(\n-                                \"prefetchPart: writing range [{}-{}] of file [{}], [{}] bytes written\",\n-                                start,\n-                                end,\n-                                fileInfo.physicalName(),\n-                                writtenBytes\n-                            );\n-                            totalBytesWritten.addAndGet(writtenBytes);\n-                        });\n+                        final long readEnd = readStart + bytesRead;\n+\n+                        cacheFile.fetchAsync(\n+                            Tuple.tuple(readStart, readEnd),\n+                            Tuple.tuple(readStart, readStart),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "28091b888998d690d9cf926aa0e80362f75cb590"}, "originalPosition": 80}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQwMDY1OTYw", "url": "https://github.com/elastic/elasticsearch/pull/58728#pullrequestreview-440065960", "createdAt": "2020-06-30T14:21:07Z", "commit": {"oid": "28091b888998d690d9cf926aa0e80362f75cb590"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNDoyMTowN1rOGq-wug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQwODozODowNFrOGrceKQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzcyMTY1OA==", "bodyText": "++ I'd prefer that, lest we use this executor for something else in the future and then decide to change it. Can we name the other executor() method something more specific too?", "url": "https://github.com/elastic/elasticsearch/pull/58728#discussion_r447721658", "createdAt": "2020-06-30T14:21:07Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -317,6 +318,14 @@ public CacheFile getCacheFile(CacheKey cacheKey, long fileLength) throws Excepti\n         return cacheService.get(cacheKey, fileLength, cacheDir);\n     }\n \n+    public Executor executor() {\n+        return threadPool.executor(SearchableSnapshotsConstants.SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME);\n+    }\n+\n+    public Executor directExecutor() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzY3NDUwMQ=="}, "originalCommit": {"oid": "28091b888998d690d9cf926aa0e80362f75cb590"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3MzQyNA==", "bodyText": "Why do we remove the gap from the list here?", "url": "https://github.com/elastic/elasticsearch/pull/58728#discussion_r448173424", "createdAt": "2020-07-01T07:35:46Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CacheFile.java", "diffHunk": "@@ -259,38 +262,68 @@ private void ensureOpen() {\n         }\n     }\n \n-    CompletableFuture<Integer> fetchRange(\n-        long start,\n-        long end,\n-        CheckedBiFunction<Long, Long, Integer, IOException> onRangeAvailable,\n-        CheckedBiConsumer<Long, Long, IOException> onRangeMissing\n+    @FunctionalInterface\n+    interface CacheReader {\n+        int read(FileChannel channel) throws IOException;\n+    }\n+\n+    @FunctionalInterface\n+    interface CacheWriter {\n+        void write(FileChannel channel, long from, long to, Consumer<Long> progressUpdater) throws IOException;\n+    }\n+\n+    CompletableFuture<Integer> fetchAsync(\n+        final Tuple<Long, Long> rangeToWrite,\n+        final Tuple<Long, Long> rangeToRead,\n+        final CacheReader reader,\n+        final CacheWriter writer,\n+        final Executor executor\n     ) {\n         final CompletableFuture<Integer> future = new CompletableFuture<>();\n         try {\n-            if (start < 0 || start > tracker.getLength() || start > end || end > tracker.getLength()) {\n-                throw new IllegalArgumentException(\n-                    \"Invalid range [start=\" + start + \", end=\" + end + \"] for length [\" + tracker.getLength() + ']'\n-                );\n-            }\n             ensureOpen();\n             final List<SparseFileTracker.Gap> gaps = tracker.waitForRange(\n-                Tuple.tuple(start, end),\n-                Tuple.tuple(start, end), // TODO use progressive sub range to trigger read operations sooner\n-                ActionListener.wrap(\n-                    rangeReady -> future.complete(onRangeAvailable.apply(start, end)),\n-                    rangeFailure -> future.completeExceptionally(rangeFailure)\n-                )\n+                rangeToWrite,\n+                rangeToRead,\n+                ActionListener.wrap(success -> future.complete(reader.read(channel)), future::completeExceptionally)\n             );\n \n-            for (SparseFileTracker.Gap gap : gaps) {\n-                try {\n-                    ensureOpen();\n-                    onRangeMissing.accept(gap.start(), gap.end());\n-                    gap.onProgress(gap.end()); // TODO update progress in onRangeMissing\n-                    gap.onCompletion();\n-                } catch (Exception e) {\n-                    gap.onFailure(e);\n-                }\n+            if (gaps.isEmpty() == false) {\n+                final Iterator<SparseFileTracker.Gap> iterator = new ArrayList<>(gaps).iterator();\n+                executor.execute(new AbstractRunnable() {\n+\n+                    @Override\n+                    protected void doRun() {\n+                        while (iterator.hasNext()) {\n+                            final SparseFileTracker.Gap gap = iterator.next();\n+                            try {\n+                                ensureOpen();\n+                                if (readLock.tryLock() == false) {\n+                                    throw new AlreadyClosedException(\"Cache file channel is being evicted, writing attempt cancelled\");\n+                                }\n+                                try {\n+                                    ensureOpen();\n+                                    if (channel == null) {\n+                                        throw new AlreadyClosedException(\"Cache file channel has been released and closed\");\n+                                    }\n+                                    writer.write(channel, gap.start(), gap.end(), gap::onProgress);\n+                                    gap.onCompletion();\n+                                } finally {\n+                                    readLock.unlock();\n+                                }\n+                            } catch (Exception e) {\n+                                gap.onFailure(e);\n+                            } finally {\n+                                iterator.remove();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "28091b888998d690d9cf926aa0e80362f75cb590"}, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3NTI3MA==", "bodyText": "++ let's add a comment saying that in the code too.", "url": "https://github.com/elastic/elasticsearch/pull/58728#discussion_r448175270", "createdAt": "2020-07-01T07:39:32Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -224,31 +235,33 @@ public void prefetchPart(final int part) throws IOException {\n                     while (remainingBytes > 0L) {\n                         assert totalBytesRead + remainingBytes == rangeLength;\n                         final int bytesRead = readSafe(input, copyBuffer, rangeStart, rangeEnd, remainingBytes, cacheFileReference);\n+\n                         final long readStart = rangeStart + totalBytesRead;\n-                        cacheFile.fetchRange(readStart, readStart + bytesRead, (start, end) -> {\n-                            logger.trace(\n-                                \"prefetchPart: range [{}-{}] of file [{}] is available in cache\",\n-                                start,\n-                                end,\n-                                fileInfo.physicalName()\n-                            );\n-                            return Math.toIntExact(end - start);\n-                        }, (start, end) -> {\n-                            final ByteBuffer byteBuffer = ByteBuffer.wrap(\n-                                copyBuffer,\n-                                Math.toIntExact(start - readStart),\n-                                Math.toIntExact(end - start)\n-                            );\n-                            final int writtenBytes = positionalWrite(fc, start, byteBuffer);\n-                            logger.trace(\n-                                \"prefetchPart: writing range [{}-{}] of file [{}], [{}] bytes written\",\n-                                start,\n-                                end,\n-                                fileInfo.physicalName(),\n-                                writtenBytes\n-                            );\n-                            totalBytesWritten.addAndGet(writtenBytes);\n-                        });\n+                        final long readEnd = readStart + bytesRead;\n+\n+                        cacheFile.fetchAsync(\n+                            Tuple.tuple(readStart, readEnd),\n+                            Tuple.tuple(readStart, readStart),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzY3NzYzOQ=="}, "originalCommit": {"oid": "28091b888998d690d9cf926aa0e80362f75cb590"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE5NDAzMQ==", "bodyText": "++ there was a recent change to the Lucene APIs replacing byte[], int, int with ByteBuffer but this was not pushed all the way through this code.", "url": "https://github.com/elastic/elasticsearch/pull/58728#discussion_r448194031", "createdAt": "2020-07-01T08:14:08Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -141,13 +143,22 @@ protected void readInternal(ByteBuffer b) throws IOException {\n             try {\n                 final CacheFile cacheFile = getCacheFileSafe();\n                 try (Releasable ignored = cacheFile.fileLock()) {\n-                    final Tuple<Long, Long> range = computeRange(pos);\n-                    bytesRead = cacheFile.fetchRange(\n-                        range.v1(),\n-                        range.v2(),\n-                        (start, end) -> readCacheFile(cacheFile.getChannel(), end, pos, b, len),\n-                        (start, end) -> writeCacheFile(cacheFile.getChannel(), start, end)\n-                    ).get();\n+                    final Tuple<Long, Long> rangeToWrite = computeRange(pos);\n+                    final Tuple<Long, Long> rangeToRead = Tuple.tuple(pos, Math.min(pos + len, rangeToWrite.v2()));\n+\n+                    bytesRead = cacheFile.fetchAsync(rangeToWrite, rangeToRead, (channel) -> {\n+                        final int read;\n+                        if ((rangeToRead.v2() - rangeToRead.v1()) < b.remaining()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzY3NTY2MA=="}, "originalCommit": {"oid": "28091b888998d690d9cf926aa0e80362f75cb590"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODIwMTYxNQ==", "bodyText": "Can we assert that reader.read(channel) returns rangeToRead.end() - rangeToRead.start() here?", "url": "https://github.com/elastic/elasticsearch/pull/58728#discussion_r448201615", "createdAt": "2020-07-01T08:26:35Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CacheFile.java", "diffHunk": "@@ -259,38 +262,68 @@ private void ensureOpen() {\n         }\n     }\n \n-    CompletableFuture<Integer> fetchRange(\n-        long start,\n-        long end,\n-        CheckedBiFunction<Long, Long, Integer, IOException> onRangeAvailable,\n-        CheckedBiConsumer<Long, Long, IOException> onRangeMissing\n+    @FunctionalInterface\n+    interface CacheReader {\n+        int read(FileChannel channel) throws IOException;\n+    }\n+\n+    @FunctionalInterface\n+    interface CacheWriter {\n+        void write(FileChannel channel, long from, long to, Consumer<Long> progressUpdater) throws IOException;\n+    }\n+\n+    CompletableFuture<Integer> fetchAsync(\n+        final Tuple<Long, Long> rangeToWrite,\n+        final Tuple<Long, Long> rangeToRead,\n+        final CacheReader reader,\n+        final CacheWriter writer,\n+        final Executor executor\n     ) {\n         final CompletableFuture<Integer> future = new CompletableFuture<>();\n         try {\n-            if (start < 0 || start > tracker.getLength() || start > end || end > tracker.getLength()) {\n-                throw new IllegalArgumentException(\n-                    \"Invalid range [start=\" + start + \", end=\" + end + \"] for length [\" + tracker.getLength() + ']'\n-                );\n-            }\n             ensureOpen();\n             final List<SparseFileTracker.Gap> gaps = tracker.waitForRange(\n-                Tuple.tuple(start, end),\n-                Tuple.tuple(start, end), // TODO use progressive sub range to trigger read operations sooner\n-                ActionListener.wrap(\n-                    rangeReady -> future.complete(onRangeAvailable.apply(start, end)),\n-                    rangeFailure -> future.completeExceptionally(rangeFailure)\n-                )\n+                rangeToWrite,\n+                rangeToRead,\n+                ActionListener.wrap(success -> future.complete(reader.read(channel)), future::completeExceptionally)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "28091b888998d690d9cf926aa0e80362f75cb590"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODIwODQyNQ==", "bodyText": "What, no CheckedQuadConsumer<...>? \ud83d\ude01\nI'm torn about having these be separate interfaces vs combining them together perhaps along with some other arguments to fetchAsync too. I'm finding it especially strange that CacheReader#read only takes a FileChannel; this indicates that rangeToRead is now available, but that's very implicit now.\nMaybe different names would help. How about something like RangeMissingHandler#fillCacheRange and RangeAvailableHandler#onRangeAvailable?", "url": "https://github.com/elastic/elasticsearch/pull/58728#discussion_r448208425", "createdAt": "2020-07-01T08:38:04Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CacheFile.java", "diffHunk": "@@ -259,38 +262,68 @@ private void ensureOpen() {\n         }\n     }\n \n-    CompletableFuture<Integer> fetchRange(\n-        long start,\n-        long end,\n-        CheckedBiFunction<Long, Long, Integer, IOException> onRangeAvailable,\n-        CheckedBiConsumer<Long, Long, IOException> onRangeMissing\n+    @FunctionalInterface\n+    interface CacheReader {\n+        int read(FileChannel channel) throws IOException;\n+    }\n+\n+    @FunctionalInterface\n+    interface CacheWriter {\n+        void write(FileChannel channel, long from, long to, Consumer<Long> progressUpdater) throws IOException;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "28091b888998d690d9cf926aa0e80362f75cb590"}, "originalPosition": 48}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "afe11a0151726d4c66b856c410ddeba69e1be6f4", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/afe11a0151726d4c66b856c410ddeba69e1be6f4", "committedDate": "2020-07-01T09:46:19Z", "message": "apply feedback"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "36f84b7f3d3ecc003ccfc4449c665060b49b8798", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/36f84b7f3d3ecc003ccfc4449c665060b49b8798", "committedDate": "2020-07-01T09:46:39Z", "message": "Merge branch 'master' into use-progressable-listener-in-cache-file"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b0e5d1d24f0a8a0480ab85df2c3d0bcdf7287615", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/b0e5d1d24f0a8a0480ab85df2c3d0bcdf7287615", "committedDate": "2020-07-01T10:09:46Z", "message": "Revert InternalTestCluster"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "53232bd7ee3c7058ce3109edfd649cc2b25b4cec", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/53232bd7ee3c7058ce3109edfd649cc2b25b4cec", "committedDate": "2020-07-01T10:16:08Z", "message": "assert thread pool in positionalWrite"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQwNzk0MjI4", "url": "https://github.com/elastic/elasticsearch/pull/58728#pullrequestreview-440794228", "createdAt": "2020-07-01T11:45:45Z", "commit": {"oid": "53232bd7ee3c7058ce3109edfd649cc2b25b4cec"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2586, "cost": 1, "resetAt": "2021-10-28T18:54:27Z"}}}