{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDI3MTY3NTMx", "number": 57592, "reviewThreads": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxMzozOTo0N1rOECKFmQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxODoyNjo0OFrOECrpuw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwNjk3ODgxOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/loadingservice/ModelLoadingService.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxMzozOTo0N1rOGecR_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxMzozOTo0N1rOGecR_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDU3MzgyMw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        // explicit declaration of the listener lambda necessary for Eclipse IDE 4.14", "url": "https://github.com/elastic/elasticsearch/pull/57592#discussion_r434573823", "createdAt": "2020-06-03T13:39:47Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/loadingservice/ModelLoadingService.java", "diffHunk": "@@ -112,104 +130,130 @@ public ModelLoadingService(TrainedModelProvider trainedModelProvider,\n         this.modelStatsService = modelStatsService;\n         this.shouldNotAudit = new HashSet<>();\n         this.namedXContentRegistry = namedXContentRegistry;\n-        this.localModelCache = CacheBuilder.<String, LocalModel>builder()\n+        this.localModelCache = CacheBuilder.<String, ModelAndConsumer>builder()\n             .setMaximumWeight(this.maxCacheSize.getBytes())\n-            .weigher((id, localModel) -> localModel.ramBytesUsed())\n+            .weigher((id, modelAndConsumer) -> modelAndConsumer.model.ramBytesUsed())\n             // explicit declaration of the listener lambda necessary for Eclipse IDE 4.14", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "990d5a67db47d1263bfc7719e142e8bfd0e3d8e7"}, "originalPosition": 82}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxMjQwMTU3OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/loadingservice/ModelLoadingService.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxODowNzo0MlrOGfRxug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxODowNzo0MlrOGfRxug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ1MDI5OA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public boolean modelIsCached(String modelId) {\n          \n          \n            \n                boolean modelIsCached(String modelId) {\n          \n      \n    \n    \n  \n\nLets make this package private so folks aren't tempted to use it. In a real scenario, this could return true, and then the model is removed when the caller attempts to get it.", "url": "https://github.com/elastic/elasticsearch/pull/57592#discussion_r435450298", "createdAt": "2020-06-04T18:07:42Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/loadingservice/ModelLoadingService.java", "diffHunk": "@@ -112,104 +130,130 @@ public ModelLoadingService(TrainedModelProvider trainedModelProvider,\n         this.modelStatsService = modelStatsService;\n         this.shouldNotAudit = new HashSet<>();\n         this.namedXContentRegistry = namedXContentRegistry;\n-        this.localModelCache = CacheBuilder.<String, LocalModel>builder()\n+        this.localModelCache = CacheBuilder.<String, ModelAndConsumer>builder()\n             .setMaximumWeight(this.maxCacheSize.getBytes())\n-            .weigher((id, localModel) -> localModel.ramBytesUsed())\n+            .weigher((id, modelAndConsumer) -> modelAndConsumer.model.ramBytesUsed())\n             // explicit declaration of the listener lambda necessary for Eclipse IDE 4.14\n-            .removalListener(notification -> cacheEvictionListener(notification))\n+            .removalListener(this::cacheEvictionListener)\n             .setExpireAfterAccess(INFERENCE_MODEL_CACHE_TTL.get(settings))\n             .build();\n         clusterService.addListener(this);\n         this.localNode = localNode;\n     }\n \n+    public boolean modelIsCached(String modelId) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "990d5a67db47d1263bfc7719e142e8bfd0e3d8e7"}, "originalPosition": 91}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxMjQzMzg2OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/loadingservice/ModelLoadingService.java", "isResolved": false, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxODoxNzozNFrOGfSHAA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQxOTo1Njo0OFrOGhZ--Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ1NTc0NA==", "bodyText": "There is a pretty frustrating race condition here, unsure if it is worth fixing.\n\nModelAndConsumer cachedModel = localModelCache.get(modelId); returns from cache successfully\nModel is removed from cache as it is only referenced by a pipeline (updated via cluster state change)\nSearch adds itself as a consume too late and it is evicted\n\nThe referenced model from the get call will still be around. But the next call for the same model will load from the index again :/.\nThe only way I could see of fixing this is locking here (which defeats the purpose of the fast get). Since the cost is low, it might just be OK to have this as it is recoverable later.", "url": "https://github.com/elastic/elasticsearch/pull/57592#discussion_r435455744", "createdAt": "2020-06-04T18:17:34Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/loadingservice/ModelLoadingService.java", "diffHunk": "@@ -112,104 +130,130 @@ public ModelLoadingService(TrainedModelProvider trainedModelProvider,\n         this.modelStatsService = modelStatsService;\n         this.shouldNotAudit = new HashSet<>();\n         this.namedXContentRegistry = namedXContentRegistry;\n-        this.localModelCache = CacheBuilder.<String, LocalModel>builder()\n+        this.localModelCache = CacheBuilder.<String, ModelAndConsumer>builder()\n             .setMaximumWeight(this.maxCacheSize.getBytes())\n-            .weigher((id, localModel) -> localModel.ramBytesUsed())\n+            .weigher((id, modelAndConsumer) -> modelAndConsumer.model.ramBytesUsed())\n             // explicit declaration of the listener lambda necessary for Eclipse IDE 4.14\n-            .removalListener(notification -> cacheEvictionListener(notification))\n+            .removalListener(this::cacheEvictionListener)\n             .setExpireAfterAccess(INFERENCE_MODEL_CACHE_TTL.get(settings))\n             .build();\n         clusterService.addListener(this);\n         this.localNode = localNode;\n     }\n \n+    public boolean modelIsCached(String modelId) {\n+        return localModelCache.get(modelId) != null;\n+    }\n+\n     /**\n-     * Gets the model referenced by `modelId` and responds to the listener.\n+     * Load the model for use by an ingest pipeline. The model will not be cached if there is no\n+     * ingest pipeline referencing it i.e. it is used in simulate mode\n+     *\n+     * @param modelId  the model to get\n+     * @param modelActionListener the listener to alert when the model has been retrieved\n+     */\n+    public void getModelForPipeline(String modelId, ActionListener<Model> modelActionListener) {\n+        getModel(modelId, Consumer.PIPELINE, modelActionListener);\n+    }\n+\n+    /**\n+     * Load the model for use by at search. Models requested by search are always cached.\n      *\n+     * @param modelId  the model to get\n+     * @param modelActionListener the listener to alert when the model has been retrieved\n+     */\n+    public void getModelForSearch(String modelId, ActionListener<Model> modelActionListener) {\n+        getModel(modelId, Consumer.SEARCH, modelActionListener);\n+    }\n+\n+    /**\n+     * Gets the model referenced by `modelId` and responds to the listener.\n+     * <p>\n      * This method first checks the local LRU cache for the model. If it is present, it is returned from cache.\n+     * <p>\n+     * In the case of search if the model is not present one of the following occurs:\n+     * - If it is currently being loaded the `modelActionListener`\n+     * is added to the list of listeners to be alerted when the model is fully loaded.\n+     * - Otherwise the model is loaded and cached\n      *\n-     * If it is not present, one of the following occurs:\n+     * In the case of an ingest processor if it is not present, one of the following occurs:\n+     * <p>\n+     * - If the model is referenced by a pipeline and is currently being loaded, the `modelActionListener`\n+     * is added to the list of listeners to be alerted when the model is fully loaded.\n+     * - If the model is referenced by a pipeline and is currently NOT being loaded, a new load attempt is made and the resulting\n+     * model will attempt to be cached for future reference\n+     * - If the models is NOT referenced by a pipeline, the model is simply loaded from the index and given to the listener.\n+     * It is not cached.\n      *\n-     *  - If the model is referenced by a pipeline and is currently being loaded, the `modelActionListener`\n-     *    is added to the list of listeners to be alerted when the model is fully loaded.\n-     *  - If the model is referenced by a pipeline and is currently NOT being loaded, a new load attempt is made and the resulting\n-     *    model will attempt to be cached for future reference\n-     *  - If the models is NOT referenced by a pipeline, the model is simply loaded from the index and given to the listener.\n-     *    It is not cached.\n+     * The main difference being that models for search are always cached whereas pipeline models\n+     * are only cached if they are referenced by an ingest pipeline\n      *\n-     * @param modelId the model to get\n+     * @param modelId             the model to get\n+     * @param consumer            which feature is requesting the model\n      * @param modelActionListener the listener to alert when the model has been retrieved.\n      */\n-    public void getModel(String modelId, ActionListener<Model> modelActionListener) {\n-        LocalModel cachedModel = localModelCache.get(modelId);\n+    private void getModel(String modelId, Consumer consumer, ActionListener<Model> modelActionListener) {\n+        ModelAndConsumer cachedModel = localModelCache.get(modelId);\n         if (cachedModel != null) {\n-            modelActionListener.onResponse(cachedModel);\n+            cachedModel.consumers.add(consumer);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "990d5a67db47d1263bfc7719e142e8bfd0e3d8e7"}, "originalPosition": 157}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTc0MDUyMQ==", "bodyText": "Good point it is recoverable and there will always be cases where the model is requested after it has been evicted so it is not too bad. But I also think if the cache entry is being modified we need to ensure that change is visible to other threads and synchronised is an easy way of doing that.\nI can't decide which to do here \ud83e\udd14", "url": "https://github.com/elastic/elasticsearch/pull/57592#discussion_r435740521", "createdAt": "2020-06-05T07:33:20Z", "author": {"login": "davidkyle"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/loadingservice/ModelLoadingService.java", "diffHunk": "@@ -112,104 +130,130 @@ public ModelLoadingService(TrainedModelProvider trainedModelProvider,\n         this.modelStatsService = modelStatsService;\n         this.shouldNotAudit = new HashSet<>();\n         this.namedXContentRegistry = namedXContentRegistry;\n-        this.localModelCache = CacheBuilder.<String, LocalModel>builder()\n+        this.localModelCache = CacheBuilder.<String, ModelAndConsumer>builder()\n             .setMaximumWeight(this.maxCacheSize.getBytes())\n-            .weigher((id, localModel) -> localModel.ramBytesUsed())\n+            .weigher((id, modelAndConsumer) -> modelAndConsumer.model.ramBytesUsed())\n             // explicit declaration of the listener lambda necessary for Eclipse IDE 4.14\n-            .removalListener(notification -> cacheEvictionListener(notification))\n+            .removalListener(this::cacheEvictionListener)\n             .setExpireAfterAccess(INFERENCE_MODEL_CACHE_TTL.get(settings))\n             .build();\n         clusterService.addListener(this);\n         this.localNode = localNode;\n     }\n \n+    public boolean modelIsCached(String modelId) {\n+        return localModelCache.get(modelId) != null;\n+    }\n+\n     /**\n-     * Gets the model referenced by `modelId` and responds to the listener.\n+     * Load the model for use by an ingest pipeline. The model will not be cached if there is no\n+     * ingest pipeline referencing it i.e. it is used in simulate mode\n+     *\n+     * @param modelId  the model to get\n+     * @param modelActionListener the listener to alert when the model has been retrieved\n+     */\n+    public void getModelForPipeline(String modelId, ActionListener<Model> modelActionListener) {\n+        getModel(modelId, Consumer.PIPELINE, modelActionListener);\n+    }\n+\n+    /**\n+     * Load the model for use by at search. Models requested by search are always cached.\n      *\n+     * @param modelId  the model to get\n+     * @param modelActionListener the listener to alert when the model has been retrieved\n+     */\n+    public void getModelForSearch(String modelId, ActionListener<Model> modelActionListener) {\n+        getModel(modelId, Consumer.SEARCH, modelActionListener);\n+    }\n+\n+    /**\n+     * Gets the model referenced by `modelId` and responds to the listener.\n+     * <p>\n      * This method first checks the local LRU cache for the model. If it is present, it is returned from cache.\n+     * <p>\n+     * In the case of search if the model is not present one of the following occurs:\n+     * - If it is currently being loaded the `modelActionListener`\n+     * is added to the list of listeners to be alerted when the model is fully loaded.\n+     * - Otherwise the model is loaded and cached\n      *\n-     * If it is not present, one of the following occurs:\n+     * In the case of an ingest processor if it is not present, one of the following occurs:\n+     * <p>\n+     * - If the model is referenced by a pipeline and is currently being loaded, the `modelActionListener`\n+     * is added to the list of listeners to be alerted when the model is fully loaded.\n+     * - If the model is referenced by a pipeline and is currently NOT being loaded, a new load attempt is made and the resulting\n+     * model will attempt to be cached for future reference\n+     * - If the models is NOT referenced by a pipeline, the model is simply loaded from the index and given to the listener.\n+     * It is not cached.\n      *\n-     *  - If the model is referenced by a pipeline and is currently being loaded, the `modelActionListener`\n-     *    is added to the list of listeners to be alerted when the model is fully loaded.\n-     *  - If the model is referenced by a pipeline and is currently NOT being loaded, a new load attempt is made and the resulting\n-     *    model will attempt to be cached for future reference\n-     *  - If the models is NOT referenced by a pipeline, the model is simply loaded from the index and given to the listener.\n-     *    It is not cached.\n+     * The main difference being that models for search are always cached whereas pipeline models\n+     * are only cached if they are referenced by an ingest pipeline\n      *\n-     * @param modelId the model to get\n+     * @param modelId             the model to get\n+     * @param consumer            which feature is requesting the model\n      * @param modelActionListener the listener to alert when the model has been retrieved.\n      */\n-    public void getModel(String modelId, ActionListener<Model> modelActionListener) {\n-        LocalModel cachedModel = localModelCache.get(modelId);\n+    private void getModel(String modelId, Consumer consumer, ActionListener<Model> modelActionListener) {\n+        ModelAndConsumer cachedModel = localModelCache.get(modelId);\n         if (cachedModel != null) {\n-            modelActionListener.onResponse(cachedModel);\n+            cachedModel.consumers.add(consumer);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ1NTc0NA=="}, "originalCommit": {"oid": "990d5a67db47d1263bfc7719e142e8bfd0e3d8e7"}, "originalPosition": 157}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTg1MzkxOQ==", "bodyText": "But I also think if the cache entry is being modified we need to ensure that change is visible to other threads and synchronised is an easy way of doing that.\n\nAre enum set changes visible to other threads? If they are not, we should use a MT safe/acceptable set for storing which consumers have the model.\nI don't think we should synchronize here. Simply having something volatile somewhere in the call path will work.", "url": "https://github.com/elastic/elasticsearch/pull/57592#discussion_r435853919", "createdAt": "2020-06-05T11:14:30Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/loadingservice/ModelLoadingService.java", "diffHunk": "@@ -112,104 +130,130 @@ public ModelLoadingService(TrainedModelProvider trainedModelProvider,\n         this.modelStatsService = modelStatsService;\n         this.shouldNotAudit = new HashSet<>();\n         this.namedXContentRegistry = namedXContentRegistry;\n-        this.localModelCache = CacheBuilder.<String, LocalModel>builder()\n+        this.localModelCache = CacheBuilder.<String, ModelAndConsumer>builder()\n             .setMaximumWeight(this.maxCacheSize.getBytes())\n-            .weigher((id, localModel) -> localModel.ramBytesUsed())\n+            .weigher((id, modelAndConsumer) -> modelAndConsumer.model.ramBytesUsed())\n             // explicit declaration of the listener lambda necessary for Eclipse IDE 4.14\n-            .removalListener(notification -> cacheEvictionListener(notification))\n+            .removalListener(this::cacheEvictionListener)\n             .setExpireAfterAccess(INFERENCE_MODEL_CACHE_TTL.get(settings))\n             .build();\n         clusterService.addListener(this);\n         this.localNode = localNode;\n     }\n \n+    public boolean modelIsCached(String modelId) {\n+        return localModelCache.get(modelId) != null;\n+    }\n+\n     /**\n-     * Gets the model referenced by `modelId` and responds to the listener.\n+     * Load the model for use by an ingest pipeline. The model will not be cached if there is no\n+     * ingest pipeline referencing it i.e. it is used in simulate mode\n+     *\n+     * @param modelId  the model to get\n+     * @param modelActionListener the listener to alert when the model has been retrieved\n+     */\n+    public void getModelForPipeline(String modelId, ActionListener<Model> modelActionListener) {\n+        getModel(modelId, Consumer.PIPELINE, modelActionListener);\n+    }\n+\n+    /**\n+     * Load the model for use by at search. Models requested by search are always cached.\n      *\n+     * @param modelId  the model to get\n+     * @param modelActionListener the listener to alert when the model has been retrieved\n+     */\n+    public void getModelForSearch(String modelId, ActionListener<Model> modelActionListener) {\n+        getModel(modelId, Consumer.SEARCH, modelActionListener);\n+    }\n+\n+    /**\n+     * Gets the model referenced by `modelId` and responds to the listener.\n+     * <p>\n      * This method first checks the local LRU cache for the model. If it is present, it is returned from cache.\n+     * <p>\n+     * In the case of search if the model is not present one of the following occurs:\n+     * - If it is currently being loaded the `modelActionListener`\n+     * is added to the list of listeners to be alerted when the model is fully loaded.\n+     * - Otherwise the model is loaded and cached\n      *\n-     * If it is not present, one of the following occurs:\n+     * In the case of an ingest processor if it is not present, one of the following occurs:\n+     * <p>\n+     * - If the model is referenced by a pipeline and is currently being loaded, the `modelActionListener`\n+     * is added to the list of listeners to be alerted when the model is fully loaded.\n+     * - If the model is referenced by a pipeline and is currently NOT being loaded, a new load attempt is made and the resulting\n+     * model will attempt to be cached for future reference\n+     * - If the models is NOT referenced by a pipeline, the model is simply loaded from the index and given to the listener.\n+     * It is not cached.\n      *\n-     *  - If the model is referenced by a pipeline and is currently being loaded, the `modelActionListener`\n-     *    is added to the list of listeners to be alerted when the model is fully loaded.\n-     *  - If the model is referenced by a pipeline and is currently NOT being loaded, a new load attempt is made and the resulting\n-     *    model will attempt to be cached for future reference\n-     *  - If the models is NOT referenced by a pipeline, the model is simply loaded from the index and given to the listener.\n-     *    It is not cached.\n+     * The main difference being that models for search are always cached whereas pipeline models\n+     * are only cached if they are referenced by an ingest pipeline\n      *\n-     * @param modelId the model to get\n+     * @param modelId             the model to get\n+     * @param consumer            which feature is requesting the model\n      * @param modelActionListener the listener to alert when the model has been retrieved.\n      */\n-    public void getModel(String modelId, ActionListener<Model> modelActionListener) {\n-        LocalModel cachedModel = localModelCache.get(modelId);\n+    private void getModel(String modelId, Consumer consumer, ActionListener<Model> modelActionListener) {\n+        ModelAndConsumer cachedModel = localModelCache.get(modelId);\n         if (cachedModel != null) {\n-            modelActionListener.onResponse(cachedModel);\n+            cachedModel.consumers.add(consumer);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ1NTc0NA=="}, "originalCommit": {"oid": "990d5a67db47d1263bfc7719e142e8bfd0e3d8e7"}, "originalPosition": 157}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjEwNDQwMQ==", "bodyText": "The enum set is a nice fast efficient bit set under the hood I'd rather not change that for something thread safe.\nIt is the eviction where we really care about reading the changes to the enum set as that is where the check for who is using the model, that code is synchronised so no visibility problems there.\nI think we are good with the fast unsynchronised lookup, it is optimised for the common case and we are eventually consistent anyway", "url": "https://github.com/elastic/elasticsearch/pull/57592#discussion_r436104401", "createdAt": "2020-06-05T18:49:35Z", "author": {"login": "davidkyle"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/loadingservice/ModelLoadingService.java", "diffHunk": "@@ -112,104 +130,130 @@ public ModelLoadingService(TrainedModelProvider trainedModelProvider,\n         this.modelStatsService = modelStatsService;\n         this.shouldNotAudit = new HashSet<>();\n         this.namedXContentRegistry = namedXContentRegistry;\n-        this.localModelCache = CacheBuilder.<String, LocalModel>builder()\n+        this.localModelCache = CacheBuilder.<String, ModelAndConsumer>builder()\n             .setMaximumWeight(this.maxCacheSize.getBytes())\n-            .weigher((id, localModel) -> localModel.ramBytesUsed())\n+            .weigher((id, modelAndConsumer) -> modelAndConsumer.model.ramBytesUsed())\n             // explicit declaration of the listener lambda necessary for Eclipse IDE 4.14\n-            .removalListener(notification -> cacheEvictionListener(notification))\n+            .removalListener(this::cacheEvictionListener)\n             .setExpireAfterAccess(INFERENCE_MODEL_CACHE_TTL.get(settings))\n             .build();\n         clusterService.addListener(this);\n         this.localNode = localNode;\n     }\n \n+    public boolean modelIsCached(String modelId) {\n+        return localModelCache.get(modelId) != null;\n+    }\n+\n     /**\n-     * Gets the model referenced by `modelId` and responds to the listener.\n+     * Load the model for use by an ingest pipeline. The model will not be cached if there is no\n+     * ingest pipeline referencing it i.e. it is used in simulate mode\n+     *\n+     * @param modelId  the model to get\n+     * @param modelActionListener the listener to alert when the model has been retrieved\n+     */\n+    public void getModelForPipeline(String modelId, ActionListener<Model> modelActionListener) {\n+        getModel(modelId, Consumer.PIPELINE, modelActionListener);\n+    }\n+\n+    /**\n+     * Load the model for use by at search. Models requested by search are always cached.\n      *\n+     * @param modelId  the model to get\n+     * @param modelActionListener the listener to alert when the model has been retrieved\n+     */\n+    public void getModelForSearch(String modelId, ActionListener<Model> modelActionListener) {\n+        getModel(modelId, Consumer.SEARCH, modelActionListener);\n+    }\n+\n+    /**\n+     * Gets the model referenced by `modelId` and responds to the listener.\n+     * <p>\n      * This method first checks the local LRU cache for the model. If it is present, it is returned from cache.\n+     * <p>\n+     * In the case of search if the model is not present one of the following occurs:\n+     * - If it is currently being loaded the `modelActionListener`\n+     * is added to the list of listeners to be alerted when the model is fully loaded.\n+     * - Otherwise the model is loaded and cached\n      *\n-     * If it is not present, one of the following occurs:\n+     * In the case of an ingest processor if it is not present, one of the following occurs:\n+     * <p>\n+     * - If the model is referenced by a pipeline and is currently being loaded, the `modelActionListener`\n+     * is added to the list of listeners to be alerted when the model is fully loaded.\n+     * - If the model is referenced by a pipeline and is currently NOT being loaded, a new load attempt is made and the resulting\n+     * model will attempt to be cached for future reference\n+     * - If the models is NOT referenced by a pipeline, the model is simply loaded from the index and given to the listener.\n+     * It is not cached.\n      *\n-     *  - If the model is referenced by a pipeline and is currently being loaded, the `modelActionListener`\n-     *    is added to the list of listeners to be alerted when the model is fully loaded.\n-     *  - If the model is referenced by a pipeline and is currently NOT being loaded, a new load attempt is made and the resulting\n-     *    model will attempt to be cached for future reference\n-     *  - If the models is NOT referenced by a pipeline, the model is simply loaded from the index and given to the listener.\n-     *    It is not cached.\n+     * The main difference being that models for search are always cached whereas pipeline models\n+     * are only cached if they are referenced by an ingest pipeline\n      *\n-     * @param modelId the model to get\n+     * @param modelId             the model to get\n+     * @param consumer            which feature is requesting the model\n      * @param modelActionListener the listener to alert when the model has been retrieved.\n      */\n-    public void getModel(String modelId, ActionListener<Model> modelActionListener) {\n-        LocalModel cachedModel = localModelCache.get(modelId);\n+    private void getModel(String modelId, Consumer consumer, ActionListener<Model> modelActionListener) {\n+        ModelAndConsumer cachedModel = localModelCache.get(modelId);\n         if (cachedModel != null) {\n-            modelActionListener.onResponse(cachedModel);\n+            cachedModel.consumers.add(consumer);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ1NTc0NA=="}, "originalCommit": {"oid": "990d5a67db47d1263bfc7719e142e8bfd0e3d8e7"}, "originalPosition": 157}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzYxMDc4MA==", "bodyText": "I think we are good with the fast unsynchronised lookup, it is optimised for the common case and we are eventually consistent anyway\n\nOK, sounds good. The PR looks like that has been removed?", "url": "https://github.com/elastic/elasticsearch/pull/57592#discussion_r437610780", "createdAt": "2020-06-09T17:48:22Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/loadingservice/ModelLoadingService.java", "diffHunk": "@@ -112,104 +130,130 @@ public ModelLoadingService(TrainedModelProvider trainedModelProvider,\n         this.modelStatsService = modelStatsService;\n         this.shouldNotAudit = new HashSet<>();\n         this.namedXContentRegistry = namedXContentRegistry;\n-        this.localModelCache = CacheBuilder.<String, LocalModel>builder()\n+        this.localModelCache = CacheBuilder.<String, ModelAndConsumer>builder()\n             .setMaximumWeight(this.maxCacheSize.getBytes())\n-            .weigher((id, localModel) -> localModel.ramBytesUsed())\n+            .weigher((id, modelAndConsumer) -> modelAndConsumer.model.ramBytesUsed())\n             // explicit declaration of the listener lambda necessary for Eclipse IDE 4.14\n-            .removalListener(notification -> cacheEvictionListener(notification))\n+            .removalListener(this::cacheEvictionListener)\n             .setExpireAfterAccess(INFERENCE_MODEL_CACHE_TTL.get(settings))\n             .build();\n         clusterService.addListener(this);\n         this.localNode = localNode;\n     }\n \n+    public boolean modelIsCached(String modelId) {\n+        return localModelCache.get(modelId) != null;\n+    }\n+\n     /**\n-     * Gets the model referenced by `modelId` and responds to the listener.\n+     * Load the model for use by an ingest pipeline. The model will not be cached if there is no\n+     * ingest pipeline referencing it i.e. it is used in simulate mode\n+     *\n+     * @param modelId  the model to get\n+     * @param modelActionListener the listener to alert when the model has been retrieved\n+     */\n+    public void getModelForPipeline(String modelId, ActionListener<Model> modelActionListener) {\n+        getModel(modelId, Consumer.PIPELINE, modelActionListener);\n+    }\n+\n+    /**\n+     * Load the model for use by at search. Models requested by search are always cached.\n      *\n+     * @param modelId  the model to get\n+     * @param modelActionListener the listener to alert when the model has been retrieved\n+     */\n+    public void getModelForSearch(String modelId, ActionListener<Model> modelActionListener) {\n+        getModel(modelId, Consumer.SEARCH, modelActionListener);\n+    }\n+\n+    /**\n+     * Gets the model referenced by `modelId` and responds to the listener.\n+     * <p>\n      * This method first checks the local LRU cache for the model. If it is present, it is returned from cache.\n+     * <p>\n+     * In the case of search if the model is not present one of the following occurs:\n+     * - If it is currently being loaded the `modelActionListener`\n+     * is added to the list of listeners to be alerted when the model is fully loaded.\n+     * - Otherwise the model is loaded and cached\n      *\n-     * If it is not present, one of the following occurs:\n+     * In the case of an ingest processor if it is not present, one of the following occurs:\n+     * <p>\n+     * - If the model is referenced by a pipeline and is currently being loaded, the `modelActionListener`\n+     * is added to the list of listeners to be alerted when the model is fully loaded.\n+     * - If the model is referenced by a pipeline and is currently NOT being loaded, a new load attempt is made and the resulting\n+     * model will attempt to be cached for future reference\n+     * - If the models is NOT referenced by a pipeline, the model is simply loaded from the index and given to the listener.\n+     * It is not cached.\n      *\n-     *  - If the model is referenced by a pipeline and is currently being loaded, the `modelActionListener`\n-     *    is added to the list of listeners to be alerted when the model is fully loaded.\n-     *  - If the model is referenced by a pipeline and is currently NOT being loaded, a new load attempt is made and the resulting\n-     *    model will attempt to be cached for future reference\n-     *  - If the models is NOT referenced by a pipeline, the model is simply loaded from the index and given to the listener.\n-     *    It is not cached.\n+     * The main difference being that models for search are always cached whereas pipeline models\n+     * are only cached if they are referenced by an ingest pipeline\n      *\n-     * @param modelId the model to get\n+     * @param modelId             the model to get\n+     * @param consumer            which feature is requesting the model\n      * @param modelActionListener the listener to alert when the model has been retrieved.\n      */\n-    public void getModel(String modelId, ActionListener<Model> modelActionListener) {\n-        LocalModel cachedModel = localModelCache.get(modelId);\n+    private void getModel(String modelId, Consumer consumer, ActionListener<Model> modelActionListener) {\n+        ModelAndConsumer cachedModel = localModelCache.get(modelId);\n         if (cachedModel != null) {\n-            modelActionListener.onResponse(cachedModel);\n+            cachedModel.consumers.add(consumer);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ1NTc0NA=="}, "originalCommit": {"oid": "990d5a67db47d1263bfc7719e142e8bfd0e3d8e7"}, "originalPosition": 157}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY4MTkxMw==", "bodyText": "Oops, reinstated. I'll put that down to a fat fingered rebase", "url": "https://github.com/elastic/elasticsearch/pull/57592#discussion_r437681913", "createdAt": "2020-06-09T19:56:48Z", "author": {"login": "davidkyle"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/loadingservice/ModelLoadingService.java", "diffHunk": "@@ -112,104 +130,130 @@ public ModelLoadingService(TrainedModelProvider trainedModelProvider,\n         this.modelStatsService = modelStatsService;\n         this.shouldNotAudit = new HashSet<>();\n         this.namedXContentRegistry = namedXContentRegistry;\n-        this.localModelCache = CacheBuilder.<String, LocalModel>builder()\n+        this.localModelCache = CacheBuilder.<String, ModelAndConsumer>builder()\n             .setMaximumWeight(this.maxCacheSize.getBytes())\n-            .weigher((id, localModel) -> localModel.ramBytesUsed())\n+            .weigher((id, modelAndConsumer) -> modelAndConsumer.model.ramBytesUsed())\n             // explicit declaration of the listener lambda necessary for Eclipse IDE 4.14\n-            .removalListener(notification -> cacheEvictionListener(notification))\n+            .removalListener(this::cacheEvictionListener)\n             .setExpireAfterAccess(INFERENCE_MODEL_CACHE_TTL.get(settings))\n             .build();\n         clusterService.addListener(this);\n         this.localNode = localNode;\n     }\n \n+    public boolean modelIsCached(String modelId) {\n+        return localModelCache.get(modelId) != null;\n+    }\n+\n     /**\n-     * Gets the model referenced by `modelId` and responds to the listener.\n+     * Load the model for use by an ingest pipeline. The model will not be cached if there is no\n+     * ingest pipeline referencing it i.e. it is used in simulate mode\n+     *\n+     * @param modelId  the model to get\n+     * @param modelActionListener the listener to alert when the model has been retrieved\n+     */\n+    public void getModelForPipeline(String modelId, ActionListener<Model> modelActionListener) {\n+        getModel(modelId, Consumer.PIPELINE, modelActionListener);\n+    }\n+\n+    /**\n+     * Load the model for use by at search. Models requested by search are always cached.\n      *\n+     * @param modelId  the model to get\n+     * @param modelActionListener the listener to alert when the model has been retrieved\n+     */\n+    public void getModelForSearch(String modelId, ActionListener<Model> modelActionListener) {\n+        getModel(modelId, Consumer.SEARCH, modelActionListener);\n+    }\n+\n+    /**\n+     * Gets the model referenced by `modelId` and responds to the listener.\n+     * <p>\n      * This method first checks the local LRU cache for the model. If it is present, it is returned from cache.\n+     * <p>\n+     * In the case of search if the model is not present one of the following occurs:\n+     * - If it is currently being loaded the `modelActionListener`\n+     * is added to the list of listeners to be alerted when the model is fully loaded.\n+     * - Otherwise the model is loaded and cached\n      *\n-     * If it is not present, one of the following occurs:\n+     * In the case of an ingest processor if it is not present, one of the following occurs:\n+     * <p>\n+     * - If the model is referenced by a pipeline and is currently being loaded, the `modelActionListener`\n+     * is added to the list of listeners to be alerted when the model is fully loaded.\n+     * - If the model is referenced by a pipeline and is currently NOT being loaded, a new load attempt is made and the resulting\n+     * model will attempt to be cached for future reference\n+     * - If the models is NOT referenced by a pipeline, the model is simply loaded from the index and given to the listener.\n+     * It is not cached.\n      *\n-     *  - If the model is referenced by a pipeline and is currently being loaded, the `modelActionListener`\n-     *    is added to the list of listeners to be alerted when the model is fully loaded.\n-     *  - If the model is referenced by a pipeline and is currently NOT being loaded, a new load attempt is made and the resulting\n-     *    model will attempt to be cached for future reference\n-     *  - If the models is NOT referenced by a pipeline, the model is simply loaded from the index and given to the listener.\n-     *    It is not cached.\n+     * The main difference being that models for search are always cached whereas pipeline models\n+     * are only cached if they are referenced by an ingest pipeline\n      *\n-     * @param modelId the model to get\n+     * @param modelId             the model to get\n+     * @param consumer            which feature is requesting the model\n      * @param modelActionListener the listener to alert when the model has been retrieved.\n      */\n-    public void getModel(String modelId, ActionListener<Model> modelActionListener) {\n-        LocalModel cachedModel = localModelCache.get(modelId);\n+    private void getModel(String modelId, Consumer consumer, ActionListener<Model> modelActionListener) {\n+        ModelAndConsumer cachedModel = localModelCache.get(modelId);\n         if (cachedModel != null) {\n-            modelActionListener.onResponse(cachedModel);\n+            cachedModel.consumers.add(consumer);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ1NTc0NA=="}, "originalCommit": {"oid": "990d5a67db47d1263bfc7719e142e8bfd0e3d8e7"}, "originalPosition": 157}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxMjQ0NzQyOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/loadingservice/ModelLoadingService.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxODoyMDo1NlrOGfSPNw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxODoyMDo1NlrOGfSPNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ1Nzg0Nw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    if (loadModelIfNecessary(modelId, consumer, modelActionListener) == false) {\n          \n          \n            \n                    if (loadModelIfNecessary(modelId, consumer, modelActionListener)) {\n          \n      \n    \n    \n  \n\nReturning true from the method means it is loaded or already loading. As the logging message suggests :).", "url": "https://github.com/elastic/elasticsearch/pull/57592#discussion_r435457847", "createdAt": "2020-06-04T18:20:56Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/loadingservice/ModelLoadingService.java", "diffHunk": "@@ -112,104 +130,130 @@ public ModelLoadingService(TrainedModelProvider trainedModelProvider,\n         this.modelStatsService = modelStatsService;\n         this.shouldNotAudit = new HashSet<>();\n         this.namedXContentRegistry = namedXContentRegistry;\n-        this.localModelCache = CacheBuilder.<String, LocalModel>builder()\n+        this.localModelCache = CacheBuilder.<String, ModelAndConsumer>builder()\n             .setMaximumWeight(this.maxCacheSize.getBytes())\n-            .weigher((id, localModel) -> localModel.ramBytesUsed())\n+            .weigher((id, modelAndConsumer) -> modelAndConsumer.model.ramBytesUsed())\n             // explicit declaration of the listener lambda necessary for Eclipse IDE 4.14\n-            .removalListener(notification -> cacheEvictionListener(notification))\n+            .removalListener(this::cacheEvictionListener)\n             .setExpireAfterAccess(INFERENCE_MODEL_CACHE_TTL.get(settings))\n             .build();\n         clusterService.addListener(this);\n         this.localNode = localNode;\n     }\n \n+    public boolean modelIsCached(String modelId) {\n+        return localModelCache.get(modelId) != null;\n+    }\n+\n     /**\n-     * Gets the model referenced by `modelId` and responds to the listener.\n+     * Load the model for use by an ingest pipeline. The model will not be cached if there is no\n+     * ingest pipeline referencing it i.e. it is used in simulate mode\n+     *\n+     * @param modelId  the model to get\n+     * @param modelActionListener the listener to alert when the model has been retrieved\n+     */\n+    public void getModelForPipeline(String modelId, ActionListener<Model> modelActionListener) {\n+        getModel(modelId, Consumer.PIPELINE, modelActionListener);\n+    }\n+\n+    /**\n+     * Load the model for use by at search. Models requested by search are always cached.\n      *\n+     * @param modelId  the model to get\n+     * @param modelActionListener the listener to alert when the model has been retrieved\n+     */\n+    public void getModelForSearch(String modelId, ActionListener<Model> modelActionListener) {\n+        getModel(modelId, Consumer.SEARCH, modelActionListener);\n+    }\n+\n+    /**\n+     * Gets the model referenced by `modelId` and responds to the listener.\n+     * <p>\n      * This method first checks the local LRU cache for the model. If it is present, it is returned from cache.\n+     * <p>\n+     * In the case of search if the model is not present one of the following occurs:\n+     * - If it is currently being loaded the `modelActionListener`\n+     * is added to the list of listeners to be alerted when the model is fully loaded.\n+     * - Otherwise the model is loaded and cached\n      *\n-     * If it is not present, one of the following occurs:\n+     * In the case of an ingest processor if it is not present, one of the following occurs:\n+     * <p>\n+     * - If the model is referenced by a pipeline and is currently being loaded, the `modelActionListener`\n+     * is added to the list of listeners to be alerted when the model is fully loaded.\n+     * - If the model is referenced by a pipeline and is currently NOT being loaded, a new load attempt is made and the resulting\n+     * model will attempt to be cached for future reference\n+     * - If the models is NOT referenced by a pipeline, the model is simply loaded from the index and given to the listener.\n+     * It is not cached.\n      *\n-     *  - If the model is referenced by a pipeline and is currently being loaded, the `modelActionListener`\n-     *    is added to the list of listeners to be alerted when the model is fully loaded.\n-     *  - If the model is referenced by a pipeline and is currently NOT being loaded, a new load attempt is made and the resulting\n-     *    model will attempt to be cached for future reference\n-     *  - If the models is NOT referenced by a pipeline, the model is simply loaded from the index and given to the listener.\n-     *    It is not cached.\n+     * The main difference being that models for search are always cached whereas pipeline models\n+     * are only cached if they are referenced by an ingest pipeline\n      *\n-     * @param modelId the model to get\n+     * @param modelId             the model to get\n+     * @param consumer            which feature is requesting the model\n      * @param modelActionListener the listener to alert when the model has been retrieved.\n      */\n-    public void getModel(String modelId, ActionListener<Model> modelActionListener) {\n-        LocalModel cachedModel = localModelCache.get(modelId);\n+    private void getModel(String modelId, Consumer consumer, ActionListener<Model> modelActionListener) {\n+        ModelAndConsumer cachedModel = localModelCache.get(modelId);\n         if (cachedModel != null) {\n-            modelActionListener.onResponse(cachedModel);\n+            cachedModel.consumers.add(consumer);\n+            modelActionListener.onResponse(cachedModel.model);\n             logger.trace(() -> new ParameterizedMessage(\"[{}] loaded from cache\", modelId));\n             return;\n         }\n-        if (loadModelIfNecessary(modelId, modelActionListener) == false) {\n-            // If we the model is not loaded and we did not kick off a new loading attempt, this means that we may be getting called\n-            // by a simulated pipeline\n-            logger.trace(() -> new ParameterizedMessage(\"[{}] not actively loading, eager loading without cache\", modelId));\n-            provider.getTrainedModel(modelId, true, ActionListener.wrap(\n-                trainedModelConfig -> {\n-                    trainedModelConfig.ensureParsedDefinition(namedXContentRegistry);\n-                    InferenceConfig inferenceConfig = trainedModelConfig.getInferenceConfig() == null ?\n-                        inferenceConfigFromTargetType(trainedModelConfig.getModelDefinition().getTrainedModel().targetType()) :\n-                        trainedModelConfig.getInferenceConfig();\n-                    modelActionListener.onResponse(new LocalModel(\n-                        trainedModelConfig.getModelId(),\n-                        localNode,\n-                        trainedModelConfig.getModelDefinition(),\n-                        trainedModelConfig.getInput(),\n-                        trainedModelConfig.getDefaultFieldMap(),\n-                        inferenceConfig,\n-                        modelStatsService));\n-                },\n-                modelActionListener::onFailure\n-            ));\n-        } else {\n+\n+        if (loadModelIfNecessary(modelId, consumer, modelActionListener) == false) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "990d5a67db47d1263bfc7719e142e8bfd0e3d8e7"}, "originalPosition": 185}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxMjQ3Njc0OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/loadingservice/ModelLoadingService.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxODoyNjoyNVrOGfShvw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxODoyNjoyNVrOGfShvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ2MjU5MQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        removedModels.forEach(modleId -> {\n          \n          \n            \n                        removedModels.forEach(modelId -> {", "url": "https://github.com/elastic/elasticsearch/pull/57592#discussion_r435462591", "createdAt": "2020-06-04T18:26:25Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/loadingservice/ModelLoadingService.java", "diffHunk": "@@ -313,7 +378,13 @@ public void clusterChanged(ClusterChangedEvent event) {\n             removedModels = Sets.difference(referencedModelsBeforeClusterState, allReferencedModelKeys);\n \n             // Remove all cached models that are not referenced by any processors\n-            removedModels.forEach(localModelCache::invalidate);\n+            // and are not used in search\n+            removedModels.forEach(modleId -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "990d5a67db47d1263bfc7719e142e8bfd0e3d8e7"}, "originalPosition": 334}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxMjQ3NzQ2OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/loadingservice/ModelLoadingService.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxODoyNjozNlrOGfSiLw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxODoyNjozNlrOGfSiLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ2MjcwMw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            ModelAndConsumer modelAndConsumer = localModelCache.get(modleId);\n          \n          \n            \n                            ModelAndConsumer modelAndConsumer = localModelCache.get(modelId);", "url": "https://github.com/elastic/elasticsearch/pull/57592#discussion_r435462703", "createdAt": "2020-06-04T18:26:36Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/loadingservice/ModelLoadingService.java", "diffHunk": "@@ -313,7 +378,13 @@ public void clusterChanged(ClusterChangedEvent event) {\n             removedModels = Sets.difference(referencedModelsBeforeClusterState, allReferencedModelKeys);\n \n             // Remove all cached models that are not referenced by any processors\n-            removedModels.forEach(localModelCache::invalidate);\n+            // and are not used in search\n+            removedModels.forEach(modleId -> {\n+                ModelAndConsumer modelAndConsumer = localModelCache.get(modleId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "990d5a67db47d1263bfc7719e142e8bfd0e3d8e7"}, "originalPosition": 335}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxMjQ3ODAzOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/loadingservice/ModelLoadingService.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxODoyNjo0OFrOGfSilA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxODoyNjo0OFrOGfSilA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ2MjgwNA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                localModelCache.invalidate(modleId);\n          \n          \n            \n                                localModelCache.invalidate(modelId);", "url": "https://github.com/elastic/elasticsearch/pull/57592#discussion_r435462804", "createdAt": "2020-06-04T18:26:48Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/loadingservice/ModelLoadingService.java", "diffHunk": "@@ -313,7 +378,13 @@ public void clusterChanged(ClusterChangedEvent event) {\n             removedModels = Sets.difference(referencedModelsBeforeClusterState, allReferencedModelKeys);\n \n             // Remove all cached models that are not referenced by any processors\n-            removedModels.forEach(localModelCache::invalidate);\n+            // and are not used in search\n+            removedModels.forEach(modleId -> {\n+                ModelAndConsumer modelAndConsumer = localModelCache.get(modleId);\n+                if (modelAndConsumer != null && modelAndConsumer.consumers.contains(Consumer.SEARCH) == false) {\n+                    localModelCache.invalidate(modleId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "990d5a67db47d1263bfc7719e142e8bfd0e3d8e7"}, "originalPosition": 337}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3760, "cost": 1, "resetAt": "2021-11-12T13:16:51Z"}}}