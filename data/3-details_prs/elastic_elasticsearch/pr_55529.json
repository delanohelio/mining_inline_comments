{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDA2NjgxOTQx", "number": 55529, "title": "[ML] Add effective max model memory limit to ML info", "bodyText": "The ML info endpoint returns the max_model_memory_limit setting\nif one is configured.  However, it is still possible to create\na job that cannot run anywhere in the current cluster because\nno node in the cluster has enough memory to accommodate it.\nThis change adds an extra piece of information,\nlimits.effective_max_model_memory_limit, to the ML info\nresponse that returns the biggest model memory limit that could\nbe run in the current cluster assuming no other jobs were\nrunning.\nThe idea is that the ML UI will be able to warn users who try to\ncreate jobs with higher model memory limits that their jobs will\nnot be able to start unless they add a bigger ML node to their\ncluster.\nRelates elastic/kibana#63942", "createdAt": "2020-04-21T13:40:22Z", "url": "https://github.com/elastic/elasticsearch/pull/55529", "merged": true, "mergeCommit": {"oid": "d1a9b3a54529b89d08b32027b65407f4ee9e9a3e"}, "closed": true, "closedAt": "2020-04-22T10:36:59Z", "author": {"login": "droberts195"}, "timelineItems": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcZz1VQAH2gAyNDA2NjgxOTQxOjFkNGRiNjhmODlkNjNiNzE2NGU0ZjRlY2E4NDVmZTFkZTg4ZjIyNzQ=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcaFIDVAH2gAyNDA2NjgxOTQxOjliMTgzZjQzZjUwMWE0NDgwODAwYTRmZDQ0ZmI3MGFmNTQ4NDZlMWU=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "1d4db68f89d63b7164e4f4eca845fe1de88f2274", "author": {"user": {"login": "droberts195", "name": "David Roberts"}}, "url": "https://github.com/elastic/elasticsearch/commit/1d4db68f89d63b7164e4f4eca845fe1de88f2274", "committedDate": "2020-04-21T13:37:36Z", "message": "[ML] Add effective current max model memory limit to ML info\n\nThe ML info endpoint returns the max_model_memory_limit setting\nif one is configured.  However, it is still possible to create\na job that cannot run anywhere in the current cluster because\nno node in the cluster has enough memory to accommodate it.\n\nThis change adds an extra piece of information,\nlimits.current_effective_max_model_memory_limit, to the ML info\nresponse that returns the biggest model memory limit that could\nbe run in the current cluster assuming no other jobs were\nrunning.\n\nThe idea is that the ML UI will be able to warn users who try to\ncreate jobs with higher model memory limits that their jobs will\nnot be able to start unless they add a bigger ML node to their\ncluster.\n\nRelates elastic/kibana#63942"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk3Mzk1ODQx", "url": "https://github.com/elastic/elasticsearch/pull/55529#pullrequestreview-397395841", "createdAt": "2020-04-21T14:36:56Z", "commit": {"oid": "1d4db68f89d63b7164e4f4eca845fe1de88f2274"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQxNDozNjo1N1rOGJJQrg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQxNDozNjo1N1rOGJJQrg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjI0MjA5NA==", "bodyText": "It might be nice to indicate that there is room available for larger jobs if they increased their MAX_MODEL_MEMORY_LIMIT setting.\nBut, in the scenarios where the user could take action, it seems to me that they SHOULD already know the native memory available.", "url": "https://github.com/elastic/elasticsearch/pull/55529#discussion_r412242094", "createdAt": "2020-04-21T14:36:57Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/action/TransportMlInfoAction.java", "diffHunk": "@@ -106,11 +111,50 @@ private ByteSizeValue defaultModelMemoryLimit() {\n         return anomalyDetectorsDefaults;\n     }\n \n+    static ByteSizeValue calculateCurrentEffectiveMaxModelMemoryLimit(int maxMachineMemoryPercent, DiscoveryNodes nodes) {\n+\n+        long maxMlMemory = -1;\n+\n+        for (DiscoveryNode node : nodes) {\n+\n+            Map<String, String> nodeAttributes = node.getAttributes();\n+            String machineMemoryStr = nodeAttributes.get(MachineLearning.MACHINE_MEMORY_NODE_ATTR);\n+            if (machineMemoryStr == null) {\n+                continue;\n+            }\n+            long machineMemory;\n+            try {\n+                machineMemory = Long.parseLong(machineMemoryStr);\n+            } catch (NumberFormatException e) {\n+                continue;\n+            }\n+            maxMlMemory = Math.max(maxMlMemory, machineMemory * maxMachineMemoryPercent / 100);\n+        }\n+\n+        if (maxMlMemory <= 0) {\n+            // This implies there are currently no ML nodes in the cluster, so we\n+            // have no idea what the effective limit would be if one were added\n+            return null;\n+        }\n+\n+        maxMlMemory -= Math.max(Job.PROCESS_MEMORY_OVERHEAD.getBytes(), DataFrameAnalyticsConfig.PROCESS_MEMORY_OVERHEAD.getBytes());\n+        maxMlMemory -= MachineLearning.NATIVE_EXECUTABLE_CODE_OVERHEAD.getBytes();\n+        return new ByteSizeValue(Math.max(0L, maxMlMemory) / 1024 / 1024, ByteSizeUnit.MB);\n+    }\n+\n     private Map<String, Object> limits() {\n         Map<String, Object> limits = new HashMap<>();\n+        ByteSizeValue currentEffectiveMaxModelMemoryLimit = calculateCurrentEffectiveMaxModelMemoryLimit(\n+            clusterService.getClusterSettings().get(MachineLearning.MAX_MACHINE_MEMORY_PERCENT), clusterService.state().getNodes());\n         ByteSizeValue maxModelMemoryLimit = clusterService.getClusterSettings().get(MachineLearningField.MAX_MODEL_MEMORY_LIMIT);\n         if (maxModelMemoryLimit != null && maxModelMemoryLimit.getBytes() > 0) {\n-            limits.put(\"max_model_memory_limit\", maxModelMemoryLimit);\n+            limits.put(\"max_model_memory_limit\", maxModelMemoryLimit.getStringRep());\n+            if (currentEffectiveMaxModelMemoryLimit == null || currentEffectiveMaxModelMemoryLimit.compareTo(maxModelMemoryLimit) > 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d4db68f89d63b7164e4f4eca845fe1de88f2274"}, "originalPosition": 65}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5bfc7d41e8620077c4f12d6f82729cde0265d189", "author": {"user": {"login": "droberts195", "name": "David Roberts"}}, "url": "https://github.com/elastic/elasticsearch/commit/5bfc7d41e8620077c4f12d6f82729cde0265d189", "committedDate": "2020-04-21T15:56:28Z", "message": "Fix docs test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3c08ac562d6ac45447126534562c1351ca066279", "author": {"user": {"login": "droberts195", "name": "David Roberts"}}, "url": "https://github.com/elastic/elasticsearch/commit/3c08ac562d6ac45447126534562c1351ca066279", "committedDate": "2020-04-21T16:33:29Z", "message": "Fix checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9b183f43f501a4480800a4fd44fb70af54846e1e", "author": {"user": {"login": "droberts195", "name": "David Roberts"}}, "url": "https://github.com/elastic/elasticsearch/commit/9b183f43f501a4480800a4fd44fb70af54846e1e", "committedDate": "2020-04-22T09:46:26Z", "message": "current_effective -> effective\n\nWe decided that using two words was overly verbose"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 625, "cost": 1, "resetAt": "2021-10-28T18:54:27Z"}}}