{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ2OTM1NDI0", "number": 59309, "title": "[ML] Autoscaling for machine learning", "bodyText": "This provides an autoscaling service integration for machine learning.\nThe underlying logic is fairly straightforward with a couple of caveats:\n\nWhen considering to scale up/down, ML will automatically translate between Node size and the memory that the node will potentially provide for ML after the scaling plan is implemented.\nIf knowledge of job sizes is out of date, we will do a best effort check for scaling up. But, if that cannot be determined with our current view of job memory, we attempt a refresh and return a no_scale event\nWe assume that if the auto memory percent calculation is being used, we treat all JVM sizes on the nodes the same.\nFor scale down, we keep our last scale down calculation time in memory. So, if master nodes are changed in between, we reset the scale down delay.", "createdAt": "2020-07-09T15:30:07Z", "url": "https://github.com/elastic/elasticsearch/pull/59309", "merged": true, "mergeCommit": {"oid": "ebd099689e59fe933fe4cfb9b5f872f05b763aea"}, "closed": true, "closedAt": "2020-11-17T15:54:12Z", "author": {"login": "benwtrent"}, "timelineItems": {"totalCount": 41, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdCWEhWABqjM2ODk3NjYxODA=", "endCursor": "Y3Vyc29yOnYyOpPPAAABddameqAH2gAyNDQ2OTM1NDI0OjI5YWRiMTdlYzI1ZWE4N2YzNDljZmI3NTY0MTJhMDU4OTkwYjY4ZjA=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "aecf1cb8b07bf25c27f8a5e7a8d069f00d12bdfb", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/aecf1cb8b07bf25c27f8a5e7a8d069f00d12bdfb", "committedDate": "2020-07-09T15:28:34Z", "message": "[ML] Adding ML autoscaling decider integration"}, "afterCommit": {"oid": "c4e1f9acb86b50fbe9dc0b4154249e4aa0bf4adb", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/c4e1f9acb86b50fbe9dc0b4154249e4aa0bf4adb", "committedDate": "2020-08-25T12:06:16Z", "message": "[ML] Adding ML autoscaling decider integration"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c4e1f9acb86b50fbe9dc0b4154249e4aa0bf4adb", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/c4e1f9acb86b50fbe9dc0b4154249e4aa0bf4adb", "committedDate": "2020-08-25T12:06:16Z", "message": "[ML] Adding ML autoscaling decider integration"}, "afterCommit": {"oid": "e5f75e57a799454d623f5ebc1a3acbca56388c1a", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/e5f75e57a799454d623f5ebc1a3acbca56388c1a", "committedDate": "2020-08-25T12:21:24Z", "message": "[ML] Adding ML autoscaling decider integration"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4ceb88163e9ed7759ccdc6aeedf2a147d185f18f", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/4ceb88163e9ed7759ccdc6aeedf2a147d185f18f", "committedDate": "2020-08-27T18:20:54Z", "message": "[ML] Adding ML autoscaling decider integration"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e5f75e57a799454d623f5ebc1a3acbca56388c1a", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/e5f75e57a799454d623f5ebc1a3acbca56388c1a", "committedDate": "2020-08-25T12:21:24Z", "message": "[ML] Adding ML autoscaling decider integration"}, "afterCommit": {"oid": "4ceb88163e9ed7759ccdc6aeedf2a147d185f18f", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/4ceb88163e9ed7759ccdc6aeedf2a147d185f18f", "committedDate": "2020-08-27T18:20:54Z", "message": "[ML] Adding ML autoscaling decider integration"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgxNzY2MDQz", "url": "https://github.com/elastic/elasticsearch/pull/59309#pullrequestreview-481766043", "createdAt": "2020-09-03T11:58:49Z", "commit": {"oid": "4ceb88163e9ed7759ccdc6aeedf2a147d185f18f"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wM1QxMTo1ODo0OVrOHMjQpw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wM1QxMzowODo0MVrOHMlxVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjkyMjY2Mw==", "bodyText": "I don't see why this is being changed.  The default model memory limit is 1GB, but if a user wants to set 2GB for one of their jobs then we shouldn't out-of-the-box stop them doing that.", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r482922663", "createdAt": "2020-09-03T11:58:49Z", "author": {"login": "droberts195"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ml/MachineLearningField.java", "diffHunk": "@@ -21,8 +23,9 @@\n     public static final Setting<Boolean> AUTODETECT_PROCESS =\n             Setting.boolSetting(\"xpack.ml.autodetect_process\", true, Setting.Property.NodeScope);\n     public static final Setting<ByteSizeValue> MAX_MODEL_MEMORY_LIMIT =\n-            Setting.memorySizeSetting(\"xpack.ml.max_model_memory_limit\", ByteSizeValue.ZERO,\n-                    Setting.Property.Dynamic, Setting.Property.NodeScope);\n+            Setting.memorySizeSetting(\"xpack.ml.max_model_memory_limit\",\n+                new ByteSizeValue(AnalysisLimits.DEFAULT_MODEL_MEMORY_LIMIT_MB, ByteSizeUnit.MB),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4ceb88163e9ed7759ccdc6aeedf2a147d185f18f"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjkyNDQ1OQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                private static final TimeValue DEFAULT_ANALYSIS_JOB_TIME_IN_QUEUE = TimeValue.timeValueMinutes(5);\n          \n          \n            \n                private static final TimeValue DEFAULT_ANALYTICS_JOB_TIME_IN_QUEUE = TimeValue.timeValueMinutes(5);", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r482924459", "createdAt": "2020-09-03T12:02:06Z", "author": {"login": "droberts195"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/autoscaling/MlAutoscalingDeciderConfiguration.java", "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.autoscaling;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.common.xcontent.ObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+import org.elasticsearch.xpack.autoscaling.decision.AutoscalingDeciderConfiguration;\n+import org.elasticsearch.xpack.core.ml.utils.ExceptionsHelper;\n+\n+import java.io.IOException;\n+import java.util.Objects;\n+\n+public class MlAutoscalingDeciderConfiguration implements AutoscalingDeciderConfiguration {\n+    private static final String NAME = \"ml\";\n+\n+    private static final int DEFAULT_MIN_NUM_NODES = 1;\n+\n+    private static final TimeValue DEFAULT_ANOMALY_JOB_TIME_IN_QUEUE = TimeValue.ZERO;\n+    private static final TimeValue DEFAULT_ANALYSIS_JOB_TIME_IN_QUEUE = TimeValue.timeValueMinutes(5);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4ceb88163e9ed7759ccdc6aeedf2a147d185f18f"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjkyNDYxOQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                private static final ParseField ANALYSIS_JOB_TIME_IN_QUEUE = new ParseField(\"analysis_job_time_in_queue\");\n          \n          \n            \n                private static final ParseField ANALYTICS_JOB_TIME_IN_QUEUE = new ParseField(\"analytics_job_time_in_queue\");", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r482924619", "createdAt": "2020-09-03T12:02:23Z", "author": {"login": "droberts195"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/autoscaling/MlAutoscalingDeciderConfiguration.java", "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.autoscaling;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.common.xcontent.ObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+import org.elasticsearch.xpack.autoscaling.decision.AutoscalingDeciderConfiguration;\n+import org.elasticsearch.xpack.core.ml.utils.ExceptionsHelper;\n+\n+import java.io.IOException;\n+import java.util.Objects;\n+\n+public class MlAutoscalingDeciderConfiguration implements AutoscalingDeciderConfiguration {\n+    private static final String NAME = \"ml\";\n+\n+    private static final int DEFAULT_MIN_NUM_NODES = 1;\n+\n+    private static final TimeValue DEFAULT_ANOMALY_JOB_TIME_IN_QUEUE = TimeValue.ZERO;\n+    private static final TimeValue DEFAULT_ANALYSIS_JOB_TIME_IN_QUEUE = TimeValue.timeValueMinutes(5);\n+\n+    private static final ParseField MIN_NUM_NODES = new ParseField(\"min_num_nodes\");\n+    private static final ParseField ANOMALY_JOB_TIME_IN_QUEUE = new ParseField(\"anomaly_job_time_in_queue\");\n+    private static final ParseField ANALYSIS_JOB_TIME_IN_QUEUE = new ParseField(\"analysis_job_time_in_queue\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4ceb88163e9ed7759ccdc6aeedf2a147d185f18f"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjk0NzU0Nw==", "bodyText": "I think maxJobMemoryBytes here should really be max_machine_memory_percent * biggestPossibleMlNodeSize rather than max_model_memory_limit.  In Cloud the two should be approximately the same, but in self managed they won't be.  Yet this logic is being used for both Cloud and self managed.  Obviously in self-managed we don't know biggestPossibleMlNodeSize if we think the user might add more nodes, so we'll have to assume they won't.\nOverall I would recommend:\n\nRevert the addition of a default value for max_model_memory_limit, so that it stays with the default for on-prem being unlimited and the default for Cloud being set by Cloud.\nAssume that if autoscaling is being used then max_model_memory_limit has been set sensibly by whatever is orchestrating the autoscaling.  Make it an error for autoscaling to be used if max_model_memory_limit is unlimited.\nIf autoscaling is being used then maxJobMemoryBytes can be set equal to max_model_memory_limit as it is currently.\nIf autoscaling is not being used then maxJobMemoryBytes can be calculated from the cluster state by taking the max of max_machine_memory_percent * ml.machine_memory over all nodes that have a value for ml.machine_memory in their node attributes.\n\nThis implies we need a way to tell if autoscaling is being used without waiting to see if the autoscaling APIs are called.  If there currently isn't a way then we need to raise this at the next meeting.\nWhat is going on here also needs to be commented in the code, as there is now a very complex yet subtle interaction with the Cloud orchestration code that future readers of the code won't understand.", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r482947547", "createdAt": "2020-09-03T12:42:41Z", "author": {"login": "droberts195"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/action/TransportOpenJobAction.java", "diffHunk": "@@ -438,7 +443,7 @@ public OpenJobPersistentTasksExecutor(Settings settings, ClusterService clusterS\n             JobNodeSelector jobNodeSelector = new JobNodeSelector(clusterState, jobId, MlTasks.JOB_TASK_NAME, memoryTracker,\n                 job.allowLazyOpen() ? Integer.MAX_VALUE : maxLazyMLNodes, node -> nodeFilter(node, job));\n             return jobNodeSelector.selectNode(\n-                maxOpenJobs, maxConcurrentJobAllocations, maxMachineMemoryPercent, isMemoryTrackerRecentlyRefreshed);\n+                maxOpenJobs, maxConcurrentJobAllocations, maxMachineMemoryPercent, maxJobMemoryBytes, isMemoryTrackerRecentlyRefreshed);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4ceb88163e9ed7759ccdc6aeedf2a147d185f18f"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjk1MTMyOA==", "bodyText": "I wonder if we could make a rule that autoscaling cannot be used in a mixed version cluster where some of the nodes are on a version that doesn't implement autoscaling?  If we can then this can be hardcoded to true even now.", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r482951328", "createdAt": "2020-09-03T12:48:59Z", "author": {"login": "droberts195"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/autoscaling/MlAutoscalingDeciderService.java", "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.ml.autoscaling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.LocalNodeMasterListener;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.component.LifecycleListener;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.persistent.PersistentTasksCustomMetadata;\n+import org.elasticsearch.persistent.PersistentTasksCustomMetadata.PersistentTask;\n+import org.elasticsearch.xpack.autoscaling.decision.AutoscalingDeciderContext;\n+import org.elasticsearch.xpack.autoscaling.decision.AutoscalingDeciderService;\n+import org.elasticsearch.xpack.autoscaling.decision.AutoscalingDecision;\n+import org.elasticsearch.xpack.autoscaling.decision.AutoscalingDecisionType;\n+import org.elasticsearch.xpack.core.ml.MlTasks;\n+import org.elasticsearch.xpack.core.ml.dataframe.DataFrameAnalyticsState;\n+import org.elasticsearch.xpack.core.ml.job.config.JobState;\n+import org.elasticsearch.xpack.ml.MachineLearning;\n+import org.elasticsearch.xpack.ml.job.NodeLoadDetector;\n+import org.elasticsearch.xpack.ml.process.MlMemoryTracker;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+\n+import static org.elasticsearch.xpack.core.ml.MlTasks.getDataFrameAnalyticsState;\n+import static org.elasticsearch.xpack.core.ml.MlTasks.getJobStateModifiedForReassignments;\n+import static org.elasticsearch.xpack.ml.job.JobNodeSelector.AWAITING_LAZY_ASSIGNMENT;\n+\n+public class MlAutoscalingDeciderService implements\n+    AutoscalingDeciderService<MlAutoscalingDeciderConfiguration>,\n+    LocalNodeMasterListener {\n+\n+    private static final Logger logger = LogManager.getLogger(MlAutoscalingDeciderService.class);\n+\n+    private final NodeLoadDetector nodeLoadDetector;\n+    private final Map<String, Long> anomalyJobsTimeInQueue;\n+    private final Map<String, Long> analyticsJobsTimeInQueue;\n+    private final Supplier<Long> timeSupplier;\n+\n+    private volatile boolean isMaster;\n+    private volatile boolean running;\n+    private volatile int maxMachineMemoryPercent;\n+    private volatile int maxOpenJobs;\n+    private volatile long lastTimeToScale;\n+\n+    public MlAutoscalingDeciderService(MlMemoryTracker memoryTracker, Settings settings, ClusterService clusterService) {\n+        this(new NodeLoadDetector(memoryTracker), settings, clusterService, System::currentTimeMillis);\n+    }\n+\n+    MlAutoscalingDeciderService(NodeLoadDetector nodeLoadDetector,\n+                                Settings settings,\n+                                ClusterService clusterService,\n+                                Supplier<Long> timeSupplier) {\n+        this.nodeLoadDetector = nodeLoadDetector;\n+        this.maxMachineMemoryPercent = MachineLearning.MAX_MACHINE_MEMORY_PERCENT.get(settings);\n+        this.maxOpenJobs = MachineLearning.MAX_OPEN_JOBS_PER_NODE.get(settings);\n+        this.analyticsJobsTimeInQueue = new ConcurrentHashMap<>();\n+        this.anomalyJobsTimeInQueue = new ConcurrentHashMap<>();\n+        this.timeSupplier = timeSupplier;\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MachineLearning.MAX_MACHINE_MEMORY_PERCENT,\n+            this::setMaxMachineMemoryPercent);\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MachineLearning.MAX_OPEN_JOBS_PER_NODE, this::setMaxOpenJobs);\n+        clusterService.addLocalNodeMasterListener(this);\n+        clusterService.addLifecycleListener(new LifecycleListener() {\n+            @Override\n+            public void afterStart() {\n+                running = true;\n+                if (isMaster) {\n+                    nodeLoadDetector.getMlMemoryTracker().asyncRefresh();\n+                }\n+            }\n+\n+            @Override\n+            public void beforeStop() {\n+                running = false;\n+            }\n+        });\n+    }\n+\n+    void setMaxMachineMemoryPercent(int maxMachineMemoryPercent) {\n+        this.maxMachineMemoryPercent = maxMachineMemoryPercent;\n+    }\n+\n+    void setMaxOpenJobs(int maxOpenJobs) {\n+        this.maxOpenJobs = maxOpenJobs;\n+    }\n+\n+    @Override\n+    public void onMaster() {\n+        isMaster = true;\n+        if (running) {\n+            nodeLoadDetector.getMlMemoryTracker().asyncRefresh();\n+        }\n+    }\n+\n+    @Override\n+    public void offMaster() {\n+        isMaster = false;\n+    }\n+\n+    @Override\n+    public AutoscalingDecision scale(MlAutoscalingDeciderConfiguration decider, AutoscalingDeciderContext context) {\n+        if (isMaster == false) {\n+            throw new IllegalArgumentException(\"request for scaling information is only allowed on the master node\");\n+        }\n+        long previousTimeStamp = this.lastTimeToScale;\n+        this.lastTimeToScale = this.timeSupplier.get();\n+        if (previousTimeStamp == 0L) {\n+            previousTimeStamp = lastTimeToScale;\n+        }\n+        final long timeDiff = Math.max(0L, this.lastTimeToScale - previousTimeStamp);\n+\n+        final ClusterState clusterState = context.state();\n+\n+        PersistentTasksCustomMetadata tasks = clusterState.getMetadata().custom(PersistentTasksCustomMetadata.TYPE);\n+        Collection<PersistentTask<?>> anomalyDetectionTasks = anomalyDetectionTasks(tasks);\n+        Collection<PersistentTask<?>> dataframeAnalyticsTasks = dataframeAnalyticsTasks(tasks);\n+        List<DiscoveryNode> nodes = clusterState.nodes()\n+            .mastersFirstStream()\n+            .filter(MachineLearning::isMlNode)\n+            .collect(Collectors.toList());\n+\n+        final AutoscalingDecision scaleUpDecision = checkForScaleUp(decider,\n+            nodes,\n+            anomalyDetectionTasks,\n+            dataframeAnalyticsTasks,\n+            timeDiff);\n+        if (AutoscalingDecisionType.SCALE_UP.equals(scaleUpDecision.type())) {\n+            return scaleUpDecision;\n+        }\n+\n+        final AutoscalingDecision scaleDownDecision = checkForScaleDown(decider, nodes, clusterState);\n+        if (AutoscalingDecisionType.SCALE_DOWN.equals(scaleDownDecision.type())) {\n+            return scaleDownDecision;\n+        }\n+\n+        return new AutoscalingDecision(name(),\n+            AutoscalingDecisionType.NO_SCALE,\n+            scaleUpDecision.reason() + \"|\" + scaleDownDecision.reason());\n+    }\n+\n+    AutoscalingDecision checkForScaleUp(MlAutoscalingDeciderConfiguration decider,\n+                                        List<DiscoveryNode> nodes,\n+                                        Collection<PersistentTask<?>> anomalyDetectionTasks,\n+                                        Collection<PersistentTask<?>> dataframeAnalyticsTasks,\n+                                        long timeSinceLastCheckMs) {\n+        Set<String> waitingAnomalyJobs = anomalyDetectionTasks.stream()\n+            .filter(t -> AWAITING_LAZY_ASSIGNMENT.equals(t.getAssignment()))\n+            .map(t -> MlTasks.jobId(t.getId()))\n+            .collect(Collectors.toSet());\n+        Set<String> waitingAnalysisJobs = dataframeAnalyticsTasks.stream()\n+            .filter(t -> AWAITING_LAZY_ASSIGNMENT.equals(t.getAssignment()))\n+            .map(t -> MlTasks.dataFrameAnalyticsId(t.getId()))\n+            .collect(Collectors.toSet());\n+        anomalyJobsTimeInQueue.keySet().retainAll(waitingAnomalyJobs);\n+        analyticsJobsTimeInQueue.keySet().retainAll(waitingAnalysisJobs);\n+\n+        if (waitingAnomalyJobs.isEmpty() == false || waitingAnalysisJobs.isEmpty() == false || nodes.size() < decider.getMinNumNodes()) {\n+            if (nodes.size() < decider.getMinNumNodes() || nodes.isEmpty()) {\n+                return new AutoscalingDecision(name(),\n+                    AutoscalingDecisionType.SCALE_UP,\n+                    \"number of machine learning nodes [\"\n+                        + nodes.size()\n+                        + \"] is below the configured minimum number of [\"\n+                        + decider.getMinNumNodes()\n+                        + \"] or is zero\");\n+            }\n+            Set<String> timedUpJobs = new HashSet<>();\n+            for (String jobId : waitingAnomalyJobs) {\n+                long time = anomalyJobsTimeInQueue.compute(jobId, (k, v) -> v == null ? 0L : v + timeSinceLastCheckMs);\n+                if (time >= decider.getAnomalyJobTimeInQueue().getMillis()) {\n+                    timedUpJobs.add(jobId);\n+                }\n+            }\n+            for (String jobId : waitingAnalysisJobs) {\n+                long time = analyticsJobsTimeInQueue.compute(jobId, (k, v) -> v == null ? 0L : v + timeSinceLastCheckMs);\n+                if (time >= decider.getAnalysisJobTimeInQueue().getMillis()) {\n+                    timedUpJobs.add(jobId);\n+                }\n+            }\n+            if (timedUpJobs.isEmpty() == false) {\n+                return new AutoscalingDecision(name(),\n+                    AutoscalingDecisionType.SCALE_UP,\n+                    \"jobs \" + timedUpJobs + \" have been waiting for assignment too long\");\n+            }\n+        }\n+        return new AutoscalingDecision(name(), AutoscalingDecisionType.NO_SCALE, \"no jobs have waited long enough for assignment\");\n+    }\n+\n+    AutoscalingDecision checkForScaleDown(MlAutoscalingDeciderConfiguration decider,\n+                                          List<DiscoveryNode> nodes,\n+                                          ClusterState clusterState) {\n+        if (nodes.size() == decider.getMinNumNodes()) {\n+            return new AutoscalingDecision(name(),\n+                AutoscalingDecisionType.NO_SCALE,\n+                \"| already at configured minimum node count [\"\n+                    + decider.getMinNumNodes()\n+                    +\"]\");\n+        }\n+        // kick of a refresh if our data around memory usage is stale.\n+        if (nodeLoadDetector.getMlMemoryTracker().isRecentlyRefreshed() == false) {\n+            logger.debug(\"job memory tracker is stale. Request refresh before making scaling decision.\");\n+            nodeLoadDetector.getMlMemoryTracker().asyncRefresh();\n+            return new AutoscalingDecision(\n+                name(),\n+                AutoscalingDecisionType.NO_SCALE,\n+                \"job memory tracker is stale. Unable to make accurate scale down recommendation\");\n+        }\n+        // TODO: remove in 8.0.0\n+        boolean allNodesHaveDynamicMaxWorkers = clusterState.getNodes().getMinNodeVersion().onOrAfter(Version.V_7_2_0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4ceb88163e9ed7759ccdc6aeedf2a147d185f18f"}, "originalPosition": 226}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjk1OTM2Ng==", "bodyText": "One nasty thing is that in Cloud the value of max_machine_memory_percent will change as the nodes are scaled: https://github.com/elastic/cloud/blob/65712c90f8b1bc67df949c7b711d5aff5e1f43ea/scala-services/constructor/src/main/scala/no/found/constructor/steps/ml/OptimizeMlClusterSettings.scala#L191-L194\nAs the ML nodes get bigger the JVM occupies a smaller proportion of the node, so a bigger proportion is available for ML native processes.  So as the nodes get bigger max_machine_memory_percent increases.  This is potentially suboptimal but not a disaster when scaling up - we might scale up one step more than necessary.  But after scaling down it might mean the jobs don't fit in the permitted memory. \ud83d\ude31\nIdeally we would do the calculation with the value of max_machine_memory_percent that would apply at the size we were scaling to, both when scaling up and down.  But this requires that we know how max_machine_memory_percent changes as the node size changes.  So, either we have to duplicate the Cloud formula in ES code (and assume only Cloud will use autoscaling), or add a setting that is an alternative way to specify max_machine_memory as an arbitrary function of node size and JVM heap size instead of as a percentage of node size.  That would actually make the Cloud code simpler in the future, as they could then just put their formula in a static setting instead of having to dynamically change the value for different node sizes.", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r482959366", "createdAt": "2020-09-03T13:01:32Z", "author": {"login": "droberts195"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/autoscaling/MlAutoscalingDeciderService.java", "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.ml.autoscaling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.LocalNodeMasterListener;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.component.LifecycleListener;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.persistent.PersistentTasksCustomMetadata;\n+import org.elasticsearch.persistent.PersistentTasksCustomMetadata.PersistentTask;\n+import org.elasticsearch.xpack.autoscaling.decision.AutoscalingDeciderContext;\n+import org.elasticsearch.xpack.autoscaling.decision.AutoscalingDeciderService;\n+import org.elasticsearch.xpack.autoscaling.decision.AutoscalingDecision;\n+import org.elasticsearch.xpack.autoscaling.decision.AutoscalingDecisionType;\n+import org.elasticsearch.xpack.core.ml.MlTasks;\n+import org.elasticsearch.xpack.core.ml.dataframe.DataFrameAnalyticsState;\n+import org.elasticsearch.xpack.core.ml.job.config.JobState;\n+import org.elasticsearch.xpack.ml.MachineLearning;\n+import org.elasticsearch.xpack.ml.job.NodeLoadDetector;\n+import org.elasticsearch.xpack.ml.process.MlMemoryTracker;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+\n+import static org.elasticsearch.xpack.core.ml.MlTasks.getDataFrameAnalyticsState;\n+import static org.elasticsearch.xpack.core.ml.MlTasks.getJobStateModifiedForReassignments;\n+import static org.elasticsearch.xpack.ml.job.JobNodeSelector.AWAITING_LAZY_ASSIGNMENT;\n+\n+public class MlAutoscalingDeciderService implements\n+    AutoscalingDeciderService<MlAutoscalingDeciderConfiguration>,\n+    LocalNodeMasterListener {\n+\n+    private static final Logger logger = LogManager.getLogger(MlAutoscalingDeciderService.class);\n+\n+    private final NodeLoadDetector nodeLoadDetector;\n+    private final Map<String, Long> anomalyJobsTimeInQueue;\n+    private final Map<String, Long> analyticsJobsTimeInQueue;\n+    private final Supplier<Long> timeSupplier;\n+\n+    private volatile boolean isMaster;\n+    private volatile boolean running;\n+    private volatile int maxMachineMemoryPercent;\n+    private volatile int maxOpenJobs;\n+    private volatile long lastTimeToScale;\n+\n+    public MlAutoscalingDeciderService(MlMemoryTracker memoryTracker, Settings settings, ClusterService clusterService) {\n+        this(new NodeLoadDetector(memoryTracker), settings, clusterService, System::currentTimeMillis);\n+    }\n+\n+    MlAutoscalingDeciderService(NodeLoadDetector nodeLoadDetector,\n+                                Settings settings,\n+                                ClusterService clusterService,\n+                                Supplier<Long> timeSupplier) {\n+        this.nodeLoadDetector = nodeLoadDetector;\n+        this.maxMachineMemoryPercent = MachineLearning.MAX_MACHINE_MEMORY_PERCENT.get(settings);\n+        this.maxOpenJobs = MachineLearning.MAX_OPEN_JOBS_PER_NODE.get(settings);\n+        this.analyticsJobsTimeInQueue = new ConcurrentHashMap<>();\n+        this.anomalyJobsTimeInQueue = new ConcurrentHashMap<>();\n+        this.timeSupplier = timeSupplier;\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MachineLearning.MAX_MACHINE_MEMORY_PERCENT,\n+            this::setMaxMachineMemoryPercent);\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MachineLearning.MAX_OPEN_JOBS_PER_NODE, this::setMaxOpenJobs);\n+        clusterService.addLocalNodeMasterListener(this);\n+        clusterService.addLifecycleListener(new LifecycleListener() {\n+            @Override\n+            public void afterStart() {\n+                running = true;\n+                if (isMaster) {\n+                    nodeLoadDetector.getMlMemoryTracker().asyncRefresh();\n+                }\n+            }\n+\n+            @Override\n+            public void beforeStop() {\n+                running = false;\n+            }\n+        });\n+    }\n+\n+    void setMaxMachineMemoryPercent(int maxMachineMemoryPercent) {\n+        this.maxMachineMemoryPercent = maxMachineMemoryPercent;\n+    }\n+\n+    void setMaxOpenJobs(int maxOpenJobs) {\n+        this.maxOpenJobs = maxOpenJobs;\n+    }\n+\n+    @Override\n+    public void onMaster() {\n+        isMaster = true;\n+        if (running) {\n+            nodeLoadDetector.getMlMemoryTracker().asyncRefresh();\n+        }\n+    }\n+\n+    @Override\n+    public void offMaster() {\n+        isMaster = false;\n+    }\n+\n+    @Override\n+    public AutoscalingDecision scale(MlAutoscalingDeciderConfiguration decider, AutoscalingDeciderContext context) {\n+        if (isMaster == false) {\n+            throw new IllegalArgumentException(\"request for scaling information is only allowed on the master node\");\n+        }\n+        long previousTimeStamp = this.lastTimeToScale;\n+        this.lastTimeToScale = this.timeSupplier.get();\n+        if (previousTimeStamp == 0L) {\n+            previousTimeStamp = lastTimeToScale;\n+        }\n+        final long timeDiff = Math.max(0L, this.lastTimeToScale - previousTimeStamp);\n+\n+        final ClusterState clusterState = context.state();\n+\n+        PersistentTasksCustomMetadata tasks = clusterState.getMetadata().custom(PersistentTasksCustomMetadata.TYPE);\n+        Collection<PersistentTask<?>> anomalyDetectionTasks = anomalyDetectionTasks(tasks);\n+        Collection<PersistentTask<?>> dataframeAnalyticsTasks = dataframeAnalyticsTasks(tasks);\n+        List<DiscoveryNode> nodes = clusterState.nodes()\n+            .mastersFirstStream()\n+            .filter(MachineLearning::isMlNode)\n+            .collect(Collectors.toList());\n+\n+        final AutoscalingDecision scaleUpDecision = checkForScaleUp(decider,\n+            nodes,\n+            anomalyDetectionTasks,\n+            dataframeAnalyticsTasks,\n+            timeDiff);\n+        if (AutoscalingDecisionType.SCALE_UP.equals(scaleUpDecision.type())) {\n+            return scaleUpDecision;\n+        }\n+\n+        final AutoscalingDecision scaleDownDecision = checkForScaleDown(decider, nodes, clusterState);\n+        if (AutoscalingDecisionType.SCALE_DOWN.equals(scaleDownDecision.type())) {\n+            return scaleDownDecision;\n+        }\n+\n+        return new AutoscalingDecision(name(),\n+            AutoscalingDecisionType.NO_SCALE,\n+            scaleUpDecision.reason() + \"|\" + scaleDownDecision.reason());\n+    }\n+\n+    AutoscalingDecision checkForScaleUp(MlAutoscalingDeciderConfiguration decider,\n+                                        List<DiscoveryNode> nodes,\n+                                        Collection<PersistentTask<?>> anomalyDetectionTasks,\n+                                        Collection<PersistentTask<?>> dataframeAnalyticsTasks,\n+                                        long timeSinceLastCheckMs) {\n+        Set<String> waitingAnomalyJobs = anomalyDetectionTasks.stream()\n+            .filter(t -> AWAITING_LAZY_ASSIGNMENT.equals(t.getAssignment()))\n+            .map(t -> MlTasks.jobId(t.getId()))\n+            .collect(Collectors.toSet());\n+        Set<String> waitingAnalysisJobs = dataframeAnalyticsTasks.stream()\n+            .filter(t -> AWAITING_LAZY_ASSIGNMENT.equals(t.getAssignment()))\n+            .map(t -> MlTasks.dataFrameAnalyticsId(t.getId()))\n+            .collect(Collectors.toSet());\n+        anomalyJobsTimeInQueue.keySet().retainAll(waitingAnomalyJobs);\n+        analyticsJobsTimeInQueue.keySet().retainAll(waitingAnalysisJobs);\n+\n+        if (waitingAnomalyJobs.isEmpty() == false || waitingAnalysisJobs.isEmpty() == false || nodes.size() < decider.getMinNumNodes()) {\n+            if (nodes.size() < decider.getMinNumNodes() || nodes.isEmpty()) {\n+                return new AutoscalingDecision(name(),\n+                    AutoscalingDecisionType.SCALE_UP,\n+                    \"number of machine learning nodes [\"\n+                        + nodes.size()\n+                        + \"] is below the configured minimum number of [\"\n+                        + decider.getMinNumNodes()\n+                        + \"] or is zero\");\n+            }\n+            Set<String> timedUpJobs = new HashSet<>();\n+            for (String jobId : waitingAnomalyJobs) {\n+                long time = anomalyJobsTimeInQueue.compute(jobId, (k, v) -> v == null ? 0L : v + timeSinceLastCheckMs);\n+                if (time >= decider.getAnomalyJobTimeInQueue().getMillis()) {\n+                    timedUpJobs.add(jobId);\n+                }\n+            }\n+            for (String jobId : waitingAnalysisJobs) {\n+                long time = analyticsJobsTimeInQueue.compute(jobId, (k, v) -> v == null ? 0L : v + timeSinceLastCheckMs);\n+                if (time >= decider.getAnalysisJobTimeInQueue().getMillis()) {\n+                    timedUpJobs.add(jobId);\n+                }\n+            }\n+            if (timedUpJobs.isEmpty() == false) {\n+                return new AutoscalingDecision(name(),\n+                    AutoscalingDecisionType.SCALE_UP,\n+                    \"jobs \" + timedUpJobs + \" have been waiting for assignment too long\");\n+            }\n+        }\n+        return new AutoscalingDecision(name(), AutoscalingDecisionType.NO_SCALE, \"no jobs have waited long enough for assignment\");\n+    }\n+\n+    AutoscalingDecision checkForScaleDown(MlAutoscalingDeciderConfiguration decider,\n+                                          List<DiscoveryNode> nodes,\n+                                          ClusterState clusterState) {\n+        if (nodes.size() == decider.getMinNumNodes()) {\n+            return new AutoscalingDecision(name(),\n+                AutoscalingDecisionType.NO_SCALE,\n+                \"| already at configured minimum node count [\"\n+                    + decider.getMinNumNodes()\n+                    +\"]\");\n+        }\n+        // kick of a refresh if our data around memory usage is stale.\n+        if (nodeLoadDetector.getMlMemoryTracker().isRecentlyRefreshed() == false) {\n+            logger.debug(\"job memory tracker is stale. Request refresh before making scaling decision.\");\n+            nodeLoadDetector.getMlMemoryTracker().asyncRefresh();\n+            return new AutoscalingDecision(\n+                name(),\n+                AutoscalingDecisionType.NO_SCALE,\n+                \"job memory tracker is stale. Unable to make accurate scale down recommendation\");\n+        }\n+        // TODO: remove in 8.0.0\n+        boolean allNodesHaveDynamicMaxWorkers = clusterState.getNodes().getMinNodeVersion().onOrAfter(Version.V_7_2_0);\n+        List<NodeLoadDetector.NodeLoad> nodeLoads = new ArrayList<>();\n+        boolean isMemoryAccurateFlag = true;\n+        for (DiscoveryNode node : nodes) {\n+            NodeLoadDetector.NodeLoad nodeLoad = nodeLoadDetector.detectNodeLoad(clusterState,\n+                allNodesHaveDynamicMaxWorkers,\n+                node,\n+                maxOpenJobs,\n+                maxMachineMemoryPercent,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4ceb88163e9ed7759ccdc6aeedf2a147d185f18f"}, "originalPosition": 234}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjk2Mzc5Ng==", "bodyText": "The definition of isRecentlyRefreshed() is currently \"in the last 90 seconds\".  We should probably change that so it's greater than the expected interval at which the autoscaling decider is called.  Otherwise we'll get situations where the memory tracker is stale on every single call to autoscaling.  I expect this won't happen as we scale up, because we refresh the memory tracker when attempting to open jobs.  But for scaling down when jobs are being closed and no new jobs opened we could easily get into the state where we repeatedly say \"no scale\" because the memory tracker is stale.  Maybe there is a way to tie the two intervals together in code instead of hardcoding MlMemoryTracker.RECENT_UPDATE_THRESHOLD to some value in the Cloud code.", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r482963796", "createdAt": "2020-09-03T13:08:41Z", "author": {"login": "droberts195"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/autoscaling/MlAutoscalingDeciderService.java", "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.ml.autoscaling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.LocalNodeMasterListener;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.component.LifecycleListener;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.persistent.PersistentTasksCustomMetadata;\n+import org.elasticsearch.persistent.PersistentTasksCustomMetadata.PersistentTask;\n+import org.elasticsearch.xpack.autoscaling.decision.AutoscalingDeciderContext;\n+import org.elasticsearch.xpack.autoscaling.decision.AutoscalingDeciderService;\n+import org.elasticsearch.xpack.autoscaling.decision.AutoscalingDecision;\n+import org.elasticsearch.xpack.autoscaling.decision.AutoscalingDecisionType;\n+import org.elasticsearch.xpack.core.ml.MlTasks;\n+import org.elasticsearch.xpack.core.ml.dataframe.DataFrameAnalyticsState;\n+import org.elasticsearch.xpack.core.ml.job.config.JobState;\n+import org.elasticsearch.xpack.ml.MachineLearning;\n+import org.elasticsearch.xpack.ml.job.NodeLoadDetector;\n+import org.elasticsearch.xpack.ml.process.MlMemoryTracker;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+\n+import static org.elasticsearch.xpack.core.ml.MlTasks.getDataFrameAnalyticsState;\n+import static org.elasticsearch.xpack.core.ml.MlTasks.getJobStateModifiedForReassignments;\n+import static org.elasticsearch.xpack.ml.job.JobNodeSelector.AWAITING_LAZY_ASSIGNMENT;\n+\n+public class MlAutoscalingDeciderService implements\n+    AutoscalingDeciderService<MlAutoscalingDeciderConfiguration>,\n+    LocalNodeMasterListener {\n+\n+    private static final Logger logger = LogManager.getLogger(MlAutoscalingDeciderService.class);\n+\n+    private final NodeLoadDetector nodeLoadDetector;\n+    private final Map<String, Long> anomalyJobsTimeInQueue;\n+    private final Map<String, Long> analyticsJobsTimeInQueue;\n+    private final Supplier<Long> timeSupplier;\n+\n+    private volatile boolean isMaster;\n+    private volatile boolean running;\n+    private volatile int maxMachineMemoryPercent;\n+    private volatile int maxOpenJobs;\n+    private volatile long lastTimeToScale;\n+\n+    public MlAutoscalingDeciderService(MlMemoryTracker memoryTracker, Settings settings, ClusterService clusterService) {\n+        this(new NodeLoadDetector(memoryTracker), settings, clusterService, System::currentTimeMillis);\n+    }\n+\n+    MlAutoscalingDeciderService(NodeLoadDetector nodeLoadDetector,\n+                                Settings settings,\n+                                ClusterService clusterService,\n+                                Supplier<Long> timeSupplier) {\n+        this.nodeLoadDetector = nodeLoadDetector;\n+        this.maxMachineMemoryPercent = MachineLearning.MAX_MACHINE_MEMORY_PERCENT.get(settings);\n+        this.maxOpenJobs = MachineLearning.MAX_OPEN_JOBS_PER_NODE.get(settings);\n+        this.analyticsJobsTimeInQueue = new ConcurrentHashMap<>();\n+        this.anomalyJobsTimeInQueue = new ConcurrentHashMap<>();\n+        this.timeSupplier = timeSupplier;\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MachineLearning.MAX_MACHINE_MEMORY_PERCENT,\n+            this::setMaxMachineMemoryPercent);\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MachineLearning.MAX_OPEN_JOBS_PER_NODE, this::setMaxOpenJobs);\n+        clusterService.addLocalNodeMasterListener(this);\n+        clusterService.addLifecycleListener(new LifecycleListener() {\n+            @Override\n+            public void afterStart() {\n+                running = true;\n+                if (isMaster) {\n+                    nodeLoadDetector.getMlMemoryTracker().asyncRefresh();\n+                }\n+            }\n+\n+            @Override\n+            public void beforeStop() {\n+                running = false;\n+            }\n+        });\n+    }\n+\n+    void setMaxMachineMemoryPercent(int maxMachineMemoryPercent) {\n+        this.maxMachineMemoryPercent = maxMachineMemoryPercent;\n+    }\n+\n+    void setMaxOpenJobs(int maxOpenJobs) {\n+        this.maxOpenJobs = maxOpenJobs;\n+    }\n+\n+    @Override\n+    public void onMaster() {\n+        isMaster = true;\n+        if (running) {\n+            nodeLoadDetector.getMlMemoryTracker().asyncRefresh();\n+        }\n+    }\n+\n+    @Override\n+    public void offMaster() {\n+        isMaster = false;\n+    }\n+\n+    @Override\n+    public AutoscalingDecision scale(MlAutoscalingDeciderConfiguration decider, AutoscalingDeciderContext context) {\n+        if (isMaster == false) {\n+            throw new IllegalArgumentException(\"request for scaling information is only allowed on the master node\");\n+        }\n+        long previousTimeStamp = this.lastTimeToScale;\n+        this.lastTimeToScale = this.timeSupplier.get();\n+        if (previousTimeStamp == 0L) {\n+            previousTimeStamp = lastTimeToScale;\n+        }\n+        final long timeDiff = Math.max(0L, this.lastTimeToScale - previousTimeStamp);\n+\n+        final ClusterState clusterState = context.state();\n+\n+        PersistentTasksCustomMetadata tasks = clusterState.getMetadata().custom(PersistentTasksCustomMetadata.TYPE);\n+        Collection<PersistentTask<?>> anomalyDetectionTasks = anomalyDetectionTasks(tasks);\n+        Collection<PersistentTask<?>> dataframeAnalyticsTasks = dataframeAnalyticsTasks(tasks);\n+        List<DiscoveryNode> nodes = clusterState.nodes()\n+            .mastersFirstStream()\n+            .filter(MachineLearning::isMlNode)\n+            .collect(Collectors.toList());\n+\n+        final AutoscalingDecision scaleUpDecision = checkForScaleUp(decider,\n+            nodes,\n+            anomalyDetectionTasks,\n+            dataframeAnalyticsTasks,\n+            timeDiff);\n+        if (AutoscalingDecisionType.SCALE_UP.equals(scaleUpDecision.type())) {\n+            return scaleUpDecision;\n+        }\n+\n+        final AutoscalingDecision scaleDownDecision = checkForScaleDown(decider, nodes, clusterState);\n+        if (AutoscalingDecisionType.SCALE_DOWN.equals(scaleDownDecision.type())) {\n+            return scaleDownDecision;\n+        }\n+\n+        return new AutoscalingDecision(name(),\n+            AutoscalingDecisionType.NO_SCALE,\n+            scaleUpDecision.reason() + \"|\" + scaleDownDecision.reason());\n+    }\n+\n+    AutoscalingDecision checkForScaleUp(MlAutoscalingDeciderConfiguration decider,\n+                                        List<DiscoveryNode> nodes,\n+                                        Collection<PersistentTask<?>> anomalyDetectionTasks,\n+                                        Collection<PersistentTask<?>> dataframeAnalyticsTasks,\n+                                        long timeSinceLastCheckMs) {\n+        Set<String> waitingAnomalyJobs = anomalyDetectionTasks.stream()\n+            .filter(t -> AWAITING_LAZY_ASSIGNMENT.equals(t.getAssignment()))\n+            .map(t -> MlTasks.jobId(t.getId()))\n+            .collect(Collectors.toSet());\n+        Set<String> waitingAnalysisJobs = dataframeAnalyticsTasks.stream()\n+            .filter(t -> AWAITING_LAZY_ASSIGNMENT.equals(t.getAssignment()))\n+            .map(t -> MlTasks.dataFrameAnalyticsId(t.getId()))\n+            .collect(Collectors.toSet());\n+        anomalyJobsTimeInQueue.keySet().retainAll(waitingAnomalyJobs);\n+        analyticsJobsTimeInQueue.keySet().retainAll(waitingAnalysisJobs);\n+\n+        if (waitingAnomalyJobs.isEmpty() == false || waitingAnalysisJobs.isEmpty() == false || nodes.size() < decider.getMinNumNodes()) {\n+            if (nodes.size() < decider.getMinNumNodes() || nodes.isEmpty()) {\n+                return new AutoscalingDecision(name(),\n+                    AutoscalingDecisionType.SCALE_UP,\n+                    \"number of machine learning nodes [\"\n+                        + nodes.size()\n+                        + \"] is below the configured minimum number of [\"\n+                        + decider.getMinNumNodes()\n+                        + \"] or is zero\");\n+            }\n+            Set<String> timedUpJobs = new HashSet<>();\n+            for (String jobId : waitingAnomalyJobs) {\n+                long time = anomalyJobsTimeInQueue.compute(jobId, (k, v) -> v == null ? 0L : v + timeSinceLastCheckMs);\n+                if (time >= decider.getAnomalyJobTimeInQueue().getMillis()) {\n+                    timedUpJobs.add(jobId);\n+                }\n+            }\n+            for (String jobId : waitingAnalysisJobs) {\n+                long time = analyticsJobsTimeInQueue.compute(jobId, (k, v) -> v == null ? 0L : v + timeSinceLastCheckMs);\n+                if (time >= decider.getAnalysisJobTimeInQueue().getMillis()) {\n+                    timedUpJobs.add(jobId);\n+                }\n+            }\n+            if (timedUpJobs.isEmpty() == false) {\n+                return new AutoscalingDecision(name(),\n+                    AutoscalingDecisionType.SCALE_UP,\n+                    \"jobs \" + timedUpJobs + \" have been waiting for assignment too long\");\n+            }\n+        }\n+        return new AutoscalingDecision(name(), AutoscalingDecisionType.NO_SCALE, \"no jobs have waited long enough for assignment\");\n+    }\n+\n+    AutoscalingDecision checkForScaleDown(MlAutoscalingDeciderConfiguration decider,\n+                                          List<DiscoveryNode> nodes,\n+                                          ClusterState clusterState) {\n+        if (nodes.size() == decider.getMinNumNodes()) {\n+            return new AutoscalingDecision(name(),\n+                AutoscalingDecisionType.NO_SCALE,\n+                \"| already at configured minimum node count [\"\n+                    + decider.getMinNumNodes()\n+                    +\"]\");\n+        }\n+        // kick of a refresh if our data around memory usage is stale.\n+        if (nodeLoadDetector.getMlMemoryTracker().isRecentlyRefreshed() == false) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4ceb88163e9ed7759ccdc6aeedf2a147d185f18f"}, "originalPosition": 217}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dacbffc98700f630d39e45d2b7a32e04a3519d80", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/dacbffc98700f630d39e45d2b7a32e04a3519d80", "committedDate": "2020-09-14T13:25:08Z", "message": "Merge branch 'master' into feature/ml-autoscaling-integration"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f5a1d8031fec0c9e157ee8ea559243bf33335694", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/f5a1d8031fec0c9e157ee8ea559243bf33335694", "committedDate": "2020-09-22T19:11:31Z", "message": "Merge branch 'master' into feature/ml-autoscaling-integration"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bb639c608f778d55c3f2f465886b91f224232837", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/bb639c608f778d55c3f2f465886b91f224232837", "committedDate": "2020-09-28T12:09:28Z", "message": "Merge remote-tracking branch 'upstream/master' into feature/ml-autoscaling-integration"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "040b16d7239743505b678df7eb60db1d365dd6ab", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/040b16d7239743505b678df7eb60db1d365dd6ab", "committedDate": "2020-09-28T16:57:49Z", "message": "adjusting autoscaling decider"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d199dcac1756ddb091456b104f38591165cdac59", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/d199dcac1756ddb091456b104f38591165cdac59", "committedDate": "2020-09-28T17:37:43Z", "message": "fixing setting handling"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1394ca18bac61340d98480907f09730ae02d25b4", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/1394ca18bac61340d98480907f09730ae02d25b4", "committedDate": "2020-09-28T19:54:34Z", "message": "addressing scale up"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5b267e01c69d781be7149bb7ce0a66f08acaa2d7", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/5b267e01c69d781be7149bb7ce0a66f08acaa2d7", "committedDate": "2020-09-29T20:26:36Z", "message": "finalizing scale up logic"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8b8462ee547d4c8168de7eb0d4eec30cd8de135f", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/8b8462ee547d4c8168de7eb0d4eec30cd8de135f", "committedDate": "2020-10-13T14:55:41Z", "message": "adding downscale delay option"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0aa0301841f7b33606462c488db825d47916964f", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/0aa0301841f7b33606462c488db825d47916964f", "committedDate": "2020-10-13T14:59:17Z", "message": "Merge remote-tracking branch 'upstream/master' into feature/ml-autoscaling-integration"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b2139a584e24c3e0110d0ed2fe22dd13604ff9b5", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/b2139a584e24c3e0110d0ed2fe22dd13604ff9b5", "committedDate": "2020-10-14T13:45:36Z", "message": "adding native memory calculator for special dynamic case"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1e8183b865758df1758a60c916c9576c23bd1897", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/1e8183b865758df1758a60c916c9576c23bd1897", "committedDate": "2020-10-14T20:42:23Z", "message": "adjusting how we calculate memory percentage"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "453d4a7914fd658f379ab65ce7b6c58eefa3f670", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/453d4a7914fd658f379ab65ce7b6c58eefa3f670", "committedDate": "2020-10-14T20:57:46Z", "message": "Merge remote-tracking branch 'upstream/master' into feature/ml-autoscaling-integration"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8e846e41ad47a51c7124c326cf45ef916036db94", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/8e846e41ad47a51c7124c326cf45ef916036db94", "committedDate": "2020-10-15T18:32:52Z", "message": "handling native size to node size and scale down"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "28f2257e31961dcbe6dd5d66f042b6a921cb59a3", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/28f2257e31961dcbe6dd5d66f042b6a921cb59a3", "committedDate": "2020-10-21T17:36:24Z", "message": "Merge branch 'master' into feature/ml-autoscaling-integration"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a6652454f38af740c163f27d0859f49559dab453", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/a6652454f38af740c163f27d0859f49559dab453", "committedDate": "2020-10-21T18:16:41Z", "message": "updating from master"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6e45e6aec124d317b8d130afe1ae0574c2c67686", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/6e45e6aec124d317b8d130afe1ae0574c2c67686", "committedDate": "2020-10-21T18:17:20Z", "message": "undo bad delete"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d5e065b526dcef8f2e7bb54edacb8f8bf01dbcc2", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/d5e065b526dcef8f2e7bb54edacb8f8bf01dbcc2", "committedDate": "2020-10-21T18:21:31Z", "message": "minor adjustments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a4a7f237410cfe9ccaa7124070e2779ff35256fa", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/a4a7f237410cfe9ccaa7124070e2779ff35256fa", "committedDate": "2020-11-10T19:54:25Z", "message": "Merge branch 'master' into feature/ml-autoscaling-integration"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f4ed9829300f5e943cb99646588a400c5816fea9", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/f4ed9829300f5e943cb99646588a400c5816fea9", "committedDate": "2020-11-10T21:07:02Z", "message": "fixing tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a00e1290b07f854311b2e1679035e262ff9156c1", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/a00e1290b07f854311b2e1679035e262ff9156c1", "committedDate": "2020-11-11T16:42:46Z", "message": "fixing tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e47a136b8a3aa9f0b4e03e60d627d625324281a6", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/e47a136b8a3aa9f0b4e03e60d627d625324281a6", "committedDate": "2020-11-11T16:43:07Z", "message": "Merge remote-tracking branch 'upstream/master' into feature/ml-autoscaling-integration"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7fbe30e60f24a14d7ba0f84bb6c572bf77eeafc2", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/7fbe30e60f24a14d7ba0f84bb6c572bf77eeafc2", "committedDate": "2020-11-11T21:23:58Z", "message": "adding scaledown tests, refactoring nodeload class"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8cf46edcbe8a2707a1dda235694e9359bec086f3", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/8cf46edcbe8a2707a1dda235694e9359bec086f3", "committedDate": "2020-11-12T15:25:56Z", "message": "adding tests and validations"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "66843332894db3833d1851b0ec0bffea9b6f7453", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/66843332894db3833d1851b0ec0bffea9b6f7453", "committedDate": "2020-11-12T15:26:15Z", "message": "Merge remote-tracking branch 'upstream/master' into feature/ml-autoscaling-integration"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5MjIwMTk2", "url": "https://github.com/elastic/elasticsearch/pull/59309#pullrequestreview-529220196", "createdAt": "2020-11-12T15:45:01Z", "commit": {"oid": "66843332894db3833d1851b0ec0bffea9b6f7453"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQxNTo0NTowMVrOHyA8VA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQxNjo1NToxMFrOHyEPWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjIwNjI5Mg==", "bodyText": "nit: it can just be Property.NodeScope like in the setting above", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r522206292", "createdAt": "2020-11-12T15:45:01Z", "author": {"login": "droberts195"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/MachineLearning.java", "diffHunk": "@@ -478,6 +482,17 @@\n             Property.NodeScope\n         );\n \n+    /**\n+     * This is the maximum possible node size for a machine learning node. It is useful when determining if a job could ever be opened\n+     * on the cluster.\n+     *\n+     * If the value is the default special case of `0b`, that means the value is ignored when assigning jobs.\n+     */\n+    public static final Setting<ByteSizeValue> MAX_ML_NODE_SIZE = Setting.byteSizeSetting(\n+        \"xpack.ml.max_ml_node_size\",\n+        ByteSizeValue.ZERO,\n+        Setting.Property.NodeScope);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "66843332894db3833d1851b0ec0bffea9b6f7453"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjI2MDMxMg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        this.downScaleDelay = downScaleDelay;\n          \n          \n            \n                        this.downScaleDelay = Objects.requireNonNull(downScaleDelay);", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r522260312", "createdAt": "2020-11-12T16:55:10Z", "author": {"login": "droberts195"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/autoscaling/MlAutoscalingDeciderConfiguration.java", "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.autoscaling;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.common.xcontent.ObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderConfiguration;\n+\n+import java.io.IOException;\n+import java.util.Objects;\n+\n+public class MlAutoscalingDeciderConfiguration implements AutoscalingDeciderConfiguration {\n+    static final String NAME = \"ml\";\n+\n+    private static final int DEFAULT_ANOMALY_JOBS_IN_QUEUE = 0;\n+    private static final int DEFAULT_ANALYTICS_JOBS_IN_QUEUE = 0;\n+\n+    private static final ParseField NUM_ANOMALY_JOBS_IN_QUEUE = new ParseField(\"num_anomaly_jobs_in_queue\");\n+    private static final ParseField NUM_ANALYTICS_JOBS_IN_QUEUE = new ParseField(\"num_analytics_jobs_in_queue\");\n+    private static final ParseField DOWN_SCALE_DELAY = new ParseField(\"down_scale_delay\");\n+\n+    private static final ObjectParser<MlAutoscalingDeciderConfiguration.Builder, Void> PARSER = new ObjectParser<>(NAME,\n+        MlAutoscalingDeciderConfiguration.Builder::new);\n+\n+    static {\n+        PARSER.declareInt(MlAutoscalingDeciderConfiguration.Builder::setNumAnomalyJobsInQueue, NUM_ANOMALY_JOBS_IN_QUEUE);\n+        PARSER.declareInt(MlAutoscalingDeciderConfiguration.Builder::setNumAnalyticsJobsInQueue, NUM_ANALYTICS_JOBS_IN_QUEUE);\n+        PARSER.declareString(MlAutoscalingDeciderConfiguration.Builder::setDownScaleDelay, DOWN_SCALE_DELAY);\n+    }\n+\n+    public static MlAutoscalingDeciderConfiguration parse(final XContentParser parser) {\n+        return PARSER.apply(parser, null).build();\n+    }\n+\n+    private final int numAnomalyJobsInQueue;\n+    private final int numAnalyticsJobsInQueue;\n+    private final TimeValue downScaleDelay;\n+\n+    MlAutoscalingDeciderConfiguration(int numAnomalyJobsInQueue, int numAnalyticsJobsInQueue, TimeValue downScaleDelay) {\n+        if (numAnomalyJobsInQueue < 0) {\n+            throw new IllegalArgumentException(\"[\" + NUM_ANOMALY_JOBS_IN_QUEUE.getPreferredName() + \"] must be non-negative\");\n+        }\n+        if (numAnalyticsJobsInQueue < 0) {\n+            throw new IllegalArgumentException(\"[\" + NUM_ANALYTICS_JOBS_IN_QUEUE.getPreferredName() + \"] must be non-negative\");\n+        }\n+        this.numAnalyticsJobsInQueue = numAnalyticsJobsInQueue;\n+        this.numAnomalyJobsInQueue = numAnomalyJobsInQueue;\n+        this.downScaleDelay = downScaleDelay;\n+    }\n+\n+    public MlAutoscalingDeciderConfiguration(StreamInput in) throws IOException {\n+        numAnomalyJobsInQueue = in.readVInt();\n+        numAnalyticsJobsInQueue = in.readVInt();\n+        downScaleDelay = in.readTimeValue();\n+    }\n+\n+    @Override\n+    public String name() {\n+        return NAME;\n+    }\n+\n+    @Override\n+    public String getWriteableName() {\n+        return NAME;\n+    }\n+\n+    @Override\n+    public void writeTo(StreamOutput out) throws IOException {\n+        out.writeVInt(numAnomalyJobsInQueue);\n+        out.writeVInt(numAnalyticsJobsInQueue);\n+        out.writeTimeValue(downScaleDelay);\n+    }\n+\n+    public int getNumAnomalyJobsInQueue() {\n+        return numAnomalyJobsInQueue;\n+    }\n+\n+    public int getNumAnalyticsJobsInQueue() {\n+        return numAnalyticsJobsInQueue;\n+    }\n+\n+    public TimeValue getDownScaleDelay() {\n+        return downScaleDelay;\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+        if (this == o) return true;\n+        if (o == null || getClass() != o.getClass()) return false;\n+        MlAutoscalingDeciderConfiguration that = (MlAutoscalingDeciderConfiguration) o;\n+        return numAnomalyJobsInQueue == that.numAnomalyJobsInQueue &&\n+            numAnalyticsJobsInQueue == that.numAnalyticsJobsInQueue &&\n+            Objects.equals(downScaleDelay, that.downScaleDelay);\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+        return Objects.hash(numAnomalyJobsInQueue, numAnalyticsJobsInQueue, downScaleDelay);\n+    }\n+\n+    @Override\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        builder.startObject();\n+        builder.field(NUM_ANOMALY_JOBS_IN_QUEUE .getPreferredName(), numAnomalyJobsInQueue);\n+        builder.field(NUM_ANALYTICS_JOBS_IN_QUEUE.getPreferredName(), numAnalyticsJobsInQueue);\n+        builder.field(DOWN_SCALE_DELAY.getPreferredName(), downScaleDelay.getStringRep());\n+        builder.endObject();\n+        return builder;\n+    }\n+\n+    public static Builder builder() {\n+        return new Builder();\n+    }\n+\n+    public static class Builder {\n+\n+        private int numAnomalyJobsInQueue = DEFAULT_ANOMALY_JOBS_IN_QUEUE;\n+        private int numAnalyticsJobsInQueue = DEFAULT_ANALYTICS_JOBS_IN_QUEUE;\n+        private TimeValue downScaleDelay = TimeValue.ZERO;\n+\n+        public Builder setNumAnomalyJobsInQueue(int numAnomalyJobsInQueue) {\n+            this.numAnomalyJobsInQueue = numAnomalyJobsInQueue;\n+            return this;\n+        }\n+\n+        public Builder setNumAnalyticsJobsInQueue(int numAnalyticsJobsInQueue) {\n+            this.numAnalyticsJobsInQueue = numAnalyticsJobsInQueue;\n+            return this;\n+        }\n+\n+        Builder setDownScaleDelay(String unparsedTimeValue) {\n+            return setDownScaleDelay(TimeValue.parseTimeValue(unparsedTimeValue, DOWN_SCALE_DELAY.getPreferredName()));\n+        }\n+\n+        public Builder setDownScaleDelay(TimeValue downScaleDelay) {\n+            this.downScaleDelay = downScaleDelay;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "66843332894db3833d1851b0ec0bffea9b6f7453"}, "originalPosition": 145}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6dc17aa48d205e1d9fc7128cf5efaf57dd0c884f", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/6dc17aa48d205e1d9fc7128cf5efaf57dd0c884f", "committedDate": "2020-11-12T19:28:48Z", "message": "Apply suggestions from code review\n\nCo-authored-by: David Roberts <dave.roberts@elastic.co>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f023e77f700d178b604ae26625de7d35bc2b10e5", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/f023e77f700d178b604ae26625de7d35bc2b10e5", "committedDate": "2020-11-16T13:51:09Z", "message": "Merge remote-tracking branch 'upstream/master' into feature/ml-autoscaling-integration"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b181b409a5b8135d95086177b416e8b3935ac2d8", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/b181b409a5b8135d95086177b416e8b3935ac2d8", "committedDate": "2020-11-16T14:02:35Z", "message": "fixing compilation"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "94a969e1b2b23eaa6c7e7b8814dc1471f0dde032", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/94a969e1b2b23eaa6c7e7b8814dc1471f0dde032", "committedDate": "2020-11-16T14:02:48Z", "message": "Merge remote-tracking branch 'upstream/master' into feature/ml-autoscaling-integration"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMxMzQ3NjY4", "url": "https://github.com/elastic/elasticsearch/pull/59309#pullrequestreview-531347668", "createdAt": "2020-11-16T14:04:01Z", "commit": {"oid": "f023e77f700d178b604ae26625de7d35bc2b10e5"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQxNDowNDo0MlrOH0AIVQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQxNDowNzo1NVrOH0AQ2A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDI5MDEzMw==", "bodyText": "I think this is dangerous.  It means we could scale the cluster significantly due to the memory tracker not having the required information.  It would also be difficult to diagnose this based on a user complaint that their cluster scaled wildly.  For example, suppose there are 50 jobs each with a memory requirement of 0.1GB, total requirement 5GB, but then due to a glitch in the memory tracker we treat that as 50GB and scale up to a much more costly cluster.\nSince this is for scaling up, I think we should use 0 for jobs where we have no memory information, so we'll only scale up if the sum of the memory requirements for jobs that we know the memory requirement for imply a scale up.\n(Same a few lines below.)", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r524290133", "createdAt": "2020-11-16T14:04:42Z", "author": {"login": "droberts195"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/autoscaling/MlAutoscalingDeciderService.java", "diffHunk": "@@ -0,0 +1,561 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.ml.autoscaling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.LocalNodeMasterListener;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.component.LifecycleListener;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.persistent.PersistentTasksCustomMetadata;\n+import org.elasticsearch.persistent.PersistentTasksCustomMetadata.PersistentTask;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderContext;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderResult;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderService;\n+import org.elasticsearch.xpack.core.ml.MlTasks;\n+import org.elasticsearch.xpack.core.ml.action.StartDatafeedAction.DatafeedParams;\n+import org.elasticsearch.xpack.core.ml.dataframe.DataFrameAnalyticsState;\n+import org.elasticsearch.xpack.core.ml.job.config.AnalysisLimits;\n+import org.elasticsearch.xpack.core.ml.job.config.JobState;\n+import org.elasticsearch.xpack.ml.MachineLearning;\n+import org.elasticsearch.xpack.ml.job.NodeLoad;\n+import org.elasticsearch.xpack.ml.job.NodeLoadDetector;\n+import org.elasticsearch.xpack.ml.process.MlMemoryTracker;\n+import org.elasticsearch.xpack.ml.utils.NativeMemoryCalculator;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.OptionalLong;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.elasticsearch.xpack.core.ml.MlTasks.getDataFrameAnalyticsState;\n+import static org.elasticsearch.xpack.core.ml.MlTasks.getJobStateModifiedForReassignments;\n+import static org.elasticsearch.xpack.ml.job.JobNodeSelector.AWAITING_LAZY_ASSIGNMENT;\n+\n+public class MlAutoscalingDeciderService implements AutoscalingDeciderService<MlAutoscalingDeciderConfiguration>,\n+    LocalNodeMasterListener {\n+\n+    private static final Logger logger = LogManager.getLogger(MlAutoscalingDeciderService.class);\n+    private static final Duration DEFAULT_MEMORY_REFRESH_RATE = Duration.ofMinutes(15);\n+    private static final String MEMORY_STALE = \"unable to make scaling decision as job memory requirements are stale\";\n+    private static final long NO_SCALE_DOWN_POSSIBLE = -1L;\n+\n+    private final NodeLoadDetector nodeLoadDetector;\n+    private final MlMemoryTracker mlMemoryTracker;\n+    private final Supplier<Long> timeSupplier;\n+    private final boolean useAuto;\n+\n+    private volatile boolean isMaster;\n+    private volatile boolean running;\n+    private volatile int maxMachineMemoryPercent;\n+    private volatile int maxOpenJobs;\n+    private volatile long lastTimeToScale;\n+    private volatile long scaleDownDetected;\n+\n+    public MlAutoscalingDeciderService(MlMemoryTracker memoryTracker, Settings settings, ClusterService clusterService) {\n+        this(new NodeLoadDetector(memoryTracker), settings, clusterService, System::currentTimeMillis);\n+    }\n+\n+    MlAutoscalingDeciderService(NodeLoadDetector nodeLoadDetector,\n+                                Settings settings,\n+                                ClusterService clusterService,\n+                                Supplier<Long> timeSupplier) {\n+        this.nodeLoadDetector = nodeLoadDetector;\n+        this.mlMemoryTracker = nodeLoadDetector.getMlMemoryTracker();\n+        this.maxMachineMemoryPercent = MachineLearning.MAX_MACHINE_MEMORY_PERCENT.get(settings);\n+        this.maxOpenJobs = MachineLearning.MAX_OPEN_JOBS_PER_NODE.get(settings);\n+        this.useAuto = MachineLearning.USE_AUTO_MACHINE_MEMORY_PERCENT.get(settings);\n+        this.timeSupplier = timeSupplier;\n+        this.scaleDownDetected = NO_SCALE_DOWN_POSSIBLE;\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MachineLearning.MAX_MACHINE_MEMORY_PERCENT,\n+            this::setMaxMachineMemoryPercent);\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MachineLearning.MAX_OPEN_JOBS_PER_NODE, this::setMaxOpenJobs);\n+        clusterService.addLocalNodeMasterListener(this);\n+        clusterService.addLifecycleListener(new LifecycleListener() {\n+            @Override\n+            public void afterStart() {\n+                running = true;\n+                if (isMaster) {\n+                    mlMemoryTracker.asyncRefresh();\n+                }\n+            }\n+\n+            @Override\n+            public void beforeStop() {\n+                running = false;\n+            }\n+        });\n+    }\n+\n+    static OptionalLong getNodeJvmSize(DiscoveryNode node) {\n+        Map<String, String> nodeAttributes = node.getAttributes();\n+        OptionalLong value = OptionalLong.empty();\n+        String valueStr = nodeAttributes.get(MachineLearning.MAX_JVM_SIZE_NODE_ATTR);\n+        try {\n+            value = OptionalLong.of(Long.parseLong(valueStr));\n+        } catch (NumberFormatException e) {\n+            logger.debug(() -> new ParameterizedMessage(\n+                \"could not parse stored string value [{}] in node attribute [{}]\",\n+                valueStr,\n+                MachineLearning.MAX_JVM_SIZE_NODE_ATTR));\n+        }\n+        return value;\n+    }\n+\n+    static List<DiscoveryNode> getNodes(final ClusterState clusterState) {\n+        return clusterState.nodes()\n+            .mastersFirstStream()\n+            .filter(MachineLearning::isMlNode)\n+            .collect(Collectors.toList());\n+    }\n+\n+    /**\n+     * @param unassignedJobs The list of unassigned jobs\n+     * @param sizeFunction   Function providing the memory required for a job\n+     * @param defaultSize    The default memory size (if the sizeFunction returns null)\n+     * @param maxNumInQueue  The number of unassigned jobs allowed.\n+     * @return The capacity needed to reduce the length of `unassignedJobs` to `maxNumInQueue`\n+     */\n+    static Optional<NativeMemoryCapacity> requiredCapacityForUnassignedJobs(List<String> unassignedJobs,\n+                                                                            Function<String, Long> sizeFunction,\n+                                                                            long defaultSize,\n+                                                                            int maxNumInQueue) {\n+        List<Long> jobSizes = unassignedJobs\n+            .stream()\n+            // TODO do we want to verify memory requirements aren't stale? Or just consider `null` a fastpath?\n+            .map(sizeFunction)\n+            .map(l -> l == null ? defaultSize : l)\n+            .collect(Collectors.toList());\n+        // Only possible if unassignedJobs was empty.\n+        if (jobSizes.isEmpty()) {\n+            return Optional.empty();\n+        }\n+        jobSizes.sort(Comparator.comparingLong(Long::longValue).reversed());\n+        long tierMemory = 0L;\n+        long nodeMemory = jobSizes.get(0);\n+        Iterator<Long> iter = jobSizes.iterator();\n+        while (jobSizes.size() > maxNumInQueue && iter.hasNext()) {\n+            tierMemory += iter.next();\n+            iter.remove();\n+        }\n+        return Optional.of(new NativeMemoryCapacity(tierMemory, nodeMemory));\n+    }\n+\n+    private static Collection<PersistentTask<?>> anomalyDetectionTasks(PersistentTasksCustomMetadata tasksCustomMetadata) {\n+        if (tasksCustomMetadata == null) {\n+            return Collections.emptyList();\n+        }\n+\n+        return tasksCustomMetadata.findTasks(MlTasks.JOB_TASK_NAME,\n+            t -> getJobStateModifiedForReassignments(t).isAnyOf(JobState.OPENED, JobState.OPENING));\n+    }\n+\n+    private static Collection<PersistentTask<?>> dataframeAnalyticsTasks(PersistentTasksCustomMetadata tasksCustomMetadata) {\n+        if (tasksCustomMetadata == null) {\n+            return Collections.emptyList();\n+        }\n+\n+        return tasksCustomMetadata.findTasks(MlTasks.DATA_FRAME_ANALYTICS_TASK_NAME,\n+            t -> getDataFrameAnalyticsState(t).isAnyOf(DataFrameAnalyticsState.STARTED, DataFrameAnalyticsState.STARTING));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private static Collection<PersistentTask<DatafeedParams>> datafeedTasks(PersistentTasksCustomMetadata tasksCustomMetadata) {\n+        if (tasksCustomMetadata == null) {\n+            return Collections.emptyList();\n+        }\n+\n+        return tasksCustomMetadata.findTasks(MlTasks.DATAFEED_TASK_NAME, t -> true)\n+            .stream()\n+            .map(p -> (PersistentTask<DatafeedParams>) p)\n+            .collect(Collectors.toList());\n+    }\n+\n+    void setMaxMachineMemoryPercent(int maxMachineMemoryPercent) {\n+        this.maxMachineMemoryPercent = maxMachineMemoryPercent;\n+    }\n+\n+    void setMaxOpenJobs(int maxOpenJobs) {\n+        this.maxOpenJobs = maxOpenJobs;\n+    }\n+\n+    @Override\n+    public void onMaster() {\n+        isMaster = true;\n+        if (running) {\n+            mlMemoryTracker.asyncRefresh();\n+        }\n+    }\n+\n+    private void resetScaleDownCoolDown() {\n+        this.scaleDownDetected = NO_SCALE_DOWN_POSSIBLE;\n+    }\n+\n+    private boolean canScaleDown(TimeValue coolDown) {\n+        if (this.scaleDownDetected == NO_SCALE_DOWN_POSSIBLE) {\n+            return false;\n+        }\n+        return timeSupplier.get() - scaleDownDetected >= coolDown.millis();\n+    }\n+\n+    private boolean newScaleDownCheck() {\n+        return scaleDownDetected == NO_SCALE_DOWN_POSSIBLE;\n+    }\n+\n+    NativeMemoryCapacity currentScale(final List<DiscoveryNode> machineLearningNodes) {\n+        long[] mlMemory = machineLearningNodes.stream()\n+            .mapToLong(node -> NativeMemoryCalculator.allowedBytesForMl(node, maxMachineMemoryPercent, useAuto).orElse(0L))\n+            .toArray();\n+\n+        return new NativeMemoryCapacity(\n+            Arrays.stream(mlMemory).sum(),\n+            Arrays.stream(mlMemory).max().orElse(0L),\n+            // We assume that JVM size is universal, at least, the largest JVM indicates the largest node\n+            machineLearningNodes.stream()\n+                .map(MlAutoscalingDeciderService::getNodeJvmSize)\n+                .mapToLong(l -> l.orElse(0L))\n+                .boxed()\n+                .max(Long::compare)\n+                .orElse(null)\n+        );\n+    }\n+\n+    @Override\n+    public void offMaster() {\n+        isMaster = false;\n+    }\n+\n+    @Override\n+    public AutoscalingDeciderResult scale(MlAutoscalingDeciderConfiguration decider, AutoscalingDeciderContext context) {\n+        if (isMaster == false) {\n+            throw new IllegalArgumentException(\"request for scaling information is only allowed on the master node\");\n+        }\n+        final Duration memoryTrackingStale;\n+        long previousTimeStamp = this.lastTimeToScale;\n+        this.lastTimeToScale = this.timeSupplier.get();\n+        if (previousTimeStamp == 0L) {\n+            memoryTrackingStale = DEFAULT_MEMORY_REFRESH_RATE;\n+        } else {\n+            memoryTrackingStale = Duration.ofMillis(TimeValue.timeValueMinutes(1).millis() + this.lastTimeToScale - previousTimeStamp);\n+        }\n+\n+        final ClusterState clusterState = context.state();\n+\n+        PersistentTasksCustomMetadata tasks = clusterState.getMetadata().custom(PersistentTasksCustomMetadata.TYPE);\n+        Collection<PersistentTask<?>> anomalyDetectionTasks = anomalyDetectionTasks(tasks);\n+        Collection<PersistentTask<?>> dataframeAnalyticsTasks = dataframeAnalyticsTasks(tasks);\n+        final List<DiscoveryNode> nodes = getNodes(clusterState);\n+        Optional<NativeMemoryCapacity> futureFreedCapacity = calculateFutureFreedCapacity(tasks, memoryTrackingStale);\n+\n+        final List<String> waitingAnomalyJobs = anomalyDetectionTasks.stream()\n+            .filter(t -> AWAITING_LAZY_ASSIGNMENT.equals(t.getAssignment()))\n+            .map(t -> MlTasks.jobId(t.getId()))\n+            .collect(Collectors.toList());\n+        final List<String> waitingAnalyticsJobs = dataframeAnalyticsTasks.stream()\n+            .filter(t -> AWAITING_LAZY_ASSIGNMENT.equals(t.getAssignment()))\n+            .map(t -> MlTasks.dataFrameAnalyticsId(t.getId()))\n+            .collect(Collectors.toList());\n+\n+        final NativeMemoryCapacity currentScale = currentScale(nodes);\n+        final MlScalingReason.Builder reasonBuilder = MlScalingReason.builder()\n+            .setWaitingAnomalyJobs(waitingAnomalyJobs)\n+            .setWaitingAnalyticsJobs(waitingAnalyticsJobs)\n+            .setCurrentMlCapacity(currentScale.autoscalingCapacity(maxMachineMemoryPercent, useAuto))\n+            .setPassedConfiguration(decider);\n+\n+        final Optional<AutoscalingDeciderResult> scaleUpDecision = checkForScaleUp(decider,\n+            waitingAnomalyJobs,\n+            waitingAnalyticsJobs,\n+            futureFreedCapacity.orElse(null),\n+            currentScale,\n+            reasonBuilder);\n+\n+        if (scaleUpDecision.isPresent()) {\n+            resetScaleDownCoolDown();\n+            return scaleUpDecision.get();\n+        }\n+        if (waitingAnalyticsJobs.isEmpty() == false || waitingAnomalyJobs.isEmpty() == false) {\n+            // We don't want to continue to consider a scale down if there are now waiting jobs\n+            resetScaleDownCoolDown();\n+            return new AutoscalingDeciderResult(\n+                context.currentCapacity(),\n+                reasonBuilder\n+                    .setSimpleReason(\"Passing currently perceived capacity as there are analytics and anomaly jobs in the queue, \" +\n+                        \"but the number in the queue is less than the configured maximum allowed.\")\n+                    .build());\n+        }\n+        if (mlMemoryTracker.isRecentlyRefreshed(memoryTrackingStale) == false) {\n+            return buildDecisionAndRequestRefresh(reasonBuilder);\n+        }\n+\n+        long largestJob = Math.max(\n+            anomalyDetectionTasks.stream()\n+                .filter(PersistentTask::isAssigned)\n+                // Memory SHOULD be recently refreshed, so in our current state, we should at least have an idea of the memory used\n+                .mapToLong(this::getAnomalyMemoryRequirement)\n+                .max()\n+                .orElse(0L),\n+            dataframeAnalyticsTasks.stream()\n+                .filter(PersistentTask::isAssigned)\n+                // Memory SHOULD be recently refreshed, so in our current state, we should at least have an idea of the memory used\n+                .mapToLong(this::getAnalyticsMemoryRequirement)\n+                .max()\n+                .orElse(0L));\n+\n+        final Optional<AutoscalingDeciderResult> scaleDownDecision =\n+            checkForScaleDown(nodes, clusterState, largestJob, currentScale, reasonBuilder);\n+\n+        if (scaleDownDecision.isPresent()) {\n+            if (newScaleDownCheck()) {\n+                scaleDownDetected = timeSupplier.get();\n+            }\n+            if (canScaleDown(decider.getDownScaleDelay())) {\n+                return scaleDownDecision.get();\n+            }\n+            return new AutoscalingDeciderResult(\n+                context.currentCapacity(),\n+                reasonBuilder\n+                    .setSimpleReason(\n+                        \"Passing currently perceived capacity as configured down scale delay has not be satisfied; configured delay [\"\n+                            + decider.getDownScaleDelay().millis()\n+                            + \"] last detected scale down event [\"\n+                            + scaleDownDetected\n+                            + \"]\")\n+                    .build());\n+        }\n+\n+        return new AutoscalingDeciderResult(context.currentCapacity(),\n+            reasonBuilder\n+                .setSimpleReason(\"Passing currently perceived capacity as no scaling changes were detected to be possible\")\n+                .build());\n+    }\n+\n+    Optional<AutoscalingDeciderResult> checkForScaleUp(MlAutoscalingDeciderConfiguration decider,\n+                                                       List<String> waitingAnomalyJobs,\n+                                                       List<String> waitingAnalyticsJobs,\n+                                                       @Nullable NativeMemoryCapacity futureFreedCapacity,\n+                                                       NativeMemoryCapacity currentScale,\n+                                                       MlScalingReason.Builder reasonBuilder) {\n+\n+        // Are we in breach of maximum waiting jobs?\n+        if (waitingAnalyticsJobs.size() > decider.getNumAnalyticsJobsInQueue()\n+            || waitingAnomalyJobs.size() > decider.getNumAnomalyJobsInQueue()) {\n+            NativeMemoryCapacity updatedCapacity = NativeMemoryCapacity.from(currentScale);\n+            Optional<NativeMemoryCapacity> analyticsCapacity = requiredCapacityForUnassignedJobs(waitingAnalyticsJobs,\n+                this::getAnalyticsMemoryRequirement,\n+                // TODO Better default???\n+                AnalysisLimits.DEFAULT_MODEL_MEMORY_LIMIT_MB,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94a969e1b2b23eaa6c7e7b8814dc1471f0dde032"}, "originalPosition": 369}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDI5MjMxMg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    // Even if we verify that memory usage is up today before checking node capacity, we could still run into stale information.\n          \n          \n            \n                    // Even if we verify that memory usage is up to date before checking node capacity, we could still run into stale information.", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r524292312", "createdAt": "2020-11-16T14:07:55Z", "author": {"login": "droberts195"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/autoscaling/MlAutoscalingDeciderService.java", "diffHunk": "@@ -0,0 +1,561 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.ml.autoscaling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.LocalNodeMasterListener;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.component.LifecycleListener;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.persistent.PersistentTasksCustomMetadata;\n+import org.elasticsearch.persistent.PersistentTasksCustomMetadata.PersistentTask;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderContext;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderResult;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderService;\n+import org.elasticsearch.xpack.core.ml.MlTasks;\n+import org.elasticsearch.xpack.core.ml.action.StartDatafeedAction.DatafeedParams;\n+import org.elasticsearch.xpack.core.ml.dataframe.DataFrameAnalyticsState;\n+import org.elasticsearch.xpack.core.ml.job.config.AnalysisLimits;\n+import org.elasticsearch.xpack.core.ml.job.config.JobState;\n+import org.elasticsearch.xpack.ml.MachineLearning;\n+import org.elasticsearch.xpack.ml.job.NodeLoad;\n+import org.elasticsearch.xpack.ml.job.NodeLoadDetector;\n+import org.elasticsearch.xpack.ml.process.MlMemoryTracker;\n+import org.elasticsearch.xpack.ml.utils.NativeMemoryCalculator;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.OptionalLong;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.elasticsearch.xpack.core.ml.MlTasks.getDataFrameAnalyticsState;\n+import static org.elasticsearch.xpack.core.ml.MlTasks.getJobStateModifiedForReassignments;\n+import static org.elasticsearch.xpack.ml.job.JobNodeSelector.AWAITING_LAZY_ASSIGNMENT;\n+\n+public class MlAutoscalingDeciderService implements AutoscalingDeciderService<MlAutoscalingDeciderConfiguration>,\n+    LocalNodeMasterListener {\n+\n+    private static final Logger logger = LogManager.getLogger(MlAutoscalingDeciderService.class);\n+    private static final Duration DEFAULT_MEMORY_REFRESH_RATE = Duration.ofMinutes(15);\n+    private static final String MEMORY_STALE = \"unable to make scaling decision as job memory requirements are stale\";\n+    private static final long NO_SCALE_DOWN_POSSIBLE = -1L;\n+\n+    private final NodeLoadDetector nodeLoadDetector;\n+    private final MlMemoryTracker mlMemoryTracker;\n+    private final Supplier<Long> timeSupplier;\n+    private final boolean useAuto;\n+\n+    private volatile boolean isMaster;\n+    private volatile boolean running;\n+    private volatile int maxMachineMemoryPercent;\n+    private volatile int maxOpenJobs;\n+    private volatile long lastTimeToScale;\n+    private volatile long scaleDownDetected;\n+\n+    public MlAutoscalingDeciderService(MlMemoryTracker memoryTracker, Settings settings, ClusterService clusterService) {\n+        this(new NodeLoadDetector(memoryTracker), settings, clusterService, System::currentTimeMillis);\n+    }\n+\n+    MlAutoscalingDeciderService(NodeLoadDetector nodeLoadDetector,\n+                                Settings settings,\n+                                ClusterService clusterService,\n+                                Supplier<Long> timeSupplier) {\n+        this.nodeLoadDetector = nodeLoadDetector;\n+        this.mlMemoryTracker = nodeLoadDetector.getMlMemoryTracker();\n+        this.maxMachineMemoryPercent = MachineLearning.MAX_MACHINE_MEMORY_PERCENT.get(settings);\n+        this.maxOpenJobs = MachineLearning.MAX_OPEN_JOBS_PER_NODE.get(settings);\n+        this.useAuto = MachineLearning.USE_AUTO_MACHINE_MEMORY_PERCENT.get(settings);\n+        this.timeSupplier = timeSupplier;\n+        this.scaleDownDetected = NO_SCALE_DOWN_POSSIBLE;\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MachineLearning.MAX_MACHINE_MEMORY_PERCENT,\n+            this::setMaxMachineMemoryPercent);\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MachineLearning.MAX_OPEN_JOBS_PER_NODE, this::setMaxOpenJobs);\n+        clusterService.addLocalNodeMasterListener(this);\n+        clusterService.addLifecycleListener(new LifecycleListener() {\n+            @Override\n+            public void afterStart() {\n+                running = true;\n+                if (isMaster) {\n+                    mlMemoryTracker.asyncRefresh();\n+                }\n+            }\n+\n+            @Override\n+            public void beforeStop() {\n+                running = false;\n+            }\n+        });\n+    }\n+\n+    static OptionalLong getNodeJvmSize(DiscoveryNode node) {\n+        Map<String, String> nodeAttributes = node.getAttributes();\n+        OptionalLong value = OptionalLong.empty();\n+        String valueStr = nodeAttributes.get(MachineLearning.MAX_JVM_SIZE_NODE_ATTR);\n+        try {\n+            value = OptionalLong.of(Long.parseLong(valueStr));\n+        } catch (NumberFormatException e) {\n+            logger.debug(() -> new ParameterizedMessage(\n+                \"could not parse stored string value [{}] in node attribute [{}]\",\n+                valueStr,\n+                MachineLearning.MAX_JVM_SIZE_NODE_ATTR));\n+        }\n+        return value;\n+    }\n+\n+    static List<DiscoveryNode> getNodes(final ClusterState clusterState) {\n+        return clusterState.nodes()\n+            .mastersFirstStream()\n+            .filter(MachineLearning::isMlNode)\n+            .collect(Collectors.toList());\n+    }\n+\n+    /**\n+     * @param unassignedJobs The list of unassigned jobs\n+     * @param sizeFunction   Function providing the memory required for a job\n+     * @param defaultSize    The default memory size (if the sizeFunction returns null)\n+     * @param maxNumInQueue  The number of unassigned jobs allowed.\n+     * @return The capacity needed to reduce the length of `unassignedJobs` to `maxNumInQueue`\n+     */\n+    static Optional<NativeMemoryCapacity> requiredCapacityForUnassignedJobs(List<String> unassignedJobs,\n+                                                                            Function<String, Long> sizeFunction,\n+                                                                            long defaultSize,\n+                                                                            int maxNumInQueue) {\n+        List<Long> jobSizes = unassignedJobs\n+            .stream()\n+            // TODO do we want to verify memory requirements aren't stale? Or just consider `null` a fastpath?\n+            .map(sizeFunction)\n+            .map(l -> l == null ? defaultSize : l)\n+            .collect(Collectors.toList());\n+        // Only possible if unassignedJobs was empty.\n+        if (jobSizes.isEmpty()) {\n+            return Optional.empty();\n+        }\n+        jobSizes.sort(Comparator.comparingLong(Long::longValue).reversed());\n+        long tierMemory = 0L;\n+        long nodeMemory = jobSizes.get(0);\n+        Iterator<Long> iter = jobSizes.iterator();\n+        while (jobSizes.size() > maxNumInQueue && iter.hasNext()) {\n+            tierMemory += iter.next();\n+            iter.remove();\n+        }\n+        return Optional.of(new NativeMemoryCapacity(tierMemory, nodeMemory));\n+    }\n+\n+    private static Collection<PersistentTask<?>> anomalyDetectionTasks(PersistentTasksCustomMetadata tasksCustomMetadata) {\n+        if (tasksCustomMetadata == null) {\n+            return Collections.emptyList();\n+        }\n+\n+        return tasksCustomMetadata.findTasks(MlTasks.JOB_TASK_NAME,\n+            t -> getJobStateModifiedForReassignments(t).isAnyOf(JobState.OPENED, JobState.OPENING));\n+    }\n+\n+    private static Collection<PersistentTask<?>> dataframeAnalyticsTasks(PersistentTasksCustomMetadata tasksCustomMetadata) {\n+        if (tasksCustomMetadata == null) {\n+            return Collections.emptyList();\n+        }\n+\n+        return tasksCustomMetadata.findTasks(MlTasks.DATA_FRAME_ANALYTICS_TASK_NAME,\n+            t -> getDataFrameAnalyticsState(t).isAnyOf(DataFrameAnalyticsState.STARTED, DataFrameAnalyticsState.STARTING));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private static Collection<PersistentTask<DatafeedParams>> datafeedTasks(PersistentTasksCustomMetadata tasksCustomMetadata) {\n+        if (tasksCustomMetadata == null) {\n+            return Collections.emptyList();\n+        }\n+\n+        return tasksCustomMetadata.findTasks(MlTasks.DATAFEED_TASK_NAME, t -> true)\n+            .stream()\n+            .map(p -> (PersistentTask<DatafeedParams>) p)\n+            .collect(Collectors.toList());\n+    }\n+\n+    void setMaxMachineMemoryPercent(int maxMachineMemoryPercent) {\n+        this.maxMachineMemoryPercent = maxMachineMemoryPercent;\n+    }\n+\n+    void setMaxOpenJobs(int maxOpenJobs) {\n+        this.maxOpenJobs = maxOpenJobs;\n+    }\n+\n+    @Override\n+    public void onMaster() {\n+        isMaster = true;\n+        if (running) {\n+            mlMemoryTracker.asyncRefresh();\n+        }\n+    }\n+\n+    private void resetScaleDownCoolDown() {\n+        this.scaleDownDetected = NO_SCALE_DOWN_POSSIBLE;\n+    }\n+\n+    private boolean canScaleDown(TimeValue coolDown) {\n+        if (this.scaleDownDetected == NO_SCALE_DOWN_POSSIBLE) {\n+            return false;\n+        }\n+        return timeSupplier.get() - scaleDownDetected >= coolDown.millis();\n+    }\n+\n+    private boolean newScaleDownCheck() {\n+        return scaleDownDetected == NO_SCALE_DOWN_POSSIBLE;\n+    }\n+\n+    NativeMemoryCapacity currentScale(final List<DiscoveryNode> machineLearningNodes) {\n+        long[] mlMemory = machineLearningNodes.stream()\n+            .mapToLong(node -> NativeMemoryCalculator.allowedBytesForMl(node, maxMachineMemoryPercent, useAuto).orElse(0L))\n+            .toArray();\n+\n+        return new NativeMemoryCapacity(\n+            Arrays.stream(mlMemory).sum(),\n+            Arrays.stream(mlMemory).max().orElse(0L),\n+            // We assume that JVM size is universal, at least, the largest JVM indicates the largest node\n+            machineLearningNodes.stream()\n+                .map(MlAutoscalingDeciderService::getNodeJvmSize)\n+                .mapToLong(l -> l.orElse(0L))\n+                .boxed()\n+                .max(Long::compare)\n+                .orElse(null)\n+        );\n+    }\n+\n+    @Override\n+    public void offMaster() {\n+        isMaster = false;\n+    }\n+\n+    @Override\n+    public AutoscalingDeciderResult scale(MlAutoscalingDeciderConfiguration decider, AutoscalingDeciderContext context) {\n+        if (isMaster == false) {\n+            throw new IllegalArgumentException(\"request for scaling information is only allowed on the master node\");\n+        }\n+        final Duration memoryTrackingStale;\n+        long previousTimeStamp = this.lastTimeToScale;\n+        this.lastTimeToScale = this.timeSupplier.get();\n+        if (previousTimeStamp == 0L) {\n+            memoryTrackingStale = DEFAULT_MEMORY_REFRESH_RATE;\n+        } else {\n+            memoryTrackingStale = Duration.ofMillis(TimeValue.timeValueMinutes(1).millis() + this.lastTimeToScale - previousTimeStamp);\n+        }\n+\n+        final ClusterState clusterState = context.state();\n+\n+        PersistentTasksCustomMetadata tasks = clusterState.getMetadata().custom(PersistentTasksCustomMetadata.TYPE);\n+        Collection<PersistentTask<?>> anomalyDetectionTasks = anomalyDetectionTasks(tasks);\n+        Collection<PersistentTask<?>> dataframeAnalyticsTasks = dataframeAnalyticsTasks(tasks);\n+        final List<DiscoveryNode> nodes = getNodes(clusterState);\n+        Optional<NativeMemoryCapacity> futureFreedCapacity = calculateFutureFreedCapacity(tasks, memoryTrackingStale);\n+\n+        final List<String> waitingAnomalyJobs = anomalyDetectionTasks.stream()\n+            .filter(t -> AWAITING_LAZY_ASSIGNMENT.equals(t.getAssignment()))\n+            .map(t -> MlTasks.jobId(t.getId()))\n+            .collect(Collectors.toList());\n+        final List<String> waitingAnalyticsJobs = dataframeAnalyticsTasks.stream()\n+            .filter(t -> AWAITING_LAZY_ASSIGNMENT.equals(t.getAssignment()))\n+            .map(t -> MlTasks.dataFrameAnalyticsId(t.getId()))\n+            .collect(Collectors.toList());\n+\n+        final NativeMemoryCapacity currentScale = currentScale(nodes);\n+        final MlScalingReason.Builder reasonBuilder = MlScalingReason.builder()\n+            .setWaitingAnomalyJobs(waitingAnomalyJobs)\n+            .setWaitingAnalyticsJobs(waitingAnalyticsJobs)\n+            .setCurrentMlCapacity(currentScale.autoscalingCapacity(maxMachineMemoryPercent, useAuto))\n+            .setPassedConfiguration(decider);\n+\n+        final Optional<AutoscalingDeciderResult> scaleUpDecision = checkForScaleUp(decider,\n+            waitingAnomalyJobs,\n+            waitingAnalyticsJobs,\n+            futureFreedCapacity.orElse(null),\n+            currentScale,\n+            reasonBuilder);\n+\n+        if (scaleUpDecision.isPresent()) {\n+            resetScaleDownCoolDown();\n+            return scaleUpDecision.get();\n+        }\n+        if (waitingAnalyticsJobs.isEmpty() == false || waitingAnomalyJobs.isEmpty() == false) {\n+            // We don't want to continue to consider a scale down if there are now waiting jobs\n+            resetScaleDownCoolDown();\n+            return new AutoscalingDeciderResult(\n+                context.currentCapacity(),\n+                reasonBuilder\n+                    .setSimpleReason(\"Passing currently perceived capacity as there are analytics and anomaly jobs in the queue, \" +\n+                        \"but the number in the queue is less than the configured maximum allowed.\")\n+                    .build());\n+        }\n+        if (mlMemoryTracker.isRecentlyRefreshed(memoryTrackingStale) == false) {\n+            return buildDecisionAndRequestRefresh(reasonBuilder);\n+        }\n+\n+        long largestJob = Math.max(\n+            anomalyDetectionTasks.stream()\n+                .filter(PersistentTask::isAssigned)\n+                // Memory SHOULD be recently refreshed, so in our current state, we should at least have an idea of the memory used\n+                .mapToLong(this::getAnomalyMemoryRequirement)\n+                .max()\n+                .orElse(0L),\n+            dataframeAnalyticsTasks.stream()\n+                .filter(PersistentTask::isAssigned)\n+                // Memory SHOULD be recently refreshed, so in our current state, we should at least have an idea of the memory used\n+                .mapToLong(this::getAnalyticsMemoryRequirement)\n+                .max()\n+                .orElse(0L));\n+\n+        final Optional<AutoscalingDeciderResult> scaleDownDecision =\n+            checkForScaleDown(nodes, clusterState, largestJob, currentScale, reasonBuilder);\n+\n+        if (scaleDownDecision.isPresent()) {\n+            if (newScaleDownCheck()) {\n+                scaleDownDetected = timeSupplier.get();\n+            }\n+            if (canScaleDown(decider.getDownScaleDelay())) {\n+                return scaleDownDecision.get();\n+            }\n+            return new AutoscalingDeciderResult(\n+                context.currentCapacity(),\n+                reasonBuilder\n+                    .setSimpleReason(\n+                        \"Passing currently perceived capacity as configured down scale delay has not be satisfied; configured delay [\"\n+                            + decider.getDownScaleDelay().millis()\n+                            + \"] last detected scale down event [\"\n+                            + scaleDownDetected\n+                            + \"]\")\n+                    .build());\n+        }\n+\n+        return new AutoscalingDeciderResult(context.currentCapacity(),\n+            reasonBuilder\n+                .setSimpleReason(\"Passing currently perceived capacity as no scaling changes were detected to be possible\")\n+                .build());\n+    }\n+\n+    Optional<AutoscalingDeciderResult> checkForScaleUp(MlAutoscalingDeciderConfiguration decider,\n+                                                       List<String> waitingAnomalyJobs,\n+                                                       List<String> waitingAnalyticsJobs,\n+                                                       @Nullable NativeMemoryCapacity futureFreedCapacity,\n+                                                       NativeMemoryCapacity currentScale,\n+                                                       MlScalingReason.Builder reasonBuilder) {\n+\n+        // Are we in breach of maximum waiting jobs?\n+        if (waitingAnalyticsJobs.size() > decider.getNumAnalyticsJobsInQueue()\n+            || waitingAnomalyJobs.size() > decider.getNumAnomalyJobsInQueue()) {\n+            NativeMemoryCapacity updatedCapacity = NativeMemoryCapacity.from(currentScale);\n+            Optional<NativeMemoryCapacity> analyticsCapacity = requiredCapacityForUnassignedJobs(waitingAnalyticsJobs,\n+                this::getAnalyticsMemoryRequirement,\n+                // TODO Better default???\n+                AnalysisLimits.DEFAULT_MODEL_MEMORY_LIMIT_MB,\n+                decider.getNumAnalyticsJobsInQueue());\n+            Optional<NativeMemoryCapacity> anomalyCapacity = requiredCapacityForUnassignedJobs(waitingAnomalyJobs,\n+                this::getAnomalyMemoryRequirement,\n+                AnalysisLimits.DEFAULT_MODEL_MEMORY_LIMIT_MB,\n+                decider.getNumAnomalyJobsInQueue());\n+\n+            updatedCapacity.merge(anomalyCapacity.orElse(NativeMemoryCapacity.ZERO))\n+                .merge(analyticsCapacity.orElse(NativeMemoryCapacity.ZERO));\n+            return Optional.of(new AutoscalingDeciderResult(\n+                updatedCapacity.autoscalingCapacity(maxMachineMemoryPercent, useAuto),\n+                reasonBuilder.setSimpleReason(\"requesting scale up as number of jobs in queues exceeded configured limit\").build()));\n+        }\n+\n+        // Could the currently waiting jobs ever be assigned?\n+        if (waitingAnalyticsJobs.isEmpty() == false || waitingAnomalyJobs.isEmpty() == false) {\n+            // we are unable to determine new tier size, but maybe we can see if our nodes are big enough.\n+            if (futureFreedCapacity == null) {\n+                Optional<Long> maxSize = Stream.concat(\n+                    waitingAnalyticsJobs.stream().map(mlMemoryTracker::getDataFrameAnalyticsJobMemoryRequirement),\n+                    waitingAnomalyJobs.stream().map(mlMemoryTracker::getAnomalyDetectorJobMemoryRequirement))\n+                    .filter(Objects::nonNull)\n+                    .max(Long::compareTo);\n+                if (maxSize.isPresent() && maxSize.get() > currentScale.getNode()) {\n+                    return Optional.of(new AutoscalingDeciderResult(\n+                        new NativeMemoryCapacity(Math.max(currentScale.getTier(), maxSize.get()), maxSize.get())\n+                            .autoscalingCapacity(maxMachineMemoryPercent, useAuto),\n+                        reasonBuilder.setSimpleReason(\"requesting scale up as there is no node large enough to handle queued jobs\")\n+                            .build()));\n+                }\n+                // we have no info, allow the caller to make the appropriate action, probably returning a no_scale\n+                return Optional.empty();\n+            }\n+            long newTierNeeded = 0L;\n+            // could any of the nodes actually run the job?\n+            long newNodeMax = currentScale.getNode();\n+            for (String analyticsJob : waitingAnalyticsJobs) {\n+                Long requiredMemory = mlMemoryTracker.getDataFrameAnalyticsJobMemoryRequirement(analyticsJob);\n+                // it is OK to continue here as we have not breached our queuing limit\n+                if (requiredMemory == null) {\n+                    continue;\n+                }\n+                // Is there \"future capacity\" on a node that could run this job? If not, we need that much more in the tier.\n+                if (futureFreedCapacity.getNode() < requiredMemory) {\n+                    newTierNeeded = Math.max(requiredMemory, newTierNeeded);\n+                }\n+                newNodeMax = Math.max(newNodeMax, requiredMemory);\n+            }\n+            for (String anomalyJob : waitingAnomalyJobs) {\n+                Long requiredMemory = mlMemoryTracker.getAnomalyDetectorJobMemoryRequirement(anomalyJob);\n+                // it is OK to continue here as we have not breached our queuing limit\n+                if (requiredMemory == null) {\n+                    continue;\n+                }\n+                // Is there \"future capacity\" on a node that could run this job? If not, we need that much more in the tier.\n+                if (futureFreedCapacity.getNode() < requiredMemory) {\n+                    newTierNeeded = Math.max(requiredMemory, newTierNeeded);\n+                }\n+                newNodeMax = Math.max(newNodeMax, requiredMemory);\n+            }\n+            if (newNodeMax > currentScale.getNode() || newTierNeeded > 0L) {\n+                NativeMemoryCapacity newCapacity = new NativeMemoryCapacity(newTierNeeded, newNodeMax);\n+                return Optional.of(new AutoscalingDeciderResult(\n+                    // We need more memory in the tier, or our individual node size requirements has increased\n+                    NativeMemoryCapacity.from(currentScale).merge(newCapacity).autoscalingCapacity(maxMachineMemoryPercent, useAuto),\n+                    reasonBuilder\n+                        .setSimpleReason(\"scaling up as adequate space would not automatically become available when running jobs finish\")\n+                        .build()\n+                ));\n+            }\n+        }\n+\n+        return Optional.empty();\n+    }\n+\n+    // This calculates the the following the potentially automatically free capacity of sometime in the future\n+    // Since jobs with lookback only datafeeds, and data frame analytics jobs all have some potential future end date\n+    // we can assume (without user intervention) that these will eventually stop and free their currently occupied resources.\n+    //\n+    // The capacity is as follows:\n+    //  tier: The sum total of the resources that will be removed\n+    //  node: The largest block of memory that will be freed on a given node.\n+    //      - If > 1 \"batch\" ml tasks are running on the same node, we sum their resources.\n+    Optional<NativeMemoryCapacity> calculateFutureFreedCapacity(PersistentTasksCustomMetadata tasks, Duration jobMemoryExpiry) {\n+        final List<PersistentTask<DatafeedParams>> jobsWithLookbackDatafeeds = datafeedTasks(tasks).stream()\n+            .filter(t -> t.getParams().getEndTime() != null && t.getExecutorNode() != null)\n+            .collect(Collectors.toList());\n+        final List<PersistentTask<?>> assignedAnalyticsJobs = dataframeAnalyticsTasks(tasks).stream()\n+            .filter(t -> t.getExecutorNode() != null)\n+            .collect(Collectors.toList());\n+\n+        if (jobsWithLookbackDatafeeds.isEmpty() && assignedAnalyticsJobs.isEmpty()) {\n+            return Optional.of(NativeMemoryCapacity.ZERO);\n+        }\n+        if (mlMemoryTracker.isRecentlyRefreshed(jobMemoryExpiry) == false) {\n+            return Optional.empty();\n+        }\n+        // What is the largest chunk of memory that could be freed on a node in the future\n+        Map<String, Long> maxNodeBytes = new HashMap<>();\n+        for (PersistentTask<DatafeedParams> lookbackOnlyDf : jobsWithLookbackDatafeeds) {\n+            Long jobSize = mlMemoryTracker.getAnomalyDetectorJobMemoryRequirement(lookbackOnlyDf.getParams().getJobId());\n+            if (jobSize == null) {\n+                return Optional.empty();\n+            }\n+            maxNodeBytes.compute(lookbackOnlyDf.getExecutorNode(), (_k, v) -> v == null ? jobSize : jobSize + v);\n+        }\n+        for (PersistentTask<?> task : assignedAnalyticsJobs) {\n+            Long jobSize = mlMemoryTracker.getDataFrameAnalyticsJobMemoryRequirement(MlTasks.dataFrameAnalyticsId(task.getId()));\n+            if (jobSize == null) {\n+                return Optional.empty();\n+            }\n+            maxNodeBytes.compute(task.getExecutorNode(), (_k, v) -> v == null ? jobSize : jobSize + v);\n+        }\n+        return Optional.of(new NativeMemoryCapacity(\n+            maxNodeBytes.values().stream().mapToLong(Long::longValue).sum(),\n+            maxNodeBytes.values().stream().mapToLong(Long::longValue).max().orElse(0L)));\n+    }\n+\n+    private AutoscalingDeciderResult buildDecisionAndRequestRefresh(MlScalingReason.Builder reasonBuilder) {\n+        mlMemoryTracker.asyncRefresh();\n+        return new AutoscalingDeciderResult(null, reasonBuilder.setSimpleReason(MEMORY_STALE).build());\n+    }\n+\n+    private Long getAnalyticsMemoryRequirement(String analyticsId) {\n+        return mlMemoryTracker.getDataFrameAnalyticsJobMemoryRequirement(analyticsId);\n+    }\n+\n+    private Long getAnalyticsMemoryRequirement(PersistentTask<?> task) {\n+        return getAnalyticsMemoryRequirement(MlTasks.dataFrameAnalyticsId(task.getId()));\n+    }\n+\n+    private Long getAnomalyMemoryRequirement(String anomalyId) {\n+        return mlMemoryTracker.getAnomalyDetectorJobMemoryRequirement(anomalyId);\n+    }\n+\n+    private Long getAnomalyMemoryRequirement(PersistentTask<?> task) {\n+        return getAnomalyMemoryRequirement(MlTasks.jobId(task.getId()));\n+    }\n+\n+    Optional<AutoscalingDeciderResult> checkForScaleDown(List<DiscoveryNode> nodes,\n+                                                         ClusterState clusterState,\n+                                                         long largestJob,\n+                                                         NativeMemoryCapacity currentCapacity,\n+                                                         MlScalingReason.Builder reasonBuilder) {\n+        List<NodeLoad> nodeLoads = new ArrayList<>();\n+        boolean isMemoryAccurateFlag = true;\n+        for (DiscoveryNode node : nodes) {\n+            NodeLoad nodeLoad = nodeLoadDetector.detectNodeLoad(clusterState,\n+                true,\n+                node,\n+                maxOpenJobs,\n+                maxMachineMemoryPercent,\n+                true,\n+                useAuto);\n+            if (nodeLoad.getError() != null) {\n+                logger.warn(\"[{}] failed to gather node load limits, failure [{}]\", node.getId(), nodeLoad.getError());\n+                return Optional.empty();\n+            }\n+            nodeLoads.add(nodeLoad);\n+            isMemoryAccurateFlag = isMemoryAccurateFlag && nodeLoad.isUseMemory();\n+        }\n+        // Even if we verify that memory usage is up today before checking node capacity, we could still run into stale information.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94a969e1b2b23eaa6c7e7b8814dc1471f0dde032"}, "originalPosition": 530}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "15a8a7b613ef44fd7cad1484d35a6b59eb6f56b0", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/15a8a7b613ef44fd7cad1484d35a6b59eb6f56b0", "committedDate": "2020-11-16T14:55:00Z", "message": "addressing PR concerns"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6536073d0983c0cc4c49f30c41392a2b2a23fff0", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/6536073d0983c0cc4c49f30c41392a2b2a23fff0", "committedDate": "2020-11-16T14:55:15Z", "message": "Merge remote-tracking branch 'upstream/master' into feature/ml-autoscaling-integration"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMyMjM3MzIy", "url": "https://github.com/elastic/elasticsearch/pull/59309#pullrequestreview-532237322", "createdAt": "2020-11-17T10:46:46Z", "commit": {"oid": "6536073d0983c0cc4c49f30c41392a2b2a23fff0"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QxMDo0Njo0NlrOH0u_rw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QxMTowNjowN1rOH0vuUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTA1Nzk2Nw==", "bodyText": "Am I correct that getting here should be incredibly rare, because we checked isRecentlyRefreshed near the beginning of the scale method, so we'll only get here if the definition of \"recently\" was breached while the code of the scale method was running?\nIf this is correct then we should log a warning here, because if some strange bug causes us to get here more often then we'll want to know when dealing with the \"why doesn't my cluster scale\" support case.", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r525057967", "createdAt": "2020-11-17T10:46:46Z", "author": {"login": "droberts195"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/autoscaling/MlAutoscalingDeciderService.java", "diffHunk": "@@ -0,0 +1,565 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.ml.autoscaling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.LocalNodeMasterListener;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.component.LifecycleListener;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.persistent.PersistentTasksCustomMetadata;\n+import org.elasticsearch.persistent.PersistentTasksCustomMetadata.PersistentTask;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderContext;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderResult;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderService;\n+import org.elasticsearch.xpack.core.ml.MlTasks;\n+import org.elasticsearch.xpack.core.ml.action.StartDatafeedAction.DatafeedParams;\n+import org.elasticsearch.xpack.core.ml.dataframe.DataFrameAnalyticsState;\n+import org.elasticsearch.xpack.core.ml.job.config.JobState;\n+import org.elasticsearch.xpack.ml.MachineLearning;\n+import org.elasticsearch.xpack.ml.job.NodeLoad;\n+import org.elasticsearch.xpack.ml.job.NodeLoadDetector;\n+import org.elasticsearch.xpack.ml.process.MlMemoryTracker;\n+import org.elasticsearch.xpack.ml.utils.NativeMemoryCalculator;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.OptionalLong;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.elasticsearch.xpack.core.ml.MlTasks.getDataFrameAnalyticsState;\n+import static org.elasticsearch.xpack.core.ml.MlTasks.getJobStateModifiedForReassignments;\n+import static org.elasticsearch.xpack.ml.job.JobNodeSelector.AWAITING_LAZY_ASSIGNMENT;\n+\n+public class MlAutoscalingDeciderService implements AutoscalingDeciderService<MlAutoscalingDeciderConfiguration>,\n+    LocalNodeMasterListener {\n+\n+    private static final Logger logger = LogManager.getLogger(MlAutoscalingDeciderService.class);\n+    private static final Duration DEFAULT_MEMORY_REFRESH_RATE = Duration.ofMinutes(15);\n+    private static final String MEMORY_STALE = \"unable to make scaling decision as job memory requirements are stale\";\n+    private static final long NO_SCALE_DOWN_POSSIBLE = -1L;\n+\n+    private final NodeLoadDetector nodeLoadDetector;\n+    private final MlMemoryTracker mlMemoryTracker;\n+    private final Supplier<Long> timeSupplier;\n+    private final boolean useAuto;\n+\n+    private volatile boolean isMaster;\n+    private volatile boolean running;\n+    private volatile int maxMachineMemoryPercent;\n+    private volatile int maxOpenJobs;\n+    private volatile long lastTimeToScale;\n+    private volatile long scaleDownDetected;\n+\n+    public MlAutoscalingDeciderService(MlMemoryTracker memoryTracker, Settings settings, ClusterService clusterService) {\n+        this(new NodeLoadDetector(memoryTracker), settings, clusterService, System::currentTimeMillis);\n+    }\n+\n+    MlAutoscalingDeciderService(NodeLoadDetector nodeLoadDetector,\n+                                Settings settings,\n+                                ClusterService clusterService,\n+                                Supplier<Long> timeSupplier) {\n+        this.nodeLoadDetector = nodeLoadDetector;\n+        this.mlMemoryTracker = nodeLoadDetector.getMlMemoryTracker();\n+        this.maxMachineMemoryPercent = MachineLearning.MAX_MACHINE_MEMORY_PERCENT.get(settings);\n+        this.maxOpenJobs = MachineLearning.MAX_OPEN_JOBS_PER_NODE.get(settings);\n+        this.useAuto = MachineLearning.USE_AUTO_MACHINE_MEMORY_PERCENT.get(settings);\n+        this.timeSupplier = timeSupplier;\n+        this.scaleDownDetected = NO_SCALE_DOWN_POSSIBLE;\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MachineLearning.MAX_MACHINE_MEMORY_PERCENT,\n+            this::setMaxMachineMemoryPercent);\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MachineLearning.MAX_OPEN_JOBS_PER_NODE, this::setMaxOpenJobs);\n+        clusterService.addLocalNodeMasterListener(this);\n+        clusterService.addLifecycleListener(new LifecycleListener() {\n+            @Override\n+            public void afterStart() {\n+                running = true;\n+                if (isMaster) {\n+                    mlMemoryTracker.asyncRefresh();\n+                }\n+            }\n+\n+            @Override\n+            public void beforeStop() {\n+                running = false;\n+            }\n+        });\n+    }\n+\n+    static OptionalLong getNodeJvmSize(DiscoveryNode node) {\n+        Map<String, String> nodeAttributes = node.getAttributes();\n+        OptionalLong value = OptionalLong.empty();\n+        String valueStr = nodeAttributes.get(MachineLearning.MAX_JVM_SIZE_NODE_ATTR);\n+        try {\n+            value = OptionalLong.of(Long.parseLong(valueStr));\n+        } catch (NumberFormatException e) {\n+            logger.debug(() -> new ParameterizedMessage(\n+                \"could not parse stored string value [{}] in node attribute [{}]\",\n+                valueStr,\n+                MachineLearning.MAX_JVM_SIZE_NODE_ATTR));\n+        }\n+        return value;\n+    }\n+\n+    static List<DiscoveryNode> getNodes(final ClusterState clusterState) {\n+        return clusterState.nodes()\n+            .mastersFirstStream()\n+            .filter(MachineLearning::isMlNode)\n+            .collect(Collectors.toList());\n+    }\n+\n+    /**\n+     * @param unassignedJobs The list of unassigned jobs\n+     * @param sizeFunction   Function providing the memory required for a job\n+     * @param maxNumInQueue  The number of unassigned jobs allowed.\n+     * @return The capacity needed to reduce the length of `unassignedJobs` to `maxNumInQueue`\n+     */\n+    static Optional<NativeMemoryCapacity> requiredCapacityForUnassignedJobs(List<String> unassignedJobs,\n+                                                                            Function<String, Long> sizeFunction,\n+                                                                            int maxNumInQueue) {\n+        List<Long> jobSizes = unassignedJobs\n+            .stream()\n+            // TODO do we want to verify memory requirements aren't stale? Or just consider `null` a fastpath?\n+            .map(sizeFunction)\n+            .map(l -> l == null ? 0L : l)\n+            .collect(Collectors.toList());\n+        // Only possible if unassignedJobs was empty.\n+        if (jobSizes.isEmpty()) {\n+            return Optional.empty();\n+        }\n+        jobSizes.sort(Comparator.comparingLong(Long::longValue).reversed());\n+        long tierMemory = 0L;\n+        long nodeMemory = jobSizes.get(0);\n+        Iterator<Long> iter = jobSizes.iterator();\n+        while (jobSizes.size() > maxNumInQueue && iter.hasNext()) {\n+            tierMemory += iter.next();\n+            iter.remove();\n+        }\n+        return Optional.of(new NativeMemoryCapacity(tierMemory, nodeMemory));\n+    }\n+\n+    private static Collection<PersistentTask<?>> anomalyDetectionTasks(PersistentTasksCustomMetadata tasksCustomMetadata) {\n+        if (tasksCustomMetadata == null) {\n+            return Collections.emptyList();\n+        }\n+\n+        return tasksCustomMetadata.findTasks(MlTasks.JOB_TASK_NAME,\n+            t -> getJobStateModifiedForReassignments(t).isAnyOf(JobState.OPENED, JobState.OPENING));\n+    }\n+\n+    private static Collection<PersistentTask<?>> dataframeAnalyticsTasks(PersistentTasksCustomMetadata tasksCustomMetadata) {\n+        if (tasksCustomMetadata == null) {\n+            return Collections.emptyList();\n+        }\n+\n+        return tasksCustomMetadata.findTasks(MlTasks.DATA_FRAME_ANALYTICS_TASK_NAME,\n+            t -> getDataFrameAnalyticsState(t).isAnyOf(DataFrameAnalyticsState.STARTED, DataFrameAnalyticsState.STARTING));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private static Collection<PersistentTask<DatafeedParams>> datafeedTasks(PersistentTasksCustomMetadata tasksCustomMetadata) {\n+        if (tasksCustomMetadata == null) {\n+            return Collections.emptyList();\n+        }\n+\n+        return tasksCustomMetadata.findTasks(MlTasks.DATAFEED_TASK_NAME, t -> true)\n+            .stream()\n+            .map(p -> (PersistentTask<DatafeedParams>) p)\n+            .collect(Collectors.toList());\n+    }\n+\n+    void setMaxMachineMemoryPercent(int maxMachineMemoryPercent) {\n+        this.maxMachineMemoryPercent = maxMachineMemoryPercent;\n+    }\n+\n+    void setMaxOpenJobs(int maxOpenJobs) {\n+        this.maxOpenJobs = maxOpenJobs;\n+    }\n+\n+    @Override\n+    public void onMaster() {\n+        isMaster = true;\n+        if (running) {\n+            mlMemoryTracker.asyncRefresh();\n+        }\n+    }\n+\n+    private void resetScaleDownCoolDown() {\n+        this.scaleDownDetected = NO_SCALE_DOWN_POSSIBLE;\n+    }\n+\n+    private boolean canScaleDown(TimeValue coolDown) {\n+        if (this.scaleDownDetected == NO_SCALE_DOWN_POSSIBLE) {\n+            return false;\n+        }\n+        return timeSupplier.get() - scaleDownDetected >= coolDown.millis();\n+    }\n+\n+    private boolean newScaleDownCheck() {\n+        return scaleDownDetected == NO_SCALE_DOWN_POSSIBLE;\n+    }\n+\n+    NativeMemoryCapacity currentScale(final List<DiscoveryNode> machineLearningNodes) {\n+        long[] mlMemory = machineLearningNodes.stream()\n+            .mapToLong(node -> NativeMemoryCalculator.allowedBytesForMl(node, maxMachineMemoryPercent, useAuto).orElse(0L))\n+            .toArray();\n+\n+        return new NativeMemoryCapacity(\n+            Arrays.stream(mlMemory).sum(),\n+            Arrays.stream(mlMemory).max().orElse(0L),\n+            // We assume that JVM size is universal, at least, the largest JVM indicates the largest node\n+            machineLearningNodes.stream()\n+                .map(MlAutoscalingDeciderService::getNodeJvmSize)\n+                .mapToLong(l -> l.orElse(0L))\n+                .boxed()\n+                .max(Long::compare)\n+                .orElse(null)\n+        );\n+    }\n+\n+    @Override\n+    public void offMaster() {\n+        isMaster = false;\n+    }\n+\n+    @Override\n+    public AutoscalingDeciderResult scale(MlAutoscalingDeciderConfiguration decider, AutoscalingDeciderContext context) {\n+        if (isMaster == false) {\n+            throw new IllegalArgumentException(\"request for scaling information is only allowed on the master node\");\n+        }\n+        final Duration memoryTrackingStale;\n+        long previousTimeStamp = this.lastTimeToScale;\n+        this.lastTimeToScale = this.timeSupplier.get();\n+        if (previousTimeStamp == 0L) {\n+            memoryTrackingStale = DEFAULT_MEMORY_REFRESH_RATE;\n+        } else {\n+            memoryTrackingStale = Duration.ofMillis(TimeValue.timeValueMinutes(1).millis() + this.lastTimeToScale - previousTimeStamp);\n+        }\n+\n+        final ClusterState clusterState = context.state();\n+\n+        PersistentTasksCustomMetadata tasks = clusterState.getMetadata().custom(PersistentTasksCustomMetadata.TYPE);\n+        Collection<PersistentTask<?>> anomalyDetectionTasks = anomalyDetectionTasks(tasks);\n+        Collection<PersistentTask<?>> dataframeAnalyticsTasks = dataframeAnalyticsTasks(tasks);\n+        final List<DiscoveryNode> nodes = getNodes(clusterState);\n+        Optional<NativeMemoryCapacity> futureFreedCapacity = calculateFutureFreedCapacity(tasks, memoryTrackingStale);\n+\n+        final List<String> waitingAnomalyJobs = anomalyDetectionTasks.stream()\n+            .filter(t -> AWAITING_LAZY_ASSIGNMENT.equals(t.getAssignment()))\n+            .map(t -> MlTasks.jobId(t.getId()))\n+            .collect(Collectors.toList());\n+        final List<String> waitingAnalyticsJobs = dataframeAnalyticsTasks.stream()\n+            .filter(t -> AWAITING_LAZY_ASSIGNMENT.equals(t.getAssignment()))\n+            .map(t -> MlTasks.dataFrameAnalyticsId(t.getId()))\n+            .collect(Collectors.toList());\n+\n+        final NativeMemoryCapacity currentScale = currentScale(nodes);\n+        final MlScalingReason.Builder reasonBuilder = MlScalingReason.builder()\n+            .setWaitingAnomalyJobs(waitingAnomalyJobs)\n+            .setWaitingAnalyticsJobs(waitingAnalyticsJobs)\n+            .setCurrentMlCapacity(currentScale.autoscalingCapacity(maxMachineMemoryPercent, useAuto))\n+            .setPassedConfiguration(decider);\n+\n+        final Optional<AutoscalingDeciderResult> scaleUpDecision = checkForScaleUp(decider,\n+            waitingAnomalyJobs,\n+            waitingAnalyticsJobs,\n+            futureFreedCapacity.orElse(null),\n+            currentScale,\n+            reasonBuilder);\n+\n+        if (scaleUpDecision.isPresent()) {\n+            resetScaleDownCoolDown();\n+            return scaleUpDecision.get();\n+        }\n+        if (waitingAnalyticsJobs.isEmpty() == false || waitingAnomalyJobs.isEmpty() == false) {\n+            // We don't want to continue to consider a scale down if there are now waiting jobs\n+            resetScaleDownCoolDown();\n+            noScaleResultOrRefresh(reasonBuilder, memoryTrackingStale, new AutoscalingDeciderResult(\n+                context.currentCapacity(),\n+                reasonBuilder\n+                    .setSimpleReason(\"Passing currently perceived capacity as there are analytics and anomaly jobs in the queue, \" +\n+                        \"but the number in the queue is less than the configured maximum allowed.\")\n+                    .build()));\n+        }\n+        if (mlMemoryTracker.isRecentlyRefreshed(memoryTrackingStale) == false) {\n+            return buildDecisionAndRequestRefresh(reasonBuilder);\n+        }\n+\n+        long largestJob = Math.max(\n+            anomalyDetectionTasks.stream()\n+                .filter(PersistentTask::isAssigned)\n+                // Memory SHOULD be recently refreshed, so in our current state, we should at least have an idea of the memory used\n+                .mapToLong(this::getAnomalyMemoryRequirement)\n+                .max()\n+                .orElse(0L),\n+            dataframeAnalyticsTasks.stream()\n+                .filter(PersistentTask::isAssigned)\n+                // Memory SHOULD be recently refreshed, so in our current state, we should at least have an idea of the memory used\n+                .mapToLong(this::getAnalyticsMemoryRequirement)\n+                .max()\n+                .orElse(0L));\n+\n+        final Optional<AutoscalingDeciderResult> scaleDownDecision =\n+            checkForScaleDown(nodes, clusterState, largestJob, currentScale, reasonBuilder);\n+\n+        if (scaleDownDecision.isPresent()) {\n+            if (newScaleDownCheck()) {\n+                scaleDownDetected = timeSupplier.get();\n+            }\n+            if (canScaleDown(decider.getDownScaleDelay())) {\n+                return scaleDownDecision.get();\n+            }\n+            return new AutoscalingDeciderResult(\n+                context.currentCapacity(),\n+                reasonBuilder\n+                    .setSimpleReason(\n+                        \"Passing currently perceived capacity as configured down scale delay has not be satisfied; configured delay [\"\n+                            + decider.getDownScaleDelay().millis()\n+                            + \"] last detected scale down event [\"\n+                            + scaleDownDetected\n+                            + \"]\")\n+                    .build());\n+        }\n+\n+        return noScaleResultOrRefresh(reasonBuilder, memoryTrackingStale, new AutoscalingDeciderResult(context.currentCapacity(),\n+            reasonBuilder\n+                .setSimpleReason(\"Passing currently perceived capacity as no scaling changes were detected to be possible\")\n+                .build()));\n+    }\n+\n+    AutoscalingDeciderResult noScaleResultOrRefresh(MlScalingReason.Builder reasonBuilder,\n+                                                    Duration memoryTrackingStale,\n+                                                    AutoscalingDeciderResult potentialResult) {\n+        if (mlMemoryTracker.isRecentlyRefreshed(memoryTrackingStale) == false) {\n+            return buildDecisionAndRequestRefresh(reasonBuilder);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6536073d0983c0cc4c49f30c41392a2b2a23fff0"}, "originalPosition": 356}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTA2MDg1OA==", "bodyText": "Please remove this comment, as staleness is already being checked elsewhere.", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r525060858", "createdAt": "2020-11-17T10:51:16Z", "author": {"login": "droberts195"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/autoscaling/MlAutoscalingDeciderService.java", "diffHunk": "@@ -0,0 +1,565 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.ml.autoscaling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.LocalNodeMasterListener;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.component.LifecycleListener;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.persistent.PersistentTasksCustomMetadata;\n+import org.elasticsearch.persistent.PersistentTasksCustomMetadata.PersistentTask;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderContext;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderResult;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderService;\n+import org.elasticsearch.xpack.core.ml.MlTasks;\n+import org.elasticsearch.xpack.core.ml.action.StartDatafeedAction.DatafeedParams;\n+import org.elasticsearch.xpack.core.ml.dataframe.DataFrameAnalyticsState;\n+import org.elasticsearch.xpack.core.ml.job.config.JobState;\n+import org.elasticsearch.xpack.ml.MachineLearning;\n+import org.elasticsearch.xpack.ml.job.NodeLoad;\n+import org.elasticsearch.xpack.ml.job.NodeLoadDetector;\n+import org.elasticsearch.xpack.ml.process.MlMemoryTracker;\n+import org.elasticsearch.xpack.ml.utils.NativeMemoryCalculator;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.OptionalLong;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.elasticsearch.xpack.core.ml.MlTasks.getDataFrameAnalyticsState;\n+import static org.elasticsearch.xpack.core.ml.MlTasks.getJobStateModifiedForReassignments;\n+import static org.elasticsearch.xpack.ml.job.JobNodeSelector.AWAITING_LAZY_ASSIGNMENT;\n+\n+public class MlAutoscalingDeciderService implements AutoscalingDeciderService<MlAutoscalingDeciderConfiguration>,\n+    LocalNodeMasterListener {\n+\n+    private static final Logger logger = LogManager.getLogger(MlAutoscalingDeciderService.class);\n+    private static final Duration DEFAULT_MEMORY_REFRESH_RATE = Duration.ofMinutes(15);\n+    private static final String MEMORY_STALE = \"unable to make scaling decision as job memory requirements are stale\";\n+    private static final long NO_SCALE_DOWN_POSSIBLE = -1L;\n+\n+    private final NodeLoadDetector nodeLoadDetector;\n+    private final MlMemoryTracker mlMemoryTracker;\n+    private final Supplier<Long> timeSupplier;\n+    private final boolean useAuto;\n+\n+    private volatile boolean isMaster;\n+    private volatile boolean running;\n+    private volatile int maxMachineMemoryPercent;\n+    private volatile int maxOpenJobs;\n+    private volatile long lastTimeToScale;\n+    private volatile long scaleDownDetected;\n+\n+    public MlAutoscalingDeciderService(MlMemoryTracker memoryTracker, Settings settings, ClusterService clusterService) {\n+        this(new NodeLoadDetector(memoryTracker), settings, clusterService, System::currentTimeMillis);\n+    }\n+\n+    MlAutoscalingDeciderService(NodeLoadDetector nodeLoadDetector,\n+                                Settings settings,\n+                                ClusterService clusterService,\n+                                Supplier<Long> timeSupplier) {\n+        this.nodeLoadDetector = nodeLoadDetector;\n+        this.mlMemoryTracker = nodeLoadDetector.getMlMemoryTracker();\n+        this.maxMachineMemoryPercent = MachineLearning.MAX_MACHINE_MEMORY_PERCENT.get(settings);\n+        this.maxOpenJobs = MachineLearning.MAX_OPEN_JOBS_PER_NODE.get(settings);\n+        this.useAuto = MachineLearning.USE_AUTO_MACHINE_MEMORY_PERCENT.get(settings);\n+        this.timeSupplier = timeSupplier;\n+        this.scaleDownDetected = NO_SCALE_DOWN_POSSIBLE;\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MachineLearning.MAX_MACHINE_MEMORY_PERCENT,\n+            this::setMaxMachineMemoryPercent);\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MachineLearning.MAX_OPEN_JOBS_PER_NODE, this::setMaxOpenJobs);\n+        clusterService.addLocalNodeMasterListener(this);\n+        clusterService.addLifecycleListener(new LifecycleListener() {\n+            @Override\n+            public void afterStart() {\n+                running = true;\n+                if (isMaster) {\n+                    mlMemoryTracker.asyncRefresh();\n+                }\n+            }\n+\n+            @Override\n+            public void beforeStop() {\n+                running = false;\n+            }\n+        });\n+    }\n+\n+    static OptionalLong getNodeJvmSize(DiscoveryNode node) {\n+        Map<String, String> nodeAttributes = node.getAttributes();\n+        OptionalLong value = OptionalLong.empty();\n+        String valueStr = nodeAttributes.get(MachineLearning.MAX_JVM_SIZE_NODE_ATTR);\n+        try {\n+            value = OptionalLong.of(Long.parseLong(valueStr));\n+        } catch (NumberFormatException e) {\n+            logger.debug(() -> new ParameterizedMessage(\n+                \"could not parse stored string value [{}] in node attribute [{}]\",\n+                valueStr,\n+                MachineLearning.MAX_JVM_SIZE_NODE_ATTR));\n+        }\n+        return value;\n+    }\n+\n+    static List<DiscoveryNode> getNodes(final ClusterState clusterState) {\n+        return clusterState.nodes()\n+            .mastersFirstStream()\n+            .filter(MachineLearning::isMlNode)\n+            .collect(Collectors.toList());\n+    }\n+\n+    /**\n+     * @param unassignedJobs The list of unassigned jobs\n+     * @param sizeFunction   Function providing the memory required for a job\n+     * @param maxNumInQueue  The number of unassigned jobs allowed.\n+     * @return The capacity needed to reduce the length of `unassignedJobs` to `maxNumInQueue`\n+     */\n+    static Optional<NativeMemoryCapacity> requiredCapacityForUnassignedJobs(List<String> unassignedJobs,\n+                                                                            Function<String, Long> sizeFunction,\n+                                                                            int maxNumInQueue) {\n+        List<Long> jobSizes = unassignedJobs\n+            .stream()\n+            // TODO do we want to verify memory requirements aren't stale? Or just consider `null` a fastpath?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6536073d0983c0cc4c49f30c41392a2b2a23fff0"}, "originalPosition": 144}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTA2OTgxOQ==", "bodyText": "Given that we check staleness at the beginning of the scale method I think it should impossible to return null here given where it's called from.  So:\n\nPlease add a Javadoc comment saying this method must only be called after checking the that memory tracker is recently refreshed\nAdd an assertion that the return value isn't null so we can catch unexpected situations in tests\nAdd a warning log so that if we return null in production and it causes a support case that autoscaling isn't working we have something in the log", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r525069819", "createdAt": "2020-11-17T11:05:56Z", "author": {"login": "droberts195"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/autoscaling/MlAutoscalingDeciderService.java", "diffHunk": "@@ -0,0 +1,565 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.ml.autoscaling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.LocalNodeMasterListener;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.component.LifecycleListener;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.persistent.PersistentTasksCustomMetadata;\n+import org.elasticsearch.persistent.PersistentTasksCustomMetadata.PersistentTask;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderContext;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderResult;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderService;\n+import org.elasticsearch.xpack.core.ml.MlTasks;\n+import org.elasticsearch.xpack.core.ml.action.StartDatafeedAction.DatafeedParams;\n+import org.elasticsearch.xpack.core.ml.dataframe.DataFrameAnalyticsState;\n+import org.elasticsearch.xpack.core.ml.job.config.JobState;\n+import org.elasticsearch.xpack.ml.MachineLearning;\n+import org.elasticsearch.xpack.ml.job.NodeLoad;\n+import org.elasticsearch.xpack.ml.job.NodeLoadDetector;\n+import org.elasticsearch.xpack.ml.process.MlMemoryTracker;\n+import org.elasticsearch.xpack.ml.utils.NativeMemoryCalculator;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.OptionalLong;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.elasticsearch.xpack.core.ml.MlTasks.getDataFrameAnalyticsState;\n+import static org.elasticsearch.xpack.core.ml.MlTasks.getJobStateModifiedForReassignments;\n+import static org.elasticsearch.xpack.ml.job.JobNodeSelector.AWAITING_LAZY_ASSIGNMENT;\n+\n+public class MlAutoscalingDeciderService implements AutoscalingDeciderService<MlAutoscalingDeciderConfiguration>,\n+    LocalNodeMasterListener {\n+\n+    private static final Logger logger = LogManager.getLogger(MlAutoscalingDeciderService.class);\n+    private static final Duration DEFAULT_MEMORY_REFRESH_RATE = Duration.ofMinutes(15);\n+    private static final String MEMORY_STALE = \"unable to make scaling decision as job memory requirements are stale\";\n+    private static final long NO_SCALE_DOWN_POSSIBLE = -1L;\n+\n+    private final NodeLoadDetector nodeLoadDetector;\n+    private final MlMemoryTracker mlMemoryTracker;\n+    private final Supplier<Long> timeSupplier;\n+    private final boolean useAuto;\n+\n+    private volatile boolean isMaster;\n+    private volatile boolean running;\n+    private volatile int maxMachineMemoryPercent;\n+    private volatile int maxOpenJobs;\n+    private volatile long lastTimeToScale;\n+    private volatile long scaleDownDetected;\n+\n+    public MlAutoscalingDeciderService(MlMemoryTracker memoryTracker, Settings settings, ClusterService clusterService) {\n+        this(new NodeLoadDetector(memoryTracker), settings, clusterService, System::currentTimeMillis);\n+    }\n+\n+    MlAutoscalingDeciderService(NodeLoadDetector nodeLoadDetector,\n+                                Settings settings,\n+                                ClusterService clusterService,\n+                                Supplier<Long> timeSupplier) {\n+        this.nodeLoadDetector = nodeLoadDetector;\n+        this.mlMemoryTracker = nodeLoadDetector.getMlMemoryTracker();\n+        this.maxMachineMemoryPercent = MachineLearning.MAX_MACHINE_MEMORY_PERCENT.get(settings);\n+        this.maxOpenJobs = MachineLearning.MAX_OPEN_JOBS_PER_NODE.get(settings);\n+        this.useAuto = MachineLearning.USE_AUTO_MACHINE_MEMORY_PERCENT.get(settings);\n+        this.timeSupplier = timeSupplier;\n+        this.scaleDownDetected = NO_SCALE_DOWN_POSSIBLE;\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MachineLearning.MAX_MACHINE_MEMORY_PERCENT,\n+            this::setMaxMachineMemoryPercent);\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MachineLearning.MAX_OPEN_JOBS_PER_NODE, this::setMaxOpenJobs);\n+        clusterService.addLocalNodeMasterListener(this);\n+        clusterService.addLifecycleListener(new LifecycleListener() {\n+            @Override\n+            public void afterStart() {\n+                running = true;\n+                if (isMaster) {\n+                    mlMemoryTracker.asyncRefresh();\n+                }\n+            }\n+\n+            @Override\n+            public void beforeStop() {\n+                running = false;\n+            }\n+        });\n+    }\n+\n+    static OptionalLong getNodeJvmSize(DiscoveryNode node) {\n+        Map<String, String> nodeAttributes = node.getAttributes();\n+        OptionalLong value = OptionalLong.empty();\n+        String valueStr = nodeAttributes.get(MachineLearning.MAX_JVM_SIZE_NODE_ATTR);\n+        try {\n+            value = OptionalLong.of(Long.parseLong(valueStr));\n+        } catch (NumberFormatException e) {\n+            logger.debug(() -> new ParameterizedMessage(\n+                \"could not parse stored string value [{}] in node attribute [{}]\",\n+                valueStr,\n+                MachineLearning.MAX_JVM_SIZE_NODE_ATTR));\n+        }\n+        return value;\n+    }\n+\n+    static List<DiscoveryNode> getNodes(final ClusterState clusterState) {\n+        return clusterState.nodes()\n+            .mastersFirstStream()\n+            .filter(MachineLearning::isMlNode)\n+            .collect(Collectors.toList());\n+    }\n+\n+    /**\n+     * @param unassignedJobs The list of unassigned jobs\n+     * @param sizeFunction   Function providing the memory required for a job\n+     * @param maxNumInQueue  The number of unassigned jobs allowed.\n+     * @return The capacity needed to reduce the length of `unassignedJobs` to `maxNumInQueue`\n+     */\n+    static Optional<NativeMemoryCapacity> requiredCapacityForUnassignedJobs(List<String> unassignedJobs,\n+                                                                            Function<String, Long> sizeFunction,\n+                                                                            int maxNumInQueue) {\n+        List<Long> jobSizes = unassignedJobs\n+            .stream()\n+            // TODO do we want to verify memory requirements aren't stale? Or just consider `null` a fastpath?\n+            .map(sizeFunction)\n+            .map(l -> l == null ? 0L : l)\n+            .collect(Collectors.toList());\n+        // Only possible if unassignedJobs was empty.\n+        if (jobSizes.isEmpty()) {\n+            return Optional.empty();\n+        }\n+        jobSizes.sort(Comparator.comparingLong(Long::longValue).reversed());\n+        long tierMemory = 0L;\n+        long nodeMemory = jobSizes.get(0);\n+        Iterator<Long> iter = jobSizes.iterator();\n+        while (jobSizes.size() > maxNumInQueue && iter.hasNext()) {\n+            tierMemory += iter.next();\n+            iter.remove();\n+        }\n+        return Optional.of(new NativeMemoryCapacity(tierMemory, nodeMemory));\n+    }\n+\n+    private static Collection<PersistentTask<?>> anomalyDetectionTasks(PersistentTasksCustomMetadata tasksCustomMetadata) {\n+        if (tasksCustomMetadata == null) {\n+            return Collections.emptyList();\n+        }\n+\n+        return tasksCustomMetadata.findTasks(MlTasks.JOB_TASK_NAME,\n+            t -> getJobStateModifiedForReassignments(t).isAnyOf(JobState.OPENED, JobState.OPENING));\n+    }\n+\n+    private static Collection<PersistentTask<?>> dataframeAnalyticsTasks(PersistentTasksCustomMetadata tasksCustomMetadata) {\n+        if (tasksCustomMetadata == null) {\n+            return Collections.emptyList();\n+        }\n+\n+        return tasksCustomMetadata.findTasks(MlTasks.DATA_FRAME_ANALYTICS_TASK_NAME,\n+            t -> getDataFrameAnalyticsState(t).isAnyOf(DataFrameAnalyticsState.STARTED, DataFrameAnalyticsState.STARTING));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private static Collection<PersistentTask<DatafeedParams>> datafeedTasks(PersistentTasksCustomMetadata tasksCustomMetadata) {\n+        if (tasksCustomMetadata == null) {\n+            return Collections.emptyList();\n+        }\n+\n+        return tasksCustomMetadata.findTasks(MlTasks.DATAFEED_TASK_NAME, t -> true)\n+            .stream()\n+            .map(p -> (PersistentTask<DatafeedParams>) p)\n+            .collect(Collectors.toList());\n+    }\n+\n+    void setMaxMachineMemoryPercent(int maxMachineMemoryPercent) {\n+        this.maxMachineMemoryPercent = maxMachineMemoryPercent;\n+    }\n+\n+    void setMaxOpenJobs(int maxOpenJobs) {\n+        this.maxOpenJobs = maxOpenJobs;\n+    }\n+\n+    @Override\n+    public void onMaster() {\n+        isMaster = true;\n+        if (running) {\n+            mlMemoryTracker.asyncRefresh();\n+        }\n+    }\n+\n+    private void resetScaleDownCoolDown() {\n+        this.scaleDownDetected = NO_SCALE_DOWN_POSSIBLE;\n+    }\n+\n+    private boolean canScaleDown(TimeValue coolDown) {\n+        if (this.scaleDownDetected == NO_SCALE_DOWN_POSSIBLE) {\n+            return false;\n+        }\n+        return timeSupplier.get() - scaleDownDetected >= coolDown.millis();\n+    }\n+\n+    private boolean newScaleDownCheck() {\n+        return scaleDownDetected == NO_SCALE_DOWN_POSSIBLE;\n+    }\n+\n+    NativeMemoryCapacity currentScale(final List<DiscoveryNode> machineLearningNodes) {\n+        long[] mlMemory = machineLearningNodes.stream()\n+            .mapToLong(node -> NativeMemoryCalculator.allowedBytesForMl(node, maxMachineMemoryPercent, useAuto).orElse(0L))\n+            .toArray();\n+\n+        return new NativeMemoryCapacity(\n+            Arrays.stream(mlMemory).sum(),\n+            Arrays.stream(mlMemory).max().orElse(0L),\n+            // We assume that JVM size is universal, at least, the largest JVM indicates the largest node\n+            machineLearningNodes.stream()\n+                .map(MlAutoscalingDeciderService::getNodeJvmSize)\n+                .mapToLong(l -> l.orElse(0L))\n+                .boxed()\n+                .max(Long::compare)\n+                .orElse(null)\n+        );\n+    }\n+\n+    @Override\n+    public void offMaster() {\n+        isMaster = false;\n+    }\n+\n+    @Override\n+    public AutoscalingDeciderResult scale(MlAutoscalingDeciderConfiguration decider, AutoscalingDeciderContext context) {\n+        if (isMaster == false) {\n+            throw new IllegalArgumentException(\"request for scaling information is only allowed on the master node\");\n+        }\n+        final Duration memoryTrackingStale;\n+        long previousTimeStamp = this.lastTimeToScale;\n+        this.lastTimeToScale = this.timeSupplier.get();\n+        if (previousTimeStamp == 0L) {\n+            memoryTrackingStale = DEFAULT_MEMORY_REFRESH_RATE;\n+        } else {\n+            memoryTrackingStale = Duration.ofMillis(TimeValue.timeValueMinutes(1).millis() + this.lastTimeToScale - previousTimeStamp);\n+        }\n+\n+        final ClusterState clusterState = context.state();\n+\n+        PersistentTasksCustomMetadata tasks = clusterState.getMetadata().custom(PersistentTasksCustomMetadata.TYPE);\n+        Collection<PersistentTask<?>> anomalyDetectionTasks = anomalyDetectionTasks(tasks);\n+        Collection<PersistentTask<?>> dataframeAnalyticsTasks = dataframeAnalyticsTasks(tasks);\n+        final List<DiscoveryNode> nodes = getNodes(clusterState);\n+        Optional<NativeMemoryCapacity> futureFreedCapacity = calculateFutureFreedCapacity(tasks, memoryTrackingStale);\n+\n+        final List<String> waitingAnomalyJobs = anomalyDetectionTasks.stream()\n+            .filter(t -> AWAITING_LAZY_ASSIGNMENT.equals(t.getAssignment()))\n+            .map(t -> MlTasks.jobId(t.getId()))\n+            .collect(Collectors.toList());\n+        final List<String> waitingAnalyticsJobs = dataframeAnalyticsTasks.stream()\n+            .filter(t -> AWAITING_LAZY_ASSIGNMENT.equals(t.getAssignment()))\n+            .map(t -> MlTasks.dataFrameAnalyticsId(t.getId()))\n+            .collect(Collectors.toList());\n+\n+        final NativeMemoryCapacity currentScale = currentScale(nodes);\n+        final MlScalingReason.Builder reasonBuilder = MlScalingReason.builder()\n+            .setWaitingAnomalyJobs(waitingAnomalyJobs)\n+            .setWaitingAnalyticsJobs(waitingAnalyticsJobs)\n+            .setCurrentMlCapacity(currentScale.autoscalingCapacity(maxMachineMemoryPercent, useAuto))\n+            .setPassedConfiguration(decider);\n+\n+        final Optional<AutoscalingDeciderResult> scaleUpDecision = checkForScaleUp(decider,\n+            waitingAnomalyJobs,\n+            waitingAnalyticsJobs,\n+            futureFreedCapacity.orElse(null),\n+            currentScale,\n+            reasonBuilder);\n+\n+        if (scaleUpDecision.isPresent()) {\n+            resetScaleDownCoolDown();\n+            return scaleUpDecision.get();\n+        }\n+        if (waitingAnalyticsJobs.isEmpty() == false || waitingAnomalyJobs.isEmpty() == false) {\n+            // We don't want to continue to consider a scale down if there are now waiting jobs\n+            resetScaleDownCoolDown();\n+            noScaleResultOrRefresh(reasonBuilder, memoryTrackingStale, new AutoscalingDeciderResult(\n+                context.currentCapacity(),\n+                reasonBuilder\n+                    .setSimpleReason(\"Passing currently perceived capacity as there are analytics and anomaly jobs in the queue, \" +\n+                        \"but the number in the queue is less than the configured maximum allowed.\")\n+                    .build()));\n+        }\n+        if (mlMemoryTracker.isRecentlyRefreshed(memoryTrackingStale) == false) {\n+            return buildDecisionAndRequestRefresh(reasonBuilder);\n+        }\n+\n+        long largestJob = Math.max(\n+            anomalyDetectionTasks.stream()\n+                .filter(PersistentTask::isAssigned)\n+                // Memory SHOULD be recently refreshed, so in our current state, we should at least have an idea of the memory used\n+                .mapToLong(this::getAnomalyMemoryRequirement)\n+                .max()\n+                .orElse(0L),\n+            dataframeAnalyticsTasks.stream()\n+                .filter(PersistentTask::isAssigned)\n+                // Memory SHOULD be recently refreshed, so in our current state, we should at least have an idea of the memory used\n+                .mapToLong(this::getAnalyticsMemoryRequirement)\n+                .max()\n+                .orElse(0L));\n+\n+        final Optional<AutoscalingDeciderResult> scaleDownDecision =\n+            checkForScaleDown(nodes, clusterState, largestJob, currentScale, reasonBuilder);\n+\n+        if (scaleDownDecision.isPresent()) {\n+            if (newScaleDownCheck()) {\n+                scaleDownDetected = timeSupplier.get();\n+            }\n+            if (canScaleDown(decider.getDownScaleDelay())) {\n+                return scaleDownDecision.get();\n+            }\n+            return new AutoscalingDeciderResult(\n+                context.currentCapacity(),\n+                reasonBuilder\n+                    .setSimpleReason(\n+                        \"Passing currently perceived capacity as configured down scale delay has not be satisfied; configured delay [\"\n+                            + decider.getDownScaleDelay().millis()\n+                            + \"] last detected scale down event [\"\n+                            + scaleDownDetected\n+                            + \"]\")\n+                    .build());\n+        }\n+\n+        return noScaleResultOrRefresh(reasonBuilder, memoryTrackingStale, new AutoscalingDeciderResult(context.currentCapacity(),\n+            reasonBuilder\n+                .setSimpleReason(\"Passing currently perceived capacity as no scaling changes were detected to be possible\")\n+                .build()));\n+    }\n+\n+    AutoscalingDeciderResult noScaleResultOrRefresh(MlScalingReason.Builder reasonBuilder,\n+                                                    Duration memoryTrackingStale,\n+                                                    AutoscalingDeciderResult potentialResult) {\n+        if (mlMemoryTracker.isRecentlyRefreshed(memoryTrackingStale) == false) {\n+            return buildDecisionAndRequestRefresh(reasonBuilder);\n+        } else {\n+            return potentialResult;\n+        }\n+    }\n+\n+    Optional<AutoscalingDeciderResult> checkForScaleUp(MlAutoscalingDeciderConfiguration decider,\n+                                                       List<String> waitingAnomalyJobs,\n+                                                       List<String> waitingAnalyticsJobs,\n+                                                       @Nullable NativeMemoryCapacity futureFreedCapacity,\n+                                                       NativeMemoryCapacity currentScale,\n+                                                       MlScalingReason.Builder reasonBuilder) {\n+\n+        // Are we in breach of maximum waiting jobs?\n+        if (waitingAnalyticsJobs.size() > decider.getNumAnalyticsJobsInQueue()\n+            || waitingAnomalyJobs.size() > decider.getNumAnomalyJobsInQueue()) {\n+            NativeMemoryCapacity updatedCapacity = NativeMemoryCapacity.from(currentScale);\n+            Optional<NativeMemoryCapacity> analyticsCapacity = requiredCapacityForUnassignedJobs(waitingAnalyticsJobs,\n+                this::getAnalyticsMemoryRequirement,\n+                decider.getNumAnalyticsJobsInQueue());\n+            Optional<NativeMemoryCapacity> anomalyCapacity = requiredCapacityForUnassignedJobs(waitingAnomalyJobs,\n+                this::getAnomalyMemoryRequirement,\n+                decider.getNumAnomalyJobsInQueue());\n+\n+            updatedCapacity.merge(anomalyCapacity.orElse(NativeMemoryCapacity.ZERO))\n+                .merge(analyticsCapacity.orElse(NativeMemoryCapacity.ZERO));\n+            return Optional.of(new AutoscalingDeciderResult(\n+                updatedCapacity.autoscalingCapacity(maxMachineMemoryPercent, useAuto),\n+                reasonBuilder.setSimpleReason(\"requesting scale up as number of jobs in queues exceeded configured limit\").build()));\n+        }\n+\n+        // Could the currently waiting jobs ever be assigned?\n+        if (waitingAnalyticsJobs.isEmpty() == false || waitingAnomalyJobs.isEmpty() == false) {\n+            // we are unable to determine new tier size, but maybe we can see if our nodes are big enough.\n+            if (futureFreedCapacity == null) {\n+                Optional<Long> maxSize = Stream.concat(\n+                    waitingAnalyticsJobs.stream().map(mlMemoryTracker::getDataFrameAnalyticsJobMemoryRequirement),\n+                    waitingAnomalyJobs.stream().map(mlMemoryTracker::getAnomalyDetectorJobMemoryRequirement))\n+                    .filter(Objects::nonNull)\n+                    .max(Long::compareTo);\n+                if (maxSize.isPresent() && maxSize.get() > currentScale.getNode()) {\n+                    return Optional.of(new AutoscalingDeciderResult(\n+                        new NativeMemoryCapacity(Math.max(currentScale.getTier(), maxSize.get()), maxSize.get())\n+                            .autoscalingCapacity(maxMachineMemoryPercent, useAuto),\n+                        reasonBuilder.setSimpleReason(\"requesting scale up as there is no node large enough to handle queued jobs\")\n+                            .build()));\n+                }\n+                // we have no info, allow the caller to make the appropriate action, probably returning a no_scale\n+                return Optional.empty();\n+            }\n+            long newTierNeeded = 0L;\n+            // could any of the nodes actually run the job?\n+            long newNodeMax = currentScale.getNode();\n+            for (String analyticsJob : waitingAnalyticsJobs) {\n+                Long requiredMemory = mlMemoryTracker.getDataFrameAnalyticsJobMemoryRequirement(analyticsJob);\n+                // it is OK to continue here as we have not breached our queuing limit\n+                if (requiredMemory == null) {\n+                    continue;\n+                }\n+                // Is there \"future capacity\" on a node that could run this job? If not, we need that much more in the tier.\n+                if (futureFreedCapacity.getNode() < requiredMemory) {\n+                    newTierNeeded = Math.max(requiredMemory, newTierNeeded);\n+                }\n+                newNodeMax = Math.max(newNodeMax, requiredMemory);\n+            }\n+            for (String anomalyJob : waitingAnomalyJobs) {\n+                Long requiredMemory = mlMemoryTracker.getAnomalyDetectorJobMemoryRequirement(anomalyJob);\n+                // it is OK to continue here as we have not breached our queuing limit\n+                if (requiredMemory == null) {\n+                    continue;\n+                }\n+                // Is there \"future capacity\" on a node that could run this job? If not, we need that much more in the tier.\n+                if (futureFreedCapacity.getNode() < requiredMemory) {\n+                    newTierNeeded = Math.max(requiredMemory, newTierNeeded);\n+                }\n+                newNodeMax = Math.max(newNodeMax, requiredMemory);\n+            }\n+            if (newNodeMax > currentScale.getNode() || newTierNeeded > 0L) {\n+                NativeMemoryCapacity newCapacity = new NativeMemoryCapacity(newTierNeeded, newNodeMax);\n+                return Optional.of(new AutoscalingDeciderResult(\n+                    // We need more memory in the tier, or our individual node size requirements has increased\n+                    NativeMemoryCapacity.from(currentScale).merge(newCapacity).autoscalingCapacity(maxMachineMemoryPercent, useAuto),\n+                    reasonBuilder\n+                        .setSimpleReason(\"scaling up as adequate space would not automatically become available when running jobs finish\")\n+                        .build()\n+                ));\n+            }\n+        }\n+\n+        return Optional.empty();\n+    }\n+\n+    // This calculates the the following the potentially automatically free capacity of sometime in the future\n+    // Since jobs with lookback only datafeeds, and data frame analytics jobs all have some potential future end date\n+    // we can assume (without user intervention) that these will eventually stop and free their currently occupied resources.\n+    //\n+    // The capacity is as follows:\n+    //  tier: The sum total of the resources that will be removed\n+    //  node: The largest block of memory that will be freed on a given node.\n+    //      - If > 1 \"batch\" ml tasks are running on the same node, we sum their resources.\n+    Optional<NativeMemoryCapacity> calculateFutureFreedCapacity(PersistentTasksCustomMetadata tasks, Duration jobMemoryExpiry) {\n+        final List<PersistentTask<DatafeedParams>> jobsWithLookbackDatafeeds = datafeedTasks(tasks).stream()\n+            .filter(t -> t.getParams().getEndTime() != null && t.getExecutorNode() != null)\n+            .collect(Collectors.toList());\n+        final List<PersistentTask<?>> assignedAnalyticsJobs = dataframeAnalyticsTasks(tasks).stream()\n+            .filter(t -> t.getExecutorNode() != null)\n+            .collect(Collectors.toList());\n+\n+        if (jobsWithLookbackDatafeeds.isEmpty() && assignedAnalyticsJobs.isEmpty()) {\n+            return Optional.of(NativeMemoryCapacity.ZERO);\n+        }\n+        if (mlMemoryTracker.isRecentlyRefreshed(jobMemoryExpiry) == false) {\n+            return Optional.empty();\n+        }\n+        // What is the largest chunk of memory that could be freed on a node in the future\n+        Map<String, Long> maxNodeBytes = new HashMap<>();\n+        for (PersistentTask<DatafeedParams> lookbackOnlyDf : jobsWithLookbackDatafeeds) {\n+            Long jobSize = mlMemoryTracker.getAnomalyDetectorJobMemoryRequirement(lookbackOnlyDf.getParams().getJobId());\n+            if (jobSize == null) {\n+                return Optional.empty();\n+            }\n+            maxNodeBytes.compute(lookbackOnlyDf.getExecutorNode(), (_k, v) -> v == null ? jobSize : jobSize + v);\n+        }\n+        for (PersistentTask<?> task : assignedAnalyticsJobs) {\n+            Long jobSize = mlMemoryTracker.getDataFrameAnalyticsJobMemoryRequirement(MlTasks.dataFrameAnalyticsId(task.getId()));\n+            if (jobSize == null) {\n+                return Optional.empty();\n+            }\n+            maxNodeBytes.compute(task.getExecutorNode(), (_k, v) -> v == null ? jobSize : jobSize + v);\n+        }\n+        return Optional.of(new NativeMemoryCapacity(\n+            maxNodeBytes.values().stream().mapToLong(Long::longValue).sum(),\n+            maxNodeBytes.values().stream().mapToLong(Long::longValue).max().orElse(0L)));\n+    }\n+\n+    private AutoscalingDeciderResult buildDecisionAndRequestRefresh(MlScalingReason.Builder reasonBuilder) {\n+        mlMemoryTracker.asyncRefresh();\n+        return new AutoscalingDeciderResult(null, reasonBuilder.setSimpleReason(MEMORY_STALE).build());\n+    }\n+\n+    private Long getAnalyticsMemoryRequirement(String analyticsId) {\n+        return mlMemoryTracker.getDataFrameAnalyticsJobMemoryRequirement(analyticsId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6536073d0983c0cc4c49f30c41392a2b2a23fff0"}, "originalPosition": 497}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTA2OTkwNA==", "bodyText": "Given that we check staleness at the beginning of the scale method I think it should impossible to return null here given where it's called from.  So:\n\nPlease add a Javadoc comment saying this method must only be called after checking the that memory tracker is recently refreshed\nAdd an assertion that the return value isn't null so we can catch unexpected situations in tests\nAdd a warning log so that if we return null in production and it causes a support case that autoscaling isn't working we have something in the log", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r525069904", "createdAt": "2020-11-17T11:06:07Z", "author": {"login": "droberts195"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/autoscaling/MlAutoscalingDeciderService.java", "diffHunk": "@@ -0,0 +1,565 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.ml.autoscaling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.LocalNodeMasterListener;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.component.LifecycleListener;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.persistent.PersistentTasksCustomMetadata;\n+import org.elasticsearch.persistent.PersistentTasksCustomMetadata.PersistentTask;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderContext;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderResult;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderService;\n+import org.elasticsearch.xpack.core.ml.MlTasks;\n+import org.elasticsearch.xpack.core.ml.action.StartDatafeedAction.DatafeedParams;\n+import org.elasticsearch.xpack.core.ml.dataframe.DataFrameAnalyticsState;\n+import org.elasticsearch.xpack.core.ml.job.config.JobState;\n+import org.elasticsearch.xpack.ml.MachineLearning;\n+import org.elasticsearch.xpack.ml.job.NodeLoad;\n+import org.elasticsearch.xpack.ml.job.NodeLoadDetector;\n+import org.elasticsearch.xpack.ml.process.MlMemoryTracker;\n+import org.elasticsearch.xpack.ml.utils.NativeMemoryCalculator;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.OptionalLong;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.elasticsearch.xpack.core.ml.MlTasks.getDataFrameAnalyticsState;\n+import static org.elasticsearch.xpack.core.ml.MlTasks.getJobStateModifiedForReassignments;\n+import static org.elasticsearch.xpack.ml.job.JobNodeSelector.AWAITING_LAZY_ASSIGNMENT;\n+\n+public class MlAutoscalingDeciderService implements AutoscalingDeciderService<MlAutoscalingDeciderConfiguration>,\n+    LocalNodeMasterListener {\n+\n+    private static final Logger logger = LogManager.getLogger(MlAutoscalingDeciderService.class);\n+    private static final Duration DEFAULT_MEMORY_REFRESH_RATE = Duration.ofMinutes(15);\n+    private static final String MEMORY_STALE = \"unable to make scaling decision as job memory requirements are stale\";\n+    private static final long NO_SCALE_DOWN_POSSIBLE = -1L;\n+\n+    private final NodeLoadDetector nodeLoadDetector;\n+    private final MlMemoryTracker mlMemoryTracker;\n+    private final Supplier<Long> timeSupplier;\n+    private final boolean useAuto;\n+\n+    private volatile boolean isMaster;\n+    private volatile boolean running;\n+    private volatile int maxMachineMemoryPercent;\n+    private volatile int maxOpenJobs;\n+    private volatile long lastTimeToScale;\n+    private volatile long scaleDownDetected;\n+\n+    public MlAutoscalingDeciderService(MlMemoryTracker memoryTracker, Settings settings, ClusterService clusterService) {\n+        this(new NodeLoadDetector(memoryTracker), settings, clusterService, System::currentTimeMillis);\n+    }\n+\n+    MlAutoscalingDeciderService(NodeLoadDetector nodeLoadDetector,\n+                                Settings settings,\n+                                ClusterService clusterService,\n+                                Supplier<Long> timeSupplier) {\n+        this.nodeLoadDetector = nodeLoadDetector;\n+        this.mlMemoryTracker = nodeLoadDetector.getMlMemoryTracker();\n+        this.maxMachineMemoryPercent = MachineLearning.MAX_MACHINE_MEMORY_PERCENT.get(settings);\n+        this.maxOpenJobs = MachineLearning.MAX_OPEN_JOBS_PER_NODE.get(settings);\n+        this.useAuto = MachineLearning.USE_AUTO_MACHINE_MEMORY_PERCENT.get(settings);\n+        this.timeSupplier = timeSupplier;\n+        this.scaleDownDetected = NO_SCALE_DOWN_POSSIBLE;\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MachineLearning.MAX_MACHINE_MEMORY_PERCENT,\n+            this::setMaxMachineMemoryPercent);\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MachineLearning.MAX_OPEN_JOBS_PER_NODE, this::setMaxOpenJobs);\n+        clusterService.addLocalNodeMasterListener(this);\n+        clusterService.addLifecycleListener(new LifecycleListener() {\n+            @Override\n+            public void afterStart() {\n+                running = true;\n+                if (isMaster) {\n+                    mlMemoryTracker.asyncRefresh();\n+                }\n+            }\n+\n+            @Override\n+            public void beforeStop() {\n+                running = false;\n+            }\n+        });\n+    }\n+\n+    static OptionalLong getNodeJvmSize(DiscoveryNode node) {\n+        Map<String, String> nodeAttributes = node.getAttributes();\n+        OptionalLong value = OptionalLong.empty();\n+        String valueStr = nodeAttributes.get(MachineLearning.MAX_JVM_SIZE_NODE_ATTR);\n+        try {\n+            value = OptionalLong.of(Long.parseLong(valueStr));\n+        } catch (NumberFormatException e) {\n+            logger.debug(() -> new ParameterizedMessage(\n+                \"could not parse stored string value [{}] in node attribute [{}]\",\n+                valueStr,\n+                MachineLearning.MAX_JVM_SIZE_NODE_ATTR));\n+        }\n+        return value;\n+    }\n+\n+    static List<DiscoveryNode> getNodes(final ClusterState clusterState) {\n+        return clusterState.nodes()\n+            .mastersFirstStream()\n+            .filter(MachineLearning::isMlNode)\n+            .collect(Collectors.toList());\n+    }\n+\n+    /**\n+     * @param unassignedJobs The list of unassigned jobs\n+     * @param sizeFunction   Function providing the memory required for a job\n+     * @param maxNumInQueue  The number of unassigned jobs allowed.\n+     * @return The capacity needed to reduce the length of `unassignedJobs` to `maxNumInQueue`\n+     */\n+    static Optional<NativeMemoryCapacity> requiredCapacityForUnassignedJobs(List<String> unassignedJobs,\n+                                                                            Function<String, Long> sizeFunction,\n+                                                                            int maxNumInQueue) {\n+        List<Long> jobSizes = unassignedJobs\n+            .stream()\n+            // TODO do we want to verify memory requirements aren't stale? Or just consider `null` a fastpath?\n+            .map(sizeFunction)\n+            .map(l -> l == null ? 0L : l)\n+            .collect(Collectors.toList());\n+        // Only possible if unassignedJobs was empty.\n+        if (jobSizes.isEmpty()) {\n+            return Optional.empty();\n+        }\n+        jobSizes.sort(Comparator.comparingLong(Long::longValue).reversed());\n+        long tierMemory = 0L;\n+        long nodeMemory = jobSizes.get(0);\n+        Iterator<Long> iter = jobSizes.iterator();\n+        while (jobSizes.size() > maxNumInQueue && iter.hasNext()) {\n+            tierMemory += iter.next();\n+            iter.remove();\n+        }\n+        return Optional.of(new NativeMemoryCapacity(tierMemory, nodeMemory));\n+    }\n+\n+    private static Collection<PersistentTask<?>> anomalyDetectionTasks(PersistentTasksCustomMetadata tasksCustomMetadata) {\n+        if (tasksCustomMetadata == null) {\n+            return Collections.emptyList();\n+        }\n+\n+        return tasksCustomMetadata.findTasks(MlTasks.JOB_TASK_NAME,\n+            t -> getJobStateModifiedForReassignments(t).isAnyOf(JobState.OPENED, JobState.OPENING));\n+    }\n+\n+    private static Collection<PersistentTask<?>> dataframeAnalyticsTasks(PersistentTasksCustomMetadata tasksCustomMetadata) {\n+        if (tasksCustomMetadata == null) {\n+            return Collections.emptyList();\n+        }\n+\n+        return tasksCustomMetadata.findTasks(MlTasks.DATA_FRAME_ANALYTICS_TASK_NAME,\n+            t -> getDataFrameAnalyticsState(t).isAnyOf(DataFrameAnalyticsState.STARTED, DataFrameAnalyticsState.STARTING));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private static Collection<PersistentTask<DatafeedParams>> datafeedTasks(PersistentTasksCustomMetadata tasksCustomMetadata) {\n+        if (tasksCustomMetadata == null) {\n+            return Collections.emptyList();\n+        }\n+\n+        return tasksCustomMetadata.findTasks(MlTasks.DATAFEED_TASK_NAME, t -> true)\n+            .stream()\n+            .map(p -> (PersistentTask<DatafeedParams>) p)\n+            .collect(Collectors.toList());\n+    }\n+\n+    void setMaxMachineMemoryPercent(int maxMachineMemoryPercent) {\n+        this.maxMachineMemoryPercent = maxMachineMemoryPercent;\n+    }\n+\n+    void setMaxOpenJobs(int maxOpenJobs) {\n+        this.maxOpenJobs = maxOpenJobs;\n+    }\n+\n+    @Override\n+    public void onMaster() {\n+        isMaster = true;\n+        if (running) {\n+            mlMemoryTracker.asyncRefresh();\n+        }\n+    }\n+\n+    private void resetScaleDownCoolDown() {\n+        this.scaleDownDetected = NO_SCALE_DOWN_POSSIBLE;\n+    }\n+\n+    private boolean canScaleDown(TimeValue coolDown) {\n+        if (this.scaleDownDetected == NO_SCALE_DOWN_POSSIBLE) {\n+            return false;\n+        }\n+        return timeSupplier.get() - scaleDownDetected >= coolDown.millis();\n+    }\n+\n+    private boolean newScaleDownCheck() {\n+        return scaleDownDetected == NO_SCALE_DOWN_POSSIBLE;\n+    }\n+\n+    NativeMemoryCapacity currentScale(final List<DiscoveryNode> machineLearningNodes) {\n+        long[] mlMemory = machineLearningNodes.stream()\n+            .mapToLong(node -> NativeMemoryCalculator.allowedBytesForMl(node, maxMachineMemoryPercent, useAuto).orElse(0L))\n+            .toArray();\n+\n+        return new NativeMemoryCapacity(\n+            Arrays.stream(mlMemory).sum(),\n+            Arrays.stream(mlMemory).max().orElse(0L),\n+            // We assume that JVM size is universal, at least, the largest JVM indicates the largest node\n+            machineLearningNodes.stream()\n+                .map(MlAutoscalingDeciderService::getNodeJvmSize)\n+                .mapToLong(l -> l.orElse(0L))\n+                .boxed()\n+                .max(Long::compare)\n+                .orElse(null)\n+        );\n+    }\n+\n+    @Override\n+    public void offMaster() {\n+        isMaster = false;\n+    }\n+\n+    @Override\n+    public AutoscalingDeciderResult scale(MlAutoscalingDeciderConfiguration decider, AutoscalingDeciderContext context) {\n+        if (isMaster == false) {\n+            throw new IllegalArgumentException(\"request for scaling information is only allowed on the master node\");\n+        }\n+        final Duration memoryTrackingStale;\n+        long previousTimeStamp = this.lastTimeToScale;\n+        this.lastTimeToScale = this.timeSupplier.get();\n+        if (previousTimeStamp == 0L) {\n+            memoryTrackingStale = DEFAULT_MEMORY_REFRESH_RATE;\n+        } else {\n+            memoryTrackingStale = Duration.ofMillis(TimeValue.timeValueMinutes(1).millis() + this.lastTimeToScale - previousTimeStamp);\n+        }\n+\n+        final ClusterState clusterState = context.state();\n+\n+        PersistentTasksCustomMetadata tasks = clusterState.getMetadata().custom(PersistentTasksCustomMetadata.TYPE);\n+        Collection<PersistentTask<?>> anomalyDetectionTasks = anomalyDetectionTasks(tasks);\n+        Collection<PersistentTask<?>> dataframeAnalyticsTasks = dataframeAnalyticsTasks(tasks);\n+        final List<DiscoveryNode> nodes = getNodes(clusterState);\n+        Optional<NativeMemoryCapacity> futureFreedCapacity = calculateFutureFreedCapacity(tasks, memoryTrackingStale);\n+\n+        final List<String> waitingAnomalyJobs = anomalyDetectionTasks.stream()\n+            .filter(t -> AWAITING_LAZY_ASSIGNMENT.equals(t.getAssignment()))\n+            .map(t -> MlTasks.jobId(t.getId()))\n+            .collect(Collectors.toList());\n+        final List<String> waitingAnalyticsJobs = dataframeAnalyticsTasks.stream()\n+            .filter(t -> AWAITING_LAZY_ASSIGNMENT.equals(t.getAssignment()))\n+            .map(t -> MlTasks.dataFrameAnalyticsId(t.getId()))\n+            .collect(Collectors.toList());\n+\n+        final NativeMemoryCapacity currentScale = currentScale(nodes);\n+        final MlScalingReason.Builder reasonBuilder = MlScalingReason.builder()\n+            .setWaitingAnomalyJobs(waitingAnomalyJobs)\n+            .setWaitingAnalyticsJobs(waitingAnalyticsJobs)\n+            .setCurrentMlCapacity(currentScale.autoscalingCapacity(maxMachineMemoryPercent, useAuto))\n+            .setPassedConfiguration(decider);\n+\n+        final Optional<AutoscalingDeciderResult> scaleUpDecision = checkForScaleUp(decider,\n+            waitingAnomalyJobs,\n+            waitingAnalyticsJobs,\n+            futureFreedCapacity.orElse(null),\n+            currentScale,\n+            reasonBuilder);\n+\n+        if (scaleUpDecision.isPresent()) {\n+            resetScaleDownCoolDown();\n+            return scaleUpDecision.get();\n+        }\n+        if (waitingAnalyticsJobs.isEmpty() == false || waitingAnomalyJobs.isEmpty() == false) {\n+            // We don't want to continue to consider a scale down if there are now waiting jobs\n+            resetScaleDownCoolDown();\n+            noScaleResultOrRefresh(reasonBuilder, memoryTrackingStale, new AutoscalingDeciderResult(\n+                context.currentCapacity(),\n+                reasonBuilder\n+                    .setSimpleReason(\"Passing currently perceived capacity as there are analytics and anomaly jobs in the queue, \" +\n+                        \"but the number in the queue is less than the configured maximum allowed.\")\n+                    .build()));\n+        }\n+        if (mlMemoryTracker.isRecentlyRefreshed(memoryTrackingStale) == false) {\n+            return buildDecisionAndRequestRefresh(reasonBuilder);\n+        }\n+\n+        long largestJob = Math.max(\n+            anomalyDetectionTasks.stream()\n+                .filter(PersistentTask::isAssigned)\n+                // Memory SHOULD be recently refreshed, so in our current state, we should at least have an idea of the memory used\n+                .mapToLong(this::getAnomalyMemoryRequirement)\n+                .max()\n+                .orElse(0L),\n+            dataframeAnalyticsTasks.stream()\n+                .filter(PersistentTask::isAssigned)\n+                // Memory SHOULD be recently refreshed, so in our current state, we should at least have an idea of the memory used\n+                .mapToLong(this::getAnalyticsMemoryRequirement)\n+                .max()\n+                .orElse(0L));\n+\n+        final Optional<AutoscalingDeciderResult> scaleDownDecision =\n+            checkForScaleDown(nodes, clusterState, largestJob, currentScale, reasonBuilder);\n+\n+        if (scaleDownDecision.isPresent()) {\n+            if (newScaleDownCheck()) {\n+                scaleDownDetected = timeSupplier.get();\n+            }\n+            if (canScaleDown(decider.getDownScaleDelay())) {\n+                return scaleDownDecision.get();\n+            }\n+            return new AutoscalingDeciderResult(\n+                context.currentCapacity(),\n+                reasonBuilder\n+                    .setSimpleReason(\n+                        \"Passing currently perceived capacity as configured down scale delay has not be satisfied; configured delay [\"\n+                            + decider.getDownScaleDelay().millis()\n+                            + \"] last detected scale down event [\"\n+                            + scaleDownDetected\n+                            + \"]\")\n+                    .build());\n+        }\n+\n+        return noScaleResultOrRefresh(reasonBuilder, memoryTrackingStale, new AutoscalingDeciderResult(context.currentCapacity(),\n+            reasonBuilder\n+                .setSimpleReason(\"Passing currently perceived capacity as no scaling changes were detected to be possible\")\n+                .build()));\n+    }\n+\n+    AutoscalingDeciderResult noScaleResultOrRefresh(MlScalingReason.Builder reasonBuilder,\n+                                                    Duration memoryTrackingStale,\n+                                                    AutoscalingDeciderResult potentialResult) {\n+        if (mlMemoryTracker.isRecentlyRefreshed(memoryTrackingStale) == false) {\n+            return buildDecisionAndRequestRefresh(reasonBuilder);\n+        } else {\n+            return potentialResult;\n+        }\n+    }\n+\n+    Optional<AutoscalingDeciderResult> checkForScaleUp(MlAutoscalingDeciderConfiguration decider,\n+                                                       List<String> waitingAnomalyJobs,\n+                                                       List<String> waitingAnalyticsJobs,\n+                                                       @Nullable NativeMemoryCapacity futureFreedCapacity,\n+                                                       NativeMemoryCapacity currentScale,\n+                                                       MlScalingReason.Builder reasonBuilder) {\n+\n+        // Are we in breach of maximum waiting jobs?\n+        if (waitingAnalyticsJobs.size() > decider.getNumAnalyticsJobsInQueue()\n+            || waitingAnomalyJobs.size() > decider.getNumAnomalyJobsInQueue()) {\n+            NativeMemoryCapacity updatedCapacity = NativeMemoryCapacity.from(currentScale);\n+            Optional<NativeMemoryCapacity> analyticsCapacity = requiredCapacityForUnassignedJobs(waitingAnalyticsJobs,\n+                this::getAnalyticsMemoryRequirement,\n+                decider.getNumAnalyticsJobsInQueue());\n+            Optional<NativeMemoryCapacity> anomalyCapacity = requiredCapacityForUnassignedJobs(waitingAnomalyJobs,\n+                this::getAnomalyMemoryRequirement,\n+                decider.getNumAnomalyJobsInQueue());\n+\n+            updatedCapacity.merge(anomalyCapacity.orElse(NativeMemoryCapacity.ZERO))\n+                .merge(analyticsCapacity.orElse(NativeMemoryCapacity.ZERO));\n+            return Optional.of(new AutoscalingDeciderResult(\n+                updatedCapacity.autoscalingCapacity(maxMachineMemoryPercent, useAuto),\n+                reasonBuilder.setSimpleReason(\"requesting scale up as number of jobs in queues exceeded configured limit\").build()));\n+        }\n+\n+        // Could the currently waiting jobs ever be assigned?\n+        if (waitingAnalyticsJobs.isEmpty() == false || waitingAnomalyJobs.isEmpty() == false) {\n+            // we are unable to determine new tier size, but maybe we can see if our nodes are big enough.\n+            if (futureFreedCapacity == null) {\n+                Optional<Long> maxSize = Stream.concat(\n+                    waitingAnalyticsJobs.stream().map(mlMemoryTracker::getDataFrameAnalyticsJobMemoryRequirement),\n+                    waitingAnomalyJobs.stream().map(mlMemoryTracker::getAnomalyDetectorJobMemoryRequirement))\n+                    .filter(Objects::nonNull)\n+                    .max(Long::compareTo);\n+                if (maxSize.isPresent() && maxSize.get() > currentScale.getNode()) {\n+                    return Optional.of(new AutoscalingDeciderResult(\n+                        new NativeMemoryCapacity(Math.max(currentScale.getTier(), maxSize.get()), maxSize.get())\n+                            .autoscalingCapacity(maxMachineMemoryPercent, useAuto),\n+                        reasonBuilder.setSimpleReason(\"requesting scale up as there is no node large enough to handle queued jobs\")\n+                            .build()));\n+                }\n+                // we have no info, allow the caller to make the appropriate action, probably returning a no_scale\n+                return Optional.empty();\n+            }\n+            long newTierNeeded = 0L;\n+            // could any of the nodes actually run the job?\n+            long newNodeMax = currentScale.getNode();\n+            for (String analyticsJob : waitingAnalyticsJobs) {\n+                Long requiredMemory = mlMemoryTracker.getDataFrameAnalyticsJobMemoryRequirement(analyticsJob);\n+                // it is OK to continue here as we have not breached our queuing limit\n+                if (requiredMemory == null) {\n+                    continue;\n+                }\n+                // Is there \"future capacity\" on a node that could run this job? If not, we need that much more in the tier.\n+                if (futureFreedCapacity.getNode() < requiredMemory) {\n+                    newTierNeeded = Math.max(requiredMemory, newTierNeeded);\n+                }\n+                newNodeMax = Math.max(newNodeMax, requiredMemory);\n+            }\n+            for (String anomalyJob : waitingAnomalyJobs) {\n+                Long requiredMemory = mlMemoryTracker.getAnomalyDetectorJobMemoryRequirement(anomalyJob);\n+                // it is OK to continue here as we have not breached our queuing limit\n+                if (requiredMemory == null) {\n+                    continue;\n+                }\n+                // Is there \"future capacity\" on a node that could run this job? If not, we need that much more in the tier.\n+                if (futureFreedCapacity.getNode() < requiredMemory) {\n+                    newTierNeeded = Math.max(requiredMemory, newTierNeeded);\n+                }\n+                newNodeMax = Math.max(newNodeMax, requiredMemory);\n+            }\n+            if (newNodeMax > currentScale.getNode() || newTierNeeded > 0L) {\n+                NativeMemoryCapacity newCapacity = new NativeMemoryCapacity(newTierNeeded, newNodeMax);\n+                return Optional.of(new AutoscalingDeciderResult(\n+                    // We need more memory in the tier, or our individual node size requirements has increased\n+                    NativeMemoryCapacity.from(currentScale).merge(newCapacity).autoscalingCapacity(maxMachineMemoryPercent, useAuto),\n+                    reasonBuilder\n+                        .setSimpleReason(\"scaling up as adequate space would not automatically become available when running jobs finish\")\n+                        .build()\n+                ));\n+            }\n+        }\n+\n+        return Optional.empty();\n+    }\n+\n+    // This calculates the the following the potentially automatically free capacity of sometime in the future\n+    // Since jobs with lookback only datafeeds, and data frame analytics jobs all have some potential future end date\n+    // we can assume (without user intervention) that these will eventually stop and free their currently occupied resources.\n+    //\n+    // The capacity is as follows:\n+    //  tier: The sum total of the resources that will be removed\n+    //  node: The largest block of memory that will be freed on a given node.\n+    //      - If > 1 \"batch\" ml tasks are running on the same node, we sum their resources.\n+    Optional<NativeMemoryCapacity> calculateFutureFreedCapacity(PersistentTasksCustomMetadata tasks, Duration jobMemoryExpiry) {\n+        final List<PersistentTask<DatafeedParams>> jobsWithLookbackDatafeeds = datafeedTasks(tasks).stream()\n+            .filter(t -> t.getParams().getEndTime() != null && t.getExecutorNode() != null)\n+            .collect(Collectors.toList());\n+        final List<PersistentTask<?>> assignedAnalyticsJobs = dataframeAnalyticsTasks(tasks).stream()\n+            .filter(t -> t.getExecutorNode() != null)\n+            .collect(Collectors.toList());\n+\n+        if (jobsWithLookbackDatafeeds.isEmpty() && assignedAnalyticsJobs.isEmpty()) {\n+            return Optional.of(NativeMemoryCapacity.ZERO);\n+        }\n+        if (mlMemoryTracker.isRecentlyRefreshed(jobMemoryExpiry) == false) {\n+            return Optional.empty();\n+        }\n+        // What is the largest chunk of memory that could be freed on a node in the future\n+        Map<String, Long> maxNodeBytes = new HashMap<>();\n+        for (PersistentTask<DatafeedParams> lookbackOnlyDf : jobsWithLookbackDatafeeds) {\n+            Long jobSize = mlMemoryTracker.getAnomalyDetectorJobMemoryRequirement(lookbackOnlyDf.getParams().getJobId());\n+            if (jobSize == null) {\n+                return Optional.empty();\n+            }\n+            maxNodeBytes.compute(lookbackOnlyDf.getExecutorNode(), (_k, v) -> v == null ? jobSize : jobSize + v);\n+        }\n+        for (PersistentTask<?> task : assignedAnalyticsJobs) {\n+            Long jobSize = mlMemoryTracker.getDataFrameAnalyticsJobMemoryRequirement(MlTasks.dataFrameAnalyticsId(task.getId()));\n+            if (jobSize == null) {\n+                return Optional.empty();\n+            }\n+            maxNodeBytes.compute(task.getExecutorNode(), (_k, v) -> v == null ? jobSize : jobSize + v);\n+        }\n+        return Optional.of(new NativeMemoryCapacity(\n+            maxNodeBytes.values().stream().mapToLong(Long::longValue).sum(),\n+            maxNodeBytes.values().stream().mapToLong(Long::longValue).max().orElse(0L)));\n+    }\n+\n+    private AutoscalingDeciderResult buildDecisionAndRequestRefresh(MlScalingReason.Builder reasonBuilder) {\n+        mlMemoryTracker.asyncRefresh();\n+        return new AutoscalingDeciderResult(null, reasonBuilder.setSimpleReason(MEMORY_STALE).build());\n+    }\n+\n+    private Long getAnalyticsMemoryRequirement(String analyticsId) {\n+        return mlMemoryTracker.getDataFrameAnalyticsJobMemoryRequirement(analyticsId);\n+    }\n+\n+    private Long getAnalyticsMemoryRequirement(PersistentTask<?> task) {\n+        return getAnalyticsMemoryRequirement(MlTasks.dataFrameAnalyticsId(task.getId()));\n+    }\n+\n+    private Long getAnomalyMemoryRequirement(String anomalyId) {\n+        return mlMemoryTracker.getAnomalyDetectorJobMemoryRequirement(anomalyId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6536073d0983c0cc4c49f30c41392a2b2a23fff0"}, "originalPosition": 505}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ac8ca5913958f4e7ff245ea91ae0dd9f307856e9", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/ac8ca5913958f4e7ff245ea91ae0dd9f307856e9", "committedDate": "2020-11-17T13:30:33Z", "message": "adding logging"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMyMzc2OTE3", "url": "https://github.com/elastic/elasticsearch/pull/59309#pullrequestreview-532376917", "createdAt": "2020-11-17T13:47:51Z", "commit": {"oid": "ac8ca5913958f4e7ff245ea91ae0dd9f307856e9"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QxMzo0Nzo1MVrOH01ozQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QxMzo0Nzo1MVrOH01ozQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTE2Njc5Nw==", "bodyText": "Does expectThrows work with AssertionError?  If it does then that would be the more idiomatic way to write this.", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r525166797", "createdAt": "2020-11-17T13:47:51Z", "author": {"login": "droberts195"}, "path": "x-pack/plugin/ml/src/test/java/org/elasticsearch/xpack/ml/autoscaling/MlAutoscalingDeciderServiceTests.java", "diffHunk": "@@ -213,11 +213,17 @@ public void testScaleDown_WhenMemoryIsInaccurate() {\n \n         MlAutoscalingDeciderService service = buildService();\n         MlScalingReason.Builder reasonBuilder = new MlScalingReason.Builder();\n-        assertThat(service.checkForScaleDown(nodes,\n-            ClusterState.EMPTY_STATE,\n-            Long.MAX_VALUE,\n-            new NativeMemoryCapacity(ByteSizeValue.ofGb(3).getBytes(), ByteSizeValue.ofGb(1).getBytes()),\n-            reasonBuilder).isEmpty(), is(true));\n+        try {\n+            service.checkForScaleDown(nodes,\n+                ClusterState.EMPTY_STATE,\n+                Long.MAX_VALUE,\n+                new NativeMemoryCapacity(ByteSizeValue.ofGb(3).getBytes(), ByteSizeValue.ofGb(1).getBytes()),\n+                reasonBuilder);\n+        } catch (AssertionError ae) {\n+            // the scale method should thrown an assertion failure\n+            return;\n+        }\n+        assert false : \"call for scale down should have thrown an assertion error\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ac8ca5913958f4e7ff245ea91ae0dd9f307856e9"}, "originalPosition": 19}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "29adb17ec25ea87f349cfb756412a058990b68f0", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/29adb17ec25ea87f349cfb756412a058990b68f0", "committedDate": "2020-11-17T14:40:36Z", "message": "fixing test"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2127, "cost": 1, "resetAt": "2021-10-28T18:54:27Z"}}}