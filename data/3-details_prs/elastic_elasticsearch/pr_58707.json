{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQxNjc3NTAw", "number": 58707, "title": "Data Stream Stats API", "bodyText": "This API will report on statistics important for data streams. Goal statistics to gather in this API are:\n\nNumber of data streams\nNumber of backing indices for those streams\nThe disk usage for each data stream\nMaximum timestamp for each data stream\n\nRelates to #58316 #53100", "createdAt": "2020-06-29T22:15:30Z", "url": "https://github.com/elastic/elasticsearch/pull/58707", "merged": true, "mergeCommit": {"oid": "589bb1f26c03f6e1ad2aba27bc14f1cd0c637471"}, "closed": true, "closedAt": "2020-07-14T18:51:32Z", "author": {"login": "jbaiera"}, "timelineItems": {"totalCount": 37, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcwF9nCAH2gAyNDQxNjc3NTAwOjNjMmQ0ZGVlMTE5MTMxNmYzNDNjNzAwOTM3MTQzMzI2MzcyMzYxYjY=", "endCursor": "Y3Vyc29yOnYyOpPPAAABc05zCogH2gAyNDQxNjc3NTAwOmQzYjhiZTM5OGUwYzRiZWY4MGE2MWQxYzdmNjM0YTM4YjliOGY1ZGU=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "3c2d4dee1191316f343c700937143326372361b6", "author": {"user": {"login": "jbaiera", "name": "James Baiera"}}, "url": "https://github.com/elastic/elasticsearch/commit/3c2d4dee1191316f343c700937143326372361b6", "committedDate": "2020-06-29T19:11:16Z", "message": "WIP"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9505e33bde84ce157eea132296e3aa5765db0d4f", "author": {"user": {"login": "jbaiera", "name": "James Baiera"}}, "url": "https://github.com/elastic/elasticsearch/commit/9505e33bde84ce157eea132296e3aa5765db0d4f", "committedDate": "2020-06-29T22:07:23Z", "message": "Some clean up"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM5NTM5OTY4", "url": "https://github.com/elastic/elasticsearch/pull/58707#pullrequestreview-439539968", "createdAt": "2020-06-29T22:16:21Z", "commit": {"oid": "9505e33bde84ce157eea132296e3aa5765db0d4f"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOVQyMjoxNjoyMVrOGqkQpA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOVQyMjoxNzoxOFrOGqkT2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI4NzQ2MA==", "bodyText": "This is on the short list to move to a utility where the logic can be shared between the places it is used.", "url": "https://github.com/elastic/elasticsearch/pull/58707#discussion_r447287460", "createdAt": "2020-06-29T22:16:21Z", "author": {"login": "jbaiera"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/stats/TransportDataStreamsStatsAction.java", "diffHunk": "@@ -0,0 +1,304 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.stats;\n+\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.IndicesOptions;\n+import org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexAbstraction;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.Metadata;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.routing.ShardsIterator;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.regex.Regex;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.index.IndexNotFoundException;\n+import org.elasticsearch.index.IndexService;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardNotFoundException;\n+import org.elasticsearch.index.store.StoreStats;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.SortedMap;\n+import java.util.stream.Stream;\n+\n+public class TransportDataStreamsStatsAction extends TransportBroadcastByNodeAction<DataStreamsStatsRequest, DataStreamsStatsResponse,\n+    DataStreamShardStats> {\n+\n+    private final IndicesService indicesService;\n+    private final IndexNameExpressionResolver indexNameExpressionResolver;\n+\n+    @Inject\n+    public TransportDataStreamsStatsAction(ClusterService clusterService, TransportService transportService, IndicesService indicesService,\n+                                       ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver) {\n+        super(DataStreamStatsAction.NAME, clusterService, transportService, actionFilters, indexNameExpressionResolver,\n+            DataStreamsStatsRequest::new, ThreadPool.Names.MANAGEMENT);\n+        this.indicesService = indicesService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+    }\n+\n+    @Override\n+    protected DataStreamsStatsRequest readRequestFrom(StreamInput in) throws IOException {\n+        return new DataStreamsStatsRequest(in);\n+    }\n+\n+    @Override\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, DataStreamsStatsRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.METADATA_READ);\n+    }\n+\n+    @Override\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, DataStreamsStatsRequest request, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.METADATA_READ, concreteIndices);\n+    }\n+\n+    private static List<String> resolveIndexAbstractions(String[] indices, IndicesOptions indicesOptions, Metadata metadata,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9505e33bde84ce157eea132296e3aa5765db0d4f"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI4ODI4Mw==", "bodyText": "Working on doing the lucene searching at the shard level to obtain this. Could use some guidance here.", "url": "https://github.com/elastic/elasticsearch/pull/58707#discussion_r447288283", "createdAt": "2020-06-29T22:17:18Z", "author": {"login": "jbaiera"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/stats/TransportDataStreamsStatsAction.java", "diffHunk": "@@ -0,0 +1,304 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.stats;\n+\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.IndicesOptions;\n+import org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexAbstraction;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.Metadata;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.routing.ShardsIterator;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.regex.Regex;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.index.IndexNotFoundException;\n+import org.elasticsearch.index.IndexService;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardNotFoundException;\n+import org.elasticsearch.index.store.StoreStats;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.SortedMap;\n+import java.util.stream.Stream;\n+\n+public class TransportDataStreamsStatsAction extends TransportBroadcastByNodeAction<DataStreamsStatsRequest, DataStreamsStatsResponse,\n+    DataStreamShardStats> {\n+\n+    private final IndicesService indicesService;\n+    private final IndexNameExpressionResolver indexNameExpressionResolver;\n+\n+    @Inject\n+    public TransportDataStreamsStatsAction(ClusterService clusterService, TransportService transportService, IndicesService indicesService,\n+                                       ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver) {\n+        super(DataStreamStatsAction.NAME, clusterService, transportService, actionFilters, indexNameExpressionResolver,\n+            DataStreamsStatsRequest::new, ThreadPool.Names.MANAGEMENT);\n+        this.indicesService = indicesService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+    }\n+\n+    @Override\n+    protected DataStreamsStatsRequest readRequestFrom(StreamInput in) throws IOException {\n+        return new DataStreamsStatsRequest(in);\n+    }\n+\n+    @Override\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, DataStreamsStatsRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.METADATA_READ);\n+    }\n+\n+    @Override\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, DataStreamsStatsRequest request, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.METADATA_READ, concreteIndices);\n+    }\n+\n+    private static List<String> resolveIndexAbstractions(String[] indices, IndicesOptions indicesOptions, Metadata metadata,\n+                                                         IndexNameExpressionResolver indexNameExpressionResolver) {\n+        final boolean replaceWildcards = indicesOptions.expandWildcardsOpen() || indicesOptions.expandWildcardsClosed();\n+        Set<String> availableIndexAbstractions = metadata.getIndicesLookup().keySet();\n+        List<String> finalIndices = new ArrayList<>();\n+        boolean wildcardSeen = false;\n+        for (String index : indices) {\n+            String indexAbstraction;\n+            boolean minus = false;\n+            if (index.charAt(0) == '-' && wildcardSeen) {\n+                indexAbstraction = index.substring(1);\n+                minus = true;\n+            } else {\n+                indexAbstraction = index;\n+            }\n+\n+            // we always need to check for date math expressions\n+            final String dateMathName = indexNameExpressionResolver.resolveDateMathExpression(indexAbstraction);\n+            if (dateMathName != indexAbstraction) {\n+                assert dateMathName.equals(indexAbstraction) == false;\n+                if (replaceWildcards && Regex.isSimpleMatchPattern(dateMathName)) {\n+                    // continue\n+                    indexAbstraction = dateMathName;\n+                } else if (availableIndexAbstractions.contains(dateMathName) &&\n+                    isIndexVisible(indexAbstraction, dateMathName, indicesOptions, metadata, true)) {\n+                    if (minus) {\n+                        finalIndices.remove(dateMathName);\n+                    } else {\n+                        finalIndices.add(dateMathName);\n+                    }\n+                } else {\n+                    if (indicesOptions.ignoreUnavailable() == false) {\n+                        throw new IndexNotFoundException(dateMathName);\n+                    }\n+                }\n+            }\n+\n+            if (replaceWildcards && Regex.isSimpleMatchPattern(indexAbstraction)) {\n+                wildcardSeen = true;\n+                Set<String> resolvedIndices = new HashSet<>();\n+                for (String authorizedIndex : availableIndexAbstractions) {\n+                    if (Regex.simpleMatch(indexAbstraction, authorizedIndex) &&\n+                        isIndexVisible(indexAbstraction, authorizedIndex, indicesOptions, metadata)) {\n+                        resolvedIndices.add(authorizedIndex);\n+                    }\n+                }\n+                if (resolvedIndices.isEmpty()) {\n+                    //es core honours allow_no_indices for each wildcard expression, we do the same here by throwing index not found.\n+                    if (indicesOptions.allowNoIndices() == false) {\n+                        throw new IndexNotFoundException(indexAbstraction);\n+                    }\n+                } else {\n+                    if (minus) {\n+                        finalIndices.removeAll(resolvedIndices);\n+                    } else {\n+                        finalIndices.addAll(resolvedIndices);\n+                    }\n+                }\n+            } else if (dateMathName.equals(indexAbstraction)) {\n+                if (minus) {\n+                    finalIndices.remove(indexAbstraction);\n+                } else {\n+                    finalIndices.add(indexAbstraction);\n+                }\n+            }\n+        }\n+        return finalIndices;\n+    }\n+\n+    private static boolean isIndexVisible(String expression, String index, IndicesOptions indicesOptions, Metadata metadata) {\n+        return isIndexVisible(expression, index, indicesOptions, metadata, false);\n+    }\n+\n+    private static boolean isIndexVisible(String expression, String index, IndicesOptions indicesOptions, Metadata metadata,\n+                                          boolean dateMathExpression) {\n+        IndexAbstraction indexAbstraction = metadata.getIndicesLookup().get(index);\n+        final boolean isHidden = indexAbstraction.isHidden();\n+        if (indexAbstraction.getType() == IndexAbstraction.Type.ALIAS) {\n+            //it's an alias, ignore expandWildcardsOpen and expandWildcardsClosed.\n+            //complicated to support those options with aliases pointing to multiple indices...\n+            if (indicesOptions.ignoreAliases()) {\n+                return false;\n+            } else if (isHidden == false || indicesOptions.expandWildcardsHidden() || isVisibleDueToImplicitHidden(expression, index)) {\n+                return true;\n+            } else {\n+                return false;\n+            }\n+        }\n+        if (indexAbstraction.getType() == IndexAbstraction.Type.DATA_STREAM) {\n+            // If indicesOptions.includeDataStreams() returns false then we fail later in IndexNameExpressionResolver.\n+            if (isHidden == false || indicesOptions.expandWildcardsHidden()) {\n+                return true;\n+            } else {\n+                return false;\n+            }\n+        }\n+        assert indexAbstraction.getIndices().size() == 1 : \"concrete index must point to a single index\";\n+        IndexMetadata indexMetadata = indexAbstraction.getIndices().get(0);\n+        if (isHidden && indicesOptions.expandWildcardsHidden() == false && isVisibleDueToImplicitHidden(expression, index) == false) {\n+            return false;\n+        }\n+\n+        // the index is not hidden and since it is a date math expression, we consider it visible regardless of open/closed\n+        if (dateMathExpression) {\n+            assert IndexMetadata.State.values().length == 2 : \"a new IndexMetadata.State value may need to be handled!\";\n+            return true;\n+        }\n+        if (indexMetadata.getState() == IndexMetadata.State.CLOSE && indicesOptions.expandWildcardsClosed()) {\n+            return true;\n+        }\n+        if (indexMetadata.getState() == IndexMetadata.State.OPEN && indicesOptions.expandWildcardsOpen()) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    private static boolean isVisibleDueToImplicitHidden(String expression, String index) {\n+        return index.startsWith(\".\") && expression.startsWith(\".\") && Regex.isSimpleMatchPattern(expression);\n+    }\n+\n+    @Override\n+    protected ShardsIterator shards(ClusterState clusterState, DataStreamsStatsRequest request, String[] concreteIndices) {\n+        String[] requestIndices = request.indices();\n+        if (requestIndices == null || requestIndices.length == 0) {\n+            requestIndices = new String[]{\"*\"};\n+        }\n+        List<String> abstractionNames = resolveIndexAbstractions(requestIndices, request.indicesOptions(), clusterState.getMetadata(), indexNameExpressionResolver);\n+        SortedMap<String, IndexAbstraction> indicesLookup = clusterState.getMetadata().getIndicesLookup();\n+\n+        String[] concreteDatastreamIndices = abstractionNames.stream().flatMap(abstractionName -> {\n+            IndexAbstraction indexAbstraction = indicesLookup.get(abstractionName);\n+            assert indexAbstraction != null;\n+            if (indexAbstraction.getType() == IndexAbstraction.Type.DATA_STREAM) {\n+                IndexAbstraction.DataStream dataStream = (IndexAbstraction.DataStream) indexAbstraction;\n+                List<IndexMetadata> indices = dataStream.getIndices();\n+                return indices.stream().map(idx -> idx.getIndex().getName());\n+            } else {\n+                return Stream.empty();\n+            }\n+        }).toArray(String[]::new);\n+        return clusterState.getRoutingTable().allShards(concreteDatastreamIndices);\n+    }\n+\n+    @Override\n+    protected DataStreamShardStats shardOperation(DataStreamsStatsRequest request, ShardRouting shardRouting) throws IOException {\n+        IndexService indexService = indicesService.indexServiceSafe(shardRouting.shardId().getIndex());\n+        IndexShard indexShard = indexService.getShard(shardRouting.shardId().id());\n+        // if we don't have the routing entry yet, we need it stats wise, we treat it as if the shard is not ready yet\n+        if (indexShard.routingEntry() == null) {\n+            throw new ShardNotFoundException(indexShard.shardId());\n+        }\n+        StoreStats storeStats = indexShard.storeStats();\n+        return new DataStreamShardStats(\n+            indexShard.routingEntry(),\n+            storeStats,\n+            0L //TODO", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9505e33bde84ce157eea132296e3aa5765db0d4f"}, "originalPosition": 244}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9c0271d9b953442a2d0308b587ca3bc9fd8f3c4a", "author": {"user": {"login": "jbaiera", "name": "James Baiera"}}, "url": "https://github.com/elastic/elasticsearch/commit/9c0271d9b953442a2d0308b587ca3bc9fd8f3c4a", "committedDate": "2020-06-30T19:24:20Z", "message": "Merge branch 'master' into data-stream-stats"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b3a903a30ba1158cbbfe33ff27c84055ce50f426", "author": {"user": {"login": "jbaiera", "name": "James Baiera"}}, "url": "https://github.com/elastic/elasticsearch/commit/b3a903a30ba1158cbbfe33ff27c84055ce50f426", "committedDate": "2020-06-30T23:05:23Z", "message": "Add collection of max timestamp.\n\nClean up precommit errors.\nAdd some more testing."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3ad81e6acdb7cc05818a39a10b0d935080f8ad1a", "author": {"user": {"login": "jbaiera", "name": "James Baiera"}}, "url": "https://github.com/elastic/elasticsearch/commit/3ad81e6acdb7cc05818a39a10b0d935080f8ad1a", "committedDate": "2020-07-01T17:43:20Z", "message": "De-duplicate index abstraction resolution code"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0f3f6ef839a87e8f98813dcbed1b9dba30648868", "author": {"user": {"login": "jbaiera", "name": "James Baiera"}}, "url": "https://github.com/elastic/elasticsearch/commit/0f3f6ef839a87e8f98813dcbed1b9dba30648868", "committedDate": "2020-07-01T18:56:26Z", "message": "Collapse stats classes into one file.\n\nFix some compilation errors."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c1dbc97a71833f56f042aec6b07b34b9e7342668", "author": {"user": {"login": "jbaiera", "name": "James Baiera"}}, "url": "https://github.com/elastic/elasticsearch/commit/c1dbc97a71833f56f042aec6b07b34b9e7342668", "committedDate": "2020-07-02T05:44:57Z", "message": "Rest API, spec, and tests.\n\nStandardize action name (\"streams\" plural)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a6e43946398966adc91e5f512342d05455afa50a", "author": {"user": {"login": "jbaiera", "name": "James Baiera"}}, "url": "https://github.com/elastic/elasticsearch/commit/a6e43946398966adc91e5f512342d05455afa50a", "committedDate": "2020-07-02T05:58:16Z", "message": "Move the new code to the data stream package"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b8bf667ee91eaed952cc3b35d308bc1b0b4c6184", "author": {"user": {"login": "jbaiera", "name": "James Baiera"}}, "url": "https://github.com/elastic/elasticsearch/commit/b8bf667ee91eaed952cc3b35d308bc1b0b4c6184", "committedDate": "2020-07-02T16:39:09Z", "message": "Fix some errors"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9e83e67b16e93e13d749b5bc0b5be65e9f9f6596", "author": {"user": {"login": "jbaiera", "name": "James Baiera"}}, "url": "https://github.com/elastic/elasticsearch/commit/9e83e67b16e93e13d749b5bc0b5be65e9f9f6596", "committedDate": "2020-07-02T16:40:02Z", "message": "Cleanup"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "74cef55e49c4cbbe0da787fae04d2e34b4cf2733", "author": {"user": {"login": "jbaiera", "name": "James Baiera"}}, "url": "https://github.com/elastic/elasticsearch/commit/74cef55e49c4cbbe0da787fae04d2e34b4cf2733", "committedDate": "2020-07-02T20:49:47Z", "message": "Added HLRC methods. Standardized method/field names."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyOTAyNjc4", "url": "https://github.com/elastic/elasticsearch/pull/58707#pullrequestreview-442902678", "createdAt": "2020-07-06T09:08:45Z", "commit": {"oid": "74cef55e49c4cbbe0da787fae04d2e34b4cf2733"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNlQwOTowODo0NVrOGtPIiw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNlQwOToxNTo0N1rOGtPYUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDA4NzA1MQ==", "bodyText": "Maybe we should fail the request if anything other than a data stream is specified?", "url": "https://github.com/elastic/elasticsearch/pull/58707#discussion_r450087051", "createdAt": "2020-07-06T09:08:45Z", "author": {"login": "martijnvg"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/datastream/DataStreamsStatsAction.java", "diffHunk": "@@ -0,0 +1,377 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.datastream;\n+\n+import org.apache.lucene.document.LongPoint;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.PointValues;\n+import org.elasticsearch.action.ActionType;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.broadcast.BroadcastRequest;\n+import org.elasticsearch.action.support.broadcast.BroadcastResponse;\n+import org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexAbstraction;\n+import org.elasticsearch.cluster.metadata.IndexAbstractionResolver;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.routing.ShardsIterator;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.io.stream.Writeable;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.common.xcontent.ToXContentObject;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.index.IndexService;\n+import org.elasticsearch.index.engine.Engine;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardNotFoundException;\n+import org.elasticsearch.index.store.StoreStats;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.SortedMap;\n+import java.util.stream.Stream;\n+\n+public class DataStreamsStatsAction extends ActionType<DataStreamsStatsAction.Response> {\n+\n+    public static final DataStreamsStatsAction INSTANCE = new DataStreamsStatsAction();\n+    public static final String NAME = \"indices:monitor/data_stream/stats\";\n+\n+    public DataStreamsStatsAction() {\n+        super(NAME, DataStreamsStatsAction.Response::new);\n+    }\n+\n+    public static class Request extends BroadcastRequest<Request> {\n+        public Request() {\n+            super((String[]) null);\n+        }\n+\n+        public Request(StreamInput in) throws IOException {\n+            super(in);\n+        }\n+    }\n+\n+    public static class Response extends BroadcastResponse {\n+        private final int dataStreamCount;\n+        private final int backingIndices;\n+        private final ByteSizeValue totalStoreSize;\n+        private final DataStreamStats[] dataStreams;\n+\n+        public Response(int totalShards, int successfulShards, int failedShards, List<DefaultShardOperationFailedException> shardFailures,\n+                        int dataStreamCount, int backingIndices, ByteSizeValue totalStoreSize, DataStreamStats[] dataStreams) {\n+            super(totalShards, successfulShards, failedShards, shardFailures);\n+            this.dataStreamCount = dataStreamCount;\n+            this.backingIndices = backingIndices;\n+            this.totalStoreSize = totalStoreSize;\n+            this.dataStreams = dataStreams;\n+        }\n+\n+        public Response(StreamInput in) throws IOException {\n+            super(in);\n+            this.dataStreamCount = in.readVInt();\n+            this.backingIndices = in.readVInt();\n+            this.totalStoreSize = new ByteSizeValue(in);\n+            this.dataStreams = in.readArray(DataStreamStats::new, DataStreamStats[]::new);\n+        }\n+\n+        @Override\n+        public void writeTo(StreamOutput out) throws IOException {\n+            super.writeTo(out);\n+            out.writeVInt(dataStreamCount);\n+            out.writeVInt(backingIndices);\n+            totalStoreSize.writeTo(out);\n+            out.writeArray(dataStreams);\n+        }\n+\n+        @Override\n+        protected void addCustomXContentFields(XContentBuilder builder, Params params) throws IOException {\n+            builder.field(\"data_stream_count\", dataStreamCount);\n+            builder.field(\"backing_indices\", backingIndices);\n+            builder.humanReadableField(\"total_store_size_bytes\", \"total_store_size\", totalStoreSize);\n+            builder.array(\"data_streams\", (Object[]) dataStreams);\n+        }\n+\n+        public int getDataStreamCount() {\n+            return dataStreamCount;\n+        }\n+\n+        public int getBackingIndices() {\n+            return backingIndices;\n+        }\n+\n+        public ByteSizeValue getTotalStoreSize() {\n+            return totalStoreSize;\n+        }\n+\n+        public DataStreamStats[] getDataStreams() {\n+            return dataStreams;\n+        }\n+    }\n+\n+    public static class DataStreamStats implements ToXContentObject, Writeable {\n+        private final String dataStream;\n+        private final int backingIndices;\n+        private final ByteSizeValue storeSize;\n+        private final long maximumTimestamp;\n+\n+        public DataStreamStats(String dataStream, int backingIndices, ByteSizeValue storeSize, long maximumTimestamp) {\n+            this.dataStream = dataStream;\n+            this.backingIndices = backingIndices;\n+            this.storeSize = storeSize;\n+            this.maximumTimestamp = maximumTimestamp;\n+        }\n+\n+        public DataStreamStats(StreamInput in) throws IOException {\n+            this.dataStream = in.readString();\n+            this.backingIndices = in.readVInt();\n+            this.storeSize = new ByteSizeValue(in);\n+            this.maximumTimestamp = in.readVLong();\n+        }\n+\n+        @Override\n+        public void writeTo(StreamOutput out) throws IOException {\n+            out.writeString(dataStream);\n+            out.writeVInt(backingIndices);\n+            storeSize.writeTo(out);\n+            out.writeVLong(maximumTimestamp);\n+        }\n+\n+        @Override\n+        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+            builder.startObject();\n+            builder.field(\"data_stream\", dataStream);\n+            builder.field(\"backing_indices\", backingIndices);\n+            builder.humanReadableField(\"store_size_bytes\", \"store_size\", storeSize);\n+            builder.field(\"maximum_timestamp\", maximumTimestamp);\n+            builder.endObject();\n+            return builder;\n+        }\n+\n+        public String getDataStream() {\n+            return dataStream;\n+        }\n+\n+        public int getBackingIndices() {\n+            return backingIndices;\n+        }\n+\n+        public ByteSizeValue getStoreSize() {\n+            return storeSize;\n+        }\n+\n+        public long getMaximumTimestamp() {\n+            return maximumTimestamp;\n+        }\n+    }\n+\n+    public static class DataStreamShardStats implements Writeable {\n+        private final ShardRouting shardRouting;\n+        private final StoreStats storeStats;\n+        private final long maxTimestamp;\n+\n+        public DataStreamShardStats(ShardRouting shardRouting, StoreStats storeStats, long maxTimestamp) {\n+            this.shardRouting = shardRouting;\n+            this.storeStats = storeStats;\n+            this.maxTimestamp = maxTimestamp;\n+        }\n+\n+        public DataStreamShardStats(StreamInput in) throws IOException {\n+            this.shardRouting = new ShardRouting(in);\n+            this.storeStats = new StoreStats(in);\n+            this.maxTimestamp = in.readVLong();\n+        }\n+\n+        @Override\n+        public void writeTo(StreamOutput out) throws IOException {\n+            shardRouting.writeTo(out);\n+            storeStats.writeTo(out);\n+            out.writeVLong(maxTimestamp);\n+        }\n+\n+        public ShardRouting getShardRouting() {\n+            return shardRouting;\n+        }\n+\n+        public StoreStats getStoreStats() {\n+            return storeStats;\n+        }\n+\n+        public long getMaxTimestamp() {\n+            return maxTimestamp;\n+        }\n+    }\n+\n+    private static class AggregatedStats {\n+        Set<String> backingIndices = new HashSet<>();\n+        long storageBytes = 0L;\n+        long maxTimestamp = 0L;\n+    }\n+\n+    public static class TransportAction extends TransportBroadcastByNodeAction<Request, Response, DataStreamShardStats> {\n+\n+        private final ClusterService clusterService;\n+        private final IndicesService indicesService;\n+        private final IndexAbstractionResolver indexAbstractionResolver;\n+\n+        @Inject\n+        public TransportAction(ClusterService clusterService, TransportService transportService, IndicesService indicesService,\n+                                               ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver) {\n+            super(DataStreamsStatsAction.NAME, clusterService, transportService, actionFilters, indexNameExpressionResolver,\n+                Request::new, ThreadPool.Names.MANAGEMENT);\n+            this.clusterService = clusterService;\n+            this.indicesService = indicesService;\n+            this.indexAbstractionResolver = new IndexAbstractionResolver(indexNameExpressionResolver);\n+        }\n+\n+        @Override\n+        protected Request readRequestFrom(StreamInput in) throws IOException {\n+            return new Request(in);\n+        }\n+\n+        @Override\n+        protected ClusterBlockException checkGlobalBlock(ClusterState state, Request request) {\n+            return state.blocks().globalBlockedException(ClusterBlockLevel.METADATA_READ);\n+        }\n+\n+        @Override\n+        protected ClusterBlockException checkRequestBlock(ClusterState state, Request request, String[] concreteIndices) {\n+            return state.blocks().indicesBlockedException(ClusterBlockLevel.METADATA_READ, concreteIndices);\n+        }\n+\n+        @Override\n+        protected ShardsIterator shards(ClusterState clusterState, Request request, String[] concreteIndices) {\n+            String[] requestIndices = request.indices();\n+            if (requestIndices == null || requestIndices.length == 0) {\n+                requestIndices = new String[]{\"*\"};\n+            }\n+            List<String> abstractionNames = indexAbstractionResolver.resolveIndexAbstractions(requestIndices, request.indicesOptions(),\n+                clusterState.getMetadata());\n+            SortedMap<String, IndexAbstraction> indicesLookup = clusterState.getMetadata().getIndicesLookup();\n+\n+            String[] concreteDatastreamIndices = abstractionNames.stream().flatMap(abstractionName -> {\n+                IndexAbstraction indexAbstraction = indicesLookup.get(abstractionName);\n+                assert indexAbstraction != null;\n+                if (indexAbstraction.getType() == IndexAbstraction.Type.DATA_STREAM) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "74cef55e49c4cbbe0da787fae04d2e34b4cf2733"}, "originalPosition": 285}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDA4OTg5NQ==", "bodyText": "I think we only need to capture this information from the latest backing index?\nAll other backing indices should have max timestamp that is lower than\nthe timestamp in the latest index.\nMaybe we need to collect the max timestamp from the two latest backing indices,\nin the case the write index just rolled over.", "url": "https://github.com/elastic/elasticsearch/pull/58707#discussion_r450089895", "createdAt": "2020-07-06T09:13:53Z", "author": {"login": "martijnvg"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/datastream/DataStreamsStatsAction.java", "diffHunk": "@@ -0,0 +1,377 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.datastream;\n+\n+import org.apache.lucene.document.LongPoint;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.PointValues;\n+import org.elasticsearch.action.ActionType;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.broadcast.BroadcastRequest;\n+import org.elasticsearch.action.support.broadcast.BroadcastResponse;\n+import org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexAbstraction;\n+import org.elasticsearch.cluster.metadata.IndexAbstractionResolver;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.routing.ShardsIterator;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.io.stream.Writeable;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.common.xcontent.ToXContentObject;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.index.IndexService;\n+import org.elasticsearch.index.engine.Engine;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardNotFoundException;\n+import org.elasticsearch.index.store.StoreStats;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.SortedMap;\n+import java.util.stream.Stream;\n+\n+public class DataStreamsStatsAction extends ActionType<DataStreamsStatsAction.Response> {\n+\n+    public static final DataStreamsStatsAction INSTANCE = new DataStreamsStatsAction();\n+    public static final String NAME = \"indices:monitor/data_stream/stats\";\n+\n+    public DataStreamsStatsAction() {\n+        super(NAME, DataStreamsStatsAction.Response::new);\n+    }\n+\n+    public static class Request extends BroadcastRequest<Request> {\n+        public Request() {\n+            super((String[]) null);\n+        }\n+\n+        public Request(StreamInput in) throws IOException {\n+            super(in);\n+        }\n+    }\n+\n+    public static class Response extends BroadcastResponse {\n+        private final int dataStreamCount;\n+        private final int backingIndices;\n+        private final ByteSizeValue totalStoreSize;\n+        private final DataStreamStats[] dataStreams;\n+\n+        public Response(int totalShards, int successfulShards, int failedShards, List<DefaultShardOperationFailedException> shardFailures,\n+                        int dataStreamCount, int backingIndices, ByteSizeValue totalStoreSize, DataStreamStats[] dataStreams) {\n+            super(totalShards, successfulShards, failedShards, shardFailures);\n+            this.dataStreamCount = dataStreamCount;\n+            this.backingIndices = backingIndices;\n+            this.totalStoreSize = totalStoreSize;\n+            this.dataStreams = dataStreams;\n+        }\n+\n+        public Response(StreamInput in) throws IOException {\n+            super(in);\n+            this.dataStreamCount = in.readVInt();\n+            this.backingIndices = in.readVInt();\n+            this.totalStoreSize = new ByteSizeValue(in);\n+            this.dataStreams = in.readArray(DataStreamStats::new, DataStreamStats[]::new);\n+        }\n+\n+        @Override\n+        public void writeTo(StreamOutput out) throws IOException {\n+            super.writeTo(out);\n+            out.writeVInt(dataStreamCount);\n+            out.writeVInt(backingIndices);\n+            totalStoreSize.writeTo(out);\n+            out.writeArray(dataStreams);\n+        }\n+\n+        @Override\n+        protected void addCustomXContentFields(XContentBuilder builder, Params params) throws IOException {\n+            builder.field(\"data_stream_count\", dataStreamCount);\n+            builder.field(\"backing_indices\", backingIndices);\n+            builder.humanReadableField(\"total_store_size_bytes\", \"total_store_size\", totalStoreSize);\n+            builder.array(\"data_streams\", (Object[]) dataStreams);\n+        }\n+\n+        public int getDataStreamCount() {\n+            return dataStreamCount;\n+        }\n+\n+        public int getBackingIndices() {\n+            return backingIndices;\n+        }\n+\n+        public ByteSizeValue getTotalStoreSize() {\n+            return totalStoreSize;\n+        }\n+\n+        public DataStreamStats[] getDataStreams() {\n+            return dataStreams;\n+        }\n+    }\n+\n+    public static class DataStreamStats implements ToXContentObject, Writeable {\n+        private final String dataStream;\n+        private final int backingIndices;\n+        private final ByteSizeValue storeSize;\n+        private final long maximumTimestamp;\n+\n+        public DataStreamStats(String dataStream, int backingIndices, ByteSizeValue storeSize, long maximumTimestamp) {\n+            this.dataStream = dataStream;\n+            this.backingIndices = backingIndices;\n+            this.storeSize = storeSize;\n+            this.maximumTimestamp = maximumTimestamp;\n+        }\n+\n+        public DataStreamStats(StreamInput in) throws IOException {\n+            this.dataStream = in.readString();\n+            this.backingIndices = in.readVInt();\n+            this.storeSize = new ByteSizeValue(in);\n+            this.maximumTimestamp = in.readVLong();\n+        }\n+\n+        @Override\n+        public void writeTo(StreamOutput out) throws IOException {\n+            out.writeString(dataStream);\n+            out.writeVInt(backingIndices);\n+            storeSize.writeTo(out);\n+            out.writeVLong(maximumTimestamp);\n+        }\n+\n+        @Override\n+        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+            builder.startObject();\n+            builder.field(\"data_stream\", dataStream);\n+            builder.field(\"backing_indices\", backingIndices);\n+            builder.humanReadableField(\"store_size_bytes\", \"store_size\", storeSize);\n+            builder.field(\"maximum_timestamp\", maximumTimestamp);\n+            builder.endObject();\n+            return builder;\n+        }\n+\n+        public String getDataStream() {\n+            return dataStream;\n+        }\n+\n+        public int getBackingIndices() {\n+            return backingIndices;\n+        }\n+\n+        public ByteSizeValue getStoreSize() {\n+            return storeSize;\n+        }\n+\n+        public long getMaximumTimestamp() {\n+            return maximumTimestamp;\n+        }\n+    }\n+\n+    public static class DataStreamShardStats implements Writeable {\n+        private final ShardRouting shardRouting;\n+        private final StoreStats storeStats;\n+        private final long maxTimestamp;\n+\n+        public DataStreamShardStats(ShardRouting shardRouting, StoreStats storeStats, long maxTimestamp) {\n+            this.shardRouting = shardRouting;\n+            this.storeStats = storeStats;\n+            this.maxTimestamp = maxTimestamp;\n+        }\n+\n+        public DataStreamShardStats(StreamInput in) throws IOException {\n+            this.shardRouting = new ShardRouting(in);\n+            this.storeStats = new StoreStats(in);\n+            this.maxTimestamp = in.readVLong();\n+        }\n+\n+        @Override\n+        public void writeTo(StreamOutput out) throws IOException {\n+            shardRouting.writeTo(out);\n+            storeStats.writeTo(out);\n+            out.writeVLong(maxTimestamp);\n+        }\n+\n+        public ShardRouting getShardRouting() {\n+            return shardRouting;\n+        }\n+\n+        public StoreStats getStoreStats() {\n+            return storeStats;\n+        }\n+\n+        public long getMaxTimestamp() {\n+            return maxTimestamp;\n+        }\n+    }\n+\n+    private static class AggregatedStats {\n+        Set<String> backingIndices = new HashSet<>();\n+        long storageBytes = 0L;\n+        long maxTimestamp = 0L;\n+    }\n+\n+    public static class TransportAction extends TransportBroadcastByNodeAction<Request, Response, DataStreamShardStats> {\n+\n+        private final ClusterService clusterService;\n+        private final IndicesService indicesService;\n+        private final IndexAbstractionResolver indexAbstractionResolver;\n+\n+        @Inject\n+        public TransportAction(ClusterService clusterService, TransportService transportService, IndicesService indicesService,\n+                                               ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver) {\n+            super(DataStreamsStatsAction.NAME, clusterService, transportService, actionFilters, indexNameExpressionResolver,\n+                Request::new, ThreadPool.Names.MANAGEMENT);\n+            this.clusterService = clusterService;\n+            this.indicesService = indicesService;\n+            this.indexAbstractionResolver = new IndexAbstractionResolver(indexNameExpressionResolver);\n+        }\n+\n+        @Override\n+        protected Request readRequestFrom(StreamInput in) throws IOException {\n+            return new Request(in);\n+        }\n+\n+        @Override\n+        protected ClusterBlockException checkGlobalBlock(ClusterState state, Request request) {\n+            return state.blocks().globalBlockedException(ClusterBlockLevel.METADATA_READ);\n+        }\n+\n+        @Override\n+        protected ClusterBlockException checkRequestBlock(ClusterState state, Request request, String[] concreteIndices) {\n+            return state.blocks().indicesBlockedException(ClusterBlockLevel.METADATA_READ, concreteIndices);\n+        }\n+\n+        @Override\n+        protected ShardsIterator shards(ClusterState clusterState, Request request, String[] concreteIndices) {\n+            String[] requestIndices = request.indices();\n+            if (requestIndices == null || requestIndices.length == 0) {\n+                requestIndices = new String[]{\"*\"};\n+            }\n+            List<String> abstractionNames = indexAbstractionResolver.resolveIndexAbstractions(requestIndices, request.indicesOptions(),\n+                clusterState.getMetadata());\n+            SortedMap<String, IndexAbstraction> indicesLookup = clusterState.getMetadata().getIndicesLookup();\n+\n+            String[] concreteDatastreamIndices = abstractionNames.stream().flatMap(abstractionName -> {\n+                IndexAbstraction indexAbstraction = indicesLookup.get(abstractionName);\n+                assert indexAbstraction != null;\n+                if (indexAbstraction.getType() == IndexAbstraction.Type.DATA_STREAM) {\n+                    IndexAbstraction.DataStream dataStream = (IndexAbstraction.DataStream) indexAbstraction;\n+                    List<IndexMetadata> indices = dataStream.getIndices();\n+                    return indices.stream().map(idx -> idx.getIndex().getName());\n+                } else {\n+                    return Stream.empty();\n+                }\n+            }).toArray(String[]::new);\n+            return clusterState.getRoutingTable().allShards(concreteDatastreamIndices);\n+        }\n+\n+        @Override\n+        protected DataStreamShardStats shardOperation(Request request, ShardRouting shardRouting) throws IOException {\n+            IndexService indexService = indicesService.indexServiceSafe(shardRouting.shardId().getIndex());\n+            IndexShard indexShard = indexService.getShard(shardRouting.shardId().id());\n+            // if we don't have the routing entry yet, we need it stats wise, we treat it as if the shard is not ready yet\n+            if (indexShard.routingEntry() == null) {\n+                throw new ShardNotFoundException(indexShard.shardId());\n+            }\n+            StoreStats storeStats = indexShard.storeStats();\n+            IndexAbstraction indexAbstraction = clusterService.state().getMetadata().getIndicesLookup().get(shardRouting.getIndexName());\n+            assert indexAbstraction != null;\n+            IndexAbstraction.DataStream dataStream = indexAbstraction.getParentDataStream();\n+            assert dataStream != null;\n+            long maxTimestamp = 0L;\n+            try (Engine.Searcher searcher = indexShard.acquireSearcher(\"data_stream_stats\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "74cef55e49c4cbbe0da787fae04d2e34b4cf2733"}, "originalPosition": 310}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDA5MTA4OA==", "bodyText": "maybe rename dataStreamsStats variable to aggregatedDataStreamsStats?", "url": "https://github.com/elastic/elasticsearch/pull/58707#discussion_r450091088", "createdAt": "2020-07-06T09:15:47Z", "author": {"login": "martijnvg"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/datastream/DataStreamsStatsAction.java", "diffHunk": "@@ -0,0 +1,377 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.datastream;\n+\n+import org.apache.lucene.document.LongPoint;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.PointValues;\n+import org.elasticsearch.action.ActionType;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.broadcast.BroadcastRequest;\n+import org.elasticsearch.action.support.broadcast.BroadcastResponse;\n+import org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexAbstraction;\n+import org.elasticsearch.cluster.metadata.IndexAbstractionResolver;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.routing.ShardsIterator;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.io.stream.Writeable;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.common.xcontent.ToXContentObject;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.index.IndexService;\n+import org.elasticsearch.index.engine.Engine;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardNotFoundException;\n+import org.elasticsearch.index.store.StoreStats;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.SortedMap;\n+import java.util.stream.Stream;\n+\n+public class DataStreamsStatsAction extends ActionType<DataStreamsStatsAction.Response> {\n+\n+    public static final DataStreamsStatsAction INSTANCE = new DataStreamsStatsAction();\n+    public static final String NAME = \"indices:monitor/data_stream/stats\";\n+\n+    public DataStreamsStatsAction() {\n+        super(NAME, DataStreamsStatsAction.Response::new);\n+    }\n+\n+    public static class Request extends BroadcastRequest<Request> {\n+        public Request() {\n+            super((String[]) null);\n+        }\n+\n+        public Request(StreamInput in) throws IOException {\n+            super(in);\n+        }\n+    }\n+\n+    public static class Response extends BroadcastResponse {\n+        private final int dataStreamCount;\n+        private final int backingIndices;\n+        private final ByteSizeValue totalStoreSize;\n+        private final DataStreamStats[] dataStreams;\n+\n+        public Response(int totalShards, int successfulShards, int failedShards, List<DefaultShardOperationFailedException> shardFailures,\n+                        int dataStreamCount, int backingIndices, ByteSizeValue totalStoreSize, DataStreamStats[] dataStreams) {\n+            super(totalShards, successfulShards, failedShards, shardFailures);\n+            this.dataStreamCount = dataStreamCount;\n+            this.backingIndices = backingIndices;\n+            this.totalStoreSize = totalStoreSize;\n+            this.dataStreams = dataStreams;\n+        }\n+\n+        public Response(StreamInput in) throws IOException {\n+            super(in);\n+            this.dataStreamCount = in.readVInt();\n+            this.backingIndices = in.readVInt();\n+            this.totalStoreSize = new ByteSizeValue(in);\n+            this.dataStreams = in.readArray(DataStreamStats::new, DataStreamStats[]::new);\n+        }\n+\n+        @Override\n+        public void writeTo(StreamOutput out) throws IOException {\n+            super.writeTo(out);\n+            out.writeVInt(dataStreamCount);\n+            out.writeVInt(backingIndices);\n+            totalStoreSize.writeTo(out);\n+            out.writeArray(dataStreams);\n+        }\n+\n+        @Override\n+        protected void addCustomXContentFields(XContentBuilder builder, Params params) throws IOException {\n+            builder.field(\"data_stream_count\", dataStreamCount);\n+            builder.field(\"backing_indices\", backingIndices);\n+            builder.humanReadableField(\"total_store_size_bytes\", \"total_store_size\", totalStoreSize);\n+            builder.array(\"data_streams\", (Object[]) dataStreams);\n+        }\n+\n+        public int getDataStreamCount() {\n+            return dataStreamCount;\n+        }\n+\n+        public int getBackingIndices() {\n+            return backingIndices;\n+        }\n+\n+        public ByteSizeValue getTotalStoreSize() {\n+            return totalStoreSize;\n+        }\n+\n+        public DataStreamStats[] getDataStreams() {\n+            return dataStreams;\n+        }\n+    }\n+\n+    public static class DataStreamStats implements ToXContentObject, Writeable {\n+        private final String dataStream;\n+        private final int backingIndices;\n+        private final ByteSizeValue storeSize;\n+        private final long maximumTimestamp;\n+\n+        public DataStreamStats(String dataStream, int backingIndices, ByteSizeValue storeSize, long maximumTimestamp) {\n+            this.dataStream = dataStream;\n+            this.backingIndices = backingIndices;\n+            this.storeSize = storeSize;\n+            this.maximumTimestamp = maximumTimestamp;\n+        }\n+\n+        public DataStreamStats(StreamInput in) throws IOException {\n+            this.dataStream = in.readString();\n+            this.backingIndices = in.readVInt();\n+            this.storeSize = new ByteSizeValue(in);\n+            this.maximumTimestamp = in.readVLong();\n+        }\n+\n+        @Override\n+        public void writeTo(StreamOutput out) throws IOException {\n+            out.writeString(dataStream);\n+            out.writeVInt(backingIndices);\n+            storeSize.writeTo(out);\n+            out.writeVLong(maximumTimestamp);\n+        }\n+\n+        @Override\n+        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+            builder.startObject();\n+            builder.field(\"data_stream\", dataStream);\n+            builder.field(\"backing_indices\", backingIndices);\n+            builder.humanReadableField(\"store_size_bytes\", \"store_size\", storeSize);\n+            builder.field(\"maximum_timestamp\", maximumTimestamp);\n+            builder.endObject();\n+            return builder;\n+        }\n+\n+        public String getDataStream() {\n+            return dataStream;\n+        }\n+\n+        public int getBackingIndices() {\n+            return backingIndices;\n+        }\n+\n+        public ByteSizeValue getStoreSize() {\n+            return storeSize;\n+        }\n+\n+        public long getMaximumTimestamp() {\n+            return maximumTimestamp;\n+        }\n+    }\n+\n+    public static class DataStreamShardStats implements Writeable {\n+        private final ShardRouting shardRouting;\n+        private final StoreStats storeStats;\n+        private final long maxTimestamp;\n+\n+        public DataStreamShardStats(ShardRouting shardRouting, StoreStats storeStats, long maxTimestamp) {\n+            this.shardRouting = shardRouting;\n+            this.storeStats = storeStats;\n+            this.maxTimestamp = maxTimestamp;\n+        }\n+\n+        public DataStreamShardStats(StreamInput in) throws IOException {\n+            this.shardRouting = new ShardRouting(in);\n+            this.storeStats = new StoreStats(in);\n+            this.maxTimestamp = in.readVLong();\n+        }\n+\n+        @Override\n+        public void writeTo(StreamOutput out) throws IOException {\n+            shardRouting.writeTo(out);\n+            storeStats.writeTo(out);\n+            out.writeVLong(maxTimestamp);\n+        }\n+\n+        public ShardRouting getShardRouting() {\n+            return shardRouting;\n+        }\n+\n+        public StoreStats getStoreStats() {\n+            return storeStats;\n+        }\n+\n+        public long getMaxTimestamp() {\n+            return maxTimestamp;\n+        }\n+    }\n+\n+    private static class AggregatedStats {\n+        Set<String> backingIndices = new HashSet<>();\n+        long storageBytes = 0L;\n+        long maxTimestamp = 0L;\n+    }\n+\n+    public static class TransportAction extends TransportBroadcastByNodeAction<Request, Response, DataStreamShardStats> {\n+\n+        private final ClusterService clusterService;\n+        private final IndicesService indicesService;\n+        private final IndexAbstractionResolver indexAbstractionResolver;\n+\n+        @Inject\n+        public TransportAction(ClusterService clusterService, TransportService transportService, IndicesService indicesService,\n+                                               ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver) {\n+            super(DataStreamsStatsAction.NAME, clusterService, transportService, actionFilters, indexNameExpressionResolver,\n+                Request::new, ThreadPool.Names.MANAGEMENT);\n+            this.clusterService = clusterService;\n+            this.indicesService = indicesService;\n+            this.indexAbstractionResolver = new IndexAbstractionResolver(indexNameExpressionResolver);\n+        }\n+\n+        @Override\n+        protected Request readRequestFrom(StreamInput in) throws IOException {\n+            return new Request(in);\n+        }\n+\n+        @Override\n+        protected ClusterBlockException checkGlobalBlock(ClusterState state, Request request) {\n+            return state.blocks().globalBlockedException(ClusterBlockLevel.METADATA_READ);\n+        }\n+\n+        @Override\n+        protected ClusterBlockException checkRequestBlock(ClusterState state, Request request, String[] concreteIndices) {\n+            return state.blocks().indicesBlockedException(ClusterBlockLevel.METADATA_READ, concreteIndices);\n+        }\n+\n+        @Override\n+        protected ShardsIterator shards(ClusterState clusterState, Request request, String[] concreteIndices) {\n+            String[] requestIndices = request.indices();\n+            if (requestIndices == null || requestIndices.length == 0) {\n+                requestIndices = new String[]{\"*\"};\n+            }\n+            List<String> abstractionNames = indexAbstractionResolver.resolveIndexAbstractions(requestIndices, request.indicesOptions(),\n+                clusterState.getMetadata());\n+            SortedMap<String, IndexAbstraction> indicesLookup = clusterState.getMetadata().getIndicesLookup();\n+\n+            String[] concreteDatastreamIndices = abstractionNames.stream().flatMap(abstractionName -> {\n+                IndexAbstraction indexAbstraction = indicesLookup.get(abstractionName);\n+                assert indexAbstraction != null;\n+                if (indexAbstraction.getType() == IndexAbstraction.Type.DATA_STREAM) {\n+                    IndexAbstraction.DataStream dataStream = (IndexAbstraction.DataStream) indexAbstraction;\n+                    List<IndexMetadata> indices = dataStream.getIndices();\n+                    return indices.stream().map(idx -> idx.getIndex().getName());\n+                } else {\n+                    return Stream.empty();\n+                }\n+            }).toArray(String[]::new);\n+            return clusterState.getRoutingTable().allShards(concreteDatastreamIndices);\n+        }\n+\n+        @Override\n+        protected DataStreamShardStats shardOperation(Request request, ShardRouting shardRouting) throws IOException {\n+            IndexService indexService = indicesService.indexServiceSafe(shardRouting.shardId().getIndex());\n+            IndexShard indexShard = indexService.getShard(shardRouting.shardId().id());\n+            // if we don't have the routing entry yet, we need it stats wise, we treat it as if the shard is not ready yet\n+            if (indexShard.routingEntry() == null) {\n+                throw new ShardNotFoundException(indexShard.shardId());\n+            }\n+            StoreStats storeStats = indexShard.storeStats();\n+            IndexAbstraction indexAbstraction = clusterService.state().getMetadata().getIndicesLookup().get(shardRouting.getIndexName());\n+            assert indexAbstraction != null;\n+            IndexAbstraction.DataStream dataStream = indexAbstraction.getParentDataStream();\n+            assert dataStream != null;\n+            long maxTimestamp = 0L;\n+            try (Engine.Searcher searcher = indexShard.acquireSearcher(\"data_stream_stats\")) {\n+                IndexReader indexReader = searcher.getIndexReader();\n+                String fieldName = dataStream.getDataStream().getTimeStampField().getName();\n+                byte[] maxPackedValue = PointValues.getMaxPackedValue(indexReader, fieldName);\n+                if (maxPackedValue != null) {\n+                    maxTimestamp = LongPoint.decodeDimension(maxPackedValue, 0);\n+                }\n+            }\n+            return new DataStreamShardStats(\n+                indexShard.routingEntry(),\n+                storeStats,\n+                maxTimestamp\n+            );\n+        }\n+\n+        @Override\n+        protected DataStreamShardStats readShardResult(StreamInput in) throws IOException {\n+            return new DataStreamShardStats(in);\n+        }\n+\n+        @Override\n+        protected Response newResponse(Request request, int totalShards, int successfulShards,\n+                                                       int failedShards, List<DataStreamShardStats> dataStreamShardStats,\n+                                                       List<DefaultShardOperationFailedException> shardFailures,\n+                                                       ClusterState clusterState) {\n+            Map<String, AggregatedStats> dataStreamsStats = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "74cef55e49c4cbbe0da787fae04d2e34b4cf2733"}, "originalPosition": 335}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f3e8eef1300c9ca8dedc0d2eae5aaf596d385e3c", "author": {"user": {"login": "jbaiera", "name": "James Baiera"}}, "url": "https://github.com/elastic/elasticsearch/commit/f3e8eef1300c9ca8dedc0d2eae5aaf596d385e3c", "committedDate": "2020-07-06T15:42:28Z", "message": "Rest client method and api spec name agreement"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b264441a1bd1936c7e737b97254604cc8ddde0ac", "author": {"user": {"login": "jbaiera", "name": "James Baiera"}}, "url": "https://github.com/elastic/elasticsearch/commit/b264441a1bd1936c7e737b97254604cc8ddde0ac", "committedDate": "2020-07-06T19:11:18Z", "message": "Fixing XContent tests and logic"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a72e35ba8029f11384fe732a3cf12c09dcaa4267", "author": {"user": {"login": "jbaiera", "name": "James Baiera"}}, "url": "https://github.com/elastic/elasticsearch/commit/a72e35ba8029f11384fe732a3cf12c09dcaa4267", "committedDate": "2020-07-06T20:49:09Z", "message": "Cleanup"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ0NzEyNTIz", "url": "https://github.com/elastic/elasticsearch/pull/58707#pullrequestreview-444712523", "createdAt": "2020-07-08T12:26:30Z", "commit": {"oid": "a72e35ba8029f11384fe732a3cf12c09dcaa4267"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOFQxMjoyNjozMVrOGulrYw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOFQxMjoyOTo1NVrOGulyxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTUwNDk5NQ==", "bodyText": "maybe add a serialization unit test for this response class that extends from AbstractWireSerializingTestCase?", "url": "https://github.com/elastic/elasticsearch/pull/58707#discussion_r451504995", "createdAt": "2020-07-08T12:26:31Z", "author": {"login": "martijnvg"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/datastream/DataStreamsStatsAction.java", "diffHunk": "@@ -0,0 +1,377 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.datastream;\n+\n+import org.apache.lucene.document.LongPoint;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.PointValues;\n+import org.elasticsearch.action.ActionType;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.broadcast.BroadcastRequest;\n+import org.elasticsearch.action.support.broadcast.BroadcastResponse;\n+import org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexAbstraction;\n+import org.elasticsearch.cluster.metadata.IndexAbstractionResolver;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.routing.ShardsIterator;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.io.stream.Writeable;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.common.xcontent.ToXContentObject;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.index.IndexService;\n+import org.elasticsearch.index.engine.Engine;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardNotFoundException;\n+import org.elasticsearch.index.store.StoreStats;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.SortedMap;\n+import java.util.stream.Stream;\n+\n+public class DataStreamsStatsAction extends ActionType<DataStreamsStatsAction.Response> {\n+\n+    public static final DataStreamsStatsAction INSTANCE = new DataStreamsStatsAction();\n+    public static final String NAME = \"indices:monitor/data_stream/stats\";\n+\n+    public DataStreamsStatsAction() {\n+        super(NAME, DataStreamsStatsAction.Response::new);\n+    }\n+\n+    public static class Request extends BroadcastRequest<Request> {\n+        public Request() {\n+            super((String[]) null);\n+        }\n+\n+        public Request(StreamInput in) throws IOException {\n+            super(in);\n+        }\n+    }\n+\n+    public static class Response extends BroadcastResponse {\n+        private final int dataStreamCount;\n+        private final int backingIndices;\n+        private final ByteSizeValue totalStoreSize;\n+        private final DataStreamStats[] dataStreams;\n+\n+        public Response(int totalShards, int successfulShards, int failedShards, List<DefaultShardOperationFailedException> shardFailures,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a72e35ba8029f11384fe732a3cf12c09dcaa4267"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTUwNjg4NA==", "bodyText": "equalTo(0) should be equalTo(0L), a test failed because of this.", "url": "https://github.com/elastic/elasticsearch/pull/58707#discussion_r451506884", "createdAt": "2020-07-08T12:29:55Z", "author": {"login": "martijnvg"}, "path": "client/rest-high-level/src/test/java/org/elasticsearch/client/IndicesClientIT.java", "diffHunk": "@@ -1608,6 +1611,26 @@ public void testDataStreams() throws Exception {\n         assertThat(dataStream.getTimeStampField(), equalTo(\"@timestamp\"));\n         assertThat(dataStream.getIndices(), hasSize(1));\n \n+        DataStreamsStatsRequest dataStreamsStatsRequest = new DataStreamsStatsRequest();\n+        DataStreamsStatsResponse dataStreamsStatsResponse = execute(dataStreamsStatsRequest, indices::dataStreamsStats,\n+            indices::dataStreamsStatsAsync);\n+        int dataStreamsCount = dataStreamsStatsResponse.getDataStreamCount();\n+        assertThat(dataStreamsCount, equalTo(1));\n+        int backingIndices = dataStreamsStatsResponse.getBackingIndices();\n+        assertThat(backingIndices, equalTo(1));\n+        ByteSizeValue byteSizeValue = dataStreamsStatsResponse.getTotalStoreSize();\n+        assertThat(byteSizeValue, notNullValue());\n+        assertThat(byteSizeValue.getBytes(), not(equalTo(0L)));\n+        Map<String, DataStreamStats> dataStreamsStats = dataStreamsStatsResponse.getDataStreams();\n+        assertThat(dataStreamsStats, notNullValue());\n+        assertThat(dataStreamsStats.size(), equalTo(1));\n+        DataStreamStats dataStreamStat = dataStreamsStats.get(dataStreamName);\n+        assertThat(dataStreamStat, notNullValue());\n+        assertThat(dataStreamStat.getDataStream(), equalTo(dataStreamName));\n+        assertThat(dataStreamStat.getBackingIndices(), equalTo(1));\n+        assertThat(dataStreamStat.getMaximumTimestamp(), equalTo(0)); // No data in here", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a72e35ba8029f11384fe732a3cf12c09dcaa4267"}, "originalPosition": 31}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ0NzI2MTk5", "url": "https://github.com/elastic/elasticsearch/pull/58707#pullrequestreview-444726199", "createdAt": "2020-07-08T12:44:25Z", "commit": {"oid": "a72e35ba8029f11384fe732a3cf12c09dcaa4267"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOFQxMjo0NDoyNVrOGumUFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOFQxMzoyMDowNlrOGunsgQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTUxNTQxMg==", "bodyText": "I think this request should implement IndicesRequest.Replaceable so that it works as an index-level action with security. You'll probably want to add a corresponding test to https://github.com/elastic/elasticsearch/blob/master/x-pack/plugin/src/test/resources/rest-api-spec/test/security/authz/50_data_streams.yml to validate that stats are retrieved only for the data streams that are authorized for the user.", "url": "https://github.com/elastic/elasticsearch/pull/58707#discussion_r451515412", "createdAt": "2020-07-08T12:44:25Z", "author": {"login": "danhermann"}, "path": "client/rest-high-level/src/main/java/org/elasticsearch/client/indices/DataStreamsStatsRequest.java", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.client.indices;\n+\n+import org.elasticsearch.action.support.IndicesOptions;\n+import org.elasticsearch.client.Validatable;\n+\n+public class DataStreamsStatsRequest implements Validatable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a72e35ba8029f11384fe732a3cf12c09dcaa4267"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTUzODA0OQ==", "bodyText": "Note that this clause was not included in the code for the resolve index action because it would never apply to data streams. If you want to consolidate all that code (which would be great!), you'll need to incorporate that clause back into the common implementation.", "url": "https://github.com/elastic/elasticsearch/pull/58707#discussion_r451538049", "createdAt": "2020-07-08T13:20:06Z", "author": {"login": "danhermann"}, "path": "x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authz/IndicesAndAliasesResolver.java", "diffHunk": "@@ -335,135 +337,6 @@ private boolean containsWildcards(IndicesRequest indicesRequest) {\n         return false;\n     }\n \n-    //TODO Investigate reusing code from vanilla es to resolve index names and wildcards\n-    private List<String> replaceWildcardsWithAuthorizedIndices(Iterable<String> indices, IndicesOptions indicesOptions, Metadata metadata,\n-                                                               List<String> authorizedIndices, boolean replaceWildcards) {\n-        //the order matters when it comes to exclusions\n-        List<String> finalIndices = new ArrayList<>();\n-        boolean wildcardSeen = false;\n-        for (String index : indices) {\n-            String aliasOrIndex;\n-            boolean minus = false;\n-            if (index.charAt(0) == '-' && wildcardSeen) {\n-                aliasOrIndex = index.substring(1);\n-                minus = true;\n-            } else {\n-                aliasOrIndex = index;\n-            }\n-\n-            // we always need to check for date math expressions\n-            final String dateMathName = nameExpressionResolver.resolveDateMathExpression(aliasOrIndex);\n-            if (dateMathName != aliasOrIndex) {\n-                assert dateMathName.equals(aliasOrIndex) == false;\n-                if (replaceWildcards && Regex.isSimpleMatchPattern(dateMathName)) {\n-                    // continue\n-                    aliasOrIndex = dateMathName;\n-                } else if (authorizedIndices.contains(dateMathName) &&\n-                    isIndexVisible(aliasOrIndex, dateMathName, indicesOptions, metadata, true)) {\n-                    if (minus) {\n-                        finalIndices.remove(dateMathName);\n-                    } else {\n-                        finalIndices.add(dateMathName);\n-                    }\n-                } else {\n-                    if (indicesOptions.ignoreUnavailable() == false) {\n-                        throw new IndexNotFoundException(dateMathName);\n-                    }\n-                }\n-            }\n-\n-            if (replaceWildcards && Regex.isSimpleMatchPattern(aliasOrIndex)) {\n-                wildcardSeen = true;\n-                Set<String> resolvedIndices = new HashSet<>();\n-                for (String authorizedIndex : authorizedIndices) {\n-                    if (Regex.simpleMatch(aliasOrIndex, authorizedIndex) &&\n-                        isIndexVisible(aliasOrIndex, authorizedIndex, indicesOptions, metadata)) {\n-                        resolvedIndices.add(authorizedIndex);\n-                    }\n-                }\n-                if (resolvedIndices.isEmpty()) {\n-                    //es core honours allow_no_indices for each wildcard expression, we do the same here by throwing index not found.\n-                    if (indicesOptions.allowNoIndices() == false) {\n-                        throw new IndexNotFoundException(aliasOrIndex);\n-                    }\n-                } else {\n-                    if (minus) {\n-                        finalIndices.removeAll(resolvedIndices);\n-                    } else {\n-                        finalIndices.addAll(resolvedIndices);\n-                    }\n-                }\n-            } else if (dateMathName == aliasOrIndex) {\n-                // we can use == here to compare strings since the name expression resolver returns the same instance, but add an assert\n-                // to ensure we catch this if it changes\n-\n-                assert dateMathName.equals(aliasOrIndex);\n-                //Metadata#convertFromWildcards checks if the index exists here and throws IndexNotFoundException if not (based on\n-                // ignore_unavailable). We only add/remove the index: if the index is missing or the current user is not authorized\n-                // to access it either an AuthorizationException will be thrown later in AuthorizationService, or the index will be\n-                // removed from the list, based on the ignore_unavailable option.\n-                if (minus) {\n-                    finalIndices.remove(aliasOrIndex);\n-                } else {\n-                    finalIndices.add(aliasOrIndex);\n-                }\n-            }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a72e35ba8029f11384fe732a3cf12c09dcaa4267"}, "originalPosition": 117}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2c0a5b47b7de2b2b8580caf7e37e244d78affd8d", "author": {"user": {"login": "jbaiera", "name": "James Baiera"}}, "url": "https://github.com/elastic/elasticsearch/commit/2c0a5b47b7de2b2b8580caf7e37e244d78affd8d", "committedDate": "2020-07-08T15:45:37Z", "message": "PR Feedback"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a4ca1689887db6829feec388e03f3e0b117c8b32", "author": {"user": {"login": "jbaiera", "name": "James Baiera"}}, "url": "https://github.com/elastic/elasticsearch/commit/a4ca1689887db6829feec388e03f3e0b117c8b32", "committedDate": "2020-07-08T20:17:14Z", "message": "Merge branch 'master' into data-stream-stats"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ad6a18877d9edf12291549b5eb1f49cd690b257f", "author": {"user": {"login": "jbaiera", "name": "James Baiera"}}, "url": "https://github.com/elastic/elasticsearch/commit/ad6a18877d9edf12291549b5eb1f49cd690b257f", "committedDate": "2020-07-09T13:58:59Z", "message": "Add rest test for data streams stats on security"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "80244460b32287c2e2a34f18a02411f8a6c879c5", "author": {"user": {"login": "jbaiera", "name": "James Baiera"}}, "url": "https://github.com/elastic/elasticsearch/commit/80244460b32287c2e2a34f18a02411f8a6c879c5", "committedDate": "2020-07-09T16:43:39Z", "message": "Fixing tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0c7b7bc83bb87f12c7a818cd27a3bd25dc26ceb4", "author": {"user": {"login": "jbaiera", "name": "James Baiera"}}, "url": "https://github.com/elastic/elasticsearch/commit/0c7b7bc83bb87f12c7a818cd27a3bd25dc26ceb4", "committedDate": "2020-07-09T17:41:39Z", "message": "Fix more tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a16e029724c8850280564a2db8acfd1554347765", "author": {"user": {"login": "jbaiera", "name": "James Baiera"}}, "url": "https://github.com/elastic/elasticsearch/commit/a16e029724c8850280564a2db8acfd1554347765", "committedDate": "2020-07-09T18:23:05Z", "message": "Fixing security test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9756c81db0b48679fbbce08adb8120f33732de03", "author": {"user": {"login": "jbaiera", "name": "James Baiera"}}, "url": "https://github.com/elastic/elasticsearch/commit/9756c81db0b48679fbbce08adb8120f33732de03", "committedDate": "2020-07-09T19:11:21Z", "message": "Precommit"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3f032132455952a76802c5177567f9ddc92b50b2", "author": {"user": {"login": "elasticmachine", "name": "Elastic Machine"}}, "url": "https://github.com/elastic/elasticsearch/commit/3f032132455952a76802c5177567f9ddc92b50b2", "committedDate": "2020-07-09T20:47:28Z", "message": "Merge branch 'master' into data-stream-stats"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ2MjYyMTEw", "url": "https://github.com/elastic/elasticsearch/pull/58707#pullrequestreview-446262110", "createdAt": "2020-07-10T09:34:12Z", "commit": {"oid": "3f032132455952a76802c5177567f9ddc92b50b2"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "28de11c9d2472484833bfd190e609569e3a9ffeb", "author": {"user": {"login": "jbaiera", "name": "James Baiera"}}, "url": "https://github.com/elastic/elasticsearch/commit/28de11c9d2472484833bfd190e609569e3a9ffeb", "committedDate": "2020-07-13T19:27:25Z", "message": "Merge branch 'master' into data-stream-stats"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "724e8b22bc083f3a70d28b057afe45a19eef62b4", "author": {"user": {"login": "jbaiera", "name": "James Baiera"}}, "url": "https://github.com/elastic/elasticsearch/commit/724e8b22bc083f3a70d28b057afe45a19eef62b4", "committedDate": "2020-07-13T19:28:37Z", "message": "Update security tests after merge"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6e8a38a45ebb399d570b42b2f6cc6cad891b8d28", "author": {"user": {"login": "jbaiera", "name": "James Baiera"}}, "url": "https://github.com/elastic/elasticsearch/commit/6e8a38a45ebb399d570b42b2f6cc6cad891b8d28", "committedDate": "2020-07-13T19:58:34Z", "message": "Merge branch 'master' into data-stream-stats"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "56ebf9b99b509ce8ab456ea82ced928e4b323178", "author": {"user": {"login": "jbaiera", "name": "James Baiera"}}, "url": "https://github.com/elastic/elasticsearch/commit/56ebf9b99b509ce8ab456ea82ced928e4b323178", "committedDate": "2020-07-13T21:27:11Z", "message": "Move test to xpack plugin so that it runs correctly"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f42f789c21195ed02a3e96b00277df0b55b39359", "author": {"user": {"login": "jbaiera", "name": "James Baiera"}}, "url": "https://github.com/elastic/elasticsearch/commit/f42f789c21195ed02a3e96b00277df0b55b39359", "committedDate": "2020-07-14T01:25:40Z", "message": "clean up correct data stream in tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5bad0ce9803b63971c8b67b8fb7655732447bbe4", "author": {"user": {"login": "elasticmachine", "name": "Elastic Machine"}}, "url": "https://github.com/elastic/elasticsearch/commit/5bad0ce9803b63971c8b67b8fb7655732447bbe4", "committedDate": "2020-07-14T05:46:13Z", "message": "Merge branch 'master' into data-stream-stats"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "39d02b403813ee6352c86013684bd7417a2b550b", "author": {"user": {"login": "jbaiera", "name": "James Baiera"}}, "url": "https://github.com/elastic/elasticsearch/commit/39d02b403813ee6352c86013684bd7417a2b550b", "committedDate": "2020-07-14T14:42:30Z", "message": "Method contract changed and one of the call sites became erroneous"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7e505fa7ebc123fc1e78cfc6aade62994f74793c", "author": {"user": {"login": "elasticmachine", "name": "Elastic Machine"}}, "url": "https://github.com/elastic/elasticsearch/commit/7e505fa7ebc123fc1e78cfc6aade62994f74793c", "committedDate": "2020-07-14T15:25:58Z", "message": "Merge branch 'master' into data-stream-stats"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dcbf5ec0f919c7beb8125e1b288e2433c769a702", "author": {"user": {"login": "jbaiera", "name": "James Baiera"}}, "url": "https://github.com/elastic/elasticsearch/commit/dcbf5ec0f919c7beb8125e1b288e2433c769a702", "committedDate": "2020-07-14T16:43:28Z", "message": "Fix test compilation"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d3b8be398e0c4bef80a61d1c7f634a38b9b8f5de", "author": {"user": {"login": "jbaiera", "name": "James Baiera"}}, "url": "https://github.com/elastic/elasticsearch/commit/d3b8be398e0c4bef80a61d1c7f634a38b9b8f5de", "committedDate": "2020-07-14T17:50:29Z", "message": "Fix rest tests again"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2573, "cost": 1, "resetAt": "2021-10-28T18:54:27Z"}}}