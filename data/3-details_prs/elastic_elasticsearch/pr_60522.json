{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDYwMjM5NDIw", "number": 60522, "title": "Introduce index based snapshot blob cache for Searchable Snapshots", "bodyText": "This pull request is a draft that introduces an index-based cache for files that are used by searchable snapshots directories.\nThis draft is based on a previous work that implemented the cache a the BlobContainer level whereas this pull request implements it at the IndexInput level.\nImplementing the cache at a higher level brings advantages like:\n\ncaching a region of the file that spans over multiple blob parts is possible\nknowing if the directory is recovering or not is straightforward (by passing the RecoveryState to the directory)\nbeing able to serve footer checksums directly from memory (for non-slice index inputs)\nbeing able to dectect if the read operation is executed from a slice/clone or regular index input and compute the right offset positions to hit the index based cache\n\nIn this pull request, only the first 4 Kbytes of the blobs are cached and the footer checksums are served directly from memory if we can. Every time some data are read from a CachedBlobContainerIndexInput, it first tries to decide if it should read the data from disk or from the blob cache index. The decision is taken on the basis of the shard's RecoveryState: if the recovery is not finished yet, it prefers to use the blob cache index. Then it detects if the read occurs within the first 4KB and if the file can be fully cached too. If one of these conditions is true then it tries to retrieve the first bytes from the index cache by computing an id for the cached entry based on the repository name, the file name, a path (containing the snapshot id, index id and shard id) and an offset (here, 0). In case of a hit, the read operation can be fully served from the cached blob. In case of a miss, it computes the regions of the file we would have liked to retrieve from the index cache. The regions are basically a list of the positions for the file we want to index. This list of regions is then passed to the CacheFile writing method which will take care to copy over the bytes of the region and indexes them if, by chance, they got to be read when writing a cache range on disk. Because we're interested in caching the beginning and the end of all files this is very likely to happen.\nThis pull request adds a new SearchableSnapshotsBlobStoreCacheIntegTests test that successively mounts a snapshot multiple times and then verifies the documents indexed in the blob cache index and the blobs read from the repository. The goal of this test is to confirm that mounting a snapshot multiple times can be executed without downloading more data from the repositories. It also tests the scenario of restarting a node (it should not download more data from the repositories but should rely on the blob cache index too). Testing the freezing and the can_match phase during search queries would also be interested.\nAs of today files of a searchable snapshot shards are accessed first during restore (when Lucene prunes unreferenced files), then when a new history UUID is boostrapped and finally when the Directory is opened. It would be great if the Lucene pruning and the bootstrapping could also benefit from the blob cache. In case it's not possible, we can bypass the Lucene.pruneUnreferencedFiles() and the store.bootstrapNewHistory() for searchable snapshot shard.\nAlso, this pull request does not handle compound files very well but I think it should be possible to adapt the behavior to also cache the header/footer of slices as well.", "createdAt": "2020-07-31T15:17:16Z", "url": "https://github.com/elastic/elasticsearch/pull/60522", "merged": true, "mergeCommit": {"oid": "a20ff514f13a5b0e897db1deeedffa246daf65b3"}, "closed": true, "closedAt": "2020-08-26T16:20:36Z", "author": {"login": "tlrx"}, "timelineItems": {"totalCount": 79, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc6VTv6AH2gAyNDYwMjM5NDIwOmQwMDE0MTk4YjY5MWQ4OThhODVkZGZjY2YxY2I3MjYyYTg0OWM1Njg=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdCt7NWgFqTQ3NTYzMDk4MQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "d0014198b691d898a85ddfccf1cb7262a849c568", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/d0014198b691d898a85ddfccf1cb7262a849c568", "committedDate": "2020-07-31T14:43:16Z", "message": "Introduce index based snapshot blob cache for Searchable Snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c4f251c97d6e23a1f358f1d5aac9f0624fd700b2", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/c4f251c97d6e23a1f358f1d5aac9f0624fd700b2", "committedDate": "2020-08-04T15:04:26Z", "message": "WIP adjust some logging and back out cache-size-times-two optimisation"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1f30c42b6f094b9f4bf4aae9c185cdfa38c680d7", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/1f30c42b6f094b9f4bf4aae9c185cdfa38c680d7", "committedDate": "2020-08-06T15:19:42Z", "message": "Avoid ActionListener#wrap"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "02a8ed12cbd5c2503081c8062cb3a47c1c351d3d", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/02a8ed12cbd5c2503081c8062cb3a47c1c351d3d", "committedDate": "2020-08-06T15:24:03Z", "message": "Add TODO"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "76b97ab793ce90a80a5af1a600eee4e11100f2da", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/76b97ab793ce90a80a5af1a600eee4e11100f2da", "committedDate": "2020-08-06T15:55:41Z", "message": "Move special openInputStream impl"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "df7c0b5c81eef57c7150e620431fdbb166c69f58", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/df7c0b5c81eef57c7150e620431fdbb166c69f58", "committedDate": "2020-08-06T16:01:35Z", "message": "Add TODOs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "922c381a1de1860f53a7bb03daa458ee2bca7146", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/922c381a1de1860f53a7bb03daa458ee2bca7146", "committedDate": "2020-08-06T16:09:04Z", "message": "Add TODOs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c24d78e6b6f92fdeb159a14ebde2be53f4bd0181", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/c24d78e6b6f92fdeb159a14ebde2be53f4bd0181", "committedDate": "2020-08-07T13:03:02Z", "message": "Rework implementation\n\nWith this commit we fill in any blob index cache misses from the cache\nfile, which may in turn be populated from the blob store.\n\nIt also renames/comments some methods and moves some methods down the\nclass hierarchy to their usage site."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7e5bbc48c1fe0f82c08fdace56c2a5852f4b1c65", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/7e5bbc48c1fe0f82c08fdace56c2a5852f4b1c65", "committedDate": "2020-08-07T16:16:53Z", "message": "Merge branch 'master' into poc-blob-cache"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1b459aed4a79035dc9366f4ff19118463f745eaf", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/1b459aed4a79035dc9366f4ff19118463f745eaf", "committedDate": "2020-08-07T20:14:30Z", "message": "One-shot reading"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYzOTk4NjYx", "url": "https://github.com/elastic/elasticsearch/pull/60522#pullrequestreview-463998661", "createdAt": "2020-08-10T07:12:40Z", "commit": {"oid": "1b459aed4a79035dc9366f4ff19118463f745eaf"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwNzoxMjo0MFrOG-DiMw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwNzoxNjo1OVrOG-DoWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzcyMjgwMw==", "bodyText": "Pushed down to the cached index input only, the other implementation doesn't use this.", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r467722803", "createdAt": "2020-08-10T07:12:40Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/BaseSearchableSnapshotIndexInput.java", "diffHunk": "@@ -101,34 +102,6 @@ public final void close() throws IOException {\n \n     public abstract void innerClose() throws IOException;\n \n-    protected InputStream openInputStream(final long position, final long length) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1b459aed4a79035dc9366f4ff19118463f745eaf"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzcyMjg1Nw==", "bodyText": "Pushed these methods down to the cached index input only, the other implementation doesn't use them.", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r467722857", "createdAt": "2020-08-10T07:12:49Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/BaseSearchableSnapshotIndexInput.java", "diffHunk": "@@ -149,29 +122,4 @@ protected final boolean assertCurrentThreadMayAccessBlobStore() {\n         return true;\n     }\n \n-    private long getPartNumberForPosition(long position) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1b459aed4a79035dc9366f4ff19118463f745eaf"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzcyMzk4Mg==", "bodyText": "In fact we (almost) never looped here, the inner read methods always satisfied the complete read. The only case where we did loop is if the read spanned two 32MB-aligned ranges, but in that case we may as well read both ranges in one go anyway so that's what we do now.", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r467723982", "createdAt": "2020-08-10T07:15:51Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -136,53 +146,221 @@ protected void readInternal(ByteBuffer b) throws IOException {\n         final long position = getFilePointer() + this.offset;\n         final int length = b.remaining();\n \n-        int totalBytesRead = 0;\n-        while (totalBytesRead < length) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1b459aed4a79035dc9366f4ff19118463f745eaf"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzcyNDI4MA==", "bodyText": "This (and subsequent methods) moved here from the base class.", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r467724280", "createdAt": "2020-08-10T07:16:43Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -370,6 +558,76 @@ private void writeCacheFile(final FileChannel fc, final long start, final long e\n         }\n     }\n \n+    /**\n+     * Opens an {@link InputStream} for the given range of bytes which reads the data directly from the blob store. If the requested range\n+     * spans multiple blobs then this stream will request them in turn.\n+     *\n+     * @param position The start of the range of bytes to read, relative to the start of the corresponding Lucene file.\n+     * @param length The number of bytes to read\n+     */\n+    private InputStream openInputStreamFromBlobStore(final long position, final long length) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1b459aed4a79035dc9366f4ff19118463f745eaf"}, "originalPosition": 380}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzcyNDM3OA==", "bodyText": "Inlined this into another method.", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r467724378", "createdAt": "2020-08-10T07:16:59Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -431,43 +689,11 @@ public String toString() {\n             + getFilePointer()\n             + \", rangeSize=\"\n             + getDefaultRangeSize()\n+            + \", directory=\"\n+            + directory\n             + '}';\n     }\n \n-    private int readDirectly(long start, long end, ByteBuffer b) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1b459aed4a79035dc9366f4ff19118463f745eaf"}, "originalPosition": 455}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY0MDMxMDI1", "url": "https://github.com/elastic/elasticsearch/pull/60522#pullrequestreview-464031025", "createdAt": "2020-08-10T08:13:38Z", "commit": {"oid": "1b459aed4a79035dc9366f4ff19118463f745eaf"}, "state": "COMMENTED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwODoxMzozOVrOG-FGow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwOToxMzo0OVrOG-G7_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc0ODUxNQ==", "bodyText": "Small preference for defining canBeFullyCached before isStartOfFile.\nAlso, I wonder whether the logic to cache up to 2 * DEFAULT_SIZE is truly needed (perhaps canBeFullyCached = fileInfo.length() <= BlobStoreCacheService.DEFAULT_SIZE; would work just as well in practice, and give us a simpler upper bound on the size of cached blobs?). Alternatively, we could extend this in the future, do it file-type or content-based instead?", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r467748515", "createdAt": "2020-08-10T08:13:39Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -136,53 +146,221 @@ protected void readInternal(ByteBuffer b) throws IOException {\n         final long position = getFilePointer() + this.offset;\n         final int length = b.remaining();\n \n-        int totalBytesRead = 0;\n-        while (totalBytesRead < length) {\n-            final long pos = position + totalBytesRead;\n-            final int len = length - totalBytesRead;\n-            int bytesRead = 0;\n-            try {\n-                final CacheFile cacheFile = getCacheFileSafe();\n-                try (Releasable ignored = cacheFile.fileLock()) {\n-                    final Tuple<Long, Long> rangeToWrite = computeRange(pos);\n-                    final Tuple<Long, Long> rangeToRead = Tuple.tuple(pos, Math.min(pos + len, rangeToWrite.v2()));\n-\n-                    bytesRead = cacheFile.fetchAsync(rangeToWrite, rangeToRead, (channel) -> {\n-                        final int read;\n-                        if ((rangeToRead.v2() - rangeToRead.v1()) < b.remaining()) {\n-                            final ByteBuffer duplicate = b.duplicate();\n-                            duplicate.limit(duplicate.position() + Math.toIntExact(rangeToRead.v2() - rangeToRead.v1()));\n-                            read = readCacheFile(channel, pos, duplicate);\n-                            assert duplicate.position() <= b.limit();\n-                            b.position(duplicate.position());\n-                        } else {\n-                            read = readCacheFile(channel, pos, b);\n-                        }\n-                        return read;\n-                    }, this::writeCacheFile, directory.cacheFetchAsyncExecutor()).get();\n+        logger.trace(\"readInternal: read [{}-{}] from [{}]\", position, position + length, this);\n+\n+        // We can detect that we're going to read the last 16 bytes (that contains the footer checksum) of the file. Such reads are often\n+        // executed when opening a Directory and since we have the checksum in the snapshot metadata we can use it to fill the ByteBuffer.\n+        if (length == CodecUtil.footerLength() && isClone == false && position == fileInfo.length() - length) {\n+            if (readChecksumFromFileInfo(b)) {\n+                logger.trace(\"read footer of file [{}] at position [{}], bypassing all caches\", fileInfo.physicalName(), position);\n+                return;\n+            }\n+            assert b.remaining() == length;\n+        }\n+\n+        final List<Tuple<Long, Long>> indexCacheMisses;\n+\n+        // We prefer to use the index cache if the recovery is not done yet\n+        if (directory.isRecoveryDone() == false) {\n+            // We try to use the snapshot blob cache if:\n+            // - we're reading the first N bytes of the file\n+            final boolean isStartOfFile = (position + length <= BlobStoreCacheService.DEFAULT_SIZE);\n+            // - the file is small enough to be fully cached in the blob cache\n+            final boolean canBeFullyCached = fileInfo.length() <= BlobStoreCacheService.DEFAULT_SIZE * 2;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1b459aed4a79035dc9366f4ff19118463f745eaf"}, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc1MTA2Ng==", "bodyText": "Is this comment addressed now?", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r467751066", "createdAt": "2020-08-10T08:18:25Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -136,53 +146,221 @@ protected void readInternal(ByteBuffer b) throws IOException {\n         final long position = getFilePointer() + this.offset;\n         final int length = b.remaining();\n \n-        int totalBytesRead = 0;\n-        while (totalBytesRead < length) {\n-            final long pos = position + totalBytesRead;\n-            final int len = length - totalBytesRead;\n-            int bytesRead = 0;\n-            try {\n-                final CacheFile cacheFile = getCacheFileSafe();\n-                try (Releasable ignored = cacheFile.fileLock()) {\n-                    final Tuple<Long, Long> rangeToWrite = computeRange(pos);\n-                    final Tuple<Long, Long> rangeToRead = Tuple.tuple(pos, Math.min(pos + len, rangeToWrite.v2()));\n-\n-                    bytesRead = cacheFile.fetchAsync(rangeToWrite, rangeToRead, (channel) -> {\n-                        final int read;\n-                        if ((rangeToRead.v2() - rangeToRead.v1()) < b.remaining()) {\n-                            final ByteBuffer duplicate = b.duplicate();\n-                            duplicate.limit(duplicate.position() + Math.toIntExact(rangeToRead.v2() - rangeToRead.v1()));\n-                            read = readCacheFile(channel, pos, duplicate);\n-                            assert duplicate.position() <= b.limit();\n-                            b.position(duplicate.position());\n-                        } else {\n-                            read = readCacheFile(channel, pos, b);\n-                        }\n-                        return read;\n-                    }, this::writeCacheFile, directory.cacheFetchAsyncExecutor()).get();\n+        logger.trace(\"readInternal: read [{}-{}] from [{}]\", position, position + length, this);\n+\n+        // We can detect that we're going to read the last 16 bytes (that contains the footer checksum) of the file. Such reads are often\n+        // executed when opening a Directory and since we have the checksum in the snapshot metadata we can use it to fill the ByteBuffer.\n+        if (length == CodecUtil.footerLength() && isClone == false && position == fileInfo.length() - length) {\n+            if (readChecksumFromFileInfo(b)) {\n+                logger.trace(\"read footer of file [{}] at position [{}], bypassing all caches\", fileInfo.physicalName(), position);\n+                return;\n+            }\n+            assert b.remaining() == length;\n+        }\n+\n+        final List<Tuple<Long, Long>> indexCacheMisses;\n+\n+        // We prefer to use the index cache if the recovery is not done yet\n+        if (directory.isRecoveryDone() == false) {\n+            // We try to use the snapshot blob cache if:\n+            // - we're reading the first N bytes of the file\n+            final boolean isStartOfFile = (position + length <= BlobStoreCacheService.DEFAULT_SIZE);\n+            // - the file is small enough to be fully cached in the blob cache\n+            final boolean canBeFullyCached = fileInfo.length() <= BlobStoreCacheService.DEFAULT_SIZE * 2;\n+\n+            if (canBeFullyCached || isStartOfFile) {\n+                final CachedBlob cachedBlob = directory.getCachedBlob(fileInfo.physicalName(), 0L, length);\n+                if (cachedBlob != null) {\n+                    logger.trace(\n+                        \"reading [{}] bytes of file [{}] at position [{}] using index cache\",\n+                        length,\n+                        fileInfo.physicalName(),\n+                        position\n+                    );\n+                    b.put(BytesReference.toBytes(cachedBlob.bytes().slice(Math.toIntExact(position), length)));\n+                    return;\n                 }\n-            } catch (final Exception e) {\n-                if (e instanceof AlreadyClosedException || (e.getCause() != null && e.getCause() instanceof AlreadyClosedException)) {\n-                    try {\n-                        // cache file was evicted during the range fetching, read bytes directly from source\n-                        bytesRead = readDirectly(pos, pos + len, b);\n-                        continue;\n-                    } catch (Exception inner) {\n-                        e.addSuppressed(inner);\n+\n+                // Cache miss may be that the cache is completely unavailable (no point in populating it) or that the blob is\n+                // definitely absent. TODO only bother populating the cache in the latter case.\n+            }\n+\n+            // We would have liked to find a cached entry but we did not find anything: the cache on the disk will be requested so\n+            // we compute the regions of the file we would like to have the next time. The regions are expressed as tuples of\n+            // {start, end} ranges where positions are relative to the whole file.\n+            if (canBeFullyCached) {\n+                // if the index input is smaller than twice the size of the blob cache, it will be fully indexed\n+                indexCacheMisses = List.of(Tuple.tuple(0L, fileInfo.length()));\n+            } else {\n+                indexCacheMisses = List.of(Tuple.tuple(0L, (long) BlobStoreCacheService.DEFAULT_SIZE));\n+            }\n+            logger.trace(\"recovery cache miss for [{}], falling through with regions [{}]\", this, indexCacheMisses);\n+        } else {\n+            indexCacheMisses = List.of();\n+        }\n+\n+        try {\n+            final CacheFile cacheFile = getCacheFileSafe();\n+            try (Releasable ignored = cacheFile.fileLock()) {\n+\n+                // Read all target ranges in one go, including any cache misses identified above.\n+                final Tuple<Long, Long> startRangeToWrite = computeRange(position);\n+                final Tuple<Long, Long> endRangeToWrite = computeRange(position + length - 1);\n+                assert startRangeToWrite.v2() <= endRangeToWrite.v2() : startRangeToWrite + \" vs \" + endRangeToWrite;\n+                final Tuple<Long, Long> rangeToWrite = Tuple.tuple(\n+                    Math.min(startRangeToWrite.v1(), indexCacheMisses.stream().mapToLong(Tuple::v1).max().orElse(Long.MAX_VALUE)),\n+                    Math.max(endRangeToWrite.v2(), indexCacheMisses.stream().mapToLong(Tuple::v2).max().orElse(Long.MIN_VALUE))\n+                );\n+\n+                assert rangeToWrite.v1() <= position && position + length <= rangeToWrite.v2() : \"[\"\n+                    + position\n+                    + \"-\"\n+                    + (position + length)\n+                    + \"] vs \"\n+                    + rangeToWrite;\n+                final Tuple<Long, Long> rangeToRead = Tuple.tuple(position, position + length);\n+\n+                final CompletableFuture<Integer> populateCacheFuture = cacheFile.populateAndRead(rangeToWrite, rangeToRead, channel -> {\n+                    final int read;\n+                    if ((rangeToRead.v2() - rangeToRead.v1()) < b.remaining()) {\n+                        final ByteBuffer duplicate = b.duplicate();\n+                        duplicate.limit(duplicate.position() + Math.toIntExact(rangeToRead.v2() - rangeToRead.v1()));\n+                        read = readCacheFile(channel, position, duplicate);\n+                        assert duplicate.position() <= b.limit();\n+                        b.position(duplicate.position());\n+                    } else {\n+                        read = readCacheFile(channel, position, b);\n                     }\n+                    return read;\n+                }, this::writeCacheFile, directory.cacheFetchAsyncExecutor());\n+\n+                for (Tuple<Long, Long> indexCacheMiss : indexCacheMisses) {\n+                    cacheFile.populateAndRead(indexCacheMiss, indexCacheMiss, channel -> {\n+                        final int indexCacheMissLength = Math.toIntExact(indexCacheMiss.v2() - indexCacheMiss.v1());\n+\n+                        // We assume that we only cache small portions of blobs so that we do not need to:\n+                        // - use a BigArrays for allocation\n+                        // - use an intermediate copy buffer to read the file in sensibly-sized chunks\n+                        // - release the buffer once the indexing operation is complete\n+                        assert indexCacheMissLength <= COPY_BUFFER_SIZE : indexCacheMiss;\n+\n+                        final ByteBuffer byteBuffer = ByteBuffer.allocate(indexCacheMissLength);\n+                        Channels.readFromFileChannelWithEofException(channel, indexCacheMiss.v1(), byteBuffer);\n+                        // NB use Channels.readFromFileChannelWithEofException not readCacheFile() to avoid counting this in the stats", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1b459aed4a79035dc9366f4ff19118463f745eaf"}, "originalPosition": 176}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc1NjUzNA==", "bodyText": "Why is indexCacheMisses a list if it only contains a single entry?", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r467756534", "createdAt": "2020-08-10T08:29:27Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -136,53 +146,221 @@ protected void readInternal(ByteBuffer b) throws IOException {\n         final long position = getFilePointer() + this.offset;\n         final int length = b.remaining();\n \n-        int totalBytesRead = 0;\n-        while (totalBytesRead < length) {\n-            final long pos = position + totalBytesRead;\n-            final int len = length - totalBytesRead;\n-            int bytesRead = 0;\n-            try {\n-                final CacheFile cacheFile = getCacheFileSafe();\n-                try (Releasable ignored = cacheFile.fileLock()) {\n-                    final Tuple<Long, Long> rangeToWrite = computeRange(pos);\n-                    final Tuple<Long, Long> rangeToRead = Tuple.tuple(pos, Math.min(pos + len, rangeToWrite.v2()));\n-\n-                    bytesRead = cacheFile.fetchAsync(rangeToWrite, rangeToRead, (channel) -> {\n-                        final int read;\n-                        if ((rangeToRead.v2() - rangeToRead.v1()) < b.remaining()) {\n-                            final ByteBuffer duplicate = b.duplicate();\n-                            duplicate.limit(duplicate.position() + Math.toIntExact(rangeToRead.v2() - rangeToRead.v1()));\n-                            read = readCacheFile(channel, pos, duplicate);\n-                            assert duplicate.position() <= b.limit();\n-                            b.position(duplicate.position());\n-                        } else {\n-                            read = readCacheFile(channel, pos, b);\n-                        }\n-                        return read;\n-                    }, this::writeCacheFile, directory.cacheFetchAsyncExecutor()).get();\n+        logger.trace(\"readInternal: read [{}-{}] from [{}]\", position, position + length, this);\n+\n+        // We can detect that we're going to read the last 16 bytes (that contains the footer checksum) of the file. Such reads are often\n+        // executed when opening a Directory and since we have the checksum in the snapshot metadata we can use it to fill the ByteBuffer.\n+        if (length == CodecUtil.footerLength() && isClone == false && position == fileInfo.length() - length) {\n+            if (readChecksumFromFileInfo(b)) {\n+                logger.trace(\"read footer of file [{}] at position [{}], bypassing all caches\", fileInfo.physicalName(), position);\n+                return;\n+            }\n+            assert b.remaining() == length;\n+        }\n+\n+        final List<Tuple<Long, Long>> indexCacheMisses;\n+\n+        // We prefer to use the index cache if the recovery is not done yet\n+        if (directory.isRecoveryDone() == false) {\n+            // We try to use the snapshot blob cache if:\n+            // - we're reading the first N bytes of the file\n+            final boolean isStartOfFile = (position + length <= BlobStoreCacheService.DEFAULT_SIZE);\n+            // - the file is small enough to be fully cached in the blob cache\n+            final boolean canBeFullyCached = fileInfo.length() <= BlobStoreCacheService.DEFAULT_SIZE * 2;\n+\n+            if (canBeFullyCached || isStartOfFile) {\n+                final CachedBlob cachedBlob = directory.getCachedBlob(fileInfo.physicalName(), 0L, length);\n+                if (cachedBlob != null) {\n+                    logger.trace(\n+                        \"reading [{}] bytes of file [{}] at position [{}] using index cache\",\n+                        length,\n+                        fileInfo.physicalName(),\n+                        position\n+                    );\n+                    b.put(BytesReference.toBytes(cachedBlob.bytes().slice(Math.toIntExact(position), length)));\n+                    return;\n                 }\n-            } catch (final Exception e) {\n-                if (e instanceof AlreadyClosedException || (e.getCause() != null && e.getCause() instanceof AlreadyClosedException)) {\n-                    try {\n-                        // cache file was evicted during the range fetching, read bytes directly from source\n-                        bytesRead = readDirectly(pos, pos + len, b);\n-                        continue;\n-                    } catch (Exception inner) {\n-                        e.addSuppressed(inner);\n+\n+                // Cache miss may be that the cache is completely unavailable (no point in populating it) or that the blob is\n+                // definitely absent. TODO only bother populating the cache in the latter case.\n+            }\n+\n+            // We would have liked to find a cached entry but we did not find anything: the cache on the disk will be requested so\n+            // we compute the regions of the file we would like to have the next time. The regions are expressed as tuples of\n+            // {start, end} ranges where positions are relative to the whole file.\n+            if (canBeFullyCached) {\n+                // if the index input is smaller than twice the size of the blob cache, it will be fully indexed\n+                indexCacheMisses = List.of(Tuple.tuple(0L, fileInfo.length()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1b459aed4a79035dc9366f4ff19118463f745eaf"}, "originalPosition": 120}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc1OTc4OA==", "bodyText": "let's make this dynamic: strict", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r467759788", "createdAt": "2020-08-10T08:36:10Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java", "diffHunk": "@@ -0,0 +1,325 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.ResourceAlreadyExistsException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.NoShardAvailableActionException;\n+import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;\n+import org.elasticsearch.action.get.GetRequest;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.index.IndexRequest;\n+import org.elasticsearch.action.index.IndexResponse;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.ClusterChangedEvent;\n+import org.elasticsearch.cluster.ClusterStateListener;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.xcontent.ToXContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.index.IndexNotFoundException;\n+import org.elasticsearch.rest.RestStatus;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.io.IOException;\n+import java.time.Instant;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;\n+import static org.elasticsearch.index.mapper.MapperService.SINGLE_MAPPING_NAME;\n+import static org.elasticsearch.xpack.core.ClientHelper.SEARCHABLE_SNAPSHOTS_ORIGIN;\n+\n+public class BlobStoreCacheService extends AbstractLifecycleComponent implements ClusterStateListener {\n+\n+    private static final Logger logger = LogManager.getLogger(BlobStoreCacheService.class);\n+\n+    public static final int DEFAULT_SIZE = Math.toIntExact(ByteSizeUnit.KB.toBytes(4L));\n+\n+    private final ClusterService clusterService;\n+    private final ThreadPool threadPool;\n+    private final AtomicBoolean ready;\n+    private final Client client;\n+    private final String index;\n+\n+    public BlobStoreCacheService(ClusterService clusterService, ThreadPool threadPool, Client client, String index) {\n+        this.client = new OriginSettingClient(client, SEARCHABLE_SNAPSHOTS_ORIGIN);\n+        this.ready = new AtomicBoolean(false);\n+        this.clusterService = clusterService;\n+        this.threadPool = threadPool;\n+        this.index = index;\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        clusterService.addListener(this);\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        clusterService.removeListener(this);\n+    }\n+\n+    @Override\n+    protected void doClose() {}\n+\n+    @Override\n+    public void clusterChanged(ClusterChangedEvent event) {\n+        if (lifecycle.started() == false || event.routingTableChanged() == false) {\n+            return;\n+        }\n+        if (event.indexRoutingTableChanged(index)) {\n+            final IndexRoutingTable indexRoutingTable = event.state().routingTable().index(index);\n+            if (indexRoutingTable == null) {\n+                ready.set(false);\n+                return;\n+            }\n+            ready.set(indexRoutingTable.allPrimaryShardsActive());\n+        }\n+    }\n+\n+    private void createIndexIfNecessary(ActionListener<String> listener) {\n+        if (clusterService.state().routingTable().hasIndex(index)) {\n+            listener.onResponse(index);\n+            return;\n+        }\n+        try {\n+            client.admin().indices().prepareCreate(index).setSettings(settings()).setMapping(mappings()).execute(new ActionListener<>() {\n+                @Override\n+                public void onResponse(CreateIndexResponse createIndexResponse) {\n+                    assert createIndexResponse.index().equals(index);\n+                    listener.onResponse(createIndexResponse.index());\n+                }\n+\n+                @Override\n+                public void onFailure(Exception e) {\n+                    if (e instanceof ResourceAlreadyExistsException\n+                        || ExceptionsHelper.unwrapCause(e) instanceof ResourceAlreadyExistsException) {\n+                        listener.onResponse(index);\n+                    } else {\n+                        listener.onFailure(e);\n+                    }\n+                }\n+            });\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+        }\n+    }\n+\n+    private static Settings settings() {\n+        return Settings.builder()\n+            .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1)\n+            .put(IndexMetadata.SETTING_AUTO_EXPAND_REPLICAS, \"0-1\")\n+            .put(IndexMetadata.SETTING_PRIORITY, \"900\")\n+            .build();\n+    }\n+\n+    private static XContentBuilder mappings() throws IOException {\n+        final XContentBuilder builder = jsonBuilder();\n+        {\n+            builder.startObject();\n+            {\n+                builder.startObject(SINGLE_MAPPING_NAME);\n+                builder.field(\"dynamic\", \"false\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1b459aed4a79035dc9366f4ff19118463f745eaf"}, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc2MTUxMg==", "bodyText": "Where is this exception propagated to? Do we fail the blobstore read because we failed to  populate the index cache?", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r467761512", "createdAt": "2020-08-10T08:39:57Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -136,53 +146,221 @@ protected void readInternal(ByteBuffer b) throws IOException {\n         final long position = getFilePointer() + this.offset;\n         final int length = b.remaining();\n \n-        int totalBytesRead = 0;\n-        while (totalBytesRead < length) {\n-            final long pos = position + totalBytesRead;\n-            final int len = length - totalBytesRead;\n-            int bytesRead = 0;\n-            try {\n-                final CacheFile cacheFile = getCacheFileSafe();\n-                try (Releasable ignored = cacheFile.fileLock()) {\n-                    final Tuple<Long, Long> rangeToWrite = computeRange(pos);\n-                    final Tuple<Long, Long> rangeToRead = Tuple.tuple(pos, Math.min(pos + len, rangeToWrite.v2()));\n-\n-                    bytesRead = cacheFile.fetchAsync(rangeToWrite, rangeToRead, (channel) -> {\n-                        final int read;\n-                        if ((rangeToRead.v2() - rangeToRead.v1()) < b.remaining()) {\n-                            final ByteBuffer duplicate = b.duplicate();\n-                            duplicate.limit(duplicate.position() + Math.toIntExact(rangeToRead.v2() - rangeToRead.v1()));\n-                            read = readCacheFile(channel, pos, duplicate);\n-                            assert duplicate.position() <= b.limit();\n-                            b.position(duplicate.position());\n-                        } else {\n-                            read = readCacheFile(channel, pos, b);\n-                        }\n-                        return read;\n-                    }, this::writeCacheFile, directory.cacheFetchAsyncExecutor()).get();\n+        logger.trace(\"readInternal: read [{}-{}] from [{}]\", position, position + length, this);\n+\n+        // We can detect that we're going to read the last 16 bytes (that contains the footer checksum) of the file. Such reads are often\n+        // executed when opening a Directory and since we have the checksum in the snapshot metadata we can use it to fill the ByteBuffer.\n+        if (length == CodecUtil.footerLength() && isClone == false && position == fileInfo.length() - length) {\n+            if (readChecksumFromFileInfo(b)) {\n+                logger.trace(\"read footer of file [{}] at position [{}], bypassing all caches\", fileInfo.physicalName(), position);\n+                return;\n+            }\n+            assert b.remaining() == length;\n+        }\n+\n+        final List<Tuple<Long, Long>> indexCacheMisses;\n+\n+        // We prefer to use the index cache if the recovery is not done yet\n+        if (directory.isRecoveryDone() == false) {\n+            // We try to use the snapshot blob cache if:\n+            // - we're reading the first N bytes of the file\n+            final boolean isStartOfFile = (position + length <= BlobStoreCacheService.DEFAULT_SIZE);\n+            // - the file is small enough to be fully cached in the blob cache\n+            final boolean canBeFullyCached = fileInfo.length() <= BlobStoreCacheService.DEFAULT_SIZE * 2;\n+\n+            if (canBeFullyCached || isStartOfFile) {\n+                final CachedBlob cachedBlob = directory.getCachedBlob(fileInfo.physicalName(), 0L, length);\n+                if (cachedBlob != null) {\n+                    logger.trace(\n+                        \"reading [{}] bytes of file [{}] at position [{}] using index cache\",\n+                        length,\n+                        fileInfo.physicalName(),\n+                        position\n+                    );\n+                    b.put(BytesReference.toBytes(cachedBlob.bytes().slice(Math.toIntExact(position), length)));\n+                    return;\n                 }\n-            } catch (final Exception e) {\n-                if (e instanceof AlreadyClosedException || (e.getCause() != null && e.getCause() instanceof AlreadyClosedException)) {\n-                    try {\n-                        // cache file was evicted during the range fetching, read bytes directly from source\n-                        bytesRead = readDirectly(pos, pos + len, b);\n-                        continue;\n-                    } catch (Exception inner) {\n-                        e.addSuppressed(inner);\n+\n+                // Cache miss may be that the cache is completely unavailable (no point in populating it) or that the blob is\n+                // definitely absent. TODO only bother populating the cache in the latter case.\n+            }\n+\n+            // We would have liked to find a cached entry but we did not find anything: the cache on the disk will be requested so\n+            // we compute the regions of the file we would like to have the next time. The regions are expressed as tuples of\n+            // {start, end} ranges where positions are relative to the whole file.\n+            if (canBeFullyCached) {\n+                // if the index input is smaller than twice the size of the blob cache, it will be fully indexed\n+                indexCacheMisses = List.of(Tuple.tuple(0L, fileInfo.length()));\n+            } else {\n+                indexCacheMisses = List.of(Tuple.tuple(0L, (long) BlobStoreCacheService.DEFAULT_SIZE));\n+            }\n+            logger.trace(\"recovery cache miss for [{}], falling through with regions [{}]\", this, indexCacheMisses);\n+        } else {\n+            indexCacheMisses = List.of();\n+        }\n+\n+        try {\n+            final CacheFile cacheFile = getCacheFileSafe();\n+            try (Releasable ignored = cacheFile.fileLock()) {\n+\n+                // Read all target ranges in one go, including any cache misses identified above.\n+                final Tuple<Long, Long> startRangeToWrite = computeRange(position);\n+                final Tuple<Long, Long> endRangeToWrite = computeRange(position + length - 1);\n+                assert startRangeToWrite.v2() <= endRangeToWrite.v2() : startRangeToWrite + \" vs \" + endRangeToWrite;\n+                final Tuple<Long, Long> rangeToWrite = Tuple.tuple(\n+                    Math.min(startRangeToWrite.v1(), indexCacheMisses.stream().mapToLong(Tuple::v1).max().orElse(Long.MAX_VALUE)),\n+                    Math.max(endRangeToWrite.v2(), indexCacheMisses.stream().mapToLong(Tuple::v2).max().orElse(Long.MIN_VALUE))\n+                );\n+\n+                assert rangeToWrite.v1() <= position && position + length <= rangeToWrite.v2() : \"[\"\n+                    + position\n+                    + \"-\"\n+                    + (position + length)\n+                    + \"] vs \"\n+                    + rangeToWrite;\n+                final Tuple<Long, Long> rangeToRead = Tuple.tuple(position, position + length);\n+\n+                final CompletableFuture<Integer> populateCacheFuture = cacheFile.populateAndRead(rangeToWrite, rangeToRead, channel -> {\n+                    final int read;\n+                    if ((rangeToRead.v2() - rangeToRead.v1()) < b.remaining()) {\n+                        final ByteBuffer duplicate = b.duplicate();\n+                        duplicate.limit(duplicate.position() + Math.toIntExact(rangeToRead.v2() - rangeToRead.v1()));\n+                        read = readCacheFile(channel, position, duplicate);\n+                        assert duplicate.position() <= b.limit();\n+                        b.position(duplicate.position());\n+                    } else {\n+                        read = readCacheFile(channel, position, b);\n                     }\n+                    return read;\n+                }, this::writeCacheFile, directory.cacheFetchAsyncExecutor());\n+\n+                for (Tuple<Long, Long> indexCacheMiss : indexCacheMisses) {\n+                    cacheFile.populateAndRead(indexCacheMiss, indexCacheMiss, channel -> {\n+                        final int indexCacheMissLength = Math.toIntExact(indexCacheMiss.v2() - indexCacheMiss.v1());\n+\n+                        // We assume that we only cache small portions of blobs so that we do not need to:\n+                        // - use a BigArrays for allocation\n+                        // - use an intermediate copy buffer to read the file in sensibly-sized chunks\n+                        // - release the buffer once the indexing operation is complete\n+                        assert indexCacheMissLength <= COPY_BUFFER_SIZE : indexCacheMiss;\n+\n+                        final ByteBuffer byteBuffer = ByteBuffer.allocate(indexCacheMissLength);\n+                        Channels.readFromFileChannelWithEofException(channel, indexCacheMiss.v1(), byteBuffer);\n+                        // NB use Channels.readFromFileChannelWithEofException not readCacheFile() to avoid counting this in the stats\n+                        byteBuffer.flip();\n+                        final BytesReference content = BytesReference.fromByteBuffer(byteBuffer);\n+                        directory.putCachedBlob(fileInfo.physicalName(), indexCacheMiss.v1(), content);\n+                        return indexCacheMissLength;\n+                    }, (channel, from, to, progressUpdater) -> {\n+                        // normally doesn't happen, we're already obtaining a range covering all cache misses above, but this\n+                        // can happen if the real populateAndRead call already failed to obtain this range of the file. In that\n+                        // case, we don't retry, we simply fail to populate the index cache.\n+                        logger.debug(\n+                            \"failed to fill index cache miss [{}-{}] of {} due to earlier failure\",\n+                            from,\n+                            to,\n+                            CachedBlobContainerIndexInput.this\n+                        );\n+                        throw new IOException(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1b459aed4a79035dc9366f4ff19118463f745eaf"}, "originalPosition": 191}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc2NTI3Nw==", "bodyText": "Probably good to address this in the PR here.", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r467765277", "createdAt": "2020-08-10T08:47:23Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java", "diffHunk": "@@ -0,0 +1,654 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.elasticsearch.action.admin.indices.forcemerge.ForceMergeResponse;\n+import org.elasticsearch.action.admin.indices.refresh.RefreshResponse;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.index.IndexRequestBuilder;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.routing.RoutingNode;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.routing.ShardsIterator;\n+import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;\n+import org.elasticsearch.cluster.routing.allocation.decider.AllocationDecider;\n+import org.elasticsearch.cluster.routing.allocation.decider.Decision;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.blobstore.BlobContainer;\n+import org.elasticsearch.common.blobstore.BlobPath;\n+import org.elasticsearch.common.blobstore.BlobStore;\n+import org.elasticsearch.common.blobstore.support.FilterBlobContainer;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.ClusterSettings;\n+import org.elasticsearch.common.settings.Setting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.common.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.env.Environment;\n+import org.elasticsearch.index.IndexNotFoundException;\n+import org.elasticsearch.index.query.QueryBuilders;\n+import org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardSnapshot;\n+import org.elasticsearch.indices.recovery.RecoverySettings;\n+import org.elasticsearch.plugins.ClusterPlugin;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.plugins.PluginsService;\n+import org.elasticsearch.plugins.RepositoryPlugin;\n+import org.elasticsearch.repositories.Repository;\n+import org.elasticsearch.repositories.fs.FsRepository;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.xpack.core.ClientHelper;\n+import org.elasticsearch.xpack.searchablesnapshots.BaseSearchableSnapshotsIntegTestCase;\n+import org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots;\n+import org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants;\n+import org.elasticsearch.xpack.searchablesnapshots.cache.CacheService;\n+\n+import java.io.FilterInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.file.FileVisitResult;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.SimpleFileVisitor;\n+import java.nio.file.attribute.BasicFileAttributes;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.function.BiConsumer;\n+\n+import static org.elasticsearch.repositories.blobstore.BlobStoreRepository.INDEX_SHARD_SNAPSHOT_FORMAT;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.CACHE_FETCH_ASYNC_THREAD_POOL_NAME;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.CACHE_PREWARMING_THREAD_POOL_NAME;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.SNAPSHOT_BLOB_CACHE_INDEX;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.greaterThanOrEqualTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.notNullValue;\n+\n+public class SearchableSnapshotsBlobStoreCacheIntegTests extends BaseSearchableSnapshotsIntegTestCase {\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        final List<Class<? extends Plugin>> plugins = new ArrayList<>();\n+        plugins.add(WaitForSnapshotBlobCacheShardsActivePlugin.class);\n+        plugins.add(TrackingRepositoryPlugin.class);\n+        plugins.addAll(super.nodePlugins());\n+        return List.copyOf(plugins);\n+    }\n+\n+    @Override\n+    protected int numberOfReplicas() {\n+        return 0;\n+    }\n+\n+    @Override\n+    protected Settings nodeSettings(int nodeOrdinal) {\n+        return Settings.builder()\n+            .put(super.nodeSettings(nodeOrdinal))\n+            // Use a cache range size setting aligned with BufferedIndexInput's buffer size and BlobStoreCacheService's default size\n+            // TODO randomized this", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1b459aed4a79035dc9366f4ff19118463f745eaf"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc3MDkwOQ==", "bodyText": "???", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r467770909", "createdAt": "2020-08-10T08:58:36Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java", "diffHunk": "@@ -0,0 +1,654 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.elasticsearch.action.admin.indices.forcemerge.ForceMergeResponse;\n+import org.elasticsearch.action.admin.indices.refresh.RefreshResponse;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.index.IndexRequestBuilder;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.routing.RoutingNode;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.routing.ShardsIterator;\n+import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;\n+import org.elasticsearch.cluster.routing.allocation.decider.AllocationDecider;\n+import org.elasticsearch.cluster.routing.allocation.decider.Decision;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.blobstore.BlobContainer;\n+import org.elasticsearch.common.blobstore.BlobPath;\n+import org.elasticsearch.common.blobstore.BlobStore;\n+import org.elasticsearch.common.blobstore.support.FilterBlobContainer;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.ClusterSettings;\n+import org.elasticsearch.common.settings.Setting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.common.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.env.Environment;\n+import org.elasticsearch.index.IndexNotFoundException;\n+import org.elasticsearch.index.query.QueryBuilders;\n+import org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardSnapshot;\n+import org.elasticsearch.indices.recovery.RecoverySettings;\n+import org.elasticsearch.plugins.ClusterPlugin;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.plugins.PluginsService;\n+import org.elasticsearch.plugins.RepositoryPlugin;\n+import org.elasticsearch.repositories.Repository;\n+import org.elasticsearch.repositories.fs.FsRepository;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.xpack.core.ClientHelper;\n+import org.elasticsearch.xpack.searchablesnapshots.BaseSearchableSnapshotsIntegTestCase;\n+import org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots;\n+import org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants;\n+import org.elasticsearch.xpack.searchablesnapshots.cache.CacheService;\n+\n+import java.io.FilterInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.file.FileVisitResult;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.SimpleFileVisitor;\n+import java.nio.file.attribute.BasicFileAttributes;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.function.BiConsumer;\n+\n+import static org.elasticsearch.repositories.blobstore.BlobStoreRepository.INDEX_SHARD_SNAPSHOT_FORMAT;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.CACHE_FETCH_ASYNC_THREAD_POOL_NAME;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.CACHE_PREWARMING_THREAD_POOL_NAME;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.SNAPSHOT_BLOB_CACHE_INDEX;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.greaterThanOrEqualTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.notNullValue;\n+\n+public class SearchableSnapshotsBlobStoreCacheIntegTests extends BaseSearchableSnapshotsIntegTestCase {\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        final List<Class<? extends Plugin>> plugins = new ArrayList<>();\n+        plugins.add(WaitForSnapshotBlobCacheShardsActivePlugin.class);\n+        plugins.add(TrackingRepositoryPlugin.class);\n+        plugins.addAll(super.nodePlugins());\n+        return List.copyOf(plugins);\n+    }\n+\n+    @Override\n+    protected int numberOfReplicas() {\n+        return 0;\n+    }\n+\n+    @Override\n+    protected Settings nodeSettings(int nodeOrdinal) {\n+        return Settings.builder()\n+            .put(super.nodeSettings(nodeOrdinal))\n+            // Use a cache range size setting aligned with BufferedIndexInput's buffer size and BlobStoreCacheService's default size\n+            // TODO randomized this\n+            .put(\n+                CacheService.SNAPSHOT_CACHE_RANGE_SIZE_SETTING.getKey(),\n+                new ByteSizeValue(BlobStoreCacheService.DEFAULT_SIZE, ByteSizeUnit.BYTES)\n+            )\n+            .put(CacheService.SNAPSHOT_CACHE_SIZE_SETTING.getKey(), new ByteSizeValue(Long.MAX_VALUE, ByteSizeUnit.BYTES))\n+            .build();\n+    }\n+\n+    public void testBlobStoreCache() throws Exception {\n+        final String indexName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT);\n+        createIndex(indexName);\n+\n+        final List<IndexRequestBuilder> indexRequestBuilders = new ArrayList<>();\n+        for (int i = scaledRandomIntBetween(0, 10_000); i >= 0; i--) {\n+            indexRequestBuilders.add(client().prepareIndex(indexName).setSource(\"text\", randomUnicodeOfLength(10), \"num\", i));\n+        }\n+        indexRandom(true, false, true, indexRequestBuilders);\n+        final long numberOfDocs = indexRequestBuilders.size();\n+        final NumShards numberOfShards = getNumShards(indexName);\n+\n+        final ForceMergeResponse forceMergeResponse = client().admin().indices().prepareForceMerge(indexName).setMaxNumSegments(1).get();\n+        assertThat(forceMergeResponse.getSuccessfulShards(), equalTo(numberOfShards.totalNumShards));\n+        assertThat(forceMergeResponse.getFailedShards(), equalTo(0));\n+\n+        final String repositoryName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT);\n+        final Path repositoryLocation = randomRepoPath();\n+        createFsRepository(repositoryName, repositoryLocation);\n+\n+        final SnapshotId snapshot = createSnapshot(repositoryName, List.of(indexName));\n+        assertAcked(client().admin().indices().prepareDelete(indexName));\n+\n+        // extract the list of blobs per shard from the snapshot directory on disk\n+        final Map<String, BlobStoreIndexShardSnapshot> blobsInSnapshot = blobsInSnapshot(repositoryLocation, snapshot.getUUID());\n+        assertThat(\"Failed to load all shard snapshot metadata files\", blobsInSnapshot.size(), equalTo(numberOfShards.numPrimaries));\n+\n+        // register a new repository that can track blob read operations\n+        assertAcked(client().admin().cluster().prepareDeleteRepository(repositoryName));\n+        createRepository(\n+            repositoryName,\n+            TrackingRepositoryPlugin.TRACKING,\n+            Settings.builder().put(FsRepository.LOCATION_SETTING.getKey(), repositoryLocation).build(),\n+            false\n+        );\n+        assertBusy(this::ensureClusterStateConsistency);\n+\n+        expectThrows(\n+            IndexNotFoundException.class,\n+            \".snapshot-blob-cache system index should not be created yet\",\n+            () -> systemClient().admin().indices().prepareGetIndex().addIndices(SNAPSHOT_BLOB_CACHE_INDEX).get()\n+        );\n+\n+        final boolean usePrewarming = false; // TODO randomize this and adapt test\n+\n+        logger.info(\"--> mount snapshot [{}] as an index for the first time\", snapshot);\n+        final String restoredIndex = mountSnapshot(\n+            repositoryName,\n+            snapshot.getName(),\n+            indexName,\n+            Settings.builder()\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_ENABLED_SETTING.getKey(), true)\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_PREWARM_ENABLED_SETTING.getKey(), usePrewarming)\n+                .build()\n+        );\n+        ensureGreen(restoredIndex);\n+        ensureExecutorsAreIdle();\n+\n+        logger.info(\"--> verifying cached documents in system index [{}]\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        assertCachedBlobsInSystemIndex(repositoryName, blobsInSnapshot);\n+\n+        refreshSystemIndex();\n+        final long numberOfCachedBlobs = systemClient().prepareSearch(SNAPSHOT_BLOB_CACHE_INDEX).get().getHits().getTotalHits().value;\n+\n+        ensureBlobStoreRepositoriesWithActiveShards(\n+            restoredIndex,\n+            (nodeId, blobStore) -> assertThat(\n+                \"Blob read operations should have been executed on node [\" + nodeId + ']',\n+                blobStore.numberOfReads(),\n+                greaterThan(0L)\n+            )\n+        );\n+\n+        logger.info(\"--> verifying documents in index [{}]\", restoredIndex);\n+        assertHitCount(client().prepareSearch(restoredIndex).setSize(0).setTrackTotalHits(true).get(), numberOfDocs);\n+        assertHitCount(\n+            client().prepareSearch(restoredIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").lte(numberOfDocs))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            numberOfDocs\n+        );\n+        assertHitCount(\n+            client().prepareSearch(restoredIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").gt(numberOfDocs + 1))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            0L\n+        );\n+\n+        assertAcked(client().admin().indices().prepareDelete(restoredIndex));\n+        resetTrackedFiles();\n+\n+        logger.info(\"--> mount snapshot [{}] as an index for the second time\", snapshot);\n+        final String restoredAgainIndex = mountSnapshot(\n+            repositoryName,\n+            snapshot.getName(),\n+            indexName,\n+            Settings.builder()\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_ENABLED_SETTING.getKey(), true)\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_PREWARM_ENABLED_SETTING.getKey(), usePrewarming)\n+                .build()\n+        );\n+        ensureGreen(restoredAgainIndex);\n+        ensureExecutorsAreIdle();\n+\n+        logger.info(\"--> verifying documents in index [{}]\", restoredAgainIndex);\n+        assertHitCount(client().prepareSearch(restoredAgainIndex).setSize(0).setTrackTotalHits(true).get(), numberOfDocs);\n+        assertHitCount(\n+            client().prepareSearch(restoredAgainIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").lte(numberOfDocs))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            numberOfDocs\n+        );\n+        assertHitCount(\n+            client().prepareSearch(restoredAgainIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").gt(numberOfDocs + 1))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            0L\n+        );\n+\n+        logger.info(\"--> verifying cached documents (again) in system index [{}]\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        assertCachedBlobsInSystemIndex(repositoryName, blobsInSnapshot);\n+\n+        logger.info(\"--> verifying that no cached blobs were indexed in system index [{}]\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        refreshSystemIndex();\n+        assertHitCount(systemClient().prepareSearch(SNAPSHOT_BLOB_CACHE_INDEX).setSize(0).get(), numberOfCachedBlobs);\n+\n+        logger.info(\"--> verifying blobs read from the repository\");\n+        assertBlobsReadFromRemoteRepository(restoredAgainIndex, blobsInSnapshot);\n+\n+        resetTrackedFiles();\n+\n+        logger.info(\"--> restarting cluster\");\n+        internalCluster().fullRestart(new InternalTestCluster.RestartCallback() {\n+            @Override\n+            public Settings onNodeStopped(String nodeName) throws Exception {\n+                return Settings.builder()\n+                    .put(super.onNodeStopped(nodeName))\n+                    .put(WaitForSnapshotBlobCacheShardsActivePlugin.ENABLED.getKey(), true)\n+                    .build();\n+            }\n+        });\n+        ensureGreen(restoredAgainIndex);\n+        ensureExecutorsAreIdle();\n+\n+        logger.info(\"--> verifying documents in index [{}]\", restoredAgainIndex);\n+        assertHitCount(client().prepareSearch(restoredAgainIndex).setSize(0).setTrackTotalHits(true).get(), numberOfDocs);\n+        assertHitCount(client().prepareSearch(restoredAgainIndex).setSize(0).setTrackTotalHits(true).get(), numberOfDocs);\n+        assertHitCount(\n+            client().prepareSearch(restoredAgainIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").lte(numberOfDocs))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            numberOfDocs\n+        );\n+        assertHitCount(\n+            client().prepareSearch(restoredAgainIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").gt(numberOfDocs + 1))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            0L\n+        );\n+\n+        logger.info(\"--> verifying cached documents (after restart) in system index [{}]\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        assertCachedBlobsInSystemIndex(repositoryName, blobsInSnapshot);\n+\n+        logger.info(\"--> verifying that no cached blobs were indexed in system index [{}] after restart\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        assertHitCount(systemClient().prepareSearch(SNAPSHOT_BLOB_CACHE_INDEX).setSize(0).get(), numberOfCachedBlobs);\n+\n+        logger.info(\"--> verifying blobs read from the repository after restart\");\n+        // Without the WaitForSnapshotBlobCacheShardsActivePlugin this would fail\n+        assertBlobsReadFromRemoteRepository(restoredAgainIndex, blobsInSnapshot);\n+\n+        // TODO would be great to test when the index is frozen\n+    }\n+\n+    /**\n+     * @return a {@link Client} that can be used to query the blob store cache system index\n+     */\n+    private Client systemClient() {\n+        return new OriginSettingClient(client(), ClientHelper.SEARCHABLE_SNAPSHOTS_ORIGIN);\n+    }\n+\n+    private void refreshSystemIndex() {\n+        final RefreshResponse refreshResponse = systemClient().admin().indices().prepareRefresh(SNAPSHOT_BLOB_CACHE_INDEX).get();\n+        assertThat(refreshResponse.getSuccessfulShards(), greaterThan(0));\n+        assertThat(refreshResponse.getFailedShards(), equalTo(0));\n+    }\n+\n+    /**\n+     * Reads a repository location on disk and extracts the list of blobs for each shards\n+     */\n+    private Map<String, BlobStoreIndexShardSnapshot> blobsInSnapshot(Path repositoryLocation, String snapshotId) throws IOException {\n+        final Map<String, BlobStoreIndexShardSnapshot> blobsPerShard = new HashMap<>();\n+        Files.walkFileTree(repositoryLocation.resolve(\"indices\"), new SimpleFileVisitor<>() {\n+            @Override\n+            public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException {\n+                final String fileName = file.getFileName().toString();\n+                if (fileName.equals(\"snap-\" + snapshotId + \".dat\")) {\n+                    blobsPerShard.put(\n+                        String.join(\n+                            \"/\",\n+                            snapshotId,\n+                            file.getParent().getParent().getFileName().toString(),\n+                            file.getParent().getFileName().toString()\n+                        ),\n+                        INDEX_SHARD_SNAPSHOT_FORMAT.deserialize(fileName, xContentRegistry(), Streams.readFully(Files.newInputStream(file)))\n+                    );\n+                }\n+                return FileVisitResult.CONTINUE;\n+            }\n+        });\n+        return Map.copyOf(blobsPerShard);\n+    }\n+\n+    private void ensureExecutorsAreIdle() throws Exception {\n+        assertBusy(() -> {\n+            for (ThreadPool threadPool : internalCluster().getDataNodeInstances(ThreadPool.class)) {\n+                for (String threadPoolName : List.of(CACHE_FETCH_ASYNC_THREAD_POOL_NAME, CACHE_PREWARMING_THREAD_POOL_NAME)) {\n+                    final ThreadPoolExecutor executor = (ThreadPoolExecutor) threadPool.executor(threadPoolName);\n+                    assertThat(threadPoolName, executor.getQueue().size(), equalTo(0));\n+                    assertThat(threadPoolName, executor.getActiveCount(), equalTo(0));\n+                }\n+            }\n+        });\n+    }\n+\n+    private void assertCachedBlobsInSystemIndex(final String repositoryName, final Map<String, BlobStoreIndexShardSnapshot> blobsInSnapshot)\n+        throws Exception {\n+        assertBusy(() -> {\n+            refreshSystemIndex();\n+\n+            long numberOfCachedBlobs = 0L;\n+            for (Map.Entry<String, BlobStoreIndexShardSnapshot> blob : blobsInSnapshot.entrySet()) {\n+                for (BlobStoreIndexShardSnapshot.FileInfo fileInfo : blob.getValue().indexFiles()) {\n+                    if (fileInfo.name().startsWith(\"__\") == false) {\n+                        continue;\n+                    }\n+\n+                    final String path = String.join(\"/\", repositoryName, blob.getKey(), fileInfo.physicalName());\n+                    if (fileInfo.length() <= BlobStoreCacheService.DEFAULT_SIZE * 2) {\n+                        // file has been fully cached\n+                        final GetResponse getResponse = systemClient().prepareGet(SNAPSHOT_BLOB_CACHE_INDEX, path + \"/@0\").get();\n+                        assertThat(\"not cached: [\" + path + \"/@0] for blob [\" + fileInfo + \"]\", getResponse.isExists(), is(true));\n+                        final CachedBlob cachedBlob = CachedBlob.fromSource(getResponse.getSourceAsMap());\n+                        assertThat(cachedBlob.from(), equalTo(0L));\n+                        assertThat(cachedBlob.to(), equalTo(fileInfo.length()));\n+                        assertThat((long) cachedBlob.length(), equalTo(fileInfo.length()));\n+                        numberOfCachedBlobs += 1;\n+\n+                    } else {\n+                        // first region of file has been cached\n+                        GetResponse getResponse = systemClient().prepareGet(SNAPSHOT_BLOB_CACHE_INDEX, path + \"/@0\").get();\n+                        assertThat(\n+                            \"not cached: [\" + path + \"/@0] for first region of blob [\" + fileInfo + \"]\",\n+                            getResponse.isExists(),\n+                            is(true)\n+                        );\n+\n+                        CachedBlob cachedBlob = CachedBlob.fromSource(getResponse.getSourceAsMap());\n+                        assertThat(cachedBlob.from(), equalTo(0L));\n+                        assertThat(cachedBlob.to(), equalTo((long) BlobStoreCacheService.DEFAULT_SIZE));\n+                        assertThat(cachedBlob.length(), equalTo(BlobStoreCacheService.DEFAULT_SIZE));\n+                        numberOfCachedBlobs += 1;\n+                    }\n+                }\n+            }\n+\n+            refreshSystemIndex();\n+            assertHitCount(systemClient().prepareSearch(SNAPSHOT_BLOB_CACHE_INDEX).setSize(0).get(), numberOfCachedBlobs);\n+        });\n+    }\n+\n+    private void assertBlobsReadFromRemoteRepository(\n+        final String indexName,\n+        final Map<String, BlobStoreIndexShardSnapshot> blobsInSnapshot\n+    ) {\n+        ensureBlobStoreRepositoriesWithActiveShards(indexName, (nodeId, blobStore) -> {\n+            for (Map.Entry<String, List<Tuple<Long, Long>>> blob : blobStore.blobs.entrySet()) {\n+                final String blobName = blob.getKey();\n+\n+                if (blobName.endsWith(\".dat\") || blobName.equals(\"index-0\")) {\n+                    // The snapshot metadata files are accessed when recovering from the snapshot during restore and do not benefit from\n+                    // the snapshot blob cache as the files are accessed outside of a searchable snapshot directory\n+                    assertThat(\n+                        blobName + \" should be fully read from the beginning\",\n+                        blob.getValue().stream().allMatch(read -> read.v1() == 0L),\n+                        is(true)\n+                    );\n+                    // TODO assert it is read til the end", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1b459aed4a79035dc9366f4ff19118463f745eaf"}, "originalPosition": 418}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc3NDY3NA==", "bodyText": "let's remove this field for now", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r467774674", "createdAt": "2020-08-10T09:06:01Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java", "diffHunk": "@@ -0,0 +1,325 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.ResourceAlreadyExistsException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.NoShardAvailableActionException;\n+import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;\n+import org.elasticsearch.action.get.GetRequest;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.index.IndexRequest;\n+import org.elasticsearch.action.index.IndexResponse;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.ClusterChangedEvent;\n+import org.elasticsearch.cluster.ClusterStateListener;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.xcontent.ToXContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.index.IndexNotFoundException;\n+import org.elasticsearch.rest.RestStatus;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.io.IOException;\n+import java.time.Instant;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;\n+import static org.elasticsearch.index.mapper.MapperService.SINGLE_MAPPING_NAME;\n+import static org.elasticsearch.xpack.core.ClientHelper.SEARCHABLE_SNAPSHOTS_ORIGIN;\n+\n+public class BlobStoreCacheService extends AbstractLifecycleComponent implements ClusterStateListener {\n+\n+    private static final Logger logger = LogManager.getLogger(BlobStoreCacheService.class);\n+\n+    public static final int DEFAULT_SIZE = Math.toIntExact(ByteSizeUnit.KB.toBytes(4L));\n+\n+    private final ClusterService clusterService;\n+    private final ThreadPool threadPool;\n+    private final AtomicBoolean ready;\n+    private final Client client;\n+    private final String index;\n+\n+    public BlobStoreCacheService(ClusterService clusterService, ThreadPool threadPool, Client client, String index) {\n+        this.client = new OriginSettingClient(client, SEARCHABLE_SNAPSHOTS_ORIGIN);\n+        this.ready = new AtomicBoolean(false);\n+        this.clusterService = clusterService;\n+        this.threadPool = threadPool;\n+        this.index = index;\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        clusterService.addListener(this);\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        clusterService.removeListener(this);\n+    }\n+\n+    @Override\n+    protected void doClose() {}\n+\n+    @Override\n+    public void clusterChanged(ClusterChangedEvent event) {\n+        if (lifecycle.started() == false || event.routingTableChanged() == false) {\n+            return;\n+        }\n+        if (event.indexRoutingTableChanged(index)) {\n+            final IndexRoutingTable indexRoutingTable = event.state().routingTable().index(index);\n+            if (indexRoutingTable == null) {\n+                ready.set(false);\n+                return;\n+            }\n+            ready.set(indexRoutingTable.allPrimaryShardsActive());\n+        }\n+    }\n+\n+    private void createIndexIfNecessary(ActionListener<String> listener) {\n+        if (clusterService.state().routingTable().hasIndex(index)) {\n+            listener.onResponse(index);\n+            return;\n+        }\n+        try {\n+            client.admin().indices().prepareCreate(index).setSettings(settings()).setMapping(mappings()).execute(new ActionListener<>() {\n+                @Override\n+                public void onResponse(CreateIndexResponse createIndexResponse) {\n+                    assert createIndexResponse.index().equals(index);\n+                    listener.onResponse(createIndexResponse.index());\n+                }\n+\n+                @Override\n+                public void onFailure(Exception e) {\n+                    if (e instanceof ResourceAlreadyExistsException\n+                        || ExceptionsHelper.unwrapCause(e) instanceof ResourceAlreadyExistsException) {\n+                        listener.onResponse(index);\n+                    } else {\n+                        listener.onFailure(e);\n+                    }\n+                }\n+            });\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+        }\n+    }\n+\n+    private static Settings settings() {\n+        return Settings.builder()\n+            .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1)\n+            .put(IndexMetadata.SETTING_AUTO_EXPAND_REPLICAS, \"0-1\")\n+            .put(IndexMetadata.SETTING_PRIORITY, \"900\")\n+            .build();\n+    }\n+\n+    private static XContentBuilder mappings() throws IOException {\n+        final XContentBuilder builder = jsonBuilder();\n+        {\n+            builder.startObject();\n+            {\n+                builder.startObject(SINGLE_MAPPING_NAME);\n+                builder.field(\"dynamic\", \"false\");\n+                {\n+                    builder.startObject(\"_meta\");\n+                    builder.field(\"version\", Version.CURRENT);\n+                    builder.endObject();\n+                }\n+                {\n+                    builder.startObject(\"properties\");\n+                    {\n+                        builder.startObject(\"type\");\n+                        builder.field(\"type\", \"keyword\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"creation_time\");\n+                        builder.field(\"type\", \"date\");\n+                        builder.field(\"format\", \"epoch_millis\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"accessed_time\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1b459aed4a79035dc9366f4ff19118463f745eaf"}, "originalPosition": 158}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc3ODU1Ng==", "bodyText": "Do we want to put the file during recovery into the node-local cache (at least if it's the full file)? Can be done in follow-up ofc.", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r467778556", "createdAt": "2020-08-10T09:13:49Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -136,53 +146,221 @@ protected void readInternal(ByteBuffer b) throws IOException {\n         final long position = getFilePointer() + this.offset;\n         final int length = b.remaining();\n \n-        int totalBytesRead = 0;\n-        while (totalBytesRead < length) {\n-            final long pos = position + totalBytesRead;\n-            final int len = length - totalBytesRead;\n-            int bytesRead = 0;\n-            try {\n-                final CacheFile cacheFile = getCacheFileSafe();\n-                try (Releasable ignored = cacheFile.fileLock()) {\n-                    final Tuple<Long, Long> rangeToWrite = computeRange(pos);\n-                    final Tuple<Long, Long> rangeToRead = Tuple.tuple(pos, Math.min(pos + len, rangeToWrite.v2()));\n-\n-                    bytesRead = cacheFile.fetchAsync(rangeToWrite, rangeToRead, (channel) -> {\n-                        final int read;\n-                        if ((rangeToRead.v2() - rangeToRead.v1()) < b.remaining()) {\n-                            final ByteBuffer duplicate = b.duplicate();\n-                            duplicate.limit(duplicate.position() + Math.toIntExact(rangeToRead.v2() - rangeToRead.v1()));\n-                            read = readCacheFile(channel, pos, duplicate);\n-                            assert duplicate.position() <= b.limit();\n-                            b.position(duplicate.position());\n-                        } else {\n-                            read = readCacheFile(channel, pos, b);\n-                        }\n-                        return read;\n-                    }, this::writeCacheFile, directory.cacheFetchAsyncExecutor()).get();\n+        logger.trace(\"readInternal: read [{}-{}] from [{}]\", position, position + length, this);\n+\n+        // We can detect that we're going to read the last 16 bytes (that contains the footer checksum) of the file. Such reads are often\n+        // executed when opening a Directory and since we have the checksum in the snapshot metadata we can use it to fill the ByteBuffer.\n+        if (length == CodecUtil.footerLength() && isClone == false && position == fileInfo.length() - length) {\n+            if (readChecksumFromFileInfo(b)) {\n+                logger.trace(\"read footer of file [{}] at position [{}], bypassing all caches\", fileInfo.physicalName(), position);\n+                return;\n+            }\n+            assert b.remaining() == length;\n+        }\n+\n+        final List<Tuple<Long, Long>> indexCacheMisses;\n+\n+        // We prefer to use the index cache if the recovery is not done yet\n+        if (directory.isRecoveryDone() == false) {\n+            // We try to use the snapshot blob cache if:\n+            // - we're reading the first N bytes of the file\n+            final boolean isStartOfFile = (position + length <= BlobStoreCacheService.DEFAULT_SIZE);\n+            // - the file is small enough to be fully cached in the blob cache\n+            final boolean canBeFullyCached = fileInfo.length() <= BlobStoreCacheService.DEFAULT_SIZE * 2;\n+\n+            if (canBeFullyCached || isStartOfFile) {\n+                final CachedBlob cachedBlob = directory.getCachedBlob(fileInfo.physicalName(), 0L, length);\n+                if (cachedBlob != null) {\n+                    logger.trace(\n+                        \"reading [{}] bytes of file [{}] at position [{}] using index cache\",\n+                        length,\n+                        fileInfo.physicalName(),\n+                        position\n+                    );\n+                    b.put(BytesReference.toBytes(cachedBlob.bytes().slice(Math.toIntExact(position), length)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1b459aed4a79035dc9366f4ff19118463f745eaf"}, "originalPosition": 99}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f06e0ae49760a3fd12ca5ff271711df2cb12d0fb", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/f06e0ae49760a3fd12ca5ff271711df2cb12d0fb", "committedDate": "2020-08-10T09:41:58Z", "message": "Try to fill the cache miss rather than immediately failing"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c5b65bc4c58eea930382bc93a1fa9600fceeab1f", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/c5b65bc4c58eea930382bc93a1fa9600fceeab1f", "committedDate": "2020-08-10T09:51:28Z", "message": "No need for a list with <= 1 element"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "974c09593f29b89546e751c42ad99f2983b1b7a4", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/974c09593f29b89546e751c42ad99f2983b1b7a4", "committedDate": "2020-08-10T09:53:58Z", "message": "Reorder defs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2abf12671472b3a86002c85d797ccea52a6e7f09", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/2abf12671472b3a86002c85d797ccea52a6e7f09", "committedDate": "2020-08-10T10:02:51Z", "message": "Mapping tweaks"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dad98d8abad891f5945a9887c323382769ed1edc", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/dad98d8abad891f5945a9887c323382769ed1edc", "committedDate": "2020-08-11T08:41:22Z", "message": "Merge branch 'master' into poc-blob-cache"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4c8ed359d350767d1123e095adfebe0fed28dad5", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/4c8ed359d350767d1123e095adfebe0fed28dad5", "committedDate": "2020-08-11T09:08:06Z", "message": "No need to index fields, we only get by ID"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4b6798a73a18c6488503d69a643ef661516632ee", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/4b6798a73a18c6488503d69a643ef661516632ee", "committedDate": "2020-08-11T09:29:06Z", "message": "Comment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8a3b1742dda99cef89bc7ce2253a86b9b227f1e5", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/8a3b1742dda99cef89bc7ce2253a86b9b227f1e5", "committedDate": "2020-08-11T10:18:26Z", "message": "Rename"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "80e42ffb0bcc09279332d3b0d5b06a9770dd7041", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/80e42ffb0bcc09279332d3b0d5b06a9770dd7041", "committedDate": "2020-08-11T11:01:41Z", "message": "Assert no more indexing into cache after first startup"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b052d7e1b15a2405e004eb998d866c898755b5d1", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/b052d7e1b15a2405e004eb998d866c898755b5d1", "committedDate": "2020-08-11T16:04:45Z", "message": "Stronger assertions about what is read from the blob store when the cache is in play"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d4919fad9f2748e9e2c6e979baeaba0151e4c8bd", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/d4919fad9f2748e9e2c6e979baeaba0151e4c8bd", "committedDate": "2020-08-11T16:10:14Z", "message": "No need for a fake blob store when we track accesses in the index input stats instead"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d2632ee1a7c5fc973f9155ea127d86a0d214327e", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/d2632ee1a7c5fc973f9155ea127d86a0d214327e", "committedDate": "2020-08-11T16:22:19Z", "message": "Assert no indexing at all after restart"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY1NzEwMTkx", "url": "https://github.com/elastic/elasticsearch/pull/60522#pullrequestreview-465710191", "createdAt": "2020-08-12T08:42:03Z", "commit": {"oid": "d2632ee1a7c5fc973f9155ea127d86a0d214327e"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQwODo0MjowM1rOG_Xqvw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQwOToxMToyNlrOG_YugQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTEwMTI0Nw==", "bodyText": "A similar case could be made for setting doc_values : false. I wonder though if we should just leave these fields indexed for now, giving us maximum flexibility later to query them whichever way we want. The overhead of indexing them should be fairly small.", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r469101247", "createdAt": "2020-08-12T08:42:03Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java", "diffHunk": "@@ -0,0 +1,336 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.ResourceAlreadyExistsException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.NoShardAvailableActionException;\n+import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;\n+import org.elasticsearch.action.get.GetRequest;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.index.IndexRequest;\n+import org.elasticsearch.action.index.IndexResponse;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.ClusterChangedEvent;\n+import org.elasticsearch.cluster.ClusterStateListener;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.xcontent.ToXContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.index.IndexNotFoundException;\n+import org.elasticsearch.rest.RestStatus;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.io.IOException;\n+import java.time.Instant;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;\n+import static org.elasticsearch.index.mapper.MapperService.SINGLE_MAPPING_NAME;\n+import static org.elasticsearch.xpack.core.ClientHelper.SEARCHABLE_SNAPSHOTS_ORIGIN;\n+\n+public class BlobStoreCacheService extends AbstractLifecycleComponent implements ClusterStateListener {\n+\n+    private static final Logger logger = LogManager.getLogger(BlobStoreCacheService.class);\n+\n+    public static final int DEFAULT_SIZE = Math.toIntExact(ByteSizeUnit.KB.toBytes(4L));\n+\n+    private final ClusterService clusterService;\n+    private final ThreadPool threadPool;\n+    private final AtomicBoolean ready;\n+    private final Client client;\n+    private final String index;\n+\n+    public BlobStoreCacheService(ClusterService clusterService, ThreadPool threadPool, Client client, String index) {\n+        this.client = new OriginSettingClient(client, SEARCHABLE_SNAPSHOTS_ORIGIN);\n+        this.ready = new AtomicBoolean(false);\n+        this.clusterService = clusterService;\n+        this.threadPool = threadPool;\n+        this.index = index;\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        clusterService.addListener(this);\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        clusterService.removeListener(this);\n+    }\n+\n+    @Override\n+    protected void doClose() {}\n+\n+    @Override\n+    public void clusterChanged(ClusterChangedEvent event) {\n+        if (lifecycle.started() == false || event.routingTableChanged() == false) {\n+            return;\n+        }\n+        if (event.indexRoutingTableChanged(index)) {\n+            final IndexRoutingTable indexRoutingTable = event.state().routingTable().index(index);\n+            if (indexRoutingTable == null) {\n+                ready.set(false);\n+                return;\n+            }\n+            ready.set(indexRoutingTable.allPrimaryShardsActive());\n+        }\n+    }\n+\n+    private void createIndexIfNecessary(ActionListener<String> listener) {\n+        if (clusterService.state().routingTable().hasIndex(index)) {\n+            listener.onResponse(index);\n+            return;\n+        }\n+        try {\n+            client.admin()\n+                .indices()\n+                .prepareCreate(index)\n+                .setSettings(indexSettings())\n+                .setMapping(mappings())\n+                .execute(new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(CreateIndexResponse createIndexResponse) {\n+                        assert createIndexResponse.index().equals(index);\n+                        listener.onResponse(createIndexResponse.index());\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        if (e instanceof ResourceAlreadyExistsException\n+                            || ExceptionsHelper.unwrapCause(e) instanceof ResourceAlreadyExistsException) {\n+                            listener.onResponse(index);\n+                        } else {\n+                            listener.onFailure(e);\n+                        }\n+                    }\n+                });\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+        }\n+    }\n+\n+    private static Settings indexSettings() {\n+        return Settings.builder()\n+            .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1)\n+            .put(IndexMetadata.SETTING_AUTO_EXPAND_REPLICAS, \"0-1\")\n+            .put(IndexMetadata.SETTING_PRIORITY, \"900\")\n+            .build();\n+    }\n+\n+    private static XContentBuilder mappings() throws IOException {\n+        final XContentBuilder builder = jsonBuilder();\n+        {\n+            builder.startObject();\n+            {\n+                builder.startObject(SINGLE_MAPPING_NAME);\n+                builder.field(\"dynamic\", \"strict\");\n+                {\n+                    builder.startObject(\"_meta\");\n+                    builder.field(\"version\", Version.CURRENT);\n+                    builder.endObject();\n+                }\n+                {\n+                    builder.startObject(\"properties\");\n+                    {\n+                        builder.startObject(\"type\");\n+                        builder.field(\"type\", \"keyword\");\n+                        builder.field(\"index\", \"false\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2632ee1a7c5fc973f9155ea127d86a0d214327e"}, "originalPosition": 154}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTEwMzkyNQ==", "bodyText": "this is no longer needed, right?", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r469103925", "createdAt": "2020-08-12T08:46:31Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java", "diffHunk": "@@ -0,0 +1,465 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.elasticsearch.action.admin.indices.forcemerge.ForceMergeResponse;\n+import org.elasticsearch.action.admin.indices.refresh.RefreshResponse;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.index.IndexRequestBuilder;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.routing.RoutingNode;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;\n+import org.elasticsearch.cluster.routing.allocation.decider.AllocationDecider;\n+import org.elasticsearch.cluster.routing.allocation.decider.Decision;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.ClusterSettings;\n+import org.elasticsearch.common.settings.Setting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.index.IndexNotFoundException;\n+import org.elasticsearch.index.query.QueryBuilders;\n+import org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardSnapshot;\n+import org.elasticsearch.plugins.ClusterPlugin;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.repositories.fs.FsRepository;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.xpack.core.ClientHelper;\n+import org.elasticsearch.xpack.core.searchablesnapshots.SearchableSnapshotShardStats;\n+import org.elasticsearch.xpack.searchablesnapshots.BaseSearchableSnapshotsIntegTestCase;\n+import org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots;\n+import org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants;\n+import org.elasticsearch.xpack.searchablesnapshots.action.SearchableSnapshotsStatsAction;\n+import org.elasticsearch.xpack.searchablesnapshots.action.SearchableSnapshotsStatsRequest;\n+import org.elasticsearch.xpack.searchablesnapshots.cache.CacheService;\n+\n+import java.io.IOException;\n+import java.nio.file.FileVisitResult;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.SimpleFileVisitor;\n+import java.nio.file.attribute.BasicFileAttributes;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.concurrent.ThreadPoolExecutor;\n+\n+import static org.elasticsearch.repositories.blobstore.BlobStoreRepository.INDEX_SHARD_SNAPSHOT_FORMAT;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.CACHE_FETCH_ASYNC_THREAD_POOL_NAME;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.CACHE_PREWARMING_THREAD_POOL_NAME;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.SNAPSHOT_BLOB_CACHE_INDEX;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.is;\n+\n+public class SearchableSnapshotsBlobStoreCacheIntegTests extends BaseSearchableSnapshotsIntegTestCase {\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        final List<Class<? extends Plugin>> plugins = new ArrayList<>();\n+        plugins.add(WaitForSnapshotBlobCacheShardsActivePlugin.class);\n+        plugins.addAll(super.nodePlugins());\n+        return List.copyOf(plugins);\n+    }\n+\n+    @Override\n+    protected int numberOfReplicas() {\n+        return 0;\n+    }\n+\n+    @Override\n+    protected Settings nodeSettings(int nodeOrdinal) {\n+        return Settings.builder()\n+            .put(super.nodeSettings(nodeOrdinal))\n+            .put(\n+                CacheService.SNAPSHOT_CACHE_RANGE_SIZE_SETTING.getKey(),\n+                randomLongBetween(new ByteSizeValue(4, ByteSizeUnit.KB).getBytes(), new ByteSizeValue(20, ByteSizeUnit.KB).getBytes()) + \"b\"\n+            )\n+            .put(CacheService.SNAPSHOT_CACHE_SIZE_SETTING.getKey(), new ByteSizeValue(Long.MAX_VALUE, ByteSizeUnit.BYTES))\n+            .build();\n+    }\n+\n+    public void testBlobStoreCache() throws Exception {\n+        final String indexName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT);\n+        createIndex(indexName);\n+\n+        final List<IndexRequestBuilder> indexRequestBuilders = new ArrayList<>();\n+        for (int i = scaledRandomIntBetween(0, 10_000); i >= 0; i--) {\n+            indexRequestBuilders.add(client().prepareIndex(indexName).setSource(\"text\", randomUnicodeOfLength(10), \"num\", i));\n+        }\n+        indexRandom(true, false, true, indexRequestBuilders);\n+        final long numberOfDocs = indexRequestBuilders.size();\n+        final NumShards numberOfShards = getNumShards(indexName);\n+\n+        if (randomBoolean()) {\n+            logger.info(\"--> force-merging index before snapshotting\");\n+            final ForceMergeResponse forceMergeResponse = client().admin()\n+                .indices()\n+                .prepareForceMerge(indexName)\n+                .setMaxNumSegments(1)\n+                .get();\n+            assertThat(forceMergeResponse.getSuccessfulShards(), equalTo(numberOfShards.totalNumShards));\n+            assertThat(forceMergeResponse.getFailedShards(), equalTo(0));\n+        }\n+\n+        final String repositoryName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT);\n+        final Path repositoryLocation = randomRepoPath();\n+        createFsRepository(repositoryName, repositoryLocation);\n+\n+        final SnapshotId snapshot = createSnapshot(repositoryName, List.of(indexName));\n+        assertAcked(client().admin().indices().prepareDelete(indexName));\n+\n+        // extract the list of blobs per shard from the snapshot directory on disk\n+        final Map<String, BlobStoreIndexShardSnapshot> blobsInSnapshot = blobsInSnapshot(repositoryLocation, snapshot.getUUID());\n+        assertThat(\"Failed to load all shard snapshot metadata files\", blobsInSnapshot.size(), equalTo(numberOfShards.numPrimaries));\n+\n+        // register a new repository that can track blob read operations", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2632ee1a7c5fc973f9155ea127d86a0d214327e"}, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTEwNTkwMQ==", "bodyText": "I wonder if this is a bit brittle perhaps, with these stats being tracked in-memory on the node that is hosting the shard. In particular, it does not survive restarts / relocations, and could also be different on a replica that recovered after the primary already had some docs. I wonder if we should instead query the number of docs in the SNAPSHOT_BLOB_CACHE_INDEX index.", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r469105901", "createdAt": "2020-08-12T08:49:50Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java", "diffHunk": "@@ -0,0 +1,465 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.elasticsearch.action.admin.indices.forcemerge.ForceMergeResponse;\n+import org.elasticsearch.action.admin.indices.refresh.RefreshResponse;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.index.IndexRequestBuilder;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.routing.RoutingNode;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;\n+import org.elasticsearch.cluster.routing.allocation.decider.AllocationDecider;\n+import org.elasticsearch.cluster.routing.allocation.decider.Decision;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.ClusterSettings;\n+import org.elasticsearch.common.settings.Setting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.index.IndexNotFoundException;\n+import org.elasticsearch.index.query.QueryBuilders;\n+import org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardSnapshot;\n+import org.elasticsearch.plugins.ClusterPlugin;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.repositories.fs.FsRepository;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.xpack.core.ClientHelper;\n+import org.elasticsearch.xpack.core.searchablesnapshots.SearchableSnapshotShardStats;\n+import org.elasticsearch.xpack.searchablesnapshots.BaseSearchableSnapshotsIntegTestCase;\n+import org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots;\n+import org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants;\n+import org.elasticsearch.xpack.searchablesnapshots.action.SearchableSnapshotsStatsAction;\n+import org.elasticsearch.xpack.searchablesnapshots.action.SearchableSnapshotsStatsRequest;\n+import org.elasticsearch.xpack.searchablesnapshots.cache.CacheService;\n+\n+import java.io.IOException;\n+import java.nio.file.FileVisitResult;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.SimpleFileVisitor;\n+import java.nio.file.attribute.BasicFileAttributes;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.concurrent.ThreadPoolExecutor;\n+\n+import static org.elasticsearch.repositories.blobstore.BlobStoreRepository.INDEX_SHARD_SNAPSHOT_FORMAT;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.CACHE_FETCH_ASYNC_THREAD_POOL_NAME;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.CACHE_PREWARMING_THREAD_POOL_NAME;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.SNAPSHOT_BLOB_CACHE_INDEX;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.is;\n+\n+public class SearchableSnapshotsBlobStoreCacheIntegTests extends BaseSearchableSnapshotsIntegTestCase {\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        final List<Class<? extends Plugin>> plugins = new ArrayList<>();\n+        plugins.add(WaitForSnapshotBlobCacheShardsActivePlugin.class);\n+        plugins.addAll(super.nodePlugins());\n+        return List.copyOf(plugins);\n+    }\n+\n+    @Override\n+    protected int numberOfReplicas() {\n+        return 0;\n+    }\n+\n+    @Override\n+    protected Settings nodeSettings(int nodeOrdinal) {\n+        return Settings.builder()\n+            .put(super.nodeSettings(nodeOrdinal))\n+            .put(\n+                CacheService.SNAPSHOT_CACHE_RANGE_SIZE_SETTING.getKey(),\n+                randomLongBetween(new ByteSizeValue(4, ByteSizeUnit.KB).getBytes(), new ByteSizeValue(20, ByteSizeUnit.KB).getBytes()) + \"b\"\n+            )\n+            .put(CacheService.SNAPSHOT_CACHE_SIZE_SETTING.getKey(), new ByteSizeValue(Long.MAX_VALUE, ByteSizeUnit.BYTES))\n+            .build();\n+    }\n+\n+    public void testBlobStoreCache() throws Exception {\n+        final String indexName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT);\n+        createIndex(indexName);\n+\n+        final List<IndexRequestBuilder> indexRequestBuilders = new ArrayList<>();\n+        for (int i = scaledRandomIntBetween(0, 10_000); i >= 0; i--) {\n+            indexRequestBuilders.add(client().prepareIndex(indexName).setSource(\"text\", randomUnicodeOfLength(10), \"num\", i));\n+        }\n+        indexRandom(true, false, true, indexRequestBuilders);\n+        final long numberOfDocs = indexRequestBuilders.size();\n+        final NumShards numberOfShards = getNumShards(indexName);\n+\n+        if (randomBoolean()) {\n+            logger.info(\"--> force-merging index before snapshotting\");\n+            final ForceMergeResponse forceMergeResponse = client().admin()\n+                .indices()\n+                .prepareForceMerge(indexName)\n+                .setMaxNumSegments(1)\n+                .get();\n+            assertThat(forceMergeResponse.getSuccessfulShards(), equalTo(numberOfShards.totalNumShards));\n+            assertThat(forceMergeResponse.getFailedShards(), equalTo(0));\n+        }\n+\n+        final String repositoryName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT);\n+        final Path repositoryLocation = randomRepoPath();\n+        createFsRepository(repositoryName, repositoryLocation);\n+\n+        final SnapshotId snapshot = createSnapshot(repositoryName, List.of(indexName));\n+        assertAcked(client().admin().indices().prepareDelete(indexName));\n+\n+        // extract the list of blobs per shard from the snapshot directory on disk\n+        final Map<String, BlobStoreIndexShardSnapshot> blobsInSnapshot = blobsInSnapshot(repositoryLocation, snapshot.getUUID());\n+        assertThat(\"Failed to load all shard snapshot metadata files\", blobsInSnapshot.size(), equalTo(numberOfShards.numPrimaries));\n+\n+        // register a new repository that can track blob read operations\n+        assertAcked(client().admin().cluster().prepareDeleteRepository(repositoryName));\n+        createRepository(\n+            repositoryName,\n+            \"fs\",\n+            Settings.builder().put(FsRepository.LOCATION_SETTING.getKey(), repositoryLocation).build(),\n+            false\n+        );\n+        assertBusy(this::ensureClusterStateConsistency);\n+\n+        expectThrows(\n+            IndexNotFoundException.class,\n+            \".snapshot-blob-cache system index should not be created yet\",\n+            () -> systemClient().admin().indices().prepareGetIndex().addIndices(SNAPSHOT_BLOB_CACHE_INDEX).get()\n+        );\n+\n+        logger.info(\"--> mount snapshot [{}] as an index for the first time\", snapshot);\n+        final String restoredIndex = mountSnapshot(\n+            repositoryName,\n+            snapshot.getName(),\n+            indexName,\n+            Settings.builder()\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_ENABLED_SETTING.getKey(), true)\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_PREWARM_ENABLED_SETTING.getKey(), false)\n+                .build()\n+        );\n+        ensureGreen(restoredIndex);\n+        ensureExecutorsAreIdle();\n+\n+        for (final SearchableSnapshotShardStats shardStats : client().execute(\n+            SearchableSnapshotsStatsAction.INSTANCE,\n+            new SearchableSnapshotsStatsRequest()\n+        ).actionGet().getStats()) {\n+            for (final SearchableSnapshotShardStats.CacheIndexInputStats indexInputStats : shardStats.getStats()) {\n+                assertThat(Strings.toString(indexInputStats), indexInputStats.getBlobStoreBytesRequested().getCount(), greaterThan(0L));\n+            }\n+        }\n+\n+        logger.info(\"--> verifying cached documents in system index [{}]\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        assertCachedBlobsInSystemIndex(repositoryName, blobsInSnapshot);\n+\n+        refreshSystemIndex();\n+        final long numberOfCachedBlobs = systemClient().prepareSearch(SNAPSHOT_BLOB_CACHE_INDEX).get().getHits().getTotalHits().value;\n+        final long numberOfCacheWrites = systemClient().admin()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2632ee1a7c5fc973f9155ea127d86a0d214327e"}, "originalPosition": 175}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTExMDczOA==", "bodyText": "is this still an issue with the latest changes?", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r469110738", "createdAt": "2020-08-12T08:57:40Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/xpack/searchablesnapshots/SearchableSnapshotsIntegTests.java", "diffHunk": "@@ -793,11 +792,12 @@ private void assertSearchableSnapshotStats(String indexName, boolean cacheEnable\n                             equalTo(0L)\n                         );\n                     } else if (nodeIdsWithLargeEnoughCache.contains(stats.getShardRouting().currentNodeId())) {\n-                        assertThat(\n-                            \"Expected at least 1 cache read or write for \" + fileName + \" of shard \" + shardRouting,\n-                            Math.max(indexInputStats.getCachedBytesRead().getCount(), indexInputStats.getCachedBytesWritten().getCount()),\n-                            greaterThan(0L)\n-                        );\n+                        // not necessarily, it may have been entirely in blob cache TODO improve stats to handle this", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2632ee1a7c5fc973f9155ea127d86a0d214327e"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTExODU5Mw==", "bodyText": "Do we still want to keep this check around? Given that we now always open a CacheFile, should we always go with that first, and then make use of this fallback instead?", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r469118593", "createdAt": "2020-08-12T09:11:26Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -136,53 +146,260 @@ protected void readInternal(ByteBuffer b) throws IOException {\n         final long position = getFilePointer() + this.offset;\n         final int length = b.remaining();\n \n-        int totalBytesRead = 0;\n-        while (totalBytesRead < length) {\n-            final long pos = position + totalBytesRead;\n-            final int len = length - totalBytesRead;\n-            int bytesRead = 0;\n-            try {\n-                final CacheFile cacheFile = getCacheFileSafe();\n-                try (Releasable ignored = cacheFile.fileLock()) {\n-                    final Tuple<Long, Long> rangeToWrite = computeRange(pos);\n-                    final Tuple<Long, Long> rangeToRead = Tuple.tuple(pos, Math.min(pos + len, rangeToWrite.v2()));\n-\n-                    bytesRead = cacheFile.fetchAsync(rangeToWrite, rangeToRead, (channel) -> {\n-                        final int read;\n-                        if ((rangeToRead.v2() - rangeToRead.v1()) < b.remaining()) {\n-                            final ByteBuffer duplicate = b.duplicate();\n-                            duplicate.limit(duplicate.position() + Math.toIntExact(rangeToRead.v2() - rangeToRead.v1()));\n-                            read = readCacheFile(channel, pos, duplicate);\n-                            assert duplicate.position() <= b.limit();\n-                            b.position(duplicate.position());\n-                        } else {\n-                            read = readCacheFile(channel, pos, b);\n+        // We can detect that we're going to read the last 16 bytes (that contains the footer checksum) of the file. Such reads are often\n+        // executed when opening a Directory and since we have the checksum in the snapshot metadata we can use it to fill the ByteBuffer.\n+        if (length == CodecUtil.footerLength() && isClone == false && position == fileInfo.length() - length) {\n+            if (readChecksumFromFileInfo(b)) {\n+                logger.trace(\"read footer of file [{}] at position [{}], bypassing all caches\", fileInfo.physicalName(), position);\n+                return;\n+            }\n+            assert b.remaining() == length;\n+        }\n+\n+        logger.trace(\"readInternal: read [{}-{}] ([{}] bytes) from [{}]\", position, position + length, length, this);\n+\n+        final Tuple<Long, Long> indexCacheMiss; // null if not a miss\n+\n+        // We prefer to use the index cache if the recovery is not done yet\n+        if (directory.isRecoveryDone() == false) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2632ee1a7c5fc973f9155ea127d86a0d214327e"}, "originalPosition": 79}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4df37d6ba01603841ff56bc31617b73e21bffa8d", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/4df37d6ba01603841ff56bc31617b73e21bffa8d", "committedDate": "2020-08-14T08:47:28Z", "message": "Merge branch 'master' into poc-blob-cache"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "24f646f8a0ba141ebebefac1cc719ffffdafb895", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/24f646f8a0ba141ebebefac1cc719ffffdafb895", "committedDate": "2020-08-14T11:08:59Z", "message": "Block test until cache fills are complete"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5e752dd926e663253929ce257faf9d64f90e100b", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/5e752dd926e663253929ce257faf9d64f90e100b", "committedDate": "2020-08-14T11:09:54Z", "message": "Add debugging"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5502545ca55441940e0d5961160d2d274deaaaaa", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/5502545ca55441940e0d5961160d2d274deaaaaa", "committedDate": "2020-08-18T11:29:49Z", "message": "Merge branch 'master' into poc-blob-cache"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "61701fef9ed4409cedb48f5911c461cd0d57f421", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/61701fef9ed4409cedb48f5911c461cd0d57f421", "committedDate": "2020-08-18T11:32:34Z", "message": "Revert"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f5948c4dfb43fb280b0897dab9f5d2c944f299a3", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/f5948c4dfb43fb280b0897dab9f5d2c944f299a3", "committedDate": "2020-08-18T11:39:22Z", "message": "Moar revert"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "51a81ac2fe9a57dffc10eab1d801e8384d6ace47", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/51a81ac2fe9a57dffc10eab1d801e8384d6ace47", "committedDate": "2020-08-18T13:01:35Z", "message": "Revert"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7116f9ff990ae3a742cd6141063922c4eaf403df", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/7116f9ff990ae3a742cd6141063922c4eaf403df", "committedDate": "2020-08-18T15:47:44Z", "message": "Revert buffer size increase"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4b01c9a01fad72570499103a843554bf2a2bf360", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/4b01c9a01fad72570499103a843554bf2a2bf360", "committedDate": "2020-08-18T16:01:03Z", "message": "Better assertion"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d3d58cd731d97ec1326f643cb8aacd18ee1d4008", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/d3d58cd731d97ec1326f643cb8aacd18ee1d4008", "committedDate": "2020-08-18T16:12:44Z", "message": "Distinguish definite cache miss from cache-not-ready"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0bcabe126b44146670d7945fc586ef8bd0eb43f2", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/0bcabe126b44146670d7945fc586ef8bd0eb43f2", "committedDate": "2020-08-18T16:14:57Z", "message": "Precommit"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "43f281ae86ec0d731480c5093eb8db079d06e084", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/43f281ae86ec0d731480c5093eb8db079d06e084", "committedDate": "2020-08-18T16:24:36Z", "message": "Test bug"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "78680bebde572afcd0a77b976ef1489c773afc3d", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/78680bebde572afcd0a77b976ef1489c773afc3d", "committedDate": "2020-08-18T16:41:46Z", "message": "Track reads from index cache too"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c50dfa6dbc431bb169b3113d2ba41892d4daf832", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/c50dfa6dbc431bb169b3113d2ba41892d4daf832", "committedDate": "2020-08-19T07:03:12Z", "message": "Retry on INFE"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d6ef9cd76017046a60a2513d784733b866a681ef", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/d6ef9cd76017046a60a2513d784733b866a681ef", "committedDate": "2020-08-20T11:47:09Z", "message": "Always put a missing cache entry even if cache not ready"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "03a575e7c92092925c10a3050ab248a3ab7dfb12", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/03a575e7c92092925c10a3050ab248a3ab7dfb12", "committedDate": "2020-08-20T11:51:25Z", "message": "Merge branch 'master' into poc-blob-cache"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4f9584c07fcc840e4f66505f6173d092de4c2b23", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/4f9584c07fcc840e4f66505f6173d092de4c2b23", "committedDate": "2020-08-20T11:52:55Z", "message": "Let's keep indexing things for now"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cd77a43bce70607c961bbb9fd8e3ce6240da2304", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/cd77a43bce70607c961bbb9fd8e3ce6240da2304", "committedDate": "2020-08-20T15:05:25Z", "message": "Unnecessary repo"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDcyMzIwNjA0", "url": "https://github.com/elastic/elasticsearch/pull/60522#pullrequestreview-472320604", "createdAt": "2020-08-21T08:55:33Z", "commit": {"oid": "cd77a43bce70607c961bbb9fd8e3ce6240da2304"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQwODo1NTozNFrOHEj6XQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQxMDozMzowNVrOHEoQjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU0NDczMw==", "bodyText": "AFAICS, we allocate a 1KB array here, I wonder if we should use ByteArrayDataOutput instead since we know the exact result size? Alternatively limit the size of the allocation.\nMight not be important, at this time I am not sure of the frequency this would be called.", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r474544733", "createdAt": "2020-08-21T08:55:34Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/checksum/ChecksumBlobContainerIndexInput.java", "diffHunk": "@@ -131,14 +131,18 @@ private static void ensureReadOnceChecksumContext(IOContext context) {\n      * @throws IOException if something goes wrong when creating the {@link ChecksumBlobContainerIndexInput}\n      */\n     public static ChecksumBlobContainerIndexInput create(String name, long length, String checksum, IOContext context) throws IOException {\n+        return new ChecksumBlobContainerIndexInput(name, length, checksumToBytesArray(checksum), context);\n+    }\n+\n+    public static byte[] checksumToBytesArray(String checksum) throws IOException {\n         final ByteBuffersDataOutput out = new ByteBuffersDataOutput();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd77a43bce70607c961bbb9fd8e3ce6240da2304"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU1MDMyNA==", "bodyText": "I wonder if this introduces a risk of deadlock if all GET threads end here?", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r474550324", "createdAt": "2020-08-21T09:01:41Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java", "diffHunk": "@@ -0,0 +1,322 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.ResourceAlreadyExistsException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.NoShardAvailableActionException;\n+import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;\n+import org.elasticsearch.action.get.GetRequest;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.index.IndexRequest;\n+import org.elasticsearch.action.index.IndexResponse;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.ClusterChangedEvent;\n+import org.elasticsearch.cluster.ClusterStateListener;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.xcontent.ToXContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.index.IndexNotFoundException;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.io.IOException;\n+import java.time.Instant;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;\n+import static org.elasticsearch.index.mapper.MapperService.SINGLE_MAPPING_NAME;\n+import static org.elasticsearch.xpack.core.ClientHelper.SEARCHABLE_SNAPSHOTS_ORIGIN;\n+\n+public class BlobStoreCacheService extends AbstractLifecycleComponent implements ClusterStateListener {\n+\n+    private static final Logger logger = LogManager.getLogger(BlobStoreCacheService.class);\n+\n+    public static final int DEFAULT_CACHED_BLOB_SIZE = Math.toIntExact(ByteSizeUnit.KB.toBytes(4L));\n+\n+    private final ClusterService clusterService;\n+    private final ThreadPool threadPool;\n+    private final AtomicBoolean ready;\n+    private final Client client;\n+    private final String index;\n+\n+    public BlobStoreCacheService(ClusterService clusterService, ThreadPool threadPool, Client client, String index) {\n+        this.client = new OriginSettingClient(client, SEARCHABLE_SNAPSHOTS_ORIGIN);\n+        this.ready = new AtomicBoolean(false);\n+        this.clusterService = clusterService;\n+        this.threadPool = threadPool;\n+        this.index = index;\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        clusterService.addListener(this);\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        clusterService.removeListener(this);\n+    }\n+\n+    @Override\n+    protected void doClose() {}\n+\n+    @Override\n+    public void clusterChanged(ClusterChangedEvent event) {\n+        if (lifecycle.started() == false || event.routingTableChanged() == false) {\n+            return;\n+        }\n+        if (event.indexRoutingTableChanged(index)) {\n+            final IndexRoutingTable indexRoutingTable = event.state().routingTable().index(index);\n+            if (indexRoutingTable == null) {\n+                ready.set(false);\n+                return;\n+            }\n+            ready.set(indexRoutingTable.allPrimaryShardsActive());\n+        }\n+    }\n+\n+    private void createIndexIfNecessary(ActionListener<String> listener) {\n+        if (clusterService.state().routingTable().hasIndex(index)) {\n+            listener.onResponse(index);\n+            return;\n+        }\n+        try {\n+            client.admin()\n+                .indices()\n+                .prepareCreate(index)\n+                .setSettings(indexSettings())\n+                .setMapping(mappings())\n+                .execute(new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(CreateIndexResponse createIndexResponse) {\n+                        assert createIndexResponse.index().equals(index);\n+                        listener.onResponse(createIndexResponse.index());\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        if (e instanceof ResourceAlreadyExistsException\n+                            || ExceptionsHelper.unwrapCause(e) instanceof ResourceAlreadyExistsException) {\n+                            listener.onResponse(index);\n+                        } else {\n+                            listener.onFailure(e);\n+                        }\n+                    }\n+                });\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+        }\n+    }\n+\n+    private static Settings indexSettings() {\n+        return Settings.builder()\n+            .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1)\n+            .put(IndexMetadata.SETTING_AUTO_EXPAND_REPLICAS, \"0-1\")\n+            .put(IndexMetadata.SETTING_PRIORITY, \"900\")\n+            .build();\n+    }\n+\n+    private static XContentBuilder mappings() throws IOException {\n+        final XContentBuilder builder = jsonBuilder();\n+        {\n+            builder.startObject();\n+            {\n+                builder.startObject(SINGLE_MAPPING_NAME);\n+                builder.field(\"dynamic\", \"strict\");\n+                {\n+                    builder.startObject(\"_meta\");\n+                    builder.field(\"version\", Version.CURRENT);\n+                    builder.endObject();\n+                }\n+                {\n+                    builder.startObject(\"properties\");\n+                    {\n+                        builder.startObject(\"type\");\n+                        builder.field(\"type\", \"keyword\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"creation_time\");\n+                        builder.field(\"type\", \"date\");\n+                        builder.field(\"format\", \"epoch_millis\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"version\");\n+                        builder.field(\"type\", \"integer\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"repository\");\n+                        builder.field(\"type\", \"keyword\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"blob\");\n+                        builder.field(\"type\", \"object\");\n+                        {\n+                            builder.startObject(\"properties\");\n+                            {\n+                                builder.startObject(\"name\");\n+                                builder.field(\"type\", \"keyword\");\n+                                builder.endObject();\n+                                builder.startObject(\"path\");\n+                                builder.field(\"type\", \"keyword\");\n+                                builder.endObject();\n+                            }\n+                            builder.endObject();\n+                        }\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"data\");\n+                        builder.field(\"type\", \"object\");\n+                        {\n+                            builder.startObject(\"properties\");\n+                            {\n+                                builder.startObject(\"content\");\n+                                builder.field(\"type\", \"binary\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"length\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"from\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"to\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            builder.endObject();\n+                        }\n+                        builder.endObject();\n+                    }\n+                    builder.endObject();\n+                }\n+                builder.endObject();\n+            }\n+            builder.endObject();\n+        }\n+        return builder;\n+    }\n+\n+    public CachedBlob get(String repository, String name, String path, long offset) {\n+        final PlainActionFuture<CachedBlob> future = PlainActionFuture.newFuture();\n+        getAsync(repository, name, path, offset, future);\n+        return future.actionGet();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd77a43bce70607c961bbb9fd8e3ce6240da2304"}, "originalPosition": 229}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU1NTkyNw==", "bodyText": "I think we would see a remote wrapper exception here? Perhaps we can use TransportActions.isShardNotAvailableException?", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r474555927", "createdAt": "2020-08-21T09:07:45Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java", "diffHunk": "@@ -0,0 +1,322 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.ResourceAlreadyExistsException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.NoShardAvailableActionException;\n+import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;\n+import org.elasticsearch.action.get.GetRequest;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.index.IndexRequest;\n+import org.elasticsearch.action.index.IndexResponse;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.ClusterChangedEvent;\n+import org.elasticsearch.cluster.ClusterStateListener;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.xcontent.ToXContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.index.IndexNotFoundException;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.io.IOException;\n+import java.time.Instant;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;\n+import static org.elasticsearch.index.mapper.MapperService.SINGLE_MAPPING_NAME;\n+import static org.elasticsearch.xpack.core.ClientHelper.SEARCHABLE_SNAPSHOTS_ORIGIN;\n+\n+public class BlobStoreCacheService extends AbstractLifecycleComponent implements ClusterStateListener {\n+\n+    private static final Logger logger = LogManager.getLogger(BlobStoreCacheService.class);\n+\n+    public static final int DEFAULT_CACHED_BLOB_SIZE = Math.toIntExact(ByteSizeUnit.KB.toBytes(4L));\n+\n+    private final ClusterService clusterService;\n+    private final ThreadPool threadPool;\n+    private final AtomicBoolean ready;\n+    private final Client client;\n+    private final String index;\n+\n+    public BlobStoreCacheService(ClusterService clusterService, ThreadPool threadPool, Client client, String index) {\n+        this.client = new OriginSettingClient(client, SEARCHABLE_SNAPSHOTS_ORIGIN);\n+        this.ready = new AtomicBoolean(false);\n+        this.clusterService = clusterService;\n+        this.threadPool = threadPool;\n+        this.index = index;\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        clusterService.addListener(this);\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        clusterService.removeListener(this);\n+    }\n+\n+    @Override\n+    protected void doClose() {}\n+\n+    @Override\n+    public void clusterChanged(ClusterChangedEvent event) {\n+        if (lifecycle.started() == false || event.routingTableChanged() == false) {\n+            return;\n+        }\n+        if (event.indexRoutingTableChanged(index)) {\n+            final IndexRoutingTable indexRoutingTable = event.state().routingTable().index(index);\n+            if (indexRoutingTable == null) {\n+                ready.set(false);\n+                return;\n+            }\n+            ready.set(indexRoutingTable.allPrimaryShardsActive());\n+        }\n+    }\n+\n+    private void createIndexIfNecessary(ActionListener<String> listener) {\n+        if (clusterService.state().routingTable().hasIndex(index)) {\n+            listener.onResponse(index);\n+            return;\n+        }\n+        try {\n+            client.admin()\n+                .indices()\n+                .prepareCreate(index)\n+                .setSettings(indexSettings())\n+                .setMapping(mappings())\n+                .execute(new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(CreateIndexResponse createIndexResponse) {\n+                        assert createIndexResponse.index().equals(index);\n+                        listener.onResponse(createIndexResponse.index());\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        if (e instanceof ResourceAlreadyExistsException\n+                            || ExceptionsHelper.unwrapCause(e) instanceof ResourceAlreadyExistsException) {\n+                            listener.onResponse(index);\n+                        } else {\n+                            listener.onFailure(e);\n+                        }\n+                    }\n+                });\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+        }\n+    }\n+\n+    private static Settings indexSettings() {\n+        return Settings.builder()\n+            .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1)\n+            .put(IndexMetadata.SETTING_AUTO_EXPAND_REPLICAS, \"0-1\")\n+            .put(IndexMetadata.SETTING_PRIORITY, \"900\")\n+            .build();\n+    }\n+\n+    private static XContentBuilder mappings() throws IOException {\n+        final XContentBuilder builder = jsonBuilder();\n+        {\n+            builder.startObject();\n+            {\n+                builder.startObject(SINGLE_MAPPING_NAME);\n+                builder.field(\"dynamic\", \"strict\");\n+                {\n+                    builder.startObject(\"_meta\");\n+                    builder.field(\"version\", Version.CURRENT);\n+                    builder.endObject();\n+                }\n+                {\n+                    builder.startObject(\"properties\");\n+                    {\n+                        builder.startObject(\"type\");\n+                        builder.field(\"type\", \"keyword\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"creation_time\");\n+                        builder.field(\"type\", \"date\");\n+                        builder.field(\"format\", \"epoch_millis\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"version\");\n+                        builder.field(\"type\", \"integer\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"repository\");\n+                        builder.field(\"type\", \"keyword\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"blob\");\n+                        builder.field(\"type\", \"object\");\n+                        {\n+                            builder.startObject(\"properties\");\n+                            {\n+                                builder.startObject(\"name\");\n+                                builder.field(\"type\", \"keyword\");\n+                                builder.endObject();\n+                                builder.startObject(\"path\");\n+                                builder.field(\"type\", \"keyword\");\n+                                builder.endObject();\n+                            }\n+                            builder.endObject();\n+                        }\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"data\");\n+                        builder.field(\"type\", \"object\");\n+                        {\n+                            builder.startObject(\"properties\");\n+                            {\n+                                builder.startObject(\"content\");\n+                                builder.field(\"type\", \"binary\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"length\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"from\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"to\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            builder.endObject();\n+                        }\n+                        builder.endObject();\n+                    }\n+                    builder.endObject();\n+                }\n+                builder.endObject();\n+            }\n+            builder.endObject();\n+        }\n+        return builder;\n+    }\n+\n+    public CachedBlob get(String repository, String name, String path, long offset) {\n+        final PlainActionFuture<CachedBlob> future = PlainActionFuture.newFuture();\n+        getAsync(repository, name, path, offset, future);\n+        return future.actionGet();\n+    }\n+\n+    protected void getAsync(String repository, String name, String path, long offset, ActionListener<CachedBlob> listener) {\n+        if ((lifecycle.started() && ready.get()) == false) {\n+            // TODO TBD can we just execute the GET request and let it fail if the index isn't ready yet?\n+            // We might get lucky and hit a started shard anyway.\n+            logger.debug(\"not ready : [{}]\", CachedBlob.generateId(repository, name, path, offset));\n+            listener.onResponse(CachedBlob.CACHE_NOT_READY);\n+            return;\n+        }\n+        try {\n+            final GetRequest request = new GetRequest(index).id(CachedBlob.generateId(repository, name, path, offset));\n+            client.get(request, new ActionListener<>() {\n+                @Override\n+                public void onResponse(GetResponse response) {\n+                    if (response.isExists()) {\n+                        logger.debug(\"cache hit : [{}]\", request.id());\n+                        assert response.isSourceEmpty() == false;\n+\n+                        final CachedBlob cachedBlob = CachedBlob.fromSource(response.getSource());\n+                        assert response.getId().equals(cachedBlob.generatedId());\n+                        listener.onResponse(cachedBlob);\n+                    } else {\n+                        logger.debug(\"cache miss: [{}]\", request.id());\n+                        listener.onResponse(CachedBlob.CACHE_MISS);\n+                    }\n+                }\n+\n+                @Override\n+                public void onFailure(Exception e) {\n+                    if (e instanceof IndexNotFoundException || e instanceof NoShardAvailableActionException) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd77a43bce70607c961bbb9fd8e3ce6240da2304"}, "originalPosition": 260}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU1OTg0MQ==", "bodyText": "I am torn on this. If the cause is a bug, we would like to see it to surface it. But in any failure that is not a bug, I think we would like to assume the cache is unavailable?\nFor instance a network issue to the node holding the shard, I think that would fall out here? And in that case, I would prefer to warn and move on.", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r474559841", "createdAt": "2020-08-21T09:12:13Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java", "diffHunk": "@@ -0,0 +1,322 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.ResourceAlreadyExistsException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.NoShardAvailableActionException;\n+import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;\n+import org.elasticsearch.action.get.GetRequest;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.index.IndexRequest;\n+import org.elasticsearch.action.index.IndexResponse;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.ClusterChangedEvent;\n+import org.elasticsearch.cluster.ClusterStateListener;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.xcontent.ToXContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.index.IndexNotFoundException;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.io.IOException;\n+import java.time.Instant;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;\n+import static org.elasticsearch.index.mapper.MapperService.SINGLE_MAPPING_NAME;\n+import static org.elasticsearch.xpack.core.ClientHelper.SEARCHABLE_SNAPSHOTS_ORIGIN;\n+\n+public class BlobStoreCacheService extends AbstractLifecycleComponent implements ClusterStateListener {\n+\n+    private static final Logger logger = LogManager.getLogger(BlobStoreCacheService.class);\n+\n+    public static final int DEFAULT_CACHED_BLOB_SIZE = Math.toIntExact(ByteSizeUnit.KB.toBytes(4L));\n+\n+    private final ClusterService clusterService;\n+    private final ThreadPool threadPool;\n+    private final AtomicBoolean ready;\n+    private final Client client;\n+    private final String index;\n+\n+    public BlobStoreCacheService(ClusterService clusterService, ThreadPool threadPool, Client client, String index) {\n+        this.client = new OriginSettingClient(client, SEARCHABLE_SNAPSHOTS_ORIGIN);\n+        this.ready = new AtomicBoolean(false);\n+        this.clusterService = clusterService;\n+        this.threadPool = threadPool;\n+        this.index = index;\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        clusterService.addListener(this);\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        clusterService.removeListener(this);\n+    }\n+\n+    @Override\n+    protected void doClose() {}\n+\n+    @Override\n+    public void clusterChanged(ClusterChangedEvent event) {\n+        if (lifecycle.started() == false || event.routingTableChanged() == false) {\n+            return;\n+        }\n+        if (event.indexRoutingTableChanged(index)) {\n+            final IndexRoutingTable indexRoutingTable = event.state().routingTable().index(index);\n+            if (indexRoutingTable == null) {\n+                ready.set(false);\n+                return;\n+            }\n+            ready.set(indexRoutingTable.allPrimaryShardsActive());\n+        }\n+    }\n+\n+    private void createIndexIfNecessary(ActionListener<String> listener) {\n+        if (clusterService.state().routingTable().hasIndex(index)) {\n+            listener.onResponse(index);\n+            return;\n+        }\n+        try {\n+            client.admin()\n+                .indices()\n+                .prepareCreate(index)\n+                .setSettings(indexSettings())\n+                .setMapping(mappings())\n+                .execute(new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(CreateIndexResponse createIndexResponse) {\n+                        assert createIndexResponse.index().equals(index);\n+                        listener.onResponse(createIndexResponse.index());\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        if (e instanceof ResourceAlreadyExistsException\n+                            || ExceptionsHelper.unwrapCause(e) instanceof ResourceAlreadyExistsException) {\n+                            listener.onResponse(index);\n+                        } else {\n+                            listener.onFailure(e);\n+                        }\n+                    }\n+                });\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+        }\n+    }\n+\n+    private static Settings indexSettings() {\n+        return Settings.builder()\n+            .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1)\n+            .put(IndexMetadata.SETTING_AUTO_EXPAND_REPLICAS, \"0-1\")\n+            .put(IndexMetadata.SETTING_PRIORITY, \"900\")\n+            .build();\n+    }\n+\n+    private static XContentBuilder mappings() throws IOException {\n+        final XContentBuilder builder = jsonBuilder();\n+        {\n+            builder.startObject();\n+            {\n+                builder.startObject(SINGLE_MAPPING_NAME);\n+                builder.field(\"dynamic\", \"strict\");\n+                {\n+                    builder.startObject(\"_meta\");\n+                    builder.field(\"version\", Version.CURRENT);\n+                    builder.endObject();\n+                }\n+                {\n+                    builder.startObject(\"properties\");\n+                    {\n+                        builder.startObject(\"type\");\n+                        builder.field(\"type\", \"keyword\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"creation_time\");\n+                        builder.field(\"type\", \"date\");\n+                        builder.field(\"format\", \"epoch_millis\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"version\");\n+                        builder.field(\"type\", \"integer\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"repository\");\n+                        builder.field(\"type\", \"keyword\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"blob\");\n+                        builder.field(\"type\", \"object\");\n+                        {\n+                            builder.startObject(\"properties\");\n+                            {\n+                                builder.startObject(\"name\");\n+                                builder.field(\"type\", \"keyword\");\n+                                builder.endObject();\n+                                builder.startObject(\"path\");\n+                                builder.field(\"type\", \"keyword\");\n+                                builder.endObject();\n+                            }\n+                            builder.endObject();\n+                        }\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"data\");\n+                        builder.field(\"type\", \"object\");\n+                        {\n+                            builder.startObject(\"properties\");\n+                            {\n+                                builder.startObject(\"content\");\n+                                builder.field(\"type\", \"binary\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"length\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"from\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"to\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            builder.endObject();\n+                        }\n+                        builder.endObject();\n+                    }\n+                    builder.endObject();\n+                }\n+                builder.endObject();\n+            }\n+            builder.endObject();\n+        }\n+        return builder;\n+    }\n+\n+    public CachedBlob get(String repository, String name, String path, long offset) {\n+        final PlainActionFuture<CachedBlob> future = PlainActionFuture.newFuture();\n+        getAsync(repository, name, path, offset, future);\n+        return future.actionGet();\n+    }\n+\n+    protected void getAsync(String repository, String name, String path, long offset, ActionListener<CachedBlob> listener) {\n+        if ((lifecycle.started() && ready.get()) == false) {\n+            // TODO TBD can we just execute the GET request and let it fail if the index isn't ready yet?\n+            // We might get lucky and hit a started shard anyway.\n+            logger.debug(\"not ready : [{}]\", CachedBlob.generateId(repository, name, path, offset));\n+            listener.onResponse(CachedBlob.CACHE_NOT_READY);\n+            return;\n+        }\n+        try {\n+            final GetRequest request = new GetRequest(index).id(CachedBlob.generateId(repository, name, path, offset));\n+            client.get(request, new ActionListener<>() {\n+                @Override\n+                public void onResponse(GetResponse response) {\n+                    if (response.isExists()) {\n+                        logger.debug(\"cache hit : [{}]\", request.id());\n+                        assert response.isSourceEmpty() == false;\n+\n+                        final CachedBlob cachedBlob = CachedBlob.fromSource(response.getSource());\n+                        assert response.getId().equals(cachedBlob.generatedId());\n+                        listener.onResponse(cachedBlob);\n+                    } else {\n+                        logger.debug(\"cache miss: [{}]\", request.id());\n+                        listener.onResponse(CachedBlob.CACHE_MISS);\n+                    }\n+                }\n+\n+                @Override\n+                public void onFailure(Exception e) {\n+                    if (e instanceof IndexNotFoundException || e instanceof NoShardAvailableActionException) {\n+                        // In case the blob cache system index got unavailable, we pretend we didn't find a cache entry and we move on.\n+                        // Failing here might bubble up the exception and fail the searchable snapshot shard which is potentially\n+                        // recovering.\n+                        logger.debug(() -> new ParameterizedMessage(\"failed to retrieve cached blob from system index [{}]\", index), e);\n+                        listener.onResponse(CachedBlob.CACHE_NOT_READY);\n+                    } else {\n+                        logger.warn(() -> new ParameterizedMessage(\"failed to retrieve cached blob from system index [{}]\", index), e);\n+                        listener.onFailure(e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd77a43bce70607c961bbb9fd8e3ce6240da2304"}, "originalPosition": 268}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU4MTkyMA==", "bodyText": "Should we also stat the time spent reading from the cache?", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r474581920", "createdAt": "2020-08-21T09:36:41Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -136,53 +148,276 @@ protected void readInternal(ByteBuffer b) throws IOException {\n         final long position = getFilePointer() + this.offset;\n         final int length = b.remaining();\n \n-        int totalBytesRead = 0;\n-        while (totalBytesRead < length) {\n-            final long pos = position + totalBytesRead;\n-            final int len = length - totalBytesRead;\n-            int bytesRead = 0;\n-            try {\n-                final CacheFile cacheFile = getCacheFileSafe();\n-                try (Releasable ignored = cacheFile.fileLock()) {\n-                    final Tuple<Long, Long> rangeToWrite = computeRange(pos);\n-                    final Tuple<Long, Long> rangeToRead = Tuple.tuple(pos, Math.min(pos + len, rangeToWrite.v2()));\n-\n-                    bytesRead = cacheFile.fetchAsync(rangeToWrite, rangeToRead, (channel) -> {\n-                        final int read;\n-                        if ((rangeToRead.v2() - rangeToRead.v1()) < b.remaining()) {\n-                            final ByteBuffer duplicate = b.duplicate();\n-                            duplicate.limit(duplicate.position() + Math.toIntExact(rangeToRead.v2() - rangeToRead.v1()));\n-                            read = readCacheFile(channel, pos, duplicate);\n-                            assert duplicate.position() <= b.limit();\n-                            b.position(duplicate.position());\n-                        } else {\n-                            read = readCacheFile(channel, pos, b);\n+        // We can detect that we're going to read the last 16 bytes (that contains the footer checksum) of the file. Such reads are often\n+        // executed when opening a Directory and since we have the checksum in the snapshot metadata we can use it to fill the ByteBuffer.\n+        if (length == CodecUtil.footerLength() && isClone == false && position == fileInfo.length() - length) {\n+            if (readChecksumFromFileInfo(b)) {\n+                logger.trace(\"read footer of file [{}] at position [{}], bypassing all caches\", fileInfo.physicalName(), position);\n+                return;\n+            }\n+            assert b.remaining() == length;\n+        }\n+\n+        logger.trace(\"readInternal: read [{}-{}] ([{}] bytes) from [{}]\", position, position + length, length, this);\n+\n+        final Tuple<Long, Long> indexCacheMiss; // null if not a miss\n+\n+        // We prefer to use the index cache if the recovery is not done yet\n+        if (directory.isRecoveryDone() == false) {\n+            // We try to use the snapshot blob cache if:\n+            // - the file is small enough to be fully cached in the blob cache\n+            final boolean canBeFullyCached = fileInfo.length() <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE * 2;\n+            // - we're reading the first N bytes of the file\n+            final boolean isStartOfFile = (position + length <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE);\n+\n+            if (canBeFullyCached || isStartOfFile) {\n+                final CachedBlob cachedBlob = directory.getCachedBlob(fileInfo.physicalName(), 0L, length);\n+\n+                if (cachedBlob == CachedBlob.CACHE_MISS || cachedBlob == CachedBlob.CACHE_NOT_READY) {\n+                    // We would have liked to find a cached entry but we did not find anything: the cache on the disk will be requested so\n+                    // we compute the regions of the file we would like to have the next time. The regions are expressed as tuples of\n+                    // {start, end} ranges where positions are relative to the whole file.\n+                    if (canBeFullyCached) {\n+                        // if the index input is smaller than twice the size of the blob cache, it will be fully indexed\n+                        indexCacheMiss = Tuple.tuple(0L, fileInfo.length());\n+                    } else {\n+                        // the index input is too large to fully cache, so just cache the initial range\n+                        indexCacheMiss = Tuple.tuple(0L, (long) BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE);\n+                    }\n+\n+                    // We must fill in a cache miss even if CACHE_NOT_READY since the cache index is only created on the first put.\n+                    // TODO TBD use a different trigger for creating the cache index and avoid a put in the CACHE_NOT_READY case.\n+                } else {\n+                    logger.trace(\n+                        \"reading [{}] bytes of file [{}] at position [{}] using index cache\",\n+                        length,\n+                        fileInfo.physicalName(),\n+                        position\n+                    );\n+                    stats.addIndexCacheBytesRead(cachedBlob.length());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd77a43bce70607c961bbb9fd8e3ce6240da2304"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDYxNTk1MA==", "bodyText": "I think we need to delegate this failure to the listener too to decrement the current index cache fills stats (and to keep the principle of listener always being notified).", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r474615950", "createdAt": "2020-08-21T10:33:05Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java", "diffHunk": "@@ -0,0 +1,322 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.ResourceAlreadyExistsException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.NoShardAvailableActionException;\n+import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;\n+import org.elasticsearch.action.get.GetRequest;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.index.IndexRequest;\n+import org.elasticsearch.action.index.IndexResponse;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.ClusterChangedEvent;\n+import org.elasticsearch.cluster.ClusterStateListener;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.xcontent.ToXContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.index.IndexNotFoundException;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.io.IOException;\n+import java.time.Instant;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;\n+import static org.elasticsearch.index.mapper.MapperService.SINGLE_MAPPING_NAME;\n+import static org.elasticsearch.xpack.core.ClientHelper.SEARCHABLE_SNAPSHOTS_ORIGIN;\n+\n+public class BlobStoreCacheService extends AbstractLifecycleComponent implements ClusterStateListener {\n+\n+    private static final Logger logger = LogManager.getLogger(BlobStoreCacheService.class);\n+\n+    public static final int DEFAULT_CACHED_BLOB_SIZE = Math.toIntExact(ByteSizeUnit.KB.toBytes(4L));\n+\n+    private final ClusterService clusterService;\n+    private final ThreadPool threadPool;\n+    private final AtomicBoolean ready;\n+    private final Client client;\n+    private final String index;\n+\n+    public BlobStoreCacheService(ClusterService clusterService, ThreadPool threadPool, Client client, String index) {\n+        this.client = new OriginSettingClient(client, SEARCHABLE_SNAPSHOTS_ORIGIN);\n+        this.ready = new AtomicBoolean(false);\n+        this.clusterService = clusterService;\n+        this.threadPool = threadPool;\n+        this.index = index;\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        clusterService.addListener(this);\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        clusterService.removeListener(this);\n+    }\n+\n+    @Override\n+    protected void doClose() {}\n+\n+    @Override\n+    public void clusterChanged(ClusterChangedEvent event) {\n+        if (lifecycle.started() == false || event.routingTableChanged() == false) {\n+            return;\n+        }\n+        if (event.indexRoutingTableChanged(index)) {\n+            final IndexRoutingTable indexRoutingTable = event.state().routingTable().index(index);\n+            if (indexRoutingTable == null) {\n+                ready.set(false);\n+                return;\n+            }\n+            ready.set(indexRoutingTable.allPrimaryShardsActive());\n+        }\n+    }\n+\n+    private void createIndexIfNecessary(ActionListener<String> listener) {\n+        if (clusterService.state().routingTable().hasIndex(index)) {\n+            listener.onResponse(index);\n+            return;\n+        }\n+        try {\n+            client.admin()\n+                .indices()\n+                .prepareCreate(index)\n+                .setSettings(indexSettings())\n+                .setMapping(mappings())\n+                .execute(new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(CreateIndexResponse createIndexResponse) {\n+                        assert createIndexResponse.index().equals(index);\n+                        listener.onResponse(createIndexResponse.index());\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        if (e instanceof ResourceAlreadyExistsException\n+                            || ExceptionsHelper.unwrapCause(e) instanceof ResourceAlreadyExistsException) {\n+                            listener.onResponse(index);\n+                        } else {\n+                            listener.onFailure(e);\n+                        }\n+                    }\n+                });\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+        }\n+    }\n+\n+    private static Settings indexSettings() {\n+        return Settings.builder()\n+            .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1)\n+            .put(IndexMetadata.SETTING_AUTO_EXPAND_REPLICAS, \"0-1\")\n+            .put(IndexMetadata.SETTING_PRIORITY, \"900\")\n+            .build();\n+    }\n+\n+    private static XContentBuilder mappings() throws IOException {\n+        final XContentBuilder builder = jsonBuilder();\n+        {\n+            builder.startObject();\n+            {\n+                builder.startObject(SINGLE_MAPPING_NAME);\n+                builder.field(\"dynamic\", \"strict\");\n+                {\n+                    builder.startObject(\"_meta\");\n+                    builder.field(\"version\", Version.CURRENT);\n+                    builder.endObject();\n+                }\n+                {\n+                    builder.startObject(\"properties\");\n+                    {\n+                        builder.startObject(\"type\");\n+                        builder.field(\"type\", \"keyword\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"creation_time\");\n+                        builder.field(\"type\", \"date\");\n+                        builder.field(\"format\", \"epoch_millis\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"version\");\n+                        builder.field(\"type\", \"integer\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"repository\");\n+                        builder.field(\"type\", \"keyword\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"blob\");\n+                        builder.field(\"type\", \"object\");\n+                        {\n+                            builder.startObject(\"properties\");\n+                            {\n+                                builder.startObject(\"name\");\n+                                builder.field(\"type\", \"keyword\");\n+                                builder.endObject();\n+                                builder.startObject(\"path\");\n+                                builder.field(\"type\", \"keyword\");\n+                                builder.endObject();\n+                            }\n+                            builder.endObject();\n+                        }\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"data\");\n+                        builder.field(\"type\", \"object\");\n+                        {\n+                            builder.startObject(\"properties\");\n+                            {\n+                                builder.startObject(\"content\");\n+                                builder.field(\"type\", \"binary\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"length\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"from\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"to\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            builder.endObject();\n+                        }\n+                        builder.endObject();\n+                    }\n+                    builder.endObject();\n+                }\n+                builder.endObject();\n+            }\n+            builder.endObject();\n+        }\n+        return builder;\n+    }\n+\n+    public CachedBlob get(String repository, String name, String path, long offset) {\n+        final PlainActionFuture<CachedBlob> future = PlainActionFuture.newFuture();\n+        getAsync(repository, name, path, offset, future);\n+        return future.actionGet();\n+    }\n+\n+    protected void getAsync(String repository, String name, String path, long offset, ActionListener<CachedBlob> listener) {\n+        if ((lifecycle.started() && ready.get()) == false) {\n+            // TODO TBD can we just execute the GET request and let it fail if the index isn't ready yet?\n+            // We might get lucky and hit a started shard anyway.\n+            logger.debug(\"not ready : [{}]\", CachedBlob.generateId(repository, name, path, offset));\n+            listener.onResponse(CachedBlob.CACHE_NOT_READY);\n+            return;\n+        }\n+        try {\n+            final GetRequest request = new GetRequest(index).id(CachedBlob.generateId(repository, name, path, offset));\n+            client.get(request, new ActionListener<>() {\n+                @Override\n+                public void onResponse(GetResponse response) {\n+                    if (response.isExists()) {\n+                        logger.debug(\"cache hit : [{}]\", request.id());\n+                        assert response.isSourceEmpty() == false;\n+\n+                        final CachedBlob cachedBlob = CachedBlob.fromSource(response.getSource());\n+                        assert response.getId().equals(cachedBlob.generatedId());\n+                        listener.onResponse(cachedBlob);\n+                    } else {\n+                        logger.debug(\"cache miss: [{}]\", request.id());\n+                        listener.onResponse(CachedBlob.CACHE_MISS);\n+                    }\n+                }\n+\n+                @Override\n+                public void onFailure(Exception e) {\n+                    if (e instanceof IndexNotFoundException || e instanceof NoShardAvailableActionException) {\n+                        // In case the blob cache system index got unavailable, we pretend we didn't find a cache entry and we move on.\n+                        // Failing here might bubble up the exception and fail the searchable snapshot shard which is potentially\n+                        // recovering.\n+                        logger.debug(() -> new ParameterizedMessage(\"failed to retrieve cached blob from system index [{}]\", index), e);\n+                        listener.onResponse(CachedBlob.CACHE_NOT_READY);\n+                    } else {\n+                        logger.warn(() -> new ParameterizedMessage(\"failed to retrieve cached blob from system index [{}]\", index), e);\n+                        listener.onFailure(e);\n+                    }\n+                }\n+            });\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+        }\n+    }\n+\n+    public void putAsync(String repository, String name, String path, long offset, BytesReference content, ActionListener<Void> listener) {\n+        createIndexIfNecessary(new ActionListener<>() {\n+            @Override\n+            public void onResponse(String s) {\n+                try {\n+                    final CachedBlob cachedBlob = new CachedBlob(\n+                        Instant.ofEpochMilli(threadPool.absoluteTimeInMillis()),\n+                        Version.CURRENT,\n+                        repository,\n+                        name,\n+                        path,\n+                        content,\n+                        offset\n+                    );\n+                    final IndexRequest request = new IndexRequest(index).id(cachedBlob.generatedId());\n+                    try (XContentBuilder builder = jsonBuilder()) {\n+                        request.source(cachedBlob.toXContent(builder, ToXContent.EMPTY_PARAMS));\n+                    }\n+                    client.index(request, new ActionListener<>() {\n+                        @Override\n+                        public void onResponse(IndexResponse indexResponse) {\n+                            logger.trace(\"cache fill ({}): [{}]\", indexResponse.status(), request.id());\n+                            listener.onResponse(null);\n+                        }\n+\n+                        @Override\n+                        public void onFailure(Exception e) {\n+                            logger.debug(new ParameterizedMessage(\"failure in cache fill: [{}]\", request.id()), e);\n+                            listener.onFailure(e);\n+                        }\n+                    });\n+                } catch (IOException e) {\n+                    logger.warn(\n+                        new ParameterizedMessage(\"cache fill failure: [{}]\", CachedBlob.generateId(repository, name, path, offset)),\n+                        e\n+                    );\n+                }\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.error(() -> new ParameterizedMessage(\"failed to create blob cache system index [{}]\", index), e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd77a43bce70607c961bbb9fd8e3ce6240da2304"}, "originalPosition": 318}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDcyMzAwMTAz", "url": "https://github.com/elastic/elasticsearch/pull/60522#pullrequestreview-472300103", "createdAt": "2020-08-21T08:25:08Z", "commit": {"oid": "cd77a43bce70607c961bbb9fd8e3ce6240da2304"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQwODoyNTowOFrOHEiNgw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQxMTozMzozN1rOHEpyCw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDUxNjg2Nw==", "bodyText": "nit: inline in the previous assertion?", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r474516867", "createdAt": "2020-08-21T08:25:08Z", "author": {"login": "fcofdez"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -585,6 +614,17 @@ public static SearchableSnapshotDirectory unwrapDirectory(Directory dir) {\n         return null;\n     }\n \n+    public CachedBlob getCachedBlob(String name, long offset, int length) {\n+        final CachedBlob cachedBlob = blobStoreCacheService.get(repository, name, blobStoreCachePath, offset);\n+        assert cachedBlob == CachedBlob.CACHE_MISS || cachedBlob == CachedBlob.CACHE_NOT_READY || cachedBlob.from() <= offset;\n+        assert cachedBlob == CachedBlob.CACHE_MISS || cachedBlob == CachedBlob.CACHE_NOT_READY || offset + length <= cachedBlob.to();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd77a43bce70607c961bbb9fd8e3ce6240da2304"}, "originalPosition": 136}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDUxOTM3Ng==", "bodyText": "I guess that it is unlikely CodecUtil.footerLength() will change in future versions, right?", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r474519376", "createdAt": "2020-08-21T08:28:03Z", "author": {"login": "fcofdez"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -136,53 +148,276 @@ protected void readInternal(ByteBuffer b) throws IOException {\n         final long position = getFilePointer() + this.offset;\n         final int length = b.remaining();\n \n-        int totalBytesRead = 0;\n-        while (totalBytesRead < length) {\n-            final long pos = position + totalBytesRead;\n-            final int len = length - totalBytesRead;\n-            int bytesRead = 0;\n-            try {\n-                final CacheFile cacheFile = getCacheFileSafe();\n-                try (Releasable ignored = cacheFile.fileLock()) {\n-                    final Tuple<Long, Long> rangeToWrite = computeRange(pos);\n-                    final Tuple<Long, Long> rangeToRead = Tuple.tuple(pos, Math.min(pos + len, rangeToWrite.v2()));\n-\n-                    bytesRead = cacheFile.fetchAsync(rangeToWrite, rangeToRead, (channel) -> {\n-                        final int read;\n-                        if ((rangeToRead.v2() - rangeToRead.v1()) < b.remaining()) {\n-                            final ByteBuffer duplicate = b.duplicate();\n-                            duplicate.limit(duplicate.position() + Math.toIntExact(rangeToRead.v2() - rangeToRead.v1()));\n-                            read = readCacheFile(channel, pos, duplicate);\n-                            assert duplicate.position() <= b.limit();\n-                            b.position(duplicate.position());\n-                        } else {\n-                            read = readCacheFile(channel, pos, b);\n+        // We can detect that we're going to read the last 16 bytes (that contains the footer checksum) of the file. Such reads are often\n+        // executed when opening a Directory and since we have the checksum in the snapshot metadata we can use it to fill the ByteBuffer.\n+        if (length == CodecUtil.footerLength() && isClone == false && position == fileInfo.length() - length) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd77a43bce70607c961bbb9fd8e3ce6240da2304"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU5MDc2OA==", "bodyText": "I think we aren't handling the exceptions for this call, is that ok? maybe we should treat them as a CACHE_NOT_READY?", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r474590768", "createdAt": "2020-08-21T09:46:20Z", "author": {"login": "fcofdez"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -136,53 +148,276 @@ protected void readInternal(ByteBuffer b) throws IOException {\n         final long position = getFilePointer() + this.offset;\n         final int length = b.remaining();\n \n-        int totalBytesRead = 0;\n-        while (totalBytesRead < length) {\n-            final long pos = position + totalBytesRead;\n-            final int len = length - totalBytesRead;\n-            int bytesRead = 0;\n-            try {\n-                final CacheFile cacheFile = getCacheFileSafe();\n-                try (Releasable ignored = cacheFile.fileLock()) {\n-                    final Tuple<Long, Long> rangeToWrite = computeRange(pos);\n-                    final Tuple<Long, Long> rangeToRead = Tuple.tuple(pos, Math.min(pos + len, rangeToWrite.v2()));\n-\n-                    bytesRead = cacheFile.fetchAsync(rangeToWrite, rangeToRead, (channel) -> {\n-                        final int read;\n-                        if ((rangeToRead.v2() - rangeToRead.v1()) < b.remaining()) {\n-                            final ByteBuffer duplicate = b.duplicate();\n-                            duplicate.limit(duplicate.position() + Math.toIntExact(rangeToRead.v2() - rangeToRead.v1()));\n-                            read = readCacheFile(channel, pos, duplicate);\n-                            assert duplicate.position() <= b.limit();\n-                            b.position(duplicate.position());\n-                        } else {\n-                            read = readCacheFile(channel, pos, b);\n+        // We can detect that we're going to read the last 16 bytes (that contains the footer checksum) of the file. Such reads are often\n+        // executed when opening a Directory and since we have the checksum in the snapshot metadata we can use it to fill the ByteBuffer.\n+        if (length == CodecUtil.footerLength() && isClone == false && position == fileInfo.length() - length) {\n+            if (readChecksumFromFileInfo(b)) {\n+                logger.trace(\"read footer of file [{}] at position [{}], bypassing all caches\", fileInfo.physicalName(), position);\n+                return;\n+            }\n+            assert b.remaining() == length;\n+        }\n+\n+        logger.trace(\"readInternal: read [{}-{}] ([{}] bytes) from [{}]\", position, position + length, length, this);\n+\n+        final Tuple<Long, Long> indexCacheMiss; // null if not a miss\n+\n+        // We prefer to use the index cache if the recovery is not done yet\n+        if (directory.isRecoveryDone() == false) {\n+            // We try to use the snapshot blob cache if:\n+            // - the file is small enough to be fully cached in the blob cache\n+            final boolean canBeFullyCached = fileInfo.length() <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE * 2;\n+            // - we're reading the first N bytes of the file\n+            final boolean isStartOfFile = (position + length <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE);\n+\n+            if (canBeFullyCached || isStartOfFile) {\n+                final CachedBlob cachedBlob = directory.getCachedBlob(fileInfo.physicalName(), 0L, length);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd77a43bce70607c961bbb9fd8e3ce6240da2304"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU5MTQ1Nw==", "bodyText": "Currently If the pre-warm phase fails, recovery won't move to DONE, I guess that's ok?", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r474591457", "createdAt": "2020-08-21T09:47:09Z", "author": {"login": "fcofdez"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -136,53 +148,276 @@ protected void readInternal(ByteBuffer b) throws IOException {\n         final long position = getFilePointer() + this.offset;\n         final int length = b.remaining();\n \n-        int totalBytesRead = 0;\n-        while (totalBytesRead < length) {\n-            final long pos = position + totalBytesRead;\n-            final int len = length - totalBytesRead;\n-            int bytesRead = 0;\n-            try {\n-                final CacheFile cacheFile = getCacheFileSafe();\n-                try (Releasable ignored = cacheFile.fileLock()) {\n-                    final Tuple<Long, Long> rangeToWrite = computeRange(pos);\n-                    final Tuple<Long, Long> rangeToRead = Tuple.tuple(pos, Math.min(pos + len, rangeToWrite.v2()));\n-\n-                    bytesRead = cacheFile.fetchAsync(rangeToWrite, rangeToRead, (channel) -> {\n-                        final int read;\n-                        if ((rangeToRead.v2() - rangeToRead.v1()) < b.remaining()) {\n-                            final ByteBuffer duplicate = b.duplicate();\n-                            duplicate.limit(duplicate.position() + Math.toIntExact(rangeToRead.v2() - rangeToRead.v1()));\n-                            read = readCacheFile(channel, pos, duplicate);\n-                            assert duplicate.position() <= b.limit();\n-                            b.position(duplicate.position());\n-                        } else {\n-                            read = readCacheFile(channel, pos, b);\n+        // We can detect that we're going to read the last 16 bytes (that contains the footer checksum) of the file. Such reads are often\n+        // executed when opening a Directory and since we have the checksum in the snapshot metadata we can use it to fill the ByteBuffer.\n+        if (length == CodecUtil.footerLength() && isClone == false && position == fileInfo.length() - length) {\n+            if (readChecksumFromFileInfo(b)) {\n+                logger.trace(\"read footer of file [{}] at position [{}], bypassing all caches\", fileInfo.physicalName(), position);\n+                return;\n+            }\n+            assert b.remaining() == length;\n+        }\n+\n+        logger.trace(\"readInternal: read [{}-{}] ([{}] bytes) from [{}]\", position, position + length, length, this);\n+\n+        final Tuple<Long, Long> indexCacheMiss; // null if not a miss\n+\n+        // We prefer to use the index cache if the recovery is not done yet\n+        if (directory.isRecoveryDone() == false) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd77a43bce70607c961bbb9fd8e3ce6240da2304"}, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDYzNzk1NA==", "bodyText": "This can be prone to races, as we add pre-warm tasks one by one. I've been hit by this in the past \ud83d\ude05 .", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r474637954", "createdAt": "2020-08-21T11:26:35Z", "author": {"login": "fcofdez"}, "path": "x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java", "diffHunk": "@@ -0,0 +1,473 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.elasticsearch.action.admin.indices.forcemerge.ForceMergeResponse;\n+import org.elasticsearch.action.admin.indices.refresh.RefreshResponse;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.index.IndexRequestBuilder;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.routing.RoutingNode;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;\n+import org.elasticsearch.cluster.routing.allocation.decider.AllocationDecider;\n+import org.elasticsearch.cluster.routing.allocation.decider.Decision;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.ClusterSettings;\n+import org.elasticsearch.common.settings.Setting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.index.IndexNotFoundException;\n+import org.elasticsearch.index.query.QueryBuilders;\n+import org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardSnapshot;\n+import org.elasticsearch.plugins.ClusterPlugin;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.junit.annotations.TestLogging;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.xpack.core.ClientHelper;\n+import org.elasticsearch.xpack.core.searchablesnapshots.SearchableSnapshotShardStats;\n+import org.elasticsearch.xpack.searchablesnapshots.BaseSearchableSnapshotsIntegTestCase;\n+import org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots;\n+import org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants;\n+import org.elasticsearch.xpack.searchablesnapshots.action.SearchableSnapshotsStatsAction;\n+import org.elasticsearch.xpack.searchablesnapshots.action.SearchableSnapshotsStatsRequest;\n+import org.elasticsearch.xpack.searchablesnapshots.cache.CacheService;\n+\n+import java.io.IOException;\n+import java.nio.file.FileVisitResult;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.SimpleFileVisitor;\n+import java.nio.file.attribute.BasicFileAttributes;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.concurrent.ThreadPoolExecutor;\n+\n+import static org.elasticsearch.repositories.blobstore.BlobStoreRepository.INDEX_SHARD_SNAPSHOT_FORMAT;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.CACHE_FETCH_ASYNC_THREAD_POOL_NAME;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.CACHE_PREWARMING_THREAD_POOL_NAME;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.SNAPSHOT_BLOB_CACHE_INDEX;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.is;\n+\n+@TestLogging(reason = \"debugging\", value = \"org.elasticsearch.index.store.cache.CachedBlobContainerIndexInput:TRACE,\"\n+    + \"org.elasticsearch.blobstore.cache.BlobStoreCacheService:TRACE\") // TODO remove this before merge\n+public class SearchableSnapshotsBlobStoreCacheIntegTests extends BaseSearchableSnapshotsIntegTestCase {\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        final List<Class<? extends Plugin>> plugins = new ArrayList<>();\n+        plugins.add(WaitForSnapshotBlobCacheShardsActivePlugin.class);\n+        plugins.addAll(super.nodePlugins());\n+        return List.copyOf(plugins);\n+    }\n+\n+    @Override\n+    protected int numberOfReplicas() {\n+        return 0;\n+    }\n+\n+    @Override\n+    protected Settings nodeSettings(int nodeOrdinal) {\n+        return Settings.builder()\n+            .put(super.nodeSettings(nodeOrdinal))\n+            .put(\n+                CacheService.SNAPSHOT_CACHE_RANGE_SIZE_SETTING.getKey(),\n+                randomLongBetween(new ByteSizeValue(4, ByteSizeUnit.KB).getBytes(), new ByteSizeValue(20, ByteSizeUnit.KB).getBytes()) + \"b\"\n+            )\n+            .put(CacheService.SNAPSHOT_CACHE_SIZE_SETTING.getKey(), new ByteSizeValue(Long.MAX_VALUE, ByteSizeUnit.BYTES))\n+            .build();\n+    }\n+\n+    public void testBlobStoreCache() throws Exception {\n+        final String indexName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT);\n+        createIndex(indexName);\n+\n+        final List<IndexRequestBuilder> indexRequestBuilders = new ArrayList<>();\n+        for (int i = scaledRandomIntBetween(0, 10_000); i >= 0; i--) {\n+            indexRequestBuilders.add(client().prepareIndex(indexName).setSource(\"text\", randomUnicodeOfLength(10), \"num\", i));\n+        }\n+        indexRandom(true, false, true, indexRequestBuilders);\n+        final long numberOfDocs = indexRequestBuilders.size();\n+        final NumShards numberOfShards = getNumShards(indexName);\n+\n+        if (randomBoolean()) {\n+            logger.info(\"--> force-merging index before snapshotting\");\n+            final ForceMergeResponse forceMergeResponse = client().admin()\n+                .indices()\n+                .prepareForceMerge(indexName)\n+                .setMaxNumSegments(1)\n+                .get();\n+            assertThat(forceMergeResponse.getSuccessfulShards(), equalTo(numberOfShards.totalNumShards));\n+            assertThat(forceMergeResponse.getFailedShards(), equalTo(0));\n+        }\n+\n+        final String repositoryName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT);\n+        final Path repositoryLocation = randomRepoPath();\n+        createFsRepository(repositoryName, repositoryLocation);\n+\n+        final SnapshotId snapshot = createSnapshot(repositoryName, List.of(indexName));\n+        assertAcked(client().admin().indices().prepareDelete(indexName));\n+\n+        // extract the list of blobs per shard from the snapshot directory on disk\n+        final Map<String, BlobStoreIndexShardSnapshot> blobsInSnapshot = blobsInSnapshot(repositoryLocation, snapshot.getUUID());\n+        assertThat(\"Failed to load all shard snapshot metadata files\", blobsInSnapshot.size(), equalTo(numberOfShards.numPrimaries));\n+\n+        expectThrows(\n+            IndexNotFoundException.class,\n+            \".snapshot-blob-cache system index should not be created yet\",\n+            () -> systemClient().admin().indices().prepareGetIndex().addIndices(SNAPSHOT_BLOB_CACHE_INDEX).get()\n+        );\n+\n+        logger.info(\"--> mount snapshot [{}] as an index for the first time\", snapshot);\n+        final String restoredIndex = mountSnapshot(\n+            repositoryName,\n+            snapshot.getName(),\n+            indexName,\n+            Settings.builder()\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_ENABLED_SETTING.getKey(), true)\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_PREWARM_ENABLED_SETTING.getKey(), false)\n+                .build()\n+        );\n+        ensureGreen(restoredIndex);\n+        ensureExecutorsAreIdle();\n+\n+        // wait for all async cache fills to complete\n+        assertBusy(() -> {\n+            for (final SearchableSnapshotShardStats shardStats : client().execute(\n+                SearchableSnapshotsStatsAction.INSTANCE,\n+                new SearchableSnapshotsStatsRequest()\n+            ).actionGet().getStats()) {\n+                for (final SearchableSnapshotShardStats.CacheIndexInputStats indexInputStats : shardStats.getStats()) {\n+                    assertThat(Strings.toString(indexInputStats), indexInputStats.getCurrentIndexCacheFills(), equalTo(0L));\n+                }\n+            }\n+        });\n+\n+        for (final SearchableSnapshotShardStats shardStats : client().execute(\n+            SearchableSnapshotsStatsAction.INSTANCE,\n+            new SearchableSnapshotsStatsRequest()\n+        ).actionGet().getStats()) {\n+            for (final SearchableSnapshotShardStats.CacheIndexInputStats indexInputStats : shardStats.getStats()) {\n+                assertThat(Strings.toString(indexInputStats), indexInputStats.getBlobStoreBytesRequested().getCount(), greaterThan(0L));\n+            }\n+        }\n+\n+        logger.info(\"--> verifying cached documents in system index [{}]\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        assertCachedBlobsInSystemIndex(repositoryName, blobsInSnapshot);\n+\n+        final long numberOfCachedBlobs = systemClient().prepareSearch(SNAPSHOT_BLOB_CACHE_INDEX).get().getHits().getTotalHits().value;\n+        final long numberOfCacheWrites = systemClient().admin()\n+            .indices()\n+            .prepareStats(SNAPSHOT_BLOB_CACHE_INDEX)\n+            .clear()\n+            .setIndexing(true)\n+            .get()\n+            .getTotal().indexing.getTotal().getIndexCount();\n+\n+        logger.info(\"--> verifying documents in index [{}]\", restoredIndex);\n+        assertHitCount(client().prepareSearch(restoredIndex).setSize(0).setTrackTotalHits(true).get(), numberOfDocs);\n+        assertHitCount(\n+            client().prepareSearch(restoredIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").lte(numberOfDocs))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            numberOfDocs\n+        );\n+        assertHitCount(\n+            client().prepareSearch(restoredIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").gt(numberOfDocs + 1))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            0L\n+        );\n+\n+        assertAcked(client().admin().indices().prepareDelete(restoredIndex));\n+\n+        logger.info(\"--> mount snapshot [{}] as an index for the second time\", snapshot);\n+        final String restoredAgainIndex = mountSnapshot(\n+            repositoryName,\n+            snapshot.getName(),\n+            indexName,\n+            Settings.builder()\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_ENABLED_SETTING.getKey(), true)\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_PREWARM_ENABLED_SETTING.getKey(), false)\n+                .build()\n+        );\n+        ensureGreen(restoredAgainIndex);\n+        ensureExecutorsAreIdle();\n+\n+        logger.info(\"--> verifying shards of [{}] were started without using the blob store more than necessary\", restoredAgainIndex);\n+        for (final SearchableSnapshotShardStats shardStats : client().execute(\n+            SearchableSnapshotsStatsAction.INSTANCE,\n+            new SearchableSnapshotsStatsRequest()\n+        ).actionGet().getStats()) {\n+            for (final SearchableSnapshotShardStats.CacheIndexInputStats indexInputStats : shardStats.getStats()) {\n+                final boolean mayReadMoreThanHeader\n+                // we read the header of each file contained within the .cfs file, which could be anywhere\n+                    = indexInputStats.getFileName().endsWith(\".cfs\")\n+                        // we read a couple of longs at the end of the .fdt file (see https://issues.apache.org/jira/browse/LUCENE-9456)\n+                        // TODO revisit this when this issue is addressed in Lucene\n+                        || indexInputStats.getFileName().endsWith(\".fdt\");\n+                if (indexInputStats.getFileLength() <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE * 2\n+                    || mayReadMoreThanHeader == false) {\n+                    assertThat(Strings.toString(indexInputStats), indexInputStats.getBlobStoreBytesRequested().getCount(), equalTo(0L));\n+                }\n+            }\n+        }\n+\n+        logger.info(\"--> verifying documents in index [{}]\", restoredAgainIndex);\n+        assertHitCount(client().prepareSearch(restoredAgainIndex).setSize(0).setTrackTotalHits(true).get(), numberOfDocs);\n+        assertHitCount(\n+            client().prepareSearch(restoredAgainIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").lte(numberOfDocs))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            numberOfDocs\n+        );\n+        assertHitCount(\n+            client().prepareSearch(restoredAgainIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").gt(numberOfDocs + 1))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            0L\n+        );\n+\n+        logger.info(\"--> verifying cached documents (again) in system index [{}]\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        assertCachedBlobsInSystemIndex(repositoryName, blobsInSnapshot);\n+\n+        logger.info(\"--> verifying that no extra cached blobs were indexed [{}]\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        refreshSystemIndex();\n+        assertHitCount(systemClient().prepareSearch(SNAPSHOT_BLOB_CACHE_INDEX).setSize(0).get(), numberOfCachedBlobs);\n+        assertThat(\n+            systemClient().admin().indices().prepareStats(SNAPSHOT_BLOB_CACHE_INDEX).clear().setIndexing(true).get().getTotal().indexing\n+                .getTotal()\n+                .getIndexCount(),\n+            equalTo(numberOfCacheWrites)\n+        );\n+\n+        logger.info(\"--> restarting cluster\");\n+        internalCluster().fullRestart(new InternalTestCluster.RestartCallback() {\n+            @Override\n+            public Settings onNodeStopped(String nodeName) throws Exception {\n+                return Settings.builder()\n+                    .put(super.onNodeStopped(nodeName))\n+                    .put(WaitForSnapshotBlobCacheShardsActivePlugin.ENABLED.getKey(), true)\n+                    .build();\n+            }\n+        });\n+        ensureGreen(restoredAgainIndex);\n+        ensureExecutorsAreIdle();\n+\n+        logger.info(\"--> verifying documents in index [{}]\", restoredAgainIndex);\n+        assertHitCount(client().prepareSearch(restoredAgainIndex).setSize(0).setTrackTotalHits(true).get(), numberOfDocs);\n+        assertHitCount(client().prepareSearch(restoredAgainIndex).setSize(0).setTrackTotalHits(true).get(), numberOfDocs);\n+        assertHitCount(\n+            client().prepareSearch(restoredAgainIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").lte(numberOfDocs))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            numberOfDocs\n+        );\n+        assertHitCount(\n+            client().prepareSearch(restoredAgainIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").gt(numberOfDocs + 1))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            0L\n+        );\n+\n+        logger.info(\"--> verifying cached documents (after restart) in system index [{}]\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        assertCachedBlobsInSystemIndex(repositoryName, blobsInSnapshot);\n+\n+        logger.info(\"--> verifying that no cached blobs were indexed in system index [{}] after restart\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        assertHitCount(systemClient().prepareSearch(SNAPSHOT_BLOB_CACHE_INDEX).setSize(0).get(), numberOfCachedBlobs);\n+        assertThat(\n+            systemClient().admin().indices().prepareStats(SNAPSHOT_BLOB_CACHE_INDEX).clear().setIndexing(true).get().getTotal().indexing\n+                .getTotal()\n+                .getIndexCount(),\n+            equalTo(0L)\n+        );\n+\n+        // TODO also test when the index is frozen\n+        // TODO also test when prewarming is enabled\n+    }\n+\n+    /**\n+     * @return a {@link Client} that can be used to query the blob store cache system index\n+     */\n+    private Client systemClient() {\n+        return new OriginSettingClient(client(), ClientHelper.SEARCHABLE_SNAPSHOTS_ORIGIN);\n+    }\n+\n+    private void refreshSystemIndex() {\n+        try {\n+            final RefreshResponse refreshResponse = systemClient().admin().indices().prepareRefresh(SNAPSHOT_BLOB_CACHE_INDEX).get();\n+            assertThat(refreshResponse.getSuccessfulShards(), greaterThan(0));\n+            assertThat(refreshResponse.getFailedShards(), equalTo(0));\n+        } catch (IndexNotFoundException indexNotFoundException) {\n+            throw new AssertionError(\"unexpected\", indexNotFoundException);\n+        }\n+    }\n+\n+    /**\n+     * Reads a repository location on disk and extracts the list of blobs for each shards\n+     */\n+    private Map<String, BlobStoreIndexShardSnapshot> blobsInSnapshot(Path repositoryLocation, String snapshotId) throws IOException {\n+        final Map<String, BlobStoreIndexShardSnapshot> blobsPerShard = new HashMap<>();\n+        Files.walkFileTree(repositoryLocation.resolve(\"indices\"), new SimpleFileVisitor<>() {\n+            @Override\n+            public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException {\n+                final String fileName = file.getFileName().toString();\n+                if (fileName.equals(\"snap-\" + snapshotId + \".dat\")) {\n+                    blobsPerShard.put(\n+                        String.join(\n+                            \"/\",\n+                            snapshotId,\n+                            file.getParent().getParent().getFileName().toString(),\n+                            file.getParent().getFileName().toString()\n+                        ),\n+                        INDEX_SHARD_SNAPSHOT_FORMAT.deserialize(fileName, xContentRegistry(), Streams.readFully(Files.newInputStream(file)))\n+                    );\n+                }\n+                return FileVisitResult.CONTINUE;\n+            }\n+        });\n+        return Map.copyOf(blobsPerShard);\n+    }\n+\n+    private void ensureExecutorsAreIdle() throws Exception {\n+        assertBusy(() -> {\n+            for (ThreadPool threadPool : internalCluster().getDataNodeInstances(ThreadPool.class)) {\n+                for (String threadPoolName : List.of(CACHE_FETCH_ASYNC_THREAD_POOL_NAME, CACHE_PREWARMING_THREAD_POOL_NAME)) {\n+                    final ThreadPoolExecutor executor = (ThreadPoolExecutor) threadPool.executor(threadPoolName);\n+                    assertThat(threadPoolName, executor.getQueue().size(), equalTo(0));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd77a43bce70607c961bbb9fd8e3ce6240da2304"}, "originalPosition": 368}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDYzOTYzMw==", "bodyText": "Can we add a test where BlobStoreCacheService#getCachedBlob fails?", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r474639633", "createdAt": "2020-08-21T11:30:32Z", "author": {"login": "fcofdez"}, "path": "x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java", "diffHunk": "@@ -0,0 +1,473 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.elasticsearch.action.admin.indices.forcemerge.ForceMergeResponse;\n+import org.elasticsearch.action.admin.indices.refresh.RefreshResponse;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.index.IndexRequestBuilder;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.routing.RoutingNode;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;\n+import org.elasticsearch.cluster.routing.allocation.decider.AllocationDecider;\n+import org.elasticsearch.cluster.routing.allocation.decider.Decision;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.ClusterSettings;\n+import org.elasticsearch.common.settings.Setting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.index.IndexNotFoundException;\n+import org.elasticsearch.index.query.QueryBuilders;\n+import org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardSnapshot;\n+import org.elasticsearch.plugins.ClusterPlugin;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.junit.annotations.TestLogging;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.xpack.core.ClientHelper;\n+import org.elasticsearch.xpack.core.searchablesnapshots.SearchableSnapshotShardStats;\n+import org.elasticsearch.xpack.searchablesnapshots.BaseSearchableSnapshotsIntegTestCase;\n+import org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots;\n+import org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants;\n+import org.elasticsearch.xpack.searchablesnapshots.action.SearchableSnapshotsStatsAction;\n+import org.elasticsearch.xpack.searchablesnapshots.action.SearchableSnapshotsStatsRequest;\n+import org.elasticsearch.xpack.searchablesnapshots.cache.CacheService;\n+\n+import java.io.IOException;\n+import java.nio.file.FileVisitResult;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.SimpleFileVisitor;\n+import java.nio.file.attribute.BasicFileAttributes;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.concurrent.ThreadPoolExecutor;\n+\n+import static org.elasticsearch.repositories.blobstore.BlobStoreRepository.INDEX_SHARD_SNAPSHOT_FORMAT;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.CACHE_FETCH_ASYNC_THREAD_POOL_NAME;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.CACHE_PREWARMING_THREAD_POOL_NAME;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.SNAPSHOT_BLOB_CACHE_INDEX;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.is;\n+\n+@TestLogging(reason = \"debugging\", value = \"org.elasticsearch.index.store.cache.CachedBlobContainerIndexInput:TRACE,\"\n+    + \"org.elasticsearch.blobstore.cache.BlobStoreCacheService:TRACE\") // TODO remove this before merge\n+public class SearchableSnapshotsBlobStoreCacheIntegTests extends BaseSearchableSnapshotsIntegTestCase {\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        final List<Class<? extends Plugin>> plugins = new ArrayList<>();\n+        plugins.add(WaitForSnapshotBlobCacheShardsActivePlugin.class);\n+        plugins.addAll(super.nodePlugins());\n+        return List.copyOf(plugins);\n+    }\n+\n+    @Override\n+    protected int numberOfReplicas() {\n+        return 0;\n+    }\n+\n+    @Override\n+    protected Settings nodeSettings(int nodeOrdinal) {\n+        return Settings.builder()\n+            .put(super.nodeSettings(nodeOrdinal))\n+            .put(\n+                CacheService.SNAPSHOT_CACHE_RANGE_SIZE_SETTING.getKey(),\n+                randomLongBetween(new ByteSizeValue(4, ByteSizeUnit.KB).getBytes(), new ByteSizeValue(20, ByteSizeUnit.KB).getBytes()) + \"b\"\n+            )\n+            .put(CacheService.SNAPSHOT_CACHE_SIZE_SETTING.getKey(), new ByteSizeValue(Long.MAX_VALUE, ByteSizeUnit.BYTES))\n+            .build();\n+    }\n+\n+    public void testBlobStoreCache() throws Exception {\n+        final String indexName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT);\n+        createIndex(indexName);\n+\n+        final List<IndexRequestBuilder> indexRequestBuilders = new ArrayList<>();\n+        for (int i = scaledRandomIntBetween(0, 10_000); i >= 0; i--) {\n+            indexRequestBuilders.add(client().prepareIndex(indexName).setSource(\"text\", randomUnicodeOfLength(10), \"num\", i));\n+        }\n+        indexRandom(true, false, true, indexRequestBuilders);\n+        final long numberOfDocs = indexRequestBuilders.size();\n+        final NumShards numberOfShards = getNumShards(indexName);\n+\n+        if (randomBoolean()) {\n+            logger.info(\"--> force-merging index before snapshotting\");\n+            final ForceMergeResponse forceMergeResponse = client().admin()\n+                .indices()\n+                .prepareForceMerge(indexName)\n+                .setMaxNumSegments(1)\n+                .get();\n+            assertThat(forceMergeResponse.getSuccessfulShards(), equalTo(numberOfShards.totalNumShards));\n+            assertThat(forceMergeResponse.getFailedShards(), equalTo(0));\n+        }\n+\n+        final String repositoryName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT);\n+        final Path repositoryLocation = randomRepoPath();\n+        createFsRepository(repositoryName, repositoryLocation);\n+\n+        final SnapshotId snapshot = createSnapshot(repositoryName, List.of(indexName));\n+        assertAcked(client().admin().indices().prepareDelete(indexName));\n+\n+        // extract the list of blobs per shard from the snapshot directory on disk\n+        final Map<String, BlobStoreIndexShardSnapshot> blobsInSnapshot = blobsInSnapshot(repositoryLocation, snapshot.getUUID());\n+        assertThat(\"Failed to load all shard snapshot metadata files\", blobsInSnapshot.size(), equalTo(numberOfShards.numPrimaries));\n+\n+        expectThrows(\n+            IndexNotFoundException.class,\n+            \".snapshot-blob-cache system index should not be created yet\",\n+            () -> systemClient().admin().indices().prepareGetIndex().addIndices(SNAPSHOT_BLOB_CACHE_INDEX).get()\n+        );\n+\n+        logger.info(\"--> mount snapshot [{}] as an index for the first time\", snapshot);\n+        final String restoredIndex = mountSnapshot(\n+            repositoryName,\n+            snapshot.getName(),\n+            indexName,\n+            Settings.builder()\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_ENABLED_SETTING.getKey(), true)\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_PREWARM_ENABLED_SETTING.getKey(), false)\n+                .build()\n+        );\n+        ensureGreen(restoredIndex);\n+        ensureExecutorsAreIdle();\n+\n+        // wait for all async cache fills to complete\n+        assertBusy(() -> {\n+            for (final SearchableSnapshotShardStats shardStats : client().execute(\n+                SearchableSnapshotsStatsAction.INSTANCE,\n+                new SearchableSnapshotsStatsRequest()\n+            ).actionGet().getStats()) {\n+                for (final SearchableSnapshotShardStats.CacheIndexInputStats indexInputStats : shardStats.getStats()) {\n+                    assertThat(Strings.toString(indexInputStats), indexInputStats.getCurrentIndexCacheFills(), equalTo(0L));\n+                }\n+            }\n+        });\n+\n+        for (final SearchableSnapshotShardStats shardStats : client().execute(\n+            SearchableSnapshotsStatsAction.INSTANCE,\n+            new SearchableSnapshotsStatsRequest()\n+        ).actionGet().getStats()) {\n+            for (final SearchableSnapshotShardStats.CacheIndexInputStats indexInputStats : shardStats.getStats()) {\n+                assertThat(Strings.toString(indexInputStats), indexInputStats.getBlobStoreBytesRequested().getCount(), greaterThan(0L));\n+            }\n+        }\n+\n+        logger.info(\"--> verifying cached documents in system index [{}]\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        assertCachedBlobsInSystemIndex(repositoryName, blobsInSnapshot);\n+\n+        final long numberOfCachedBlobs = systemClient().prepareSearch(SNAPSHOT_BLOB_CACHE_INDEX).get().getHits().getTotalHits().value;\n+        final long numberOfCacheWrites = systemClient().admin()\n+            .indices()\n+            .prepareStats(SNAPSHOT_BLOB_CACHE_INDEX)\n+            .clear()\n+            .setIndexing(true)\n+            .get()\n+            .getTotal().indexing.getTotal().getIndexCount();\n+\n+        logger.info(\"--> verifying documents in index [{}]\", restoredIndex);\n+        assertHitCount(client().prepareSearch(restoredIndex).setSize(0).setTrackTotalHits(true).get(), numberOfDocs);\n+        assertHitCount(\n+            client().prepareSearch(restoredIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").lte(numberOfDocs))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            numberOfDocs\n+        );\n+        assertHitCount(\n+            client().prepareSearch(restoredIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").gt(numberOfDocs + 1))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            0L\n+        );\n+\n+        assertAcked(client().admin().indices().prepareDelete(restoredIndex));\n+\n+        logger.info(\"--> mount snapshot [{}] as an index for the second time\", snapshot);\n+        final String restoredAgainIndex = mountSnapshot(\n+            repositoryName,\n+            snapshot.getName(),\n+            indexName,\n+            Settings.builder()\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_ENABLED_SETTING.getKey(), true)\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_PREWARM_ENABLED_SETTING.getKey(), false)\n+                .build()\n+        );\n+        ensureGreen(restoredAgainIndex);\n+        ensureExecutorsAreIdle();\n+\n+        logger.info(\"--> verifying shards of [{}] were started without using the blob store more than necessary\", restoredAgainIndex);\n+        for (final SearchableSnapshotShardStats shardStats : client().execute(\n+            SearchableSnapshotsStatsAction.INSTANCE,\n+            new SearchableSnapshotsStatsRequest()\n+        ).actionGet().getStats()) {\n+            for (final SearchableSnapshotShardStats.CacheIndexInputStats indexInputStats : shardStats.getStats()) {\n+                final boolean mayReadMoreThanHeader\n+                // we read the header of each file contained within the .cfs file, which could be anywhere\n+                    = indexInputStats.getFileName().endsWith(\".cfs\")\n+                        // we read a couple of longs at the end of the .fdt file (see https://issues.apache.org/jira/browse/LUCENE-9456)\n+                        // TODO revisit this when this issue is addressed in Lucene\n+                        || indexInputStats.getFileName().endsWith(\".fdt\");\n+                if (indexInputStats.getFileLength() <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE * 2\n+                    || mayReadMoreThanHeader == false) {\n+                    assertThat(Strings.toString(indexInputStats), indexInputStats.getBlobStoreBytesRequested().getCount(), equalTo(0L));\n+                }\n+            }\n+        }\n+\n+        logger.info(\"--> verifying documents in index [{}]\", restoredAgainIndex);\n+        assertHitCount(client().prepareSearch(restoredAgainIndex).setSize(0).setTrackTotalHits(true).get(), numberOfDocs);\n+        assertHitCount(\n+            client().prepareSearch(restoredAgainIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").lte(numberOfDocs))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            numberOfDocs\n+        );\n+        assertHitCount(\n+            client().prepareSearch(restoredAgainIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").gt(numberOfDocs + 1))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            0L\n+        );\n+\n+        logger.info(\"--> verifying cached documents (again) in system index [{}]\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        assertCachedBlobsInSystemIndex(repositoryName, blobsInSnapshot);\n+\n+        logger.info(\"--> verifying that no extra cached blobs were indexed [{}]\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        refreshSystemIndex();\n+        assertHitCount(systemClient().prepareSearch(SNAPSHOT_BLOB_CACHE_INDEX).setSize(0).get(), numberOfCachedBlobs);\n+        assertThat(\n+            systemClient().admin().indices().prepareStats(SNAPSHOT_BLOB_CACHE_INDEX).clear().setIndexing(true).get().getTotal().indexing\n+                .getTotal()\n+                .getIndexCount(),\n+            equalTo(numberOfCacheWrites)\n+        );\n+\n+        logger.info(\"--> restarting cluster\");\n+        internalCluster().fullRestart(new InternalTestCluster.RestartCallback() {\n+            @Override\n+            public Settings onNodeStopped(String nodeName) throws Exception {\n+                return Settings.builder()\n+                    .put(super.onNodeStopped(nodeName))\n+                    .put(WaitForSnapshotBlobCacheShardsActivePlugin.ENABLED.getKey(), true)\n+                    .build();\n+            }\n+        });\n+        ensureGreen(restoredAgainIndex);\n+        ensureExecutorsAreIdle();\n+\n+        logger.info(\"--> verifying documents in index [{}]\", restoredAgainIndex);\n+        assertHitCount(client().prepareSearch(restoredAgainIndex).setSize(0).setTrackTotalHits(true).get(), numberOfDocs);\n+        assertHitCount(client().prepareSearch(restoredAgainIndex).setSize(0).setTrackTotalHits(true).get(), numberOfDocs);\n+        assertHitCount(\n+            client().prepareSearch(restoredAgainIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").lte(numberOfDocs))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            numberOfDocs\n+        );\n+        assertHitCount(\n+            client().prepareSearch(restoredAgainIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").gt(numberOfDocs + 1))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            0L\n+        );\n+\n+        logger.info(\"--> verifying cached documents (after restart) in system index [{}]\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        assertCachedBlobsInSystemIndex(repositoryName, blobsInSnapshot);\n+\n+        logger.info(\"--> verifying that no cached blobs were indexed in system index [{}] after restart\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        assertHitCount(systemClient().prepareSearch(SNAPSHOT_BLOB_CACHE_INDEX).setSize(0).get(), numberOfCachedBlobs);\n+        assertThat(\n+            systemClient().admin().indices().prepareStats(SNAPSHOT_BLOB_CACHE_INDEX).clear().setIndexing(true).get().getTotal().indexing\n+                .getTotal()\n+                .getIndexCount(),\n+            equalTo(0L)\n+        );\n+\n+        // TODO also test when the index is frozen\n+        // TODO also test when prewarming is enabled", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd77a43bce70607c961bbb9fd8e3ce6240da2304"}, "originalPosition": 317}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDY0MDkwNw==", "bodyText": "We're populating this field with Version.CURRENT I think I'm missing where is this useful? I think it would make more sense to populate with the version of the snapshot itself so we can determine if it's possible to read this or not?", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r474640907", "createdAt": "2020-08-21T11:33:37Z", "author": {"login": "fcofdez"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/CachedBlob.java", "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.elasticsearch.Version;\n+import org.elasticsearch.common.bytes.BytesArray;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.xcontent.ToXContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+\n+import java.io.IOException;\n+import java.time.Instant;\n+import java.util.Base64;\n+import java.util.Map;\n+\n+public class CachedBlob implements ToXContent {\n+\n+    /**\n+     * Sentinel {@link CachedBlob} indicating that searching the cache index returned an error.\n+     */\n+    public static final CachedBlob CACHE_NOT_READY = new CachedBlob(null, null, null, \"CACHE_NOT_READY\", null, BytesArray.EMPTY, 0L, 0L);\n+\n+    /**\n+     * Sentinel {@link CachedBlob} indicating that the cache index definitely did not contain the requested data.\n+     */\n+    public static final CachedBlob CACHE_MISS = new CachedBlob(null, null, null, \"CACHE_MISS\", null, BytesArray.EMPTY, 0L, 0L);\n+\n+    private static final String TYPE = \"blob\";\n+\n+    private final Instant creationTime;\n+    private final Version version;\n+    private final String repository;\n+    private final String name;\n+    private final String path;\n+\n+    private final BytesReference bytes;\n+    private final long from;\n+    private final long to;\n+\n+    public CachedBlob(\n+        Instant creationTime,\n+        Version version,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd77a43bce70607c961bbb9fd8e3ce6240da2304"}, "originalPosition": 46}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7aa25d0dc4a406460af0e6e5e03084a784295358", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/7aa25d0dc4a406460af0e6e5e03084a784295358", "committedDate": "2020-08-24T08:47:00Z", "message": "Merge branch 'master' into poc-blob-cache"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1c38aaa0e052ca06d7aeda7e7e073ffd353be0b8", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/1c38aaa0e052ca06d7aeda7e7e073ffd353be0b8", "committedDate": "2020-08-24T08:52:29Z", "message": "Less allocation"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0fbb71f14c73e5120420329519fe4eecfa814638", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/0fbb71f14c73e5120420329519fe4eecfa814638", "committedDate": "2020-08-24T09:22:39Z", "message": "Use TransportActions.isShardNotAvailableException"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7e6eb5323617b9c19e3855a1f92e032d5cadcfad", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/7e6eb5323617b9c19e3855a1f92e032d5cadcfad", "committedDate": "2020-08-24T09:23:04Z", "message": "Always notify listener"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDczMjM4MzY4", "url": "https://github.com/elastic/elasticsearch/pull/60522#pullrequestreview-473238368", "createdAt": "2020-08-24T08:51:12Z", "commit": {"oid": "cd77a43bce70607c961bbb9fd8e3ce6240da2304"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQwODo1MToxM1rOHFab-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQwOTo0Mzo0OVrOHFcywg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTQzODA3Mw==", "bodyText": "Should we trigger the listener on this exception too?", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r475438073", "createdAt": "2020-08-24T08:51:13Z", "author": {"login": "fcofdez"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java", "diffHunk": "@@ -0,0 +1,322 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.ResourceAlreadyExistsException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.NoShardAvailableActionException;\n+import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;\n+import org.elasticsearch.action.get.GetRequest;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.index.IndexRequest;\n+import org.elasticsearch.action.index.IndexResponse;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.ClusterChangedEvent;\n+import org.elasticsearch.cluster.ClusterStateListener;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.xcontent.ToXContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.index.IndexNotFoundException;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.io.IOException;\n+import java.time.Instant;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;\n+import static org.elasticsearch.index.mapper.MapperService.SINGLE_MAPPING_NAME;\n+import static org.elasticsearch.xpack.core.ClientHelper.SEARCHABLE_SNAPSHOTS_ORIGIN;\n+\n+public class BlobStoreCacheService extends AbstractLifecycleComponent implements ClusterStateListener {\n+\n+    private static final Logger logger = LogManager.getLogger(BlobStoreCacheService.class);\n+\n+    public static final int DEFAULT_CACHED_BLOB_SIZE = Math.toIntExact(ByteSizeUnit.KB.toBytes(4L));\n+\n+    private final ClusterService clusterService;\n+    private final ThreadPool threadPool;\n+    private final AtomicBoolean ready;\n+    private final Client client;\n+    private final String index;\n+\n+    public BlobStoreCacheService(ClusterService clusterService, ThreadPool threadPool, Client client, String index) {\n+        this.client = new OriginSettingClient(client, SEARCHABLE_SNAPSHOTS_ORIGIN);\n+        this.ready = new AtomicBoolean(false);\n+        this.clusterService = clusterService;\n+        this.threadPool = threadPool;\n+        this.index = index;\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        clusterService.addListener(this);\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        clusterService.removeListener(this);\n+    }\n+\n+    @Override\n+    protected void doClose() {}\n+\n+    @Override\n+    public void clusterChanged(ClusterChangedEvent event) {\n+        if (lifecycle.started() == false || event.routingTableChanged() == false) {\n+            return;\n+        }\n+        if (event.indexRoutingTableChanged(index)) {\n+            final IndexRoutingTable indexRoutingTable = event.state().routingTable().index(index);\n+            if (indexRoutingTable == null) {\n+                ready.set(false);\n+                return;\n+            }\n+            ready.set(indexRoutingTable.allPrimaryShardsActive());\n+        }\n+    }\n+\n+    private void createIndexIfNecessary(ActionListener<String> listener) {\n+        if (clusterService.state().routingTable().hasIndex(index)) {\n+            listener.onResponse(index);\n+            return;\n+        }\n+        try {\n+            client.admin()\n+                .indices()\n+                .prepareCreate(index)\n+                .setSettings(indexSettings())\n+                .setMapping(mappings())\n+                .execute(new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(CreateIndexResponse createIndexResponse) {\n+                        assert createIndexResponse.index().equals(index);\n+                        listener.onResponse(createIndexResponse.index());\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        if (e instanceof ResourceAlreadyExistsException\n+                            || ExceptionsHelper.unwrapCause(e) instanceof ResourceAlreadyExistsException) {\n+                            listener.onResponse(index);\n+                        } else {\n+                            listener.onFailure(e);\n+                        }\n+                    }\n+                });\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+        }\n+    }\n+\n+    private static Settings indexSettings() {\n+        return Settings.builder()\n+            .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1)\n+            .put(IndexMetadata.SETTING_AUTO_EXPAND_REPLICAS, \"0-1\")\n+            .put(IndexMetadata.SETTING_PRIORITY, \"900\")\n+            .build();\n+    }\n+\n+    private static XContentBuilder mappings() throws IOException {\n+        final XContentBuilder builder = jsonBuilder();\n+        {\n+            builder.startObject();\n+            {\n+                builder.startObject(SINGLE_MAPPING_NAME);\n+                builder.field(\"dynamic\", \"strict\");\n+                {\n+                    builder.startObject(\"_meta\");\n+                    builder.field(\"version\", Version.CURRENT);\n+                    builder.endObject();\n+                }\n+                {\n+                    builder.startObject(\"properties\");\n+                    {\n+                        builder.startObject(\"type\");\n+                        builder.field(\"type\", \"keyword\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"creation_time\");\n+                        builder.field(\"type\", \"date\");\n+                        builder.field(\"format\", \"epoch_millis\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"version\");\n+                        builder.field(\"type\", \"integer\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"repository\");\n+                        builder.field(\"type\", \"keyword\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"blob\");\n+                        builder.field(\"type\", \"object\");\n+                        {\n+                            builder.startObject(\"properties\");\n+                            {\n+                                builder.startObject(\"name\");\n+                                builder.field(\"type\", \"keyword\");\n+                                builder.endObject();\n+                                builder.startObject(\"path\");\n+                                builder.field(\"type\", \"keyword\");\n+                                builder.endObject();\n+                            }\n+                            builder.endObject();\n+                        }\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"data\");\n+                        builder.field(\"type\", \"object\");\n+                        {\n+                            builder.startObject(\"properties\");\n+                            {\n+                                builder.startObject(\"content\");\n+                                builder.field(\"type\", \"binary\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"length\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"from\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"to\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            builder.endObject();\n+                        }\n+                        builder.endObject();\n+                    }\n+                    builder.endObject();\n+                }\n+                builder.endObject();\n+            }\n+            builder.endObject();\n+        }\n+        return builder;\n+    }\n+\n+    public CachedBlob get(String repository, String name, String path, long offset) {\n+        final PlainActionFuture<CachedBlob> future = PlainActionFuture.newFuture();\n+        getAsync(repository, name, path, offset, future);\n+        return future.actionGet();\n+    }\n+\n+    protected void getAsync(String repository, String name, String path, long offset, ActionListener<CachedBlob> listener) {\n+        if ((lifecycle.started() && ready.get()) == false) {\n+            // TODO TBD can we just execute the GET request and let it fail if the index isn't ready yet?\n+            // We might get lucky and hit a started shard anyway.\n+            logger.debug(\"not ready : [{}]\", CachedBlob.generateId(repository, name, path, offset));\n+            listener.onResponse(CachedBlob.CACHE_NOT_READY);\n+            return;\n+        }\n+        try {\n+            final GetRequest request = new GetRequest(index).id(CachedBlob.generateId(repository, name, path, offset));\n+            client.get(request, new ActionListener<>() {\n+                @Override\n+                public void onResponse(GetResponse response) {\n+                    if (response.isExists()) {\n+                        logger.debug(\"cache hit : [{}]\", request.id());\n+                        assert response.isSourceEmpty() == false;\n+\n+                        final CachedBlob cachedBlob = CachedBlob.fromSource(response.getSource());\n+                        assert response.getId().equals(cachedBlob.generatedId());\n+                        listener.onResponse(cachedBlob);\n+                    } else {\n+                        logger.debug(\"cache miss: [{}]\", request.id());\n+                        listener.onResponse(CachedBlob.CACHE_MISS);\n+                    }\n+                }\n+\n+                @Override\n+                public void onFailure(Exception e) {\n+                    if (e instanceof IndexNotFoundException || e instanceof NoShardAvailableActionException) {\n+                        // In case the blob cache system index got unavailable, we pretend we didn't find a cache entry and we move on.\n+                        // Failing here might bubble up the exception and fail the searchable snapshot shard which is potentially\n+                        // recovering.\n+                        logger.debug(() -> new ParameterizedMessage(\"failed to retrieve cached blob from system index [{}]\", index), e);\n+                        listener.onResponse(CachedBlob.CACHE_NOT_READY);\n+                    } else {\n+                        logger.warn(() -> new ParameterizedMessage(\"failed to retrieve cached blob from system index [{}]\", index), e);\n+                        listener.onFailure(e);\n+                    }\n+                }\n+            });\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+        }\n+    }\n+\n+    public void putAsync(String repository, String name, String path, long offset, BytesReference content, ActionListener<Void> listener) {\n+        createIndexIfNecessary(new ActionListener<>() {\n+            @Override\n+            public void onResponse(String s) {\n+                try {\n+                    final CachedBlob cachedBlob = new CachedBlob(\n+                        Instant.ofEpochMilli(threadPool.absoluteTimeInMillis()),\n+                        Version.CURRENT,\n+                        repository,\n+                        name,\n+                        path,\n+                        content,\n+                        offset\n+                    );\n+                    final IndexRequest request = new IndexRequest(index).id(cachedBlob.generatedId());\n+                    try (XContentBuilder builder = jsonBuilder()) {\n+                        request.source(cachedBlob.toXContent(builder, ToXContent.EMPTY_PARAMS));\n+                    }\n+                    client.index(request, new ActionListener<>() {\n+                        @Override\n+                        public void onResponse(IndexResponse indexResponse) {\n+                            logger.trace(\"cache fill ({}): [{}]\", indexResponse.status(), request.id());\n+                            listener.onResponse(null);\n+                        }\n+\n+                        @Override\n+                        public void onFailure(Exception e) {\n+                            logger.debug(new ParameterizedMessage(\"failure in cache fill: [{}]\", request.id()), e);\n+                            listener.onFailure(e);\n+                        }\n+                    });\n+                } catch (IOException e) {\n+                    logger.warn(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd77a43bce70607c961bbb9fd8e3ce6240da2304"}, "originalPosition": 309}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTQ3NjY3NA==", "bodyText": "Why is this value 4kb? to match the page cache size?", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r475476674", "createdAt": "2020-08-24T09:43:49Z", "author": {"login": "fcofdez"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java", "diffHunk": "@@ -0,0 +1,322 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.ResourceAlreadyExistsException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.NoShardAvailableActionException;\n+import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;\n+import org.elasticsearch.action.get.GetRequest;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.index.IndexRequest;\n+import org.elasticsearch.action.index.IndexResponse;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.ClusterChangedEvent;\n+import org.elasticsearch.cluster.ClusterStateListener;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.xcontent.ToXContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.index.IndexNotFoundException;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.io.IOException;\n+import java.time.Instant;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;\n+import static org.elasticsearch.index.mapper.MapperService.SINGLE_MAPPING_NAME;\n+import static org.elasticsearch.xpack.core.ClientHelper.SEARCHABLE_SNAPSHOTS_ORIGIN;\n+\n+public class BlobStoreCacheService extends AbstractLifecycleComponent implements ClusterStateListener {\n+\n+    private static final Logger logger = LogManager.getLogger(BlobStoreCacheService.class);\n+\n+    public static final int DEFAULT_CACHED_BLOB_SIZE = Math.toIntExact(ByteSizeUnit.KB.toBytes(4L));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd77a43bce70607c961bbb9fd8e3ce6240da2304"}, "originalPosition": 51}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6391d8fa8b77a0746da33cb0e51993bfaf0ba9ed", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/6391d8fa8b77a0746da33cb0e51993bfaf0ba9ed", "committedDate": "2020-08-24T10:40:53Z", "message": "Propagate direct exception directly to caller"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7bd0e47cd9e08d9d614cab9f8dda5e6230bfa29e", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/7bd0e47cd9e08d9d614cab9f8dda5e6230bfa29e", "committedDate": "2020-08-24T10:47:09Z", "message": "Also permit disconnected/not-connected/node-closed exceptions, and assert nothing else happens"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "60ceabb49f9f59fb06ef36219e5ab490f17a55b9", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/60ceabb49f9f59fb06ef36219e5ab490f17a55b9", "committedDate": "2020-08-24T10:56:32Z", "message": "Precommit"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b50d1589f983176a3ad597dafb1f05d3c82f9825", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/b50d1589f983176a3ad597dafb1f05d3c82f9825", "committedDate": "2020-08-24T12:27:22Z", "message": "Check the cache index even after recovery complete"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d53443e753bc24bb787298fab97665874e89a384", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/d53443e753bc24bb787298fab97665874e89a384", "committedDate": "2020-08-24T13:08:53Z", "message": "Collect timings for cache index reads"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "299443fe3e901c7fb5a10d15c48a04d4ab36865d", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/299443fe3e901c7fb5a10d15c48a04d4ab36865d", "committedDate": "2020-08-24T13:20:45Z", "message": "Assert enough space"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8b607b04ae0febff52635ae213f37a5ad7ee8d5c", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/8b607b04ae0febff52635ae213f37a5ad7ee8d5c", "committedDate": "2020-08-25T09:31:21Z", "message": "Fix bytes-read stats"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dbf6f2f2d327946d5f78f9e9e7095eeed4a4a06f", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/dbf6f2f2d327946d5f78f9e9e7095eeed4a4a06f", "committedDate": "2020-08-25T09:31:34Z", "message": "Revert back to untimed counter"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e8bc5616874e388fcc098e40b6ac5934c3901e38", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/e8bc5616874e388fcc098e40b6ac5934c3901e38", "committedDate": "2020-08-25T09:57:17Z", "message": "Another revert to counter"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1172e68e3b14291e4e56e73368106e390e2372a6", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/1172e68e3b14291e4e56e73368106e390e2372a6", "committedDate": "2020-08-25T10:40:40Z", "message": "Avoid allocation reading cached blob"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "181b1f516029fa4cbe168cc55fe58139425a29d3", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/181b1f516029fa4cbe168cc55fe58139425a29d3", "committedDate": "2020-08-25T11:44:20Z", "message": "Allow waiting for a cache file range without committing to filling any gaps"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "abdbf3cecdf4548732ef0653a65b105f84d71cab", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/abdbf3cecdf4548732ef0653a65b105f84d71cab", "committedDate": "2020-08-25T12:16:35Z", "message": "No longer need to track whether recovery is done"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "38bf6976d402dfd7abaaf5dd35375c732c2f2d85", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/38bf6976d402dfd7abaaf5dd35375c732c2f2d85", "committedDate": "2020-08-25T12:28:53Z", "message": "No need to wait for fetch threads to be idle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9fbf8ce44185797e090fcd17e808891878ba4ca7", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/9fbf8ce44185797e090fcd17e808891878ba4ca7", "committedDate": "2020-08-25T17:43:35Z", "message": "Delete restored index first to make sure it doesn't re-create the cache index"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ec9cd242c602ce850a5bab8efac252283509ae44", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/ec9cd242c602ce850a5bab8efac252283509ae44", "committedDate": "2020-08-25T17:44:23Z", "message": "Remove test logging"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1535e58f69f6284f8d44ffc98795e10b1453e650", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/1535e58f69f6284f8d44ffc98795e10b1453e650", "committedDate": "2020-08-25T17:54:16Z", "message": "And another one"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "790bdc10bc4090c4bf41510057cda0e9b6fffc9d", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/790bdc10bc4090c4bf41510057cda0e9b6fffc9d", "committedDate": "2020-08-25T19:42:40Z", "message": "Merge branch 'master' into poc-blob-cache"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc1MjI5NzE1", "url": "https://github.com/elastic/elasticsearch/pull/60522#pullrequestreview-475229715", "createdAt": "2020-08-26T07:41:07Z", "commit": {"oid": "790bdc10bc4090c4bf41510057cda0e9b6fffc9d"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwNzo0MTowOFrOHG_vCA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwOTo1MzoyN1rOHHExUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzA5NzczNg==", "bodyText": "Can we update the stats.yml tests with the new fields?", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r477097736", "createdAt": "2020-08-26T07:41:08Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/searchablesnapshots/SearchableSnapshotShardStats.java", "diffHunk": "@@ -263,6 +302,7 @@ public XContentBuilder toXContent(XContentBuilder builder, Params params) throws\n                 builder.field(\"contiguous_bytes_read\", getContiguousReads());\n                 builder.field(\"non_contiguous_bytes_read\", getNonContiguousReads());\n                 builder.field(\"cached_bytes_read\", getCachedBytesRead());\n+                builder.field(\"index_cache_bytes_read\", getIndexCacheBytesRead());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "790bdc10bc4090c4bf41510057cda0e9b6fffc9d"}, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzEwOTMwMQ==", "bodyText": "A possible alternative idea is to bypass the cache index for indices on which the GET would run on the SYSTEM_READ threadpool.\n\nThat's a interesting idea and I did not know about this new thread pool. This would work I think, but I'm struggling finding a good reason to allow system indices being mounted as searchable snapshots whereas I can see reasons to prevent them being slowish and read-only. I think #61517 makes sense today (with comments made there) and this is something we can revisit later.\n\nWhat do you think about 5s?\n\nAnything below 10s looks good to me.", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r477109301", "createdAt": "2020-08-26T08:00:19Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java", "diffHunk": "@@ -0,0 +1,322 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.ResourceAlreadyExistsException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.NoShardAvailableActionException;\n+import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;\n+import org.elasticsearch.action.get.GetRequest;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.index.IndexRequest;\n+import org.elasticsearch.action.index.IndexResponse;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.ClusterChangedEvent;\n+import org.elasticsearch.cluster.ClusterStateListener;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.xcontent.ToXContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.index.IndexNotFoundException;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.io.IOException;\n+import java.time.Instant;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;\n+import static org.elasticsearch.index.mapper.MapperService.SINGLE_MAPPING_NAME;\n+import static org.elasticsearch.xpack.core.ClientHelper.SEARCHABLE_SNAPSHOTS_ORIGIN;\n+\n+public class BlobStoreCacheService extends AbstractLifecycleComponent implements ClusterStateListener {\n+\n+    private static final Logger logger = LogManager.getLogger(BlobStoreCacheService.class);\n+\n+    public static final int DEFAULT_CACHED_BLOB_SIZE = Math.toIntExact(ByteSizeUnit.KB.toBytes(4L));\n+\n+    private final ClusterService clusterService;\n+    private final ThreadPool threadPool;\n+    private final AtomicBoolean ready;\n+    private final Client client;\n+    private final String index;\n+\n+    public BlobStoreCacheService(ClusterService clusterService, ThreadPool threadPool, Client client, String index) {\n+        this.client = new OriginSettingClient(client, SEARCHABLE_SNAPSHOTS_ORIGIN);\n+        this.ready = new AtomicBoolean(false);\n+        this.clusterService = clusterService;\n+        this.threadPool = threadPool;\n+        this.index = index;\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        clusterService.addListener(this);\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        clusterService.removeListener(this);\n+    }\n+\n+    @Override\n+    protected void doClose() {}\n+\n+    @Override\n+    public void clusterChanged(ClusterChangedEvent event) {\n+        if (lifecycle.started() == false || event.routingTableChanged() == false) {\n+            return;\n+        }\n+        if (event.indexRoutingTableChanged(index)) {\n+            final IndexRoutingTable indexRoutingTable = event.state().routingTable().index(index);\n+            if (indexRoutingTable == null) {\n+                ready.set(false);\n+                return;\n+            }\n+            ready.set(indexRoutingTable.allPrimaryShardsActive());\n+        }\n+    }\n+\n+    private void createIndexIfNecessary(ActionListener<String> listener) {\n+        if (clusterService.state().routingTable().hasIndex(index)) {\n+            listener.onResponse(index);\n+            return;\n+        }\n+        try {\n+            client.admin()\n+                .indices()\n+                .prepareCreate(index)\n+                .setSettings(indexSettings())\n+                .setMapping(mappings())\n+                .execute(new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(CreateIndexResponse createIndexResponse) {\n+                        assert createIndexResponse.index().equals(index);\n+                        listener.onResponse(createIndexResponse.index());\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        if (e instanceof ResourceAlreadyExistsException\n+                            || ExceptionsHelper.unwrapCause(e) instanceof ResourceAlreadyExistsException) {\n+                            listener.onResponse(index);\n+                        } else {\n+                            listener.onFailure(e);\n+                        }\n+                    }\n+                });\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+        }\n+    }\n+\n+    private static Settings indexSettings() {\n+        return Settings.builder()\n+            .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1)\n+            .put(IndexMetadata.SETTING_AUTO_EXPAND_REPLICAS, \"0-1\")\n+            .put(IndexMetadata.SETTING_PRIORITY, \"900\")\n+            .build();\n+    }\n+\n+    private static XContentBuilder mappings() throws IOException {\n+        final XContentBuilder builder = jsonBuilder();\n+        {\n+            builder.startObject();\n+            {\n+                builder.startObject(SINGLE_MAPPING_NAME);\n+                builder.field(\"dynamic\", \"strict\");\n+                {\n+                    builder.startObject(\"_meta\");\n+                    builder.field(\"version\", Version.CURRENT);\n+                    builder.endObject();\n+                }\n+                {\n+                    builder.startObject(\"properties\");\n+                    {\n+                        builder.startObject(\"type\");\n+                        builder.field(\"type\", \"keyword\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"creation_time\");\n+                        builder.field(\"type\", \"date\");\n+                        builder.field(\"format\", \"epoch_millis\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"version\");\n+                        builder.field(\"type\", \"integer\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"repository\");\n+                        builder.field(\"type\", \"keyword\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"blob\");\n+                        builder.field(\"type\", \"object\");\n+                        {\n+                            builder.startObject(\"properties\");\n+                            {\n+                                builder.startObject(\"name\");\n+                                builder.field(\"type\", \"keyword\");\n+                                builder.endObject();\n+                                builder.startObject(\"path\");\n+                                builder.field(\"type\", \"keyword\");\n+                                builder.endObject();\n+                            }\n+                            builder.endObject();\n+                        }\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"data\");\n+                        builder.field(\"type\", \"object\");\n+                        {\n+                            builder.startObject(\"properties\");\n+                            {\n+                                builder.startObject(\"content\");\n+                                builder.field(\"type\", \"binary\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"length\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"from\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"to\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            builder.endObject();\n+                        }\n+                        builder.endObject();\n+                    }\n+                    builder.endObject();\n+                }\n+                builder.endObject();\n+            }\n+            builder.endObject();\n+        }\n+        return builder;\n+    }\n+\n+    public CachedBlob get(String repository, String name, String path, long offset) {\n+        final PlainActionFuture<CachedBlob> future = PlainActionFuture.newFuture();\n+        getAsync(repository, name, path, offset, future);\n+        return future.actionGet();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU1MDMyNA=="}, "originalCommit": {"oid": "cd77a43bce70607c961bbb9fd8e3ce6240da2304"}, "originalPosition": 229}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzExODU0NQ==", "bodyText": "I think we should catch any exception on this and calls the listener appropriately (ActionListener.wrap() would do this for us)", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r477118545", "createdAt": "2020-08-26T08:16:02Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java", "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.ResourceAlreadyExistsException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;\n+import org.elasticsearch.action.get.GetRequest;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.index.IndexRequest;\n+import org.elasticsearch.action.index.IndexResponse;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.TransportActions;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.ClusterChangedEvent;\n+import org.elasticsearch.cluster.ClusterStateListener;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.xcontent.ToXContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.node.NodeClosedException;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.ConnectTransportException;\n+\n+import java.io.IOException;\n+import java.time.Instant;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;\n+import static org.elasticsearch.index.mapper.MapperService.SINGLE_MAPPING_NAME;\n+import static org.elasticsearch.xpack.core.ClientHelper.SEARCHABLE_SNAPSHOTS_ORIGIN;\n+\n+public class BlobStoreCacheService extends AbstractLifecycleComponent implements ClusterStateListener {\n+\n+    private static final Logger logger = LogManager.getLogger(BlobStoreCacheService.class);\n+\n+    public static final int DEFAULT_CACHED_BLOB_SIZE = Math.toIntExact(ByteSizeUnit.KB.toBytes(4L));\n+\n+    private final ClusterService clusterService;\n+    private final ThreadPool threadPool;\n+    private final AtomicBoolean ready;\n+    private final Client client;\n+    private final String index;\n+\n+    public BlobStoreCacheService(ClusterService clusterService, ThreadPool threadPool, Client client, String index) {\n+        this.client = new OriginSettingClient(client, SEARCHABLE_SNAPSHOTS_ORIGIN);\n+        this.ready = new AtomicBoolean(false);\n+        this.clusterService = clusterService;\n+        this.threadPool = threadPool;\n+        this.index = index;\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        clusterService.addListener(this);\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        clusterService.removeListener(this);\n+    }\n+\n+    @Override\n+    protected void doClose() {}\n+\n+    @Override\n+    public void clusterChanged(ClusterChangedEvent event) {\n+        if (lifecycle.started() == false || event.routingTableChanged() == false) {\n+            return;\n+        }\n+        if (event.indexRoutingTableChanged(index)) {\n+            final IndexRoutingTable indexRoutingTable = event.state().routingTable().index(index);\n+            if (indexRoutingTable == null) {\n+                ready.set(false);\n+                return;\n+            }\n+            ready.set(indexRoutingTable.allPrimaryShardsActive());\n+        }\n+    }\n+\n+    private void createIndexIfNecessary(ActionListener<String> listener) {\n+        if (clusterService.state().routingTable().hasIndex(index)) {\n+            listener.onResponse(index);\n+            return;\n+        }\n+        try {\n+            client.admin()\n+                .indices()\n+                .prepareCreate(index)\n+                .setSettings(indexSettings())\n+                .setMapping(mappings())\n+                .execute(new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(CreateIndexResponse createIndexResponse) {\n+                        assert createIndexResponse.index().equals(index);\n+                        listener.onResponse(createIndexResponse.index());\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        if (e instanceof ResourceAlreadyExistsException\n+                            || ExceptionsHelper.unwrapCause(e) instanceof ResourceAlreadyExistsException) {\n+                            listener.onResponse(index);\n+                        } else {\n+                            listener.onFailure(e);\n+                        }\n+                    }\n+                });\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+        }\n+    }\n+\n+    private static Settings indexSettings() {\n+        return Settings.builder()\n+            .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1)\n+            .put(IndexMetadata.SETTING_AUTO_EXPAND_REPLICAS, \"0-1\")\n+            .put(IndexMetadata.SETTING_PRIORITY, \"900\")\n+            .build();\n+    }\n+\n+    private static XContentBuilder mappings() throws IOException {\n+        final XContentBuilder builder = jsonBuilder();\n+        {\n+            builder.startObject();\n+            {\n+                builder.startObject(SINGLE_MAPPING_NAME);\n+                builder.field(\"dynamic\", \"strict\");\n+                {\n+                    builder.startObject(\"_meta\");\n+                    builder.field(\"version\", Version.CURRENT);\n+                    builder.endObject();\n+                }\n+                {\n+                    builder.startObject(\"properties\");\n+                    {\n+                        builder.startObject(\"type\");\n+                        builder.field(\"type\", \"keyword\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"creation_time\");\n+                        builder.field(\"type\", \"date\");\n+                        builder.field(\"format\", \"epoch_millis\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"version\");\n+                        builder.field(\"type\", \"integer\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"repository\");\n+                        builder.field(\"type\", \"keyword\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"blob\");\n+                        builder.field(\"type\", \"object\");\n+                        {\n+                            builder.startObject(\"properties\");\n+                            {\n+                                builder.startObject(\"name\");\n+                                builder.field(\"type\", \"keyword\");\n+                                builder.endObject();\n+                                builder.startObject(\"path\");\n+                                builder.field(\"type\", \"keyword\");\n+                                builder.endObject();\n+                            }\n+                            builder.endObject();\n+                        }\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"data\");\n+                        builder.field(\"type\", \"object\");\n+                        {\n+                            builder.startObject(\"properties\");\n+                            {\n+                                builder.startObject(\"content\");\n+                                builder.field(\"type\", \"binary\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"length\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"from\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"to\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            builder.endObject();\n+                        }\n+                        builder.endObject();\n+                    }\n+                    builder.endObject();\n+                }\n+                builder.endObject();\n+            }\n+            builder.endObject();\n+        }\n+        return builder;\n+    }\n+\n+    public CachedBlob get(String repository, String name, String path, long offset) {\n+        final PlainActionFuture<CachedBlob> future = PlainActionFuture.newFuture();\n+        getAsync(repository, name, path, offset, future);\n+        return future.actionGet();\n+    }\n+\n+    protected void getAsync(String repository, String name, String path, long offset, ActionListener<CachedBlob> listener) {\n+        if ((lifecycle.started() && ready.get()) == false) {\n+            // TODO TBD can we just execute the GET request and let it fail if the index isn't ready yet?\n+            // We might get lucky and hit a started shard anyway.\n+            logger.debug(\"not ready : [{}]\", CachedBlob.generateId(repository, name, path, offset));\n+            listener.onResponse(CachedBlob.CACHE_NOT_READY);\n+            return;\n+        }\n+        final GetRequest request = new GetRequest(index).id(CachedBlob.generateId(repository, name, path, offset));\n+        client.get(request, new ActionListener<>() {\n+            @Override\n+            public void onResponse(GetResponse response) {\n+                if (response.isExists()) {\n+                    logger.debug(\"cache hit : [{}]\", request.id());\n+                    assert response.isSourceEmpty() == false;\n+\n+                    final CachedBlob cachedBlob = CachedBlob.fromSource(response.getSource());\n+                    assert response.getId().equals(cachedBlob.generatedId());\n+                    listener.onResponse(cachedBlob);\n+                } else {\n+                    logger.debug(\"cache miss: [{}]\", request.id());\n+                    listener.onResponse(CachedBlob.CACHE_MISS);\n+                }\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                // In case the blob cache system index is unavailable, we indicate it's not ready and move on. We do not fail the request:\n+                // a failure here is not fatal since the data exists in the blob store, so we can simply indicate the cache is not ready.\n+                if (isExpectedCacheGetException(e)) {\n+                    logger.debug(() -> new ParameterizedMessage(\"failed to retrieve cached blob from system index [{}]\", index), e);\n+                } else {\n+                    logger.warn(() -> new ParameterizedMessage(\"failed to retrieve cached blob from system index [{}]\", index), e);\n+                    assert false : e;\n+                }\n+                listener.onResponse(CachedBlob.CACHE_NOT_READY);\n+            }\n+        });\n+    }\n+\n+    private static boolean isExpectedCacheGetException(Exception e) {\n+        return TransportActions.isShardNotAvailableException(e)\n+            || e instanceof ConnectTransportException\n+            || ExceptionsHelper.unwrapCause(e) instanceof NodeClosedException;\n+    }\n+\n+    public void putAsync(String repository, String name, String path, long offset, BytesReference content, ActionListener<Void> listener) {\n+        createIndexIfNecessary(new ActionListener<>() {\n+            @Override\n+            public void onResponse(String s) {\n+                final IndexRequest request;\n+                try {\n+                    final CachedBlob cachedBlob = new CachedBlob(\n+                        Instant.ofEpochMilli(threadPool.absoluteTimeInMillis()),\n+                        Version.CURRENT,\n+                        repository,\n+                        name,\n+                        path,\n+                        content,\n+                        offset\n+                    );\n+                    request = new IndexRequest(index).id(cachedBlob.generatedId());\n+                    try (XContentBuilder builder = jsonBuilder()) {\n+                        request.source(cachedBlob.toXContent(builder, ToXContent.EMPTY_PARAMS));\n+                    }\n+                } catch (IOException e) {\n+                    logger.warn(\n+                        new ParameterizedMessage(\"cache fill failure: [{}]\", CachedBlob.generateId(repository, name, path, offset)),\n+                        e\n+                    );\n+                    listener.onFailure(e);\n+                    return;\n+                }\n+\n+                client.index(request, new ActionListener<>() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "790bdc10bc4090c4bf41510057cda0e9b6fffc9d"}, "originalPosition": 307}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE1MDc1Mw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                    directory.putCachedBlob(fileInfo.physicalName(), indexCacheMiss.v1(), content, new ActionListener<>() {\n          \n          \n            \n                                        @Override\n          \n          \n            \n                                        public void onResponse(Void response) {\n          \n          \n            \n                                            onCacheFillComplete.close();\n          \n          \n            \n                                        }\n          \n          \n            \n            \n          \n          \n            \n                                        @Override\n          \n          \n            \n                                        public void onFailure(Exception e1) {\n          \n          \n            \n                                            onCacheFillComplete.close();\n          \n          \n            \n                                        }\n          \n          \n            \n                                    });\n          \n          \n            \n                                    directory.putCachedBlob(fileInfo.physicalName(), indexCacheMiss.v1(), content, ActionListener.wrap(onCacheFillComplete::close));", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r477150753", "createdAt": "2020-08-26T09:07:28Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -136,53 +148,296 @@ protected void readInternal(ByteBuffer b) throws IOException {\n         final long position = getFilePointer() + this.offset;\n         final int length = b.remaining();\n \n-        int totalBytesRead = 0;\n-        while (totalBytesRead < length) {\n-            final long pos = position + totalBytesRead;\n-            final int len = length - totalBytesRead;\n-            int bytesRead = 0;\n-            try {\n-                final CacheFile cacheFile = getCacheFileSafe();\n-                try (Releasable ignored = cacheFile.fileLock()) {\n-                    final Tuple<Long, Long> rangeToWrite = computeRange(pos);\n-                    final Tuple<Long, Long> rangeToRead = Tuple.tuple(pos, Math.min(pos + len, rangeToWrite.v2()));\n-\n-                    bytesRead = cacheFile.fetchAsync(rangeToWrite, rangeToRead, (channel) -> {\n-                        final int read;\n-                        if ((rangeToRead.v2() - rangeToRead.v1()) < b.remaining()) {\n-                            final ByteBuffer duplicate = b.duplicate();\n-                            duplicate.limit(duplicate.position() + Math.toIntExact(rangeToRead.v2() - rangeToRead.v1()));\n-                            read = readCacheFile(channel, pos, duplicate);\n-                            assert duplicate.position() <= b.limit();\n-                            b.position(duplicate.position());\n+        // We can detect that we're going to read the last 16 bytes (that contains the footer checksum) of the file. Such reads are often\n+        // executed when opening a Directory and since we have the checksum in the snapshot metadata we can use it to fill the ByteBuffer.\n+        if (length == CodecUtil.footerLength() && isClone == false && position == fileInfo.length() - length) {\n+            if (readChecksumFromFileInfo(b)) {\n+                logger.trace(\"read footer of file [{}] at position [{}], bypassing all caches\", fileInfo.physicalName(), position);\n+                return;\n+            }\n+            assert b.remaining() == length;\n+        }\n+\n+        logger.trace(\"readInternal: read [{}-{}] ([{}] bytes) from [{}]\", position, position + length, length, this);\n+\n+        try {\n+            final CacheFile cacheFile = getCacheFileSafe();\n+            try (Releasable ignored = cacheFile.fileLock()) {\n+\n+                // Can we serve the read directly from disk? If so, do so and don't worry about anything else.\n+\n+                final CompletableFuture<Integer> waitingForRead = cacheFile.readIfAvailableOrPending(\n+                    Tuple.tuple(position, position + length),\n+                    channel -> {\n+                        final int read = readCacheFile(channel, position, b);\n+                        assert read == length : read + \" vs \" + length;\n+                        return read;\n+                    }\n+                );\n+\n+                if (waitingForRead != null) {\n+                    final Integer read = waitingForRead.get();\n+                    assert read == length;\n+                    readComplete(position, length);\n+                    return;\n+                }\n+\n+                // Requested data is not on disk, so try the cache index next.\n+\n+                final Tuple<Long, Long> indexCacheMiss; // null if not a miss\n+\n+                // We try to use the cache index if:\n+                // - the file is small enough to be fully cached\n+                final boolean canBeFullyCached = fileInfo.length() <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE * 2;\n+                // - we're reading the first N bytes of the file\n+                final boolean isStartOfFile = (position + length <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE);\n+\n+                if (canBeFullyCached || isStartOfFile) {\n+                    final CachedBlob cachedBlob = directory.getCachedBlob(fileInfo.physicalName(), 0L, length);\n+\n+                    if (cachedBlob == CachedBlob.CACHE_MISS || cachedBlob == CachedBlob.CACHE_NOT_READY) {\n+                        // We would have liked to find a cached entry but we did not find anything: the cache on the disk will be requested\n+                        // so we compute the region of the file we would like to have the next time. The region is expressed as a tuple of\n+                        // {start, end} where positions are relative to the whole file.\n+\n+                        if (canBeFullyCached) {\n+                            // if the index input is smaller than twice the size of the blob cache, it will be fully indexed\n+                            indexCacheMiss = Tuple.tuple(0L, fileInfo.length());\n                         } else {\n-                            read = readCacheFile(channel, pos, b);\n+                            // the index input is too large to fully cache, so just cache the initial range\n+                            indexCacheMiss = Tuple.tuple(0L, (long) BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE);\n                         }\n-                        return read;\n-                    }, this::writeCacheFile, directory.cacheFetchAsyncExecutor()).get();\n+\n+                        // We must fill in a cache miss even if CACHE_NOT_READY since the cache index is only created on the first put.\n+                        // TODO TBD use a different trigger for creating the cache index and avoid a put in the CACHE_NOT_READY case.\n+                    } else {\n+                        logger.trace(\n+                            \"reading [{}] bytes of file [{}] at position [{}] using cache index\",\n+                            length,\n+                            fileInfo.physicalName(),\n+                            position\n+                        );\n+                        stats.addIndexCacheBytesRead(cachedBlob.length());\n+\n+                        final BytesRefIterator cachedBytesIterator = cachedBlob.bytes().slice(Math.toIntExact(position), length).iterator();\n+                        BytesRef bytesRef;\n+                        while ((bytesRef = cachedBytesIterator.next()) != null) {\n+                            b.put(bytesRef.bytes, bytesRef.offset, bytesRef.length);\n+                        }\n+                        assert b.position() == length : \"copied \" + b.position() + \" but expected \" + length;\n+\n+                        try {\n+                            final Tuple<Long, Long> cachedRange = Tuple.tuple(cachedBlob.from(), cachedBlob.to());\n+                            cacheFile.populateAndRead(\n+                                cachedRange,\n+                                cachedRange,\n+                                channel -> cachedBlob.length(),\n+                                (channel, from, to, progressUpdater) -> {\n+                                    final long startTimeNanos = stats.currentTimeNanos();\n+                                    final BytesRefIterator iterator = cachedBlob.bytes()\n+                                        .slice(Math.toIntExact(from - cachedBlob.from()), Math.toIntExact(to - from))\n+                                        .iterator();\n+                                    long writePosition = from;\n+                                    BytesRef current;\n+                                    while ((current = iterator.next()) != null) {\n+                                        final ByteBuffer byteBuffer = ByteBuffer.wrap(current.bytes, current.offset, current.length);\n+                                        while (byteBuffer.remaining() > 0) {\n+                                            writePosition += positionalWrite(channel, writePosition, byteBuffer);\n+                                            progressUpdater.accept(writePosition);\n+                                        }\n+                                    }\n+                                    assert writePosition == to : writePosition + \" vs \" + to;\n+                                    final long endTimeNanos = stats.currentTimeNanos();\n+                                    stats.addCachedBytesWritten(to - from, endTimeNanos - startTimeNanos);\n+                                    logger.trace(\"copied bytes [{}-{}] of file [{}] from cache index to disk\", from, to, fileInfo);\n+                                },\n+                                directory.cacheFetchAsyncExecutor()\n+                            );\n+                        } catch (Exception e) {\n+                            logger.debug(\n+                                new ParameterizedMessage(\n+                                    \"failed to store bytes [{}-{}] of file [{}] obtained from index cache\",\n+                                    cachedBlob.from(),\n+                                    cachedBlob.to(),\n+                                    fileInfo\n+                                ),\n+                                e\n+                            );\n+                            // oh well, no big deal, at least we can return them to the caller.\n+                        }\n+\n+                        readComplete(position, length);\n+\n+                        return;\n+                    }\n+                } else {\n+                    // requested range is not eligible for caching\n+                    indexCacheMiss = null;\n                 }\n-            } catch (final Exception e) {\n-                if (e instanceof AlreadyClosedException || (e.getCause() != null && e.getCause() instanceof AlreadyClosedException)) {\n-                    try {\n-                        // cache file was evicted during the range fetching, read bytes directly from source\n-                        bytesRead = readDirectly(pos, pos + len, b);\n-                        continue;\n-                    } catch (Exception inner) {\n-                        e.addSuppressed(inner);\n+\n+                // Requested data is also not in the cache index, so we must visit the blob store to satisfy both the target range and any\n+                // miss in the cache index.\n+\n+                final Tuple<Long, Long> startRangeToWrite = computeRange(position);\n+                final Tuple<Long, Long> endRangeToWrite = computeRange(position + length - 1);\n+                assert startRangeToWrite.v2() <= endRangeToWrite.v2() : startRangeToWrite + \" vs \" + endRangeToWrite;\n+                final Tuple<Long, Long> rangeToWrite = Tuple.tuple(\n+                    Math.min(startRangeToWrite.v1(), indexCacheMiss == null ? Long.MAX_VALUE : indexCacheMiss.v1()),\n+                    Math.max(endRangeToWrite.v2(), indexCacheMiss == null ? Long.MIN_VALUE : indexCacheMiss.v2())\n+                );\n+\n+                assert rangeToWrite.v1() <= position && position + length <= rangeToWrite.v2() : \"[\"\n+                    + position\n+                    + \"-\"\n+                    + (position + length)\n+                    + \"] vs \"\n+                    + rangeToWrite;\n+                final Tuple<Long, Long> rangeToRead = Tuple.tuple(position, position + length);\n+\n+                final CompletableFuture<Integer> populateCacheFuture = cacheFile.populateAndRead(rangeToWrite, rangeToRead, channel -> {\n+                    final int read;\n+                    if ((rangeToRead.v2() - rangeToRead.v1()) < b.remaining()) {\n+                        final ByteBuffer duplicate = b.duplicate();\n+                        duplicate.limit(duplicate.position() + Math.toIntExact(rangeToRead.v2() - rangeToRead.v1()));\n+                        read = readCacheFile(channel, position, duplicate);\n+                        assert duplicate.position() <= b.limit();\n+                        b.position(duplicate.position());\n+                    } else {\n+                        read = readCacheFile(channel, position, b);\n+                    }\n+                    return read;\n+                }, this::writeCacheFile, directory.cacheFetchAsyncExecutor());\n+\n+                if (indexCacheMiss != null) {\n+                    final Releasable onCacheFillComplete = stats.addIndexCacheFill();\n+                    final CompletableFuture<Integer> readFuture = cacheFile.readIfAvailableOrPending(indexCacheMiss, channel -> {\n+                        final int indexCacheMissLength = Math.toIntExact(indexCacheMiss.v2() - indexCacheMiss.v1());\n+\n+                        // We assume that we only cache small portions of blobs so that we do not need to:\n+                        // - use a BigArrays for allocation\n+                        // - use an intermediate copy buffer to read the file in sensibly-sized chunks\n+                        // - release the buffer once the indexing operation is complete\n+                        assert indexCacheMissLength <= COPY_BUFFER_SIZE : indexCacheMiss;\n+\n+                        final ByteBuffer byteBuffer = ByteBuffer.allocate(indexCacheMissLength);\n+                        Channels.readFromFileChannelWithEofException(channel, indexCacheMiss.v1(), byteBuffer);\n+                        // NB use Channels.readFromFileChannelWithEofException not readCacheFile() to avoid counting this in the stats\n+                        byteBuffer.flip();\n+                        final BytesReference content = BytesReference.fromByteBuffer(byteBuffer);\n+                        directory.putCachedBlob(fileInfo.physicalName(), indexCacheMiss.v1(), content, new ActionListener<>() {\n+                            @Override\n+                            public void onResponse(Void response) {\n+                                onCacheFillComplete.close();\n+                            }\n+\n+                            @Override\n+                            public void onFailure(Exception e1) {\n+                                onCacheFillComplete.close();\n+                            }\n+                        });", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "790bdc10bc4090c4bf41510057cda0e9b6fffc9d"}, "originalPosition": 268}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE1MjA5MQ==", "bodyText": "Would it be possible to check that readFuture is effectively done in case of indexCacheMiss not null?", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r477152091", "createdAt": "2020-08-26T09:09:50Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -136,53 +148,296 @@ protected void readInternal(ByteBuffer b) throws IOException {\n         final long position = getFilePointer() + this.offset;\n         final int length = b.remaining();\n \n-        int totalBytesRead = 0;\n-        while (totalBytesRead < length) {\n-            final long pos = position + totalBytesRead;\n-            final int len = length - totalBytesRead;\n-            int bytesRead = 0;\n-            try {\n-                final CacheFile cacheFile = getCacheFileSafe();\n-                try (Releasable ignored = cacheFile.fileLock()) {\n-                    final Tuple<Long, Long> rangeToWrite = computeRange(pos);\n-                    final Tuple<Long, Long> rangeToRead = Tuple.tuple(pos, Math.min(pos + len, rangeToWrite.v2()));\n-\n-                    bytesRead = cacheFile.fetchAsync(rangeToWrite, rangeToRead, (channel) -> {\n-                        final int read;\n-                        if ((rangeToRead.v2() - rangeToRead.v1()) < b.remaining()) {\n-                            final ByteBuffer duplicate = b.duplicate();\n-                            duplicate.limit(duplicate.position() + Math.toIntExact(rangeToRead.v2() - rangeToRead.v1()));\n-                            read = readCacheFile(channel, pos, duplicate);\n-                            assert duplicate.position() <= b.limit();\n-                            b.position(duplicate.position());\n+        // We can detect that we're going to read the last 16 bytes (that contains the footer checksum) of the file. Such reads are often\n+        // executed when opening a Directory and since we have the checksum in the snapshot metadata we can use it to fill the ByteBuffer.\n+        if (length == CodecUtil.footerLength() && isClone == false && position == fileInfo.length() - length) {\n+            if (readChecksumFromFileInfo(b)) {\n+                logger.trace(\"read footer of file [{}] at position [{}], bypassing all caches\", fileInfo.physicalName(), position);\n+                return;\n+            }\n+            assert b.remaining() == length;\n+        }\n+\n+        logger.trace(\"readInternal: read [{}-{}] ([{}] bytes) from [{}]\", position, position + length, length, this);\n+\n+        try {\n+            final CacheFile cacheFile = getCacheFileSafe();\n+            try (Releasable ignored = cacheFile.fileLock()) {\n+\n+                // Can we serve the read directly from disk? If so, do so and don't worry about anything else.\n+\n+                final CompletableFuture<Integer> waitingForRead = cacheFile.readIfAvailableOrPending(\n+                    Tuple.tuple(position, position + length),\n+                    channel -> {\n+                        final int read = readCacheFile(channel, position, b);\n+                        assert read == length : read + \" vs \" + length;\n+                        return read;\n+                    }\n+                );\n+\n+                if (waitingForRead != null) {\n+                    final Integer read = waitingForRead.get();\n+                    assert read == length;\n+                    readComplete(position, length);\n+                    return;\n+                }\n+\n+                // Requested data is not on disk, so try the cache index next.\n+\n+                final Tuple<Long, Long> indexCacheMiss; // null if not a miss\n+\n+                // We try to use the cache index if:\n+                // - the file is small enough to be fully cached\n+                final boolean canBeFullyCached = fileInfo.length() <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE * 2;\n+                // - we're reading the first N bytes of the file\n+                final boolean isStartOfFile = (position + length <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE);\n+\n+                if (canBeFullyCached || isStartOfFile) {\n+                    final CachedBlob cachedBlob = directory.getCachedBlob(fileInfo.physicalName(), 0L, length);\n+\n+                    if (cachedBlob == CachedBlob.CACHE_MISS || cachedBlob == CachedBlob.CACHE_NOT_READY) {\n+                        // We would have liked to find a cached entry but we did not find anything: the cache on the disk will be requested\n+                        // so we compute the region of the file we would like to have the next time. The region is expressed as a tuple of\n+                        // {start, end} where positions are relative to the whole file.\n+\n+                        if (canBeFullyCached) {\n+                            // if the index input is smaller than twice the size of the blob cache, it will be fully indexed\n+                            indexCacheMiss = Tuple.tuple(0L, fileInfo.length());\n                         } else {\n-                            read = readCacheFile(channel, pos, b);\n+                            // the index input is too large to fully cache, so just cache the initial range\n+                            indexCacheMiss = Tuple.tuple(0L, (long) BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE);\n                         }\n-                        return read;\n-                    }, this::writeCacheFile, directory.cacheFetchAsyncExecutor()).get();\n+\n+                        // We must fill in a cache miss even if CACHE_NOT_READY since the cache index is only created on the first put.\n+                        // TODO TBD use a different trigger for creating the cache index and avoid a put in the CACHE_NOT_READY case.\n+                    } else {\n+                        logger.trace(\n+                            \"reading [{}] bytes of file [{}] at position [{}] using cache index\",\n+                            length,\n+                            fileInfo.physicalName(),\n+                            position\n+                        );\n+                        stats.addIndexCacheBytesRead(cachedBlob.length());\n+\n+                        final BytesRefIterator cachedBytesIterator = cachedBlob.bytes().slice(Math.toIntExact(position), length).iterator();\n+                        BytesRef bytesRef;\n+                        while ((bytesRef = cachedBytesIterator.next()) != null) {\n+                            b.put(bytesRef.bytes, bytesRef.offset, bytesRef.length);\n+                        }\n+                        assert b.position() == length : \"copied \" + b.position() + \" but expected \" + length;\n+\n+                        try {\n+                            final Tuple<Long, Long> cachedRange = Tuple.tuple(cachedBlob.from(), cachedBlob.to());\n+                            cacheFile.populateAndRead(\n+                                cachedRange,\n+                                cachedRange,\n+                                channel -> cachedBlob.length(),\n+                                (channel, from, to, progressUpdater) -> {\n+                                    final long startTimeNanos = stats.currentTimeNanos();\n+                                    final BytesRefIterator iterator = cachedBlob.bytes()\n+                                        .slice(Math.toIntExact(from - cachedBlob.from()), Math.toIntExact(to - from))\n+                                        .iterator();\n+                                    long writePosition = from;\n+                                    BytesRef current;\n+                                    while ((current = iterator.next()) != null) {\n+                                        final ByteBuffer byteBuffer = ByteBuffer.wrap(current.bytes, current.offset, current.length);\n+                                        while (byteBuffer.remaining() > 0) {\n+                                            writePosition += positionalWrite(channel, writePosition, byteBuffer);\n+                                            progressUpdater.accept(writePosition);\n+                                        }\n+                                    }\n+                                    assert writePosition == to : writePosition + \" vs \" + to;\n+                                    final long endTimeNanos = stats.currentTimeNanos();\n+                                    stats.addCachedBytesWritten(to - from, endTimeNanos - startTimeNanos);\n+                                    logger.trace(\"copied bytes [{}-{}] of file [{}] from cache index to disk\", from, to, fileInfo);\n+                                },\n+                                directory.cacheFetchAsyncExecutor()\n+                            );\n+                        } catch (Exception e) {\n+                            logger.debug(\n+                                new ParameterizedMessage(\n+                                    \"failed to store bytes [{}-{}] of file [{}] obtained from index cache\",\n+                                    cachedBlob.from(),\n+                                    cachedBlob.to(),\n+                                    fileInfo\n+                                ),\n+                                e\n+                            );\n+                            // oh well, no big deal, at least we can return them to the caller.\n+                        }\n+\n+                        readComplete(position, length);\n+\n+                        return;\n+                    }\n+                } else {\n+                    // requested range is not eligible for caching\n+                    indexCacheMiss = null;\n                 }\n-            } catch (final Exception e) {\n-                if (e instanceof AlreadyClosedException || (e.getCause() != null && e.getCause() instanceof AlreadyClosedException)) {\n-                    try {\n-                        // cache file was evicted during the range fetching, read bytes directly from source\n-                        bytesRead = readDirectly(pos, pos + len, b);\n-                        continue;\n-                    } catch (Exception inner) {\n-                        e.addSuppressed(inner);\n+\n+                // Requested data is also not in the cache index, so we must visit the blob store to satisfy both the target range and any\n+                // miss in the cache index.\n+\n+                final Tuple<Long, Long> startRangeToWrite = computeRange(position);\n+                final Tuple<Long, Long> endRangeToWrite = computeRange(position + length - 1);\n+                assert startRangeToWrite.v2() <= endRangeToWrite.v2() : startRangeToWrite + \" vs \" + endRangeToWrite;\n+                final Tuple<Long, Long> rangeToWrite = Tuple.tuple(\n+                    Math.min(startRangeToWrite.v1(), indexCacheMiss == null ? Long.MAX_VALUE : indexCacheMiss.v1()),\n+                    Math.max(endRangeToWrite.v2(), indexCacheMiss == null ? Long.MIN_VALUE : indexCacheMiss.v2())\n+                );\n+\n+                assert rangeToWrite.v1() <= position && position + length <= rangeToWrite.v2() : \"[\"\n+                    + position\n+                    + \"-\"\n+                    + (position + length)\n+                    + \"] vs \"\n+                    + rangeToWrite;\n+                final Tuple<Long, Long> rangeToRead = Tuple.tuple(position, position + length);\n+\n+                final CompletableFuture<Integer> populateCacheFuture = cacheFile.populateAndRead(rangeToWrite, rangeToRead, channel -> {\n+                    final int read;\n+                    if ((rangeToRead.v2() - rangeToRead.v1()) < b.remaining()) {\n+                        final ByteBuffer duplicate = b.duplicate();\n+                        duplicate.limit(duplicate.position() + Math.toIntExact(rangeToRead.v2() - rangeToRead.v1()));\n+                        read = readCacheFile(channel, position, duplicate);\n+                        assert duplicate.position() <= b.limit();\n+                        b.position(duplicate.position());\n+                    } else {\n+                        read = readCacheFile(channel, position, b);\n+                    }\n+                    return read;\n+                }, this::writeCacheFile, directory.cacheFetchAsyncExecutor());\n+\n+                if (indexCacheMiss != null) {\n+                    final Releasable onCacheFillComplete = stats.addIndexCacheFill();\n+                    final CompletableFuture<Integer> readFuture = cacheFile.readIfAvailableOrPending(indexCacheMiss, channel -> {\n+                        final int indexCacheMissLength = Math.toIntExact(indexCacheMiss.v2() - indexCacheMiss.v1());\n+\n+                        // We assume that we only cache small portions of blobs so that we do not need to:\n+                        // - use a BigArrays for allocation\n+                        // - use an intermediate copy buffer to read the file in sensibly-sized chunks\n+                        // - release the buffer once the indexing operation is complete\n+                        assert indexCacheMissLength <= COPY_BUFFER_SIZE : indexCacheMiss;\n+\n+                        final ByteBuffer byteBuffer = ByteBuffer.allocate(indexCacheMissLength);\n+                        Channels.readFromFileChannelWithEofException(channel, indexCacheMiss.v1(), byteBuffer);\n+                        // NB use Channels.readFromFileChannelWithEofException not readCacheFile() to avoid counting this in the stats\n+                        byteBuffer.flip();\n+                        final BytesReference content = BytesReference.fromByteBuffer(byteBuffer);\n+                        directory.putCachedBlob(fileInfo.physicalName(), indexCacheMiss.v1(), content, new ActionListener<>() {\n+                            @Override\n+                            public void onResponse(Void response) {\n+                                onCacheFillComplete.close();\n+                            }\n+\n+                            @Override\n+                            public void onFailure(Exception e1) {\n+                                onCacheFillComplete.close();\n+                            }\n+                        });\n+                        return indexCacheMissLength;\n+                    });\n+\n+                    if (readFuture == null) {\n+                        // Normally doesn't happen, we're already obtaining a range covering all cache misses above, but theoretically\n+                        // possible in the case that the real populateAndRead call already failed to obtain this range of the file. In that\n+                        // case, simply move on.\n+                        onCacheFillComplete.close();\n                     }\n                 }\n-                throw new IOException(\"Fail to read data from cache\", e);\n \n-            } finally {\n-                totalBytesRead += bytesRead;\n+                final int bytesRead = populateCacheFuture.get();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "790bdc10bc4090c4bf41510057cda0e9b6fffc9d"}, "originalPosition": 283}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE3Nzk0NQ==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r477177945", "createdAt": "2020-08-26T09:49:35Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java", "diffHunk": "@@ -0,0 +1,451 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.elasticsearch.action.admin.indices.forcemerge.ForceMergeResponse;\n+import org.elasticsearch.action.admin.indices.refresh.RefreshResponse;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.index.IndexRequestBuilder;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.routing.RoutingNode;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;\n+import org.elasticsearch.cluster.routing.allocation.decider.AllocationDecider;\n+import org.elasticsearch.cluster.routing.allocation.decider.Decision;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.ClusterSettings;\n+import org.elasticsearch.common.settings.Setting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.index.IndexNotFoundException;\n+import org.elasticsearch.index.query.QueryBuilders;\n+import org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardSnapshot;\n+import org.elasticsearch.plugins.ClusterPlugin;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.xpack.core.ClientHelper;\n+import org.elasticsearch.xpack.core.searchablesnapshots.SearchableSnapshotShardStats;\n+import org.elasticsearch.xpack.searchablesnapshots.BaseSearchableSnapshotsIntegTestCase;\n+import org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots;\n+import org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants;\n+import org.elasticsearch.xpack.searchablesnapshots.action.SearchableSnapshotsStatsAction;\n+import org.elasticsearch.xpack.searchablesnapshots.action.SearchableSnapshotsStatsRequest;\n+import org.elasticsearch.xpack.searchablesnapshots.cache.CacheService;\n+\n+import java.io.IOException;\n+import java.nio.file.FileVisitResult;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.SimpleFileVisitor;\n+import java.nio.file.attribute.BasicFileAttributes;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+\n+import static org.elasticsearch.repositories.blobstore.BlobStoreRepository.INDEX_SHARD_SNAPSHOT_FORMAT;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.SNAPSHOT_BLOB_CACHE_INDEX;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.is;\n+\n+public class SearchableSnapshotsBlobStoreCacheIntegTests extends BaseSearchableSnapshotsIntegTestCase {\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        final List<Class<? extends Plugin>> plugins = new ArrayList<>();\n+        plugins.add(WaitForSnapshotBlobCacheShardsActivePlugin.class);\n+        plugins.addAll(super.nodePlugins());\n+        return List.copyOf(plugins);\n+    }\n+\n+    @Override\n+    protected int numberOfReplicas() {\n+        return 0;\n+    }\n+\n+    @Override\n+    protected Settings nodeSettings(int nodeOrdinal) {\n+        return Settings.builder()\n+            .put(super.nodeSettings(nodeOrdinal))\n+            .put(\n+                CacheService.SNAPSHOT_CACHE_RANGE_SIZE_SETTING.getKey(),\n+                randomLongBetween(new ByteSizeValue(4, ByteSizeUnit.KB).getBytes(), new ByteSizeValue(20, ByteSizeUnit.KB).getBytes()) + \"b\"\n+            )\n+            .put(CacheService.SNAPSHOT_CACHE_SIZE_SETTING.getKey(), new ByteSizeValue(Long.MAX_VALUE, ByteSizeUnit.BYTES))\n+            .build();\n+    }\n+\n+    public void testBlobStoreCache() throws Exception {\n+        final String indexName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT);\n+        createIndex(indexName);\n+\n+        final List<IndexRequestBuilder> indexRequestBuilders = new ArrayList<>();\n+        for (int i = scaledRandomIntBetween(0, 10_000); i >= 0; i--) {\n+            indexRequestBuilders.add(client().prepareIndex(indexName).setSource(\"text\", randomUnicodeOfLength(10), \"num\", i));\n+        }\n+        indexRandom(true, false, true, indexRequestBuilders);\n+        final long numberOfDocs = indexRequestBuilders.size();\n+        final NumShards numberOfShards = getNumShards(indexName);\n+\n+        if (randomBoolean()) {\n+            logger.info(\"--> force-merging index before snapshotting\");\n+            final ForceMergeResponse forceMergeResponse = client().admin()\n+                .indices()\n+                .prepareForceMerge(indexName)\n+                .setMaxNumSegments(1)\n+                .get();\n+            assertThat(forceMergeResponse.getSuccessfulShards(), equalTo(numberOfShards.totalNumShards));\n+            assertThat(forceMergeResponse.getFailedShards(), equalTo(0));\n+        }\n+\n+        final String repositoryName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT);\n+        final Path repositoryLocation = randomRepoPath();\n+        createFsRepository(repositoryName, repositoryLocation);\n+\n+        final SnapshotId snapshot = createSnapshot(repositoryName, List.of(indexName));\n+        assertAcked(client().admin().indices().prepareDelete(indexName));\n+\n+        // extract the list of blobs per shard from the snapshot directory on disk\n+        final Map<String, BlobStoreIndexShardSnapshot> blobsInSnapshot = blobsInSnapshot(repositoryLocation, snapshot.getUUID());\n+        assertThat(\"Failed to load all shard snapshot metadata files\", blobsInSnapshot.size(), equalTo(numberOfShards.numPrimaries));\n+\n+        expectThrows(\n+            IndexNotFoundException.class,\n+            \".snapshot-blob-cache system index should not be created yet\",\n+            () -> systemClient().admin().indices().prepareGetIndex().addIndices(SNAPSHOT_BLOB_CACHE_INDEX).get()\n+        );\n+\n+        logger.info(\"--> mount snapshot [{}] as an index for the first time\", snapshot);\n+        final String restoredIndex = mountSnapshot(\n+            repositoryName,\n+            snapshot.getName(),\n+            indexName,\n+            Settings.builder()\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_ENABLED_SETTING.getKey(), true)\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_PREWARM_ENABLED_SETTING.getKey(), false)\n+                .build()\n+        );\n+        ensureGreen(restoredIndex);\n+\n+        // wait for all async cache fills to complete\n+        assertBusy(() -> {\n+            for (final SearchableSnapshotShardStats shardStats : client().execute(\n+                SearchableSnapshotsStatsAction.INSTANCE,\n+                new SearchableSnapshotsStatsRequest()\n+            ).actionGet().getStats()) {\n+                for (final SearchableSnapshotShardStats.CacheIndexInputStats indexInputStats : shardStats.getStats()) {\n+                    assertThat(Strings.toString(indexInputStats), indexInputStats.getCurrentIndexCacheFills(), equalTo(0L));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "790bdc10bc4090c4bf41510057cda0e9b6fffc9d"}, "originalPosition": 152}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE4MDI0Mg==", "bodyText": "nit: duplicate", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r477180242", "createdAt": "2020-08-26T09:53:27Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java", "diffHunk": "@@ -0,0 +1,451 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.elasticsearch.action.admin.indices.forcemerge.ForceMergeResponse;\n+import org.elasticsearch.action.admin.indices.refresh.RefreshResponse;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.index.IndexRequestBuilder;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.routing.RoutingNode;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;\n+import org.elasticsearch.cluster.routing.allocation.decider.AllocationDecider;\n+import org.elasticsearch.cluster.routing.allocation.decider.Decision;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.ClusterSettings;\n+import org.elasticsearch.common.settings.Setting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.index.IndexNotFoundException;\n+import org.elasticsearch.index.query.QueryBuilders;\n+import org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardSnapshot;\n+import org.elasticsearch.plugins.ClusterPlugin;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.xpack.core.ClientHelper;\n+import org.elasticsearch.xpack.core.searchablesnapshots.SearchableSnapshotShardStats;\n+import org.elasticsearch.xpack.searchablesnapshots.BaseSearchableSnapshotsIntegTestCase;\n+import org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots;\n+import org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants;\n+import org.elasticsearch.xpack.searchablesnapshots.action.SearchableSnapshotsStatsAction;\n+import org.elasticsearch.xpack.searchablesnapshots.action.SearchableSnapshotsStatsRequest;\n+import org.elasticsearch.xpack.searchablesnapshots.cache.CacheService;\n+\n+import java.io.IOException;\n+import java.nio.file.FileVisitResult;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.SimpleFileVisitor;\n+import java.nio.file.attribute.BasicFileAttributes;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+\n+import static org.elasticsearch.repositories.blobstore.BlobStoreRepository.INDEX_SHARD_SNAPSHOT_FORMAT;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.SNAPSHOT_BLOB_CACHE_INDEX;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.is;\n+\n+public class SearchableSnapshotsBlobStoreCacheIntegTests extends BaseSearchableSnapshotsIntegTestCase {\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        final List<Class<? extends Plugin>> plugins = new ArrayList<>();\n+        plugins.add(WaitForSnapshotBlobCacheShardsActivePlugin.class);\n+        plugins.addAll(super.nodePlugins());\n+        return List.copyOf(plugins);\n+    }\n+\n+    @Override\n+    protected int numberOfReplicas() {\n+        return 0;\n+    }\n+\n+    @Override\n+    protected Settings nodeSettings(int nodeOrdinal) {\n+        return Settings.builder()\n+            .put(super.nodeSettings(nodeOrdinal))\n+            .put(\n+                CacheService.SNAPSHOT_CACHE_RANGE_SIZE_SETTING.getKey(),\n+                randomLongBetween(new ByteSizeValue(4, ByteSizeUnit.KB).getBytes(), new ByteSizeValue(20, ByteSizeUnit.KB).getBytes()) + \"b\"\n+            )\n+            .put(CacheService.SNAPSHOT_CACHE_SIZE_SETTING.getKey(), new ByteSizeValue(Long.MAX_VALUE, ByteSizeUnit.BYTES))\n+            .build();\n+    }\n+\n+    public void testBlobStoreCache() throws Exception {\n+        final String indexName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT);\n+        createIndex(indexName);\n+\n+        final List<IndexRequestBuilder> indexRequestBuilders = new ArrayList<>();\n+        for (int i = scaledRandomIntBetween(0, 10_000); i >= 0; i--) {\n+            indexRequestBuilders.add(client().prepareIndex(indexName).setSource(\"text\", randomUnicodeOfLength(10), \"num\", i));\n+        }\n+        indexRandom(true, false, true, indexRequestBuilders);\n+        final long numberOfDocs = indexRequestBuilders.size();\n+        final NumShards numberOfShards = getNumShards(indexName);\n+\n+        if (randomBoolean()) {\n+            logger.info(\"--> force-merging index before snapshotting\");\n+            final ForceMergeResponse forceMergeResponse = client().admin()\n+                .indices()\n+                .prepareForceMerge(indexName)\n+                .setMaxNumSegments(1)\n+                .get();\n+            assertThat(forceMergeResponse.getSuccessfulShards(), equalTo(numberOfShards.totalNumShards));\n+            assertThat(forceMergeResponse.getFailedShards(), equalTo(0));\n+        }\n+\n+        final String repositoryName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT);\n+        final Path repositoryLocation = randomRepoPath();\n+        createFsRepository(repositoryName, repositoryLocation);\n+\n+        final SnapshotId snapshot = createSnapshot(repositoryName, List.of(indexName));\n+        assertAcked(client().admin().indices().prepareDelete(indexName));\n+\n+        // extract the list of blobs per shard from the snapshot directory on disk\n+        final Map<String, BlobStoreIndexShardSnapshot> blobsInSnapshot = blobsInSnapshot(repositoryLocation, snapshot.getUUID());\n+        assertThat(\"Failed to load all shard snapshot metadata files\", blobsInSnapshot.size(), equalTo(numberOfShards.numPrimaries));\n+\n+        expectThrows(\n+            IndexNotFoundException.class,\n+            \".snapshot-blob-cache system index should not be created yet\",\n+            () -> systemClient().admin().indices().prepareGetIndex().addIndices(SNAPSHOT_BLOB_CACHE_INDEX).get()\n+        );\n+\n+        logger.info(\"--> mount snapshot [{}] as an index for the first time\", snapshot);\n+        final String restoredIndex = mountSnapshot(\n+            repositoryName,\n+            snapshot.getName(),\n+            indexName,\n+            Settings.builder()\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_ENABLED_SETTING.getKey(), true)\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_PREWARM_ENABLED_SETTING.getKey(), false)\n+                .build()\n+        );\n+        ensureGreen(restoredIndex);\n+\n+        // wait for all async cache fills to complete\n+        assertBusy(() -> {\n+            for (final SearchableSnapshotShardStats shardStats : client().execute(\n+                SearchableSnapshotsStatsAction.INSTANCE,\n+                new SearchableSnapshotsStatsRequest()\n+            ).actionGet().getStats()) {\n+                for (final SearchableSnapshotShardStats.CacheIndexInputStats indexInputStats : shardStats.getStats()) {\n+                    assertThat(Strings.toString(indexInputStats), indexInputStats.getCurrentIndexCacheFills(), equalTo(0L));\n+                }\n+            }\n+        });\n+\n+        for (final SearchableSnapshotShardStats shardStats : client().execute(\n+            SearchableSnapshotsStatsAction.INSTANCE,\n+            new SearchableSnapshotsStatsRequest()\n+        ).actionGet().getStats()) {\n+            for (final SearchableSnapshotShardStats.CacheIndexInputStats indexInputStats : shardStats.getStats()) {\n+                assertThat(Strings.toString(indexInputStats), indexInputStats.getBlobStoreBytesRequested().getCount(), greaterThan(0L));\n+            }\n+        }\n+\n+        logger.info(\"--> verifying cached documents in system index [{}]\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        assertCachedBlobsInSystemIndex(repositoryName, blobsInSnapshot);\n+\n+        final long numberOfCachedBlobs = systemClient().prepareSearch(SNAPSHOT_BLOB_CACHE_INDEX).get().getHits().getTotalHits().value;\n+        final long numberOfCacheWrites = systemClient().admin()\n+            .indices()\n+            .prepareStats(SNAPSHOT_BLOB_CACHE_INDEX)\n+            .clear()\n+            .setIndexing(true)\n+            .get()\n+            .getTotal().indexing.getTotal().getIndexCount();\n+\n+        logger.info(\"--> verifying documents in index [{}]\", restoredIndex);\n+        assertHitCount(client().prepareSearch(restoredIndex).setSize(0).setTrackTotalHits(true).get(), numberOfDocs);\n+        assertHitCount(\n+            client().prepareSearch(restoredIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").lte(numberOfDocs))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            numberOfDocs\n+        );\n+        assertHitCount(\n+            client().prepareSearch(restoredIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").gt(numberOfDocs + 1))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            0L\n+        );\n+\n+        assertAcked(client().admin().indices().prepareDelete(restoredIndex));\n+\n+        logger.info(\"--> mount snapshot [{}] as an index for the second time\", snapshot);\n+        final String restoredAgainIndex = mountSnapshot(\n+            repositoryName,\n+            snapshot.getName(),\n+            indexName,\n+            Settings.builder()\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_ENABLED_SETTING.getKey(), true)\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_PREWARM_ENABLED_SETTING.getKey(), false)\n+                .build()\n+        );\n+        ensureGreen(restoredAgainIndex);\n+\n+        logger.info(\"--> verifying shards of [{}] were started without using the blob store more than necessary\", restoredAgainIndex);\n+        for (final SearchableSnapshotShardStats shardStats : client().execute(\n+            SearchableSnapshotsStatsAction.INSTANCE,\n+            new SearchableSnapshotsStatsRequest()\n+        ).actionGet().getStats()) {\n+            for (final SearchableSnapshotShardStats.CacheIndexInputStats indexInputStats : shardStats.getStats()) {\n+                final boolean mayReadMoreThanHeader\n+                // we read the header of each file contained within the .cfs file, which could be anywhere\n+                    = indexInputStats.getFileName().endsWith(\".cfs\")\n+                        // we read a couple of longs at the end of the .fdt file (see https://issues.apache.org/jira/browse/LUCENE-9456)\n+                        // TODO revisit this when this issue is addressed in Lucene\n+                        || indexInputStats.getFileName().endsWith(\".fdt\");\n+                if (indexInputStats.getFileLength() <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE * 2\n+                    || mayReadMoreThanHeader == false) {\n+                    assertThat(Strings.toString(indexInputStats), indexInputStats.getBlobStoreBytesRequested().getCount(), equalTo(0L));\n+                }\n+            }\n+        }\n+\n+        logger.info(\"--> verifying documents in index [{}]\", restoredAgainIndex);\n+        assertHitCount(client().prepareSearch(restoredAgainIndex).setSize(0).setTrackTotalHits(true).get(), numberOfDocs);\n+        assertHitCount(\n+            client().prepareSearch(restoredAgainIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").lte(numberOfDocs))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            numberOfDocs\n+        );\n+        assertHitCount(\n+            client().prepareSearch(restoredAgainIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").gt(numberOfDocs + 1))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            0L\n+        );\n+\n+        logger.info(\"--> verifying cached documents (again) in system index [{}]\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        assertCachedBlobsInSystemIndex(repositoryName, blobsInSnapshot);\n+\n+        logger.info(\"--> verifying that no extra cached blobs were indexed [{}]\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        refreshSystemIndex();\n+        assertHitCount(systemClient().prepareSearch(SNAPSHOT_BLOB_CACHE_INDEX).setSize(0).get(), numberOfCachedBlobs);\n+        assertThat(\n+            systemClient().admin().indices().prepareStats(SNAPSHOT_BLOB_CACHE_INDEX).clear().setIndexing(true).get().getTotal().indexing\n+                .getTotal()\n+                .getIndexCount(),\n+            equalTo(numberOfCacheWrites)\n+        );\n+\n+        logger.info(\"--> restarting cluster\");\n+        internalCluster().fullRestart(new InternalTestCluster.RestartCallback() {\n+            @Override\n+            public Settings onNodeStopped(String nodeName) throws Exception {\n+                return Settings.builder()\n+                    .put(super.onNodeStopped(nodeName))\n+                    .put(WaitForSnapshotBlobCacheShardsActivePlugin.ENABLED.getKey(), true)\n+                    .build();\n+            }\n+        });\n+        ensureGreen(restoredAgainIndex);\n+\n+        logger.info(\"--> verifying documents in index [{}]\", restoredAgainIndex);\n+        assertHitCount(client().prepareSearch(restoredAgainIndex).setSize(0).setTrackTotalHits(true).get(), numberOfDocs);\n+        assertHitCount(client().prepareSearch(restoredAgainIndex).setSize(0).setTrackTotalHits(true).get(), numberOfDocs);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "790bdc10bc4090c4bf41510057cda0e9b6fffc9d"}, "originalPosition": 276}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "eb1a08ec99bde821f3a131ef6e684cd1140b15e7", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/eb1a08ec99bde821f3a131ef6e684cd1140b15e7", "committedDate": "2020-08-26T11:09:52Z", "message": "Time out cache get after 5s"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "deda3054dbb45980c1d8e332ba54a16e352afe4c", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/deda3054dbb45980c1d8e332ba54a16e352afe4c", "committedDate": "2020-08-26T11:20:45Z", "message": "Assert not blocking a SYSTEM_READ thread"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dad5dfd471dbeb82bcb54fcf45ce719cd64ff7f6", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/dad5dfd471dbeb82bcb54fcf45ce719cd64ff7f6", "committedDate": "2020-08-26T11:22:14Z", "message": "Spotless"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "99045e06e7052a892a8faa88ad6c63c19fed0ae4", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/99045e06e7052a892a8faa88ad6c63c19fed0ae4", "committedDate": "2020-08-26T11:34:03Z", "message": "Delimiters"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2003ec07b8a22409ba94675ce942530eb7faad95", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/2003ec07b8a22409ba94675ce942530eb7faad95", "committedDate": "2020-08-26T11:48:27Z", "message": "Add stats to tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "62fe090c99be6cc15b3f0def26763c10bf3beabe", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/62fe090c99be6cc15b3f0def26763c10bf3beabe", "committedDate": "2020-08-26T11:51:55Z", "message": "Catch exception during indexing"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2c2a7c0c6d7174a0818a1baa15c30d58daacbdca", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/2c2a7c0c6d7174a0818a1baa15c30d58daacbdca", "committedDate": "2020-08-26T11:54:58Z", "message": "Remove duplicate assertion"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bb6d4abd97cdfa1db803683d1cf30324e5f7fd6a", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/bb6d4abd97cdfa1db803683d1cf30324e5f7fd6a", "committedDate": "2020-08-26T12:02:12Z", "message": "Another test needing cleanup"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc1NTUzODYx", "url": "https://github.com/elastic/elasticsearch/pull/60522#pullrequestreview-475553861", "createdAt": "2020-08-26T14:35:17Z", "commit": {"oid": "bb6d4abd97cdfa1db803683d1cf30324e5f7fd6a"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9e228c15a5075a562243bca9b5518b5ae32170cc", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/9e228c15a5075a562243bca9b5518b5ae32170cc", "committedDate": "2020-08-26T14:49:57Z", "message": "Account for 'any kind' reads from index cache"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc1NjMwOTgx", "url": "https://github.com/elastic/elasticsearch/pull/60522#pullrequestreview-475630981", "createdAt": "2020-08-26T15:55:29Z", "commit": {"oid": "bb6d4abd97cdfa1db803683d1cf30324e5f7fd6a"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3573, "cost": 1, "resetAt": "2021-10-28T18:54:27Z"}}}