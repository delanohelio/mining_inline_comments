{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzY0MzE1Njcw", "number": 51184, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQyMToyMzoyMlrODbuU0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQyMTo0MDoyMlrODbuowg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwMzk3MTM2OnYy", "diffSide": "RIGHT", "path": "docs/reference/analysis.asciidoc", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQyMToyMzoyMlrOFjX-Tg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQyMToyMzoyMlrOFjX-Tg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjYzNzI2Mg==", "bodyText": "smart defaults?   ES just uses a default (standard) analyzer for text fields,  which many not be the best option for many use cases.", "url": "https://github.com/elastic/elasticsearch/pull/51184#discussion_r372637262", "createdAt": "2020-01-29T21:23:22Z", "author": {"login": "mayya-sharipova"}, "path": "docs/reference/analysis.asciidoc", "diffHunk": "@@ -4,141 +4,42 @@\n [partintro]\n --\n \n-_Text analysis_ is the process of converting text, like the body of any email,\n-into _tokens_ or _terms_ which are added to the inverted index for searching.\n-Analysis is performed by an <<analysis-analyzers,_analyzer_>> which can be\n-either a built-in analyzer or a <<analysis-custom-analyzer,`custom`>> analyzer\n-defined per index.\n+_Text analysis_ is the process of converting unstructured text, like\n+the body of an email or a product description, into a structured format that's\n+optimized for search.\n \n [float]\n-== Index time analysis\n+[[when-to-configure-analysis]]\n+=== When to configure text analysis\n \n-For instance, at index time the built-in <<english-analyzer,`english`>> _analyzer_ \n-will first convert the sentence:\n+{es} comes with smart defaults for text analysis. These defaults work well for", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c65e22d62da25b6b47335aaeb5f4e919e9bea640"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwMzk3NTAxOnYy", "diffSide": "RIGHT", "path": "docs/reference/analysis.asciidoc", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQyMToyNDo1MFrOFjYA0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQyMjoyMDowMVrOFjZixw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjYzNzkwNA==", "bodyText": "I don't think that configuring an analyzer is an uncommon use case,  I would think it should be quite common for text fields.\nMay be to rephrase something like, if your index doesn't use text fields, you may skip chapters in this section.", "url": "https://github.com/elastic/elasticsearch/pull/51184#discussion_r372637904", "createdAt": "2020-01-29T21:24:50Z", "author": {"login": "mayya-sharipova"}, "path": "docs/reference/analysis.asciidoc", "diffHunk": "@@ -4,141 +4,42 @@\n [partintro]\n --\n \n-_Text analysis_ is the process of converting text, like the body of any email,\n-into _tokens_ or _terms_ which are added to the inverted index for searching.\n-Analysis is performed by an <<analysis-analyzers,_analyzer_>> which can be\n-either a built-in analyzer or a <<analysis-custom-analyzer,`custom`>> analyzer\n-defined per index.\n+_Text analysis_ is the process of converting unstructured text, like\n+the body of an email or a product description, into a structured format that's\n+optimized for search.\n \n [float]\n-== Index time analysis\n+[[when-to-configure-analysis]]\n+=== When to configure text analysis\n \n-For instance, at index time the built-in <<english-analyzer,`english`>> _analyzer_ \n-will first convert the sentence:\n+{es} comes with smart defaults for text analysis. These defaults work well for\n+most use cases and require no configuration.\n \n-[source,text]\n-------\n-\"The QUICK brown foxes jumped over the lazy dog!\"\n-------\n+If you're happy with your search experience, no further setup is needed;\n+you can skip the chapters in this section.\n \n-into distinct tokens. It will then lowercase each token, remove frequent\n-stopwords (\"the\") and reduce the terms to their word stems (foxes -> fox,\n-jumped -> jump, lazy -> lazi). In the end, the following terms will be added\n-to the inverted index:\n+However, there are less common cases where configuring text analysis is", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c65e22d62da25b6b47335aaeb5f4e919e9bea640"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjY2Mjk4Mw==", "bodyText": "Good point! It's clearer if we just directly state that that if you use text fields, take a look. If not, go ahead and skip this section.\nMade those changes with a2f08d2.", "url": "https://github.com/elastic/elasticsearch/pull/51184#discussion_r372662983", "createdAt": "2020-01-29T22:20:01Z", "author": {"login": "jrodewig"}, "path": "docs/reference/analysis.asciidoc", "diffHunk": "@@ -4,141 +4,42 @@\n [partintro]\n --\n \n-_Text analysis_ is the process of converting text, like the body of any email,\n-into _tokens_ or _terms_ which are added to the inverted index for searching.\n-Analysis is performed by an <<analysis-analyzers,_analyzer_>> which can be\n-either a built-in analyzer or a <<analysis-custom-analyzer,`custom`>> analyzer\n-defined per index.\n+_Text analysis_ is the process of converting unstructured text, like\n+the body of an email or a product description, into a structured format that's\n+optimized for search.\n \n [float]\n-== Index time analysis\n+[[when-to-configure-analysis]]\n+=== When to configure text analysis\n \n-For instance, at index time the built-in <<english-analyzer,`english`>> _analyzer_ \n-will first convert the sentence:\n+{es} comes with smart defaults for text analysis. These defaults work well for\n+most use cases and require no configuration.\n \n-[source,text]\n-------\n-\"The QUICK brown foxes jumped over the lazy dog!\"\n-------\n+If you're happy with your search experience, no further setup is needed;\n+you can skip the chapters in this section.\n \n-into distinct tokens. It will then lowercase each token, remove frequent\n-stopwords (\"the\") and reduce the terms to their word stems (foxes -> fox,\n-jumped -> jump, lazy -> lazi). In the end, the following terms will be added\n-to the inverted index:\n+However, there are less common cases where configuring text analysis is", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjYzNzkwNA=="}, "originalCommit": {"oid": "c65e22d62da25b6b47335aaeb5f4e919e9bea640"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwNDAwNzQzOnYy", "diffSide": "RIGHT", "path": "docs/reference/analysis/specify-analyzer.asciidoc", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQyMTozNTo1MFrOFjYU8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQyMjoyMDo1OVrOFjZkbA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjY0MzA1OQ==", "bodyText": "No other analyzers need to be specified.\n\nThis is not very clear for me.  Sorry, if I misinterpreted this paragraph.\nAnalyzers are only specified for text fields, so it is impossible to specify analyzers for any other field types.", "url": "https://github.com/elastic/elasticsearch/pull/51184#discussion_r372643059", "createdAt": "2020-01-29T21:35:50Z", "author": {"login": "mayya-sharipova"}, "path": "docs/reference/analysis/specify-analyzer.asciidoc", "diffHunk": "@@ -0,0 +1,203 @@\n+[[specify-analyzer]]\n+=== Specify an analyzer\n+\n+{es} offers a variety of ways to specify built-in or custom analyzers:\n+\n+* By `text` field, index, or query\n+* For <<analysis-index-search-time,index or search time>>\n+\n+[TIP]\n+.Keep it simple\n+====\n+The flexibility to specify analyzers at different levels and for different times\n+is great... _but only when it's needed_.\n+\n+In most cases, a simple approach works best: Specify an analyzer for each\n+`text` field, as outlined in <<specify-index-field-analyzer>>. No other\n+analyzers need to be specified.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c65e22d62da25b6b47335aaeb5f4e919e9bea640"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjY2MzQwNA==", "bodyText": "That sentence was referring to specifying an index analyzer or field-level search analyzer. However, I agree with you: this paragraph is clearer without that sentence. Thanks!", "url": "https://github.com/elastic/elasticsearch/pull/51184#discussion_r372663404", "createdAt": "2020-01-29T22:20:59Z", "author": {"login": "jrodewig"}, "path": "docs/reference/analysis/specify-analyzer.asciidoc", "diffHunk": "@@ -0,0 +1,203 @@\n+[[specify-analyzer]]\n+=== Specify an analyzer\n+\n+{es} offers a variety of ways to specify built-in or custom analyzers:\n+\n+* By `text` field, index, or query\n+* For <<analysis-index-search-time,index or search time>>\n+\n+[TIP]\n+.Keep it simple\n+====\n+The flexibility to specify analyzers at different levels and for different times\n+is great... _but only when it's needed_.\n+\n+In most cases, a simple approach works best: Specify an analyzer for each\n+`text` field, as outlined in <<specify-index-field-analyzer>>. No other\n+analyzers need to be specified.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjY0MzA1OQ=="}, "originalCommit": {"oid": "c65e22d62da25b6b47335aaeb5f4e919e9bea640"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwNDAyMjQyOnYy", "diffSide": "RIGHT", "path": "docs/reference/analysis/index-search-time.asciidoc", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQyMTo0MDoyMlrOFjYd0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQyMTo0MDoyMlrOFjYd0Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjY0NTMyOQ==", "bodyText": "This is a very good example.\nAnother example could be a use case with synonyms, where we specify synonym filter only during search, as it is redundant to use synonyms both at indexing and querying.", "url": "https://github.com/elastic/elasticsearch/pull/51184#discussion_r372645329", "createdAt": "2020-01-29T21:40:22Z", "author": {"login": "mayya-sharipova"}, "path": "docs/reference/analysis/index-search-time.asciidoc", "diffHunk": "@@ -0,0 +1,175 @@\n+[[analysis-index-search-time]]\n+=== Index and search analysis\n+\n+Text analysis occurs at two times:\n+\n+Index time::\n+When a document is indexed, any <<text,`text`>> field values are analyzed.\n+\n+Search time::\n+When running a <<full-text-queries,full-text search>> on a `text` field,\n+the query string (the text the user is searching for) is analyzed.\n++\n+Search time is also called _query time_.\n+\n+The analyzer, or set of analysis rules, used at each time is called the _index\n+analyzer_ or _search analyzer_ respectively.\n+\n+[[analysis-same-index-search-analyzer]]\n+==== How the index and search analyzer work together\n+\n+In most cases, the same analyzer should be used at index and search time. This\n+ensures the values and query strings for a field are changed into the same form\n+of tokens. In turn, this ensures the tokens match as expected during a search.\n+\n+.**Example**\n+[%collapsible]\n+====\n+\n+A document is indexed with the following value in a `text` field:\n+\n+[source,text]\n+------\n+The QUICK brown foxes jumped over the dog!\n+------\n+\n+The index analyzer for the field converts the value into tokens and normalizes\n+them. In this case, each of the tokens represents a word:\n+\n+[source,text]\n+------\n+[ quick, brown, fox, jump, over, dog ]\n+------\n+\n+These tokens are then indexed.\n+\n+Later, a user searches the same `text` field for:\n+\n+[source,text]\n+------\n+\"Quick fox\"\n+------\n+\n+The user expects this search to match the sentence indexed earlier,\n+`The QUICK brown foxes jumped over the dog!`.\n+\n+However, the query string does not contain the exact words used in the\n+document's original text:\n+\n+* `quick` vs `QUICK`\n+* `fox` vs `foxes`\n+\n+To account for this, the query string is analyzed using the same analyzer. This\n+analyzer produces the following tokens:\n+\n+[source,text]\n+------\n+[ quick, fox ]\n+------\n+\n+To execute the serach, {es} compares these query string tokens to the tokens\n+indexed in the `text` field.\n+\n+[options=\"header\"]\n+|===\n+|Token     | Query string | `text` field\n+|`quick`   | X            | X\n+|`brown`   |              | X\n+|`fox`     | X            | X\n+|`jump`    |              | X\n+|`over`    |              | X\n+|`dog`     |              | X\n+|===\n+\n+Because the field value are query string were analyzed in the same way, they\n+created similar tokens. The tokens `quick` and `fox` are exact matches. This\n+means the search matches the document containing `\"The QUICK brown foxes jumped\n+over the dog!\"`, just as the user expects.\n+====\n+\n+[[different-analyzers]]\n+==== When to use a different search analyzer\n+\n+While less common, it sometimes makes sense to use different analyzers at index\n+and search time. To enable this, {es} allows you to\n+<<specify-search-analyzer,specify a separate search analyzer>>.\n+\n+Generally, a separate search analyzer should only be specified when using the\n+same form of tokens for field values and query strings would create unexpected\n+or irrelevant search matches.\n+\n+[[different-analyzer-ex]]\n+.*Example*\n+[%collapsible]\n+====\n+{es} is used to create a search engine that matches only words that start with\n+a provided prefix. For instance, a search for `tr` should return `tram` or\n+`trope`\u2014but never `taxi` or `bat`.\n+\n+A document is added to the search engine's index; this document contains one\n+such word in a `text` field:\n+\n+[source,text]\n+------\n+\"Apple\"\n+------\n+\n+The index analyzer for the field converts the value into tokens and normalizes\n+them. In this case, each of the tokens represents a potential prefix for\n+the word:\n+\n+[source,text]\n+------\n+[ a, ap, app, appl, apple]\n+------\n+\n+These tokens are then indexed.\n+\n+Later, a user searches the same `text` field for:\n+\n+[source,text]\n+------\n+\"appli\"\n+------\n+\n+The user expects this search to match only words that start with `appli`,\n+such as `appliance` or `application`. The search should not match `apple`.\n+\n+However, if the index analyzer is used to analyze this query string, it would\n+produce the following tokens:\n+\n+[source,text]\n+------\n+[ a, ap, app, appl, appli ]\n+------\n+\n+When {es} compares these query string tokens to the ones indexed for `apple`,\n+it finds several matches.\n+\n+[options=\"header\"]\n+|===\n+|Token      | `appli`      | `apple`\n+|`a`        | X            | X\n+|`ap`       | X            | X\n+|`app`      | X            | X\n+|`appl`     | X            | X\n+|`appli`    |              | X\n+|===\n+\n+This means the search would erroneously match `apple`. Not only that, it would", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c65e22d62da25b6b47335aaeb5f4e919e9bea640"}, "originalPosition": 159}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4543, "cost": 1, "resetAt": "2021-11-12T13:16:51Z"}}}