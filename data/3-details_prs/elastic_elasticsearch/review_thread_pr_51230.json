{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzY0OTIwNzA4", "number": 51230, "reviewThreads": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwOTo0NTowMlrODaG4_Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxNDoyNTozMVrODc_snA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NzAyNDYxOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwOTo0NTowMlrOFg4D-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QxMjo1NToyM1rOFg9JVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAxNzI3Mw==", "bodyText": "perhaps use new ByteSizeValue(32, ByteSizeUnit.MB).getBytes().\nIt auto-documents the value :)", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370017273", "createdAt": "2020-01-23T09:45:02Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -36,6 +37,7 @@\n \n     private final BlobStoreIndexShardSnapshot snapshot;\n     private final BlobContainer blobContainer;\n+    private static final long BLOB_STORE_SEQUENTIAL_READ_SIZE = 1L<<25; // 32MB", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDA1ODQ3OQ==", "bodyText": "D'oh.", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370058479", "createdAt": "2020-01-23T11:11:39Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -36,6 +37,7 @@\n \n     private final BlobStoreIndexShardSnapshot snapshot;\n     private final BlobContainer blobContainer;\n+    private static final long BLOB_STORE_SEQUENTIAL_READ_SIZE = 1L<<25; // 32MB", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAxNzI3Mw=="}, "originalCommit": {"oid": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDA5MzM5MQ==", "bodyText": "Addressed in 9359e70.", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370093391", "createdAt": "2020-01-23T12:38:21Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -36,6 +37,7 @@\n \n     private final BlobStoreIndexShardSnapshot snapshot;\n     private final BlobContainer blobContainer;\n+    private static final long BLOB_STORE_SEQUENTIAL_READ_SIZE = 1L<<25; // 32MB", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAxNzI3Mw=="}, "originalCommit": {"oid": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDEwMDU2NA==", "bodyText": "Addressed in 9359e70.", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370100564", "createdAt": "2020-01-23T12:55:23Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -36,6 +37,7 @@\n \n     private final BlobStoreIndexShardSnapshot snapshot;\n     private final BlobContainer blobContainer;\n+    private static final long BLOB_STORE_SEQUENTIAL_READ_SIZE = 1L<<25; // 32MB", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAxNzI3Mw=="}, "originalCommit": {"oid": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NzAzMzQ2OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwOTo0NzoyOVrOFg4JFw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QxMjozOTowOVrOFg8uoA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAxODU4Mw==", "bodyText": "I wonder if choosing the BLOB_STORE_SEQUENTIAL_READ_SIZE should be left to the underlying blob store implementation. For a shared file-system, it might not make sense to have BLOB_STORE_SEQUENTIAL_READ_SIZE at all (but should use Long.MAX_VALUE instead), as it requires opening the files multiple times.", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370018583", "createdAt": "2020-01-23T09:47:29Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -68,7 +70,8 @@ public long fileLength(final String name) throws IOException {\n     @Override\n     public IndexInput openInput(final String name, final IOContext context) throws IOException {\n         ensureOpen();\n-        return new SearchableSnapshotIndexInput(blobContainer, fileInfo(name));\n+        return new SearchableSnapshotIndexInput(blobContainer, fileInfo(name), BLOB_STORE_SEQUENTIAL_READ_SIZE,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDA1ODQzMA==", "bodyText": "Yes it probably should. Maybe even expose this as a setting on the repository, although with sensible defaults as you note.", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370058430", "createdAt": "2020-01-23T11:11:33Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -68,7 +70,8 @@ public long fileLength(final String name) throws IOException {\n     @Override\n     public IndexInput openInput(final String name, final IOContext context) throws IOException {\n         ensureOpen();\n-        return new SearchableSnapshotIndexInput(blobContainer, fileInfo(name));\n+        return new SearchableSnapshotIndexInput(blobContainer, fileInfo(name), BLOB_STORE_SEQUENTIAL_READ_SIZE,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAxODU4Mw=="}, "originalCommit": {"oid": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDA5MzcyOA==", "bodyText": "Allowing this to depend on the blob container in 9359e70 - we can expose a setting in a followup if needed.", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370093728", "createdAt": "2020-01-23T12:39:09Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -68,7 +70,8 @@ public long fileLength(final String name) throws IOException {\n     @Override\n     public IndexInput openInput(final String name, final IOContext context) throws IOException {\n         ensureOpen();\n-        return new SearchableSnapshotIndexInput(blobContainer, fileInfo(name));\n+        return new SearchableSnapshotIndexInput(blobContainer, fileInfo(name), BLOB_STORE_SEQUENTIAL_READ_SIZE,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAxODU4Mw=="}, "originalCommit": {"oid": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NzExODk3OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QxMDoxMzowMVrOFg48PA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QxMjo1NTowNVrOFg9IzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAzMTY3Ng==", "bodyText": "something missing here", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370031676", "createdAt": "2020-01-23T10:13:01Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,34 +105,126 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        final long currentSequentialReadSize = sequentialReadSize;\n+        if (currentSequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            final StreamForSequentialReads streamForSequentialReads = streamForSequentialReadsRef.get();\n+            if (streamForSequentialReads == null) {\n+                // start a new sequential read\n+                if (tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                    return;\n+                }\n+            } else if (streamForSequentialReads.part == part && streamForSequentialReads.pos == pos) {\n+                // continuing a sequential read that we started previously\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                int read = streamForSequentialReads.inputStream.read(b, offset, length);\n+                assert read <= length : read + \" vs \" + length;\n+                streamForSequentialReads.pos += read;\n+                position += read;\n+                pos += read;\n+                offset += read;\n+                length -= read;\n+\n+                if (streamForSequentialReads.isFullyRead()) {\n+                    if (streamForSequentialReadsRef.compareAndSet(streamForSequentialReads, null) != false) {\n+                        streamForSequentialReads.close();\n+                    } else {\n+                        // something happened concurrently, defensively stop optimizing\n+                        sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+                    }\n+\n+                    if (length == 0) {\n+                        // the current stream contained precisely enough data for this read, so we're good.\n+                        return;\n+                    } else {\n+                        // the current stream didn't contain enough data for this read, so we must read more\n+                        if (sequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION\n+                            && tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                            return;\n+                        }\n+                    }\n+                } else {\n+                    // the current stream contained enough data for this read and more besides, so we leave it alone.\n+                    assert length == 0 : length + \" remaining\";\n+                    return;\n+                }\n+            } else {\n+                // not a sequential read, so stop optimizing for this usage pattern and fall through to the unoptimized behaviour\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+                IOUtils.close(streamForSequentialReadsRef.getAndSet(null));\n+            }\n+        }\n+\n+        // read part of a blob directly; the code above falls through to this case where there is no optimization possible\n         try (InputStream inputStream = blobContainer.readBlob(fileInfo.partName(part), pos, length)) {\n-            int read = inputStream.read(b, offset, length);\n-            assert read == length;\n+            final int read = inputStream.read(b, offset, length);\n+            assert read == length : read + \" vs \" + length;\n             position += read;\n         }\n     }\n \n+    /**\n+     * If appropriate, open a new stream for sequential reading and satisfy the given read using it. Returns whether this happened or not;\n+     * if it did not happen then nothing was read, and the caller should perform the read directly.\n+     */\n+    private boolean tryReadAndKeepStreamOpen(int part, long pos, byte[] b, int offset, int length, long currentSequentialReadSize)\n+        throws IOException {\n+\n+        assert streamForSequentialReadsRef.get() == null : \"should only be called when a new stream is needed\";\n+        assert currentSequentialReadSize > 0L : \"should not be called if \";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57"}, "originalPosition": 141}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDEwMDQyOA==", "bodyText": "Fixed in 78087ab.", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370100428", "createdAt": "2020-01-23T12:55:05Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,34 +105,126 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        final long currentSequentialReadSize = sequentialReadSize;\n+        if (currentSequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            final StreamForSequentialReads streamForSequentialReads = streamForSequentialReadsRef.get();\n+            if (streamForSequentialReads == null) {\n+                // start a new sequential read\n+                if (tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                    return;\n+                }\n+            } else if (streamForSequentialReads.part == part && streamForSequentialReads.pos == pos) {\n+                // continuing a sequential read that we started previously\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                int read = streamForSequentialReads.inputStream.read(b, offset, length);\n+                assert read <= length : read + \" vs \" + length;\n+                streamForSequentialReads.pos += read;\n+                position += read;\n+                pos += read;\n+                offset += read;\n+                length -= read;\n+\n+                if (streamForSequentialReads.isFullyRead()) {\n+                    if (streamForSequentialReadsRef.compareAndSet(streamForSequentialReads, null) != false) {\n+                        streamForSequentialReads.close();\n+                    } else {\n+                        // something happened concurrently, defensively stop optimizing\n+                        sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+                    }\n+\n+                    if (length == 0) {\n+                        // the current stream contained precisely enough data for this read, so we're good.\n+                        return;\n+                    } else {\n+                        // the current stream didn't contain enough data for this read, so we must read more\n+                        if (sequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION\n+                            && tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                            return;\n+                        }\n+                    }\n+                } else {\n+                    // the current stream contained enough data for this read and more besides, so we leave it alone.\n+                    assert length == 0 : length + \" remaining\";\n+                    return;\n+                }\n+            } else {\n+                // not a sequential read, so stop optimizing for this usage pattern and fall through to the unoptimized behaviour\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+                IOUtils.close(streamForSequentialReadsRef.getAndSet(null));\n+            }\n+        }\n+\n+        // read part of a blob directly; the code above falls through to this case where there is no optimization possible\n         try (InputStream inputStream = blobContainer.readBlob(fileInfo.partName(part), pos, length)) {\n-            int read = inputStream.read(b, offset, length);\n-            assert read == length;\n+            final int read = inputStream.read(b, offset, length);\n+            assert read == length : read + \" vs \" + length;\n             position += read;\n         }\n     }\n \n+    /**\n+     * If appropriate, open a new stream for sequential reading and satisfy the given read using it. Returns whether this happened or not;\n+     * if it did not happen then nothing was read, and the caller should perform the read directly.\n+     */\n+    private boolean tryReadAndKeepStreamOpen(int part, long pos, byte[] b, int offset, int length, long currentSequentialReadSize)\n+        throws IOException {\n+\n+        assert streamForSequentialReadsRef.get() == null : \"should only be called when a new stream is needed\";\n+        assert currentSequentialReadSize > 0L : \"should not be called if \";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAzMTY3Ng=="}, "originalCommit": {"oid": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57"}, "originalPosition": 141}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NzE0MzU5OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QxMDoyMTowMlrOFg5LOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QxMjo1NDowOVrOFg9HNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAzNTUxNA==", "bodyText": "should we put this logic into StreamForSequentialReads? Perhaps that class could enforce that only sequential reads are possible from the stream (and offer a method to say isSequentialReadPossible), with the logic in this class here just trying to call these methods.", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370035514", "createdAt": "2020-01-23T10:21:02Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,34 +105,126 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        final long currentSequentialReadSize = sequentialReadSize;\n+        if (currentSequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            final StreamForSequentialReads streamForSequentialReads = streamForSequentialReadsRef.get();\n+            if (streamForSequentialReads == null) {\n+                // start a new sequential read\n+                if (tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                    return;\n+                }\n+            } else if (streamForSequentialReads.part == part && streamForSequentialReads.pos == pos) {\n+                // continuing a sequential read that we started previously\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                int read = streamForSequentialReads.inputStream.read(b, offset, length);\n+                assert read <= length : read + \" vs \" + length;\n+                streamForSequentialReads.pos += read;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDEwMDAyMQ==", "bodyText": "Moved this around in 1d69293 and 184c9b1.", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370100021", "createdAt": "2020-01-23T12:54:09Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,34 +105,126 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        final long currentSequentialReadSize = sequentialReadSize;\n+        if (currentSequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            final StreamForSequentialReads streamForSequentialReads = streamForSequentialReadsRef.get();\n+            if (streamForSequentialReads == null) {\n+                // start a new sequential read\n+                if (tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                    return;\n+                }\n+            } else if (streamForSequentialReads.part == part && streamForSequentialReads.pos == pos) {\n+                // continuing a sequential read that we started previously\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                int read = streamForSequentialReads.inputStream.read(b, offset, length);\n+                assert read <= length : read + \" vs \" + length;\n+                streamForSequentialReads.pos += read;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAzNTUxNA=="}, "originalCommit": {"oid": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57"}, "originalPosition": 86}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NzE0ODE4OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QxMDoyMjozOFrOFg5OEQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QxMjo1NDoxOVrOFg9Hcw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAzNjI0MQ==", "bodyText": "just leave off the != false", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370036241", "createdAt": "2020-01-23T10:22:38Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,34 +105,126 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        final long currentSequentialReadSize = sequentialReadSize;\n+        if (currentSequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            final StreamForSequentialReads streamForSequentialReads = streamForSequentialReadsRef.get();\n+            if (streamForSequentialReads == null) {\n+                // start a new sequential read\n+                if (tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                    return;\n+                }\n+            } else if (streamForSequentialReads.part == part && streamForSequentialReads.pos == pos) {\n+                // continuing a sequential read that we started previously\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                int read = streamForSequentialReads.inputStream.read(b, offset, length);\n+                assert read <= length : read + \" vs \" + length;\n+                streamForSequentialReads.pos += read;\n+                position += read;\n+                pos += read;\n+                offset += read;\n+                length -= read;\n+\n+                if (streamForSequentialReads.isFullyRead()) {\n+                    if (streamForSequentialReadsRef.compareAndSet(streamForSequentialReads, null) != false) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDEwMDA4Mw==", "bodyText": "33a1362", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370100083", "createdAt": "2020-01-23T12:54:19Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,34 +105,126 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        final long currentSequentialReadSize = sequentialReadSize;\n+        if (currentSequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            final StreamForSequentialReads streamForSequentialReads = streamForSequentialReadsRef.get();\n+            if (streamForSequentialReads == null) {\n+                // start a new sequential read\n+                if (tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                    return;\n+                }\n+            } else if (streamForSequentialReads.part == part && streamForSequentialReads.pos == pos) {\n+                // continuing a sequential read that we started previously\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                int read = streamForSequentialReads.inputStream.read(b, offset, length);\n+                assert read <= length : read + \" vs \" + length;\n+                streamForSequentialReads.pos += read;\n+                position += read;\n+                pos += read;\n+                offset += read;\n+                length -= read;\n+\n+                if (streamForSequentialReads.isFullyRead()) {\n+                    if (streamForSequentialReadsRef.compareAndSet(streamForSequentialReads, null) != false) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAzNjI0MQ=="}, "originalCommit": {"oid": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57"}, "originalPosition": 93}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NzE1NTE3OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QxMDoyNDo1MVrOFg5SOw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QxMjo1NDozM1rOFg9H0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAzNzMwNw==", "bodyText": "Do we ever expect concurrent activity here? If that was the case, our bookkeeping logic (position, pos, offset, length) would not work at all?", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370037307", "createdAt": "2020-01-23T10:24:51Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,34 +105,126 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        final long currentSequentialReadSize = sequentialReadSize;\n+        if (currentSequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            final StreamForSequentialReads streamForSequentialReads = streamForSequentialReadsRef.get();\n+            if (streamForSequentialReads == null) {\n+                // start a new sequential read\n+                if (tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                    return;\n+                }\n+            } else if (streamForSequentialReads.part == part && streamForSequentialReads.pos == pos) {\n+                // continuing a sequential read that we started previously\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                int read = streamForSequentialReads.inputStream.read(b, offset, length);\n+                assert read <= length : read + \" vs \" + length;\n+                streamForSequentialReads.pos += read;\n+                position += read;\n+                pos += read;\n+                offset += read;\n+                length -= read;\n+\n+                if (streamForSequentialReads.isFullyRead()) {\n+                    if (streamForSequentialReadsRef.compareAndSet(streamForSequentialReads, null) != false) {\n+                        streamForSequentialReads.close();\n+                    } else {\n+                        // something happened concurrently, defensively stop optimizing\n+                        sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDA1NjI3NQ==", "bodyText": "We don't expect concurrent reading, but we can be concurrently closed. In an earlier iteration we also dropped the optimisation if cloned or sliced, which happens on a separate thread, but in fact that shouldn't be happening concurrently with reading either, so concurrent closing is the only thing to handle. I'll adjust the comments.", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370056275", "createdAt": "2020-01-23T11:06:11Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,34 +105,126 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        final long currentSequentialReadSize = sequentialReadSize;\n+        if (currentSequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            final StreamForSequentialReads streamForSequentialReads = streamForSequentialReadsRef.get();\n+            if (streamForSequentialReads == null) {\n+                // start a new sequential read\n+                if (tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                    return;\n+                }\n+            } else if (streamForSequentialReads.part == part && streamForSequentialReads.pos == pos) {\n+                // continuing a sequential read that we started previously\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                int read = streamForSequentialReads.inputStream.read(b, offset, length);\n+                assert read <= length : read + \" vs \" + length;\n+                streamForSequentialReads.pos += read;\n+                position += read;\n+                pos += read;\n+                offset += read;\n+                length -= read;\n+\n+                if (streamForSequentialReads.isFullyRead()) {\n+                    if (streamForSequentialReadsRef.compareAndSet(streamForSequentialReads, null) != false) {\n+                        streamForSequentialReads.close();\n+                    } else {\n+                        // something happened concurrently, defensively stop optimizing\n+                        sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAzNzMwNw=="}, "originalCommit": {"oid": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDEwMDE3Ng==", "bodyText": "Adjusted in 84d12b6.", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370100176", "createdAt": "2020-01-23T12:54:33Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,34 +105,126 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        final long currentSequentialReadSize = sequentialReadSize;\n+        if (currentSequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            final StreamForSequentialReads streamForSequentialReads = streamForSequentialReadsRef.get();\n+            if (streamForSequentialReads == null) {\n+                // start a new sequential read\n+                if (tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                    return;\n+                }\n+            } else if (streamForSequentialReads.part == part && streamForSequentialReads.pos == pos) {\n+                // continuing a sequential read that we started previously\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                int read = streamForSequentialReads.inputStream.read(b, offset, length);\n+                assert read <= length : read + \" vs \" + length;\n+                streamForSequentialReads.pos += read;\n+                position += read;\n+                pos += read;\n+                offset += read;\n+                length -= read;\n+\n+                if (streamForSequentialReads.isFullyRead()) {\n+                    if (streamForSequentialReadsRef.compareAndSet(streamForSequentialReads, null) != false) {\n+                        streamForSequentialReads.close();\n+                    } else {\n+                        // something happened concurrently, defensively stop optimizing\n+                        sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAzNzMwNw=="}, "originalCommit": {"oid": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57"}, "originalPosition": 97}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NzE2NDUxOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QxMDoyNzo1NVrOFg5YEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QxMjo1NDo1NlrOFg9Igw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAzODgwMA==", "bodyText": "maybe put everything above this into a readOptimized() method that returns a boolean (denoting whether it read or not). This will allow having so many explicit returns in the above code (and the deliberate fall-through logic).", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370038800", "createdAt": "2020-01-23T10:27:55Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,34 +105,126 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        final long currentSequentialReadSize = sequentialReadSize;\n+        if (currentSequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            final StreamForSequentialReads streamForSequentialReads = streamForSequentialReadsRef.get();\n+            if (streamForSequentialReads == null) {\n+                // start a new sequential read\n+                if (tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                    return;\n+                }\n+            } else if (streamForSequentialReads.part == part && streamForSequentialReads.pos == pos) {\n+                // continuing a sequential read that we started previously\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                int read = streamForSequentialReads.inputStream.read(b, offset, length);\n+                assert read <= length : read + \" vs \" + length;\n+                streamForSequentialReads.pos += read;\n+                position += read;\n+                pos += read;\n+                offset += read;\n+                length -= read;\n+\n+                if (streamForSequentialReads.isFullyRead()) {\n+                    if (streamForSequentialReadsRef.compareAndSet(streamForSequentialReads, null) != false) {\n+                        streamForSequentialReads.close();\n+                    } else {\n+                        // something happened concurrently, defensively stop optimizing\n+                        sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+                    }\n+\n+                    if (length == 0) {\n+                        // the current stream contained precisely enough data for this read, so we're good.\n+                        return;\n+                    } else {\n+                        // the current stream didn't contain enough data for this read, so we must read more\n+                        if (sequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION\n+                            && tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                            return;\n+                        }\n+                    }\n+                } else {\n+                    // the current stream contained enough data for this read and more besides, so we leave it alone.\n+                    assert length == 0 : length + \" remaining\";\n+                    return;\n+                }\n+            } else {\n+                // not a sequential read, so stop optimizing for this usage pattern and fall through to the unoptimized behaviour\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+                IOUtils.close(streamForSequentialReadsRef.getAndSet(null));\n+            }\n+        }\n+\n+        // read part of a blob directly; the code above falls through to this case where there is no optimization possible", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57"}, "originalPosition": 123}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDA1ODAzNA==", "bodyText": "Sounds good.", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370058034", "createdAt": "2020-01-23T11:10:37Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,34 +105,126 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        final long currentSequentialReadSize = sequentialReadSize;\n+        if (currentSequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            final StreamForSequentialReads streamForSequentialReads = streamForSequentialReadsRef.get();\n+            if (streamForSequentialReads == null) {\n+                // start a new sequential read\n+                if (tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                    return;\n+                }\n+            } else if (streamForSequentialReads.part == part && streamForSequentialReads.pos == pos) {\n+                // continuing a sequential read that we started previously\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                int read = streamForSequentialReads.inputStream.read(b, offset, length);\n+                assert read <= length : read + \" vs \" + length;\n+                streamForSequentialReads.pos += read;\n+                position += read;\n+                pos += read;\n+                offset += read;\n+                length -= read;\n+\n+                if (streamForSequentialReads.isFullyRead()) {\n+                    if (streamForSequentialReadsRef.compareAndSet(streamForSequentialReads, null) != false) {\n+                        streamForSequentialReads.close();\n+                    } else {\n+                        // something happened concurrently, defensively stop optimizing\n+                        sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+                    }\n+\n+                    if (length == 0) {\n+                        // the current stream contained precisely enough data for this read, so we're good.\n+                        return;\n+                    } else {\n+                        // the current stream didn't contain enough data for this read, so we must read more\n+                        if (sequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION\n+                            && tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                            return;\n+                        }\n+                    }\n+                } else {\n+                    // the current stream contained enough data for this read and more besides, so we leave it alone.\n+                    assert length == 0 : length + \" remaining\";\n+                    return;\n+                }\n+            } else {\n+                // not a sequential read, so stop optimizing for this usage pattern and fall through to the unoptimized behaviour\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+                IOUtils.close(streamForSequentialReadsRef.getAndSet(null));\n+            }\n+        }\n+\n+        // read part of a blob directly; the code above falls through to this case where there is no optimization possible", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAzODgwMA=="}, "originalCommit": {"oid": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57"}, "originalPosition": 123}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDEwMDM1NQ==", "bodyText": "See 1d69293 and 184c9b1.", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370100355", "createdAt": "2020-01-23T12:54:56Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,34 +105,126 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        final long currentSequentialReadSize = sequentialReadSize;\n+        if (currentSequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            final StreamForSequentialReads streamForSequentialReads = streamForSequentialReadsRef.get();\n+            if (streamForSequentialReads == null) {\n+                // start a new sequential read\n+                if (tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                    return;\n+                }\n+            } else if (streamForSequentialReads.part == part && streamForSequentialReads.pos == pos) {\n+                // continuing a sequential read that we started previously\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                int read = streamForSequentialReads.inputStream.read(b, offset, length);\n+                assert read <= length : read + \" vs \" + length;\n+                streamForSequentialReads.pos += read;\n+                position += read;\n+                pos += read;\n+                offset += read;\n+                length -= read;\n+\n+                if (streamForSequentialReads.isFullyRead()) {\n+                    if (streamForSequentialReadsRef.compareAndSet(streamForSequentialReads, null) != false) {\n+                        streamForSequentialReads.close();\n+                    } else {\n+                        // something happened concurrently, defensively stop optimizing\n+                        sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+                    }\n+\n+                    if (length == 0) {\n+                        // the current stream contained precisely enough data for this read, so we're good.\n+                        return;\n+                    } else {\n+                        // the current stream didn't contain enough data for this read, so we must read more\n+                        if (sequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION\n+                            && tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                            return;\n+                        }\n+                    }\n+                } else {\n+                    // the current stream contained enough data for this read and more besides, so we leave it alone.\n+                    assert length == 0 : length + \" remaining\";\n+                    return;\n+                }\n+            } else {\n+                // not a sequential read, so stop optimizing for this usage pattern and fall through to the unoptimized behaviour\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+                IOUtils.close(streamForSequentialReadsRef.getAndSet(null));\n+            }\n+        }\n+\n+        // read part of a blob directly; the code above falls through to this case where there is no optimization possible", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAzODgwMA=="}, "originalCommit": {"oid": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57"}, "originalPosition": 123}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NzE2ODMyOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QxMDoyODo1OFrOFg5aRA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QxMTowODowN1rOFg6fUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAzOTM2NA==", "bodyText": "I guess this is to handle some kind of concurrency. I don't really understand the concurrency here though.", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370039364", "createdAt": "2020-01-23T10:28:58Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,34 +105,126 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        final long currentSequentialReadSize = sequentialReadSize;\n+        if (currentSequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            final StreamForSequentialReads streamForSequentialReads = streamForSequentialReadsRef.get();\n+            if (streamForSequentialReads == null) {\n+                // start a new sequential read\n+                if (tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                    return;\n+                }\n+            } else if (streamForSequentialReads.part == part && streamForSequentialReads.pos == pos) {\n+                // continuing a sequential read that we started previously\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                int read = streamForSequentialReads.inputStream.read(b, offset, length);\n+                assert read <= length : read + \" vs \" + length;\n+                streamForSequentialReads.pos += read;\n+                position += read;\n+                pos += read;\n+                offset += read;\n+                length -= read;\n+\n+                if (streamForSequentialReads.isFullyRead()) {\n+                    if (streamForSequentialReadsRef.compareAndSet(streamForSequentialReads, null) != false) {\n+                        streamForSequentialReads.close();\n+                    } else {\n+                        // something happened concurrently, defensively stop optimizing\n+                        sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+                    }\n+\n+                    if (length == 0) {\n+                        // the current stream contained precisely enough data for this read, so we're good.\n+                        return;\n+                    } else {\n+                        // the current stream didn't contain enough data for this read, so we must read more\n+                        if (sequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDA1NzA0MQ==", "bodyText": "Just closing.", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370057041", "createdAt": "2020-01-23T11:08:07Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,34 +105,126 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        final long currentSequentialReadSize = sequentialReadSize;\n+        if (currentSequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            final StreamForSequentialReads streamForSequentialReads = streamForSequentialReadsRef.get();\n+            if (streamForSequentialReads == null) {\n+                // start a new sequential read\n+                if (tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                    return;\n+                }\n+            } else if (streamForSequentialReads.part == part && streamForSequentialReads.pos == pos) {\n+                // continuing a sequential read that we started previously\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                int read = streamForSequentialReads.inputStream.read(b, offset, length);\n+                assert read <= length : read + \" vs \" + length;\n+                streamForSequentialReads.pos += read;\n+                position += read;\n+                pos += read;\n+                offset += read;\n+                length -= read;\n+\n+                if (streamForSequentialReads.isFullyRead()) {\n+                    if (streamForSequentialReadsRef.compareAndSet(streamForSequentialReads, null) != false) {\n+                        streamForSequentialReads.close();\n+                    } else {\n+                        // something happened concurrently, defensively stop optimizing\n+                        sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+                    }\n+\n+                    if (length == 0) {\n+                        // the current stream contained precisely enough data for this read, so we're good.\n+                        return;\n+                    } else {\n+                        // the current stream didn't contain enough data for this read, so we must read more\n+                        if (sequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAzOTM2NA=="}, "originalCommit": {"oid": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57"}, "originalPosition": 105}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NzE4ODI0OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QxMDozNToyN1rOFg5mGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QxMToxMDowNFrOFg6iWw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDA0MjM5Mw==", "bodyText": "Should we not use this existing open stream as much as possible? We might not be able to read the full bytes from this stream, but perhaps we can use it to read everything up to streamLength, and subsequently request a new stream for the rest? This might avoid redownloading data in case where the buffer size is not a proper divisor of sequentialReadSize?", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370042393", "createdAt": "2020-01-23T10:35:27Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,34 +105,126 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        final long currentSequentialReadSize = sequentialReadSize;\n+        if (currentSequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            final StreamForSequentialReads streamForSequentialReads = streamForSequentialReadsRef.get();\n+            if (streamForSequentialReads == null) {\n+                // start a new sequential read\n+                if (tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                    return;\n+                }\n+            } else if (streamForSequentialReads.part == part && streamForSequentialReads.pos == pos) {\n+                // continuing a sequential read that we started previously\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                int read = streamForSequentialReads.inputStream.read(b, offset, length);\n+                assert read <= length : read + \" vs \" + length;\n+                streamForSequentialReads.pos += read;\n+                position += read;\n+                pos += read;\n+                offset += read;\n+                length -= read;\n+\n+                if (streamForSequentialReads.isFullyRead()) {\n+                    if (streamForSequentialReadsRef.compareAndSet(streamForSequentialReads, null) != false) {\n+                        streamForSequentialReads.close();\n+                    } else {\n+                        // something happened concurrently, defensively stop optimizing\n+                        sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+                    }\n+\n+                    if (length == 0) {\n+                        // the current stream contained precisely enough data for this read, so we're good.\n+                        return;\n+                    } else {\n+                        // the current stream didn't contain enough data for this read, so we must read more\n+                        if (sequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION\n+                            && tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                            return;\n+                        }\n+                    }\n+                } else {\n+                    // the current stream contained enough data for this read and more besides, so we leave it alone.\n+                    assert length == 0 : length + \" remaining\";\n+                    return;\n+                }\n+            } else {\n+                // not a sequential read, so stop optimizing for this usage pattern and fall through to the unoptimized behaviour\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+                IOUtils.close(streamForSequentialReadsRef.getAndSet(null));\n+            }\n+        }\n+\n+        // read part of a blob directly; the code above falls through to this case where there is no optimization possible\n         try (InputStream inputStream = blobContainer.readBlob(fileInfo.partName(part), pos, length)) {\n-            int read = inputStream.read(b, offset, length);\n-            assert read == length;\n+            final int read = inputStream.read(b, offset, length);\n+            assert read == length : read + \" vs \" + length;\n             position += read;\n         }\n     }\n \n+    /**\n+     * If appropriate, open a new stream for sequential reading and satisfy the given read using it. Returns whether this happened or not;\n+     * if it did not happen then nothing was read, and the caller should perform the read directly.\n+     */\n+    private boolean tryReadAndKeepStreamOpen(int part, long pos, byte[] b, int offset, int length, long currentSequentialReadSize)\n+        throws IOException {\n+\n+        assert streamForSequentialReadsRef.get() == null : \"should only be called when a new stream is needed\";\n+        assert currentSequentialReadSize > 0L : \"should not be called if \";\n+\n+        final long streamLength = Math.min(currentSequentialReadSize, fileInfo.partBytes(part) - pos);\n+        if (length < streamLength) {\n+            // if we open a stream of length streamLength then it will not be completely consumed by this read, so it is worthwhile to open\n+            // it and keep it open for future reads\n+            final InputStream inputStream = blobContainer.readBlob(fileInfo.partName(part), pos, streamLength);\n+            final StreamForSequentialReads newStreamForSequentialReads\n+                = new StreamForSequentialReads(inputStream, part, pos, streamLength);\n+            if (streamForSequentialReadsRef.compareAndSet(null, newStreamForSequentialReads) == false) {\n+                // something happened concurrently, defensively stop optimizing and fall through to the unoptimized behaviour\n+                this.sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+                inputStream.close();\n+                return false;\n+            }\n+\n+            final int read = newStreamForSequentialReads.inputStream.read(b, offset, length);\n+            assert read == length : read + \" vs \" + length;\n+            position += read;\n+            newStreamForSequentialReads.pos += read;\n+            assert newStreamForSequentialReads.isFullyRead() == false;\n+            return true;\n+        } else {\n+            // streamLength <= length so this single read will consume the entire stream, so there is no need to keep hold of it, so we can\n+            // tell the caller to read the data directly", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57"}, "originalPosition": 165}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDA1NzgxOQ==", "bodyText": "At this point we don't have an existing open stream, we're trying to create a new one. If we can satisfy part of a read from the existing stream then we do so (see comment containing the string the current stream didn't contain enough data for this read, so we must read more).", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370057819", "createdAt": "2020-01-23T11:10:04Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,34 +105,126 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        final long currentSequentialReadSize = sequentialReadSize;\n+        if (currentSequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            final StreamForSequentialReads streamForSequentialReads = streamForSequentialReadsRef.get();\n+            if (streamForSequentialReads == null) {\n+                // start a new sequential read\n+                if (tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                    return;\n+                }\n+            } else if (streamForSequentialReads.part == part && streamForSequentialReads.pos == pos) {\n+                // continuing a sequential read that we started previously\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                int read = streamForSequentialReads.inputStream.read(b, offset, length);\n+                assert read <= length : read + \" vs \" + length;\n+                streamForSequentialReads.pos += read;\n+                position += read;\n+                pos += read;\n+                offset += read;\n+                length -= read;\n+\n+                if (streamForSequentialReads.isFullyRead()) {\n+                    if (streamForSequentialReadsRef.compareAndSet(streamForSequentialReads, null) != false) {\n+                        streamForSequentialReads.close();\n+                    } else {\n+                        // something happened concurrently, defensively stop optimizing\n+                        sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+                    }\n+\n+                    if (length == 0) {\n+                        // the current stream contained precisely enough data for this read, so we're good.\n+                        return;\n+                    } else {\n+                        // the current stream didn't contain enough data for this read, so we must read more\n+                        if (sequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION\n+                            && tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                            return;\n+                        }\n+                    }\n+                } else {\n+                    // the current stream contained enough data for this read and more besides, so we leave it alone.\n+                    assert length == 0 : length + \" remaining\";\n+                    return;\n+                }\n+            } else {\n+                // not a sequential read, so stop optimizing for this usage pattern and fall through to the unoptimized behaviour\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+                IOUtils.close(streamForSequentialReadsRef.getAndSet(null));\n+            }\n+        }\n+\n+        // read part of a blob directly; the code above falls through to this case where there is no optimization possible\n         try (InputStream inputStream = blobContainer.readBlob(fileInfo.partName(part), pos, length)) {\n-            int read = inputStream.read(b, offset, length);\n-            assert read == length;\n+            final int read = inputStream.read(b, offset, length);\n+            assert read == length : read + \" vs \" + length;\n             position += read;\n         }\n     }\n \n+    /**\n+     * If appropriate, open a new stream for sequential reading and satisfy the given read using it. Returns whether this happened or not;\n+     * if it did not happen then nothing was read, and the caller should perform the read directly.\n+     */\n+    private boolean tryReadAndKeepStreamOpen(int part, long pos, byte[] b, int offset, int length, long currentSequentialReadSize)\n+        throws IOException {\n+\n+        assert streamForSequentialReadsRef.get() == null : \"should only be called when a new stream is needed\";\n+        assert currentSequentialReadSize > 0L : \"should not be called if \";\n+\n+        final long streamLength = Math.min(currentSequentialReadSize, fileInfo.partBytes(part) - pos);\n+        if (length < streamLength) {\n+            // if we open a stream of length streamLength then it will not be completely consumed by this read, so it is worthwhile to open\n+            // it and keep it open for future reads\n+            final InputStream inputStream = blobContainer.readBlob(fileInfo.partName(part), pos, streamLength);\n+            final StreamForSequentialReads newStreamForSequentialReads\n+                = new StreamForSequentialReads(inputStream, part, pos, streamLength);\n+            if (streamForSequentialReadsRef.compareAndSet(null, newStreamForSequentialReads) == false) {\n+                // something happened concurrently, defensively stop optimizing and fall through to the unoptimized behaviour\n+                this.sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+                inputStream.close();\n+                return false;\n+            }\n+\n+            final int read = newStreamForSequentialReads.inputStream.read(b, offset, length);\n+            assert read == length : read + \" vs \" + length;\n+            position += read;\n+            newStreamForSequentialReads.pos += read;\n+            assert newStreamForSequentialReads.isFullyRead() == false;\n+            return true;\n+        } else {\n+            // streamLength <= length so this single read will consume the entire stream, so there is no need to keep hold of it, so we can\n+            // tell the caller to read the data directly", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDA0MjM5Mw=="}, "originalCommit": {"oid": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57"}, "originalPosition": 165}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxNzI4MDMyOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxNDoxOTowNlrOFlVovw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxNTo1NToxMFrOFlZasg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDY5NjEyNw==", "bodyText": "Maybe update the class javadoc to explain how/why we use this?", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r374696127", "createdAt": "2020-02-04T14:19:06Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -41,20 +44,31 @@\n     private final long length;\n \n     private long position;\n-    private boolean closed;\n+    private volatile boolean closed;\n \n-    public SearchableSnapshotIndexInput(final BlobContainer blobContainer, final FileInfo fileInfo) {\n-        this(\"SearchableSnapshotIndexInput(\" + fileInfo.physicalName() + \")\", blobContainer, fileInfo, 0L, 0L, fileInfo.length());\n+    // optimisation for the case where we perform a single seek, then read a large block of data sequentially, then close the input\n+    @Nullable // if not currently reading sequentially\n+    private StreamForSequentialReads streamForSequentialReads;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a92ced530401cece9969728d48d17b1196f1ec0b"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDc1ODA2Ng==", "bodyText": "sure, done in f18251a", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r374758066", "createdAt": "2020-02-04T15:55:10Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -41,20 +44,31 @@\n     private final long length;\n \n     private long position;\n-    private boolean closed;\n+    private volatile boolean closed;\n \n-    public SearchableSnapshotIndexInput(final BlobContainer blobContainer, final FileInfo fileInfo) {\n-        this(\"SearchableSnapshotIndexInput(\" + fileInfo.physicalName() + \")\", blobContainer, fileInfo, 0L, 0L, fileInfo.length());\n+    // optimisation for the case where we perform a single seek, then read a large block of data sequentially, then close the input\n+    @Nullable // if not currently reading sequentially\n+    private StreamForSequentialReads streamForSequentialReads;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDY5NjEyNw=="}, "originalCommit": {"oid": "a92ced530401cece9969728d48d17b1196f1ec0b"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxNzI4NzIzOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxNDoyMTowNVrOFlVtDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxNTo1NToyNVrOFlZbYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDY5NzIyOA==", "bodyText": "We're closing + nullify the streamForSequentialReads many times, maybe it deserves its own closeSequentialStream() method?", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r374697228", "createdAt": "2020-02-04T14:21:05Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,12 +107,87 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n-        try (InputStream inputStream = blobContainer.readBlob(fileInfo.partName(part), pos, length)) {\n-            int read = inputStream.read(b, offset, length);\n-            assert read == length;\n-            position += read;\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        int optimizedReadSize = readOptimized(part, pos, b, offset, length);\n+        assert optimizedReadSize <= length;\n+        position += optimizedReadSize;\n+\n+        if (optimizedReadSize < length) {\n+            // we did not read everything in an optimized fashion, so read the remainder directly\n+            try (InputStream inputStream\n+                     = blobContainer.readBlob(fileInfo.partName(part), pos + optimizedReadSize, length - optimizedReadSize)) {\n+                final int directReadSize = inputStream.read(b, offset + optimizedReadSize, length - optimizedReadSize);\n+                assert optimizedReadSize + directReadSize == length : optimizedReadSize + \" and \" + directReadSize + \" vs \" + length;\n+                position += directReadSize;\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Attempt to satisfy this read in an optimized fashion using {@code streamForSequentialReadsRef}.\n+     * @return the number of bytes read\n+     */\n+    private int readOptimized(int part, long pos, byte[] b, int offset, int length) throws IOException {\n+        if (sequentialReadSize == NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            return 0;\n+        }\n+\n+        int read = 0;\n+        if (streamForSequentialReads == null) {\n+            // starting a new sequential read\n+            read = readFromNewSequentialStream(part, pos, b, offset, length);\n+        } else if (streamForSequentialReads.canContinueSequentialRead(part, pos)) {\n+            // continuing a sequential read that we started previously\n+            read = streamForSequentialReads.read(b, offset, length);\n+            if (streamForSequentialReads.isFullyRead()) {\n+                // the current stream was exhausted by this read, so it should be closed\n+                streamForSequentialReads.close();\n+                streamForSequentialReads = null;\n+            } else {\n+                // the current stream contained enough data for this read and more besides, so we leave it in place\n+                assert read == length : length + \" remaining\";\n+            }\n+\n+            if (read < length) {\n+                // the current stream didn't contain enough data for this read, so we must read more\n+                read += readFromNewSequentialStream(part, pos + read, b, offset + read, length - read);\n+            }\n+        } else {\n+            // not a sequential read, so stop optimizing for this usage pattern and fall through to the unoptimized behaviour\n+            assert streamForSequentialReads.isFullyRead() == false;\n+            sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+            IOUtils.close(streamForSequentialReads);\n+            streamForSequentialReads = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a92ced530401cece9969728d48d17b1196f1ec0b"}, "originalPosition": 125}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDc1ODI0MQ==", "bodyText": "Good point, this is no longer a one-liner. Done in c9cf7bc.", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r374758241", "createdAt": "2020-02-04T15:55:25Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,12 +107,87 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n-        try (InputStream inputStream = blobContainer.readBlob(fileInfo.partName(part), pos, length)) {\n-            int read = inputStream.read(b, offset, length);\n-            assert read == length;\n-            position += read;\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        int optimizedReadSize = readOptimized(part, pos, b, offset, length);\n+        assert optimizedReadSize <= length;\n+        position += optimizedReadSize;\n+\n+        if (optimizedReadSize < length) {\n+            // we did not read everything in an optimized fashion, so read the remainder directly\n+            try (InputStream inputStream\n+                     = blobContainer.readBlob(fileInfo.partName(part), pos + optimizedReadSize, length - optimizedReadSize)) {\n+                final int directReadSize = inputStream.read(b, offset + optimizedReadSize, length - optimizedReadSize);\n+                assert optimizedReadSize + directReadSize == length : optimizedReadSize + \" and \" + directReadSize + \" vs \" + length;\n+                position += directReadSize;\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Attempt to satisfy this read in an optimized fashion using {@code streamForSequentialReadsRef}.\n+     * @return the number of bytes read\n+     */\n+    private int readOptimized(int part, long pos, byte[] b, int offset, int length) throws IOException {\n+        if (sequentialReadSize == NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            return 0;\n+        }\n+\n+        int read = 0;\n+        if (streamForSequentialReads == null) {\n+            // starting a new sequential read\n+            read = readFromNewSequentialStream(part, pos, b, offset, length);\n+        } else if (streamForSequentialReads.canContinueSequentialRead(part, pos)) {\n+            // continuing a sequential read that we started previously\n+            read = streamForSequentialReads.read(b, offset, length);\n+            if (streamForSequentialReads.isFullyRead()) {\n+                // the current stream was exhausted by this read, so it should be closed\n+                streamForSequentialReads.close();\n+                streamForSequentialReads = null;\n+            } else {\n+                // the current stream contained enough data for this read and more besides, so we leave it in place\n+                assert read == length : length + \" remaining\";\n+            }\n+\n+            if (read < length) {\n+                // the current stream didn't contain enough data for this read, so we must read more\n+                read += readFromNewSequentialStream(part, pos + read, b, offset + read, length - read);\n+            }\n+        } else {\n+            // not a sequential read, so stop optimizing for this usage pattern and fall through to the unoptimized behaviour\n+            assert streamForSequentialReads.isFullyRead() == false;\n+            sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+            IOUtils.close(streamForSequentialReads);\n+            streamForSequentialReads = null;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDY5NzIyOA=="}, "originalCommit": {"oid": "a92ced530401cece9969728d48d17b1196f1ec0b"}, "originalPosition": 125}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxNzI5NDk0OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxNDoyMzowOVrOFlVxyA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxNTo1NTozOFrOFlZb-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDY5ODQ0MA==", "bodyText": "The method signature can fit on a single line", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r374698440", "createdAt": "2020-02-04T14:23:09Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,12 +107,87 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n-        try (InputStream inputStream = blobContainer.readBlob(fileInfo.partName(part), pos, length)) {\n-            int read = inputStream.read(b, offset, length);\n-            assert read == length;\n-            position += read;\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        int optimizedReadSize = readOptimized(part, pos, b, offset, length);\n+        assert optimizedReadSize <= length;\n+        position += optimizedReadSize;\n+\n+        if (optimizedReadSize < length) {\n+            // we did not read everything in an optimized fashion, so read the remainder directly\n+            try (InputStream inputStream\n+                     = blobContainer.readBlob(fileInfo.partName(part), pos + optimizedReadSize, length - optimizedReadSize)) {\n+                final int directReadSize = inputStream.read(b, offset + optimizedReadSize, length - optimizedReadSize);\n+                assert optimizedReadSize + directReadSize == length : optimizedReadSize + \" and \" + directReadSize + \" vs \" + length;\n+                position += directReadSize;\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Attempt to satisfy this read in an optimized fashion using {@code streamForSequentialReadsRef}.\n+     * @return the number of bytes read\n+     */\n+    private int readOptimized(int part, long pos, byte[] b, int offset, int length) throws IOException {\n+        if (sequentialReadSize == NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            return 0;\n+        }\n+\n+        int read = 0;\n+        if (streamForSequentialReads == null) {\n+            // starting a new sequential read\n+            read = readFromNewSequentialStream(part, pos, b, offset, length);\n+        } else if (streamForSequentialReads.canContinueSequentialRead(part, pos)) {\n+            // continuing a sequential read that we started previously\n+            read = streamForSequentialReads.read(b, offset, length);\n+            if (streamForSequentialReads.isFullyRead()) {\n+                // the current stream was exhausted by this read, so it should be closed\n+                streamForSequentialReads.close();\n+                streamForSequentialReads = null;\n+            } else {\n+                // the current stream contained enough data for this read and more besides, so we leave it in place\n+                assert read == length : length + \" remaining\";\n+            }\n+\n+            if (read < length) {\n+                // the current stream didn't contain enough data for this read, so we must read more\n+                read += readFromNewSequentialStream(part, pos + read, b, offset + read, length - read);\n+            }\n+        } else {\n+            // not a sequential read, so stop optimizing for this usage pattern and fall through to the unoptimized behaviour\n+            assert streamForSequentialReads.isFullyRead() == false;\n+            sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+            IOUtils.close(streamForSequentialReads);\n+            streamForSequentialReads = null;\n         }\n+        return read;\n+    }\n+\n+    /**\n+     * If appropriate, open a new stream for sequential reading and satisfy the given read using it.\n+     * @return the number of bytes read; if a new stream wasn't opened then nothing was read so the caller should perform the read directly.\n+     */\n+    private int readFromNewSequentialStream(int part, long pos, byte[] b, int offset, int length)\n+        throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a92ced530401cece9969728d48d17b1196f1ec0b"}, "originalPosition": 135}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDc1ODM5NQ==", "bodyText": "It didn't always ;) Fixed in 51d2af5", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r374758395", "createdAt": "2020-02-04T15:55:38Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,12 +107,87 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n-        try (InputStream inputStream = blobContainer.readBlob(fileInfo.partName(part), pos, length)) {\n-            int read = inputStream.read(b, offset, length);\n-            assert read == length;\n-            position += read;\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        int optimizedReadSize = readOptimized(part, pos, b, offset, length);\n+        assert optimizedReadSize <= length;\n+        position += optimizedReadSize;\n+\n+        if (optimizedReadSize < length) {\n+            // we did not read everything in an optimized fashion, so read the remainder directly\n+            try (InputStream inputStream\n+                     = blobContainer.readBlob(fileInfo.partName(part), pos + optimizedReadSize, length - optimizedReadSize)) {\n+                final int directReadSize = inputStream.read(b, offset + optimizedReadSize, length - optimizedReadSize);\n+                assert optimizedReadSize + directReadSize == length : optimizedReadSize + \" and \" + directReadSize + \" vs \" + length;\n+                position += directReadSize;\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Attempt to satisfy this read in an optimized fashion using {@code streamForSequentialReadsRef}.\n+     * @return the number of bytes read\n+     */\n+    private int readOptimized(int part, long pos, byte[] b, int offset, int length) throws IOException {\n+        if (sequentialReadSize == NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            return 0;\n+        }\n+\n+        int read = 0;\n+        if (streamForSequentialReads == null) {\n+            // starting a new sequential read\n+            read = readFromNewSequentialStream(part, pos, b, offset, length);\n+        } else if (streamForSequentialReads.canContinueSequentialRead(part, pos)) {\n+            // continuing a sequential read that we started previously\n+            read = streamForSequentialReads.read(b, offset, length);\n+            if (streamForSequentialReads.isFullyRead()) {\n+                // the current stream was exhausted by this read, so it should be closed\n+                streamForSequentialReads.close();\n+                streamForSequentialReads = null;\n+            } else {\n+                // the current stream contained enough data for this read and more besides, so we leave it in place\n+                assert read == length : length + \" remaining\";\n+            }\n+\n+            if (read < length) {\n+                // the current stream didn't contain enough data for this read, so we must read more\n+                read += readFromNewSequentialStream(part, pos + read, b, offset + read, length - read);\n+            }\n+        } else {\n+            // not a sequential read, so stop optimizing for this usage pattern and fall through to the unoptimized behaviour\n+            assert streamForSequentialReads.isFullyRead() == false;\n+            sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+            IOUtils.close(streamForSequentialReads);\n+            streamForSequentialReads = null;\n         }\n+        return read;\n+    }\n+\n+    /**\n+     * If appropriate, open a new stream for sequential reading and satisfy the given read using it.\n+     * @return the number of bytes read; if a new stream wasn't opened then nothing was read so the caller should perform the read directly.\n+     */\n+    private int readFromNewSequentialStream(int part, long pos, byte[] b, int offset, int length)\n+        throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDY5ODQ0MA=="}, "originalCommit": {"oid": "a92ced530401cece9969728d48d17b1196f1ec0b"}, "originalPosition": 135}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxNzMwMTQzOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxNDoyNDo1NlrOFlV13A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxNTo1NTo0N1rOFlZcWQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDY5OTQ4NA==", "bodyText": "Maybe nullify in a finally block, just in case", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r374699484", "createdAt": "2020-02-04T14:24:56Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -108,19 +197,24 @@ protected void seekInternal(long pos) throws IOException {\n         } else if (pos < 0L) {\n             throw new IOException(\"Seeking to negative position [\" + pos + \"] for \" + toString());\n         }\n-        this.position = offset + pos;\n+        if (position != offset + pos) {\n+            position = offset + pos;\n+            IOUtils.close(streamForSequentialReads);\n+            streamForSequentialReads = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a92ced530401cece9969728d48d17b1196f1ec0b"}, "originalPosition": 167}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDc1ODQ4OQ==", "bodyText": "Oh good point, done in c9cf7bc.", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r374758489", "createdAt": "2020-02-04T15:55:47Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -108,19 +197,24 @@ protected void seekInternal(long pos) throws IOException {\n         } else if (pos < 0L) {\n             throw new IOException(\"Seeking to negative position [\" + pos + \"] for \" + toString());\n         }\n-        this.position = offset + pos;\n+        if (position != offset + pos) {\n+            position = offset + pos;\n+            IOUtils.close(streamForSequentialReads);\n+            streamForSequentialReads = null;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDY5OTQ4NA=="}, "originalCommit": {"oid": "a92ced530401cece9969728d48d17b1196f1ec0b"}, "originalPosition": 167}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxNzMwMzMyOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxNDoyNTozMVrOFlV3Fw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxNTo1NTo1NlrOFlZc1Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDY5OTc5OQ==", "bodyText": "Maybe add a small word on why we can't read optimized for clones?", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r374699799", "createdAt": "2020-02-04T14:25:31Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -108,19 +197,24 @@ protected void seekInternal(long pos) throws IOException {\n         } else if (pos < 0L) {\n             throw new IOException(\"Seeking to negative position [\" + pos + \"] for \" + toString());\n         }\n-        this.position = offset + pos;\n+        if (position != offset + pos) {\n+            position = offset + pos;\n+            IOUtils.close(streamForSequentialReads);\n+            streamForSequentialReads = null;\n+        }\n     }\n \n     @Override\n     public BufferedIndexInput clone() {\n-        return new SearchableSnapshotIndexInput(\"clone(\" + this + \")\", blobContainer, fileInfo, position, offset, length);\n+        return new SearchableSnapshotIndexInput(\"clone(\" + this + \")\", blobContainer, fileInfo, position, offset, length,\n+            NO_SEQUENTIAL_READ_OPTIMIZATION, getBufferSize());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a92ced530401cece9969728d48d17b1196f1ec0b"}, "originalPosition": 175}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDc1ODYxMw==", "bodyText": "Comments added in 2fee6b8.", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r374758613", "createdAt": "2020-02-04T15:55:56Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -108,19 +197,24 @@ protected void seekInternal(long pos) throws IOException {\n         } else if (pos < 0L) {\n             throw new IOException(\"Seeking to negative position [\" + pos + \"] for \" + toString());\n         }\n-        this.position = offset + pos;\n+        if (position != offset + pos) {\n+            position = offset + pos;\n+            IOUtils.close(streamForSequentialReads);\n+            streamForSequentialReads = null;\n+        }\n     }\n \n     @Override\n     public BufferedIndexInput clone() {\n-        return new SearchableSnapshotIndexInput(\"clone(\" + this + \")\", blobContainer, fileInfo, position, offset, length);\n+        return new SearchableSnapshotIndexInput(\"clone(\" + this + \")\", blobContainer, fileInfo, position, offset, length,\n+            NO_SEQUENTIAL_READ_OPTIMIZATION, getBufferSize());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDY5OTc5OQ=="}, "originalCommit": {"oid": "a92ced530401cece9969728d48d17b1196f1ec0b"}, "originalPosition": 175}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4568, "cost": 1, "resetAt": "2021-11-12T13:16:51Z"}}}