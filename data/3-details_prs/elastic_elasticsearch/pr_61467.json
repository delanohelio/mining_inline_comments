{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDcyNTAxNjgw", "number": 61467, "title": "Speed up date_histogram by precomputing ranges", "bodyText": "A few of us were talking about ways to speed up the date_histogram\nusing the index for the timestamp rather than the doc values. To do that\nwe'd have to pre-compute all of the \"round down\" points in the index. It\nturns out that just precomputing those values speeds up rounding\nfairly significantly:\nBenchmark  (count)      (interval)                   (range)            (zone)  Mode  Cnt          Score         Error  Units\nbefore    10000000  calendar month  2000-10-28 to 2000-10-31               UTC  avgt   10   96461080.982 \u00b1  616373.011  ns/op\nbefore    10000000  calendar month  2000-10-28 to 2000-10-31  America/New_York  avgt   10  130598950.850 \u00b1 1249189.867  ns/op\nafter     10000000  calendar month  2000-10-28 to 2000-10-31               UTC  avgt   10   52311775.080 \u00b1  107171.092  ns/op\nafter     10000000  calendar month  2000-10-28 to 2000-10-31  America/New_York  avgt   10   54800134.968 \u00b1  373844.796  ns/op\n\nThat's a 46% speed up when there isn't a time zone and a 58% speed up\nwhen there is.\nThis doesn't work for every time zone, specifically those that have two\nmidnights in a single day due to daylight savings time will produce wonky\nresults. So they don't get the optimization.\nSecond, this requires a few expensive computation up front to make the\ntransition array. And if the transition array is too large then we give\nup and use the original mechanism, throwing away all of the work we did\nto build the array. This seems appropriate for most usages of round,\nbut this change uses it for all usages of round. That seems ok for\nnow, but it might be worth investigating in a follow up.\nI ran a macrobenchmark as well which showed an 11% preformance\nimprovement. BUT the benchmark wasn't tuned for my desktop so it\noverwhelmed it and might have produced \"funny\" results. I think it is\npretty clear that this is an improvement, but know the measurement is\nweird:\nBefore:\n|               Min Throughput | hourly_agg |        0.11 |  ops/s |\n|            Median Throughput | hourly_agg |        0.11 |  ops/s |\n|               Max Throughput | hourly_agg |        0.11 |  ops/s |\n|      50th percentile latency | hourly_agg |      650623 |     ms |\n|      90th percentile latency | hourly_agg |      821478 |     ms |\n|      99th percentile latency | hourly_agg |      859780 |     ms |\n|     100th percentile latency | hourly_agg |      864030 |     ms |\n| 50th percentile service time | hourly_agg |     9268.71 |     ms |\n| 90th percentile service time | hourly_agg |        9380 |     ms |\n| 99th percentile service time | hourly_agg |     9626.88 |     ms |\n|100th percentile service time | hourly_agg |     9884.27 |     ms |\n|                   error rate | hourly_agg |           0 |      % |\n\nAfter:\n|               Min Throughput | hourly_agg |        0.12 |  ops/s |\n|            Median Throughput | hourly_agg |        0.12 |  ops/s |\n|               Max Throughput | hourly_agg |        0.12 |  ops/s |\n|      50th percentile latency | hourly_agg |      519254 |     ms |\n|      90th percentile latency | hourly_agg |      653099 |     ms |\n|      99th percentile latency | hourly_agg |      683276 |     ms |\n|     100th percentile latency | hourly_agg |      686611 |     ms |\n| 50th percentile service time | hourly_agg |     8371.41 |     ms |\n| 90th percentile service time | hourly_agg |     8407.02 |     ms |\n| 99th percentile service time | hourly_agg |     8536.64 |     ms |\n|100th percentile service time | hourly_agg |     8538.54 |     ms |\n|                   error rate | hourly_agg |           0 |      % |", "createdAt": "2020-08-24T13:02:40Z", "url": "https://github.com/elastic/elasticsearch/pull/61467", "merged": true, "mergeCommit": {"oid": "d9acac2672ff5a8b1f18669afb268d87c978da24"}, "closed": true, "closedAt": "2020-09-24T14:10:20Z", "author": {"login": "nik9000"}, "timelineItems": {"totalCount": 22, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdCCRbmABqjM2ODUzMjk3MDg=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdMAql_gFqTQ5NTUzNTQ3OQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a817685624db239819eb0bf7cb66c58451f6d140", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/a817685624db239819eb0bf7cb66c58451f6d140", "committedDate": "2020-08-24T13:03:42Z", "message": "Speed up date_histogram by precomputing ranges\n\nA few of us were talking about ways to speed up the `date_histogram`\nusing the index for the timestamp rather than the doc values. To do that\nwe'd have to pre-compute all of the \"round down\" points in the index. It\nturns out that *just* precomputing those values speeds up rounding\nfairly significantly:\n```\nBenchmark  (count)      (interval)                   (range)            (zone)  Mode  Cnt          Score         Error  Units\nbefore    10000000  calendar month  2000-10-28 to 2000-10-31               UTC  avgt   10   96461080.982 \u00b1  616373.011  ns/op\nbefore    10000000  calendar month  2000-10-28 to 2000-10-31  America/New_York  avgt   10  130598950.850 \u00b1 1249189.867  ns/op\nafter     10000000  calendar month  2000-10-28 to 2000-10-31               UTC  avgt   10   52311775.080 \u00b1  107171.092  ns/op\nafter     10000000  calendar month  2000-10-28 to 2000-10-31  America/New_York  avgt   10   54800134.968 \u00b1  373844.796  ns/op\n```\n\nThat's a 46% speed up when there isn't a time zone and a 58% speed up\nwhen there is.\n\nThis doesn't work for every time zone, specifically those that have two\nmidnights in a single day due to daylight savings time will produce wonky\nresults. So they don't get the optimization.\n\nSecond, this requires a few expensive computation up front to make the\ntransition array. And if the transition array is too large then we give\nup and use the original mechanism, throwing away all of the work we did\nto build the array. This seems appropriate for most usages of `round`,\nbut this change uses it for *all* usages of `round`. That seems ok for\nnow, but it might be worth investigating in a follow up.\n\nI ran a macrobenchmark as well which showed an 11% preformance\nimprovement. *BUT* the benchmark wasn't tuned for my desktop so it\noverwhelmed it and might have produced \"funny\" results. I think it is\npretty clear that this is an improvement, but know the measurement is\nweird:\n\n```\nBenchmark  (count)      (interval)                   (range)            (zone)  Mode  Cnt          Score         Error  Units\nbefore    10000000  calendar month  2000-10-28 to 2000-10-31               UTC  avgt   10   96461080.982 \u00b1  616373.011  ns/op\nbefore    10000000  calendar month  2000-10-28 to 2000-10-31  America/New_York  avgt   10  g\u00b1 1249189.867  ns/op\nafter     10000000  calendar month  2000-10-28 to 2000-10-31               UTC  avgt   10   52311775.080 \u00b1  107171.092  ns/op\nafter     10000000  calendar month  2000-10-28 to 2000-10-31  America/New_York  avgt   10   54800134.968 \u00b1  373844.796  ns/op\n\nBefore:\n|               Min Throughput | hourly_agg |        0.11 |  ops/s |\n|            Median Throughput | hourly_agg |        0.11 |  ops/s |\n|               Max Throughput | hourly_agg |        0.11 |  ops/s |\n|      50th percentile latency | hourly_agg |      650623 |     ms |\n|      90th percentile latency | hourly_agg |      821478 |     ms |\n|      99th percentile latency | hourly_agg |      859780 |     ms |\n|     100th percentile latency | hourly_agg |      864030 |     ms |\n| 50th percentile service time | hourly_agg |     9268.71 |     ms |\n| 90th percentile service time | hourly_agg |        9380 |     ms |\n| 99th percentile service time | hourly_agg |     9626.88 |     ms |\n|100th percentile service time | hourly_agg |     9884.27 |     ms |\n|                   error rate | hourly_agg |           0 |      % |\n\nAfter:\n|               Min Throughput | hourly_agg |        0.12 |  ops/s |\n|            Median Throughput | hourly_agg |        0.12 |  ops/s |\n|               Max Throughput | hourly_agg |        0.12 |  ops/s |\n|      50th percentile latency | hourly_agg |      519254 |     ms |\n|      90th percentile latency | hourly_agg |      653099 |     ms |\n|      99th percentile latency | hourly_agg |      683276 |     ms |\n|     100th percentile latency | hourly_agg |      686611 |     ms |\n| 50th percentile service time | hourly_agg |     8371.41 |     ms |\n| 90th percentile service time | hourly_agg |     8407.02 |     ms |\n| 99th percentile service time | hourly_agg |     8536.64 |     ms |\n|100th percentile service time | hourly_agg |     8538.54 |     ms |\n|                   error rate | hourly_agg |           0 |      % |\n```"}, "afterCommit": {"oid": "7687e30291243995cffeb7d633a93bf798c29f57", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/7687e30291243995cffeb7d633a93bf798c29f57", "committedDate": "2020-08-24T13:04:58Z", "message": "Speed up date_histogram by precomputing ranges\n\nA few of us were talking about ways to speed up the `date_histogram`\nusing the index for the timestamp rather than the doc values. To do that\nwe'd have to pre-compute all of the \"round down\" points in the index. It\nturns out that *just* precomputing those values speeds up rounding\nfairly significantly:\n```\nBenchmark  (count)      (interval)                   (range)            (zone)  Mode  Cnt          Score         Error  Units\nbefore    10000000  calendar month  2000-10-28 to 2000-10-31               UTC  avgt   10   96461080.982 \u00b1  616373.011  ns/op\nbefore    10000000  calendar month  2000-10-28 to 2000-10-31  America/New_York  avgt   10  130598950.850 \u00b1 1249189.867  ns/op\nafter     10000000  calendar month  2000-10-28 to 2000-10-31               UTC  avgt   10   52311775.080 \u00b1  107171.092  ns/op\nafter     10000000  calendar month  2000-10-28 to 2000-10-31  America/New_York  avgt   10   54800134.968 \u00b1  373844.796  ns/op\n```\n\nThat's a 46% speed up when there isn't a time zone and a 58% speed up\nwhen there is.\n\nThis doesn't work for every time zone, specifically those that have two\nmidnights in a single day due to daylight savings time will produce wonky\nresults. So they don't get the optimization.\n\nSecond, this requires a few expensive computation up front to make the\ntransition array. And if the transition array is too large then we give\nup and use the original mechanism, throwing away all of the work we did\nto build the array. This seems appropriate for most usages of `round`,\nbut this change uses it for *all* usages of `round`. That seems ok for\nnow, but it might be worth investigating in a follow up.\n\nI ran a macrobenchmark as well which showed an 11% preformance\nimprovement. *BUT* the benchmark wasn't tuned for my desktop so it\noverwhelmed it and might have produced \"funny\" results. I think it is\npretty clear that this is an improvement, but know the measurement is\nweird:\n\n```\nBenchmark  (count)      (interval)                   (range)            (zone)  Mode  Cnt          Score         Error  Units\nbefore    10000000  calendar month  2000-10-28 to 2000-10-31               UTC  avgt   10   96461080.982 \u00b1  616373.011  ns/op\nbefore    10000000  calendar month  2000-10-28 to 2000-10-31  America/New_York  avgt   10  g\u00b1 1249189.867  ns/op\nafter     10000000  calendar month  2000-10-28 to 2000-10-31               UTC  avgt   10   52311775.080 \u00b1  107171.092  ns/op\nafter     10000000  calendar month  2000-10-28 to 2000-10-31  America/New_York  avgt   10   54800134.968 \u00b1  373844.796  ns/op\n\nBefore:\n|               Min Throughput | hourly_agg |        0.11 |  ops/s |\n|            Median Throughput | hourly_agg |        0.11 |  ops/s |\n|               Max Throughput | hourly_agg |        0.11 |  ops/s |\n|      50th percentile latency | hourly_agg |      650623 |     ms |\n|      90th percentile latency | hourly_agg |      821478 |     ms |\n|      99th percentile latency | hourly_agg |      859780 |     ms |\n|     100th percentile latency | hourly_agg |      864030 |     ms |\n| 50th percentile service time | hourly_agg |     9268.71 |     ms |\n| 90th percentile service time | hourly_agg |        9380 |     ms |\n| 99th percentile service time | hourly_agg |     9626.88 |     ms |\n|100th percentile service time | hourly_agg |     9884.27 |     ms |\n|                   error rate | hourly_agg |           0 |      % |\n\nAfter:\n|               Min Throughput | hourly_agg |        0.12 |  ops/s |\n|            Median Throughput | hourly_agg |        0.12 |  ops/s |\n|               Max Throughput | hourly_agg |        0.12 |  ops/s |\n|      50th percentile latency | hourly_agg |      519254 |     ms |\n|      90th percentile latency | hourly_agg |      653099 |     ms |\n|      99th percentile latency | hourly_agg |      683276 |     ms |\n|     100th percentile latency | hourly_agg |      686611 |     ms |\n| 50th percentile service time | hourly_agg |     8371.41 |     ms |\n| 90th percentile service time | hourly_agg |     8407.02 |     ms |\n| 99th percentile service time | hourly_agg |     8536.64 |     ms |\n|100th percentile service time | hourly_agg |     8538.54 |     ms |\n|                   error rate | hourly_agg |           0 |      % |\n```"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7687e30291243995cffeb7d633a93bf798c29f57", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/7687e30291243995cffeb7d633a93bf798c29f57", "committedDate": "2020-08-24T13:04:58Z", "message": "Speed up date_histogram by precomputing ranges\n\nA few of us were talking about ways to speed up the `date_histogram`\nusing the index for the timestamp rather than the doc values. To do that\nwe'd have to pre-compute all of the \"round down\" points in the index. It\nturns out that *just* precomputing those values speeds up rounding\nfairly significantly:\n```\nBenchmark  (count)      (interval)                   (range)            (zone)  Mode  Cnt          Score         Error  Units\nbefore    10000000  calendar month  2000-10-28 to 2000-10-31               UTC  avgt   10   96461080.982 \u00b1  616373.011  ns/op\nbefore    10000000  calendar month  2000-10-28 to 2000-10-31  America/New_York  avgt   10  130598950.850 \u00b1 1249189.867  ns/op\nafter     10000000  calendar month  2000-10-28 to 2000-10-31               UTC  avgt   10   52311775.080 \u00b1  107171.092  ns/op\nafter     10000000  calendar month  2000-10-28 to 2000-10-31  America/New_York  avgt   10   54800134.968 \u00b1  373844.796  ns/op\n```\n\nThat's a 46% speed up when there isn't a time zone and a 58% speed up\nwhen there is.\n\nThis doesn't work for every time zone, specifically those that have two\nmidnights in a single day due to daylight savings time will produce wonky\nresults. So they don't get the optimization.\n\nSecond, this requires a few expensive computation up front to make the\ntransition array. And if the transition array is too large then we give\nup and use the original mechanism, throwing away all of the work we did\nto build the array. This seems appropriate for most usages of `round`,\nbut this change uses it for *all* usages of `round`. That seems ok for\nnow, but it might be worth investigating in a follow up.\n\nI ran a macrobenchmark as well which showed an 11% preformance\nimprovement. *BUT* the benchmark wasn't tuned for my desktop so it\noverwhelmed it and might have produced \"funny\" results. I think it is\npretty clear that this is an improvement, but know the measurement is\nweird:\n\n```\nBenchmark  (count)      (interval)                   (range)            (zone)  Mode  Cnt          Score         Error  Units\nbefore    10000000  calendar month  2000-10-28 to 2000-10-31               UTC  avgt   10   96461080.982 \u00b1  616373.011  ns/op\nbefore    10000000  calendar month  2000-10-28 to 2000-10-31  America/New_York  avgt   10  g\u00b1 1249189.867  ns/op\nafter     10000000  calendar month  2000-10-28 to 2000-10-31               UTC  avgt   10   52311775.080 \u00b1  107171.092  ns/op\nafter     10000000  calendar month  2000-10-28 to 2000-10-31  America/New_York  avgt   10   54800134.968 \u00b1  373844.796  ns/op\n\nBefore:\n|               Min Throughput | hourly_agg |        0.11 |  ops/s |\n|            Median Throughput | hourly_agg |        0.11 |  ops/s |\n|               Max Throughput | hourly_agg |        0.11 |  ops/s |\n|      50th percentile latency | hourly_agg |      650623 |     ms |\n|      90th percentile latency | hourly_agg |      821478 |     ms |\n|      99th percentile latency | hourly_agg |      859780 |     ms |\n|     100th percentile latency | hourly_agg |      864030 |     ms |\n| 50th percentile service time | hourly_agg |     9268.71 |     ms |\n| 90th percentile service time | hourly_agg |        9380 |     ms |\n| 99th percentile service time | hourly_agg |     9626.88 |     ms |\n|100th percentile service time | hourly_agg |     9884.27 |     ms |\n|                   error rate | hourly_agg |           0 |      % |\n\nAfter:\n|               Min Throughput | hourly_agg |        0.12 |  ops/s |\n|            Median Throughput | hourly_agg |        0.12 |  ops/s |\n|               Max Throughput | hourly_agg |        0.12 |  ops/s |\n|      50th percentile latency | hourly_agg |      519254 |     ms |\n|      90th percentile latency | hourly_agg |      653099 |     ms |\n|      99th percentile latency | hourly_agg |      683276 |     ms |\n|     100th percentile latency | hourly_agg |      686611 |     ms |\n| 50th percentile service time | hourly_agg |     8371.41 |     ms |\n| 90th percentile service time | hourly_agg |     8407.02 |     ms |\n| 99th percentile service time | hourly_agg |     8536.64 |     ms |\n|100th percentile service time | hourly_agg |     8538.54 |     ms |\n|                   error rate | hourly_agg |           0 |      % |\n```"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDczNDYxNjg4", "url": "https://github.com/elastic/elasticsearch/pull/61467#pullrequestreview-473461688", "createdAt": "2020-08-24T13:26:51Z", "commit": {"oid": "7687e30291243995cffeb7d633a93bf798c29f57"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQxMzoyNjo1MVrOHFkNPA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQxMzozMDoxMVrOHFkVTQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTU5ODE0MA==", "bodyText": "would it be doable to detect timezones that don't work with this optimization at runtime instead of maintaining an allowlist?", "url": "https://github.com/elastic/elasticsearch/pull/61467#discussion_r475598140", "createdAt": "2020-08-24T13:26:51Z", "author": {"login": "jpountz"}, "path": "server/src/main/java/org/elasticsearch/common/Rounding.java", "diffHunk": "@@ -401,8 +404,22 @@ private LocalDateTime truncateLocalDateTime(LocalDateTime localDateTime) {\n             }\n         }\n \n+        /**\n+         * Time zones with two midnights get \"funny\" non-continuous rounding\n+         * that isn't compatible with the pre-computed array rounding.\n+         */\n+        private static final Set<String> HAS_TWO_MIDNIGHTS = Set.of(\"America/Moncton\", \"America/St_Johns\", \"Canada/Newfoundland\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7687e30291243995cffeb7d633a93bf798c29f57"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTU5ODk2OQ==", "bodyText": "is this change fixing an existing bug?", "url": "https://github.com/elastic/elasticsearch/pull/61467#discussion_r475598969", "createdAt": "2020-08-24T13:28:17Z", "author": {"login": "jpountz"}, "path": "server/src/main/java/org/elasticsearch/common/Rounding.java", "diffHunk": "@@ -1015,7 +1031,7 @@ public byte id() {\n \n         @Override\n         public Prepared prepare(long minUtcMillis, long maxUtcMillis) {\n-            return wrapPreparedRounding(delegate.prepare(minUtcMillis, maxUtcMillis));\n+            return wrapPreparedRounding(delegate.prepare(minUtcMillis - offset, maxUtcMillis - offset));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7687e30291243995cffeb7d633a93bf798c29f57"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTYwMDA1Mw==", "bodyText": "make it final?", "url": "https://github.com/elastic/elasticsearch/pull/61467#discussion_r475600053", "createdAt": "2020-08-24T13:29:55Z", "author": {"login": "jpountz"}, "path": "server/src/main/java/org/elasticsearch/common/Rounding.java", "diffHunk": "@@ -1085,4 +1101,54 @@ public static Rounding read(StreamInput in) throws IOException {\n                 throw new ElasticsearchException(\"unknown rounding id [\" + id + \"]\");\n         }\n     }\n+\n+    /**\n+     * Attempt to build a {@link Prepared} implementation that relies on pre-calcuated\n+     * \"round down\" points. If there would be more than {@code max} points then return\n+     * the original implementation, otherwise return the new, faster implementation.\n+     */\n+    static Prepared maybeUseArray(Prepared orig, long minUtcMillis, long maxUtcMillis, int max) {\n+        long[] values = new long[1];\n+        long rounded = orig.round(minUtcMillis);\n+        int i = 0;\n+        values[i++] = rounded;\n+        while ((rounded = orig.nextRoundingValue(rounded)) <= maxUtcMillis) {\n+            if (i >= max) {\n+                return orig;\n+            }\n+            assert values[i - 1] == orig.round(rounded - 1);\n+            values = ArrayUtil.grow(values, i + 1);\n+            values[i++]= rounded;\n+        }\n+        return new ArrayRounding(values, i, orig);\n+    }\n+\n+    /**\n+     * Implementation of {@link Prepared} using pre-calculated \"round down\" points.\n+     */\n+    private static class ArrayRounding implements Prepared {\n+        private final long[] values;\n+        private int max;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7687e30291243995cffeb7d633a93bf798c29f57"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTYwMDIwNQ==", "bodyText": "maybe assert that idx is neither -1 nor -1 - max?", "url": "https://github.com/elastic/elasticsearch/pull/61467#discussion_r475600205", "createdAt": "2020-08-24T13:30:11Z", "author": {"login": "jpountz"}, "path": "server/src/main/java/org/elasticsearch/common/Rounding.java", "diffHunk": "@@ -1085,4 +1101,54 @@ public static Rounding read(StreamInput in) throws IOException {\n                 throw new ElasticsearchException(\"unknown rounding id [\" + id + \"]\");\n         }\n     }\n+\n+    /**\n+     * Attempt to build a {@link Prepared} implementation that relies on pre-calcuated\n+     * \"round down\" points. If there would be more than {@code max} points then return\n+     * the original implementation, otherwise return the new, faster implementation.\n+     */\n+    static Prepared maybeUseArray(Prepared orig, long minUtcMillis, long maxUtcMillis, int max) {\n+        long[] values = new long[1];\n+        long rounded = orig.round(minUtcMillis);\n+        int i = 0;\n+        values[i++] = rounded;\n+        while ((rounded = orig.nextRoundingValue(rounded)) <= maxUtcMillis) {\n+            if (i >= max) {\n+                return orig;\n+            }\n+            assert values[i - 1] == orig.round(rounded - 1);\n+            values = ArrayUtil.grow(values, i + 1);\n+            values[i++]= rounded;\n+        }\n+        return new ArrayRounding(values, i, orig);\n+    }\n+\n+    /**\n+     * Implementation of {@link Prepared} using pre-calculated \"round down\" points.\n+     */\n+    private static class ArrayRounding implements Prepared {\n+        private final long[] values;\n+        private int max;\n+        private final Prepared delegate;\n+\n+        private ArrayRounding(long[] values, int max, Prepared delegate) {\n+            this.values = values;\n+            this.max = max;\n+            this.delegate = delegate;\n+        }\n+\n+        @Override\n+        public long round(long utcMillis) {\n+            int idx = Arrays.binarySearch(values, 0, max, utcMillis);\n+            if (idx < 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7687e30291243995cffeb7d633a93bf798c29f57"}, "originalPosition": 102}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e68463de6718e51ad5f37692b97877f146cda60b", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/e68463de6718e51ad5f37692b97877f146cda60b", "committedDate": "2020-08-24T13:44:17Z", "message": "iter"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c13740b5d975d2e19a1521f4fdd6d9de754ee9d1", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/c13740b5d975d2e19a1521f4fdd6d9de754ee9d1", "committedDate": "2020-08-24T14:23:05Z", "message": "Another test"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc1ODQ3MDgx", "url": "https://github.com/elastic/elasticsearch/pull/61467#pullrequestreview-475847081", "createdAt": "2020-08-26T20:47:35Z", "commit": {"oid": "c13740b5d975d2e19a1521f4fdd6d9de754ee9d1"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQyMDo0NzozNVrOHHdKDw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQyMDo1MDowM1rOHHdOtg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzU3OTc5MQ==", "bodyText": "I thought Canada was changing time at 2 am, on another side, some other known time zones such as Asia/Gaza for example seems to indeed have 2 mightnights. I did some quick and dirty test and I have got a slightly different list for timezones in my JVM:\nAmerica/Asuncion 2020-03-22T00:00 -> 2020-03-21T23:00\nAmerica/Havana 2020-11-01T01:00 -> 2020-11-01T00:00\nAmerica/Santiago 2020-04-05T00:00 -> 2020-04-04T23:00\nAmerica/Scoresbysund 2020-10-25T01:00 -> 2020-10-25T00:00\nAsia/Amman 2020-10-30T01:00 -> 2020-10-30T00:00\nAsia/Beirut 2020-10-25T00:00 -> 2020-10-24T23:00\nAsia/Damascus 2020-10-30T00:00 -> 2020-10-29T23:00\nAsia/Gaza 2020-10-31T01:00 -> 2020-10-31T00:00\nAsia/Hebron 2020-10-31T01:00 -> 2020-10-31T00:00\nAsia/Tehran 2020-09-21T00:00 -> 2020-09-20T23:00\nAtlantic/Azores 2020-10-25T01:00 -> 2020-10-25T00:00\nChile/Continental 2020-04-05T00:00 -> 2020-04-04T23:00\nCuba 2020-11-01T01:00 -> 2020-11-01T00:00\nIran 2020-09-21T00:00 -> 2020-09-20T23:00", "url": "https://github.com/elastic/elasticsearch/pull/61467#discussion_r477579791", "createdAt": "2020-08-26T20:47:35Z", "author": {"login": "imotov"}, "path": "server/src/main/java/org/elasticsearch/common/Rounding.java", "diffHunk": "@@ -401,8 +404,22 @@ private LocalDateTime truncateLocalDateTime(LocalDateTime localDateTime) {\n             }\n         }\n \n+        /**\n+         * Time zones with two midnights get \"funny\" non-continuous rounding\n+         * that isn't compatible with the pre-computed array rounding.\n+         */\n+        private static final Set<String> HAS_TWO_MIDNIGHTS = Set.of(\"America/Moncton\", \"America/St_Johns\", \"Canada/Newfoundland\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTU5ODE0MA=="}, "originalCommit": {"oid": "7687e30291243995cffeb7d633a93bf798c29f57"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzU4MDk4Mg==", "bodyText": "This will need to implement new methods added in #61369.", "url": "https://github.com/elastic/elasticsearch/pull/61467#discussion_r477580982", "createdAt": "2020-08-26T20:50:03Z", "author": {"login": "imotov"}, "path": "server/src/main/java/org/elasticsearch/common/Rounding.java", "diffHunk": "@@ -1085,4 +1101,57 @@ public static Rounding read(StreamInput in) throws IOException {\n                 throw new ElasticsearchException(\"unknown rounding id [\" + id + \"]\");\n         }\n     }\n+\n+    /**\n+     * Attempt to build a {@link Prepared} implementation that relies on pre-calcuated\n+     * \"round down\" points. If there would be more than {@code max} points then return\n+     * the original implementation, otherwise return the new, faster implementation.\n+     */\n+    static Prepared maybeUseArray(Prepared orig, long minUtcMillis, long maxUtcMillis, int max) {\n+        long[] values = new long[1];\n+        long rounded = orig.round(minUtcMillis);\n+        int i = 0;\n+        values[i++] = rounded;\n+        while ((rounded = orig.nextRoundingValue(rounded)) <= maxUtcMillis) {\n+            if (i >= max) {\n+                return orig;\n+            }\n+            assert values[i - 1] == orig.round(rounded - 1);\n+            values = ArrayUtil.grow(values, i + 1);\n+            values[i++]= rounded;\n+        }\n+        return new ArrayRounding(values, i, orig);\n+    }\n+\n+    /**\n+     * Implementation of {@link Prepared} using pre-calculated \"round down\" points.\n+     */\n+    private static class ArrayRounding implements Prepared {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c13740b5d975d2e19a1521f4fdd6d9de754ee9d1"}, "originalPosition": 88}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4162ef1bcb55987e0127b2c839a0c937959842f1", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/4162ef1bcb55987e0127b2c839a0c937959842f1", "committedDate": "2020-08-27T13:04:46Z", "message": "Merge branch 'master' into binary_search_for_rounding"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "02f0fe3b032c89f896118de6150d23ece8e91eab", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/02f0fe3b032c89f896118de6150d23ece8e91eab", "committedDate": "2020-08-27T14:10:00Z", "message": "Implement new method"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4fbcfbf0ae707e70536d9796aaa65ac6035f7a6f", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/4fbcfbf0ae707e70536d9796aaa65ac6035f7a6f", "committedDate": "2020-08-31T14:47:06Z", "message": "Skip method for bad times"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7a7ef49a6b0a65dcb8f86996401f5885883be745", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/7a7ef49a6b0a65dcb8f86996401f5885883be745", "committedDate": "2020-09-01T15:13:44Z", "message": "Merge branch 'master' into binary_search_for_rounding"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgxMDE3Mjg0", "url": "https://github.com/elastic/elasticsearch/pull/61467#pullrequestreview-481017284", "createdAt": "2020-09-02T16:55:27Z", "commit": {"oid": "7a7ef49a6b0a65dcb8f86996401f5885883be745"}, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxNjo1NToyN1rOHL4Zfg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxNzoyNjoyMFrOHL5woQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIyMDQxNA==", "bodyText": "Nit - It looks like this maps timezone name to milliseconds since epoch for when the zone changed to use a different transition time.  Can you just add a comment clarifying that's how the map is structured?  It seems unlikely we'd need to add to it, but just in case we do, it'd help to have a reference for how to do so handy.", "url": "https://github.com/elastic/elasticsearch/pull/61467#discussion_r482220414", "createdAt": "2020-09-02T16:55:27Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/common/Rounding.java", "diffHunk": "@@ -467,8 +470,33 @@ private LocalDateTime truncateLocalDateTime(LocalDateTime localDateTime) {\n             }\n         }\n \n+        /**\n+         * Time zones have a daylight savings time transition after midnight that\n+         * transitions back before midnight break the array-based rounding so\n+         * we don't use it for them during those transitions. Luckily they are\n+         * fairly rare and a while in the past. Note: we can use the array based\n+         * rounding if the transition is <strong>at</strong> midnight. Just not\n+         * if it is <strong>after</strong> it.\n+         */\n+        private static final Map<String, Long> FORWARDS_BACKWRADS_ZONES = Map.ofEntries(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7a7ef49a6b0a65dcb8f86996401f5885883be745"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjI0MjcyMQ==", "bodyText": "It's not super clear to me what this assert is guarding against.  Can you add a comment please?", "url": "https://github.com/elastic/elasticsearch/pull/61467#discussion_r482242721", "createdAt": "2020-09-02T17:26:20Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/common/Rounding.java", "diffHunk": "@@ -1186,4 +1213,62 @@ public static Rounding read(StreamInput in) throws IOException {\n                 throw new ElasticsearchException(\"unknown rounding id [\" + id + \"]\");\n         }\n     }\n+\n+    /**\n+     * Attempt to build a {@link Prepared} implementation that relies on pre-calcuated\n+     * \"round down\" points. If there would be more than {@code max} points then return\n+     * the original implementation, otherwise return the new, faster implementation.\n+     */\n+    static Prepared maybeUseArray(Prepared orig, long minUtcMillis, long maxUtcMillis, int max) {\n+        long[] values = new long[1];\n+        long rounded = orig.round(minUtcMillis);\n+        int i = 0;\n+        values[i++] = rounded;\n+        while ((rounded = orig.nextRoundingValue(rounded)) <= maxUtcMillis) {\n+            if (i >= max) {\n+                return orig;\n+            }\n+            assert values[i - 1] == orig.round(rounded - 1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7a7ef49a6b0a65dcb8f86996401f5885883be745"}, "originalPosition": 89}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4f23de0f7431c34754f56f87f73edfed53ea12a9", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/4f23de0f7431c34754f56f87f73edfed53ea12a9", "committedDate": "2020-09-14T13:25:01Z", "message": "Merge branch 'master' into binary_search_for_rounding"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "178a1586527a2f208189fc11a904c1f27c8ee3de", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/178a1586527a2f208189fc11a904c1f27c8ee3de", "committedDate": "2020-09-14T13:36:52Z", "message": "comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f201a25ba7be7f2dace68730a1e2abb8458d79c3", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/f201a25ba7be7f2dace68730a1e2abb8458d79c3", "committedDate": "2020-09-14T16:00:21Z", "message": "Detect!"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0e937293300f2c2796e1cf486350f6859d110464", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/0e937293300f2c2796e1cf486350f6859d110464", "committedDate": "2020-09-14T17:50:12Z", "message": "Test update"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "35ef1c4e6f8757dcde5f47ba19678d9a9cd9a493", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/35ef1c4e6f8757dcde5f47ba19678d9a9cd9a493", "committedDate": "2020-09-14T18:07:00Z", "message": "Comment on number"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f1e286ebba0ce02bb37d3ee4ef8696852e6aec7b", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/f1e286ebba0ce02bb37d3ee4ef8696852e6aec7b", "committedDate": "2020-09-15T17:57:27Z", "message": "Only 12+"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg4ODg5MTY0", "url": "https://github.com/elastic/elasticsearch/pull/61467#pullrequestreview-488889164", "createdAt": "2020-09-15T17:05:07Z", "commit": {"oid": "35ef1c4e6f8757dcde5f47ba19678d9a9cd9a493"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQxNzowNTowOFrOHSLdrQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQxNzowNTowOFrOHSLdrQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODgyNDIzNw==", "bodyText": "This test fails in CI but not for me locally! I'm looking into it. Might be the specific JVM or something.", "url": "https://github.com/elastic/elasticsearch/pull/61467#discussion_r488824237", "createdAt": "2020-09-15T17:05:08Z", "author": {"login": "nik9000"}, "path": "server/src/test/java/org/elasticsearch/common/LocalTimeOffsetTests.java", "diffHunk": "@@ -241,6 +246,25 @@ public void testGap() {\n         assertThat(gapOffset.localToUtc(localSkippedTime, useValueForGap(gapValue)), equalTo(gapValue));\n     }\n \n+    public void testKnownMovesBackToPreviousDay() {\n+        assertKnownMovesBacktoPreviousDay(\"America/Goose_Bay\", \"2010-11-07T03:01:00\");\n+        assertKnownMovesBacktoPreviousDay(\"America/Moncton\", \"2006-10-29T03:01:00\");\n+        assertKnownMovesBacktoPreviousDay(\"America/Moncton\", \"2005-10-29T03:01:00\");\n+        assertKnownMovesBacktoPreviousDay(\"America/St_Johns\", \"2010-11-07T02:31:00\");\n+        assertKnownMovesBacktoPreviousDay(\"Canada/Newfoundland\", \"2010-11-07T02:31:00\");\n+        assertKnownMovesBacktoPreviousDay(\"Pacific/Guam\", \"1969-01-25T13:01:00\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35ef1c4e6f8757dcde5f47ba19678d9a9cd9a493"}, "originalPosition": 55}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk1MDAwMTQ1", "url": "https://github.com/elastic/elasticsearch/pull/61467#pullrequestreview-495000145", "createdAt": "2020-09-23T19:55:14Z", "commit": {"oid": "f1e286ebba0ce02bb37d3ee4ef8696852e6aec7b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QxOTo1NToxNFrOHW-rCA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QxOTo1NToxNFrOHW-rCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg1NzU0NA==", "bodyText": "should we check getHour() too for safety? Eg. if such a thing existed as moving from 1AM to 11PM the day before it looks like we'd still consider that the transition doesn't move back to the previous day with this code?", "url": "https://github.com/elastic/elasticsearch/pull/61467#discussion_r493857544", "createdAt": "2020-09-23T19:55:14Z", "author": {"login": "jpountz"}, "path": "server/src/main/java/org/elasticsearch/common/LocalTimeOffset.java", "diffHunk": "@@ -505,7 +558,25 @@ protected static Transition buildTransition(ZoneOffsetTransition transition, Loc\n             }\n             long firstOverlappingLocalTime = utcStart + offsetAfterMillis;\n             long firstNonOverlappingLocalTime = utcStart + offsetBeforeMillis;\n-            return new Overlap(offsetAfterMillis, previous, utcStart, firstOverlappingLocalTime, firstNonOverlappingLocalTime);\n+            return new Overlap(\n+                offsetAfterMillis,\n+                previous,\n+                utcStart,\n+                firstOverlappingLocalTime,\n+                firstNonOverlappingLocalTime,\n+                movesBackToPreviousDay(transition)\n+            );\n+        }\n+\n+        private static boolean movesBackToPreviousDay(ZoneOffsetTransition transition) {\n+            if (transition.getDateTimeBefore().getDayOfMonth() == transition.getDateTimeAfter().getDayOfMonth()) {\n+                return false;\n+            }\n+            if (transition.getDateTimeBefore().getMinute() == 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f1e286ebba0ce02bb37d3ee4ef8696852e6aec7b"}, "originalPosition": 145}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d2f1d2321bb6ac1f0f0819ba126b0b115bc60e98", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/d2f1d2321bb6ac1f0f0819ba126b0b115bc60e98", "committedDate": "2020-09-24T12:35:25Z", "message": "Merge branch 'master' into binary_search_for_rounding"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2281d351f748d7b5030deeb04355f32616c444ed", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/2281d351f748d7b5030deeb04355f32616c444ed", "committedDate": "2020-09-24T12:42:36Z", "message": "Nano of day!"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk1NTM1NDc5", "url": "https://github.com/elastic/elasticsearch/pull/61467#pullrequestreview-495535479", "createdAt": "2020-09-24T12:50:51Z", "commit": {"oid": "2281d351f748d7b5030deeb04355f32616c444ed"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4683, "cost": 1, "resetAt": "2021-10-28T18:54:27Z"}}}