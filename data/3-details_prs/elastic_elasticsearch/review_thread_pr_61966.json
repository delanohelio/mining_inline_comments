{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDc5NDQ0ODMy", "number": 61966, "reviewThreads": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNFQxMTo1MjozNlrOEgWnqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNFQxMTo1MjozNlrOEgWnqg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyMzYwNDkwOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/deprecation/src/main/java/org/elasticsearch/xpack/deprecation/logging/DeprecationIndexingComponent.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNFQxMTo1MjozNlrOHNKp0g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNFQxMTo1MjozNlrOHNKp0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzU2ODA4Mg==", "bodyText": "Can you add this to the body of the afterBulk() method to introspect if there are bulk item failures?\nlong items = request.numberOfActions();\nif (logger.isTraceEnabled()) {\n   logger.trace(\"indexed [{}] deprecation documents into [{}]\", items,\n   Arrays.stream(response.getItems())\n      .map(BulkItemResponse::getIndex)\n      .distinct()\n      .collect(Collectors.joining(\",\")));\n}\n\nif (response.hasFailures()) {\n   Map<String, String> failures = Arrays.stream(response.getItems())\n      .filter(BulkItemResponse::isFailed)\n      .collect(Collectors.toMap(BulkItemResponse::getId, BulkItemResponse::getFailureMessage));\n   logger.error(\"failures: [{}]\", failures);\n}", "url": "https://github.com/elastic/elasticsearch/pull/61966#discussion_r483568082", "createdAt": "2020-09-04T11:52:36Z", "author": {"login": "martijnvg"}, "path": "x-pack/plugin/deprecation/src/main/java/org/elasticsearch/xpack/deprecation/logging/DeprecationIndexingComponent.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.deprecation.logging;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.core.LoggerContext;\n+import org.apache.logging.log4j.core.config.Configuration;\n+import org.elasticsearch.action.bulk.BackoffPolicy;\n+import org.elasticsearch.action.bulk.BulkProcessor;\n+import org.elasticsearch.action.bulk.BulkRequest;\n+import org.elasticsearch.action.bulk.BulkResponse;\n+import org.elasticsearch.action.index.IndexRequest;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.ClusterChangedEvent;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ClusterStateListener;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.logging.ESJsonLayout;\n+import org.elasticsearch.common.logging.Loggers;\n+import org.elasticsearch.common.logging.RateLimitingFilter;\n+import org.elasticsearch.common.settings.Setting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.common.util.concurrent.EsExecutors;\n+import org.elasticsearch.xpack.core.ClientHelper;\n+\n+import java.util.function.Consumer;\n+\n+/**\n+ * This component manages the construction and lifecycle of the {@link DeprecationIndexingAppender}.\n+ * It also starts and stops the appender\n+ */\n+public class DeprecationIndexingComponent extends AbstractLifecycleComponent implements ClusterStateListener {\n+    private static final Logger logger = LogManager.getLogger(DeprecationIndexingComponent.class);\n+\n+    public static final Setting<Boolean> WRITE_DEPRECATION_LOGS_TO_INDEX = Setting.boolSetting(\n+        \"cluster.deprecation_indexing.enabled\",\n+        false,\n+        Setting.Property.NodeScope,\n+        Setting.Property.Dynamic\n+    );\n+\n+    private final DeprecationIndexingAppender appender;\n+    private final BulkProcessor processor;\n+    private final RateLimitingFilter filter;\n+\n+    public DeprecationIndexingComponent(Client client, Settings settings) {\n+        this.processor = getBulkProcessor(new OriginSettingClient(client, ClientHelper.DEPRECATION_ORIGIN), settings);\n+        final Consumer<IndexRequest> consumer = this.processor::add;\n+\n+        final LoggerContext context = (LoggerContext) LogManager.getContext(false);\n+        final Configuration configuration = context.getConfiguration();\n+\n+        final ESJsonLayout ecsLayout = ESJsonLayout.newBuilder()\n+            .setType(\"deprecation\")\n+            // This matches the additional fields in the DeprecatedMessage class\n+            .setESMessageFields(\"x-opaque-id,data_stream.type,data_stream.datatype,data_stream.namespace,ecs.version\")\n+            .setConfiguration(configuration)\n+            .build();\n+\n+        this.filter = new RateLimitingFilter();\n+        this.appender = new DeprecationIndexingAppender(\"deprecation_indexing_appender\", filter, ecsLayout, consumer);\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        this.appender.start();\n+        Loggers.addAppender(LogManager.getLogger(\"org.elasticsearch.deprecation\"), this.appender);\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        Loggers.removeAppender(LogManager.getLogger(\"org.elasticsearch.deprecation\"), this.appender);\n+        this.appender.stop();\n+    }\n+\n+    @Override\n+    protected void doClose() {\n+        this.processor.close();\n+    }\n+\n+    /**\n+     * Listens for changes to the cluster state, in order to know whether to toggle indexing\n+     * and to set the cluster UUID and node ID. These can't be set in the constructor because\n+     * the initial cluster state won't be set yet.\n+     *\n+     * @param event the cluster state event to process\n+     */\n+    @Override\n+    public void clusterChanged(ClusterChangedEvent event) {\n+        final ClusterState state = event.state();\n+        final boolean newEnabled = WRITE_DEPRECATION_LOGS_TO_INDEX.get(state.getMetadata().settings());\n+        if (appender.isEnabled() != newEnabled) {\n+            // We've flipped from disabled to enabled. Make sure we start with a clean cache of\n+            // previously-seen keys, otherwise we won't index anything.\n+            if (newEnabled) {\n+                this.filter.reset();\n+            }\n+            appender.setEnabled(newEnabled);\n+        }\n+    }\n+\n+    /**\n+     * Constructs a bulk processor for writing documents\n+     *\n+     * @param client   the client to use\n+     * @param settings the settings to use\n+     * @return an initialised bulk processor\n+     */\n+    private BulkProcessor getBulkProcessor(Client client, Settings settings) {\n+        final OriginSettingClient originSettingClient = new OriginSettingClient(client, ClientHelper.DEPRECATION_ORIGIN);\n+        final BulkProcessor.Listener listener = new DeprecationBulkListener();\n+\n+        // This configuration disables the size count and size thresholds,\n+        // and instead uses a scheduled flush only. This means that calling\n+        // processor.add() will not block the calling thread.\n+        return BulkProcessor.builder(originSettingClient::bulk, listener)\n+            .setBackoffPolicy(BackoffPolicy.exponentialBackoff(TimeValue.timeValueMillis(1000), 3))\n+            .setConcurrentRequests(Math.max(2, EsExecutors.allocatedProcessors(settings)))\n+            .setBulkActions(-1)\n+            .setBulkSize(new ByteSizeValue(-1, ByteSizeUnit.BYTES))\n+            .setFlushInterval(TimeValue.timeValueSeconds(5))\n+            .build();\n+    }\n+\n+    private static class DeprecationBulkListener implements BulkProcessor.Listener {\n+        @Override\n+        public void beforeBulk(long executionId, BulkRequest request) {}\n+\n+        @Override\n+        public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "441aef1bff47d55d39290e9e89a2f0e781859044"}, "originalPosition": 139}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1735, "cost": 1, "resetAt": "2021-11-12T13:16:51Z"}}}