{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTAyODExMTQ1", "number": 63643, "reviewThreads": {"totalCount": 25, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QyMDozOToyM1rOEtN0Yg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwODozMTo0NlrOE2p3hA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1ODQ3Nzc4OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QyMDozOToyM1rOHg4p6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNzozMzo1OFrOHmso6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDI0NDcxNA==", "bodyText": "This method and everything in it is kind of shameful but it gives a 2x speed improvement.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r504244714", "createdAt": "2020-10-13T20:39:23Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java", "diffHunk": "@@ -185,16 +235,234 @@ public InternalAggregation buildEmptyAggregation() {\n             buckets.add(bucket);\n         }\n \n-        if (showOtherBucket) {\n+        if (otherBucketKey != null) {\n             InternalFilters.InternalBucket bucket = new InternalFilters.InternalBucket(otherBucketKey, 0, subAggs, keyed);\n             buckets.add(bucket);\n         }\n \n         return new InternalFilters(name, buckets, keyed, metadata());\n     }\n \n-    final long bucketOrd(long owningBucketOrdinal, int filterOrd) {\n-        return owningBucketOrdinal * totalNumKeys + filterOrd;\n+    public abstract boolean collectsInFilterOrder();\n+\n+    private static class FilterOrderAggregator extends FiltersAggregator {\n+        private final Query[] filters;\n+        private Weight[] filterWeights;\n+\n+        public FilterOrderAggregator(\n+            String name,\n+            String[] keys,\n+            Query[] filters,\n+            boolean keyed,\n+            SearchContext context,\n+            Aggregator parent,\n+            CardinalityUpperBound cardinality,\n+            Map<String, Object> metadata\n+        ) throws IOException {\n+            super(name, AggregatorFactories.EMPTY, keys, keyed, null, context, parent, cardinality, metadata);\n+            this.filters = filters;\n+        }\n+\n+        @Override\n+        protected LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n+            if (filterWeights == null) {\n+                filterWeights = buildWeights(context.query(), filters);\n+            }\n+            Bits live = ctx.reader().getLiveDocs();\n+            for (int filterOrd = 0; filterOrd < filters.length; filterOrd++) {\n+                Scorer scorer = filterWeights[filterOrd].scorer(ctx);\n+                if (scorer == null) {\n+                    // the filter doesn't match any docs\n+                    continue;\n+                }\n+                DocIdSetIterator itr = scorer.iterator();\n+                if (live == null) {\n+                    while (itr.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n+                        collectBucket(sub, itr.docID(), filterOrd);\n+                    }\n+                } else {\n+                    while (itr.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n+                        if (live.get(itr.docID())) {\n+                            collectBucket(sub, itr.docID(), filterOrd);\n+                        }\n+                    }\n+                }\n+            }\n+            throw new CollectionTerminatedException();\n+        }\n+\n+        @Override\n+        public boolean collectsInFilterOrder() {\n+            return true;\n+        }\n     }\n \n+    private static class StandardOrderAggregator extends FiltersAggregator {\n+        private final Query[] filters;\n+        private Weight[] filterWeights;\n+\n+        private final int totalNumKeys;\n+\n+        public StandardOrderAggregator(\n+            String name,\n+            AggregatorFactories factories,\n+            String[] keys,\n+            Query[] filters,\n+            boolean keyed,\n+            String otherBucketKey,\n+            SearchContext context,\n+            Aggregator parent,\n+            CardinalityUpperBound cardinality,\n+            Map<String, Object> metadata\n+        ) throws IOException {\n+            super(name, factories, keys, keyed, otherBucketKey, context, parent, cardinality, metadata);\n+            this.filters = filters;\n+            if (otherBucketKey == null) {\n+                this.totalNumKeys = keys.length;\n+            } else {\n+                this.totalNumKeys = keys.length + 1;\n+            }\n+        }\n+\n+        @Override\n+        protected LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n+            if (filterWeights == null) {\n+                filterWeights = buildWeights(new MatchAllDocsQuery(), filters);\n+            }\n+            final Bits[] bits = new Bits[filters.length];\n+            for (int i = 0; i < filters.length; ++i) {\n+                bits[i] = Lucene.asSequentialAccessBits(ctx.reader().maxDoc(), filterWeights[i].scorerSupplier(ctx));\n+            }\n+            return new LeafBucketCollectorBase(sub, null) {\n+                @Override\n+                public void collect(int doc, long bucket) throws IOException {\n+                    boolean matched = false;\n+                    for (int i = 0; i < bits.length; i++) {\n+                        if (bits[i].get(doc)) {\n+                            collectBucket(sub, doc, bucketOrd(bucket, i));\n+                            matched = true;\n+                        }\n+                    }\n+                    if (otherBucketKey != null && false == matched) {\n+                        collectBucket(sub, doc, bucketOrd(bucket, bits.length));\n+                    }\n+                }\n+            };\n+        }\n+\n+        final long bucketOrd(long owningBucketOrdinal, int filterOrd) {\n+            return owningBucketOrdinal * totalNumKeys + filterOrd;\n+        }\n+\n+        @Override\n+        public boolean collectsInFilterOrder() {\n+            return false;\n+        }\n+    }\n+\n+    protected Weight[] buildWeights(Query topLevelQuery, Query filters[]) throws IOException{\n+        Weight[] weights = new Weight[filters.length];\n+        for (int i = 0; i < filters.length; ++i) {\n+            Query filter = filterMatchingBoth(topLevelQuery, filters[i]);\n+            weights[i] = context.searcher().createWeight(context.searcher().rewrite(filter), ScoreMode.COMPLETE_NO_SCORES, 1);\n+        }\n+        return weights;\n+    }\n+\n+    private Query filterMatchingBoth(Query lhs, Query rhs) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9754fb0303803ae7fec06351ca3cbdebd0ee4391"}, "originalPosition": 308}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcwMzA0NQ==", "bodyText": "so, this merges two filter queries so they can be performed in one pass?  I know it's a private method, but I still think a bit of documentation for what it does and why that's important would be good.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505703045", "createdAt": "2020-10-15T17:06:33Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java", "diffHunk": "@@ -185,16 +235,234 @@ public InternalAggregation buildEmptyAggregation() {\n             buckets.add(bucket);\n         }\n \n-        if (showOtherBucket) {\n+        if (otherBucketKey != null) {\n             InternalFilters.InternalBucket bucket = new InternalFilters.InternalBucket(otherBucketKey, 0, subAggs, keyed);\n             buckets.add(bucket);\n         }\n \n         return new InternalFilters(name, buckets, keyed, metadata());\n     }\n \n-    final long bucketOrd(long owningBucketOrdinal, int filterOrd) {\n-        return owningBucketOrdinal * totalNumKeys + filterOrd;\n+    public abstract boolean collectsInFilterOrder();\n+\n+    private static class FilterOrderAggregator extends FiltersAggregator {\n+        private final Query[] filters;\n+        private Weight[] filterWeights;\n+\n+        public FilterOrderAggregator(\n+            String name,\n+            String[] keys,\n+            Query[] filters,\n+            boolean keyed,\n+            SearchContext context,\n+            Aggregator parent,\n+            CardinalityUpperBound cardinality,\n+            Map<String, Object> metadata\n+        ) throws IOException {\n+            super(name, AggregatorFactories.EMPTY, keys, keyed, null, context, parent, cardinality, metadata);\n+            this.filters = filters;\n+        }\n+\n+        @Override\n+        protected LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n+            if (filterWeights == null) {\n+                filterWeights = buildWeights(context.query(), filters);\n+            }\n+            Bits live = ctx.reader().getLiveDocs();\n+            for (int filterOrd = 0; filterOrd < filters.length; filterOrd++) {\n+                Scorer scorer = filterWeights[filterOrd].scorer(ctx);\n+                if (scorer == null) {\n+                    // the filter doesn't match any docs\n+                    continue;\n+                }\n+                DocIdSetIterator itr = scorer.iterator();\n+                if (live == null) {\n+                    while (itr.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n+                        collectBucket(sub, itr.docID(), filterOrd);\n+                    }\n+                } else {\n+                    while (itr.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n+                        if (live.get(itr.docID())) {\n+                            collectBucket(sub, itr.docID(), filterOrd);\n+                        }\n+                    }\n+                }\n+            }\n+            throw new CollectionTerminatedException();\n+        }\n+\n+        @Override\n+        public boolean collectsInFilterOrder() {\n+            return true;\n+        }\n     }\n \n+    private static class StandardOrderAggregator extends FiltersAggregator {\n+        private final Query[] filters;\n+        private Weight[] filterWeights;\n+\n+        private final int totalNumKeys;\n+\n+        public StandardOrderAggregator(\n+            String name,\n+            AggregatorFactories factories,\n+            String[] keys,\n+            Query[] filters,\n+            boolean keyed,\n+            String otherBucketKey,\n+            SearchContext context,\n+            Aggregator parent,\n+            CardinalityUpperBound cardinality,\n+            Map<String, Object> metadata\n+        ) throws IOException {\n+            super(name, factories, keys, keyed, otherBucketKey, context, parent, cardinality, metadata);\n+            this.filters = filters;\n+            if (otherBucketKey == null) {\n+                this.totalNumKeys = keys.length;\n+            } else {\n+                this.totalNumKeys = keys.length + 1;\n+            }\n+        }\n+\n+        @Override\n+        protected LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n+            if (filterWeights == null) {\n+                filterWeights = buildWeights(new MatchAllDocsQuery(), filters);\n+            }\n+            final Bits[] bits = new Bits[filters.length];\n+            for (int i = 0; i < filters.length; ++i) {\n+                bits[i] = Lucene.asSequentialAccessBits(ctx.reader().maxDoc(), filterWeights[i].scorerSupplier(ctx));\n+            }\n+            return new LeafBucketCollectorBase(sub, null) {\n+                @Override\n+                public void collect(int doc, long bucket) throws IOException {\n+                    boolean matched = false;\n+                    for (int i = 0; i < bits.length; i++) {\n+                        if (bits[i].get(doc)) {\n+                            collectBucket(sub, doc, bucketOrd(bucket, i));\n+                            matched = true;\n+                        }\n+                    }\n+                    if (otherBucketKey != null && false == matched) {\n+                        collectBucket(sub, doc, bucketOrd(bucket, bits.length));\n+                    }\n+                }\n+            };\n+        }\n+\n+        final long bucketOrd(long owningBucketOrdinal, int filterOrd) {\n+            return owningBucketOrdinal * totalNumKeys + filterOrd;\n+        }\n+\n+        @Override\n+        public boolean collectsInFilterOrder() {\n+            return false;\n+        }\n+    }\n+\n+    protected Weight[] buildWeights(Query topLevelQuery, Query filters[]) throws IOException{\n+        Weight[] weights = new Weight[filters.length];\n+        for (int i = 0; i < filters.length; ++i) {\n+            Query filter = filterMatchingBoth(topLevelQuery, filters[i]);\n+            weights[i] = context.searcher().createWeight(context.searcher().rewrite(filter), ScoreMode.COMPLETE_NO_SCORES, 1);\n+        }\n+        return weights;\n+    }\n+\n+    private Query filterMatchingBoth(Query lhs, Query rhs) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDI0NDcxNA=="}, "originalCommit": {"oid": "9754fb0303803ae7fec06351ca3cbdebd0ee4391"}, "originalPosition": 308}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDMzOTMwNA==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510339304", "createdAt": "2020-10-22T17:33:58Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java", "diffHunk": "@@ -185,16 +235,234 @@ public InternalAggregation buildEmptyAggregation() {\n             buckets.add(bucket);\n         }\n \n-        if (showOtherBucket) {\n+        if (otherBucketKey != null) {\n             InternalFilters.InternalBucket bucket = new InternalFilters.InternalBucket(otherBucketKey, 0, subAggs, keyed);\n             buckets.add(bucket);\n         }\n \n         return new InternalFilters(name, buckets, keyed, metadata());\n     }\n \n-    final long bucketOrd(long owningBucketOrdinal, int filterOrd) {\n-        return owningBucketOrdinal * totalNumKeys + filterOrd;\n+    public abstract boolean collectsInFilterOrder();\n+\n+    private static class FilterOrderAggregator extends FiltersAggregator {\n+        private final Query[] filters;\n+        private Weight[] filterWeights;\n+\n+        public FilterOrderAggregator(\n+            String name,\n+            String[] keys,\n+            Query[] filters,\n+            boolean keyed,\n+            SearchContext context,\n+            Aggregator parent,\n+            CardinalityUpperBound cardinality,\n+            Map<String, Object> metadata\n+        ) throws IOException {\n+            super(name, AggregatorFactories.EMPTY, keys, keyed, null, context, parent, cardinality, metadata);\n+            this.filters = filters;\n+        }\n+\n+        @Override\n+        protected LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n+            if (filterWeights == null) {\n+                filterWeights = buildWeights(context.query(), filters);\n+            }\n+            Bits live = ctx.reader().getLiveDocs();\n+            for (int filterOrd = 0; filterOrd < filters.length; filterOrd++) {\n+                Scorer scorer = filterWeights[filterOrd].scorer(ctx);\n+                if (scorer == null) {\n+                    // the filter doesn't match any docs\n+                    continue;\n+                }\n+                DocIdSetIterator itr = scorer.iterator();\n+                if (live == null) {\n+                    while (itr.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n+                        collectBucket(sub, itr.docID(), filterOrd);\n+                    }\n+                } else {\n+                    while (itr.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n+                        if (live.get(itr.docID())) {\n+                            collectBucket(sub, itr.docID(), filterOrd);\n+                        }\n+                    }\n+                }\n+            }\n+            throw new CollectionTerminatedException();\n+        }\n+\n+        @Override\n+        public boolean collectsInFilterOrder() {\n+            return true;\n+        }\n     }\n \n+    private static class StandardOrderAggregator extends FiltersAggregator {\n+        private final Query[] filters;\n+        private Weight[] filterWeights;\n+\n+        private final int totalNumKeys;\n+\n+        public StandardOrderAggregator(\n+            String name,\n+            AggregatorFactories factories,\n+            String[] keys,\n+            Query[] filters,\n+            boolean keyed,\n+            String otherBucketKey,\n+            SearchContext context,\n+            Aggregator parent,\n+            CardinalityUpperBound cardinality,\n+            Map<String, Object> metadata\n+        ) throws IOException {\n+            super(name, factories, keys, keyed, otherBucketKey, context, parent, cardinality, metadata);\n+            this.filters = filters;\n+            if (otherBucketKey == null) {\n+                this.totalNumKeys = keys.length;\n+            } else {\n+                this.totalNumKeys = keys.length + 1;\n+            }\n+        }\n+\n+        @Override\n+        protected LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n+            if (filterWeights == null) {\n+                filterWeights = buildWeights(new MatchAllDocsQuery(), filters);\n+            }\n+            final Bits[] bits = new Bits[filters.length];\n+            for (int i = 0; i < filters.length; ++i) {\n+                bits[i] = Lucene.asSequentialAccessBits(ctx.reader().maxDoc(), filterWeights[i].scorerSupplier(ctx));\n+            }\n+            return new LeafBucketCollectorBase(sub, null) {\n+                @Override\n+                public void collect(int doc, long bucket) throws IOException {\n+                    boolean matched = false;\n+                    for (int i = 0; i < bits.length; i++) {\n+                        if (bits[i].get(doc)) {\n+                            collectBucket(sub, doc, bucketOrd(bucket, i));\n+                            matched = true;\n+                        }\n+                    }\n+                    if (otherBucketKey != null && false == matched) {\n+                        collectBucket(sub, doc, bucketOrd(bucket, bits.length));\n+                    }\n+                }\n+            };\n+        }\n+\n+        final long bucketOrd(long owningBucketOrdinal, int filterOrd) {\n+            return owningBucketOrdinal * totalNumKeys + filterOrd;\n+        }\n+\n+        @Override\n+        public boolean collectsInFilterOrder() {\n+            return false;\n+        }\n+    }\n+\n+    protected Weight[] buildWeights(Query topLevelQuery, Query filters[]) throws IOException{\n+        Weight[] weights = new Weight[filters.length];\n+        for (int i = 0; i < filters.length; ++i) {\n+            Query filter = filterMatchingBoth(topLevelQuery, filters[i]);\n+            weights[i] = context.searcher().createWeight(context.searcher().rewrite(filter), ScoreMode.COMPLETE_NO_SCORES, 1);\n+        }\n+        return weights;\n+    }\n+\n+    private Query filterMatchingBoth(Query lhs, Query rhs) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDI0NDcxNA=="}, "originalCommit": {"oid": "9754fb0303803ae7fec06351ca3cbdebd0ee4391"}, "originalPosition": 308}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2MTMyODY0OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/common/Rounding.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxMzozNjozMlrOHhTXjA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNzoxODo1MFrOHmsEiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY4MjM4MA==", "bodyText": "On my first read, I thought we might round down to null.  How about:\nIf this rounding mechanism precalculates rounding points, this array stores dates such that each\ndate between each entry will be rounded down to that entry.  If the rounding mechanism does not\nsupport the optimization, this array is {@code null}.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r504682380", "createdAt": "2020-10-14T13:36:32Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/common/Rounding.java", "diffHunk": "@@ -291,6 +291,12 @@ public void writeTo(StreamOutput out) throws IOException {\n          * next rounded value in specified units if possible.\n          */\n         double roundingSize(long utcMillis, DateTimeUnit timeUnit);\n+        /**\n+         * An array of dates such that each date between each entry is will\n+         * be rounded down to that entry or {@code null} if this rounding\n+         * mechanism doesn't or can't precalculate these points.\n+         */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "402132781b8530f616ad6474c0b7652a588404c7"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDMyOTk5Mw==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510329993", "createdAt": "2020-10-22T17:18:50Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/common/Rounding.java", "diffHunk": "@@ -291,6 +291,12 @@ public void writeTo(StreamOutput out) throws IOException {\n          * next rounded value in specified units if possible.\n          */\n         double roundingSize(long utcMillis, DateTimeUnit timeUnit);\n+        /**\n+         * An array of dates such that each date between each entry is will\n+         * be rounded down to that entry or {@code null} if this rounding\n+         * mechanism doesn't or can't precalculate these points.\n+         */", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY4MjM4MA=="}, "originalCommit": {"oid": "402132781b8530f616ad6474c0b7652a588404c7"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2MTczMjgzOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/GeoDistanceRangeAggregatorFactory.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxNDo1Njo1MFrOHhXUIg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNzoxMzozN1rOHmr4OA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc0NzA0Mg==", "bodyText": "I'm not entirely convinced that the way we're shoehorning GeoDistance into the range framework is a good idea.  The fact that we need this supports that viewpoint, I think.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r504747042", "createdAt": "2020-10-14T14:56:50Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/GeoDistanceRangeAggregatorFactory.java", "diffHunk": "@@ -66,7 +66,7 @@ public static void registerAggregators(ValuesSourceRegistry.Builder builder) {\n                 cardinality,\n                 metadata) -> {\n                 DistanceSource distanceSource = new DistanceSource((ValuesSource.GeoPoint) valuesSource, distanceType, origin, units);\n-                return new RangeAggregator(\n+                return RangeAggregator.buildWithoutAttemptedToAdaptToFilters(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "402132781b8530f616ad6474c0b7652a588404c7"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDMyNjg0MA==", "bodyText": "I wonder if it'd make sense to shoe horn it via adapting the data source on the way in and the result on the way out. Maybe. I'm not sure. A thing for later, I think.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510326840", "createdAt": "2020-10-22T17:13:37Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/GeoDistanceRangeAggregatorFactory.java", "diffHunk": "@@ -66,7 +66,7 @@ public static void registerAggregators(ValuesSourceRegistry.Builder builder) {\n                 cardinality,\n                 metadata) -> {\n                 DistanceSource distanceSource = new DistanceSource((ValuesSource.GeoPoint) valuesSource, distanceType, origin, units);\n-                return new RangeAggregator(\n+                return RangeAggregator.buildWithoutAttemptedToAdaptToFilters(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc0NzA0Mg=="}, "originalCommit": {"oid": "402132781b8530f616ad6474c0b7652a588404c7"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2NzM0NTkzOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactories.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQxNTo1MToxN1rOHiOndQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNzoyMToxOFrOHmsKcA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTY1MzEwOQ==", "bodyText": "This could use some javadoc.  It's not clear from the context that it's intended to be used with the AddaptingAggregator (or, really, wrapping aggs in general, although I don't think we have any others)", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505653109", "createdAt": "2020-10-15T15:51:17Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactories.java", "diffHunk": "@@ -227,6 +227,18 @@ public int countAggregators() {\n         return factories.length;\n     }\n \n+    public AggregatorFactories fixParent(Aggregator fixedParent) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDMzMTUwNA==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510331504", "createdAt": "2020-10-22T17:21:18Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactories.java", "diffHunk": "@@ -227,6 +227,18 @@ public int countAggregators() {\n         return factories.length;\n     }\n \n+    public AggregatorFactories fixParent(Aggregator fixedParent) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTY1MzEwOQ=="}, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2NzM4NjI2OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQxNTo1OTo1N1rOHiPAiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNzozNDo1MlrOHmsrCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTY1OTUyOQ==", "bodyText": "Bit of javadoc would be nice here.  It looks like the intention is for this to only have the implementations already built in this file, would be good to document that.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505659529", "createdAt": "2020-10-15T15:59:57Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java", "diffHunk": "@@ -45,9 +56,11 @@\n import java.util.List;\n import java.util.Map;\n import java.util.Objects;\n-import java.util.function.Supplier;\n+import java.util.function.BiConsumer;\n \n-public class FiltersAggregator extends BucketsAggregator {\n+import static java.util.Arrays.compareUnsigned;\n+\n+public abstract class FiltersAggregator extends BucketsAggregator {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDMzOTg0OA==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510339848", "createdAt": "2020-10-22T17:34:52Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java", "diffHunk": "@@ -45,9 +56,11 @@\n import java.util.List;\n import java.util.Map;\n import java.util.Objects;\n-import java.util.function.Supplier;\n+import java.util.function.BiConsumer;\n \n-public class FiltersAggregator extends BucketsAggregator {\n+import static java.util.Arrays.compareUnsigned;\n+\n+public abstract class FiltersAggregator extends BucketsAggregator {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTY1OTUyOQ=="}, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2NzYxMzkxOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQxNjo1NToxNVrOHiRO0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQxNjo1NToxNVrOHiRO0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTY5NTk1Mg==", "bodyText": "Javadoc here too please.  This operates differently from what this method usually does (e.g. doesn't return a LeafBucketCollector), and it would be good to make a note of why that's correct.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505695952", "createdAt": "2020-10-15T16:55:15Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java", "diffHunk": "@@ -185,16 +243,254 @@ public InternalAggregation buildEmptyAggregation() {\n             buckets.add(bucket);\n         }\n \n-        if (showOtherBucket) {\n+        if (otherBucketKey != null) {\n             InternalFilters.InternalBucket bucket = new InternalFilters.InternalBucket(otherBucketKey, 0, subAggs, keyed);\n             buckets.add(bucket);\n         }\n \n         return new InternalFilters(name, buckets, keyed, metadata());\n     }\n \n-    final long bucketOrd(long owningBucketOrdinal, int filterOrd) {\n-        return owningBucketOrdinal * totalNumKeys + filterOrd;\n+    public abstract boolean collectsInFilterOrder();\n+\n+    /**\n+     * Collects results by running each filter against the searcher and doesn't\n+     * build any {@link LeafBucketCollector}s which is generally faster than\n+     * {@link Compatible} but doesn't support when there is a parent aggregator\n+     * or any child aggregators.\n+     */\n+    private static class FilterByFilter extends FiltersAggregator {\n+        private final Query[] filters;\n+        private Weight[] filterWeights;\n+        private int segmentsWithDeletedDocs;\n+\n+        FilterByFilter(\n+            String name,\n+            String[] keys,\n+            Query[] filters,\n+            boolean keyed,\n+            SearchContext context,\n+            Aggregator parent,\n+            CardinalityUpperBound cardinality,\n+            Map<String, Object> metadata\n+        ) throws IOException {\n+            super(name, AggregatorFactories.EMPTY, keys, keyed, null, context, parent, cardinality, metadata);\n+            this.filters = filters;\n+        }\n+\n+        @Override\n+        protected LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 218}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2NzYzMzg4OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQxNzowMDowMVrOHiRa2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNzo0MDoxOFrOHms4dQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTY5OTAzNA==", "bodyText": "Why do we throw here?", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505699034", "createdAt": "2020-10-15T17:00:01Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java", "diffHunk": "@@ -185,16 +243,254 @@ public InternalAggregation buildEmptyAggregation() {\n             buckets.add(bucket);\n         }\n \n-        if (showOtherBucket) {\n+        if (otherBucketKey != null) {\n             InternalFilters.InternalBucket bucket = new InternalFilters.InternalBucket(otherBucketKey, 0, subAggs, keyed);\n             buckets.add(bucket);\n         }\n \n         return new InternalFilters(name, buckets, keyed, metadata());\n     }\n \n-    final long bucketOrd(long owningBucketOrdinal, int filterOrd) {\n-        return owningBucketOrdinal * totalNumKeys + filterOrd;\n+    public abstract boolean collectsInFilterOrder();\n+\n+    /**\n+     * Collects results by running each filter against the searcher and doesn't\n+     * build any {@link LeafBucketCollector}s which is generally faster than\n+     * {@link Compatible} but doesn't support when there is a parent aggregator\n+     * or any child aggregators.\n+     */\n+    private static class FilterByFilter extends FiltersAggregator {\n+        private final Query[] filters;\n+        private Weight[] filterWeights;\n+        private int segmentsWithDeletedDocs;\n+\n+        FilterByFilter(\n+            String name,\n+            String[] keys,\n+            Query[] filters,\n+            boolean keyed,\n+            SearchContext context,\n+            Aggregator parent,\n+            CardinalityUpperBound cardinality,\n+            Map<String, Object> metadata\n+        ) throws IOException {\n+            super(name, AggregatorFactories.EMPTY, keys, keyed, null, context, parent, cardinality, metadata);\n+            this.filters = filters;\n+        }\n+\n+        @Override\n+        protected LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n+            if (filterWeights == null) {\n+                filterWeights = buildWeights(context.query(), filters);\n+            }\n+            Bits live = ctx.reader().getLiveDocs();\n+            for (int filterOrd = 0; filterOrd < filters.length; filterOrd++) {\n+                Scorer scorer = filterWeights[filterOrd].scorer(ctx);\n+                if (scorer == null) {\n+                    // the filter doesn't match any docs\n+                    continue;\n+                }\n+                DocIdSetIterator itr = scorer.iterator();\n+                if (live == null) {\n+                    while (itr.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n+                        collectBucket(sub, itr.docID(), filterOrd);\n+                    }\n+                } else {\n+                    segmentsWithDeletedDocs++;\n+                    while (itr.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n+                        if (live.get(itr.docID())) {\n+                            collectBucket(sub, itr.docID(), filterOrd);\n+                        }\n+                    }\n+                }\n+            }\n+            throw new CollectionTerminatedException();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 243}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDM0MzI4NQ==", "bodyText": "Throwing this exception is how we communicate to the collection mechanism that we don't need the segment.. I'll add docs.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510343285", "createdAt": "2020-10-22T17:40:18Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java", "diffHunk": "@@ -185,16 +243,254 @@ public InternalAggregation buildEmptyAggregation() {\n             buckets.add(bucket);\n         }\n \n-        if (showOtherBucket) {\n+        if (otherBucketKey != null) {\n             InternalFilters.InternalBucket bucket = new InternalFilters.InternalBucket(otherBucketKey, 0, subAggs, keyed);\n             buckets.add(bucket);\n         }\n \n         return new InternalFilters(name, buckets, keyed, metadata());\n     }\n \n-    final long bucketOrd(long owningBucketOrdinal, int filterOrd) {\n-        return owningBucketOrdinal * totalNumKeys + filterOrd;\n+    public abstract boolean collectsInFilterOrder();\n+\n+    /**\n+     * Collects results by running each filter against the searcher and doesn't\n+     * build any {@link LeafBucketCollector}s which is generally faster than\n+     * {@link Compatible} but doesn't support when there is a parent aggregator\n+     * or any child aggregators.\n+     */\n+    private static class FilterByFilter extends FiltersAggregator {\n+        private final Query[] filters;\n+        private Weight[] filterWeights;\n+        private int segmentsWithDeletedDocs;\n+\n+        FilterByFilter(\n+            String name,\n+            String[] keys,\n+            Query[] filters,\n+            boolean keyed,\n+            SearchContext context,\n+            Aggregator parent,\n+            CardinalityUpperBound cardinality,\n+            Map<String, Object> metadata\n+        ) throws IOException {\n+            super(name, AggregatorFactories.EMPTY, keys, keyed, null, context, parent, cardinality, metadata);\n+            this.filters = filters;\n+        }\n+\n+        @Override\n+        protected LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n+            if (filterWeights == null) {\n+                filterWeights = buildWeights(context.query(), filters);\n+            }\n+            Bits live = ctx.reader().getLiveDocs();\n+            for (int filterOrd = 0; filterOrd < filters.length; filterOrd++) {\n+                Scorer scorer = filterWeights[filterOrd].scorer(ctx);\n+                if (scorer == null) {\n+                    // the filter doesn't match any docs\n+                    continue;\n+                }\n+                DocIdSetIterator itr = scorer.iterator();\n+                if (live == null) {\n+                    while (itr.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n+                        collectBucket(sub, itr.docID(), filterOrd);\n+                    }\n+                } else {\n+                    segmentsWithDeletedDocs++;\n+                    while (itr.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n+                        if (live.get(itr.docID())) {\n+                            collectBucket(sub, itr.docID(), filterOrd);\n+                        }\n+                    }\n+                }\n+            }\n+            throw new CollectionTerminatedException();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTY5OTAzNA=="}, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 243}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2NzY3NzUyOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQxNzoxMDo0N1rOHiR2Mw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNzoyMjowM1rOHmsMPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcwNjAzNQ==", "bodyText": "I think you lost a verb: \"because it doesn't need to the rounding points\"?  Compute, maybe?", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505706035", "createdAt": "2020-10-15T17:10:47Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java", "diffHunk": "@@ -51,6 +62,143 @@\n  * @see Rounding\n  */\n class DateHistogramAggregator extends BucketsAggregator implements SizedBucketAggregator {\n+    /**\n+     * Build an {@link Aggregator} for a {@code date_histogram} aggregation.\n+     * If we can determine the bucket boundaries from\n+     * {@link Rounding.Prepared#fixedRoundingPoints()} we use\n+     * {@link RangeAggregator} to do the actual collecting, otherwise we use\n+     * an specialized {@link DateHistogramAggregator Aggregator} specifically\n+     * for the {@code date_histogram}s. We prefer to delegate to the\n+     * {@linkplain RangeAggregator} because it can sometimes be further\n+     * optimized into a {@link FiltersAggregator}. Even when it can't be\n+     * optimized, it is going to be marginally faster and consume less memory\n+     * than the {@linkplain DateHistogramAggregator} because it doesn't need\n+     * to the rounding points and because it can pass precise cardinality", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDMzMTk2NA==", "bodyText": "\"to round points\" is what I meant.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510331964", "createdAt": "2020-10-22T17:22:03Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java", "diffHunk": "@@ -51,6 +62,143 @@\n  * @see Rounding\n  */\n class DateHistogramAggregator extends BucketsAggregator implements SizedBucketAggregator {\n+    /**\n+     * Build an {@link Aggregator} for a {@code date_histogram} aggregation.\n+     * If we can determine the bucket boundaries from\n+     * {@link Rounding.Prepared#fixedRoundingPoints()} we use\n+     * {@link RangeAggregator} to do the actual collecting, otherwise we use\n+     * an specialized {@link DateHistogramAggregator Aggregator} specifically\n+     * for the {@code date_histogram}s. We prefer to delegate to the\n+     * {@linkplain RangeAggregator} because it can sometimes be further\n+     * optimized into a {@link FiltersAggregator}. Even when it can't be\n+     * optimized, it is going to be marginally faster and consume less memory\n+     * than the {@linkplain DateHistogramAggregator} because it doesn't need\n+     * to the rounding points and because it can pass precise cardinality", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcwNjAzNQ=="}, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2NzY5NjgwOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQxNzoxNjowM1rOHiSCZw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNDo1ODozOVrOHmmHXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcwOTE1OQ==", "bodyText": "This is probably redundant, FYI.  In theory, if hasValues() == false, then we went into the createUnmapped path from the builder.  Harmless to check though.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505709159", "createdAt": "2020-10-15T17:16:03Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java", "diffHunk": "@@ -51,6 +62,143 @@\n  * @see Rounding\n  */\n class DateHistogramAggregator extends BucketsAggregator implements SizedBucketAggregator {\n+    /**\n+     * Build an {@link Aggregator} for a {@code date_histogram} aggregation.\n+     * If we can determine the bucket boundaries from\n+     * {@link Rounding.Prepared#fixedRoundingPoints()} we use\n+     * {@link RangeAggregator} to do the actual collecting, otherwise we use\n+     * an specialized {@link DateHistogramAggregator Aggregator} specifically\n+     * for the {@code date_histogram}s. We prefer to delegate to the\n+     * {@linkplain RangeAggregator} because it can sometimes be further\n+     * optimized into a {@link FiltersAggregator}. Even when it can't be\n+     * optimized, it is going to be marginally faster and consume less memory\n+     * than the {@linkplain DateHistogramAggregator} because it doesn't need\n+     * to the rounding points and because it can pass precise cardinality\n+     * estimates to its child aggregations.\n+     */\n+    public static Aggregator build(\n+        String name,\n+        AggregatorFactories factories,\n+        Rounding rounding,\n+        BucketOrder order,\n+        boolean keyed,\n+        long minDocCount,\n+        @Nullable LongBounds extendedBounds,\n+        @Nullable LongBounds hardBounds,\n+        ValuesSourceConfig valuesSourceConfig,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        Rounding.Prepared preparedRounding = valuesSourceConfig.roundingPreparer().apply(rounding);\n+        Aggregator asRange = adaptIntoRangeOrNull(\n+            name,\n+            factories,\n+            rounding,\n+            preparedRounding,\n+            order,\n+            keyed,\n+            minDocCount,\n+            extendedBounds,\n+            hardBounds,\n+            valuesSourceConfig,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+        if (asRange != null) {\n+            return asRange;\n+        }\n+        return new DateHistogramAggregator(\n+            name,\n+            factories,\n+            rounding,\n+            preparedRounding,\n+            order,\n+            keyed,\n+            minDocCount,\n+            extendedBounds,\n+            hardBounds,\n+            valuesSourceConfig,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+    }\n+\n+    private static FromDateRange adaptIntoRangeOrNull(\n+        String name,\n+        AggregatorFactories factories,\n+        Rounding rounding,\n+        Rounding.Prepared preparedRounding,\n+        BucketOrder order,\n+        boolean keyed,\n+        long minDocCount,\n+        @Nullable LongBounds extendedBounds,\n+        @Nullable LongBounds hardBounds,\n+        ValuesSourceConfig valuesSourceConfig,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        if (hardBounds != null) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.hasValues() == false) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDIzMjQxNA==", "bodyText": "I can drop it.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510232414", "createdAt": "2020-10-22T14:58:39Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java", "diffHunk": "@@ -51,6 +62,143 @@\n  * @see Rounding\n  */\n class DateHistogramAggregator extends BucketsAggregator implements SizedBucketAggregator {\n+    /**\n+     * Build an {@link Aggregator} for a {@code date_histogram} aggregation.\n+     * If we can determine the bucket boundaries from\n+     * {@link Rounding.Prepared#fixedRoundingPoints()} we use\n+     * {@link RangeAggregator} to do the actual collecting, otherwise we use\n+     * an specialized {@link DateHistogramAggregator Aggregator} specifically\n+     * for the {@code date_histogram}s. We prefer to delegate to the\n+     * {@linkplain RangeAggregator} because it can sometimes be further\n+     * optimized into a {@link FiltersAggregator}. Even when it can't be\n+     * optimized, it is going to be marginally faster and consume less memory\n+     * than the {@linkplain DateHistogramAggregator} because it doesn't need\n+     * to the rounding points and because it can pass precise cardinality\n+     * estimates to its child aggregations.\n+     */\n+    public static Aggregator build(\n+        String name,\n+        AggregatorFactories factories,\n+        Rounding rounding,\n+        BucketOrder order,\n+        boolean keyed,\n+        long minDocCount,\n+        @Nullable LongBounds extendedBounds,\n+        @Nullable LongBounds hardBounds,\n+        ValuesSourceConfig valuesSourceConfig,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        Rounding.Prepared preparedRounding = valuesSourceConfig.roundingPreparer().apply(rounding);\n+        Aggregator asRange = adaptIntoRangeOrNull(\n+            name,\n+            factories,\n+            rounding,\n+            preparedRounding,\n+            order,\n+            keyed,\n+            minDocCount,\n+            extendedBounds,\n+            hardBounds,\n+            valuesSourceConfig,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+        if (asRange != null) {\n+            return asRange;\n+        }\n+        return new DateHistogramAggregator(\n+            name,\n+            factories,\n+            rounding,\n+            preparedRounding,\n+            order,\n+            keyed,\n+            minDocCount,\n+            extendedBounds,\n+            hardBounds,\n+            valuesSourceConfig,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+    }\n+\n+    private static FromDateRange adaptIntoRangeOrNull(\n+        String name,\n+        AggregatorFactories factories,\n+        Rounding rounding,\n+        Rounding.Prepared preparedRounding,\n+        BucketOrder order,\n+        boolean keyed,\n+        long minDocCount,\n+        @Nullable LongBounds extendedBounds,\n+        @Nullable LongBounds hardBounds,\n+        ValuesSourceConfig valuesSourceConfig,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        if (hardBounds != null) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.hasValues() == false) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcwOTE1OQ=="}, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 127}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2NzcwMDA5OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQxNzoxNjo1OVrOHiSEdQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNDo1ODowOVrOHmmF2A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcwOTY4NQ==", "bodyText": "fine for now, but I'm not sure why hard bounds can't be translated into the range aggregation.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505709685", "createdAt": "2020-10-15T17:16:59Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java", "diffHunk": "@@ -51,6 +62,143 @@\n  * @see Rounding\n  */\n class DateHistogramAggregator extends BucketsAggregator implements SizedBucketAggregator {\n+    /**\n+     * Build an {@link Aggregator} for a {@code date_histogram} aggregation.\n+     * If we can determine the bucket boundaries from\n+     * {@link Rounding.Prepared#fixedRoundingPoints()} we use\n+     * {@link RangeAggregator} to do the actual collecting, otherwise we use\n+     * an specialized {@link DateHistogramAggregator Aggregator} specifically\n+     * for the {@code date_histogram}s. We prefer to delegate to the\n+     * {@linkplain RangeAggregator} because it can sometimes be further\n+     * optimized into a {@link FiltersAggregator}. Even when it can't be\n+     * optimized, it is going to be marginally faster and consume less memory\n+     * than the {@linkplain DateHistogramAggregator} because it doesn't need\n+     * to the rounding points and because it can pass precise cardinality\n+     * estimates to its child aggregations.\n+     */\n+    public static Aggregator build(\n+        String name,\n+        AggregatorFactories factories,\n+        Rounding rounding,\n+        BucketOrder order,\n+        boolean keyed,\n+        long minDocCount,\n+        @Nullable LongBounds extendedBounds,\n+        @Nullable LongBounds hardBounds,\n+        ValuesSourceConfig valuesSourceConfig,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        Rounding.Prepared preparedRounding = valuesSourceConfig.roundingPreparer().apply(rounding);\n+        Aggregator asRange = adaptIntoRangeOrNull(\n+            name,\n+            factories,\n+            rounding,\n+            preparedRounding,\n+            order,\n+            keyed,\n+            minDocCount,\n+            extendedBounds,\n+            hardBounds,\n+            valuesSourceConfig,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+        if (asRange != null) {\n+            return asRange;\n+        }\n+        return new DateHistogramAggregator(\n+            name,\n+            factories,\n+            rounding,\n+            preparedRounding,\n+            order,\n+            keyed,\n+            minDocCount,\n+            extendedBounds,\n+            hardBounds,\n+            valuesSourceConfig,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+    }\n+\n+    private static FromDateRange adaptIntoRangeOrNull(\n+        String name,\n+        AggregatorFactories factories,\n+        Rounding rounding,\n+        Rounding.Prepared preparedRounding,\n+        BucketOrder order,\n+        boolean keyed,\n+        long minDocCount,\n+        @Nullable LongBounds extendedBounds,\n+        @Nullable LongBounds hardBounds,\n+        ValuesSourceConfig valuesSourceConfig,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        if (hardBounds != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 124}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDIzMjAyNA==", "bodyText": "Mostly because I don't know what they are or how to translate them. Maybe I can poke @imotov.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510232024", "createdAt": "2020-10-22T14:58:09Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java", "diffHunk": "@@ -51,6 +62,143 @@\n  * @see Rounding\n  */\n class DateHistogramAggregator extends BucketsAggregator implements SizedBucketAggregator {\n+    /**\n+     * Build an {@link Aggregator} for a {@code date_histogram} aggregation.\n+     * If we can determine the bucket boundaries from\n+     * {@link Rounding.Prepared#fixedRoundingPoints()} we use\n+     * {@link RangeAggregator} to do the actual collecting, otherwise we use\n+     * an specialized {@link DateHistogramAggregator Aggregator} specifically\n+     * for the {@code date_histogram}s. We prefer to delegate to the\n+     * {@linkplain RangeAggregator} because it can sometimes be further\n+     * optimized into a {@link FiltersAggregator}. Even when it can't be\n+     * optimized, it is going to be marginally faster and consume less memory\n+     * than the {@linkplain DateHistogramAggregator} because it doesn't need\n+     * to the rounding points and because it can pass precise cardinality\n+     * estimates to its child aggregations.\n+     */\n+    public static Aggregator build(\n+        String name,\n+        AggregatorFactories factories,\n+        Rounding rounding,\n+        BucketOrder order,\n+        boolean keyed,\n+        long minDocCount,\n+        @Nullable LongBounds extendedBounds,\n+        @Nullable LongBounds hardBounds,\n+        ValuesSourceConfig valuesSourceConfig,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        Rounding.Prepared preparedRounding = valuesSourceConfig.roundingPreparer().apply(rounding);\n+        Aggregator asRange = adaptIntoRangeOrNull(\n+            name,\n+            factories,\n+            rounding,\n+            preparedRounding,\n+            order,\n+            keyed,\n+            minDocCount,\n+            extendedBounds,\n+            hardBounds,\n+            valuesSourceConfig,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+        if (asRange != null) {\n+            return asRange;\n+        }\n+        return new DateHistogramAggregator(\n+            name,\n+            factories,\n+            rounding,\n+            preparedRounding,\n+            order,\n+            keyed,\n+            minDocCount,\n+            extendedBounds,\n+            hardBounds,\n+            valuesSourceConfig,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+    }\n+\n+    private static FromDateRange adaptIntoRangeOrNull(\n+        String name,\n+        AggregatorFactories factories,\n+        Rounding rounding,\n+        Rounding.Prepared preparedRounding,\n+        BucketOrder order,\n+        boolean keyed,\n+        long minDocCount,\n+        @Nullable LongBounds extendedBounds,\n+        @Nullable LongBounds hardBounds,\n+        ValuesSourceConfig valuesSourceConfig,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        if (hardBounds != null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcwOTY4NQ=="}, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 124}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2Nzc1MTgyOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQxNzozMTo0NVrOHiSlwg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNzoxNjo1NFrOHmsAFA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcxODIxMA==", "bodyText": "I'm trying to move us away from null values sources.  By default, ValuesSourceConfig will return an empty values source for unmapped fields now.  Some aggregators still override that to be null internally, because of how we handle createUnmapped now.  But in general, we shouldn't be expecting null values sources.  Check ValuesSourceConfig#hasValues() instead.\nAlso, what ValuesSource?", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505718210", "createdAt": "2020-10-15T17:31:45Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java", "diffHunk": "@@ -195,4 +344,95 @@ public double bucketSize(long bucket, Rounding.DateTimeUnit unitSize) {\n             return 1.0;\n         }\n     }\n+\n+    private static class FromDateRange extends AdaptingAggregator implements SizedBucketAggregator {\n+        private final DocValueFormat format;\n+        private final Rounding rounding;\n+        private final Rounding.Prepared preparedRounding;\n+        private final BucketOrder order;\n+        private final long minDocCount;\n+        private final LongBounds extendedBounds;\n+        private final boolean keyed;\n+        private final long[] fixedRoundingPoints;\n+\n+        FromDateRange(\n+            Aggregator parent,\n+            AggregatorFactories subAggregators,\n+            CheckedFunction<AggregatorFactories, Aggregator, IOException> delegate,\n+            DocValueFormat format,\n+            Rounding rounding,\n+            Rounding.Prepared preparedRounding,\n+            BucketOrder order,\n+            long minDocCount,\n+            LongBounds extendedBounds,\n+            boolean keyed,\n+            long[] fixedRoundingPoints\n+        ) throws IOException {\n+            super(parent, subAggregators, delegate);\n+            this.format = format;\n+            this.rounding = rounding;\n+            this.preparedRounding = preparedRounding;\n+            this.order = order;\n+            order.validate(this);\n+            this.minDocCount = minDocCount;\n+            this.extendedBounds = extendedBounds;\n+            this.keyed = keyed;\n+            this.fixedRoundingPoints = fixedRoundingPoints;\n+        }\n+\n+        @Override\n+        protected InternalAggregation adapt(InternalAggregation delegateResult) {\n+            InternalDateRange range = (InternalDateRange) delegateResult;\n+            List<InternalDateHistogram.Bucket> buckets = new ArrayList<>(range.getBuckets().size());\n+            for (InternalDateRange.Bucket rangeBucket : range.getBuckets()) {\n+                if (rangeBucket.getDocCount() > 0) {\n+                    buckets.add(\n+                        new InternalDateHistogram.Bucket(\n+                            rangeBucket.getFrom().toInstant().toEpochMilli(),\n+                            rangeBucket.getDocCount(),\n+                            keyed,\n+                            format,\n+                            rangeBucket.getAggregations()\n+                        )\n+                    );\n+                }\n+            }\n+            CollectionUtil.introSort(buckets, BucketOrder.key(true).comparator());\n+\n+            // value source will be null for unmapped fields", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 257}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDIzMzE3OA==", "bodyText": "I'll have another look at this one! I missed this comment 7 days ago and am just seeing it now.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510233178", "createdAt": "2020-10-22T14:59:38Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java", "diffHunk": "@@ -195,4 +344,95 @@ public double bucketSize(long bucket, Rounding.DateTimeUnit unitSize) {\n             return 1.0;\n         }\n     }\n+\n+    private static class FromDateRange extends AdaptingAggregator implements SizedBucketAggregator {\n+        private final DocValueFormat format;\n+        private final Rounding rounding;\n+        private final Rounding.Prepared preparedRounding;\n+        private final BucketOrder order;\n+        private final long minDocCount;\n+        private final LongBounds extendedBounds;\n+        private final boolean keyed;\n+        private final long[] fixedRoundingPoints;\n+\n+        FromDateRange(\n+            Aggregator parent,\n+            AggregatorFactories subAggregators,\n+            CheckedFunction<AggregatorFactories, Aggregator, IOException> delegate,\n+            DocValueFormat format,\n+            Rounding rounding,\n+            Rounding.Prepared preparedRounding,\n+            BucketOrder order,\n+            long minDocCount,\n+            LongBounds extendedBounds,\n+            boolean keyed,\n+            long[] fixedRoundingPoints\n+        ) throws IOException {\n+            super(parent, subAggregators, delegate);\n+            this.format = format;\n+            this.rounding = rounding;\n+            this.preparedRounding = preparedRounding;\n+            this.order = order;\n+            order.validate(this);\n+            this.minDocCount = minDocCount;\n+            this.extendedBounds = extendedBounds;\n+            this.keyed = keyed;\n+            this.fixedRoundingPoints = fixedRoundingPoints;\n+        }\n+\n+        @Override\n+        protected InternalAggregation adapt(InternalAggregation delegateResult) {\n+            InternalDateRange range = (InternalDateRange) delegateResult;\n+            List<InternalDateHistogram.Bucket> buckets = new ArrayList<>(range.getBuckets().size());\n+            for (InternalDateRange.Bucket rangeBucket : range.getBuckets()) {\n+                if (rangeBucket.getDocCount() > 0) {\n+                    buckets.add(\n+                        new InternalDateHistogram.Bucket(\n+                            rangeBucket.getFrom().toInstant().toEpochMilli(),\n+                            rangeBucket.getDocCount(),\n+                            keyed,\n+                            format,\n+                            rangeBucket.getAggregations()\n+                        )\n+                    );\n+                }\n+            }\n+            CollectionUtil.introSort(buckets, BucketOrder.key(true).comparator());\n+\n+            // value source will be null for unmapped fields", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcxODIxMA=="}, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 257}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDMyODg1Mg==", "bodyText": "I copied this from above but I don't think it is accurate either place. I've zapped it.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510328852", "createdAt": "2020-10-22T17:16:54Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java", "diffHunk": "@@ -195,4 +344,95 @@ public double bucketSize(long bucket, Rounding.DateTimeUnit unitSize) {\n             return 1.0;\n         }\n     }\n+\n+    private static class FromDateRange extends AdaptingAggregator implements SizedBucketAggregator {\n+        private final DocValueFormat format;\n+        private final Rounding rounding;\n+        private final Rounding.Prepared preparedRounding;\n+        private final BucketOrder order;\n+        private final long minDocCount;\n+        private final LongBounds extendedBounds;\n+        private final boolean keyed;\n+        private final long[] fixedRoundingPoints;\n+\n+        FromDateRange(\n+            Aggregator parent,\n+            AggregatorFactories subAggregators,\n+            CheckedFunction<AggregatorFactories, Aggregator, IOException> delegate,\n+            DocValueFormat format,\n+            Rounding rounding,\n+            Rounding.Prepared preparedRounding,\n+            BucketOrder order,\n+            long minDocCount,\n+            LongBounds extendedBounds,\n+            boolean keyed,\n+            long[] fixedRoundingPoints\n+        ) throws IOException {\n+            super(parent, subAggregators, delegate);\n+            this.format = format;\n+            this.rounding = rounding;\n+            this.preparedRounding = preparedRounding;\n+            this.order = order;\n+            order.validate(this);\n+            this.minDocCount = minDocCount;\n+            this.extendedBounds = extendedBounds;\n+            this.keyed = keyed;\n+            this.fixedRoundingPoints = fixedRoundingPoints;\n+        }\n+\n+        @Override\n+        protected InternalAggregation adapt(InternalAggregation delegateResult) {\n+            InternalDateRange range = (InternalDateRange) delegateResult;\n+            List<InternalDateHistogram.Bucket> buckets = new ArrayList<>(range.getBuckets().size());\n+            for (InternalDateRange.Bucket rangeBucket : range.getBuckets()) {\n+                if (rangeBucket.getDocCount() > 0) {\n+                    buckets.add(\n+                        new InternalDateHistogram.Bucket(\n+                            rangeBucket.getFrom().toInstant().toEpochMilli(),\n+                            rangeBucket.getDocCount(),\n+                            keyed,\n+                            format,\n+                            rangeBucket.getAggregations()\n+                        )\n+                    );\n+                }\n+            }\n+            CollectionUtil.introSort(buckets, BucketOrder.key(true).comparator());\n+\n+            // value source will be null for unmapped fields", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcxODIxMA=="}, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 257}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2Nzc2NjE0OnYy", "diffSide": "LEFT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/AbstractRangeAggregatorFactory.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQxNzozNToxNVrOHiSu_Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQxNzozNToxNVrOHiSu_Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcyMDU3Mw==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505720573", "createdAt": "2020-10-15T17:35:15Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/AbstractRangeAggregatorFactory.java", "diffHunk": "@@ -92,8 +91,7 @@ protected Aggregator doCreateInternal(\n             .build(\n                 name,\n                 factories,\n-                (Numeric) config.getValuesSource(),\n-                config.format(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2Nzc3MjU1OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQxNzozNzoxMVrOHiSzPA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQxODowNzo0NVrOHiT6kA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcyMTY2MA==", "bodyText": "Like with filters, I think a note explaining when to subclass this would be helpful.  Especially since there's already a lot of weird reuse patterns in the Range family.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505721660", "createdAt": "2020-10-15T17:37:11Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java", "diffHunk": "@@ -52,7 +63,7 @@\n \n import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n \n-public class RangeAggregator extends BucketsAggregator {\n+public abstract class RangeAggregator extends BucketsAggregator {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTczOTkyMA==", "bodyText": "++", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505739920", "createdAt": "2020-10-15T18:07:45Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java", "diffHunk": "@@ -52,7 +63,7 @@\n \n import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n \n-public class RangeAggregator extends BucketsAggregator {\n+public abstract class RangeAggregator extends BucketsAggregator {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcyMTY2MA=="}, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2NzgxNjEyOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQxNzo0OTowNVrOHiTO0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNjowNjoyNlrOHmpN8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcyODcyMA==", "bodyText": "ValuesSourceConfig#getPointReaderOrNull() uses nearly the same set of checks for nearly the same reason - would it make sense to wrap all of these into one predicate on VSConfig?", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505728720", "createdAt": "2020-10-15T17:49:05Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java", "diffHunk": "@@ -215,15 +226,186 @@ public boolean equals(Object obj) {\n         }\n     }\n \n-    final ValuesSource.Numeric valuesSource;\n-    final DocValueFormat format;\n-    final Range[] ranges;\n-    final boolean keyed;\n-    final InternalRange.Factory rangeFactory;\n+    /**\n+     * Build an {@link Aggregator} for a {@code range} aggregation. If the\n+     * {@code ranges} can be converted into filters then it builds a\n+     * {@link FiltersAggregator} and uses that to collect the results\n+     * <strong>if</strong> that aggregator can run in \"filter by filter\"\n+     * collection mode. If it can't then we'll collect the ranges using\n+     * a native {@link RangeAggregator} which is significantly faster\n+     * than the \"compatible\" collection mechanism for the filters agg.\n+     */\n+    public static Aggregator build(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSourceConfig valuesSourceConfig,\n+        InternalRange.Factory<?, ?> rangeFactory,\n+        Range[] ranges,\n+        boolean keyed,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        Aggregator adapted = adaptIntoFiltersOrNull(\n+            name,\n+            factories,\n+            valuesSourceConfig,\n+            rangeFactory,\n+            ranges,\n+            keyed,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+        if (adapted != null) {\n+            return adapted;\n+        }\n+        return buildWithoutAttemptedToAdaptToFilters(\n+            name,\n+            factories,\n+            (ValuesSource.Numeric) valuesSourceConfig.getValuesSource(),\n+            valuesSourceConfig.format(),\n+            rangeFactory,\n+            ranges,\n+            keyed,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+    }\n \n-    final double[] maxTo;\n+    public static Aggregator adaptIntoFiltersOrNull(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSourceConfig valuesSourceConfig,\n+        InternalRange.Factory<?, ?> rangeFactory,\n+        Range[] ranges,\n+        boolean keyed,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        if (valuesSourceConfig.fieldType() == null) {\n+            return null;\n+        }\n+        if (false == valuesSourceConfig.fieldType().isSearchable()) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.missing() != null) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.script() != null) {\n+            return null;\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 129}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTczOTc5NQ==", "bodyText": "Yes.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505739795", "createdAt": "2020-10-15T18:07:33Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java", "diffHunk": "@@ -215,15 +226,186 @@ public boolean equals(Object obj) {\n         }\n     }\n \n-    final ValuesSource.Numeric valuesSource;\n-    final DocValueFormat format;\n-    final Range[] ranges;\n-    final boolean keyed;\n-    final InternalRange.Factory rangeFactory;\n+    /**\n+     * Build an {@link Aggregator} for a {@code range} aggregation. If the\n+     * {@code ranges} can be converted into filters then it builds a\n+     * {@link FiltersAggregator} and uses that to collect the results\n+     * <strong>if</strong> that aggregator can run in \"filter by filter\"\n+     * collection mode. If it can't then we'll collect the ranges using\n+     * a native {@link RangeAggregator} which is significantly faster\n+     * than the \"compatible\" collection mechanism for the filters agg.\n+     */\n+    public static Aggregator build(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSourceConfig valuesSourceConfig,\n+        InternalRange.Factory<?, ?> rangeFactory,\n+        Range[] ranges,\n+        boolean keyed,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        Aggregator adapted = adaptIntoFiltersOrNull(\n+            name,\n+            factories,\n+            valuesSourceConfig,\n+            rangeFactory,\n+            ranges,\n+            keyed,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+        if (adapted != null) {\n+            return adapted;\n+        }\n+        return buildWithoutAttemptedToAdaptToFilters(\n+            name,\n+            factories,\n+            (ValuesSource.Numeric) valuesSourceConfig.getValuesSource(),\n+            valuesSourceConfig.format(),\n+            rangeFactory,\n+            ranges,\n+            keyed,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+    }\n \n-    final double[] maxTo;\n+    public static Aggregator adaptIntoFiltersOrNull(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSourceConfig valuesSourceConfig,\n+        InternalRange.Factory<?, ?> rangeFactory,\n+        Range[] ranges,\n+        boolean keyed,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        if (valuesSourceConfig.fieldType() == null) {\n+            return null;\n+        }\n+        if (false == valuesSourceConfig.fieldType().isSearchable()) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.missing() != null) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.script() != null) {\n+            return null;\n+        }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcyODcyMA=="}, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 129}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDI2MDA1Ng==", "bodyText": "Do you plan to do that in this PR, or open a ticket for it?  Could be a good candidate for good first issue, since it's mostly just moving code around.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510260056", "createdAt": "2020-10-22T15:35:09Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java", "diffHunk": "@@ -215,15 +226,186 @@ public boolean equals(Object obj) {\n         }\n     }\n \n-    final ValuesSource.Numeric valuesSource;\n-    final DocValueFormat format;\n-    final Range[] ranges;\n-    final boolean keyed;\n-    final InternalRange.Factory rangeFactory;\n+    /**\n+     * Build an {@link Aggregator} for a {@code range} aggregation. If the\n+     * {@code ranges} can be converted into filters then it builds a\n+     * {@link FiltersAggregator} and uses that to collect the results\n+     * <strong>if</strong> that aggregator can run in \"filter by filter\"\n+     * collection mode. If it can't then we'll collect the ranges using\n+     * a native {@link RangeAggregator} which is significantly faster\n+     * than the \"compatible\" collection mechanism for the filters agg.\n+     */\n+    public static Aggregator build(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSourceConfig valuesSourceConfig,\n+        InternalRange.Factory<?, ?> rangeFactory,\n+        Range[] ranges,\n+        boolean keyed,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        Aggregator adapted = adaptIntoFiltersOrNull(\n+            name,\n+            factories,\n+            valuesSourceConfig,\n+            rangeFactory,\n+            ranges,\n+            keyed,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+        if (adapted != null) {\n+            return adapted;\n+        }\n+        return buildWithoutAttemptedToAdaptToFilters(\n+            name,\n+            factories,\n+            (ValuesSource.Numeric) valuesSourceConfig.getValuesSource(),\n+            valuesSourceConfig.format(),\n+            rangeFactory,\n+            ranges,\n+            keyed,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+    }\n \n-    final double[] maxTo;\n+    public static Aggregator adaptIntoFiltersOrNull(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSourceConfig valuesSourceConfig,\n+        InternalRange.Factory<?, ?> rangeFactory,\n+        Range[] ranges,\n+        boolean keyed,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        if (valuesSourceConfig.fieldType() == null) {\n+            return null;\n+        }\n+        if (false == valuesSourceConfig.fieldType().isSearchable()) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.missing() != null) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.script() != null) {\n+            return null;\n+        }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcyODcyMA=="}, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 129}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDI4MzI0OA==", "bodyText": "Actually, I see now that you added the method on VSConfig, but aren't calling it from here.  Oversight?", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510283248", "createdAt": "2020-10-22T16:06:26Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java", "diffHunk": "@@ -215,15 +226,186 @@ public boolean equals(Object obj) {\n         }\n     }\n \n-    final ValuesSource.Numeric valuesSource;\n-    final DocValueFormat format;\n-    final Range[] ranges;\n-    final boolean keyed;\n-    final InternalRange.Factory rangeFactory;\n+    /**\n+     * Build an {@link Aggregator} for a {@code range} aggregation. If the\n+     * {@code ranges} can be converted into filters then it builds a\n+     * {@link FiltersAggregator} and uses that to collect the results\n+     * <strong>if</strong> that aggregator can run in \"filter by filter\"\n+     * collection mode. If it can't then we'll collect the ranges using\n+     * a native {@link RangeAggregator} which is significantly faster\n+     * than the \"compatible\" collection mechanism for the filters agg.\n+     */\n+    public static Aggregator build(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSourceConfig valuesSourceConfig,\n+        InternalRange.Factory<?, ?> rangeFactory,\n+        Range[] ranges,\n+        boolean keyed,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        Aggregator adapted = adaptIntoFiltersOrNull(\n+            name,\n+            factories,\n+            valuesSourceConfig,\n+            rangeFactory,\n+            ranges,\n+            keyed,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+        if (adapted != null) {\n+            return adapted;\n+        }\n+        return buildWithoutAttemptedToAdaptToFilters(\n+            name,\n+            factories,\n+            (ValuesSource.Numeric) valuesSourceConfig.getValuesSource(),\n+            valuesSourceConfig.format(),\n+            rangeFactory,\n+            ranges,\n+            keyed,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+    }\n \n-    final double[] maxTo;\n+    public static Aggregator adaptIntoFiltersOrNull(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSourceConfig valuesSourceConfig,\n+        InternalRange.Factory<?, ?> rangeFactory,\n+        Range[] ranges,\n+        boolean keyed,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        if (valuesSourceConfig.fieldType() == null) {\n+            return null;\n+        }\n+        if (false == valuesSourceConfig.fieldType().isSearchable()) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.missing() != null) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.script() != null) {\n+            return null;\n+        }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcyODcyMA=="}, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 129}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2Nzg0MTY2OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQxNzo1NTozOVrOHiTe3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQxODowNzoyOVrOHiT57g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTczMjgyOA==", "bodyText": "Just to say it out loud: we don't care that we just built and then threw away an aggregator here, because aggregator creation time is not highly sensitive to performance hits, correct?", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505732828", "createdAt": "2020-10-15T17:55:39Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java", "diffHunk": "@@ -215,15 +226,186 @@ public boolean equals(Object obj) {\n         }\n     }\n \n-    final ValuesSource.Numeric valuesSource;\n-    final DocValueFormat format;\n-    final Range[] ranges;\n-    final boolean keyed;\n-    final InternalRange.Factory rangeFactory;\n+    /**\n+     * Build an {@link Aggregator} for a {@code range} aggregation. If the\n+     * {@code ranges} can be converted into filters then it builds a\n+     * {@link FiltersAggregator} and uses that to collect the results\n+     * <strong>if</strong> that aggregator can run in \"filter by filter\"\n+     * collection mode. If it can't then we'll collect the ranges using\n+     * a native {@link RangeAggregator} which is significantly faster\n+     * than the \"compatible\" collection mechanism for the filters agg.\n+     */\n+    public static Aggregator build(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSourceConfig valuesSourceConfig,\n+        InternalRange.Factory<?, ?> rangeFactory,\n+        Range[] ranges,\n+        boolean keyed,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        Aggregator adapted = adaptIntoFiltersOrNull(\n+            name,\n+            factories,\n+            valuesSourceConfig,\n+            rangeFactory,\n+            ranges,\n+            keyed,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+        if (adapted != null) {\n+            return adapted;\n+        }\n+        return buildWithoutAttemptedToAdaptToFilters(\n+            name,\n+            factories,\n+            (ValuesSource.Numeric) valuesSourceConfig.getValuesSource(),\n+            valuesSourceConfig.format(),\n+            rangeFactory,\n+            ranges,\n+            keyed,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+    }\n \n-    final double[] maxTo;\n+    public static Aggregator adaptIntoFiltersOrNull(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSourceConfig valuesSourceConfig,\n+        InternalRange.Factory<?, ?> rangeFactory,\n+        Range[] ranges,\n+        boolean keyed,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        if (valuesSourceConfig.fieldType() == null) {\n+            return null;\n+        }\n+        if (false == valuesSourceConfig.fieldType().isSearchable()) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.missing() != null) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.script() != null) {\n+            return null;\n+        }\n+        // TODO bail here for runtime fields. They'll be slower this way. Maybe we can somehow look at the Query?\n+        if (valuesSourceConfig.fieldType() instanceof DateFieldType\n+            && ((DateFieldType) valuesSourceConfig.fieldType()).resolution() == Resolution.NANOSECONDS) {\n+            // We don't generate sensible Queries for nanoseconds.\n+            return null;\n+        }\n+        String[] keys = new String[ranges.length];\n+        Query[] filters = new Query[ranges.length];\n+        for (int i = 0; i < ranges.length; i++) {\n+            keys[i] = Integer.toString(i);\n+            /*\n+             * Use the native format on the field rather than the one provided\n+             * on the valuesSourceConfig because the format on the field is what\n+             * we parse. With https://github.com/elastic/elasticsearch/pull/63692\n+             * we can just cast to a long here and it'll be taken as millis.\n+             */\n+            DocValueFormat format = valuesSourceConfig.fieldType().docValueFormat(null, null);\n+            filters[i] = valuesSourceConfig.fieldType()\n+                .rangeQuery(\n+                    ranges[i].from == Double.NEGATIVE_INFINITY ? null : format.format(ranges[i].from),\n+                    ranges[i].to == Double.POSITIVE_INFINITY ? null : format.format(ranges[i].to),\n+                    true,\n+                    false,\n+                    ShapeRelation.CONTAINS,\n+                    null,\n+                    null,\n+                    context.getQueryShardContext()\n+                );\n+        }\n+        RangeAggregator.FromFilters<?> fromFilters = new RangeAggregator.FromFilters<>(\n+            parent,\n+            factories,\n+            subAggregators -> FiltersAggregator.build(\n+                name,\n+                subAggregators,\n+                keys,\n+                filters,\n+                false,\n+                null,\n+                context,\n+                parent,\n+                cardinality,\n+                metadata\n+            ),\n+            valuesSourceConfig.format(),\n+            ranges,\n+            keyed,\n+            rangeFactory\n+        );\n+        if (false == ((FiltersAggregator) fromFilters.delegate()).collectsInFilterOrder()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 179}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTczOTc1OA==", "bodyText": "That is how believe that is so. It might be kind of me to make a method that returns null if it can't collect in order though.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505739758", "createdAt": "2020-10-15T18:07:29Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java", "diffHunk": "@@ -215,15 +226,186 @@ public boolean equals(Object obj) {\n         }\n     }\n \n-    final ValuesSource.Numeric valuesSource;\n-    final DocValueFormat format;\n-    final Range[] ranges;\n-    final boolean keyed;\n-    final InternalRange.Factory rangeFactory;\n+    /**\n+     * Build an {@link Aggregator} for a {@code range} aggregation. If the\n+     * {@code ranges} can be converted into filters then it builds a\n+     * {@link FiltersAggregator} and uses that to collect the results\n+     * <strong>if</strong> that aggregator can run in \"filter by filter\"\n+     * collection mode. If it can't then we'll collect the ranges using\n+     * a native {@link RangeAggregator} which is significantly faster\n+     * than the \"compatible\" collection mechanism for the filters agg.\n+     */\n+    public static Aggregator build(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSourceConfig valuesSourceConfig,\n+        InternalRange.Factory<?, ?> rangeFactory,\n+        Range[] ranges,\n+        boolean keyed,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        Aggregator adapted = adaptIntoFiltersOrNull(\n+            name,\n+            factories,\n+            valuesSourceConfig,\n+            rangeFactory,\n+            ranges,\n+            keyed,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+        if (adapted != null) {\n+            return adapted;\n+        }\n+        return buildWithoutAttemptedToAdaptToFilters(\n+            name,\n+            factories,\n+            (ValuesSource.Numeric) valuesSourceConfig.getValuesSource(),\n+            valuesSourceConfig.format(),\n+            rangeFactory,\n+            ranges,\n+            keyed,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+    }\n \n-    final double[] maxTo;\n+    public static Aggregator adaptIntoFiltersOrNull(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSourceConfig valuesSourceConfig,\n+        InternalRange.Factory<?, ?> rangeFactory,\n+        Range[] ranges,\n+        boolean keyed,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        if (valuesSourceConfig.fieldType() == null) {\n+            return null;\n+        }\n+        if (false == valuesSourceConfig.fieldType().isSearchable()) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.missing() != null) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.script() != null) {\n+            return null;\n+        }\n+        // TODO bail here for runtime fields. They'll be slower this way. Maybe we can somehow look at the Query?\n+        if (valuesSourceConfig.fieldType() instanceof DateFieldType\n+            && ((DateFieldType) valuesSourceConfig.fieldType()).resolution() == Resolution.NANOSECONDS) {\n+            // We don't generate sensible Queries for nanoseconds.\n+            return null;\n+        }\n+        String[] keys = new String[ranges.length];\n+        Query[] filters = new Query[ranges.length];\n+        for (int i = 0; i < ranges.length; i++) {\n+            keys[i] = Integer.toString(i);\n+            /*\n+             * Use the native format on the field rather than the one provided\n+             * on the valuesSourceConfig because the format on the field is what\n+             * we parse. With https://github.com/elastic/elasticsearch/pull/63692\n+             * we can just cast to a long here and it'll be taken as millis.\n+             */\n+            DocValueFormat format = valuesSourceConfig.fieldType().docValueFormat(null, null);\n+            filters[i] = valuesSourceConfig.fieldType()\n+                .rangeQuery(\n+                    ranges[i].from == Double.NEGATIVE_INFINITY ? null : format.format(ranges[i].from),\n+                    ranges[i].to == Double.POSITIVE_INFINITY ? null : format.format(ranges[i].to),\n+                    true,\n+                    false,\n+                    ShapeRelation.CONTAINS,\n+                    null,\n+                    null,\n+                    context.getQueryShardContext()\n+                );\n+        }\n+        RangeAggregator.FromFilters<?> fromFilters = new RangeAggregator.FromFilters<>(\n+            parent,\n+            factories,\n+            subAggregators -> FiltersAggregator.build(\n+                name,\n+                subAggregators,\n+                keys,\n+                filters,\n+                false,\n+                null,\n+                context,\n+                parent,\n+                cardinality,\n+                metadata\n+            ),\n+            valuesSourceConfig.format(),\n+            ranges,\n+            keyed,\n+            rangeFactory\n+        );\n+        if (false == ((FiltersAggregator) fromFilters.delegate()).collectsInFilterOrder()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTczMjgyOA=="}, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 179}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5NjE2NTIyOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNTowMDoyNlrOHmmM3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNjozNTo1N1rOHmqcug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDIzMzgyMw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    if ((double) max != max) {\n          \n          \n            \n                    if ((long) ((double) max) != max) {\n          \n      \n    \n    \n  \n\nLet's be explicit about the casting here.  I don't want to have to wonder if some optimization decided to cast the RHS to a double instead of the LHS to a long.  Ditto for other instances of this check.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510233823", "createdAt": "2020-10-22T15:00:26Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java", "diffHunk": "@@ -22,35 +22,186 @@\n import org.apache.lucene.index.SortedNumericDocValues;\n import org.apache.lucene.search.ScoreMode;\n import org.apache.lucene.util.CollectionUtil;\n+import org.elasticsearch.common.CheckedFunction;\n import org.elasticsearch.common.Nullable;\n import org.elasticsearch.common.Rounding;\n+import org.elasticsearch.common.Rounding.DateTimeUnit;\n import org.elasticsearch.common.lease.Releasables;\n import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.aggregations.AdaptingAggregator;\n import org.elasticsearch.search.aggregations.Aggregator;\n import org.elasticsearch.search.aggregations.AggregatorFactories;\n import org.elasticsearch.search.aggregations.BucketOrder;\n import org.elasticsearch.search.aggregations.CardinalityUpperBound;\n import org.elasticsearch.search.aggregations.InternalAggregation;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n import org.elasticsearch.search.aggregations.LeafBucketCollector;\n import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;\n import org.elasticsearch.search.aggregations.bucket.BucketsAggregator;\n+import org.elasticsearch.search.aggregations.bucket.filter.FiltersAggregator;\n+import org.elasticsearch.search.aggregations.bucket.range.InternalDateRange;\n+import org.elasticsearch.search.aggregations.bucket.range.RangeAggregationBuilder;\n+import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator;\n+import org.elasticsearch.search.aggregations.bucket.range.RangeAggregatorSupplier;\n import org.elasticsearch.search.aggregations.bucket.terms.LongKeyedBucketOrds;\n import org.elasticsearch.search.aggregations.support.ValuesSource;\n import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;\n import org.elasticsearch.search.internal.SearchContext;\n \n import java.io.IOException;\n+import java.util.ArrayList;\n import java.util.Collections;\n+import java.util.List;\n import java.util.Map;\n import java.util.function.BiConsumer;\n \n /**\n- * An aggregator for date values. Every date is rounded down using a configured\n- * {@link Rounding}.\n- *\n- * @see Rounding\n+ * Aggregator for {@code date_histogram} that rounds values using\n+ * {@link Rounding}. See {@link FromDateRange} which also aggregates for\n+ * {@code date_histogram} but does so by running a {@code range} aggregation\n+ * over the date and transforming the results. In general\n+ * {@link FromDateRange} is faster than {@link DateHistogramAggregator}\n+ * but {@linkplain DateHistogramAggregator} works when we can't precalculate\n+ * all of the {@link Rounding.Prepared#fixedRoundingPoints() fixed rounding points}.\n  */\n class DateHistogramAggregator extends BucketsAggregator implements SizedBucketAggregator {\n+    /**\n+     * Build an {@link Aggregator} for a {@code date_histogram} aggregation.\n+     * If we can determine the bucket boundaries from\n+     * {@link Rounding.Prepared#fixedRoundingPoints()} we use\n+     * {@link RangeAggregator} to do the actual collecting, otherwise we use\n+     * an specialized {@link DateHistogramAggregator Aggregator} specifically\n+     * for the {@code date_histogram}s. We prefer to delegate to the\n+     * {@linkplain RangeAggregator} because it can sometimes be further\n+     * optimized into a {@link FiltersAggregator}. Even when it can't be\n+     * optimized, it is going to be marginally faster and consume less memory\n+     * than the {@linkplain DateHistogramAggregator} because it doesn't need\n+     * to the rounding points and because it can pass precise cardinality\n+     * estimates to its child aggregations.\n+     */\n+    public static Aggregator build(\n+        String name,\n+        AggregatorFactories factories,\n+        Rounding rounding,\n+        BucketOrder order,\n+        boolean keyed,\n+        long minDocCount,\n+        @Nullable LongBounds extendedBounds,\n+        @Nullable LongBounds hardBounds,\n+        ValuesSourceConfig valuesSourceConfig,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        Rounding.Prepared preparedRounding = valuesSourceConfig.roundingPreparer().apply(rounding);\n+        Aggregator asRange = adaptIntoRangeOrNull(\n+            name,\n+            factories,\n+            rounding,\n+            preparedRounding,\n+            order,\n+            keyed,\n+            minDocCount,\n+            extendedBounds,\n+            hardBounds,\n+            valuesSourceConfig,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+        if (asRange != null) {\n+            return asRange;\n+        }\n+        return new DateHistogramAggregator(\n+            name,\n+            factories,\n+            rounding,\n+            preparedRounding,\n+            order,\n+            keyed,\n+            minDocCount,\n+            extendedBounds,\n+            hardBounds,\n+            valuesSourceConfig,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+    }\n+\n+    private static FromDateRange adaptIntoRangeOrNull(\n+        String name,\n+        AggregatorFactories factories,\n+        Rounding rounding,\n+        Rounding.Prepared preparedRounding,\n+        BucketOrder order,\n+        boolean keyed,\n+        long minDocCount,\n+        @Nullable LongBounds extendedBounds,\n+        @Nullable LongBounds hardBounds,\n+        ValuesSourceConfig valuesSourceConfig,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        if (hardBounds != null) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.hasValues() == false) {\n+            return null;\n+        }\n+        long[] fixedRoundingPoints = preparedRounding.fixedRoundingPoints();\n+        if (fixedRoundingPoints == null) {\n+            return null;\n+        }\n+        // Range aggs use a double to aggregate and we don't want to lose precision.\n+        long max = fixedRoundingPoints[fixedRoundingPoints.length - 1];\n+        if ((double) max != max) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "117eb77edc372d3d707dca96dbcdf161e3c33faf"}, "originalPosition": 146}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDMwMzQxOA==", "bodyText": "I'm going to replace this with a hard comparison which is easier to reason about. max > 1L << 53.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510303418", "createdAt": "2020-10-22T16:35:57Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java", "diffHunk": "@@ -22,35 +22,186 @@\n import org.apache.lucene.index.SortedNumericDocValues;\n import org.apache.lucene.search.ScoreMode;\n import org.apache.lucene.util.CollectionUtil;\n+import org.elasticsearch.common.CheckedFunction;\n import org.elasticsearch.common.Nullable;\n import org.elasticsearch.common.Rounding;\n+import org.elasticsearch.common.Rounding.DateTimeUnit;\n import org.elasticsearch.common.lease.Releasables;\n import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.aggregations.AdaptingAggregator;\n import org.elasticsearch.search.aggregations.Aggregator;\n import org.elasticsearch.search.aggregations.AggregatorFactories;\n import org.elasticsearch.search.aggregations.BucketOrder;\n import org.elasticsearch.search.aggregations.CardinalityUpperBound;\n import org.elasticsearch.search.aggregations.InternalAggregation;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n import org.elasticsearch.search.aggregations.LeafBucketCollector;\n import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;\n import org.elasticsearch.search.aggregations.bucket.BucketsAggregator;\n+import org.elasticsearch.search.aggregations.bucket.filter.FiltersAggregator;\n+import org.elasticsearch.search.aggregations.bucket.range.InternalDateRange;\n+import org.elasticsearch.search.aggregations.bucket.range.RangeAggregationBuilder;\n+import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator;\n+import org.elasticsearch.search.aggregations.bucket.range.RangeAggregatorSupplier;\n import org.elasticsearch.search.aggregations.bucket.terms.LongKeyedBucketOrds;\n import org.elasticsearch.search.aggregations.support.ValuesSource;\n import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;\n import org.elasticsearch.search.internal.SearchContext;\n \n import java.io.IOException;\n+import java.util.ArrayList;\n import java.util.Collections;\n+import java.util.List;\n import java.util.Map;\n import java.util.function.BiConsumer;\n \n /**\n- * An aggregator for date values. Every date is rounded down using a configured\n- * {@link Rounding}.\n- *\n- * @see Rounding\n+ * Aggregator for {@code date_histogram} that rounds values using\n+ * {@link Rounding}. See {@link FromDateRange} which also aggregates for\n+ * {@code date_histogram} but does so by running a {@code range} aggregation\n+ * over the date and transforming the results. In general\n+ * {@link FromDateRange} is faster than {@link DateHistogramAggregator}\n+ * but {@linkplain DateHistogramAggregator} works when we can't precalculate\n+ * all of the {@link Rounding.Prepared#fixedRoundingPoints() fixed rounding points}.\n  */\n class DateHistogramAggregator extends BucketsAggregator implements SizedBucketAggregator {\n+    /**\n+     * Build an {@link Aggregator} for a {@code date_histogram} aggregation.\n+     * If we can determine the bucket boundaries from\n+     * {@link Rounding.Prepared#fixedRoundingPoints()} we use\n+     * {@link RangeAggregator} to do the actual collecting, otherwise we use\n+     * an specialized {@link DateHistogramAggregator Aggregator} specifically\n+     * for the {@code date_histogram}s. We prefer to delegate to the\n+     * {@linkplain RangeAggregator} because it can sometimes be further\n+     * optimized into a {@link FiltersAggregator}. Even when it can't be\n+     * optimized, it is going to be marginally faster and consume less memory\n+     * than the {@linkplain DateHistogramAggregator} because it doesn't need\n+     * to the rounding points and because it can pass precise cardinality\n+     * estimates to its child aggregations.\n+     */\n+    public static Aggregator build(\n+        String name,\n+        AggregatorFactories factories,\n+        Rounding rounding,\n+        BucketOrder order,\n+        boolean keyed,\n+        long minDocCount,\n+        @Nullable LongBounds extendedBounds,\n+        @Nullable LongBounds hardBounds,\n+        ValuesSourceConfig valuesSourceConfig,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        Rounding.Prepared preparedRounding = valuesSourceConfig.roundingPreparer().apply(rounding);\n+        Aggregator asRange = adaptIntoRangeOrNull(\n+            name,\n+            factories,\n+            rounding,\n+            preparedRounding,\n+            order,\n+            keyed,\n+            minDocCount,\n+            extendedBounds,\n+            hardBounds,\n+            valuesSourceConfig,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+        if (asRange != null) {\n+            return asRange;\n+        }\n+        return new DateHistogramAggregator(\n+            name,\n+            factories,\n+            rounding,\n+            preparedRounding,\n+            order,\n+            keyed,\n+            minDocCount,\n+            extendedBounds,\n+            hardBounds,\n+            valuesSourceConfig,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+    }\n+\n+    private static FromDateRange adaptIntoRangeOrNull(\n+        String name,\n+        AggregatorFactories factories,\n+        Rounding rounding,\n+        Rounding.Prepared preparedRounding,\n+        BucketOrder order,\n+        boolean keyed,\n+        long minDocCount,\n+        @Nullable LongBounds extendedBounds,\n+        @Nullable LongBounds hardBounds,\n+        ValuesSourceConfig valuesSourceConfig,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        if (hardBounds != null) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.hasValues() == false) {\n+            return null;\n+        }\n+        long[] fixedRoundingPoints = preparedRounding.fixedRoundingPoints();\n+        if (fixedRoundingPoints == null) {\n+            return null;\n+        }\n+        // Range aggs use a double to aggregate and we don't want to lose precision.\n+        long max = fixedRoundingPoints[fixedRoundingPoints.length - 1];\n+        if ((double) max != max) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDIzMzgyMw=="}, "originalCommit": {"oid": "117eb77edc372d3d707dca96dbcdf161e3c33faf"}, "originalPosition": 146}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5NjI5MTkyOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceConfig.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNToyNzoxMFrOHmnb5Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNToyNzoxMFrOHmnb5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDI1NDA1Mw==", "bodyText": "Looks like I forgot to use this in RangeAggregator!", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510254053", "createdAt": "2020-10-22T15:27:10Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceConfig.java", "diffHunk": "@@ -385,11 +385,17 @@ public boolean hasGlobalOrdinals() {\n      */\n     @Nullable\n     public Function<byte[], Number> getPointReaderOrNull() {\n-        MappedFieldType fieldType = fieldType();\n-        if (fieldType != null && script() == null && missing() == null) {\n-            return fieldType.pointReaderIfPossible();\n-        }\n-        return null;\n+        return alignesWithSearchIndex() ? fieldType().pointReaderIfPossible() : null;\n+    }\n+\n+    /**\n+     * Do {@link ValuesSource}s built by this config line up with the search\n+     * index of the underlying field? This'll only return true if the fields\n+     * is searchable and there aren't missing values or a script to confuse\n+     * the ordering.\n+     */\n+    public boolean alignesWithSearchIndex() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "117eb77edc372d3d707dca96dbcdf161e3c33faf"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5NjM0ODc4OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNTozOTo0M1rOHmoAtQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNzozNDoyMFrOHmspyg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDI2MzQ3Nw==", "bodyText": "My instinct is that this should be delegated to the ValuesSourceType in some way, but I'm not sure how right now.  Something to think about.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510263477", "createdAt": "2020-10-22T15:39:43Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java", "diffHunk": "@@ -215,15 +235,207 @@ public boolean equals(Object obj) {\n         }\n     }\n \n-    final ValuesSource.Numeric valuesSource;\n-    final DocValueFormat format;\n-    final Range[] ranges;\n-    final boolean keyed;\n-    final InternalRange.Factory rangeFactory;\n+    /**\n+     * Build an {@link Aggregator} for a {@code range} aggregation. If the\n+     * {@code ranges} can be converted into filters then it builds a\n+     * {@link FiltersAggregator} and uses that to collect the results\n+     * <strong>if</strong> that aggregator can run in \"filter by filter\"\n+     * collection mode. If it can't then we'll collect the ranges using\n+     * a native {@link RangeAggregator} which is significantly faster\n+     * than the \"compatible\" collection mechanism for the filters agg.\n+     */\n+    public static Aggregator build(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSourceConfig valuesSourceConfig,\n+        InternalRange.Factory<?, ?> rangeFactory,\n+        Range[] ranges,\n+        boolean keyed,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        Aggregator adapted = adaptIntoFiltersOrNull(\n+            name,\n+            factories,\n+            valuesSourceConfig,\n+            rangeFactory,\n+            ranges,\n+            keyed,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+        if (adapted != null) {\n+            return adapted;\n+        }\n+        return buildWithoutAttemptedToAdaptToFilters(\n+            name,\n+            factories,\n+            (ValuesSource.Numeric) valuesSourceConfig.getValuesSource(),\n+            valuesSourceConfig.format(),\n+            rangeFactory,\n+            ranges,\n+            keyed,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+    }\n+\n+    public static Aggregator adaptIntoFiltersOrNull(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSourceConfig valuesSourceConfig,\n+        InternalRange.Factory<?, ?> rangeFactory,\n+        Range[] ranges,\n+        boolean keyed,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        if (valuesSourceConfig.fieldType() == null) {\n+            return null;\n+        }\n+        if (false == valuesSourceConfig.fieldType().isSearchable()) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.missing() != null) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.script() != null) {\n+            return null;\n+        }\n+        // TODO bail here for runtime fields. They'll be slower this way. Maybe we can somehow look at the Query?\n+        if (valuesSourceConfig.fieldType() instanceof DateFieldType\n+            && ((DateFieldType) valuesSourceConfig.fieldType()).resolution() == Resolution.NANOSECONDS) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "117eb77edc372d3d707dca96dbcdf161e3c33faf"}, "originalPosition": 140}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDMzOTUzMA==", "bodyText": "Yeah.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510339530", "createdAt": "2020-10-22T17:34:20Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java", "diffHunk": "@@ -215,15 +235,207 @@ public boolean equals(Object obj) {\n         }\n     }\n \n-    final ValuesSource.Numeric valuesSource;\n-    final DocValueFormat format;\n-    final Range[] ranges;\n-    final boolean keyed;\n-    final InternalRange.Factory rangeFactory;\n+    /**\n+     * Build an {@link Aggregator} for a {@code range} aggregation. If the\n+     * {@code ranges} can be converted into filters then it builds a\n+     * {@link FiltersAggregator} and uses that to collect the results\n+     * <strong>if</strong> that aggregator can run in \"filter by filter\"\n+     * collection mode. If it can't then we'll collect the ranges using\n+     * a native {@link RangeAggregator} which is significantly faster\n+     * than the \"compatible\" collection mechanism for the filters agg.\n+     */\n+    public static Aggregator build(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSourceConfig valuesSourceConfig,\n+        InternalRange.Factory<?, ?> rangeFactory,\n+        Range[] ranges,\n+        boolean keyed,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        Aggregator adapted = adaptIntoFiltersOrNull(\n+            name,\n+            factories,\n+            valuesSourceConfig,\n+            rangeFactory,\n+            ranges,\n+            keyed,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+        if (adapted != null) {\n+            return adapted;\n+        }\n+        return buildWithoutAttemptedToAdaptToFilters(\n+            name,\n+            factories,\n+            (ValuesSource.Numeric) valuesSourceConfig.getValuesSource(),\n+            valuesSourceConfig.format(),\n+            rangeFactory,\n+            ranges,\n+            keyed,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+    }\n+\n+    public static Aggregator adaptIntoFiltersOrNull(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSourceConfig valuesSourceConfig,\n+        InternalRange.Factory<?, ?> rangeFactory,\n+        Range[] ranges,\n+        boolean keyed,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        if (valuesSourceConfig.fieldType() == null) {\n+            return null;\n+        }\n+        if (false == valuesSourceConfig.fieldType().isSearchable()) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.missing() != null) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.script() != null) {\n+            return null;\n+        }\n+        // TODO bail here for runtime fields. They'll be slower this way. Maybe we can somehow look at the Query?\n+        if (valuesSourceConfig.fieldType() instanceof DateFieldType\n+            && ((DateFieldType) valuesSourceConfig.fieldType()).resolution() == Resolution.NANOSECONDS) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDI2MzQ3Nw=="}, "originalCommit": {"oid": "117eb77edc372d3d707dca96dbcdf161e3c33faf"}, "originalPosition": 140}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5NjM3NjEyOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNTo0NTozM1rOHmoRkA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNTo0NTozM1rOHmoRkA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDI2Nzc5Mg==", "bodyText": "Because we've been talking about it all week, I know what 1 << 53 is doing here, but someone coming fresh to this code without the IEEE floating point spec on their mind probably won't know why that number is magic.  I'd suggest making a constant on this class LARGEST_PRECISE_DOUBLE or something like that", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510267792", "createdAt": "2020-10-22T15:45:33Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java", "diffHunk": "@@ -215,15 +235,207 @@ public boolean equals(Object obj) {\n         }\n     }\n \n-    final ValuesSource.Numeric valuesSource;\n-    final DocValueFormat format;\n-    final Range[] ranges;\n-    final boolean keyed;\n-    final InternalRange.Factory rangeFactory;\n+    /**\n+     * Build an {@link Aggregator} for a {@code range} aggregation. If the\n+     * {@code ranges} can be converted into filters then it builds a\n+     * {@link FiltersAggregator} and uses that to collect the results\n+     * <strong>if</strong> that aggregator can run in \"filter by filter\"\n+     * collection mode. If it can't then we'll collect the ranges using\n+     * a native {@link RangeAggregator} which is significantly faster\n+     * than the \"compatible\" collection mechanism for the filters agg.\n+     */\n+    public static Aggregator build(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSourceConfig valuesSourceConfig,\n+        InternalRange.Factory<?, ?> rangeFactory,\n+        Range[] ranges,\n+        boolean keyed,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        Aggregator adapted = adaptIntoFiltersOrNull(\n+            name,\n+            factories,\n+            valuesSourceConfig,\n+            rangeFactory,\n+            ranges,\n+            keyed,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+        if (adapted != null) {\n+            return adapted;\n+        }\n+        return buildWithoutAttemptedToAdaptToFilters(\n+            name,\n+            factories,\n+            (ValuesSource.Numeric) valuesSourceConfig.getValuesSource(),\n+            valuesSourceConfig.format(),\n+            rangeFactory,\n+            ranges,\n+            keyed,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+    }\n+\n+    public static Aggregator adaptIntoFiltersOrNull(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSourceConfig valuesSourceConfig,\n+        InternalRange.Factory<?, ?> rangeFactory,\n+        Range[] ranges,\n+        boolean keyed,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        if (valuesSourceConfig.fieldType() == null) {\n+            return null;\n+        }\n+        if (false == valuesSourceConfig.fieldType().isSearchable()) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.missing() != null) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.script() != null) {\n+            return null;\n+        }\n+        // TODO bail here for runtime fields. They'll be slower this way. Maybe we can somehow look at the Query?\n+        if (valuesSourceConfig.fieldType() instanceof DateFieldType\n+            && ((DateFieldType) valuesSourceConfig.fieldType()).resolution() == Resolution.NANOSECONDS) {\n+            // We don't generate sensible Queries for nanoseconds.\n+            return null;\n+        }\n+        boolean wholeNumbersOnly = false == ((ValuesSource.Numeric) valuesSourceConfig.getValuesSource()).isFloatingPoint();\n+        String[] keys = new String[ranges.length];\n+        Query[] filters = new Query[ranges.length];\n+        for (int i = 0; i < ranges.length; i++) {\n+            /*\n+             * If the bounds on the ranges are too high then the `double`s\n+             * that we work with will round differently in the native range\n+             * aggregator than in the filters aggregator. So we can't use\n+             * the filters. That is, if the input data type is a `long` in\n+             * the first place. If it isn't then \n+             */\n+            if (wholeNumbersOnly && ranges[i].from != Double.NEGATIVE_INFINITY && Math.abs(ranges[i].from) > 1L << 53) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "117eb77edc372d3d707dca96dbcdf161e3c33faf"}, "originalPosition": 155}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMzU0NTAwOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxMzoyMDo1NFrOHsEFKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxOTo1ODo1OVrOHuUSCQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTk2NjI1MQ==", "bodyText": "I think we don't need to create this weight here but do it lazy in the same way we are doing for singleValue.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r515966251", "createdAt": "2020-11-02T13:20:54Z", "author": {"login": "iverase"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java", "diffHunk": "@@ -0,0 +1,201 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.search.aggregations.bucket.filter;\n+\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.PointValues;\n+import org.apache.lucene.index.Term;\n+import org.apache.lucene.search.BooleanClause.Occur;\n+import org.apache.lucene.search.BooleanQuery;\n+import org.apache.lucene.search.BulkScorer;\n+import org.apache.lucene.search.Explanation;\n+import org.apache.lucene.search.IndexSearcher;\n+import org.apache.lucene.search.MatchNoDocsQuery;\n+import org.apache.lucene.search.PointRangeQuery;\n+import org.apache.lucene.search.Query;\n+import org.apache.lucene.search.ScoreMode;\n+import org.apache.lucene.search.Scorer;\n+import org.apache.lucene.search.ScorerSupplier;\n+import org.apache.lucene.search.Weight;\n+\n+import java.io.IOException;\n+import java.util.Set;\n+\n+import static java.util.Arrays.compareUnsigned;\n+\n+/**\n+ * Query merging two point in range queries.\n+ */\n+public class MergedPointRangeQuery extends Query {\n+    /**\n+     * Merge two {@linkplain PointRangeQuery}s into a {@linkplain MergedPointRangeQuery}\n+     * that matches points that match both filters.\n+     */\n+    public static Query merge(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        if (lhs.equals(rhs)) {\n+            // Lucky case! The queries were the same so their UNION is just the query itself.\n+            return lhs;\n+        }\n+        if (lhs.getField() != rhs.getField() || lhs.getNumDims() != rhs.getNumDims() || lhs.getBytesPerDim() != rhs.getBytesPerDim()) {\n+            return null;\n+        }\n+        return new MergedPointRangeQuery(lhs, rhs);\n+    }\n+\n+    private final String field;\n+    private final Query delegateForMultiValuedSegments;\n+    private final Query delegateForSingleValuedSegments;\n+\n+    private MergedPointRangeQuery(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        field = lhs.getField();\n+        delegateForMultiValuedSegments = new BooleanQuery.Builder().add(lhs, Occur.MUST).add(rhs, Occur.MUST).build();\n+        int numDims = lhs.getNumDims();\n+        int bytesPerDim = lhs.getBytesPerDim();\n+        this.delegateForSingleValuedSegments = pickDelegateForSingleValuedSegments(\n+            mergeBound(lhs.getLowerPoint(), rhs.getLowerPoint(), numDims, bytesPerDim, true),\n+            mergeBound(lhs.getUpperPoint(), rhs.getUpperPoint(), numDims, bytesPerDim, false),\n+            numDims,\n+            bytesPerDim\n+        );\n+    }\n+\n+    private Query pickDelegateForSingleValuedSegments(byte[] lower, byte[] upper, int numDims, int bytesPerDim) {\n+        // If we ended up with disjoint ranges in any dimension then on single valued segments we can't match any docs.\n+        for (int dim = 0; dim < numDims; dim++) {\n+            int offset = dim * bytesPerDim;\n+            if (compareUnsigned(lower, offset, offset + bytesPerDim, upper, offset, offset + bytesPerDim) > 0) {\n+                return new MatchNoDocsQuery(\"disjoint ranges\");\n+            }\n+        }\n+        // Otherwise on single valued segments we can only match docs the match the UNION of the two ranges.\n+        return new PointRangeQuery(field, lower, upper, numDims) {\n+            @Override\n+            protected String toString(int dimension, byte[] value) {\n+                // Stolen from Lucene's Binary range query. It'd be best to delegate, but the method isn't visible.\n+                StringBuilder sb = new StringBuilder();\n+                sb.append(\"(\");\n+                for (int i = 0; i < value.length; i++) {\n+                    if (i > 0) {\n+                        sb.append(' ');\n+                    }\n+                    sb.append(Integer.toHexString(value[i] & 0xFF));\n+                }\n+                sb.append(')');\n+                return sb.toString();\n+            }\n+        };\n+    }\n+\n+    @Override\n+    public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n+        Weight delegateForMultiValuedSegmentsWeight = delegateForMultiValuedSegments.createWeight(searcher, scoreMode, boost);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "66684f2983799e2430770b8a8daa95349f20ea7e"}, "originalPosition": 108}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODMyODg0MQ==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r518328841", "createdAt": "2020-11-05T19:58:59Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java", "diffHunk": "@@ -0,0 +1,201 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.search.aggregations.bucket.filter;\n+\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.PointValues;\n+import org.apache.lucene.index.Term;\n+import org.apache.lucene.search.BooleanClause.Occur;\n+import org.apache.lucene.search.BooleanQuery;\n+import org.apache.lucene.search.BulkScorer;\n+import org.apache.lucene.search.Explanation;\n+import org.apache.lucene.search.IndexSearcher;\n+import org.apache.lucene.search.MatchNoDocsQuery;\n+import org.apache.lucene.search.PointRangeQuery;\n+import org.apache.lucene.search.Query;\n+import org.apache.lucene.search.ScoreMode;\n+import org.apache.lucene.search.Scorer;\n+import org.apache.lucene.search.ScorerSupplier;\n+import org.apache.lucene.search.Weight;\n+\n+import java.io.IOException;\n+import java.util.Set;\n+\n+import static java.util.Arrays.compareUnsigned;\n+\n+/**\n+ * Query merging two point in range queries.\n+ */\n+public class MergedPointRangeQuery extends Query {\n+    /**\n+     * Merge two {@linkplain PointRangeQuery}s into a {@linkplain MergedPointRangeQuery}\n+     * that matches points that match both filters.\n+     */\n+    public static Query merge(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        if (lhs.equals(rhs)) {\n+            // Lucky case! The queries were the same so their UNION is just the query itself.\n+            return lhs;\n+        }\n+        if (lhs.getField() != rhs.getField() || lhs.getNumDims() != rhs.getNumDims() || lhs.getBytesPerDim() != rhs.getBytesPerDim()) {\n+            return null;\n+        }\n+        return new MergedPointRangeQuery(lhs, rhs);\n+    }\n+\n+    private final String field;\n+    private final Query delegateForMultiValuedSegments;\n+    private final Query delegateForSingleValuedSegments;\n+\n+    private MergedPointRangeQuery(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        field = lhs.getField();\n+        delegateForMultiValuedSegments = new BooleanQuery.Builder().add(lhs, Occur.MUST).add(rhs, Occur.MUST).build();\n+        int numDims = lhs.getNumDims();\n+        int bytesPerDim = lhs.getBytesPerDim();\n+        this.delegateForSingleValuedSegments = pickDelegateForSingleValuedSegments(\n+            mergeBound(lhs.getLowerPoint(), rhs.getLowerPoint(), numDims, bytesPerDim, true),\n+            mergeBound(lhs.getUpperPoint(), rhs.getUpperPoint(), numDims, bytesPerDim, false),\n+            numDims,\n+            bytesPerDim\n+        );\n+    }\n+\n+    private Query pickDelegateForSingleValuedSegments(byte[] lower, byte[] upper, int numDims, int bytesPerDim) {\n+        // If we ended up with disjoint ranges in any dimension then on single valued segments we can't match any docs.\n+        for (int dim = 0; dim < numDims; dim++) {\n+            int offset = dim * bytesPerDim;\n+            if (compareUnsigned(lower, offset, offset + bytesPerDim, upper, offset, offset + bytesPerDim) > 0) {\n+                return new MatchNoDocsQuery(\"disjoint ranges\");\n+            }\n+        }\n+        // Otherwise on single valued segments we can only match docs the match the UNION of the two ranges.\n+        return new PointRangeQuery(field, lower, upper, numDims) {\n+            @Override\n+            protected String toString(int dimension, byte[] value) {\n+                // Stolen from Lucene's Binary range query. It'd be best to delegate, but the method isn't visible.\n+                StringBuilder sb = new StringBuilder();\n+                sb.append(\"(\");\n+                for (int i = 0; i < value.length; i++) {\n+                    if (i > 0) {\n+                        sb.append(' ');\n+                    }\n+                    sb.append(Integer.toHexString(value[i] & 0xFF));\n+                }\n+                sb.append(')');\n+                return sb.toString();\n+            }\n+        };\n+    }\n+\n+    @Override\n+    public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n+        Weight delegateForMultiValuedSegmentsWeight = delegateForMultiValuedSegments.createWeight(searcher, scoreMode, boost);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTk2NjI1MQ=="}, "originalCommit": {"oid": "66684f2983799e2430770b8a8daa95349f20ea7e"}, "originalPosition": 108}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMzU0NjY2OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxMzoyMToyNFrOHsEGNg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxOTo1ODo1NFrOHuUR3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTk2NjUxOA==", "bodyText": "point queries always return true so we can probably just return true here?", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r515966518", "createdAt": "2020-11-02T13:21:24Z", "author": {"login": "iverase"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java", "diffHunk": "@@ -0,0 +1,201 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.search.aggregations.bucket.filter;\n+\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.PointValues;\n+import org.apache.lucene.index.Term;\n+import org.apache.lucene.search.BooleanClause.Occur;\n+import org.apache.lucene.search.BooleanQuery;\n+import org.apache.lucene.search.BulkScorer;\n+import org.apache.lucene.search.Explanation;\n+import org.apache.lucene.search.IndexSearcher;\n+import org.apache.lucene.search.MatchNoDocsQuery;\n+import org.apache.lucene.search.PointRangeQuery;\n+import org.apache.lucene.search.Query;\n+import org.apache.lucene.search.ScoreMode;\n+import org.apache.lucene.search.Scorer;\n+import org.apache.lucene.search.ScorerSupplier;\n+import org.apache.lucene.search.Weight;\n+\n+import java.io.IOException;\n+import java.util.Set;\n+\n+import static java.util.Arrays.compareUnsigned;\n+\n+/**\n+ * Query merging two point in range queries.\n+ */\n+public class MergedPointRangeQuery extends Query {\n+    /**\n+     * Merge two {@linkplain PointRangeQuery}s into a {@linkplain MergedPointRangeQuery}\n+     * that matches points that match both filters.\n+     */\n+    public static Query merge(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        if (lhs.equals(rhs)) {\n+            // Lucky case! The queries were the same so their UNION is just the query itself.\n+            return lhs;\n+        }\n+        if (lhs.getField() != rhs.getField() || lhs.getNumDims() != rhs.getNumDims() || lhs.getBytesPerDim() != rhs.getBytesPerDim()) {\n+            return null;\n+        }\n+        return new MergedPointRangeQuery(lhs, rhs);\n+    }\n+\n+    private final String field;\n+    private final Query delegateForMultiValuedSegments;\n+    private final Query delegateForSingleValuedSegments;\n+\n+    private MergedPointRangeQuery(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        field = lhs.getField();\n+        delegateForMultiValuedSegments = new BooleanQuery.Builder().add(lhs, Occur.MUST).add(rhs, Occur.MUST).build();\n+        int numDims = lhs.getNumDims();\n+        int bytesPerDim = lhs.getBytesPerDim();\n+        this.delegateForSingleValuedSegments = pickDelegateForSingleValuedSegments(\n+            mergeBound(lhs.getLowerPoint(), rhs.getLowerPoint(), numDims, bytesPerDim, true),\n+            mergeBound(lhs.getUpperPoint(), rhs.getUpperPoint(), numDims, bytesPerDim, false),\n+            numDims,\n+            bytesPerDim\n+        );\n+    }\n+\n+    private Query pickDelegateForSingleValuedSegments(byte[] lower, byte[] upper, int numDims, int bytesPerDim) {\n+        // If we ended up with disjoint ranges in any dimension then on single valued segments we can't match any docs.\n+        for (int dim = 0; dim < numDims; dim++) {\n+            int offset = dim * bytesPerDim;\n+            if (compareUnsigned(lower, offset, offset + bytesPerDim, upper, offset, offset + bytesPerDim) > 0) {\n+                return new MatchNoDocsQuery(\"disjoint ranges\");\n+            }\n+        }\n+        // Otherwise on single valued segments we can only match docs the match the UNION of the two ranges.\n+        return new PointRangeQuery(field, lower, upper, numDims) {\n+            @Override\n+            protected String toString(int dimension, byte[] value) {\n+                // Stolen from Lucene's Binary range query. It'd be best to delegate, but the method isn't visible.\n+                StringBuilder sb = new StringBuilder();\n+                sb.append(\"(\");\n+                for (int i = 0; i < value.length; i++) {\n+                    if (i > 0) {\n+                        sb.append(' ');\n+                    }\n+                    sb.append(Integer.toHexString(value[i] & 0xFF));\n+                }\n+                sb.append(')');\n+                return sb.toString();\n+            }\n+        };\n+    }\n+\n+    @Override\n+    public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n+        Weight delegateForMultiValuedSegmentsWeight = delegateForMultiValuedSegments.createWeight(searcher, scoreMode, boost);\n+        return new Weight(this) {\n+            Weight mostCompactWeight;\n+\n+            @Override\n+            public boolean isCacheable(LeafReaderContext ctx) {\n+                return delegateForMultiValuedSegmentsWeight.isCacheable(ctx);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "66684f2983799e2430770b8a8daa95349f20ea7e"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODMyODc5OQ==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r518328799", "createdAt": "2020-11-05T19:58:54Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java", "diffHunk": "@@ -0,0 +1,201 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.search.aggregations.bucket.filter;\n+\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.PointValues;\n+import org.apache.lucene.index.Term;\n+import org.apache.lucene.search.BooleanClause.Occur;\n+import org.apache.lucene.search.BooleanQuery;\n+import org.apache.lucene.search.BulkScorer;\n+import org.apache.lucene.search.Explanation;\n+import org.apache.lucene.search.IndexSearcher;\n+import org.apache.lucene.search.MatchNoDocsQuery;\n+import org.apache.lucene.search.PointRangeQuery;\n+import org.apache.lucene.search.Query;\n+import org.apache.lucene.search.ScoreMode;\n+import org.apache.lucene.search.Scorer;\n+import org.apache.lucene.search.ScorerSupplier;\n+import org.apache.lucene.search.Weight;\n+\n+import java.io.IOException;\n+import java.util.Set;\n+\n+import static java.util.Arrays.compareUnsigned;\n+\n+/**\n+ * Query merging two point in range queries.\n+ */\n+public class MergedPointRangeQuery extends Query {\n+    /**\n+     * Merge two {@linkplain PointRangeQuery}s into a {@linkplain MergedPointRangeQuery}\n+     * that matches points that match both filters.\n+     */\n+    public static Query merge(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        if (lhs.equals(rhs)) {\n+            // Lucky case! The queries were the same so their UNION is just the query itself.\n+            return lhs;\n+        }\n+        if (lhs.getField() != rhs.getField() || lhs.getNumDims() != rhs.getNumDims() || lhs.getBytesPerDim() != rhs.getBytesPerDim()) {\n+            return null;\n+        }\n+        return new MergedPointRangeQuery(lhs, rhs);\n+    }\n+\n+    private final String field;\n+    private final Query delegateForMultiValuedSegments;\n+    private final Query delegateForSingleValuedSegments;\n+\n+    private MergedPointRangeQuery(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        field = lhs.getField();\n+        delegateForMultiValuedSegments = new BooleanQuery.Builder().add(lhs, Occur.MUST).add(rhs, Occur.MUST).build();\n+        int numDims = lhs.getNumDims();\n+        int bytesPerDim = lhs.getBytesPerDim();\n+        this.delegateForSingleValuedSegments = pickDelegateForSingleValuedSegments(\n+            mergeBound(lhs.getLowerPoint(), rhs.getLowerPoint(), numDims, bytesPerDim, true),\n+            mergeBound(lhs.getUpperPoint(), rhs.getUpperPoint(), numDims, bytesPerDim, false),\n+            numDims,\n+            bytesPerDim\n+        );\n+    }\n+\n+    private Query pickDelegateForSingleValuedSegments(byte[] lower, byte[] upper, int numDims, int bytesPerDim) {\n+        // If we ended up with disjoint ranges in any dimension then on single valued segments we can't match any docs.\n+        for (int dim = 0; dim < numDims; dim++) {\n+            int offset = dim * bytesPerDim;\n+            if (compareUnsigned(lower, offset, offset + bytesPerDim, upper, offset, offset + bytesPerDim) > 0) {\n+                return new MatchNoDocsQuery(\"disjoint ranges\");\n+            }\n+        }\n+        // Otherwise on single valued segments we can only match docs the match the UNION of the two ranges.\n+        return new PointRangeQuery(field, lower, upper, numDims) {\n+            @Override\n+            protected String toString(int dimension, byte[] value) {\n+                // Stolen from Lucene's Binary range query. It'd be best to delegate, but the method isn't visible.\n+                StringBuilder sb = new StringBuilder();\n+                sb.append(\"(\");\n+                for (int i = 0; i < value.length; i++) {\n+                    if (i > 0) {\n+                        sb.append(' ');\n+                    }\n+                    sb.append(Integer.toHexString(value[i] & 0xFF));\n+                }\n+                sb.append(')');\n+                return sb.toString();\n+            }\n+        };\n+    }\n+\n+    @Override\n+    public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n+        Weight delegateForMultiValuedSegmentsWeight = delegateForMultiValuedSegments.createWeight(searcher, scoreMode, boost);\n+        return new Weight(this) {\n+            Weight mostCompactWeight;\n+\n+            @Override\n+            public boolean isCacheable(LeafReaderContext ctx) {\n+                return delegateForMultiValuedSegmentsWeight.isCacheable(ctx);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTk2NjUxOA=="}, "originalCommit": {"oid": "66684f2983799e2430770b8a8daa95349f20ea7e"}, "originalPosition": 114}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMzU0ODA5OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxMzoyMTo1MFrOHsEHHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxOTo1ODo1MFrOHuURzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTk2Njc0OQ==", "bodyText": "I think this can be safely a NOOP", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r515966749", "createdAt": "2020-11-02T13:21:50Z", "author": {"login": "iverase"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java", "diffHunk": "@@ -0,0 +1,201 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.search.aggregations.bucket.filter;\n+\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.PointValues;\n+import org.apache.lucene.index.Term;\n+import org.apache.lucene.search.BooleanClause.Occur;\n+import org.apache.lucene.search.BooleanQuery;\n+import org.apache.lucene.search.BulkScorer;\n+import org.apache.lucene.search.Explanation;\n+import org.apache.lucene.search.IndexSearcher;\n+import org.apache.lucene.search.MatchNoDocsQuery;\n+import org.apache.lucene.search.PointRangeQuery;\n+import org.apache.lucene.search.Query;\n+import org.apache.lucene.search.ScoreMode;\n+import org.apache.lucene.search.Scorer;\n+import org.apache.lucene.search.ScorerSupplier;\n+import org.apache.lucene.search.Weight;\n+\n+import java.io.IOException;\n+import java.util.Set;\n+\n+import static java.util.Arrays.compareUnsigned;\n+\n+/**\n+ * Query merging two point in range queries.\n+ */\n+public class MergedPointRangeQuery extends Query {\n+    /**\n+     * Merge two {@linkplain PointRangeQuery}s into a {@linkplain MergedPointRangeQuery}\n+     * that matches points that match both filters.\n+     */\n+    public static Query merge(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        if (lhs.equals(rhs)) {\n+            // Lucky case! The queries were the same so their UNION is just the query itself.\n+            return lhs;\n+        }\n+        if (lhs.getField() != rhs.getField() || lhs.getNumDims() != rhs.getNumDims() || lhs.getBytesPerDim() != rhs.getBytesPerDim()) {\n+            return null;\n+        }\n+        return new MergedPointRangeQuery(lhs, rhs);\n+    }\n+\n+    private final String field;\n+    private final Query delegateForMultiValuedSegments;\n+    private final Query delegateForSingleValuedSegments;\n+\n+    private MergedPointRangeQuery(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        field = lhs.getField();\n+        delegateForMultiValuedSegments = new BooleanQuery.Builder().add(lhs, Occur.MUST).add(rhs, Occur.MUST).build();\n+        int numDims = lhs.getNumDims();\n+        int bytesPerDim = lhs.getBytesPerDim();\n+        this.delegateForSingleValuedSegments = pickDelegateForSingleValuedSegments(\n+            mergeBound(lhs.getLowerPoint(), rhs.getLowerPoint(), numDims, bytesPerDim, true),\n+            mergeBound(lhs.getUpperPoint(), rhs.getUpperPoint(), numDims, bytesPerDim, false),\n+            numDims,\n+            bytesPerDim\n+        );\n+    }\n+\n+    private Query pickDelegateForSingleValuedSegments(byte[] lower, byte[] upper, int numDims, int bytesPerDim) {\n+        // If we ended up with disjoint ranges in any dimension then on single valued segments we can't match any docs.\n+        for (int dim = 0; dim < numDims; dim++) {\n+            int offset = dim * bytesPerDim;\n+            if (compareUnsigned(lower, offset, offset + bytesPerDim, upper, offset, offset + bytesPerDim) > 0) {\n+                return new MatchNoDocsQuery(\"disjoint ranges\");\n+            }\n+        }\n+        // Otherwise on single valued segments we can only match docs the match the UNION of the two ranges.\n+        return new PointRangeQuery(field, lower, upper, numDims) {\n+            @Override\n+            protected String toString(int dimension, byte[] value) {\n+                // Stolen from Lucene's Binary range query. It'd be best to delegate, but the method isn't visible.\n+                StringBuilder sb = new StringBuilder();\n+                sb.append(\"(\");\n+                for (int i = 0; i < value.length; i++) {\n+                    if (i > 0) {\n+                        sb.append(' ');\n+                    }\n+                    sb.append(Integer.toHexString(value[i] & 0xFF));\n+                }\n+                sb.append(')');\n+                return sb.toString();\n+            }\n+        };\n+    }\n+\n+    @Override\n+    public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n+        Weight delegateForMultiValuedSegmentsWeight = delegateForMultiValuedSegments.createWeight(searcher, scoreMode, boost);\n+        return new Weight(this) {\n+            Weight mostCompactWeight;\n+\n+            @Override\n+            public boolean isCacheable(LeafReaderContext ctx) {\n+                return delegateForMultiValuedSegmentsWeight.isCacheable(ctx);\n+            }\n+\n+            @Override\n+            public Scorer scorer(LeafReaderContext context) throws IOException {\n+                ScorerSupplier scorerSupplier = scorerSupplier(context);\n+                if (scorerSupplier == null) {\n+                    return null;\n+                }\n+                return scorerSupplier.get(Long.MAX_VALUE);\n+            }\n+\n+            @Override\n+            public ScorerSupplier scorerSupplier(LeafReaderContext context) throws IOException {\n+                /*\n+                 * If we're sure docs only have a single value for the field\n+                 * we can pick the most compact bounds. If there are multiple values\n+                 * for the field we have to run the boolean query.\n+                 */\n+                PointValues points = context.reader().getPointValues(field);\n+                if (points == null) {\n+                    return null;\n+                }\n+                if (points.size() == points.getDocCount()) {\n+                    // Each doc that has points has exactly one point.\n+                    if (mostCompactWeight == null) {\n+                        mostCompactWeight = delegateForSingleValuedSegments.createWeight(searcher, scoreMode, boost);\n+                    }\n+                    return mostCompactWeight.scorerSupplier(context);\n+                }\n+                return delegateForMultiValuedSegmentsWeight.scorerSupplier(context);\n+            }\n+\n+            @Override\n+            public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n+                return super.bulkScorer(context);\n+            }\n+\n+            @Override\n+            @Deprecated\n+            public void extractTerms(Set<Term> terms) {\n+                delegateForMultiValuedSegmentsWeight.extractTerms(terms);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "66684f2983799e2430770b8a8daa95349f20ea7e"}, "originalPosition": 155}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODMyODc4MA==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r518328780", "createdAt": "2020-11-05T19:58:50Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java", "diffHunk": "@@ -0,0 +1,201 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.search.aggregations.bucket.filter;\n+\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.PointValues;\n+import org.apache.lucene.index.Term;\n+import org.apache.lucene.search.BooleanClause.Occur;\n+import org.apache.lucene.search.BooleanQuery;\n+import org.apache.lucene.search.BulkScorer;\n+import org.apache.lucene.search.Explanation;\n+import org.apache.lucene.search.IndexSearcher;\n+import org.apache.lucene.search.MatchNoDocsQuery;\n+import org.apache.lucene.search.PointRangeQuery;\n+import org.apache.lucene.search.Query;\n+import org.apache.lucene.search.ScoreMode;\n+import org.apache.lucene.search.Scorer;\n+import org.apache.lucene.search.ScorerSupplier;\n+import org.apache.lucene.search.Weight;\n+\n+import java.io.IOException;\n+import java.util.Set;\n+\n+import static java.util.Arrays.compareUnsigned;\n+\n+/**\n+ * Query merging two point in range queries.\n+ */\n+public class MergedPointRangeQuery extends Query {\n+    /**\n+     * Merge two {@linkplain PointRangeQuery}s into a {@linkplain MergedPointRangeQuery}\n+     * that matches points that match both filters.\n+     */\n+    public static Query merge(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        if (lhs.equals(rhs)) {\n+            // Lucky case! The queries were the same so their UNION is just the query itself.\n+            return lhs;\n+        }\n+        if (lhs.getField() != rhs.getField() || lhs.getNumDims() != rhs.getNumDims() || lhs.getBytesPerDim() != rhs.getBytesPerDim()) {\n+            return null;\n+        }\n+        return new MergedPointRangeQuery(lhs, rhs);\n+    }\n+\n+    private final String field;\n+    private final Query delegateForMultiValuedSegments;\n+    private final Query delegateForSingleValuedSegments;\n+\n+    private MergedPointRangeQuery(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        field = lhs.getField();\n+        delegateForMultiValuedSegments = new BooleanQuery.Builder().add(lhs, Occur.MUST).add(rhs, Occur.MUST).build();\n+        int numDims = lhs.getNumDims();\n+        int bytesPerDim = lhs.getBytesPerDim();\n+        this.delegateForSingleValuedSegments = pickDelegateForSingleValuedSegments(\n+            mergeBound(lhs.getLowerPoint(), rhs.getLowerPoint(), numDims, bytesPerDim, true),\n+            mergeBound(lhs.getUpperPoint(), rhs.getUpperPoint(), numDims, bytesPerDim, false),\n+            numDims,\n+            bytesPerDim\n+        );\n+    }\n+\n+    private Query pickDelegateForSingleValuedSegments(byte[] lower, byte[] upper, int numDims, int bytesPerDim) {\n+        // If we ended up with disjoint ranges in any dimension then on single valued segments we can't match any docs.\n+        for (int dim = 0; dim < numDims; dim++) {\n+            int offset = dim * bytesPerDim;\n+            if (compareUnsigned(lower, offset, offset + bytesPerDim, upper, offset, offset + bytesPerDim) > 0) {\n+                return new MatchNoDocsQuery(\"disjoint ranges\");\n+            }\n+        }\n+        // Otherwise on single valued segments we can only match docs the match the UNION of the two ranges.\n+        return new PointRangeQuery(field, lower, upper, numDims) {\n+            @Override\n+            protected String toString(int dimension, byte[] value) {\n+                // Stolen from Lucene's Binary range query. It'd be best to delegate, but the method isn't visible.\n+                StringBuilder sb = new StringBuilder();\n+                sb.append(\"(\");\n+                for (int i = 0; i < value.length; i++) {\n+                    if (i > 0) {\n+                        sb.append(' ');\n+                    }\n+                    sb.append(Integer.toHexString(value[i] & 0xFF));\n+                }\n+                sb.append(')');\n+                return sb.toString();\n+            }\n+        };\n+    }\n+\n+    @Override\n+    public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n+        Weight delegateForMultiValuedSegmentsWeight = delegateForMultiValuedSegments.createWeight(searcher, scoreMode, boost);\n+        return new Weight(this) {\n+            Weight mostCompactWeight;\n+\n+            @Override\n+            public boolean isCacheable(LeafReaderContext ctx) {\n+                return delegateForMultiValuedSegmentsWeight.isCacheable(ctx);\n+            }\n+\n+            @Override\n+            public Scorer scorer(LeafReaderContext context) throws IOException {\n+                ScorerSupplier scorerSupplier = scorerSupplier(context);\n+                if (scorerSupplier == null) {\n+                    return null;\n+                }\n+                return scorerSupplier.get(Long.MAX_VALUE);\n+            }\n+\n+            @Override\n+            public ScorerSupplier scorerSupplier(LeafReaderContext context) throws IOException {\n+                /*\n+                 * If we're sure docs only have a single value for the field\n+                 * we can pick the most compact bounds. If there are multiple values\n+                 * for the field we have to run the boolean query.\n+                 */\n+                PointValues points = context.reader().getPointValues(field);\n+                if (points == null) {\n+                    return null;\n+                }\n+                if (points.size() == points.getDocCount()) {\n+                    // Each doc that has points has exactly one point.\n+                    if (mostCompactWeight == null) {\n+                        mostCompactWeight = delegateForSingleValuedSegments.createWeight(searcher, scoreMode, boost);\n+                    }\n+                    return mostCompactWeight.scorerSupplier(context);\n+                }\n+                return delegateForMultiValuedSegmentsWeight.scorerSupplier(context);\n+            }\n+\n+            @Override\n+            public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n+                return super.bulkScorer(context);\n+            }\n+\n+            @Override\n+            @Deprecated\n+            public void extractTerms(Set<Term> terms) {\n+                delegateForMultiValuedSegmentsWeight.extractTerms(terms);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTk2Njc0OQ=="}, "originalCommit": {"oid": "66684f2983799e2430770b8a8daa95349f20ea7e"}, "originalPosition": 155}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMzU1MTEwOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxMzoyMjozOFrOHsEI6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxNDowMDowN1rOHuvIBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTk2NzIxMQ==", "bodyText": "We can safely delegate this method to the default implementation?", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r515967211", "createdAt": "2020-11-02T13:22:38Z", "author": {"login": "iverase"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java", "diffHunk": "@@ -0,0 +1,201 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.search.aggregations.bucket.filter;\n+\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.PointValues;\n+import org.apache.lucene.index.Term;\n+import org.apache.lucene.search.BooleanClause.Occur;\n+import org.apache.lucene.search.BooleanQuery;\n+import org.apache.lucene.search.BulkScorer;\n+import org.apache.lucene.search.Explanation;\n+import org.apache.lucene.search.IndexSearcher;\n+import org.apache.lucene.search.MatchNoDocsQuery;\n+import org.apache.lucene.search.PointRangeQuery;\n+import org.apache.lucene.search.Query;\n+import org.apache.lucene.search.ScoreMode;\n+import org.apache.lucene.search.Scorer;\n+import org.apache.lucene.search.ScorerSupplier;\n+import org.apache.lucene.search.Weight;\n+\n+import java.io.IOException;\n+import java.util.Set;\n+\n+import static java.util.Arrays.compareUnsigned;\n+\n+/**\n+ * Query merging two point in range queries.\n+ */\n+public class MergedPointRangeQuery extends Query {\n+    /**\n+     * Merge two {@linkplain PointRangeQuery}s into a {@linkplain MergedPointRangeQuery}\n+     * that matches points that match both filters.\n+     */\n+    public static Query merge(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        if (lhs.equals(rhs)) {\n+            // Lucky case! The queries were the same so their UNION is just the query itself.\n+            return lhs;\n+        }\n+        if (lhs.getField() != rhs.getField() || lhs.getNumDims() != rhs.getNumDims() || lhs.getBytesPerDim() != rhs.getBytesPerDim()) {\n+            return null;\n+        }\n+        return new MergedPointRangeQuery(lhs, rhs);\n+    }\n+\n+    private final String field;\n+    private final Query delegateForMultiValuedSegments;\n+    private final Query delegateForSingleValuedSegments;\n+\n+    private MergedPointRangeQuery(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        field = lhs.getField();\n+        delegateForMultiValuedSegments = new BooleanQuery.Builder().add(lhs, Occur.MUST).add(rhs, Occur.MUST).build();\n+        int numDims = lhs.getNumDims();\n+        int bytesPerDim = lhs.getBytesPerDim();\n+        this.delegateForSingleValuedSegments = pickDelegateForSingleValuedSegments(\n+            mergeBound(lhs.getLowerPoint(), rhs.getLowerPoint(), numDims, bytesPerDim, true),\n+            mergeBound(lhs.getUpperPoint(), rhs.getUpperPoint(), numDims, bytesPerDim, false),\n+            numDims,\n+            bytesPerDim\n+        );\n+    }\n+\n+    private Query pickDelegateForSingleValuedSegments(byte[] lower, byte[] upper, int numDims, int bytesPerDim) {\n+        // If we ended up with disjoint ranges in any dimension then on single valued segments we can't match any docs.\n+        for (int dim = 0; dim < numDims; dim++) {\n+            int offset = dim * bytesPerDim;\n+            if (compareUnsigned(lower, offset, offset + bytesPerDim, upper, offset, offset + bytesPerDim) > 0) {\n+                return new MatchNoDocsQuery(\"disjoint ranges\");\n+            }\n+        }\n+        // Otherwise on single valued segments we can only match docs the match the UNION of the two ranges.\n+        return new PointRangeQuery(field, lower, upper, numDims) {\n+            @Override\n+            protected String toString(int dimension, byte[] value) {\n+                // Stolen from Lucene's Binary range query. It'd be best to delegate, but the method isn't visible.\n+                StringBuilder sb = new StringBuilder();\n+                sb.append(\"(\");\n+                for (int i = 0; i < value.length; i++) {\n+                    if (i > 0) {\n+                        sb.append(' ');\n+                    }\n+                    sb.append(Integer.toHexString(value[i] & 0xFF));\n+                }\n+                sb.append(')');\n+                return sb.toString();\n+            }\n+        };\n+    }\n+\n+    @Override\n+    public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n+        Weight delegateForMultiValuedSegmentsWeight = delegateForMultiValuedSegments.createWeight(searcher, scoreMode, boost);\n+        return new Weight(this) {\n+            Weight mostCompactWeight;\n+\n+            @Override\n+            public boolean isCacheable(LeafReaderContext ctx) {\n+                return delegateForMultiValuedSegmentsWeight.isCacheable(ctx);\n+            }\n+\n+            @Override\n+            public Scorer scorer(LeafReaderContext context) throws IOException {\n+                ScorerSupplier scorerSupplier = scorerSupplier(context);\n+                if (scorerSupplier == null) {\n+                    return null;\n+                }\n+                return scorerSupplier.get(Long.MAX_VALUE);\n+            }\n+\n+            @Override\n+            public ScorerSupplier scorerSupplier(LeafReaderContext context) throws IOException {\n+                /*\n+                 * If we're sure docs only have a single value for the field\n+                 * we can pick the most compact bounds. If there are multiple values\n+                 * for the field we have to run the boolean query.\n+                 */\n+                PointValues points = context.reader().getPointValues(field);\n+                if (points == null) {\n+                    return null;\n+                }\n+                if (points.size() == points.getDocCount()) {\n+                    // Each doc that has points has exactly one point.\n+                    if (mostCompactWeight == null) {\n+                        mostCompactWeight = delegateForSingleValuedSegments.createWeight(searcher, scoreMode, boost);\n+                    }\n+                    return mostCompactWeight.scorerSupplier(context);\n+                }\n+                return delegateForMultiValuedSegmentsWeight.scorerSupplier(context);\n+            }\n+\n+            @Override\n+            public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n+                return super.bulkScorer(context);\n+            }\n+\n+            @Override\n+            @Deprecated\n+            public void extractTerms(Set<Term> terms) {\n+                delegateForMultiValuedSegmentsWeight.extractTerms(terms);\n+            }\n+\n+            @Override\n+            public Explanation explain(LeafReaderContext context, int doc) throws IOException {\n+                return delegateForMultiValuedSegmentsWeight.explain(context, doc);\n+            }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "66684f2983799e2430770b8a8daa95349f20ea7e"}, "originalPosition": 161}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODMyNzE1OQ==", "bodyText": "Sorry, I don't understand which default implementation you mean.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r518327159", "createdAt": "2020-11-05T19:55:51Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java", "diffHunk": "@@ -0,0 +1,201 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.search.aggregations.bucket.filter;\n+\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.PointValues;\n+import org.apache.lucene.index.Term;\n+import org.apache.lucene.search.BooleanClause.Occur;\n+import org.apache.lucene.search.BooleanQuery;\n+import org.apache.lucene.search.BulkScorer;\n+import org.apache.lucene.search.Explanation;\n+import org.apache.lucene.search.IndexSearcher;\n+import org.apache.lucene.search.MatchNoDocsQuery;\n+import org.apache.lucene.search.PointRangeQuery;\n+import org.apache.lucene.search.Query;\n+import org.apache.lucene.search.ScoreMode;\n+import org.apache.lucene.search.Scorer;\n+import org.apache.lucene.search.ScorerSupplier;\n+import org.apache.lucene.search.Weight;\n+\n+import java.io.IOException;\n+import java.util.Set;\n+\n+import static java.util.Arrays.compareUnsigned;\n+\n+/**\n+ * Query merging two point in range queries.\n+ */\n+public class MergedPointRangeQuery extends Query {\n+    /**\n+     * Merge two {@linkplain PointRangeQuery}s into a {@linkplain MergedPointRangeQuery}\n+     * that matches points that match both filters.\n+     */\n+    public static Query merge(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        if (lhs.equals(rhs)) {\n+            // Lucky case! The queries were the same so their UNION is just the query itself.\n+            return lhs;\n+        }\n+        if (lhs.getField() != rhs.getField() || lhs.getNumDims() != rhs.getNumDims() || lhs.getBytesPerDim() != rhs.getBytesPerDim()) {\n+            return null;\n+        }\n+        return new MergedPointRangeQuery(lhs, rhs);\n+    }\n+\n+    private final String field;\n+    private final Query delegateForMultiValuedSegments;\n+    private final Query delegateForSingleValuedSegments;\n+\n+    private MergedPointRangeQuery(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        field = lhs.getField();\n+        delegateForMultiValuedSegments = new BooleanQuery.Builder().add(lhs, Occur.MUST).add(rhs, Occur.MUST).build();\n+        int numDims = lhs.getNumDims();\n+        int bytesPerDim = lhs.getBytesPerDim();\n+        this.delegateForSingleValuedSegments = pickDelegateForSingleValuedSegments(\n+            mergeBound(lhs.getLowerPoint(), rhs.getLowerPoint(), numDims, bytesPerDim, true),\n+            mergeBound(lhs.getUpperPoint(), rhs.getUpperPoint(), numDims, bytesPerDim, false),\n+            numDims,\n+            bytesPerDim\n+        );\n+    }\n+\n+    private Query pickDelegateForSingleValuedSegments(byte[] lower, byte[] upper, int numDims, int bytesPerDim) {\n+        // If we ended up with disjoint ranges in any dimension then on single valued segments we can't match any docs.\n+        for (int dim = 0; dim < numDims; dim++) {\n+            int offset = dim * bytesPerDim;\n+            if (compareUnsigned(lower, offset, offset + bytesPerDim, upper, offset, offset + bytesPerDim) > 0) {\n+                return new MatchNoDocsQuery(\"disjoint ranges\");\n+            }\n+        }\n+        // Otherwise on single valued segments we can only match docs the match the UNION of the two ranges.\n+        return new PointRangeQuery(field, lower, upper, numDims) {\n+            @Override\n+            protected String toString(int dimension, byte[] value) {\n+                // Stolen from Lucene's Binary range query. It'd be best to delegate, but the method isn't visible.\n+                StringBuilder sb = new StringBuilder();\n+                sb.append(\"(\");\n+                for (int i = 0; i < value.length; i++) {\n+                    if (i > 0) {\n+                        sb.append(' ');\n+                    }\n+                    sb.append(Integer.toHexString(value[i] & 0xFF));\n+                }\n+                sb.append(')');\n+                return sb.toString();\n+            }\n+        };\n+    }\n+\n+    @Override\n+    public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n+        Weight delegateForMultiValuedSegmentsWeight = delegateForMultiValuedSegments.createWeight(searcher, scoreMode, boost);\n+        return new Weight(this) {\n+            Weight mostCompactWeight;\n+\n+            @Override\n+            public boolean isCacheable(LeafReaderContext ctx) {\n+                return delegateForMultiValuedSegmentsWeight.isCacheable(ctx);\n+            }\n+\n+            @Override\n+            public Scorer scorer(LeafReaderContext context) throws IOException {\n+                ScorerSupplier scorerSupplier = scorerSupplier(context);\n+                if (scorerSupplier == null) {\n+                    return null;\n+                }\n+                return scorerSupplier.get(Long.MAX_VALUE);\n+            }\n+\n+            @Override\n+            public ScorerSupplier scorerSupplier(LeafReaderContext context) throws IOException {\n+                /*\n+                 * If we're sure docs only have a single value for the field\n+                 * we can pick the most compact bounds. If there are multiple values\n+                 * for the field we have to run the boolean query.\n+                 */\n+                PointValues points = context.reader().getPointValues(field);\n+                if (points == null) {\n+                    return null;\n+                }\n+                if (points.size() == points.getDocCount()) {\n+                    // Each doc that has points has exactly one point.\n+                    if (mostCompactWeight == null) {\n+                        mostCompactWeight = delegateForSingleValuedSegments.createWeight(searcher, scoreMode, boost);\n+                    }\n+                    return mostCompactWeight.scorerSupplier(context);\n+                }\n+                return delegateForMultiValuedSegmentsWeight.scorerSupplier(context);\n+            }\n+\n+            @Override\n+            public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n+                return super.bulkScorer(context);\n+            }\n+\n+            @Override\n+            @Deprecated\n+            public void extractTerms(Set<Term> terms) {\n+                delegateForMultiValuedSegmentsWeight.extractTerms(terms);\n+            }\n+\n+            @Override\n+            public Explanation explain(LeafReaderContext context, int doc) throws IOException {\n+                return delegateForMultiValuedSegmentsWeight.explain(context, doc);\n+            }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTk2NzIxMQ=="}, "originalCommit": {"oid": "66684f2983799e2430770b8a8daa95349f20ea7e"}, "originalPosition": 161}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODc1Mjk2MQ==", "bodyText": "I meant that the parent class has already an implementation and we do not need to override it. In any case I think this method is only use when called explain and in this case, it will never be used?", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r518752961", "createdAt": "2020-11-06T13:33:04Z", "author": {"login": "iverase"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java", "diffHunk": "@@ -0,0 +1,201 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.search.aggregations.bucket.filter;\n+\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.PointValues;\n+import org.apache.lucene.index.Term;\n+import org.apache.lucene.search.BooleanClause.Occur;\n+import org.apache.lucene.search.BooleanQuery;\n+import org.apache.lucene.search.BulkScorer;\n+import org.apache.lucene.search.Explanation;\n+import org.apache.lucene.search.IndexSearcher;\n+import org.apache.lucene.search.MatchNoDocsQuery;\n+import org.apache.lucene.search.PointRangeQuery;\n+import org.apache.lucene.search.Query;\n+import org.apache.lucene.search.ScoreMode;\n+import org.apache.lucene.search.Scorer;\n+import org.apache.lucene.search.ScorerSupplier;\n+import org.apache.lucene.search.Weight;\n+\n+import java.io.IOException;\n+import java.util.Set;\n+\n+import static java.util.Arrays.compareUnsigned;\n+\n+/**\n+ * Query merging two point in range queries.\n+ */\n+public class MergedPointRangeQuery extends Query {\n+    /**\n+     * Merge two {@linkplain PointRangeQuery}s into a {@linkplain MergedPointRangeQuery}\n+     * that matches points that match both filters.\n+     */\n+    public static Query merge(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        if (lhs.equals(rhs)) {\n+            // Lucky case! The queries were the same so their UNION is just the query itself.\n+            return lhs;\n+        }\n+        if (lhs.getField() != rhs.getField() || lhs.getNumDims() != rhs.getNumDims() || lhs.getBytesPerDim() != rhs.getBytesPerDim()) {\n+            return null;\n+        }\n+        return new MergedPointRangeQuery(lhs, rhs);\n+    }\n+\n+    private final String field;\n+    private final Query delegateForMultiValuedSegments;\n+    private final Query delegateForSingleValuedSegments;\n+\n+    private MergedPointRangeQuery(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        field = lhs.getField();\n+        delegateForMultiValuedSegments = new BooleanQuery.Builder().add(lhs, Occur.MUST).add(rhs, Occur.MUST).build();\n+        int numDims = lhs.getNumDims();\n+        int bytesPerDim = lhs.getBytesPerDim();\n+        this.delegateForSingleValuedSegments = pickDelegateForSingleValuedSegments(\n+            mergeBound(lhs.getLowerPoint(), rhs.getLowerPoint(), numDims, bytesPerDim, true),\n+            mergeBound(lhs.getUpperPoint(), rhs.getUpperPoint(), numDims, bytesPerDim, false),\n+            numDims,\n+            bytesPerDim\n+        );\n+    }\n+\n+    private Query pickDelegateForSingleValuedSegments(byte[] lower, byte[] upper, int numDims, int bytesPerDim) {\n+        // If we ended up with disjoint ranges in any dimension then on single valued segments we can't match any docs.\n+        for (int dim = 0; dim < numDims; dim++) {\n+            int offset = dim * bytesPerDim;\n+            if (compareUnsigned(lower, offset, offset + bytesPerDim, upper, offset, offset + bytesPerDim) > 0) {\n+                return new MatchNoDocsQuery(\"disjoint ranges\");\n+            }\n+        }\n+        // Otherwise on single valued segments we can only match docs the match the UNION of the two ranges.\n+        return new PointRangeQuery(field, lower, upper, numDims) {\n+            @Override\n+            protected String toString(int dimension, byte[] value) {\n+                // Stolen from Lucene's Binary range query. It'd be best to delegate, but the method isn't visible.\n+                StringBuilder sb = new StringBuilder();\n+                sb.append(\"(\");\n+                for (int i = 0; i < value.length; i++) {\n+                    if (i > 0) {\n+                        sb.append(' ');\n+                    }\n+                    sb.append(Integer.toHexString(value[i] & 0xFF));\n+                }\n+                sb.append(')');\n+                return sb.toString();\n+            }\n+        };\n+    }\n+\n+    @Override\n+    public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n+        Weight delegateForMultiValuedSegmentsWeight = delegateForMultiValuedSegments.createWeight(searcher, scoreMode, boost);\n+        return new Weight(this) {\n+            Weight mostCompactWeight;\n+\n+            @Override\n+            public boolean isCacheable(LeafReaderContext ctx) {\n+                return delegateForMultiValuedSegmentsWeight.isCacheable(ctx);\n+            }\n+\n+            @Override\n+            public Scorer scorer(LeafReaderContext context) throws IOException {\n+                ScorerSupplier scorerSupplier = scorerSupplier(context);\n+                if (scorerSupplier == null) {\n+                    return null;\n+                }\n+                return scorerSupplier.get(Long.MAX_VALUE);\n+            }\n+\n+            @Override\n+            public ScorerSupplier scorerSupplier(LeafReaderContext context) throws IOException {\n+                /*\n+                 * If we're sure docs only have a single value for the field\n+                 * we can pick the most compact bounds. If there are multiple values\n+                 * for the field we have to run the boolean query.\n+                 */\n+                PointValues points = context.reader().getPointValues(field);\n+                if (points == null) {\n+                    return null;\n+                }\n+                if (points.size() == points.getDocCount()) {\n+                    // Each doc that has points has exactly one point.\n+                    if (mostCompactWeight == null) {\n+                        mostCompactWeight = delegateForSingleValuedSegments.createWeight(searcher, scoreMode, boost);\n+                    }\n+                    return mostCompactWeight.scorerSupplier(context);\n+                }\n+                return delegateForMultiValuedSegmentsWeight.scorerSupplier(context);\n+            }\n+\n+            @Override\n+            public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n+                return super.bulkScorer(context);\n+            }\n+\n+            @Override\n+            @Deprecated\n+            public void extractTerms(Set<Term> terms) {\n+                delegateForMultiValuedSegmentsWeight.extractTerms(terms);\n+            }\n+\n+            @Override\n+            public Explanation explain(LeafReaderContext context, int doc) throws IOException {\n+                return delegateForMultiValuedSegmentsWeight.explain(context, doc);\n+            }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTk2NzIxMQ=="}, "originalCommit": {"oid": "66684f2983799e2430770b8a8daa95349f20ea7e"}, "originalPosition": 161}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODc2ODY0NA==", "bodyText": "I talked to @iverase and he thought I was using ConstantScoreWeight. I probably should have been using it. So I am now.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r518768644", "createdAt": "2020-11-06T14:00:07Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java", "diffHunk": "@@ -0,0 +1,201 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.search.aggregations.bucket.filter;\n+\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.PointValues;\n+import org.apache.lucene.index.Term;\n+import org.apache.lucene.search.BooleanClause.Occur;\n+import org.apache.lucene.search.BooleanQuery;\n+import org.apache.lucene.search.BulkScorer;\n+import org.apache.lucene.search.Explanation;\n+import org.apache.lucene.search.IndexSearcher;\n+import org.apache.lucene.search.MatchNoDocsQuery;\n+import org.apache.lucene.search.PointRangeQuery;\n+import org.apache.lucene.search.Query;\n+import org.apache.lucene.search.ScoreMode;\n+import org.apache.lucene.search.Scorer;\n+import org.apache.lucene.search.ScorerSupplier;\n+import org.apache.lucene.search.Weight;\n+\n+import java.io.IOException;\n+import java.util.Set;\n+\n+import static java.util.Arrays.compareUnsigned;\n+\n+/**\n+ * Query merging two point in range queries.\n+ */\n+public class MergedPointRangeQuery extends Query {\n+    /**\n+     * Merge two {@linkplain PointRangeQuery}s into a {@linkplain MergedPointRangeQuery}\n+     * that matches points that match both filters.\n+     */\n+    public static Query merge(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        if (lhs.equals(rhs)) {\n+            // Lucky case! The queries were the same so their UNION is just the query itself.\n+            return lhs;\n+        }\n+        if (lhs.getField() != rhs.getField() || lhs.getNumDims() != rhs.getNumDims() || lhs.getBytesPerDim() != rhs.getBytesPerDim()) {\n+            return null;\n+        }\n+        return new MergedPointRangeQuery(lhs, rhs);\n+    }\n+\n+    private final String field;\n+    private final Query delegateForMultiValuedSegments;\n+    private final Query delegateForSingleValuedSegments;\n+\n+    private MergedPointRangeQuery(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        field = lhs.getField();\n+        delegateForMultiValuedSegments = new BooleanQuery.Builder().add(lhs, Occur.MUST).add(rhs, Occur.MUST).build();\n+        int numDims = lhs.getNumDims();\n+        int bytesPerDim = lhs.getBytesPerDim();\n+        this.delegateForSingleValuedSegments = pickDelegateForSingleValuedSegments(\n+            mergeBound(lhs.getLowerPoint(), rhs.getLowerPoint(), numDims, bytesPerDim, true),\n+            mergeBound(lhs.getUpperPoint(), rhs.getUpperPoint(), numDims, bytesPerDim, false),\n+            numDims,\n+            bytesPerDim\n+        );\n+    }\n+\n+    private Query pickDelegateForSingleValuedSegments(byte[] lower, byte[] upper, int numDims, int bytesPerDim) {\n+        // If we ended up with disjoint ranges in any dimension then on single valued segments we can't match any docs.\n+        for (int dim = 0; dim < numDims; dim++) {\n+            int offset = dim * bytesPerDim;\n+            if (compareUnsigned(lower, offset, offset + bytesPerDim, upper, offset, offset + bytesPerDim) > 0) {\n+                return new MatchNoDocsQuery(\"disjoint ranges\");\n+            }\n+        }\n+        // Otherwise on single valued segments we can only match docs the match the UNION of the two ranges.\n+        return new PointRangeQuery(field, lower, upper, numDims) {\n+            @Override\n+            protected String toString(int dimension, byte[] value) {\n+                // Stolen from Lucene's Binary range query. It'd be best to delegate, but the method isn't visible.\n+                StringBuilder sb = new StringBuilder();\n+                sb.append(\"(\");\n+                for (int i = 0; i < value.length; i++) {\n+                    if (i > 0) {\n+                        sb.append(' ');\n+                    }\n+                    sb.append(Integer.toHexString(value[i] & 0xFF));\n+                }\n+                sb.append(')');\n+                return sb.toString();\n+            }\n+        };\n+    }\n+\n+    @Override\n+    public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n+        Weight delegateForMultiValuedSegmentsWeight = delegateForMultiValuedSegments.createWeight(searcher, scoreMode, boost);\n+        return new Weight(this) {\n+            Weight mostCompactWeight;\n+\n+            @Override\n+            public boolean isCacheable(LeafReaderContext ctx) {\n+                return delegateForMultiValuedSegmentsWeight.isCacheable(ctx);\n+            }\n+\n+            @Override\n+            public Scorer scorer(LeafReaderContext context) throws IOException {\n+                ScorerSupplier scorerSupplier = scorerSupplier(context);\n+                if (scorerSupplier == null) {\n+                    return null;\n+                }\n+                return scorerSupplier.get(Long.MAX_VALUE);\n+            }\n+\n+            @Override\n+            public ScorerSupplier scorerSupplier(LeafReaderContext context) throws IOException {\n+                /*\n+                 * If we're sure docs only have a single value for the field\n+                 * we can pick the most compact bounds. If there are multiple values\n+                 * for the field we have to run the boolean query.\n+                 */\n+                PointValues points = context.reader().getPointValues(field);\n+                if (points == null) {\n+                    return null;\n+                }\n+                if (points.size() == points.getDocCount()) {\n+                    // Each doc that has points has exactly one point.\n+                    if (mostCompactWeight == null) {\n+                        mostCompactWeight = delegateForSingleValuedSegments.createWeight(searcher, scoreMode, boost);\n+                    }\n+                    return mostCompactWeight.scorerSupplier(context);\n+                }\n+                return delegateForMultiValuedSegmentsWeight.scorerSupplier(context);\n+            }\n+\n+            @Override\n+            public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n+                return super.bulkScorer(context);\n+            }\n+\n+            @Override\n+            @Deprecated\n+            public void extractTerms(Set<Term> terms) {\n+                delegateForMultiValuedSegmentsWeight.extractTerms(terms);\n+            }\n+\n+            @Override\n+            public Explanation explain(LeafReaderContext context, int doc) throws IOException {\n+                return delegateForMultiValuedSegmentsWeight.explain(context, doc);\n+            }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTk2NzIxMQ=="}, "originalCommit": {"oid": "66684f2983799e2430770b8a8daa95349f20ea7e"}, "originalPosition": 161}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1NzQ0MTUzOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwODozMDo0OVrOHvjrVw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxNToxOTo0OFrOHvzs5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTYyOTY1NQ==", "bodyText": "I think this is incomplete, we should check delegateForSingleValuedSegments as well?", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r519629655", "createdAt": "2020-11-09T08:30:49Z", "author": {"login": "iverase"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java", "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.search.aggregations.bucket.filter;\n+\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.PointValues;\n+import org.apache.lucene.search.BooleanClause.Occur;\n+import org.apache.lucene.search.BooleanQuery;\n+import org.apache.lucene.search.BulkScorer;\n+import org.apache.lucene.search.ConstantScoreWeight;\n+import org.apache.lucene.search.IndexSearcher;\n+import org.apache.lucene.search.MatchNoDocsQuery;\n+import org.apache.lucene.search.PointRangeQuery;\n+import org.apache.lucene.search.Query;\n+import org.apache.lucene.search.ScoreMode;\n+import org.apache.lucene.search.Scorer;\n+import org.apache.lucene.search.ScorerSupplier;\n+import org.apache.lucene.search.Weight;\n+\n+import java.io.IOException;\n+\n+import static java.util.Arrays.compareUnsigned;\n+\n+/**\n+ * Query merging two point in range queries.\n+ */\n+public class MergedPointRangeQuery extends Query {\n+    /**\n+     * Merge two {@linkplain PointRangeQuery}s into a {@linkplain MergedPointRangeQuery}\n+     * that matches points that match both filters.\n+     */\n+    public static Query merge(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        if (lhs.equals(rhs)) {\n+            // Lucky case! The queries were the same so their UNION is just the query itself.\n+            return lhs;\n+        }\n+        if (lhs.getField() != rhs.getField() || lhs.getNumDims() != rhs.getNumDims() || lhs.getBytesPerDim() != rhs.getBytesPerDim()) {\n+            return null;\n+        }\n+        return new MergedPointRangeQuery(lhs, rhs);\n+    }\n+\n+    private final String field;\n+    private final Query delegateForMultiValuedSegments;\n+    private final Query delegateForSingleValuedSegments;\n+\n+    private MergedPointRangeQuery(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        field = lhs.getField();\n+        delegateForMultiValuedSegments = new BooleanQuery.Builder().add(lhs, Occur.MUST).add(rhs, Occur.MUST).build();\n+        int numDims = lhs.getNumDims();\n+        int bytesPerDim = lhs.getBytesPerDim();\n+        this.delegateForSingleValuedSegments = pickDelegateForSingleValuedSegments(\n+            mergeBound(lhs.getLowerPoint(), rhs.getLowerPoint(), numDims, bytesPerDim, true),\n+            mergeBound(lhs.getUpperPoint(), rhs.getUpperPoint(), numDims, bytesPerDim, false),\n+            numDims,\n+            bytesPerDim\n+        );\n+    }\n+\n+    private Query pickDelegateForSingleValuedSegments(byte[] lower, byte[] upper, int numDims, int bytesPerDim) {\n+        // If we ended up with disjoint ranges in any dimension then on single valued segments we can't match any docs.\n+        for (int dim = 0; dim < numDims; dim++) {\n+            int offset = dim * bytesPerDim;\n+            if (compareUnsigned(lower, offset, offset + bytesPerDim, upper, offset, offset + bytesPerDim) > 0) {\n+                return new MatchNoDocsQuery(\"disjoint ranges\");\n+            }\n+        }\n+        // Otherwise on single valued segments we can only match docs the match the UNION of the two ranges.\n+        return new PointRangeQuery(field, lower, upper, numDims) {\n+            @Override\n+            protected String toString(int dimension, byte[] value) {\n+                // Stolen from Lucene's Binary range query. It'd be best to delegate, but the method isn't visible.\n+                StringBuilder sb = new StringBuilder();\n+                sb.append(\"(\");\n+                for (int i = 0; i < value.length; i++) {\n+                    if (i > 0) {\n+                        sb.append(' ');\n+                    }\n+                    sb.append(Integer.toHexString(value[i] & 0xFF));\n+                }\n+                sb.append(')');\n+                return sb.toString();\n+            }\n+        };\n+    }\n+\n+    @Override\n+    public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n+        return new ConstantScoreWeight(this, boost) {\n+            Weight multiValuedSegmentWeight;\n+            Weight singleValuedSegmentWeight;\n+\n+            @Override\n+            public boolean isCacheable(LeafReaderContext ctx) {\n+                return true;\n+            }\n+\n+            @Override\n+            public Scorer scorer(LeafReaderContext context) throws IOException {\n+                ScorerSupplier scorerSupplier = scorerSupplier(context);\n+                if (scorerSupplier == null) {\n+                    return null;\n+                }\n+                return scorerSupplier.get(Long.MAX_VALUE);\n+            }\n+\n+            @Override\n+            public ScorerSupplier scorerSupplier(LeafReaderContext context) throws IOException {\n+                /*\n+                 * If we're sure docs only have a single value for the field\n+                 * we can pick the most compact bounds. If there are multiple values\n+                 * for the field we have to run the boolean query.\n+                 */\n+                PointValues points = context.reader().getPointValues(field);\n+                if (points == null) {\n+                    return null;\n+                }\n+                if (points.size() == points.getDocCount()) {\n+                    // Each doc that has points has exactly one point.\n+                    return singleValuedSegmentWeight().scorerSupplier(context);\n+                }\n+                return multiValuedSegmentWeight().scorerSupplier(context);\n+            }\n+\n+            @Override\n+            public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n+                PointValues points = context.reader().getPointValues(field);\n+                if (points == null) {\n+                    return null;\n+                }\n+                if (points.size() == points.getDocCount()) {\n+                    // Each doc that has points has exactly one point.\n+                    return singleValuedSegmentWeight().bulkScorer(context);\n+                }\n+                return multiValuedSegmentWeight().bulkScorer(context);\n+            }\n+\n+            private Weight singleValuedSegmentWeight() throws IOException {\n+                if (singleValuedSegmentWeight == null) {\n+                    singleValuedSegmentWeight = delegateForSingleValuedSegments.createWeight(searcher, scoreMode, boost);\n+                }\n+                return singleValuedSegmentWeight;\n+            }\n+\n+            private Weight multiValuedSegmentWeight() throws IOException {\n+                if (multiValuedSegmentWeight == null) {\n+                    multiValuedSegmentWeight = delegateForMultiValuedSegments.createWeight(searcher, scoreMode, boost);\n+                }\n+                return multiValuedSegmentWeight;\n+            }\n+        };\n+    }\n+\n+    /**\n+     * The query used when we have single valued segments.\n+     */\n+    Query delegateForSingleValuedSegments() {\n+        return delegateForSingleValuedSegments;\n+    }\n+\n+    @Override\n+    public String toString(String field) {\n+        return \"MergedPointRange[\" + delegateForMultiValuedSegments.toString(field) + \"]\";\n+    }\n+\n+    @Override\n+    public boolean equals(Object obj) {\n+        if (obj == null || obj.getClass() != getClass()) {\n+            return false;\n+        }\n+        MergedPointRangeQuery other = (MergedPointRangeQuery) obj;\n+        return delegateForMultiValuedSegments.equals(other.delegateForMultiValuedSegments);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca09463ad6846a4579d477713a9d26e54f27bf07"}, "originalPosition": 189}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTg5MjE5OQ==", "bodyText": "They depend on one another so I don't think we technically have to, but I agree, we may as well.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r519892199", "createdAt": "2020-11-09T15:19:48Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java", "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.search.aggregations.bucket.filter;\n+\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.PointValues;\n+import org.apache.lucene.search.BooleanClause.Occur;\n+import org.apache.lucene.search.BooleanQuery;\n+import org.apache.lucene.search.BulkScorer;\n+import org.apache.lucene.search.ConstantScoreWeight;\n+import org.apache.lucene.search.IndexSearcher;\n+import org.apache.lucene.search.MatchNoDocsQuery;\n+import org.apache.lucene.search.PointRangeQuery;\n+import org.apache.lucene.search.Query;\n+import org.apache.lucene.search.ScoreMode;\n+import org.apache.lucene.search.Scorer;\n+import org.apache.lucene.search.ScorerSupplier;\n+import org.apache.lucene.search.Weight;\n+\n+import java.io.IOException;\n+\n+import static java.util.Arrays.compareUnsigned;\n+\n+/**\n+ * Query merging two point in range queries.\n+ */\n+public class MergedPointRangeQuery extends Query {\n+    /**\n+     * Merge two {@linkplain PointRangeQuery}s into a {@linkplain MergedPointRangeQuery}\n+     * that matches points that match both filters.\n+     */\n+    public static Query merge(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        if (lhs.equals(rhs)) {\n+            // Lucky case! The queries were the same so their UNION is just the query itself.\n+            return lhs;\n+        }\n+        if (lhs.getField() != rhs.getField() || lhs.getNumDims() != rhs.getNumDims() || lhs.getBytesPerDim() != rhs.getBytesPerDim()) {\n+            return null;\n+        }\n+        return new MergedPointRangeQuery(lhs, rhs);\n+    }\n+\n+    private final String field;\n+    private final Query delegateForMultiValuedSegments;\n+    private final Query delegateForSingleValuedSegments;\n+\n+    private MergedPointRangeQuery(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        field = lhs.getField();\n+        delegateForMultiValuedSegments = new BooleanQuery.Builder().add(lhs, Occur.MUST).add(rhs, Occur.MUST).build();\n+        int numDims = lhs.getNumDims();\n+        int bytesPerDim = lhs.getBytesPerDim();\n+        this.delegateForSingleValuedSegments = pickDelegateForSingleValuedSegments(\n+            mergeBound(lhs.getLowerPoint(), rhs.getLowerPoint(), numDims, bytesPerDim, true),\n+            mergeBound(lhs.getUpperPoint(), rhs.getUpperPoint(), numDims, bytesPerDim, false),\n+            numDims,\n+            bytesPerDim\n+        );\n+    }\n+\n+    private Query pickDelegateForSingleValuedSegments(byte[] lower, byte[] upper, int numDims, int bytesPerDim) {\n+        // If we ended up with disjoint ranges in any dimension then on single valued segments we can't match any docs.\n+        for (int dim = 0; dim < numDims; dim++) {\n+            int offset = dim * bytesPerDim;\n+            if (compareUnsigned(lower, offset, offset + bytesPerDim, upper, offset, offset + bytesPerDim) > 0) {\n+                return new MatchNoDocsQuery(\"disjoint ranges\");\n+            }\n+        }\n+        // Otherwise on single valued segments we can only match docs the match the UNION of the two ranges.\n+        return new PointRangeQuery(field, lower, upper, numDims) {\n+            @Override\n+            protected String toString(int dimension, byte[] value) {\n+                // Stolen from Lucene's Binary range query. It'd be best to delegate, but the method isn't visible.\n+                StringBuilder sb = new StringBuilder();\n+                sb.append(\"(\");\n+                for (int i = 0; i < value.length; i++) {\n+                    if (i > 0) {\n+                        sb.append(' ');\n+                    }\n+                    sb.append(Integer.toHexString(value[i] & 0xFF));\n+                }\n+                sb.append(')');\n+                return sb.toString();\n+            }\n+        };\n+    }\n+\n+    @Override\n+    public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n+        return new ConstantScoreWeight(this, boost) {\n+            Weight multiValuedSegmentWeight;\n+            Weight singleValuedSegmentWeight;\n+\n+            @Override\n+            public boolean isCacheable(LeafReaderContext ctx) {\n+                return true;\n+            }\n+\n+            @Override\n+            public Scorer scorer(LeafReaderContext context) throws IOException {\n+                ScorerSupplier scorerSupplier = scorerSupplier(context);\n+                if (scorerSupplier == null) {\n+                    return null;\n+                }\n+                return scorerSupplier.get(Long.MAX_VALUE);\n+            }\n+\n+            @Override\n+            public ScorerSupplier scorerSupplier(LeafReaderContext context) throws IOException {\n+                /*\n+                 * If we're sure docs only have a single value for the field\n+                 * we can pick the most compact bounds. If there are multiple values\n+                 * for the field we have to run the boolean query.\n+                 */\n+                PointValues points = context.reader().getPointValues(field);\n+                if (points == null) {\n+                    return null;\n+                }\n+                if (points.size() == points.getDocCount()) {\n+                    // Each doc that has points has exactly one point.\n+                    return singleValuedSegmentWeight().scorerSupplier(context);\n+                }\n+                return multiValuedSegmentWeight().scorerSupplier(context);\n+            }\n+\n+            @Override\n+            public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n+                PointValues points = context.reader().getPointValues(field);\n+                if (points == null) {\n+                    return null;\n+                }\n+                if (points.size() == points.getDocCount()) {\n+                    // Each doc that has points has exactly one point.\n+                    return singleValuedSegmentWeight().bulkScorer(context);\n+                }\n+                return multiValuedSegmentWeight().bulkScorer(context);\n+            }\n+\n+            private Weight singleValuedSegmentWeight() throws IOException {\n+                if (singleValuedSegmentWeight == null) {\n+                    singleValuedSegmentWeight = delegateForSingleValuedSegments.createWeight(searcher, scoreMode, boost);\n+                }\n+                return singleValuedSegmentWeight;\n+            }\n+\n+            private Weight multiValuedSegmentWeight() throws IOException {\n+                if (multiValuedSegmentWeight == null) {\n+                    multiValuedSegmentWeight = delegateForMultiValuedSegments.createWeight(searcher, scoreMode, boost);\n+                }\n+                return multiValuedSegmentWeight;\n+            }\n+        };\n+    }\n+\n+    /**\n+     * The query used when we have single valued segments.\n+     */\n+    Query delegateForSingleValuedSegments() {\n+        return delegateForSingleValuedSegments;\n+    }\n+\n+    @Override\n+    public String toString(String field) {\n+        return \"MergedPointRange[\" + delegateForMultiValuedSegments.toString(field) + \"]\";\n+    }\n+\n+    @Override\n+    public boolean equals(Object obj) {\n+        if (obj == null || obj.getClass() != getClass()) {\n+            return false;\n+        }\n+        MergedPointRangeQuery other = (MergedPointRangeQuery) obj;\n+        return delegateForMultiValuedSegments.equals(other.delegateForMultiValuedSegments);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTYyOTY1NQ=="}, "originalCommit": {"oid": "ca09463ad6846a4579d477713a9d26e54f27bf07"}, "originalPosition": 189}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1NzQ0NTE2OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwODozMTo0NlrOHvjtXw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxNToxMDozNlrOHvzSyQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTYzMDE3NQ==", "bodyText": "My feeling is that we do not need to implement this method. Note that queries are used in a filter context so we do not need to worry about scores?", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r519630175", "createdAt": "2020-11-09T08:31:46Z", "author": {"login": "iverase"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java", "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.search.aggregations.bucket.filter;\n+\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.PointValues;\n+import org.apache.lucene.search.BooleanClause.Occur;\n+import org.apache.lucene.search.BooleanQuery;\n+import org.apache.lucene.search.BulkScorer;\n+import org.apache.lucene.search.ConstantScoreWeight;\n+import org.apache.lucene.search.IndexSearcher;\n+import org.apache.lucene.search.MatchNoDocsQuery;\n+import org.apache.lucene.search.PointRangeQuery;\n+import org.apache.lucene.search.Query;\n+import org.apache.lucene.search.ScoreMode;\n+import org.apache.lucene.search.Scorer;\n+import org.apache.lucene.search.ScorerSupplier;\n+import org.apache.lucene.search.Weight;\n+\n+import java.io.IOException;\n+\n+import static java.util.Arrays.compareUnsigned;\n+\n+/**\n+ * Query merging two point in range queries.\n+ */\n+public class MergedPointRangeQuery extends Query {\n+    /**\n+     * Merge two {@linkplain PointRangeQuery}s into a {@linkplain MergedPointRangeQuery}\n+     * that matches points that match both filters.\n+     */\n+    public static Query merge(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        if (lhs.equals(rhs)) {\n+            // Lucky case! The queries were the same so their UNION is just the query itself.\n+            return lhs;\n+        }\n+        if (lhs.getField() != rhs.getField() || lhs.getNumDims() != rhs.getNumDims() || lhs.getBytesPerDim() != rhs.getBytesPerDim()) {\n+            return null;\n+        }\n+        return new MergedPointRangeQuery(lhs, rhs);\n+    }\n+\n+    private final String field;\n+    private final Query delegateForMultiValuedSegments;\n+    private final Query delegateForSingleValuedSegments;\n+\n+    private MergedPointRangeQuery(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        field = lhs.getField();\n+        delegateForMultiValuedSegments = new BooleanQuery.Builder().add(lhs, Occur.MUST).add(rhs, Occur.MUST).build();\n+        int numDims = lhs.getNumDims();\n+        int bytesPerDim = lhs.getBytesPerDim();\n+        this.delegateForSingleValuedSegments = pickDelegateForSingleValuedSegments(\n+            mergeBound(lhs.getLowerPoint(), rhs.getLowerPoint(), numDims, bytesPerDim, true),\n+            mergeBound(lhs.getUpperPoint(), rhs.getUpperPoint(), numDims, bytesPerDim, false),\n+            numDims,\n+            bytesPerDim\n+        );\n+    }\n+\n+    private Query pickDelegateForSingleValuedSegments(byte[] lower, byte[] upper, int numDims, int bytesPerDim) {\n+        // If we ended up with disjoint ranges in any dimension then on single valued segments we can't match any docs.\n+        for (int dim = 0; dim < numDims; dim++) {\n+            int offset = dim * bytesPerDim;\n+            if (compareUnsigned(lower, offset, offset + bytesPerDim, upper, offset, offset + bytesPerDim) > 0) {\n+                return new MatchNoDocsQuery(\"disjoint ranges\");\n+            }\n+        }\n+        // Otherwise on single valued segments we can only match docs the match the UNION of the two ranges.\n+        return new PointRangeQuery(field, lower, upper, numDims) {\n+            @Override\n+            protected String toString(int dimension, byte[] value) {\n+                // Stolen from Lucene's Binary range query. It'd be best to delegate, but the method isn't visible.\n+                StringBuilder sb = new StringBuilder();\n+                sb.append(\"(\");\n+                for (int i = 0; i < value.length; i++) {\n+                    if (i > 0) {\n+                        sb.append(' ');\n+                    }\n+                    sb.append(Integer.toHexString(value[i] & 0xFF));\n+                }\n+                sb.append(')');\n+                return sb.toString();\n+            }\n+        };\n+    }\n+\n+    @Override\n+    public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n+        return new ConstantScoreWeight(this, boost) {\n+            Weight multiValuedSegmentWeight;\n+            Weight singleValuedSegmentWeight;\n+\n+            @Override\n+            public boolean isCacheable(LeafReaderContext ctx) {\n+                return true;\n+            }\n+\n+            @Override\n+            public Scorer scorer(LeafReaderContext context) throws IOException {\n+                ScorerSupplier scorerSupplier = scorerSupplier(context);\n+                if (scorerSupplier == null) {\n+                    return null;\n+                }\n+                return scorerSupplier.get(Long.MAX_VALUE);\n+            }\n+\n+            @Override\n+            public ScorerSupplier scorerSupplier(LeafReaderContext context) throws IOException {\n+                /*\n+                 * If we're sure docs only have a single value for the field\n+                 * we can pick the most compact bounds. If there are multiple values\n+                 * for the field we have to run the boolean query.\n+                 */\n+                PointValues points = context.reader().getPointValues(field);\n+                if (points == null) {\n+                    return null;\n+                }\n+                if (points.size() == points.getDocCount()) {\n+                    // Each doc that has points has exactly one point.\n+                    return singleValuedSegmentWeight().scorerSupplier(context);\n+                }\n+                return multiValuedSegmentWeight().scorerSupplier(context);\n+            }\n+\n+            @Override\n+            public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n+                PointValues points = context.reader().getPointValues(field);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca09463ad6846a4579d477713a9d26e54f27bf07"}, "originalPosition": 144}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTg4NTUxMw==", "bodyText": "Spoke with Nik offline and even though at the moment there is probably no benefit in implementing this method, in a future it can be improved on the Lucene side, so this implementation will take advance of it.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r519885513", "createdAt": "2020-11-09T15:10:36Z", "author": {"login": "iverase"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java", "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.search.aggregations.bucket.filter;\n+\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.PointValues;\n+import org.apache.lucene.search.BooleanClause.Occur;\n+import org.apache.lucene.search.BooleanQuery;\n+import org.apache.lucene.search.BulkScorer;\n+import org.apache.lucene.search.ConstantScoreWeight;\n+import org.apache.lucene.search.IndexSearcher;\n+import org.apache.lucene.search.MatchNoDocsQuery;\n+import org.apache.lucene.search.PointRangeQuery;\n+import org.apache.lucene.search.Query;\n+import org.apache.lucene.search.ScoreMode;\n+import org.apache.lucene.search.Scorer;\n+import org.apache.lucene.search.ScorerSupplier;\n+import org.apache.lucene.search.Weight;\n+\n+import java.io.IOException;\n+\n+import static java.util.Arrays.compareUnsigned;\n+\n+/**\n+ * Query merging two point in range queries.\n+ */\n+public class MergedPointRangeQuery extends Query {\n+    /**\n+     * Merge two {@linkplain PointRangeQuery}s into a {@linkplain MergedPointRangeQuery}\n+     * that matches points that match both filters.\n+     */\n+    public static Query merge(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        if (lhs.equals(rhs)) {\n+            // Lucky case! The queries were the same so their UNION is just the query itself.\n+            return lhs;\n+        }\n+        if (lhs.getField() != rhs.getField() || lhs.getNumDims() != rhs.getNumDims() || lhs.getBytesPerDim() != rhs.getBytesPerDim()) {\n+            return null;\n+        }\n+        return new MergedPointRangeQuery(lhs, rhs);\n+    }\n+\n+    private final String field;\n+    private final Query delegateForMultiValuedSegments;\n+    private final Query delegateForSingleValuedSegments;\n+\n+    private MergedPointRangeQuery(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        field = lhs.getField();\n+        delegateForMultiValuedSegments = new BooleanQuery.Builder().add(lhs, Occur.MUST).add(rhs, Occur.MUST).build();\n+        int numDims = lhs.getNumDims();\n+        int bytesPerDim = lhs.getBytesPerDim();\n+        this.delegateForSingleValuedSegments = pickDelegateForSingleValuedSegments(\n+            mergeBound(lhs.getLowerPoint(), rhs.getLowerPoint(), numDims, bytesPerDim, true),\n+            mergeBound(lhs.getUpperPoint(), rhs.getUpperPoint(), numDims, bytesPerDim, false),\n+            numDims,\n+            bytesPerDim\n+        );\n+    }\n+\n+    private Query pickDelegateForSingleValuedSegments(byte[] lower, byte[] upper, int numDims, int bytesPerDim) {\n+        // If we ended up with disjoint ranges in any dimension then on single valued segments we can't match any docs.\n+        for (int dim = 0; dim < numDims; dim++) {\n+            int offset = dim * bytesPerDim;\n+            if (compareUnsigned(lower, offset, offset + bytesPerDim, upper, offset, offset + bytesPerDim) > 0) {\n+                return new MatchNoDocsQuery(\"disjoint ranges\");\n+            }\n+        }\n+        // Otherwise on single valued segments we can only match docs the match the UNION of the two ranges.\n+        return new PointRangeQuery(field, lower, upper, numDims) {\n+            @Override\n+            protected String toString(int dimension, byte[] value) {\n+                // Stolen from Lucene's Binary range query. It'd be best to delegate, but the method isn't visible.\n+                StringBuilder sb = new StringBuilder();\n+                sb.append(\"(\");\n+                for (int i = 0; i < value.length; i++) {\n+                    if (i > 0) {\n+                        sb.append(' ');\n+                    }\n+                    sb.append(Integer.toHexString(value[i] & 0xFF));\n+                }\n+                sb.append(')');\n+                return sb.toString();\n+            }\n+        };\n+    }\n+\n+    @Override\n+    public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n+        return new ConstantScoreWeight(this, boost) {\n+            Weight multiValuedSegmentWeight;\n+            Weight singleValuedSegmentWeight;\n+\n+            @Override\n+            public boolean isCacheable(LeafReaderContext ctx) {\n+                return true;\n+            }\n+\n+            @Override\n+            public Scorer scorer(LeafReaderContext context) throws IOException {\n+                ScorerSupplier scorerSupplier = scorerSupplier(context);\n+                if (scorerSupplier == null) {\n+                    return null;\n+                }\n+                return scorerSupplier.get(Long.MAX_VALUE);\n+            }\n+\n+            @Override\n+            public ScorerSupplier scorerSupplier(LeafReaderContext context) throws IOException {\n+                /*\n+                 * If we're sure docs only have a single value for the field\n+                 * we can pick the most compact bounds. If there are multiple values\n+                 * for the field we have to run the boolean query.\n+                 */\n+                PointValues points = context.reader().getPointValues(field);\n+                if (points == null) {\n+                    return null;\n+                }\n+                if (points.size() == points.getDocCount()) {\n+                    // Each doc that has points has exactly one point.\n+                    return singleValuedSegmentWeight().scorerSupplier(context);\n+                }\n+                return multiValuedSegmentWeight().scorerSupplier(context);\n+            }\n+\n+            @Override\n+            public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n+                PointValues points = context.reader().getPointValues(field);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTYzMDE3NQ=="}, "originalCommit": {"oid": "ca09463ad6846a4579d477713a9d26e54f27bf07"}, "originalPosition": 144}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2998, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}