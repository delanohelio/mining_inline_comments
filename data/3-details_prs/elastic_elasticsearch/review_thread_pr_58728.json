{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQxOTQwNDIx", "number": 58728, "reviewThreads": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMzoxODo0MlrOEKF6FQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQwODozODowNFrOEKazzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MDE4MDA1OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMzoxODo0MlrOGq74hQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQwOToxNTo0NVrOGrdycw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzY3NDUwMQ==", "bodyText": "Could also be named prewarmExecutor()", "url": "https://github.com/elastic/elasticsearch/pull/58728#discussion_r447674501", "createdAt": "2020-06-30T13:18:42Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -317,6 +318,14 @@ public CacheFile getCacheFile(CacheKey cacheKey, long fileLength) throws Excepti\n         return cacheService.get(cacheKey, fileLength, cacheDir);\n     }\n \n+    public Executor executor() {\n+        return threadPool.executor(SearchableSnapshotsConstants.SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME);\n+    }\n+\n+    public Executor directExecutor() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "28091b888998d690d9cf926aa0e80362f75cb590"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzcyMTY1OA==", "bodyText": "++ I'd prefer that, lest we use this executor for something else in the future and then decide to change it. Can we name the other executor() method something more specific too?", "url": "https://github.com/elastic/elasticsearch/pull/58728#discussion_r447721658", "createdAt": "2020-06-30T14:21:07Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -317,6 +318,14 @@ public CacheFile getCacheFile(CacheKey cacheKey, long fileLength) throws Excepti\n         return cacheService.get(cacheKey, fileLength, cacheDir);\n     }\n \n+    public Executor executor() {\n+        return threadPool.executor(SearchableSnapshotsConstants.SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME);\n+    }\n+\n+    public Executor directExecutor() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzY3NDUwMQ=="}, "originalCommit": {"oid": "28091b888998d690d9cf926aa0e80362f75cb590"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODIyMDIwNA==", "bodyText": "Can we name the other executor() method something more specific too?\n\nI'm terrible at naming things (sorry, kids)... what do you think of  searchExecutor() ? cacheWritingExecutor() ?", "url": "https://github.com/elastic/elasticsearch/pull/58728#discussion_r448220204", "createdAt": "2020-07-01T08:58:33Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -317,6 +318,14 @@ public CacheFile getCacheFile(CacheKey cacheKey, long fileLength) throws Excepti\n         return cacheService.get(cacheKey, fileLength, cacheDir);\n     }\n \n+    public Executor executor() {\n+        return threadPool.executor(SearchableSnapshotsConstants.SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME);\n+    }\n+\n+    public Executor directExecutor() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzY3NDUwMQ=="}, "originalCommit": {"oid": "28091b888998d690d9cf926aa0e80362f75cb590"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODIyNzYxNA==", "bodyText": "Tricky one TBH. How about cacheFetchAsyncExecutor()?", "url": "https://github.com/elastic/elasticsearch/pull/58728#discussion_r448227614", "createdAt": "2020-07-01T09:11:33Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -317,6 +318,14 @@ public CacheFile getCacheFile(CacheKey cacheKey, long fileLength) throws Excepti\n         return cacheService.get(cacheKey, fileLength, cacheDir);\n     }\n \n+    public Executor executor() {\n+        return threadPool.executor(SearchableSnapshotsConstants.SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME);\n+    }\n+\n+    public Executor directExecutor() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzY3NDUwMQ=="}, "originalCommit": {"oid": "28091b888998d690d9cf926aa0e80362f75cb590"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODIzMDAwMw==", "bodyText": "+1", "url": "https://github.com/elastic/elasticsearch/pull/58728#discussion_r448230003", "createdAt": "2020-07-01T09:15:45Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -317,6 +318,14 @@ public CacheFile getCacheFile(CacheKey cacheKey, long fileLength) throws Excepti\n         return cacheService.get(cacheKey, fileLength, cacheDir);\n     }\n \n+    public Executor executor() {\n+        return threadPool.executor(SearchableSnapshotsConstants.SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME);\n+    }\n+\n+    public Executor directExecutor() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzY3NDUwMQ=="}, "originalCommit": {"oid": "28091b888998d690d9cf926aa0e80362f75cb590"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MDE4MjA5OnYy", "diffSide": "LEFT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CacheFile.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMzoxOTowN1rOGq75pQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMzoxOTowN1rOGq75pQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzY3NDc4OQ==", "bodyText": "This is already verified by the SparseFileTracker itself", "url": "https://github.com/elastic/elasticsearch/pull/58728#discussion_r447674789", "createdAt": "2020-06-30T13:19:07Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CacheFile.java", "diffHunk": "@@ -259,38 +262,68 @@ private void ensureOpen() {\n         }\n     }\n \n-    CompletableFuture<Integer> fetchRange(\n-        long start,\n-        long end,\n-        CheckedBiFunction<Long, Long, Integer, IOException> onRangeAvailable,\n-        CheckedBiConsumer<Long, Long, IOException> onRangeMissing\n+    @FunctionalInterface\n+    interface CacheReader {\n+        int read(FileChannel channel) throws IOException;\n+    }\n+\n+    @FunctionalInterface\n+    interface CacheWriter {\n+        void write(FileChannel channel, long from, long to, Consumer<Long> progressUpdater) throws IOException;\n+    }\n+\n+    CompletableFuture<Integer> fetchAsync(\n+        final Tuple<Long, Long> rangeToWrite,\n+        final Tuple<Long, Long> rangeToRead,\n+        final CacheReader reader,\n+        final CacheWriter writer,\n+        final Executor executor\n     ) {\n         final CompletableFuture<Integer> future = new CompletableFuture<>();\n         try {\n-            if (start < 0 || start > tracker.getLength() || start > end || end > tracker.getLength()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "28091b888998d690d9cf926aa0e80362f75cb590"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MDE4Nzk0OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMzoyMDoxN1rOGq79DA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQwODoxNDowOFrOGrbl7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzY3NTY2MA==", "bodyText": "This was previously embedded in the readCacheFile() method but feels wrong to me; the readCacheFile() should expect a ByteBuffer with an appropriate limit.", "url": "https://github.com/elastic/elasticsearch/pull/58728#discussion_r447675660", "createdAt": "2020-06-30T13:20:17Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -141,13 +143,22 @@ protected void readInternal(ByteBuffer b) throws IOException {\n             try {\n                 final CacheFile cacheFile = getCacheFileSafe();\n                 try (Releasable ignored = cacheFile.fileLock()) {\n-                    final Tuple<Long, Long> range = computeRange(pos);\n-                    bytesRead = cacheFile.fetchRange(\n-                        range.v1(),\n-                        range.v2(),\n-                        (start, end) -> readCacheFile(cacheFile.getChannel(), end, pos, b, len),\n-                        (start, end) -> writeCacheFile(cacheFile.getChannel(), start, end)\n-                    ).get();\n+                    final Tuple<Long, Long> rangeToWrite = computeRange(pos);\n+                    final Tuple<Long, Long> rangeToRead = Tuple.tuple(pos, Math.min(pos + len, rangeToWrite.v2()));\n+\n+                    bytesRead = cacheFile.fetchAsync(rangeToWrite, rangeToRead, (channel) -> {\n+                        final int read;\n+                        if ((rangeToRead.v2() - rangeToRead.v1()) < b.remaining()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "28091b888998d690d9cf926aa0e80362f75cb590"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE5NDAzMQ==", "bodyText": "++ there was a recent change to the Lucene APIs replacing byte[], int, int with ByteBuffer but this was not pushed all the way through this code.", "url": "https://github.com/elastic/elasticsearch/pull/58728#discussion_r448194031", "createdAt": "2020-07-01T08:14:08Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -141,13 +143,22 @@ protected void readInternal(ByteBuffer b) throws IOException {\n             try {\n                 final CacheFile cacheFile = getCacheFileSafe();\n                 try (Releasable ignored = cacheFile.fileLock()) {\n-                    final Tuple<Long, Long> range = computeRange(pos);\n-                    bytesRead = cacheFile.fetchRange(\n-                        range.v1(),\n-                        range.v2(),\n-                        (start, end) -> readCacheFile(cacheFile.getChannel(), end, pos, b, len),\n-                        (start, end) -> writeCacheFile(cacheFile.getChannel(), start, end)\n-                    ).get();\n+                    final Tuple<Long, Long> rangeToWrite = computeRange(pos);\n+                    final Tuple<Long, Long> rangeToRead = Tuple.tuple(pos, Math.min(pos + len, rangeToWrite.v2()));\n+\n+                    bytesRead = cacheFile.fetchAsync(rangeToWrite, rangeToRead, (channel) -> {\n+                        final int read;\n+                        if ((rangeToRead.v2() - rangeToRead.v1()) < b.remaining()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzY3NTY2MA=="}, "originalCommit": {"oid": "28091b888998d690d9cf926aa0e80362f75cb590"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MDIwMDY4OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMzoyMjo1N1rOGq8Exw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQwNzozOTozMlrOGracpg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzY3NzYzOQ==", "bodyText": "Maybe it deserves some comment: we don't care about reading the cached data for prewarming, it just need to cache the range. This is why it got passed an empty range to read. This way if the range is already (or about to be) written by a concurrent search it returns immediately and process the next small range. If the range is not available then the cache data will be written by this prewarming task.", "url": "https://github.com/elastic/elasticsearch/pull/58728#discussion_r447677639", "createdAt": "2020-06-30T13:22:57Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -224,31 +235,33 @@ public void prefetchPart(final int part) throws IOException {\n                     while (remainingBytes > 0L) {\n                         assert totalBytesRead + remainingBytes == rangeLength;\n                         final int bytesRead = readSafe(input, copyBuffer, rangeStart, rangeEnd, remainingBytes, cacheFileReference);\n+\n                         final long readStart = rangeStart + totalBytesRead;\n-                        cacheFile.fetchRange(readStart, readStart + bytesRead, (start, end) -> {\n-                            logger.trace(\n-                                \"prefetchPart: range [{}-{}] of file [{}] is available in cache\",\n-                                start,\n-                                end,\n-                                fileInfo.physicalName()\n-                            );\n-                            return Math.toIntExact(end - start);\n-                        }, (start, end) -> {\n-                            final ByteBuffer byteBuffer = ByteBuffer.wrap(\n-                                copyBuffer,\n-                                Math.toIntExact(start - readStart),\n-                                Math.toIntExact(end - start)\n-                            );\n-                            final int writtenBytes = positionalWrite(fc, start, byteBuffer);\n-                            logger.trace(\n-                                \"prefetchPart: writing range [{}-{}] of file [{}], [{}] bytes written\",\n-                                start,\n-                                end,\n-                                fileInfo.physicalName(),\n-                                writtenBytes\n-                            );\n-                            totalBytesWritten.addAndGet(writtenBytes);\n-                        });\n+                        final long readEnd = readStart + bytesRead;\n+\n+                        cacheFile.fetchAsync(\n+                            Tuple.tuple(readStart, readEnd),\n+                            Tuple.tuple(readStart, readStart),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "28091b888998d690d9cf926aa0e80362f75cb590"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3NTI3MA==", "bodyText": "++ let's add a comment saying that in the code too.", "url": "https://github.com/elastic/elasticsearch/pull/58728#discussion_r448175270", "createdAt": "2020-07-01T07:39:32Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -224,31 +235,33 @@ public void prefetchPart(final int part) throws IOException {\n                     while (remainingBytes > 0L) {\n                         assert totalBytesRead + remainingBytes == rangeLength;\n                         final int bytesRead = readSafe(input, copyBuffer, rangeStart, rangeEnd, remainingBytes, cacheFileReference);\n+\n                         final long readStart = rangeStart + totalBytesRead;\n-                        cacheFile.fetchRange(readStart, readStart + bytesRead, (start, end) -> {\n-                            logger.trace(\n-                                \"prefetchPart: range [{}-{}] of file [{}] is available in cache\",\n-                                start,\n-                                end,\n-                                fileInfo.physicalName()\n-                            );\n-                            return Math.toIntExact(end - start);\n-                        }, (start, end) -> {\n-                            final ByteBuffer byteBuffer = ByteBuffer.wrap(\n-                                copyBuffer,\n-                                Math.toIntExact(start - readStart),\n-                                Math.toIntExact(end - start)\n-                            );\n-                            final int writtenBytes = positionalWrite(fc, start, byteBuffer);\n-                            logger.trace(\n-                                \"prefetchPart: writing range [{}-{}] of file [{}], [{}] bytes written\",\n-                                start,\n-                                end,\n-                                fileInfo.physicalName(),\n-                                writtenBytes\n-                            );\n-                            totalBytesWritten.addAndGet(writtenBytes);\n-                        });\n+                        final long readEnd = readStart + bytesRead;\n+\n+                        cacheFile.fetchAsync(\n+                            Tuple.tuple(readStart, readEnd),\n+                            Tuple.tuple(readStart, readStart),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzY3NzYzOQ=="}, "originalCommit": {"oid": "28091b888998d690d9cf926aa0e80362f75cb590"}, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MzM4NDcxOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CacheFile.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQwNzozNTo0NlrOGraVcA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQwOToyOTo0MlrOGreRXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3MzQyNA==", "bodyText": "Why do we remove the gap from the list here?", "url": "https://github.com/elastic/elasticsearch/pull/58728#discussion_r448173424", "createdAt": "2020-07-01T07:35:46Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CacheFile.java", "diffHunk": "@@ -259,38 +262,68 @@ private void ensureOpen() {\n         }\n     }\n \n-    CompletableFuture<Integer> fetchRange(\n-        long start,\n-        long end,\n-        CheckedBiFunction<Long, Long, Integer, IOException> onRangeAvailable,\n-        CheckedBiConsumer<Long, Long, IOException> onRangeMissing\n+    @FunctionalInterface\n+    interface CacheReader {\n+        int read(FileChannel channel) throws IOException;\n+    }\n+\n+    @FunctionalInterface\n+    interface CacheWriter {\n+        void write(FileChannel channel, long from, long to, Consumer<Long> progressUpdater) throws IOException;\n+    }\n+\n+    CompletableFuture<Integer> fetchAsync(\n+        final Tuple<Long, Long> rangeToWrite,\n+        final Tuple<Long, Long> rangeToRead,\n+        final CacheReader reader,\n+        final CacheWriter writer,\n+        final Executor executor\n     ) {\n         final CompletableFuture<Integer> future = new CompletableFuture<>();\n         try {\n-            if (start < 0 || start > tracker.getLength() || start > end || end > tracker.getLength()) {\n-                throw new IllegalArgumentException(\n-                    \"Invalid range [start=\" + start + \", end=\" + end + \"] for length [\" + tracker.getLength() + ']'\n-                );\n-            }\n             ensureOpen();\n             final List<SparseFileTracker.Gap> gaps = tracker.waitForRange(\n-                Tuple.tuple(start, end),\n-                Tuple.tuple(start, end), // TODO use progressive sub range to trigger read operations sooner\n-                ActionListener.wrap(\n-                    rangeReady -> future.complete(onRangeAvailable.apply(start, end)),\n-                    rangeFailure -> future.completeExceptionally(rangeFailure)\n-                )\n+                rangeToWrite,\n+                rangeToRead,\n+                ActionListener.wrap(success -> future.complete(reader.read(channel)), future::completeExceptionally)\n             );\n \n-            for (SparseFileTracker.Gap gap : gaps) {\n-                try {\n-                    ensureOpen();\n-                    onRangeMissing.accept(gap.start(), gap.end());\n-                    gap.onProgress(gap.end()); // TODO update progress in onRangeMissing\n-                    gap.onCompletion();\n-                } catch (Exception e) {\n-                    gap.onFailure(e);\n-                }\n+            if (gaps.isEmpty() == false) {\n+                final Iterator<SparseFileTracker.Gap> iterator = new ArrayList<>(gaps).iterator();\n+                executor.execute(new AbstractRunnable() {\n+\n+                    @Override\n+                    protected void doRun() {\n+                        while (iterator.hasNext()) {\n+                            final SparseFileTracker.Gap gap = iterator.next();\n+                            try {\n+                                ensureOpen();\n+                                if (readLock.tryLock() == false) {\n+                                    throw new AlreadyClosedException(\"Cache file channel is being evicted, writing attempt cancelled\");\n+                                }\n+                                try {\n+                                    ensureOpen();\n+                                    if (channel == null) {\n+                                        throw new AlreadyClosedException(\"Cache file channel has been released and closed\");\n+                                    }\n+                                    writer.write(channel, gap.start(), gap.end(), gap::onProgress);\n+                                    gap.onCompletion();\n+                                } finally {\n+                                    readLock.unlock();\n+                                }\n+                            } catch (Exception e) {\n+                                gap.onFailure(e);\n+                            } finally {\n+                                iterator.remove();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "28091b888998d690d9cf926aa0e80362f75cb590"}, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODIzNzkxOA==", "bodyText": "This is a left over - I think we can just iterate over the list of gaps.", "url": "https://github.com/elastic/elasticsearch/pull/58728#discussion_r448237918", "createdAt": "2020-07-01T09:29:42Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CacheFile.java", "diffHunk": "@@ -259,38 +262,68 @@ private void ensureOpen() {\n         }\n     }\n \n-    CompletableFuture<Integer> fetchRange(\n-        long start,\n-        long end,\n-        CheckedBiFunction<Long, Long, Integer, IOException> onRangeAvailable,\n-        CheckedBiConsumer<Long, Long, IOException> onRangeMissing\n+    @FunctionalInterface\n+    interface CacheReader {\n+        int read(FileChannel channel) throws IOException;\n+    }\n+\n+    @FunctionalInterface\n+    interface CacheWriter {\n+        void write(FileChannel channel, long from, long to, Consumer<Long> progressUpdater) throws IOException;\n+    }\n+\n+    CompletableFuture<Integer> fetchAsync(\n+        final Tuple<Long, Long> rangeToWrite,\n+        final Tuple<Long, Long> rangeToRead,\n+        final CacheReader reader,\n+        final CacheWriter writer,\n+        final Executor executor\n     ) {\n         final CompletableFuture<Integer> future = new CompletableFuture<>();\n         try {\n-            if (start < 0 || start > tracker.getLength() || start > end || end > tracker.getLength()) {\n-                throw new IllegalArgumentException(\n-                    \"Invalid range [start=\" + start + \", end=\" + end + \"] for length [\" + tracker.getLength() + ']'\n-                );\n-            }\n             ensureOpen();\n             final List<SparseFileTracker.Gap> gaps = tracker.waitForRange(\n-                Tuple.tuple(start, end),\n-                Tuple.tuple(start, end), // TODO use progressive sub range to trigger read operations sooner\n-                ActionListener.wrap(\n-                    rangeReady -> future.complete(onRangeAvailable.apply(start, end)),\n-                    rangeFailure -> future.completeExceptionally(rangeFailure)\n-                )\n+                rangeToWrite,\n+                rangeToRead,\n+                ActionListener.wrap(success -> future.complete(reader.read(channel)), future::completeExceptionally)\n             );\n \n-            for (SparseFileTracker.Gap gap : gaps) {\n-                try {\n-                    ensureOpen();\n-                    onRangeMissing.accept(gap.start(), gap.end());\n-                    gap.onProgress(gap.end()); // TODO update progress in onRangeMissing\n-                    gap.onCompletion();\n-                } catch (Exception e) {\n-                    gap.onFailure(e);\n-                }\n+            if (gaps.isEmpty() == false) {\n+                final Iterator<SparseFileTracker.Gap> iterator = new ArrayList<>(gaps).iterator();\n+                executor.execute(new AbstractRunnable() {\n+\n+                    @Override\n+                    protected void doRun() {\n+                        while (iterator.hasNext()) {\n+                            final SparseFileTracker.Gap gap = iterator.next();\n+                            try {\n+                                ensureOpen();\n+                                if (readLock.tryLock() == false) {\n+                                    throw new AlreadyClosedException(\"Cache file channel is being evicted, writing attempt cancelled\");\n+                                }\n+                                try {\n+                                    ensureOpen();\n+                                    if (channel == null) {\n+                                        throw new AlreadyClosedException(\"Cache file channel has been released and closed\");\n+                                    }\n+                                    writer.write(channel, gap.start(), gap.end(), gap::onProgress);\n+                                    gap.onCompletion();\n+                                } finally {\n+                                    readLock.unlock();\n+                                }\n+                            } catch (Exception e) {\n+                                gap.onFailure(e);\n+                            } finally {\n+                                iterator.remove();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3MzQyNA=="}, "originalCommit": {"oid": "28091b888998d690d9cf926aa0e80362f75cb590"}, "originalPosition": 113}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MzU2MjAxOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CacheFile.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQwODoyNjozNVrOGrcDjw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQwOTozMDowNVrOGreSJw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODIwMTYxNQ==", "bodyText": "Can we assert that reader.read(channel) returns rangeToRead.end() - rangeToRead.start() here?", "url": "https://github.com/elastic/elasticsearch/pull/58728#discussion_r448201615", "createdAt": "2020-07-01T08:26:35Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CacheFile.java", "diffHunk": "@@ -259,38 +262,68 @@ private void ensureOpen() {\n         }\n     }\n \n-    CompletableFuture<Integer> fetchRange(\n-        long start,\n-        long end,\n-        CheckedBiFunction<Long, Long, Integer, IOException> onRangeAvailable,\n-        CheckedBiConsumer<Long, Long, IOException> onRangeMissing\n+    @FunctionalInterface\n+    interface CacheReader {\n+        int read(FileChannel channel) throws IOException;\n+    }\n+\n+    @FunctionalInterface\n+    interface CacheWriter {\n+        void write(FileChannel channel, long from, long to, Consumer<Long> progressUpdater) throws IOException;\n+    }\n+\n+    CompletableFuture<Integer> fetchAsync(\n+        final Tuple<Long, Long> rangeToWrite,\n+        final Tuple<Long, Long> rangeToRead,\n+        final CacheReader reader,\n+        final CacheWriter writer,\n+        final Executor executor\n     ) {\n         final CompletableFuture<Integer> future = new CompletableFuture<>();\n         try {\n-            if (start < 0 || start > tracker.getLength() || start > end || end > tracker.getLength()) {\n-                throw new IllegalArgumentException(\n-                    \"Invalid range [start=\" + start + \", end=\" + end + \"] for length [\" + tracker.getLength() + ']'\n-                );\n-            }\n             ensureOpen();\n             final List<SparseFileTracker.Gap> gaps = tracker.waitForRange(\n-                Tuple.tuple(start, end),\n-                Tuple.tuple(start, end), // TODO use progressive sub range to trigger read operations sooner\n-                ActionListener.wrap(\n-                    rangeReady -> future.complete(onRangeAvailable.apply(start, end)),\n-                    rangeFailure -> future.completeExceptionally(rangeFailure)\n-                )\n+                rangeToWrite,\n+                rangeToRead,\n+                ActionListener.wrap(success -> future.complete(reader.read(channel)), future::completeExceptionally)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "28091b888998d690d9cf926aa0e80362f75cb590"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODIzODExOQ==", "bodyText": "Good suggestion, thanks", "url": "https://github.com/elastic/elasticsearch/pull/58728#discussion_r448238119", "createdAt": "2020-07-01T09:30:05Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CacheFile.java", "diffHunk": "@@ -259,38 +262,68 @@ private void ensureOpen() {\n         }\n     }\n \n-    CompletableFuture<Integer> fetchRange(\n-        long start,\n-        long end,\n-        CheckedBiFunction<Long, Long, Integer, IOException> onRangeAvailable,\n-        CheckedBiConsumer<Long, Long, IOException> onRangeMissing\n+    @FunctionalInterface\n+    interface CacheReader {\n+        int read(FileChannel channel) throws IOException;\n+    }\n+\n+    @FunctionalInterface\n+    interface CacheWriter {\n+        void write(FileChannel channel, long from, long to, Consumer<Long> progressUpdater) throws IOException;\n+    }\n+\n+    CompletableFuture<Integer> fetchAsync(\n+        final Tuple<Long, Long> rangeToWrite,\n+        final Tuple<Long, Long> rangeToRead,\n+        final CacheReader reader,\n+        final CacheWriter writer,\n+        final Executor executor\n     ) {\n         final CompletableFuture<Integer> future = new CompletableFuture<>();\n         try {\n-            if (start < 0 || start > tracker.getLength() || start > end || end > tracker.getLength()) {\n-                throw new IllegalArgumentException(\n-                    \"Invalid range [start=\" + start + \", end=\" + end + \"] for length [\" + tracker.getLength() + ']'\n-                );\n-            }\n             ensureOpen();\n             final List<SparseFileTracker.Gap> gaps = tracker.waitForRange(\n-                Tuple.tuple(start, end),\n-                Tuple.tuple(start, end), // TODO use progressive sub range to trigger read operations sooner\n-                ActionListener.wrap(\n-                    rangeReady -> future.complete(onRangeAvailable.apply(start, end)),\n-                    rangeFailure -> future.completeExceptionally(rangeFailure)\n-                )\n+                rangeToWrite,\n+                rangeToRead,\n+                ActionListener.wrap(success -> future.complete(reader.read(channel)), future::completeExceptionally)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODIwMTYxNQ=="}, "originalCommit": {"oid": "28091b888998d690d9cf926aa0e80362f75cb590"}, "originalPosition": 75}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MzYwNDYwOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CacheFile.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQwODozODowNFrOGrceKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQwOTo0MzowOFrOGreutw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODIwODQyNQ==", "bodyText": "What, no CheckedQuadConsumer<...>? \ud83d\ude01\nI'm torn about having these be separate interfaces vs combining them together perhaps along with some other arguments to fetchAsync too. I'm finding it especially strange that CacheReader#read only takes a FileChannel; this indicates that rangeToRead is now available, but that's very implicit now.\nMaybe different names would help. How about something like RangeMissingHandler#fillCacheRange and RangeAvailableHandler#onRangeAvailable?", "url": "https://github.com/elastic/elasticsearch/pull/58728#discussion_r448208425", "createdAt": "2020-07-01T08:38:04Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CacheFile.java", "diffHunk": "@@ -259,38 +262,68 @@ private void ensureOpen() {\n         }\n     }\n \n-    CompletableFuture<Integer> fetchRange(\n-        long start,\n-        long end,\n-        CheckedBiFunction<Long, Long, Integer, IOException> onRangeAvailable,\n-        CheckedBiConsumer<Long, Long, IOException> onRangeMissing\n+    @FunctionalInterface\n+    interface CacheReader {\n+        int read(FileChannel channel) throws IOException;\n+    }\n+\n+    @FunctionalInterface\n+    interface CacheWriter {\n+        void write(FileChannel channel, long from, long to, Consumer<Long> progressUpdater) throws IOException;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "28091b888998d690d9cf926aa0e80362f75cb590"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODI0NTQzMQ==", "bodyText": "What, no CheckedQuadConsumer<...>? grin\n\nDon't tempt me :)\n\nMaybe different names would help. How about something like RangeMissingHandler#fillCacheRange and RangeAvailableHandler#onRangeAvailable?\n\nI went this route and renamed to RangeMissingHandler/RangeAvailableHandler", "url": "https://github.com/elastic/elasticsearch/pull/58728#discussion_r448245431", "createdAt": "2020-07-01T09:43:08Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CacheFile.java", "diffHunk": "@@ -259,38 +262,68 @@ private void ensureOpen() {\n         }\n     }\n \n-    CompletableFuture<Integer> fetchRange(\n-        long start,\n-        long end,\n-        CheckedBiFunction<Long, Long, Integer, IOException> onRangeAvailable,\n-        CheckedBiConsumer<Long, Long, IOException> onRangeMissing\n+    @FunctionalInterface\n+    interface CacheReader {\n+        int read(FileChannel channel) throws IOException;\n+    }\n+\n+    @FunctionalInterface\n+    interface CacheWriter {\n+        void write(FileChannel channel, long from, long to, Consumer<Long> progressUpdater) throws IOException;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODIwODQyNQ=="}, "originalCommit": {"oid": "28091b888998d690d9cf926aa0e80362f75cb590"}, "originalPosition": 48}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2271, "cost": 1, "resetAt": "2021-11-12T13:16:51Z"}}}