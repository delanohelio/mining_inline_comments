{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzg4NzI2NTU0", "number": 53583, "title": "add nori_number token filter in analysis-nori", "bodyText": "The KoreanNumberFilter has included in Nori after Lucene 8.2.0. (LUCENE-8812)\nHowever, it isn't supported now in Nori Analysis plugin. (Kuromoji supports kuromoji_number)\nIt seems to be missing(#30397) because KoreanNumberFilter didn't exist at Lucene 7.4.0 that supports Nori first time.\nThis PR is for that.", "createdAt": "2020-03-15T14:40:00Z", "url": "https://github.com/elastic/elasticsearch/pull/53583", "merged": true, "mergeCommit": {"oid": "8d4ff290cc6361cf23ae2b825c46bb83976e50f0"}, "closed": true, "closedAt": "2020-03-23T18:28:50Z", "author": {"login": "danmuzi"}, "timelineItems": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcOdrj9gFqTM3NTc5NDIwNA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcQidKpAFqTM3OTY5OTcxNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc1Nzk0MjA0", "url": "https://github.com/elastic/elasticsearch/pull/53583#pullrequestreview-375794204", "createdAt": "2020-03-17T07:34:39Z", "commit": {"oid": "5fa6abaa71e0b0406c69eb90edb98a17b5f74443"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QwNzozNDozOVrOF3QnbA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QwNzozNDozOVrOF3QnbA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzQ4ODIzNg==", "bodyText": "We also need to expose discard_punctuation in the tokenizer in order to detect floating point numbers and to split numbers correctly if they are separated with whitespaces ? We should then advise in the documentation to set discard_punctuation to false when using the nori_number filter and to remove punctuations in a subsequent filter ?", "url": "https://github.com/elastic/elasticsearch/pull/53583#discussion_r393488236", "createdAt": "2020-03-17T07:34:39Z", "author": {"login": "jimczi"}, "path": "docs/plugins/analysis-nori.asciidoc", "diffHunk": "@@ -434,3 +434,52 @@ Which responds with:\n --------------------------------------------------\n \n <1> The Hanja form is replaced by the Hangul translation.\n+\n+\n+[[analysis-nori-number]]\n+==== `nori_number` token filter", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5fa6abaa71e0b0406c69eb90edb98a17b5f74443"}, "originalPosition": 7}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc4MDU3Mzg5", "url": "https://github.com/elastic/elasticsearch/pull/53583#pullrequestreview-378057389", "createdAt": "2020-03-19T20:10:30Z", "commit": {"oid": "82191a3032f19729807dd2377af70a5c8272b0a4"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQyMDoxMDozMVrOF4-rHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQyMDoxMDozNlrOF4-rUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTI5MTQyMg==", "bodyText": "Can you add a small test for the new discard_punctuation option of the tokenizer ?", "url": "https://github.com/elastic/elasticsearch/pull/53583#discussion_r395291422", "createdAt": "2020-03-19T20:10:31Z", "author": {"login": "jimczi"}, "path": "plugins/analysis-nori/src/test/java/org/elasticsearch/index/analysis/NoriAnalysisTests.java", "diffHunk": "@@ -159,6 +162,20 @@ public void testNoriReadingForm() throws IOException {\n         assertTokenStreamContents(stream, new String[] {\"\ud5a5\uac00\"});\n     }\n \n+    public void testNoriNumber() throws IOException {\n+        Settings settings = Settings.builder()\n+            .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)\n+            .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())\n+            .put(\"index.analysis.filter.my_filter.type\", \"nori_number\")\n+            .build();\n+        TestAnalysis analysis = AnalysisTestsHelper.createTestAnalysisFromSettings(settings, new AnalysisNoriPlugin());\n+        TokenFilterFactory factory = analysis.tokenFilter.get(\"my_filter\");\n+        Tokenizer tokenizer = new KoreanTokenizer();\n+        tokenizer.setReader(new StringReader(\"\uc624\ub298 \uc2ed\ub9cc\uc774\ucc9c\uc624\ubc31\uc6d0\uc9dc\ub9ac \uc640\uc778 \uad6c\uc785\"));\n+        TokenStream stream = factory.create(tokenizer);\n+        assertTokenStreamContents(stream, new String[] {\"\uc624\ub298\", \"102500\", \"\uc6d0\", \"\uc9dc\ub9ac\", \"\uc640\uc778\", \"\uad6c\uc785\"});\n+    }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "82191a3032f19729807dd2377af70a5c8272b0a4"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTI5MTQ3NA==", "bodyText": "Maybe add add a NOTE: to emphasize this part ?", "url": "https://github.com/elastic/elasticsearch/pull/53583#discussion_r395291474", "createdAt": "2020-03-19T20:10:36Z", "author": {"login": "jimczi"}, "path": "docs/plugins/analysis-nori.asciidoc", "diffHunk": "@@ -434,3 +439,105 @@ Which responds with:\n --------------------------------------------------\n \n <1> The Hanja form is replaced by the Hangul translation.\n+\n+\n+[[analysis-nori-number]]\n+==== `nori_number` token filter\n+\n+The `nori_number` token filter normalizes Korean numbers\n+to regular Arabic decimal numbers in half-width characters.\n+\n+Korean numbers are often written using a combination of Hangul and Arabic numbers with various kinds punctuation.\n+For example, \uff13\uff0e\uff12\ucc9c means 3200.\n+This filter does this kind of normalization and allows a search for 3200 to match \uff13\uff0e\uff12\ucc9c in text,\n+but can also be used to make range facets based on the normalized numbers and so on.\n+\n+Notice that this analyzer uses a token composition scheme and relies on punctuation tokens", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "82191a3032f19729807dd2377af70a5c8272b0a4"}, "originalPosition": 36}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b6221c5748099e6902e2ad91c5207ab5ac2005b3", "author": {"user": {"login": "danmuzi", "name": "Namgyu Kim"}}, "url": "https://github.com/elastic/elasticsearch/commit/b6221c5748099e6902e2ad91c5207ab5ac2005b3", "committedDate": "2020-03-23T17:06:45Z", "message": "add nori_number token filter"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d6dbe514fa70c8278f180a3a602a75601d588d39", "author": {"user": {"login": "danmuzi", "name": "Namgyu Kim"}}, "url": "https://github.com/elastic/elasticsearch/commit/d6dbe514fa70c8278f180a3a602a75601d588d39", "committedDate": "2020-03-23T17:06:45Z", "message": "add discard_punctuation option in nori_tokenizer"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9bd0ebe9ab119a49f6a829d308d8238d24050ea2", "author": {"user": {"login": "danmuzi", "name": "Namgyu Kim"}}, "url": "https://github.com/elastic/elasticsearch/commit/9bd0ebe9ab119a49f6a829d308d8238d24050ea2", "committedDate": "2020-03-23T17:06:45Z", "message": "add description about using discard_punctuation in nori_number"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6ed33a19b7a9f5e4c6dc9b628cf2f6202113c25d", "author": {"user": {"login": "danmuzi", "name": "Namgyu Kim"}}, "url": "https://github.com/elastic/elasticsearch/commit/6ed33a19b7a9f5e4c6dc9b628cf2f6202113c25d", "committedDate": "2020-03-23T17:06:45Z", "message": "add note in asciidoc and test cases for discard_punctuation"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1a4367ea993cfc34d7d034e03c5b9cb80790fe3c", "author": {"user": {"login": "danmuzi", "name": "Namgyu Kim"}}, "url": "https://github.com/elastic/elasticsearch/commit/1a4367ea993cfc34d7d034e03c5b9cb80790fe3c", "committedDate": "2020-03-23T17:06:45Z", "message": "fix wrong indentation in nori_number test"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3864137d501738a3e65c9753ae794a2f6f594590", "author": {"user": {"login": "danmuzi", "name": "Namgyu Kim"}}, "url": "https://github.com/elastic/elasticsearch/commit/3864137d501738a3e65c9753ae794a2f6f594590", "committedDate": "2020-03-23T16:57:16Z", "message": "fix wrong indentation in nori_number test"}, "afterCommit": {"oid": "1a4367ea993cfc34d7d034e03c5b9cb80790fe3c", "author": {"user": {"login": "danmuzi", "name": "Namgyu Kim"}}, "url": "https://github.com/elastic/elasticsearch/commit/1a4367ea993cfc34d7d034e03c5b9cb80790fe3c", "committedDate": "2020-03-23T17:06:45Z", "message": "fix wrong indentation in nori_number test"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc5Njk5NzE2", "url": "https://github.com/elastic/elasticsearch/pull/53583#pullrequestreview-379699716", "createdAt": "2020-03-23T18:17:30Z", "commit": {"oid": "1a4367ea993cfc34d7d034e03c5b9cb80790fe3c"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1427, "cost": 1, "resetAt": "2021-10-28T18:54:27Z"}}}