{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDg3ODc4MDgz", "number": 62441, "title": "Also abort ongoing file restores when snapshot restore is aborted", "bodyText": "Today when a snapshot restore is aborted (for example when the index is explicitly deleted) while the restoration of the files from the repository has already started the file restores are not interrupted. It means that Elasticsearch will continue to read the files from the repository and will continue to write them to disk until all files are restored; the store will then be closed and files will be deleted from disk at some point but this can take a while. This will also take some slots in the SNAPSHOT thread pool too. The Recovery API won't show any files actively being recovered, the only notable indicator would be the active threads in the SNAPSHOT thread pool.\nThis pull request adds a check before reading a file to restore and before writing bytes on disk so that a closing store can be detected more quickly and the file recovery process aborted. This way the file restores just stops and for most of the repository implementations it means that no more bytes are read (see #62370 for S3), finishing threads in the SNAPSHOT thread pool more quickly too.", "createdAt": "2020-09-16T10:08:05Z", "url": "https://github.com/elastic/elasticsearch/pull/62441", "merged": true, "mergeCommit": {"oid": "fe2ce8a3fd36b83d73a669a021c9f93e1d1d5bed"}, "closed": true, "closedAt": "2020-09-18T08:51:22Z", "author": {"login": "tlrx"}, "timelineItems": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdJZNsKAH2gAyNDg3ODc4MDgzOjE2ODNkNzVhYjU2OTU5ZjNlYmQ4ZmNlMjEwZmM5OTE0ZmE5YTIyNTU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdKAmp9AH2gAyNDg3ODc4MDgzOmVlYjM5OWVmN2E3ZmNlNzgwMTQ0Nzc3NDIyN2QyYjZiN2EzNGU2ZTI=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "1683d75ab56959f3ebd8fce210fc9914fa9a2255", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/1683d75ab56959f3ebd8fce210fc9914fa9a2255", "committedDate": "2020-09-16T09:45:08Z", "message": "Also abort ongoing file restores when snapshot restore is aborted"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a7c76e0dcaa5a572a38ad721a33109979938210a", "author": {"user": {"login": "elasticmachine", "name": "Elastic Machine"}}, "url": "https://github.com/elastic/elasticsearch/commit/a7c76e0dcaa5a572a38ad721a33109979938210a", "committedDate": "2020-09-16T12:16:29Z", "message": "Merge branch 'master' into abort-file-restores-when-restore-is-aborted"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg5NTU3NjI0", "url": "https://github.com/elastic/elasticsearch/pull/62441#pullrequestreview-489557624", "createdAt": "2020-09-16T12:09:44Z", "commit": {"oid": "1683d75ab56959f3ebd8fce210fc9914fa9a2255"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQxMjowOTo0NFrOHStxkg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQxMjoyNToxNlrOHSuUkA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTM4NjM4Ng==", "bodyText": "I'd reword this a little:\n/**\n * @return true if the {@link Store#close()} method has been called. This indicates that the current \n * store is either closed or being closed waiting for all references to it to be released. \n * You might prefer to use {@link Store#ensureOpen()} instead.\n */", "url": "https://github.com/elastic/elasticsearch/pull/62441#discussion_r489386386", "createdAt": "2020-09-16T12:09:44Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/index/store/Store.java", "diffHunk": "@@ -403,14 +403,21 @@ public final void decRef() {\n \n     @Override\n     public void close() {\n-\n         if (isClosed.compareAndSet(false, true)) {\n             // only do this once!\n             decRef();\n             logger.debug(\"store reference count on close: {}\", refCounter.refCount());\n         }\n     }\n \n+    /**\n+     * @return true if the {@link Store#close()} method has been called indicating that the current store is being closed but potentially\n+     * not yet fully closed and released. You might prefer to use {@link Store#ensureOpen()} instead.\n+     */\n+    public boolean isClosing() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1683d75ab56959f3ebd8fce210fc9914fa9a2255"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTM4ODQ4NA==", "bodyText": "See below comment: I think you can just use createRepository(repositoryName, BlockingDataFileReadsRepository.TYPE) if you check the store on every chunk read?", "url": "https://github.com/elastic/elasticsearch/pull/62441#discussion_r489388484", "createdAt": "2020-09-16T12:13:34Z", "author": {"login": "original-brownbear"}, "path": "server/src/internalClusterTest/java/org/elasticsearch/snapshots/AbortedRestoreIT.java", "diffHunk": "@@ -0,0 +1,352 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.snapshots;\n+\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotResponse;\n+import org.elasticsearch.action.admin.indices.recovery.RecoveryResponse;\n+import org.elasticsearch.action.support.IndicesOptions;\n+import org.elasticsearch.cluster.metadata.RepositoryMetadata;\n+import org.elasticsearch.cluster.routing.RecoverySource;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.blobstore.BlobContainer;\n+import org.elasticsearch.common.blobstore.BlobPath;\n+import org.elasticsearch.common.blobstore.BlobStore;\n+import org.elasticsearch.common.blobstore.support.FilterBlobContainer;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.env.Environment;\n+import org.elasticsearch.indices.recovery.RecoverySettings;\n+import org.elasticsearch.indices.recovery.RecoveryState;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.plugins.RepositoryPlugin;\n+import org.elasticsearch.repositories.RepositoriesService;\n+import org.elasticsearch.repositories.fs.FsRepository;\n+import org.elasticsearch.snapshots.AbortedRestoreIT.BlockingDataFileReadsRepository.BlockingInputStream;\n+import org.elasticsearch.snapshots.mockstore.BlobStoreWrapper;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.threadpool.ThreadPoolStats;\n+\n+import java.io.FilterInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Stream;\n+\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.hamcrest.Matchers.allOf;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.is;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+public class AbortedRestoreIT extends AbstractSnapshotIntegTestCase {\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        final Collection<Class<? extends Plugin>> plugins = new ArrayList<>(super.nodePlugins());\n+        plugins.add(BlockingDataFileReadsRepository.Plugin.class);\n+        return plugins;\n+    }\n+\n+    public void testAbortedRestoreAlsoAbortFileRestores() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+\n+        final String indexName = \"test-abort-restore\";\n+        final int numPrimaries = randomIntBetween(1, 3);\n+        createIndex(indexName, indexSettingsNoReplicas(numPrimaries).build());\n+        indexRandomDocs(indexName, scaledRandomIntBetween(10, 1_000));\n+        ensureGreen();\n+        forceMerge();\n+\n+        final String repositoryName = \"repository\";\n+        createRepository(repositoryName, BlockingDataFileReadsRepository.TYPE, Settings.builder().put(\"location\", randomRepoPath()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1683d75ab56959f3ebd8fce210fc9914fa9a2255"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTM5MTgwMA==", "bodyText": "Maybe just override org.elasticsearch.snapshots.mockstore.MockRepository#blockOnDataFiles and add a method to verify no more interactions (could just add a flag that throws an AssertionError past a certain point)? Then you can just wait for the snapshot threads to all block, unblock and wait for the snapshot pool to be empty. Much less new test code to maintain and tests the same thing pretty much?", "url": "https://github.com/elastic/elasticsearch/pull/62441#discussion_r489391800", "createdAt": "2020-09-16T12:19:16Z", "author": {"login": "original-brownbear"}, "path": "server/src/internalClusterTest/java/org/elasticsearch/snapshots/AbortedRestoreIT.java", "diffHunk": "@@ -0,0 +1,352 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.snapshots;\n+\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotResponse;\n+import org.elasticsearch.action.admin.indices.recovery.RecoveryResponse;\n+import org.elasticsearch.action.support.IndicesOptions;\n+import org.elasticsearch.cluster.metadata.RepositoryMetadata;\n+import org.elasticsearch.cluster.routing.RecoverySource;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.blobstore.BlobContainer;\n+import org.elasticsearch.common.blobstore.BlobPath;\n+import org.elasticsearch.common.blobstore.BlobStore;\n+import org.elasticsearch.common.blobstore.support.FilterBlobContainer;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.env.Environment;\n+import org.elasticsearch.indices.recovery.RecoverySettings;\n+import org.elasticsearch.indices.recovery.RecoveryState;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.plugins.RepositoryPlugin;\n+import org.elasticsearch.repositories.RepositoriesService;\n+import org.elasticsearch.repositories.fs.FsRepository;\n+import org.elasticsearch.snapshots.AbortedRestoreIT.BlockingDataFileReadsRepository.BlockingInputStream;\n+import org.elasticsearch.snapshots.mockstore.BlobStoreWrapper;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.threadpool.ThreadPoolStats;\n+\n+import java.io.FilterInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Stream;\n+\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.hamcrest.Matchers.allOf;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.is;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+public class AbortedRestoreIT extends AbstractSnapshotIntegTestCase {\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        final Collection<Class<? extends Plugin>> plugins = new ArrayList<>(super.nodePlugins());\n+        plugins.add(BlockingDataFileReadsRepository.Plugin.class);\n+        return plugins;\n+    }\n+\n+    public void testAbortedRestoreAlsoAbortFileRestores() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+\n+        final String indexName = \"test-abort-restore\";\n+        final int numPrimaries = randomIntBetween(1, 3);\n+        createIndex(indexName, indexSettingsNoReplicas(numPrimaries).build());\n+        indexRandomDocs(indexName, scaledRandomIntBetween(10, 1_000));\n+        ensureGreen();\n+        forceMerge();\n+\n+        final String repositoryName = \"repository\";\n+        createRepository(repositoryName, BlockingDataFileReadsRepository.TYPE, Settings.builder().put(\"location\", randomRepoPath()));\n+\n+        final String snapshotName = \"snapshot\";\n+        createFullSnapshot(repositoryName, snapshotName);\n+        assertAcked(client().admin().indices().prepareDelete(indexName));\n+\n+        logger.info(\"--> blocking node on data files [{}] before restore\", dataNode);\n+        final BlockingDataFileReadsRepository blockRepository = ((BlockingDataFileReadsRepository) internalCluster()\n+            .getInstance(RepositoriesService.class, dataNode)\n+            .repository(repositoryName));\n+        blockRepository.block();\n+\n+        logger.info(\"--> starting restore\");\n+        final ActionFuture<RestoreSnapshotResponse> future = client().admin().cluster().prepareRestoreSnapshot(repositoryName, snapshotName)\n+            .setWaitForCompletion(true)\n+            .setIndices(indexName)\n+            .execute();\n+\n+        assertBusy(() -> {\n+            final RecoveryResponse recoveries = client().admin().indices().prepareRecoveries(indexName)\n+                .setIndicesOptions(IndicesOptions.LENIENT_EXPAND_OPEN).setActiveOnly(true).get();\n+            assertThat(recoveries.hasRecoveries(), is(true));\n+            final List<RecoveryState> shardRecoveries = recoveries.shardRecoveryStates().get(indexName);\n+            assertThat(shardRecoveries, hasSize(numPrimaries));\n+            assertThat(future.isDone(), is(false));\n+\n+            for (RecoveryState shardRecovery : shardRecoveries) {\n+                assertThat(shardRecovery.getRecoverySource().getType(), equalTo(RecoverySource.Type.SNAPSHOT));\n+                assertThat(shardRecovery.getStage(), equalTo(RecoveryState.Stage.INDEX));\n+            }\n+        });\n+\n+        logger.info(\"--> waiting for snapshot thread pool to be full\");\n+        assertBusy(() -> {\n+            ThreadPool threadPool = internalCluster().getInstance(ClusterService.class, dataNode).getClusterApplierService().threadPool();\n+            int activeSnapshotThreads = -1;\n+            for (ThreadPoolStats.Stats threadPoolStats : threadPool.stats()) {\n+                if (threadPoolStats.getName().equals(ThreadPool.Names.SNAPSHOT)) {\n+                    activeSnapshotThreads = threadPoolStats.getActive();\n+                    break;\n+                }\n+            }\n+            final ThreadPool.Info threadPoolInfo = threadPool.info(ThreadPool.Names.SNAPSHOT);\n+            assertThat(activeSnapshotThreads, allOf(greaterThan(0), equalTo(threadPoolInfo.getMax())));\n+            assertThat(blockRepository.streams().filter(BlockingInputStream::isBlocked).count(), equalTo((long) activeSnapshotThreads));\n+        }, 30L, TimeUnit.SECONDS);\n+\n+        logger.info(\"--> aborting restore by deleting the index\");\n+        assertAcked(client().admin().indices().prepareDelete(indexName));\n+\n+        // Total number of blobs that have been opened from the blob store\n+        final long totalBlobsRead = blockRepository.streams().count();\n+\n+        // Total number of bytes that have been read from the blob store\n+        final long totalBytesRead = blockRepository.streams().mapToLong(BlockingInputStream::getCount).sum();\n+\n+        logger.info(\"--> unblocking node [{}]\", dataNode);\n+        blockRepository.unblock();\n+        assertThat(blockRepository.streams().noneMatch(BlockingInputStream::isBlocked), is(true));\n+\n+        logger.info(\"--> restore should have failed\");\n+        final RestoreSnapshotResponse restoreSnapshotResponse = future.get();\n+        assertThat(restoreSnapshotResponse.getRestoreInfo().failedShards(), equalTo(numPrimaries));\n+        assertThat(restoreSnapshotResponse.getRestoreInfo().successfulShards(), equalTo(0));\n+\n+        logger.info(\"--> waiting for snapshot thread pool to be empty\");\n+        assertBusy(() -> {\n+            ThreadPool threadPool = internalCluster().getInstance(ClusterService.class, dataNode).getClusterApplierService().threadPool();\n+            int activeSnapshotThreads = -1;\n+            for (ThreadPoolStats.Stats threadPoolStats : threadPool.stats()) {\n+                if (threadPoolStats.getName().equals(ThreadPool.Names.SNAPSHOT)) {\n+                    activeSnapshotThreads = threadPoolStats.getActive();\n+                    break;\n+                }\n+            }\n+            assertThat(activeSnapshotThreads, equalTo(0));\n+            assertThat(blockRepository.streams().filter(BlockingInputStream::isBlocked).count(), equalTo(0L));\n+        }, 30L, TimeUnit.SECONDS);\n+\n+        assertThat(\"No more blobs should have been opened from the blob store\",\n+            blockRepository.streams().count(), equalTo(totalBlobsRead));\n+        assertThat(\"No more bytes should have been read from the blob store\",\n+            blockRepository.streams().mapToLong(BlockingInputStream::getCount).sum(), equalTo(totalBytesRead));\n+    }\n+\n+    /**\n+     * A blob store repository that blocks read operations on {@link InputStream} when the {@code blockStreams} flag\n+     * is set to true. It also keep track of the number of bytes read from {@link InputStream} it opens.\n+     */\n+    public static class BlockingDataFileReadsRepository extends FsRepository {\n+\n+        static final String TYPE = \"block_on_data_file_reads\";\n+\n+        private final List<BlockingInputStream> streams = Collections.synchronizedList(new ArrayList<>());\n+        private volatile boolean blockStreams = false;\n+\n+        public BlockingDataFileReadsRepository(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1683d75ab56959f3ebd8fce210fc9914fa9a2255"}, "originalPosition": 186}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTM5NTM0NA==", "bodyText": "Maybe put ensureNotClosing here as well? Then you can run your test with random chunk_size as well I think?", "url": "https://github.com/elastic/elasticsearch/pull/62441#discussion_r489395344", "createdAt": "2020-09-16T12:25:16Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -2101,9 +2107,10 @@ protected InputStream openSlice(int slice) throws IOException {\n                                     return container.readBlob(fileInfo.partName(slice));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7c76e0dcaa5a572a38ad721a33109979938210a"}, "originalPosition": 37}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7e350b94462da2652444cff70bcb6c21ba949cee", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/7e350b94462da2652444cff70bcb6c21ba949cee", "committedDate": "2020-09-17T10:36:12Z", "message": "feedback"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkwNjEzMTY1", "url": "https://github.com/elastic/elasticsearch/pull/62441#pullrequestreview-490613165", "createdAt": "2020-09-17T13:47:58Z", "commit": {"oid": "7e350b94462da2652444cff70bcb6c21ba949cee"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xN1QxMzo0Nzo1OVrOHTjGHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xN1QxMzo1NDo0MVrOHTjZ5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDI1OTk5OA==", "bodyText": "Do we still need this override (and associated changes)? It seems with recent changes we're not doing any more partial reading of data blobs? If so I think this whole class/plugin can go away?", "url": "https://github.com/elastic/elasticsearch/pull/62441#discussion_r490259998", "createdAt": "2020-09-17T13:47:59Z", "author": {"login": "original-brownbear"}, "path": "server/src/internalClusterTest/java/org/elasticsearch/snapshots/AbortedRestoreIT.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.snapshots;\n+\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotResponse;\n+import org.elasticsearch.action.admin.indices.recovery.RecoveryResponse;\n+import org.elasticsearch.action.support.IndicesOptions;\n+import org.elasticsearch.cluster.metadata.RepositoryMetadata;\n+import org.elasticsearch.cluster.routing.RecoverySource;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.env.Environment;\n+import org.elasticsearch.indices.recovery.RecoverySettings;\n+import org.elasticsearch.indices.recovery.RecoveryState;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.plugins.RepositoryPlugin;\n+import org.elasticsearch.snapshots.mockstore.MockRepository;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.threadpool.ThreadPoolStats;\n+import org.hamcrest.Matcher;\n+\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.StreamSupport;\n+\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.is;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+public class AbortedRestoreIT extends AbstractSnapshotIntegTestCase {\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        final Collection<Class<? extends Plugin>> plugins = new ArrayList<>(super.nodePlugins());\n+        plugins.add(BlockingDataFileReadsRepository.Plugin.class);\n+        return plugins;\n+    }\n+\n+    public void testAbortedRestoreAlsoAbortFileRestores() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+\n+        final String indexName = \"test-abort-restore\";\n+        createIndex(indexName, indexSettingsNoReplicas(1).build());\n+        indexRandomDocs(indexName, scaledRandomIntBetween(10, 1_000));\n+        ensureGreen();\n+        forceMerge();\n+\n+        final String repositoryName = \"repository\";\n+        createRepository(repositoryName, BlockingDataFileReadsRepository.TYPE,\n+            Settings.builder()\n+                .put(randomRepositorySettings().build())\n+                .put(\"fail_reads_after_unblock\", true));\n+\n+        final String snapshotName = \"snapshot\";\n+        createFullSnapshot(repositoryName, snapshotName);\n+        assertAcked(client().admin().indices().prepareDelete(indexName));\n+\n+        logger.info(\"--> blocking all data nodes for repository [{}]\", repositoryName);\n+        blockAllDataNodes(repositoryName);\n+\n+        logger.info(\"--> starting restore\");\n+        final ActionFuture<RestoreSnapshotResponse> future = client().admin().cluster().prepareRestoreSnapshot(repositoryName, snapshotName)\n+            .setWaitForCompletion(true)\n+            .setIndices(indexName)\n+            .execute();\n+\n+        assertBusy(() -> {\n+            final RecoveryResponse recoveries = client().admin().indices().prepareRecoveries(indexName)\n+                .setIndicesOptions(IndicesOptions.LENIENT_EXPAND_OPEN).setActiveOnly(true).get();\n+            assertThat(recoveries.hasRecoveries(), is(true));\n+            final List<RecoveryState> shardRecoveries = recoveries.shardRecoveryStates().get(indexName);\n+            assertThat(shardRecoveries, hasSize(1));\n+            assertThat(future.isDone(), is(false));\n+\n+            for (RecoveryState shardRecovery : shardRecoveries) {\n+                assertThat(shardRecovery.getRecoverySource().getType(), equalTo(RecoverySource.Type.SNAPSHOT));\n+                assertThat(shardRecovery.getStage(), equalTo(RecoveryState.Stage.INDEX));\n+            }\n+        });\n+\n+        final ThreadPool.Info snapshotThreadPoolInfo = threadPool(dataNode).info(ThreadPool.Names.SNAPSHOT);\n+        assertThat(snapshotThreadPoolInfo.getMax(), greaterThan(0));\n+\n+        logger.info(\"--> waiting for snapshot thread [max={}] pool to be full\", snapshotThreadPoolInfo.getMax());\n+        waitForMaxActiveSnapshotThreads(dataNode, equalTo(snapshotThreadPoolInfo.getMax()));\n+\n+        logger.info(\"--> aborting restore by deleting the index\");\n+        assertAcked(client().admin().indices().prepareDelete(indexName));\n+\n+        logger.info(\"--> unblocking repository [{}]\", repositoryName);\n+        unblockAllDataNodes(repositoryName);\n+\n+        logger.info(\"--> restore should have failed\");\n+        final RestoreSnapshotResponse restoreSnapshotResponse = future.get();\n+        assertThat(restoreSnapshotResponse.getRestoreInfo().failedShards(), equalTo(1));\n+        assertThat(restoreSnapshotResponse.getRestoreInfo().successfulShards(), equalTo(0));\n+\n+        logger.info(\"--> waiting for snapshot thread pool to be empty\");\n+        waitForMaxActiveSnapshotThreads(dataNode, equalTo(0));\n+    }\n+\n+    private static void waitForMaxActiveSnapshotThreads(final String node, final Matcher<Integer> matcher) throws Exception {\n+        assertBusy(() -> assertThat(threadPoolStats(node, ThreadPool.Names.SNAPSHOT).getActive(), matcher), 30L, TimeUnit.SECONDS);\n+    }\n+\n+    private static ThreadPool threadPool(final String node) {\n+        return internalCluster().getInstance(ClusterService.class, node).getClusterApplierService().threadPool();\n+    }\n+\n+    private static ThreadPoolStats.Stats threadPoolStats(final String node, final String threadPoolName) {\n+        return StreamSupport.stream(threadPool(node).stats().spliterator(), false)\n+            .filter(threadPool -> threadPool.getName().equals(threadPoolName))\n+            .findFirst()\n+            .orElseThrow(() -> new AssertionError(\"Failed to find thread pool \" + threadPoolName));\n+    }\n+\n+    /**\n+     * A blob store repository that blocks read operations on {@link InputStream} when the {@code blockStreams} flag\n+     * is set to true. It also keep track of the number of bytes read from {@link InputStream} it opens.\n+     */\n+    public static class BlockingDataFileReadsRepository extends MockRepository {\n+\n+        static final String TYPE = \"block_on_data_file_reads\";\n+\n+        public BlockingDataFileReadsRepository(\n+            RepositoryMetadata metadata,\n+            Environment environment,\n+            NamedXContentRegistry namedXContentRegistry,\n+            ClusterService clusterService,\n+            RecoverySettings recoverySettings\n+        ) {\n+            super(metadata, environment, namedXContentRegistry, clusterService, recoverySettings);\n+        }\n+\n+        @Override\n+        protected int bufferSize() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7e350b94462da2652444cff70bcb6c21ba949cee"}, "originalPosition": 165}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDI2NTA2Mw==", "bodyText": "Could we make this less general and more in line with existing usage of the MockRepository and maybe just add another setter for this field? Hiding this functionality behind a setting makes it much hard for others that might have use for it to find it (or for us to remove it once it's unused ... many of the settings we currently have here seem unused already ... I'll open a PR for dealing with those soon).", "url": "https://github.com/elastic/elasticsearch/pull/62441#discussion_r490265063", "createdAt": "2020-09-17T13:54:41Z", "author": {"login": "original-brownbear"}, "path": "test/framework/src/main/java/org/elasticsearch/snapshots/mockstore/MockRepository.java", "diffHunk": "@@ -138,6 +144,7 @@ public MockRepository(RepositoryMetadata metadata, Environment environment,\n         blockAndFailOnWriteSnapFile = metadata.settings().getAsBoolean(\"block_on_snap\", false);\n         randomPrefix = metadata.settings().get(\"random\", \"default\");\n         waitAfterUnblock = metadata.settings().getAsLong(\"wait_after_unblock\", 0L);\n+        failReadsAfterUnblock = metadata.settings().getAsBoolean(\"fail_reads_after_unblock\", false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7e350b94462da2652444cff70bcb6c21ba949cee"}, "originalPosition": 17}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0699acf09f766e660c44d382be05cbf28200b475", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/0699acf09f766e660c44d382be05cbf28200b475", "committedDate": "2020-09-17T14:21:55Z", "message": "Add test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "068e297b5d4a76499f6eaaae88191ef67ee7e6a1", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/068e297b5d4a76499f6eaaae88191ef67ee7e6a1", "committedDate": "2020-09-17T14:25:09Z", "message": "revert buffersize"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkwNjU4ODg1", "url": "https://github.com/elastic/elasticsearch/pull/62441#pullrequestreview-490658885", "createdAt": "2020-09-17T14:28:49Z", "commit": {"oid": "068e297b5d4a76499f6eaaae88191ef67ee7e6a1"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "725ce72122b6c857cdacb718920d271e502d0c83", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/725ce72122b6c857cdacb718920d271e502d0c83", "committedDate": "2020-09-18T07:38:22Z", "message": "Merge branch 'master' into abort-file-restores-when-restore-is-aborted"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "eeb399ef7a7fce7801447774227d2b6b7a34e6e2", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/eeb399ef7a7fce7801447774227d2b6b7a34e6e2", "committedDate": "2020-09-18T07:38:42Z", "message": "import"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4475, "cost": 1, "resetAt": "2021-10-28T19:08:13Z"}}}