{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDEzMzYzNTg4", "number": 56175, "reviewThreads": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQwODoxODowN1rOD5TKqA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQxNDoxNTo1MlrOD7Qiow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxNDA5NDQ4OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQwODoxODowN1rOGQfATw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQwOTowNTo1NVrOGQgoVg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTkzODM4Mw==", "bodyText": "We should try to read directly into the ByteBuffer instead of materializing a byte[]?", "url": "https://github.com/elastic/elasticsearch/pull/56175#discussion_r419938383", "createdAt": "2020-05-05T08:18:07Z", "author": {"login": "jpountz"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -319,9 +317,16 @@ private boolean assertRangeIsAlignedWithPart(Tuple<Long, Long> range) {\n         return true;\n     }\n \n-    private int readCacheFile(FileChannel fc, long end, long position, byte[] buffer, int offset, long length) throws IOException {\n+    private int readCacheFile(FileChannel fc, long end, long position, ByteBuffer b, long length) throws IOException {\n         assert assertFileChannelOpen(fc);\n-        int bytesRead = Channels.readFromFileChannel(fc, position, buffer, offset, Math.toIntExact(Math.min(length, end - position)));\n+        int bytesRead;\n+        if (end - position < b.remaining()) {\n+            int l = Math.toIntExact(end - position);\n+            b.put(Channels.readFromFileChannel(fc, position, l));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70c5bde889030c1014eb503756b77ea6ce72f1e2"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTk2NTAxNA==", "bodyText": "If I recall correctly if the size of the data is lower that the remaining size of the byte buffer you get an EOF exception. We are only materialising to a byte[] in that case. Maybe there is a better way?", "url": "https://github.com/elastic/elasticsearch/pull/56175#discussion_r419965014", "createdAt": "2020-05-05T09:05:55Z", "author": {"login": "iverase"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -319,9 +317,16 @@ private boolean assertRangeIsAlignedWithPart(Tuple<Long, Long> range) {\n         return true;\n     }\n \n-    private int readCacheFile(FileChannel fc, long end, long position, byte[] buffer, int offset, long length) throws IOException {\n+    private int readCacheFile(FileChannel fc, long end, long position, ByteBuffer b, long length) throws IOException {\n         assert assertFileChannelOpen(fc);\n-        int bytesRead = Channels.readFromFileChannel(fc, position, buffer, offset, Math.toIntExact(Math.min(length, end - position)));\n+        int bytesRead;\n+        if (end - position < b.remaining()) {\n+            int l = Math.toIntExact(end - position);\n+            b.put(Channels.readFromFileChannel(fc, position, l));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTkzODM4Mw=="}, "originalCommit": {"oid": "70c5bde889030c1014eb503756b77ea6ce72f1e2"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxNTM2NTkzOnYy", "diffSide": "LEFT", "path": "qa/evil-tests/src/test/java/org/elasticsearch/index/engine/EvilInternalEngineTests.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxNDowOTozMVrOGQrPSg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxNToxNDozN1rOGQuSNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDEzODgyNg==", "bodyText": "After removing this, the test times out. I have not found a way to do this in Lucene 8.6 as this is now private in the Index writer. @original-brownbear you were the last one to modify this class, you might have an idea what we can do here?", "url": "https://github.com/elastic/elasticsearch/pull/56175#discussion_r420138826", "createdAt": "2020-05-05T14:09:31Z", "author": {"login": "iverase"}, "path": "qa/evil-tests/src/test/java/org/elasticsearch/index/engine/EvilInternalEngineTests.java", "diffHunk": "@@ -64,21 +64,6 @@ public void testOutOfMemoryErrorWhileMergingIsRethrownAndIsUncaught() throws IOE\n                         public void merge(final MergePolicy.OneMerge merge) throws IOException {\n                             throw new OutOfMemoryError(\"640K ought to be enough for anybody\");\n                         }\n-\n-                        @Override\n-                        public synchronized MergePolicy.OneMerge getNextMerge() {\n-                            /*\n-                             * This will be called when we flush when we will not be ready to return the segments. After the segments are on\n-                             * disk, we can only return them from here once or the merge scheduler will be stuck in a loop repeatedly\n-                             * peeling off the same segments to schedule for merging.\n-                             */\n-                            if (segmentsReference.get() == null) {\n-                                return super.getNextMerge();\n-                            } else {\n-                                final List<SegmentCommitInfo> segments = segmentsReference.getAndSet(null);\n-                                return new MergePolicy.OneMerge(segments);\n-                            }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20ec5f7087604f51ebfe9148db5f421c7d526ce6"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDE4ODcyNw==", "bodyText": "The problem is that we need to somehow set a merge policy here that will actually trigger a background merge (the random policy used in this test triggers nothing which is kind of obvious when we only have a single document in the engine).\n=> As far as I understand it we need to craft a merge policy that will actually cause a background merge", "url": "https://github.com/elastic/elasticsearch/pull/56175#discussion_r420188727", "createdAt": "2020-05-05T15:14:37Z", "author": {"login": "original-brownbear"}, "path": "qa/evil-tests/src/test/java/org/elasticsearch/index/engine/EvilInternalEngineTests.java", "diffHunk": "@@ -64,21 +64,6 @@ public void testOutOfMemoryErrorWhileMergingIsRethrownAndIsUncaught() throws IOE\n                         public void merge(final MergePolicy.OneMerge merge) throws IOException {\n                             throw new OutOfMemoryError(\"640K ought to be enough for anybody\");\n                         }\n-\n-                        @Override\n-                        public synchronized MergePolicy.OneMerge getNextMerge() {\n-                            /*\n-                             * This will be called when we flush when we will not be ready to return the segments. After the segments are on\n-                             * disk, we can only return them from here once or the merge scheduler will be stuck in a loop repeatedly\n-                             * peeling off the same segments to schedule for merging.\n-                             */\n-                            if (segmentsReference.get() == null) {\n-                                return super.getNextMerge();\n-                            } else {\n-                                final List<SegmentCommitInfo> segments = segmentsReference.getAndSet(null);\n-                                return new MergePolicy.OneMerge(segments);\n-                            }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDEzODgyNg=="}, "originalCommit": {"oid": "20ec5f7087604f51ebfe9148db5f421c7d526ce6"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxNTUwMzU5OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/snapshots/SourceOnlySnapshot.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxNDozOToxMVrOGQsnYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxNDozOToxMVrOGQsnYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDE2MTM3OA==", "bodyText": "nit: no need to break the line", "url": "https://github.com/elastic/elasticsearch/pull/56175#discussion_r420161378", "createdAt": "2020-05-05T14:39:11Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/snapshots/SourceOnlySnapshot.java", "diffHunk": "@@ -219,7 +218,8 @@ private SegmentCommitInfo syncSegment(SegmentCommitInfo segmentCommitInfo, LiveD\n                 SegmentInfo newSegmentInfo = new SegmentInfo(targetDirectory, si.getVersion(), si.getMinVersion(), si.name, si.maxDoc(),\n                     false, si.getCodec(), si.getDiagnostics(), si.getId(), si.getAttributes(), null);\n                 // we drop the sort on purpose since the field we sorted on doesn't exist in the target index anymore.\n-                newInfo = new SegmentCommitInfo(newSegmentInfo, 0, 0, -1, -1, -1);\n+                newInfo = new SegmentCommitInfo(newSegmentInfo, 0, 0, -1,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20ec5f7087604f51ebfe9148db5f421c7d526ce6"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxNTU4NjMwOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxNDo1NjoxNlrOGQtcVw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxNDo1NjoxNlrOGQtcVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDE3NDkzNQ==", "bodyText": "This should be final", "url": "https://github.com/elastic/elasticsearch/pull/56175#discussion_r420174935", "createdAt": "2020-05-05T14:56:16Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -128,16 +128,14 @@ private CacheFile getCacheFileSafe() throws Exception {\n     }\n \n     @Override\n-    protected void readInternal(final byte[] buffer, final int offset, final int length) throws IOException {\n+    protected void readInternal(ByteBuffer b) throws IOException {\n         ensureContext(ctx -> ctx != CACHE_WARMING_CONTEXT);\n         final long position = getFilePointer() + this.offset;\n-\n+        int length = b.remaining();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20ec5f7087604f51ebfe9148db5f421c7d526ce6"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxODU0NDkwOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQwODo0NzoyNlrOGRJhrw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQwOTo1Njo0OFrOGRL3Xg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDYzNTA1NQ==", "bodyText": "This does not restore the original limit if something goes wrong while reading the channel. I think we should duplicate the ByteBuffer first (by using the duplicate() method) and sets a new limit on the duplicate, and only update the position if the read on the duplicate succeed. WDYT?", "url": "https://github.com/elastic/elasticsearch/pull/56175#discussion_r420635055", "createdAt": "2020-05-06T08:47:26Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -319,9 +317,18 @@ private boolean assertRangeIsAlignedWithPart(Tuple<Long, Long> range) {\n         return true;\n     }\n \n-    private int readCacheFile(FileChannel fc, long end, long position, byte[] buffer, int offset, long length) throws IOException {\n+    private int readCacheFile(FileChannel fc, long end, long position, ByteBuffer b) throws IOException {\n         assert assertFileChannelOpen(fc);\n-        int bytesRead = Channels.readFromFileChannel(fc, position, buffer, offset, Math.toIntExact(Math.min(length, end - position)));\n+        final int bytesRead;\n+        if (end - position < b.remaining()) {\n+            final int originalLimit = b.limit();\n+            b.limit(b.position() + Math.toIntExact(end - position));\n+            bytesRead = Channels.readFromFileChannel(fc, position, b);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "835b783d42568f6aba38a9a8a8dfce494b694b42"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDYzNjM2OQ==", "bodyText": "I also think that removing the length method parameter changes the meaning of the EOFException message", "url": "https://github.com/elastic/elasticsearch/pull/56175#discussion_r420636369", "createdAt": "2020-05-06T08:49:54Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -319,9 +317,18 @@ private boolean assertRangeIsAlignedWithPart(Tuple<Long, Long> range) {\n         return true;\n     }\n \n-    private int readCacheFile(FileChannel fc, long end, long position, byte[] buffer, int offset, long length) throws IOException {\n+    private int readCacheFile(FileChannel fc, long end, long position, ByteBuffer b) throws IOException {\n         assert assertFileChannelOpen(fc);\n-        int bytesRead = Channels.readFromFileChannel(fc, position, buffer, offset, Math.toIntExact(Math.min(length, end - position)));\n+        final int bytesRead;\n+        if (end - position < b.remaining()) {\n+            final int originalLimit = b.limit();\n+            b.limit(b.position() + Math.toIntExact(end - position));\n+            bytesRead = Channels.readFromFileChannel(fc, position, b);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDYzNTA1NQ=="}, "originalCommit": {"oid": "835b783d42568f6aba38a9a8a8dfce494b694b42"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDY3MzM3NA==", "bodyText": "I think that was a great catch, I duplicate the buffer in the case we are reading partial buffers and change the position of the original buffer if reading the file was successful. I revert back the change of the length as well.", "url": "https://github.com/elastic/elasticsearch/pull/56175#discussion_r420673374", "createdAt": "2020-05-06T09:56:48Z", "author": {"login": "iverase"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -319,9 +317,18 @@ private boolean assertRangeIsAlignedWithPart(Tuple<Long, Long> range) {\n         return true;\n     }\n \n-    private int readCacheFile(FileChannel fc, long end, long position, byte[] buffer, int offset, long length) throws IOException {\n+    private int readCacheFile(FileChannel fc, long end, long position, ByteBuffer b) throws IOException {\n         assert assertFileChannelOpen(fc);\n-        int bytesRead = Channels.readFromFileChannel(fc, position, buffer, offset, Math.toIntExact(Math.min(length, end - position)));\n+        final int bytesRead;\n+        if (end - position < b.remaining()) {\n+            final int originalLimit = b.limit();\n+            b.limit(b.position() + Math.toIntExact(end - position));\n+            bytesRead = Channels.readFromFileChannel(fc, position, b);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDYzNTA1NQ=="}, "originalCommit": {"oid": "835b783d42568f6aba38a9a8a8dfce494b694b42"}, "originalPosition": 49}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzNDYzNTg3OnYy", "diffSide": "RIGHT", "path": "server/src/test/java/org/elasticsearch/search/query/QueryPhaseTests.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQxNDoxNTo1MlrOGTeOsA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwOTo0ODowOVrOGT-4iw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzA3MTQwOA==", "bodyText": "This part feels wrong, we should instead use trackTotalHitsUpTo() or fix assertSortResults to no longer fail if the expected hit count is greater than the actual hit count and relation=gte? @mayya-sharipova @jimczi opinions?", "url": "https://github.com/elastic/elasticsearch/pull/56175#discussion_r423071408", "createdAt": "2020-05-11T14:15:52Z", "author": {"login": "jpountz"}, "path": "server/src/test/java/org/elasticsearch/search/query/QueryPhaseTests.java", "diffHunk": "@@ -675,7 +675,7 @@ public void testNumericLongOrDateSortOptimization() throws Exception {\n         searchContext.sort(sortAndFormats);\n         searchContext.parsedQuery(new ParsedQuery(new MatchAllDocsQuery()));\n         searchContext.setTask(new SearchShardTask(123L, \"\", \"\", \"\", null, Collections.emptyMap()));\n-        searchContext.setSize(10);\n+        searchContext.setSize(numDocs);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c2046d19ff52c9502c75ab95b1b97bf612546bf"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzUxNjIxMQ==", "bodyText": "Agreed, I think we should fix assertSortResults since the optimization could be disabled (not at the moment) if trackTotalHitsUpTo is greater than or equals to the number  of the docs in the index ?", "url": "https://github.com/elastic/elasticsearch/pull/56175#discussion_r423516211", "createdAt": "2020-05-12T07:22:59Z", "author": {"login": "jimczi"}, "path": "server/src/test/java/org/elasticsearch/search/query/QueryPhaseTests.java", "diffHunk": "@@ -675,7 +675,7 @@ public void testNumericLongOrDateSortOptimization() throws Exception {\n         searchContext.sort(sortAndFormats);\n         searchContext.parsedQuery(new ParsedQuery(new MatchAllDocsQuery()));\n         searchContext.setTask(new SearchShardTask(123L, \"\", \"\", \"\", null, Collections.emptyMap()));\n-        searchContext.setSize(10);\n+        searchContext.setSize(numDocs);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzA3MTQwOA=="}, "originalCommit": {"oid": "7c2046d19ff52c9502c75ab95b1b97bf612546bf"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzUyMTkwMA==", "bodyText": "I am thinking in fixing it like:\n[CORRECTED]\n  // assert score docs are in order and their number is as expected\n    private void assertSortResults(TopDocs topDocs, long expectedNumDocs, boolean isDoubleSort) {\n        if (topDocs.totalHits.relation == TotalHits.Relation.GREATER_THAN_OR_EQUAL_TO) {\n            assertThat(topDocs.totalHits.value, lessThanOrEqualTo(expectedNumDocs));\n        } else {\n            assertEquals(topDocs.totalHits.value, expectedNumDocs);\n        }\n ......\n}\n\n@jimczi wdyt?", "url": "https://github.com/elastic/elasticsearch/pull/56175#discussion_r423521900", "createdAt": "2020-05-12T07:33:20Z", "author": {"login": "iverase"}, "path": "server/src/test/java/org/elasticsearch/search/query/QueryPhaseTests.java", "diffHunk": "@@ -675,7 +675,7 @@ public void testNumericLongOrDateSortOptimization() throws Exception {\n         searchContext.sort(sortAndFormats);\n         searchContext.parsedQuery(new ParsedQuery(new MatchAllDocsQuery()));\n         searchContext.setTask(new SearchShardTask(123L, \"\", \"\", \"\", null, Collections.emptyMap()));\n-        searchContext.setSize(10);\n+        searchContext.setSize(numDocs);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzA3MTQwOA=="}, "originalCommit": {"oid": "7c2046d19ff52c9502c75ab95b1b97bf612546bf"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzYwNjQxMQ==", "bodyText": "Looks good", "url": "https://github.com/elastic/elasticsearch/pull/56175#discussion_r423606411", "createdAt": "2020-05-12T09:48:09Z", "author": {"login": "jimczi"}, "path": "server/src/test/java/org/elasticsearch/search/query/QueryPhaseTests.java", "diffHunk": "@@ -675,7 +675,7 @@ public void testNumericLongOrDateSortOptimization() throws Exception {\n         searchContext.sort(sortAndFormats);\n         searchContext.parsedQuery(new ParsedQuery(new MatchAllDocsQuery()));\n         searchContext.setTask(new SearchShardTask(123L, \"\", \"\", \"\", null, Collections.emptyMap()));\n-        searchContext.setSize(10);\n+        searchContext.setSize(numDocs);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzA3MTQwOA=="}, "originalCommit": {"oid": "7c2046d19ff52c9502c75ab95b1b97bf612546bf"}, "originalPosition": 16}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2371, "cost": 1, "resetAt": "2021-11-12T13:16:51Z"}}}