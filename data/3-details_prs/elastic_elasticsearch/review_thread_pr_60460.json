{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU5MjUwMTA5", "number": 60460, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwNzozOTo1MlrOEVtIRg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwNzo0NToxMlrOEVtPMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMTk0OTUwOnYy", "diffSide": "RIGHT", "path": "server/src/internalClusterTest/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderIT.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwNzozOTo1MlrOG8m-9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNFQxMjoxMTo0MVrOHNLKJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIwNjQ1Mg==", "bodyText": "why the force-merge?", "url": "https://github.com/elastic/elasticsearch/pull/60460#discussion_r466206452", "createdAt": "2020-08-06T07:39:52Z", "author": {"login": "ywelsch"}, "path": "server/src/internalClusterTest/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderIT.java", "diffHunk": "@@ -0,0 +1,339 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.cluster.routing.allocation.decider;\n+\n+import org.apache.lucene.mockfile.FilterFileStore;\n+import org.apache.lucene.mockfile.FilterFileSystemProvider;\n+import org.apache.lucene.mockfile.FilterPath;\n+import org.apache.lucene.util.Constants;\n+import org.elasticsearch.action.admin.indices.stats.ShardStats;\n+import org.elasticsearch.action.index.IndexRequestBuilder;\n+import org.elasticsearch.cluster.ClusterInfoService;\n+import org.elasticsearch.cluster.InternalClusterInfoService;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexShardRoutingTable;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.routing.ShardRoutingState;\n+import org.elasticsearch.cluster.routing.allocation.DiskThresholdSettings;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Priority;\n+import org.elasticsearch.common.io.PathUtils;\n+import org.elasticsearch.common.io.PathUtilsForTesting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.env.Environment;\n+import org.elasticsearch.env.NodeEnvironment;\n+import org.elasticsearch.monitor.fs.FsService;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalSettingsPlugin;\n+import org.junit.AfterClass;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.nio.file.DirectoryStream;\n+import java.nio.file.FileStore;\n+import java.nio.file.FileSystem;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.NotDirectoryException;\n+import java.nio.file.Path;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.elasticsearch.common.util.concurrent.ConcurrentCollections.newConcurrentMap;\n+import static org.elasticsearch.index.store.Store.INDEX_STORE_STATS_REFRESH_INTERVAL_SETTING;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.hamcrest.Matchers.anyOf;\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.is;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+public class DiskThresholdDeciderIT extends ESIntegTestCase {\n+\n+    private static TestFileSystemProvider fileSystemProvider;\n+\n+    @BeforeClass\n+    public static void installFilesystemProvider() {\n+        assertNull(fileSystemProvider);\n+        fileSystemProvider = new TestFileSystemProvider(PathUtils.getDefaultFileSystem(), createTempDir());\n+        PathUtilsForTesting.installMock(fileSystemProvider.getFileSystem(null));\n+    }\n+\n+    @AfterClass\n+    public static void removeFilesystemProvider() {\n+        fileSystemProvider = null;\n+        // ESIntegTestCase takes care of tearing down the mock file system\n+    }\n+\n+    @Before\n+    public void clearTrackedPaths() throws IOException {\n+        fileSystemProvider.clearTrackedPaths();\n+    }\n+\n+    private static final long WATERMARK_BYTES = new ByteSizeValue(10, ByteSizeUnit.KB).getBytes();\n+\n+    @Override\n+    protected Settings nodeSettings(int nodeOrdinal) {\n+        final Path dataPath = fileSystemProvider.getRootDir().resolve(\"node-\" + nodeOrdinal);\n+        try {\n+            Files.createDirectories(dataPath);\n+        } catch (IOException e) {\n+            throw new AssertionError(\"unexpected\", e);\n+        }\n+        fileSystemProvider.addTrackedPath(dataPath);\n+        return Settings.builder()\n+                .put(super.nodeSettings(nodeOrdinal))\n+                .put(Environment.PATH_DATA_SETTING.getKey(), dataPath)\n+                .put(FsService.ALWAYS_REFRESH_SETTING.getKey(), true)\n+                .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING.getKey(), WATERMARK_BYTES + \"b\")\n+                .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK_SETTING.getKey(), WATERMARK_BYTES + \"b\")\n+                .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_DISK_FLOOD_STAGE_WATERMARK_SETTING.getKey(), \"0b\")\n+                .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_REROUTE_INTERVAL_SETTING.getKey(), \"0ms\")\n+                .build();\n+    }\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        return List.of(InternalSettingsPlugin.class);\n+    }\n+\n+    public void testHighWatermarkNotExceeded() throws InterruptedException {\n+        internalCluster().startMasterOnlyNode();\n+        internalCluster().startDataOnlyNode();\n+        final String dataNodeName = internalCluster().startDataOnlyNode();\n+\n+        final InternalClusterInfoService clusterInfoService\n+                = (InternalClusterInfoService) internalCluster().getMasterNodeInstance(ClusterInfoService.class);\n+        internalCluster().getMasterNodeInstance(ClusterService.class).addListener(event -> clusterInfoService.refresh());\n+\n+        final String dataNode0Id = internalCluster().getInstance(NodeEnvironment.class, dataNodeName).nodeId();\n+        final Path dataNode0Path = internalCluster().getInstance(Environment.class, dataNodeName).dataFiles()[0];\n+\n+        createIndex(\"test\", Settings.builder()\n+                .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 0)\n+                .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 6)\n+                .put(INDEX_STORE_STATS_REFRESH_INTERVAL_SETTING.getKey(), \"0ms\")\n+                .build());\n+        final long minShardSize = createReasonableSizedShards();\n+\n+        // reduce disk size of node 0 so that no shards fit below the high watermark, forcing all shards onto the other data node\n+        // (subtract the translog size since the disk threshold decider ignores this and may therefore move the shard back again)\n+        fileSystemProvider.getTestFileStore(dataNode0Path).setTotalSpace(minShardSize + WATERMARK_BYTES - 1L);\n+        refreshDiskUsage();\n+        assertThat(getShardRoutings(dataNode0Id), empty());\n+\n+        // increase disk size of node 0 to allow just enough room for one shard, and check that it's rebalanced back\n+        fileSystemProvider.getTestFileStore(dataNode0Path).setTotalSpace(minShardSize + WATERMARK_BYTES + 1L);\n+        refreshDiskUsage();\n+        assertThat(getShardRoutings(dataNode0Id), hasSize(1));\n+    }\n+\n+    private Set<ShardRouting> getShardRoutings(String nodeId) {\n+        final Set<ShardRouting> shardRoutings = new HashSet<>();\n+        for (IndexShardRoutingTable indexShardRoutingTable : client().admin().cluster().prepareState().clear().setRoutingTable(true)\n+                .get().getState().getRoutingTable().index(\"test\")) {\n+            for (ShardRouting shard : indexShardRoutingTable.shards()) {\n+                assertThat(shard.state(), equalTo(ShardRoutingState.STARTED));\n+                if (shard.currentNodeId().equals(nodeId)) {\n+                    shardRoutings.add(shard);\n+                }\n+            }\n+        }\n+        return shardRoutings;\n+    }\n+\n+    /**\n+     * Index documents until all the shards are at least WATERMARK_BYTES in size, and return the size of the smallest shard\n+     */\n+    private long createReasonableSizedShards() throws InterruptedException {\n+        while (true) {\n+            final IndexRequestBuilder[] indexRequestBuilders = new IndexRequestBuilder[scaledRandomIntBetween(100, 10000)];\n+            for (int i = 0; i < indexRequestBuilders.length; i++) {\n+                indexRequestBuilders[i] = client().prepareIndex(\"test\").setSource(\"field\", randomAlphaOfLength(10));\n+            }\n+            indexRandom(true, indexRequestBuilders);\n+            forceMerge();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946e2b6e54f0148af499fa495965e99282f69967"}, "originalPosition": 185}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzU3NjM1OA==", "bodyText": "seems not to work otherwise. Not really relevant whether or not we  force-merge, so I left as is", "url": "https://github.com/elastic/elasticsearch/pull/60460#discussion_r483576358", "createdAt": "2020-09-04T12:11:41Z", "author": {"login": "ywelsch"}, "path": "server/src/internalClusterTest/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderIT.java", "diffHunk": "@@ -0,0 +1,339 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.cluster.routing.allocation.decider;\n+\n+import org.apache.lucene.mockfile.FilterFileStore;\n+import org.apache.lucene.mockfile.FilterFileSystemProvider;\n+import org.apache.lucene.mockfile.FilterPath;\n+import org.apache.lucene.util.Constants;\n+import org.elasticsearch.action.admin.indices.stats.ShardStats;\n+import org.elasticsearch.action.index.IndexRequestBuilder;\n+import org.elasticsearch.cluster.ClusterInfoService;\n+import org.elasticsearch.cluster.InternalClusterInfoService;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexShardRoutingTable;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.routing.ShardRoutingState;\n+import org.elasticsearch.cluster.routing.allocation.DiskThresholdSettings;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Priority;\n+import org.elasticsearch.common.io.PathUtils;\n+import org.elasticsearch.common.io.PathUtilsForTesting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.env.Environment;\n+import org.elasticsearch.env.NodeEnvironment;\n+import org.elasticsearch.monitor.fs.FsService;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalSettingsPlugin;\n+import org.junit.AfterClass;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.nio.file.DirectoryStream;\n+import java.nio.file.FileStore;\n+import java.nio.file.FileSystem;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.NotDirectoryException;\n+import java.nio.file.Path;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.elasticsearch.common.util.concurrent.ConcurrentCollections.newConcurrentMap;\n+import static org.elasticsearch.index.store.Store.INDEX_STORE_STATS_REFRESH_INTERVAL_SETTING;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.hamcrest.Matchers.anyOf;\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.is;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+public class DiskThresholdDeciderIT extends ESIntegTestCase {\n+\n+    private static TestFileSystemProvider fileSystemProvider;\n+\n+    @BeforeClass\n+    public static void installFilesystemProvider() {\n+        assertNull(fileSystemProvider);\n+        fileSystemProvider = new TestFileSystemProvider(PathUtils.getDefaultFileSystem(), createTempDir());\n+        PathUtilsForTesting.installMock(fileSystemProvider.getFileSystem(null));\n+    }\n+\n+    @AfterClass\n+    public static void removeFilesystemProvider() {\n+        fileSystemProvider = null;\n+        // ESIntegTestCase takes care of tearing down the mock file system\n+    }\n+\n+    @Before\n+    public void clearTrackedPaths() throws IOException {\n+        fileSystemProvider.clearTrackedPaths();\n+    }\n+\n+    private static final long WATERMARK_BYTES = new ByteSizeValue(10, ByteSizeUnit.KB).getBytes();\n+\n+    @Override\n+    protected Settings nodeSettings(int nodeOrdinal) {\n+        final Path dataPath = fileSystemProvider.getRootDir().resolve(\"node-\" + nodeOrdinal);\n+        try {\n+            Files.createDirectories(dataPath);\n+        } catch (IOException e) {\n+            throw new AssertionError(\"unexpected\", e);\n+        }\n+        fileSystemProvider.addTrackedPath(dataPath);\n+        return Settings.builder()\n+                .put(super.nodeSettings(nodeOrdinal))\n+                .put(Environment.PATH_DATA_SETTING.getKey(), dataPath)\n+                .put(FsService.ALWAYS_REFRESH_SETTING.getKey(), true)\n+                .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING.getKey(), WATERMARK_BYTES + \"b\")\n+                .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK_SETTING.getKey(), WATERMARK_BYTES + \"b\")\n+                .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_DISK_FLOOD_STAGE_WATERMARK_SETTING.getKey(), \"0b\")\n+                .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_REROUTE_INTERVAL_SETTING.getKey(), \"0ms\")\n+                .build();\n+    }\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        return List.of(InternalSettingsPlugin.class);\n+    }\n+\n+    public void testHighWatermarkNotExceeded() throws InterruptedException {\n+        internalCluster().startMasterOnlyNode();\n+        internalCluster().startDataOnlyNode();\n+        final String dataNodeName = internalCluster().startDataOnlyNode();\n+\n+        final InternalClusterInfoService clusterInfoService\n+                = (InternalClusterInfoService) internalCluster().getMasterNodeInstance(ClusterInfoService.class);\n+        internalCluster().getMasterNodeInstance(ClusterService.class).addListener(event -> clusterInfoService.refresh());\n+\n+        final String dataNode0Id = internalCluster().getInstance(NodeEnvironment.class, dataNodeName).nodeId();\n+        final Path dataNode0Path = internalCluster().getInstance(Environment.class, dataNodeName).dataFiles()[0];\n+\n+        createIndex(\"test\", Settings.builder()\n+                .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 0)\n+                .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 6)\n+                .put(INDEX_STORE_STATS_REFRESH_INTERVAL_SETTING.getKey(), \"0ms\")\n+                .build());\n+        final long minShardSize = createReasonableSizedShards();\n+\n+        // reduce disk size of node 0 so that no shards fit below the high watermark, forcing all shards onto the other data node\n+        // (subtract the translog size since the disk threshold decider ignores this and may therefore move the shard back again)\n+        fileSystemProvider.getTestFileStore(dataNode0Path).setTotalSpace(minShardSize + WATERMARK_BYTES - 1L);\n+        refreshDiskUsage();\n+        assertThat(getShardRoutings(dataNode0Id), empty());\n+\n+        // increase disk size of node 0 to allow just enough room for one shard, and check that it's rebalanced back\n+        fileSystemProvider.getTestFileStore(dataNode0Path).setTotalSpace(minShardSize + WATERMARK_BYTES + 1L);\n+        refreshDiskUsage();\n+        assertThat(getShardRoutings(dataNode0Id), hasSize(1));\n+    }\n+\n+    private Set<ShardRouting> getShardRoutings(String nodeId) {\n+        final Set<ShardRouting> shardRoutings = new HashSet<>();\n+        for (IndexShardRoutingTable indexShardRoutingTable : client().admin().cluster().prepareState().clear().setRoutingTable(true)\n+                .get().getState().getRoutingTable().index(\"test\")) {\n+            for (ShardRouting shard : indexShardRoutingTable.shards()) {\n+                assertThat(shard.state(), equalTo(ShardRoutingState.STARTED));\n+                if (shard.currentNodeId().equals(nodeId)) {\n+                    shardRoutings.add(shard);\n+                }\n+            }\n+        }\n+        return shardRoutings;\n+    }\n+\n+    /**\n+     * Index documents until all the shards are at least WATERMARK_BYTES in size, and return the size of the smallest shard\n+     */\n+    private long createReasonableSizedShards() throws InterruptedException {\n+        while (true) {\n+            final IndexRequestBuilder[] indexRequestBuilders = new IndexRequestBuilder[scaledRandomIntBetween(100, 10000)];\n+            for (int i = 0; i < indexRequestBuilders.length; i++) {\n+                indexRequestBuilders[i] = client().prepareIndex(\"test\").setSource(\"field\", randomAlphaOfLength(10));\n+            }\n+            indexRandom(true, indexRequestBuilders);\n+            forceMerge();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIwNjQ1Mg=="}, "originalCommit": {"oid": "946e2b6e54f0148af499fa495965e99282f69967"}, "originalPosition": 185}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMTk1MzcyOnYy", "diffSide": "RIGHT", "path": "server/src/internalClusterTest/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderIT.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwNzo0MTowM1rOG8nBlg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwNzo0MTowM1rOG8nBlg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIwNzEyNg==", "bodyText": "can we somehow only conditionally do this reroute here, i.e.  if all nodes under low watermark?", "url": "https://github.com/elastic/elasticsearch/pull/60460#discussion_r466207126", "createdAt": "2020-08-06T07:41:03Z", "author": {"login": "ywelsch"}, "path": "server/src/internalClusterTest/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderIT.java", "diffHunk": "@@ -0,0 +1,339 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.cluster.routing.allocation.decider;\n+\n+import org.apache.lucene.mockfile.FilterFileStore;\n+import org.apache.lucene.mockfile.FilterFileSystemProvider;\n+import org.apache.lucene.mockfile.FilterPath;\n+import org.apache.lucene.util.Constants;\n+import org.elasticsearch.action.admin.indices.stats.ShardStats;\n+import org.elasticsearch.action.index.IndexRequestBuilder;\n+import org.elasticsearch.cluster.ClusterInfoService;\n+import org.elasticsearch.cluster.InternalClusterInfoService;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexShardRoutingTable;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.routing.ShardRoutingState;\n+import org.elasticsearch.cluster.routing.allocation.DiskThresholdSettings;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Priority;\n+import org.elasticsearch.common.io.PathUtils;\n+import org.elasticsearch.common.io.PathUtilsForTesting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.env.Environment;\n+import org.elasticsearch.env.NodeEnvironment;\n+import org.elasticsearch.monitor.fs.FsService;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalSettingsPlugin;\n+import org.junit.AfterClass;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.nio.file.DirectoryStream;\n+import java.nio.file.FileStore;\n+import java.nio.file.FileSystem;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.NotDirectoryException;\n+import java.nio.file.Path;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.elasticsearch.common.util.concurrent.ConcurrentCollections.newConcurrentMap;\n+import static org.elasticsearch.index.store.Store.INDEX_STORE_STATS_REFRESH_INTERVAL_SETTING;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.hamcrest.Matchers.anyOf;\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.is;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+public class DiskThresholdDeciderIT extends ESIntegTestCase {\n+\n+    private static TestFileSystemProvider fileSystemProvider;\n+\n+    @BeforeClass\n+    public static void installFilesystemProvider() {\n+        assertNull(fileSystemProvider);\n+        fileSystemProvider = new TestFileSystemProvider(PathUtils.getDefaultFileSystem(), createTempDir());\n+        PathUtilsForTesting.installMock(fileSystemProvider.getFileSystem(null));\n+    }\n+\n+    @AfterClass\n+    public static void removeFilesystemProvider() {\n+        fileSystemProvider = null;\n+        // ESIntegTestCase takes care of tearing down the mock file system\n+    }\n+\n+    @Before\n+    public void clearTrackedPaths() throws IOException {\n+        fileSystemProvider.clearTrackedPaths();\n+    }\n+\n+    private static final long WATERMARK_BYTES = new ByteSizeValue(10, ByteSizeUnit.KB).getBytes();\n+\n+    @Override\n+    protected Settings nodeSettings(int nodeOrdinal) {\n+        final Path dataPath = fileSystemProvider.getRootDir().resolve(\"node-\" + nodeOrdinal);\n+        try {\n+            Files.createDirectories(dataPath);\n+        } catch (IOException e) {\n+            throw new AssertionError(\"unexpected\", e);\n+        }\n+        fileSystemProvider.addTrackedPath(dataPath);\n+        return Settings.builder()\n+                .put(super.nodeSettings(nodeOrdinal))\n+                .put(Environment.PATH_DATA_SETTING.getKey(), dataPath)\n+                .put(FsService.ALWAYS_REFRESH_SETTING.getKey(), true)\n+                .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING.getKey(), WATERMARK_BYTES + \"b\")\n+                .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK_SETTING.getKey(), WATERMARK_BYTES + \"b\")\n+                .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_DISK_FLOOD_STAGE_WATERMARK_SETTING.getKey(), \"0b\")\n+                .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_REROUTE_INTERVAL_SETTING.getKey(), \"0ms\")\n+                .build();\n+    }\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        return List.of(InternalSettingsPlugin.class);\n+    }\n+\n+    public void testHighWatermarkNotExceeded() throws InterruptedException {\n+        internalCluster().startMasterOnlyNode();\n+        internalCluster().startDataOnlyNode();\n+        final String dataNodeName = internalCluster().startDataOnlyNode();\n+\n+        final InternalClusterInfoService clusterInfoService\n+                = (InternalClusterInfoService) internalCluster().getMasterNodeInstance(ClusterInfoService.class);\n+        internalCluster().getMasterNodeInstance(ClusterService.class).addListener(event -> clusterInfoService.refresh());\n+\n+        final String dataNode0Id = internalCluster().getInstance(NodeEnvironment.class, dataNodeName).nodeId();\n+        final Path dataNode0Path = internalCluster().getInstance(Environment.class, dataNodeName).dataFiles()[0];\n+\n+        createIndex(\"test\", Settings.builder()\n+                .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 0)\n+                .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 6)\n+                .put(INDEX_STORE_STATS_REFRESH_INTERVAL_SETTING.getKey(), \"0ms\")\n+                .build());\n+        final long minShardSize = createReasonableSizedShards();\n+\n+        // reduce disk size of node 0 so that no shards fit below the high watermark, forcing all shards onto the other data node\n+        // (subtract the translog size since the disk threshold decider ignores this and may therefore move the shard back again)\n+        fileSystemProvider.getTestFileStore(dataNode0Path).setTotalSpace(minShardSize + WATERMARK_BYTES - 1L);\n+        refreshDiskUsage();\n+        assertThat(getShardRoutings(dataNode0Id), empty());\n+\n+        // increase disk size of node 0 to allow just enough room for one shard, and check that it's rebalanced back\n+        fileSystemProvider.getTestFileStore(dataNode0Path).setTotalSpace(minShardSize + WATERMARK_BYTES + 1L);\n+        refreshDiskUsage();\n+        assertThat(getShardRoutings(dataNode0Id), hasSize(1));\n+    }\n+\n+    private Set<ShardRouting> getShardRoutings(String nodeId) {\n+        final Set<ShardRouting> shardRoutings = new HashSet<>();\n+        for (IndexShardRoutingTable indexShardRoutingTable : client().admin().cluster().prepareState().clear().setRoutingTable(true)\n+                .get().getState().getRoutingTable().index(\"test\")) {\n+            for (ShardRouting shard : indexShardRoutingTable.shards()) {\n+                assertThat(shard.state(), equalTo(ShardRoutingState.STARTED));\n+                if (shard.currentNodeId().equals(nodeId)) {\n+                    shardRoutings.add(shard);\n+                }\n+            }\n+        }\n+        return shardRoutings;\n+    }\n+\n+    /**\n+     * Index documents until all the shards are at least WATERMARK_BYTES in size, and return the size of the smallest shard\n+     */\n+    private long createReasonableSizedShards() throws InterruptedException {\n+        while (true) {\n+            final IndexRequestBuilder[] indexRequestBuilders = new IndexRequestBuilder[scaledRandomIntBetween(100, 10000)];\n+            for (int i = 0; i < indexRequestBuilders.length; i++) {\n+                indexRequestBuilders[i] = client().prepareIndex(\"test\").setSource(\"field\", randomAlphaOfLength(10));\n+            }\n+            indexRandom(true, indexRequestBuilders);\n+            forceMerge();\n+            refresh();\n+\n+            final ShardStats[] shardStatses = client().admin().indices().prepareStats(\"test\")\n+                    .clear().setStore(true).setTranslog(true).get().getShards();\n+            final long[] shardSizes = new long[shardStatses.length];\n+            for (ShardStats shardStats : shardStatses) {\n+                shardSizes[shardStats.getShardRouting().id()] = shardStats.getStats().getStore().sizeInBytes();\n+            }\n+\n+            final long minShardSize = Arrays.stream(shardSizes).min().orElseThrow(() -> new AssertionError(\"no shards\"));\n+            if (minShardSize > WATERMARK_BYTES) {\n+                return minShardSize;\n+            }\n+        }\n+    }\n+\n+    private void refreshDiskUsage() {\n+        ((InternalClusterInfoService) internalCluster().getMasterNodeInstance(ClusterInfoService.class)).refresh();\n+        // if the nodes were all under the low watermark already (but unbalanced) then a change in the disk usage doesn't trigger a reroute\n+        // even though it's now possible to achieve better balance, so we have to do an explicit reroute. TODO fix this?\n+        assertAcked(client().admin().cluster().prepareReroute());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946e2b6e54f0148af499fa495965e99282f69967"}, "originalPosition": 206}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMTk2NzIwOnYy", "diffSide": "RIGHT", "path": "server/src/internalClusterTest/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderIT.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwNzo0NToxMlrOG8nJ5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNFQxMjoxMjoyMVrOHNLLZA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIwOTI1Mg==", "bodyText": "Instead of excluding _state and  translog path, should we select the index folder explicitly? Or  assert that  there are  no other paths?", "url": "https://github.com/elastic/elasticsearch/pull/60460#discussion_r466209252", "createdAt": "2020-08-06T07:45:12Z", "author": {"login": "ywelsch"}, "path": "server/src/internalClusterTest/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderIT.java", "diffHunk": "@@ -0,0 +1,339 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.cluster.routing.allocation.decider;\n+\n+import org.apache.lucene.mockfile.FilterFileStore;\n+import org.apache.lucene.mockfile.FilterFileSystemProvider;\n+import org.apache.lucene.mockfile.FilterPath;\n+import org.apache.lucene.util.Constants;\n+import org.elasticsearch.action.admin.indices.stats.ShardStats;\n+import org.elasticsearch.action.index.IndexRequestBuilder;\n+import org.elasticsearch.cluster.ClusterInfoService;\n+import org.elasticsearch.cluster.InternalClusterInfoService;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexShardRoutingTable;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.routing.ShardRoutingState;\n+import org.elasticsearch.cluster.routing.allocation.DiskThresholdSettings;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Priority;\n+import org.elasticsearch.common.io.PathUtils;\n+import org.elasticsearch.common.io.PathUtilsForTesting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.env.Environment;\n+import org.elasticsearch.env.NodeEnvironment;\n+import org.elasticsearch.monitor.fs.FsService;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalSettingsPlugin;\n+import org.junit.AfterClass;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.nio.file.DirectoryStream;\n+import java.nio.file.FileStore;\n+import java.nio.file.FileSystem;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.NotDirectoryException;\n+import java.nio.file.Path;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.elasticsearch.common.util.concurrent.ConcurrentCollections.newConcurrentMap;\n+import static org.elasticsearch.index.store.Store.INDEX_STORE_STATS_REFRESH_INTERVAL_SETTING;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.hamcrest.Matchers.anyOf;\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.is;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+public class DiskThresholdDeciderIT extends ESIntegTestCase {\n+\n+    private static TestFileSystemProvider fileSystemProvider;\n+\n+    @BeforeClass\n+    public static void installFilesystemProvider() {\n+        assertNull(fileSystemProvider);\n+        fileSystemProvider = new TestFileSystemProvider(PathUtils.getDefaultFileSystem(), createTempDir());\n+        PathUtilsForTesting.installMock(fileSystemProvider.getFileSystem(null));\n+    }\n+\n+    @AfterClass\n+    public static void removeFilesystemProvider() {\n+        fileSystemProvider = null;\n+        // ESIntegTestCase takes care of tearing down the mock file system\n+    }\n+\n+    @Before\n+    public void clearTrackedPaths() throws IOException {\n+        fileSystemProvider.clearTrackedPaths();\n+    }\n+\n+    private static final long WATERMARK_BYTES = new ByteSizeValue(10, ByteSizeUnit.KB).getBytes();\n+\n+    @Override\n+    protected Settings nodeSettings(int nodeOrdinal) {\n+        final Path dataPath = fileSystemProvider.getRootDir().resolve(\"node-\" + nodeOrdinal);\n+        try {\n+            Files.createDirectories(dataPath);\n+        } catch (IOException e) {\n+            throw new AssertionError(\"unexpected\", e);\n+        }\n+        fileSystemProvider.addTrackedPath(dataPath);\n+        return Settings.builder()\n+                .put(super.nodeSettings(nodeOrdinal))\n+                .put(Environment.PATH_DATA_SETTING.getKey(), dataPath)\n+                .put(FsService.ALWAYS_REFRESH_SETTING.getKey(), true)\n+                .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING.getKey(), WATERMARK_BYTES + \"b\")\n+                .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK_SETTING.getKey(), WATERMARK_BYTES + \"b\")\n+                .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_DISK_FLOOD_STAGE_WATERMARK_SETTING.getKey(), \"0b\")\n+                .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_REROUTE_INTERVAL_SETTING.getKey(), \"0ms\")\n+                .build();\n+    }\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        return List.of(InternalSettingsPlugin.class);\n+    }\n+\n+    public void testHighWatermarkNotExceeded() throws InterruptedException {\n+        internalCluster().startMasterOnlyNode();\n+        internalCluster().startDataOnlyNode();\n+        final String dataNodeName = internalCluster().startDataOnlyNode();\n+\n+        final InternalClusterInfoService clusterInfoService\n+                = (InternalClusterInfoService) internalCluster().getMasterNodeInstance(ClusterInfoService.class);\n+        internalCluster().getMasterNodeInstance(ClusterService.class).addListener(event -> clusterInfoService.refresh());\n+\n+        final String dataNode0Id = internalCluster().getInstance(NodeEnvironment.class, dataNodeName).nodeId();\n+        final Path dataNode0Path = internalCluster().getInstance(Environment.class, dataNodeName).dataFiles()[0];\n+\n+        createIndex(\"test\", Settings.builder()\n+                .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 0)\n+                .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 6)\n+                .put(INDEX_STORE_STATS_REFRESH_INTERVAL_SETTING.getKey(), \"0ms\")\n+                .build());\n+        final long minShardSize = createReasonableSizedShards();\n+\n+        // reduce disk size of node 0 so that no shards fit below the high watermark, forcing all shards onto the other data node\n+        // (subtract the translog size since the disk threshold decider ignores this and may therefore move the shard back again)\n+        fileSystemProvider.getTestFileStore(dataNode0Path).setTotalSpace(minShardSize + WATERMARK_BYTES - 1L);\n+        refreshDiskUsage();\n+        assertThat(getShardRoutings(dataNode0Id), empty());\n+\n+        // increase disk size of node 0 to allow just enough room for one shard, and check that it's rebalanced back\n+        fileSystemProvider.getTestFileStore(dataNode0Path).setTotalSpace(minShardSize + WATERMARK_BYTES + 1L);\n+        refreshDiskUsage();\n+        assertThat(getShardRoutings(dataNode0Id), hasSize(1));\n+    }\n+\n+    private Set<ShardRouting> getShardRoutings(String nodeId) {\n+        final Set<ShardRouting> shardRoutings = new HashSet<>();\n+        for (IndexShardRoutingTable indexShardRoutingTable : client().admin().cluster().prepareState().clear().setRoutingTable(true)\n+                .get().getState().getRoutingTable().index(\"test\")) {\n+            for (ShardRouting shard : indexShardRoutingTable.shards()) {\n+                assertThat(shard.state(), equalTo(ShardRoutingState.STARTED));\n+                if (shard.currentNodeId().equals(nodeId)) {\n+                    shardRoutings.add(shard);\n+                }\n+            }\n+        }\n+        return shardRoutings;\n+    }\n+\n+    /**\n+     * Index documents until all the shards are at least WATERMARK_BYTES in size, and return the size of the smallest shard\n+     */\n+    private long createReasonableSizedShards() throws InterruptedException {\n+        while (true) {\n+            final IndexRequestBuilder[] indexRequestBuilders = new IndexRequestBuilder[scaledRandomIntBetween(100, 10000)];\n+            for (int i = 0; i < indexRequestBuilders.length; i++) {\n+                indexRequestBuilders[i] = client().prepareIndex(\"test\").setSource(\"field\", randomAlphaOfLength(10));\n+            }\n+            indexRandom(true, indexRequestBuilders);\n+            forceMerge();\n+            refresh();\n+\n+            final ShardStats[] shardStatses = client().admin().indices().prepareStats(\"test\")\n+                    .clear().setStore(true).setTranslog(true).get().getShards();\n+            final long[] shardSizes = new long[shardStatses.length];\n+            for (ShardStats shardStats : shardStatses) {\n+                shardSizes[shardStats.getShardRouting().id()] = shardStats.getStats().getStore().sizeInBytes();\n+            }\n+\n+            final long minShardSize = Arrays.stream(shardSizes).min().orElseThrow(() -> new AssertionError(\"no shards\"));\n+            if (minShardSize > WATERMARK_BYTES) {\n+                return minShardSize;\n+            }\n+        }\n+    }\n+\n+    private void refreshDiskUsage() {\n+        ((InternalClusterInfoService) internalCluster().getMasterNodeInstance(ClusterInfoService.class)).refresh();\n+        // if the nodes were all under the low watermark already (but unbalanced) then a change in the disk usage doesn't trigger a reroute\n+        // even though it's now possible to achieve better balance, so we have to do an explicit reroute. TODO fix this?\n+        assertAcked(client().admin().cluster().prepareReroute());\n+        assertFalse(client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID)\n+                .setWaitForNoRelocatingShards(true)\n+                .setWaitForNoInitializingShards(true).get().isTimedOut());\n+    }\n+\n+    private static class TestFileStore extends FilterFileStore {\n+\n+        private final Path path;\n+\n+        private volatile long totalSpace = -1;\n+\n+        TestFileStore(FileStore delegate, String scheme, Path path) {\n+            super(delegate, scheme);\n+            this.path = path;\n+        }\n+\n+        @Override\n+        public String name() {\n+            return \"fake\"; // Lucene's is-spinning-disk check expects the device name here\n+        }\n+\n+        @Override\n+        public long getTotalSpace() throws IOException {\n+            final long totalSpace = this.totalSpace;\n+            if (totalSpace == -1) {\n+                return super.getTotalSpace();\n+            } else {\n+                return totalSpace;\n+            }\n+        }\n+\n+        public void setTotalSpace(long totalSpace) {\n+            assertThat(totalSpace, anyOf(is(-1L), greaterThan(0L)));\n+            this.totalSpace = totalSpace;\n+        }\n+\n+        @Override\n+        public long getUsableSpace() throws IOException {\n+            final long totalSpace = this.totalSpace;\n+            if (totalSpace == -1) {\n+                return super.getUsableSpace();\n+            } else {\n+                return Math.max(0L, totalSpace - getTotalFileSize(path));\n+            }\n+        }\n+\n+        @Override\n+        public long getUnallocatedSpace() throws IOException {\n+            final long totalSpace = this.totalSpace;\n+            if (totalSpace == -1) {\n+                return super.getUnallocatedSpace();\n+            } else {\n+                return Math.max(0L, totalSpace - getTotalFileSize(path));\n+            }\n+        }\n+\n+        private static long getTotalFileSize(Path path) throws IOException {\n+            if (Files.isRegularFile(path)) {\n+                try {\n+                    return Files.size(path);\n+                } catch (NoSuchFileException | FileNotFoundException e) {\n+                    // probably removed\n+                    return 0L;\n+                }\n+            } else if (path.getFileName().toString().equals(\"_state\") || path.getFileName().toString().equals(\"translog\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946e2b6e54f0148af499fa495965e99282f69967"}, "originalPosition": 271}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzU3NjY3Ng==", "bodyText": "we recursively step into these directories from the root path. including folder is therefore  much trickier", "url": "https://github.com/elastic/elasticsearch/pull/60460#discussion_r483576676", "createdAt": "2020-09-04T12:12:21Z", "author": {"login": "ywelsch"}, "path": "server/src/internalClusterTest/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderIT.java", "diffHunk": "@@ -0,0 +1,339 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.cluster.routing.allocation.decider;\n+\n+import org.apache.lucene.mockfile.FilterFileStore;\n+import org.apache.lucene.mockfile.FilterFileSystemProvider;\n+import org.apache.lucene.mockfile.FilterPath;\n+import org.apache.lucene.util.Constants;\n+import org.elasticsearch.action.admin.indices.stats.ShardStats;\n+import org.elasticsearch.action.index.IndexRequestBuilder;\n+import org.elasticsearch.cluster.ClusterInfoService;\n+import org.elasticsearch.cluster.InternalClusterInfoService;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexShardRoutingTable;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.routing.ShardRoutingState;\n+import org.elasticsearch.cluster.routing.allocation.DiskThresholdSettings;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Priority;\n+import org.elasticsearch.common.io.PathUtils;\n+import org.elasticsearch.common.io.PathUtilsForTesting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.env.Environment;\n+import org.elasticsearch.env.NodeEnvironment;\n+import org.elasticsearch.monitor.fs.FsService;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalSettingsPlugin;\n+import org.junit.AfterClass;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.nio.file.DirectoryStream;\n+import java.nio.file.FileStore;\n+import java.nio.file.FileSystem;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.NotDirectoryException;\n+import java.nio.file.Path;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.elasticsearch.common.util.concurrent.ConcurrentCollections.newConcurrentMap;\n+import static org.elasticsearch.index.store.Store.INDEX_STORE_STATS_REFRESH_INTERVAL_SETTING;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.hamcrest.Matchers.anyOf;\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.is;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+public class DiskThresholdDeciderIT extends ESIntegTestCase {\n+\n+    private static TestFileSystemProvider fileSystemProvider;\n+\n+    @BeforeClass\n+    public static void installFilesystemProvider() {\n+        assertNull(fileSystemProvider);\n+        fileSystemProvider = new TestFileSystemProvider(PathUtils.getDefaultFileSystem(), createTempDir());\n+        PathUtilsForTesting.installMock(fileSystemProvider.getFileSystem(null));\n+    }\n+\n+    @AfterClass\n+    public static void removeFilesystemProvider() {\n+        fileSystemProvider = null;\n+        // ESIntegTestCase takes care of tearing down the mock file system\n+    }\n+\n+    @Before\n+    public void clearTrackedPaths() throws IOException {\n+        fileSystemProvider.clearTrackedPaths();\n+    }\n+\n+    private static final long WATERMARK_BYTES = new ByteSizeValue(10, ByteSizeUnit.KB).getBytes();\n+\n+    @Override\n+    protected Settings nodeSettings(int nodeOrdinal) {\n+        final Path dataPath = fileSystemProvider.getRootDir().resolve(\"node-\" + nodeOrdinal);\n+        try {\n+            Files.createDirectories(dataPath);\n+        } catch (IOException e) {\n+            throw new AssertionError(\"unexpected\", e);\n+        }\n+        fileSystemProvider.addTrackedPath(dataPath);\n+        return Settings.builder()\n+                .put(super.nodeSettings(nodeOrdinal))\n+                .put(Environment.PATH_DATA_SETTING.getKey(), dataPath)\n+                .put(FsService.ALWAYS_REFRESH_SETTING.getKey(), true)\n+                .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING.getKey(), WATERMARK_BYTES + \"b\")\n+                .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK_SETTING.getKey(), WATERMARK_BYTES + \"b\")\n+                .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_DISK_FLOOD_STAGE_WATERMARK_SETTING.getKey(), \"0b\")\n+                .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_REROUTE_INTERVAL_SETTING.getKey(), \"0ms\")\n+                .build();\n+    }\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        return List.of(InternalSettingsPlugin.class);\n+    }\n+\n+    public void testHighWatermarkNotExceeded() throws InterruptedException {\n+        internalCluster().startMasterOnlyNode();\n+        internalCluster().startDataOnlyNode();\n+        final String dataNodeName = internalCluster().startDataOnlyNode();\n+\n+        final InternalClusterInfoService clusterInfoService\n+                = (InternalClusterInfoService) internalCluster().getMasterNodeInstance(ClusterInfoService.class);\n+        internalCluster().getMasterNodeInstance(ClusterService.class).addListener(event -> clusterInfoService.refresh());\n+\n+        final String dataNode0Id = internalCluster().getInstance(NodeEnvironment.class, dataNodeName).nodeId();\n+        final Path dataNode0Path = internalCluster().getInstance(Environment.class, dataNodeName).dataFiles()[0];\n+\n+        createIndex(\"test\", Settings.builder()\n+                .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 0)\n+                .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 6)\n+                .put(INDEX_STORE_STATS_REFRESH_INTERVAL_SETTING.getKey(), \"0ms\")\n+                .build());\n+        final long minShardSize = createReasonableSizedShards();\n+\n+        // reduce disk size of node 0 so that no shards fit below the high watermark, forcing all shards onto the other data node\n+        // (subtract the translog size since the disk threshold decider ignores this and may therefore move the shard back again)\n+        fileSystemProvider.getTestFileStore(dataNode0Path).setTotalSpace(minShardSize + WATERMARK_BYTES - 1L);\n+        refreshDiskUsage();\n+        assertThat(getShardRoutings(dataNode0Id), empty());\n+\n+        // increase disk size of node 0 to allow just enough room for one shard, and check that it's rebalanced back\n+        fileSystemProvider.getTestFileStore(dataNode0Path).setTotalSpace(minShardSize + WATERMARK_BYTES + 1L);\n+        refreshDiskUsage();\n+        assertThat(getShardRoutings(dataNode0Id), hasSize(1));\n+    }\n+\n+    private Set<ShardRouting> getShardRoutings(String nodeId) {\n+        final Set<ShardRouting> shardRoutings = new HashSet<>();\n+        for (IndexShardRoutingTable indexShardRoutingTable : client().admin().cluster().prepareState().clear().setRoutingTable(true)\n+                .get().getState().getRoutingTable().index(\"test\")) {\n+            for (ShardRouting shard : indexShardRoutingTable.shards()) {\n+                assertThat(shard.state(), equalTo(ShardRoutingState.STARTED));\n+                if (shard.currentNodeId().equals(nodeId)) {\n+                    shardRoutings.add(shard);\n+                }\n+            }\n+        }\n+        return shardRoutings;\n+    }\n+\n+    /**\n+     * Index documents until all the shards are at least WATERMARK_BYTES in size, and return the size of the smallest shard\n+     */\n+    private long createReasonableSizedShards() throws InterruptedException {\n+        while (true) {\n+            final IndexRequestBuilder[] indexRequestBuilders = new IndexRequestBuilder[scaledRandomIntBetween(100, 10000)];\n+            for (int i = 0; i < indexRequestBuilders.length; i++) {\n+                indexRequestBuilders[i] = client().prepareIndex(\"test\").setSource(\"field\", randomAlphaOfLength(10));\n+            }\n+            indexRandom(true, indexRequestBuilders);\n+            forceMerge();\n+            refresh();\n+\n+            final ShardStats[] shardStatses = client().admin().indices().prepareStats(\"test\")\n+                    .clear().setStore(true).setTranslog(true).get().getShards();\n+            final long[] shardSizes = new long[shardStatses.length];\n+            for (ShardStats shardStats : shardStatses) {\n+                shardSizes[shardStats.getShardRouting().id()] = shardStats.getStats().getStore().sizeInBytes();\n+            }\n+\n+            final long minShardSize = Arrays.stream(shardSizes).min().orElseThrow(() -> new AssertionError(\"no shards\"));\n+            if (minShardSize > WATERMARK_BYTES) {\n+                return minShardSize;\n+            }\n+        }\n+    }\n+\n+    private void refreshDiskUsage() {\n+        ((InternalClusterInfoService) internalCluster().getMasterNodeInstance(ClusterInfoService.class)).refresh();\n+        // if the nodes were all under the low watermark already (but unbalanced) then a change in the disk usage doesn't trigger a reroute\n+        // even though it's now possible to achieve better balance, so we have to do an explicit reroute. TODO fix this?\n+        assertAcked(client().admin().cluster().prepareReroute());\n+        assertFalse(client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID)\n+                .setWaitForNoRelocatingShards(true)\n+                .setWaitForNoInitializingShards(true).get().isTimedOut());\n+    }\n+\n+    private static class TestFileStore extends FilterFileStore {\n+\n+        private final Path path;\n+\n+        private volatile long totalSpace = -1;\n+\n+        TestFileStore(FileStore delegate, String scheme, Path path) {\n+            super(delegate, scheme);\n+            this.path = path;\n+        }\n+\n+        @Override\n+        public String name() {\n+            return \"fake\"; // Lucene's is-spinning-disk check expects the device name here\n+        }\n+\n+        @Override\n+        public long getTotalSpace() throws IOException {\n+            final long totalSpace = this.totalSpace;\n+            if (totalSpace == -1) {\n+                return super.getTotalSpace();\n+            } else {\n+                return totalSpace;\n+            }\n+        }\n+\n+        public void setTotalSpace(long totalSpace) {\n+            assertThat(totalSpace, anyOf(is(-1L), greaterThan(0L)));\n+            this.totalSpace = totalSpace;\n+        }\n+\n+        @Override\n+        public long getUsableSpace() throws IOException {\n+            final long totalSpace = this.totalSpace;\n+            if (totalSpace == -1) {\n+                return super.getUsableSpace();\n+            } else {\n+                return Math.max(0L, totalSpace - getTotalFileSize(path));\n+            }\n+        }\n+\n+        @Override\n+        public long getUnallocatedSpace() throws IOException {\n+            final long totalSpace = this.totalSpace;\n+            if (totalSpace == -1) {\n+                return super.getUnallocatedSpace();\n+            } else {\n+                return Math.max(0L, totalSpace - getTotalFileSize(path));\n+            }\n+        }\n+\n+        private static long getTotalFileSize(Path path) throws IOException {\n+            if (Files.isRegularFile(path)) {\n+                try {\n+                    return Files.size(path);\n+                } catch (NoSuchFileException | FileNotFoundException e) {\n+                    // probably removed\n+                    return 0L;\n+                }\n+            } else if (path.getFileName().toString().equals(\"_state\") || path.getFileName().toString().equals(\"translog\")) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIwOTI1Mg=="}, "originalCommit": {"oid": "946e2b6e54f0148af499fa495965e99282f69967"}, "originalPosition": 271}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2751, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}