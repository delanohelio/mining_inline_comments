{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk4NDQyNTM3", "number": 63315, "title": "[Transform] improve continuous transform date_histogram group_by with ingest timestamps", "bodyText": "optimize continuous data histogram group_by for other time fields independent\nof sync, this allows the usage of ingest timestamps in continuous mode\nfixes #59061\nHow it works in a nutshell:\nWhen querying for changes, transform adds min and max aggregations to later narrow the update query\nby adding a range query given the output of min and max.\n\nThe optimization is similar to #54254 (comment), worst case we re-query and re-write every document for each checkpoint without this optimization.", "createdAt": "2020-10-06T10:44:00Z", "url": "https://github.com/elastic/elasticsearch/pull/63315", "merged": true, "mergeCommit": {"oid": "b9d530575831b42b876919f8f2d5716819e00aa4"}, "closed": true, "closedAt": "2020-10-16T09:13:29Z", "author": {"login": "hendrikmuhs"}, "timelineItems": {"totalCount": 15, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdP3nwRABqjM4NDUxNzYxNTQ=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdTCuB6gH2gAyNDk4NDQyNTM3OjIzYmZhYzRlOTE1NDZlYmNlMWE3OWY3NDE3NGYyOWM4YzEyMjVmMWE=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f5004f535d6e463cc0da7918b539ce1fa9f52c0a", "author": {"user": {"login": "hendrikmuhs", "name": "Hendrik Muhs"}}, "url": "https://github.com/elastic/elasticsearch/commit/f5004f535d6e463cc0da7918b539ce1fa9f52c0a", "committedDate": "2020-10-06T10:29:47Z", "message": "optimize continuous data histogram group_by for other time fields\nindependent of sync, this allows the usage of ingest timestamp in continuous\nmode"}, "afterCommit": {"oid": "544ef6d5515473f316813f9f739a5ed1972ac815", "author": {"user": {"login": "hendrikmuhs", "name": "Hendrik Muhs"}}, "url": "https://github.com/elastic/elasticsearch/commit/544ef6d5515473f316813f9f739a5ed1972ac815", "committedDate": "2020-10-06T12:33:53Z", "message": "optimize continuous data histogram group_by for other time fields\nindependent of sync, this allows the usage of ingest timestamp in continuous\nmode"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "544ef6d5515473f316813f9f739a5ed1972ac815", "author": {"user": {"login": "hendrikmuhs", "name": "Hendrik Muhs"}}, "url": "https://github.com/elastic/elasticsearch/commit/544ef6d5515473f316813f9f739a5ed1972ac815", "committedDate": "2020-10-06T12:33:53Z", "message": "optimize continuous data histogram group_by for other time fields\nindependent of sync, this allows the usage of ingest timestamp in continuous\nmode"}, "afterCommit": {"oid": "02ba4cc766a391fa8ced0f65ca130603d8fefcd9", "author": {"user": {"login": "hendrikmuhs", "name": "Hendrik Muhs"}}, "url": "https://github.com/elastic/elasticsearch/commit/02ba4cc766a391fa8ced0f65ca130603d8fefcd9", "committedDate": "2020-10-06T15:27:23Z", "message": "remove workaround"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e1a6d6132e1ab2cac66136c1025fb869cf258f85", "author": {"user": {"login": "hendrikmuhs", "name": "Hendrik Muhs"}}, "url": "https://github.com/elastic/elasticsearch/commit/e1a6d6132e1ab2cac66136c1025fb869cf258f85", "committedDate": "2020-10-07T12:11:00Z", "message": "optimize continuous data histogram group_by for other time fields\nindependent of sync, this allows the usage of ingest timestamp in continuous\nmode"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "82983c0e5ec87ba1b31c5b564e912263ccdda000", "author": {"user": {"login": "hendrikmuhs", "name": "Hendrik Muhs"}}, "url": "https://github.com/elastic/elasticsearch/commit/82983c0e5ec87ba1b31c5b564e912263ccdda000", "committedDate": "2020-10-07T12:11:00Z", "message": "remove workaround"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d7c6de5366d5aac72426db167fd9fd6c66d9b17d", "author": {"user": {"login": "hendrikmuhs", "name": "Hendrik Muhs"}}, "url": "https://github.com/elastic/elasticsearch/commit/d7c6de5366d5aac72426db167fd9fd6c66d9b17d", "committedDate": "2020-10-07T12:11:00Z", "message": "improve comment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7ccf8367b08b1645940012d238d983b2691f9a3b", "author": {"user": {"login": "hendrikmuhs", "name": "Hendrik Muhs"}}, "url": "https://github.com/elastic/elasticsearch/commit/7ccf8367b08b1645940012d238d983b2691f9a3b", "committedDate": "2020-10-07T12:11:00Z", "message": "extend test case to randomly add an additional group_by on terms"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "387018224d40890045bcfc2e522e0d27209c44fb", "author": {"user": {"login": "hendrikmuhs", "name": "Hendrik Muhs"}}, "url": "https://github.com/elastic/elasticsearch/commit/387018224d40890045bcfc2e522e0d27209c44fb", "committedDate": "2020-10-07T10:01:31Z", "message": "extend test case to randomly add an additional group_by on terms"}, "afterCommit": {"oid": "7ccf8367b08b1645940012d238d983b2691f9a3b", "author": {"user": {"login": "hendrikmuhs", "name": "Hendrik Muhs"}}, "url": "https://github.com/elastic/elasticsearch/commit/7ccf8367b08b1645940012d238d983b2691f9a3b", "committedDate": "2020-10-07T12:11:00Z", "message": "extend test case to randomly add an additional group_by on terms"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA0MTk2ODA0", "url": "https://github.com/elastic/elasticsearch/pull/63315#pullrequestreview-504196804", "createdAt": "2020-10-07T19:23:43Z", "commit": {"oid": "7ccf8367b08b1645940012d238d983b2691f9a3b"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxOToyMzo0M1rOHeCMjQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QyMDowMzozNVrOHeDfAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI1NTMwOQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        logger.debug(\"[{}] Querying for data: {}\", getJobId(), sourceBuilder);\n          \n          \n            \n                        logger.debug(() -> new ParameterizedMessage(\"[{}] Querying for data: {}\", getJobId(), sourceBuilder));\n          \n      \n    \n    \n  \n\nIt is probably wise to not needlessly create strings and such, especially calling toString on the sourceBuilder", "url": "https://github.com/elastic/elasticsearch/pull/63315#discussion_r501255309", "createdAt": "2020-10-07T19:23:43Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/transform/src/main/java/org/elasticsearch/xpack/transform/transforms/TransformIndexer.java", "diffHunk": "@@ -863,7 +863,7 @@ private SearchSourceBuilder buildUpdateQuery(SearchSourceBuilder sourceBuilder)\n         // if its either the 1st run or not continuous, do not apply extra filters\n         if (nextCheckpoint.getCheckpoint() == 1 || isContinuous() == false) {\n             sourceBuilder.query(queryBuilder);\n-            logger.trace(\"running query: {}\", sourceBuilder);\n+            logger.debug(\"[{}] Querying for data: {}\", getJobId(), sourceBuilder);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7ccf8367b08b1645940012d238d983b2691f9a3b"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI1NTUxMw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    logger.debug(\"[{}] Querying for data: {}\", getJobId(), sourceBuilder);\n          \n          \n            \n                    logger.debug(() -> new ParameterizedMessage(\"[{}] Querying for data: {}\", getJobId(), sourceBuilder));", "url": "https://github.com/elastic/elasticsearch/pull/63315#discussion_r501255513", "createdAt": "2020-10-07T19:24:06Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/transform/src/main/java/org/elasticsearch/xpack/transform/transforms/TransformIndexer.java", "diffHunk": "@@ -880,7 +880,7 @@ private SearchSourceBuilder buildUpdateQuery(SearchSourceBuilder sourceBuilder)\n         }\n \n         sourceBuilder.query(filteredQuery);\n-        logger.trace(\"running query: {}\", sourceBuilder);\n+        logger.debug(\"[{}] Querying for data: {}\", getJobId(), sourceBuilder);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7ccf8367b08b1645940012d238d983b2691f9a3b"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI3MzY4OQ==", "bodyText": "I don't know why this is necessary. If compositeAggregation == null, that means it was never set.\nConsequently, a composite aggregation should never be executed for this changes check.\nWhy can't afterKey just stay null?", "url": "https://github.com/elastic/elasticsearch/pull/63315#discussion_r501273689", "createdAt": "2020-10-07T19:58:22Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/transform/src/main/java/org/elasticsearch/xpack/transform/transforms/pivot/CompositeBucketsChangeCollector.java", "diffHunk": "@@ -492,20 +670,26 @@ public boolean processSearchResponse(final SearchResponse searchResponse) {\n             return true;\n         }\n \n-        final CompositeAggregation agg = aggregations.get(compositeAggregation.getName());\n-\n-        Collection<? extends Bucket> buckets = agg.getBuckets();\n-        afterKey = agg.afterKey();\n+        boolean isDone = true;\n \n-        if (buckets.isEmpty()) {\n-            return true;\n+        // collect changes from composite buckets\n+        if (compositeAggregation != null) {\n+            final CompositeAggregation agg = aggregations.get(compositeAggregation.getName());\n+            Collection<? extends Bucket> buckets = agg.getBuckets();\n+            afterKey = agg.afterKey();\n+            for (FieldCollector fieldCollector : fieldCollectors.values()) {\n+                isDone &= fieldCollector.collectChangesFromCompositeBuckets(buckets);\n+            }\n+        } else {\n+            afterKey = AFTER_KEY_MAGIC;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7ccf8367b08b1645940012d238d983b2691f9a3b"}, "originalPosition": 465}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI3NDA2OQ==", "bodyText": "Is this the purpose of the AFTER_KEY_MAGIC ?", "url": "https://github.com/elastic/elasticsearch/pull/63315#discussion_r501274069", "createdAt": "2020-10-07T19:59:05Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/transform/src/main/java/org/elasticsearch/xpack/transform/transforms/pivot/CompositeBucketsChangeCollector.java", "diffHunk": "@@ -449,10 +625,12 @@ private CompositeBucketsChangeCollector(\n     public SearchSourceBuilder buildChangesQuery(SearchSourceBuilder sourceBuilder, Map<String, Object> position, int pageSize) {\n         sourceBuilder.size(0);\n         for (FieldCollector fieldCollector : fieldCollectors.values()) {\n-            AggregationBuilder aggregationForField = fieldCollector.aggregateChanges();\n \n-            if (aggregationForField != null) {\n-                sourceBuilder.aggregation(aggregationForField);\n+            // add aggregations, but only for the 1st run\n+            if (position == null || position.isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7ccf8367b08b1645940012d238d983b2691f9a3b"}, "originalPosition": 437}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI3NDc3OA==", "bodyText": "This is a strange optimization to me. If you only want to optionally construct the compositeAggregationBuilder make it a CachedSupplier that is only called once or something.", "url": "https://github.com/elastic/elasticsearch/pull/63315#discussion_r501274778", "createdAt": "2020-10-07T20:00:25Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/transform/src/main/java/org/elasticsearch/xpack/transform/transforms/pivot/CompositeBucketsChangeCollector.java", "diffHunk": "@@ -523,13 +707,22 @@ public boolean isOptimized() {\n         return fieldCollectors.values().stream().anyMatch(FieldCollector::isOptimized);\n     }\n \n+    @Override\n+    public boolean queryForChanges() {\n+        return fieldCollectors.values().stream().anyMatch(FieldCollector::queryForChanges);\n+    }\n+\n     public static ChangeCollector buildChangeCollector(\n-        @Nullable CompositeAggregationBuilder compositeAggregationBuilder,\n+        CompositeAggregationBuilder compositeAggregationBuilder,\n         Map<String, SingleGroupSource> groups,\n         String synchronizationField\n     ) {\n         Map<String, FieldCollector> fieldCollectors = createFieldCollectors(groups, synchronizationField);\n-        return new CompositeBucketsChangeCollector(compositeAggregationBuilder, fieldCollectors);\n+\n+        // after building the field collectors we know whether we need the composite aggregation\n+        boolean keepCompositeAggregatonBuilder = fieldCollectors.values().stream().anyMatch(FieldCollector::requiresCompositeSources);\n+\n+        return new CompositeBucketsChangeCollector(keepCompositeAggregatonBuilder ? compositeAggregationBuilder : null, fieldCollectors);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7ccf8367b08b1645940012d238d983b2691f9a3b"}, "originalPosition": 500}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI3NTMzNg==", "bodyText": "Why are you doing this null check? Wouldn't it be better to rely on if any of fieldCollector items requiresCompositeSources ?", "url": "https://github.com/elastic/elasticsearch/pull/63315#discussion_r501275336", "createdAt": "2020-10-07T20:01:27Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/transform/src/main/java/org/elasticsearch/xpack/transform/transforms/pivot/CompositeBucketsChangeCollector.java", "diffHunk": "@@ -492,20 +670,26 @@ public boolean processSearchResponse(final SearchResponse searchResponse) {\n             return true;\n         }\n \n-        final CompositeAggregation agg = aggregations.get(compositeAggregation.getName());\n-\n-        Collection<? extends Bucket> buckets = agg.getBuckets();\n-        afterKey = agg.afterKey();\n+        boolean isDone = true;\n \n-        if (buckets.isEmpty()) {\n-            return true;\n+        // collect changes from composite buckets\n+        if (compositeAggregation != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7ccf8367b08b1645940012d238d983b2691f9a3b"}, "originalPosition": 457}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI3NjQxOQ==", "bodyText": "If the changes detector is not using a composite agg, the caller shouldn't attempt to iterate on it. Consequently a null after key would be acceptable as it would only be called once.", "url": "https://github.com/elastic/elasticsearch/pull/63315#discussion_r501276419", "createdAt": "2020-10-07T20:03:35Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/transform/src/main/java/org/elasticsearch/xpack/transform/transforms/pivot/CompositeBucketsChangeCollector.java", "diffHunk": "@@ -492,20 +670,26 @@ public boolean processSearchResponse(final SearchResponse searchResponse) {\n             return true;\n         }\n \n-        final CompositeAggregation agg = aggregations.get(compositeAggregation.getName());\n-\n-        Collection<? extends Bucket> buckets = agg.getBuckets();\n-        afterKey = agg.afterKey();\n+        boolean isDone = true;\n \n-        if (buckets.isEmpty()) {\n-            return true;\n+        // collect changes from composite buckets\n+        if (compositeAggregation != null) {\n+            final CompositeAggregation agg = aggregations.get(compositeAggregation.getName());\n+            Collection<? extends Bucket> buckets = agg.getBuckets();\n+            afterKey = agg.afterKey();\n+            for (FieldCollector fieldCollector : fieldCollectors.values()) {\n+                isDone &= fieldCollector.collectChangesFromCompositeBuckets(buckets);\n+            }\n+        } else {\n+            afterKey = AFTER_KEY_MAGIC;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI3MzY4OQ=="}, "originalCommit": {"oid": "7ccf8367b08b1645940012d238d983b2691f9a3b"}, "originalPosition": 465}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c4535c13d6aa7fb390a7a7c184426957bb09979a", "author": {"user": {"login": "hendrikmuhs", "name": "Hendrik Muhs"}}, "url": "https://github.com/elastic/elasticsearch/commit/c4535c13d6aa7fb390a7a7c184426957bb09979a", "committedDate": "2020-10-08T07:04:08Z", "message": "Apply suggestions from code review\n\nCo-authored-by: Benjamin Trent <ben.w.trent@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e73645fc9a9337fff7ec34328e65b6fcaf4176ae", "author": {"user": {"login": "hendrikmuhs", "name": "Hendrik Muhs"}}, "url": "https://github.com/elastic/elasticsearch/commit/e73645fc9a9337fff7ec34328e65b6fcaf4176ae", "committedDate": "2020-10-08T09:59:26Z", "message": "Merge branch 'master' of github.com:elastic/elasticsearch into transform-dh-opt2"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "67ecae55243004aceafa8ada55cfe169075e1c45", "author": {"user": {"login": "hendrikmuhs", "name": "Hendrik Muhs"}}, "url": "https://github.com/elastic/elasticsearch/commit/67ecae55243004aceafa8ada55cfe169075e1c45", "committedDate": "2020-10-14T11:59:30Z", "message": "improve separation between composite bucket change collector and pivot"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3689b71849b32c324833b5f0fa42e0860b180579", "author": {"user": {"login": "hendrikmuhs", "name": "Hendrik Muhs"}}, "url": "https://github.com/elastic/elasticsearch/commit/3689b71849b32c324833b5f0fa42e0860b180579", "committedDate": "2020-10-15T11:48:06Z", "message": "simplify keeping track of the change collector bucket position by moving it to the indexer"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA5NDUyNTYz", "url": "https://github.com/elastic/elasticsearch/pull/63315#pullrequestreview-509452563", "createdAt": "2020-10-15T14:38:23Z", "commit": {"oid": "3689b71849b32c324833b5f0fa42e0860b180579"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTEwMjMxNzYz", "url": "https://github.com/elastic/elasticsearch/pull/63315#pullrequestreview-510231763", "createdAt": "2020-10-16T07:51:25Z", "commit": {"oid": "3689b71849b32c324833b5f0fa42e0860b180579"}, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQwNzo1MToyNlrOHisOQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQwNzo1MzoxOVrOHisTpA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjEzODE3Nw==", "bodyText": "[optional] Alternatively, you could express it this way:\n        assert (compositeAggregation == null) == (compositeAggResults == null);\n\nbut I'm not sure which one is more readable.", "url": "https://github.com/elastic/elasticsearch/pull/63315#discussion_r506138177", "createdAt": "2020-10-16T07:51:26Z", "author": {"login": "przemekwitek"}, "path": "x-pack/plugin/transform/src/main/java/org/elasticsearch/xpack/transform/transforms/pivot/CompositeBucketsChangeCollector.java", "diffHunk": "@@ -486,26 +672,42 @@ public QueryBuilder buildFilterQuery(long lastCheckpointTimestamp, long nextchec\n     }\n \n     @Override\n-    public boolean processSearchResponse(final SearchResponse searchResponse) {\n+    public Map<String, Object> processSearchResponse(final SearchResponse searchResponse) {\n         final Aggregations aggregations = searchResponse.getAggregations();\n         if (aggregations == null) {\n-            return true;\n+            return null;\n         }\n \n-        final CompositeAggregation agg = aggregations.get(compositeAggregation.getName());\n-\n-        Collection<? extends Bucket> buckets = agg.getBuckets();\n-        afterKey = agg.afterKey();\n+        // every collector reports if the collection of changes is done after processing, if all are done, the indexer\n+        // will not run a query for change collection again\n+        boolean isDone = true;\n \n-        if (buckets.isEmpty()) {\n-            return true;\n+        // collect changes from aggregations added by field collectors\n+        for (FieldCollector fieldCollector : fieldCollectors.values()) {\n+            isDone &= fieldCollector.collectChangesFromAggregations(aggregations);\n         }\n \n-        for (FieldCollector fieldCollector : fieldCollectors.values()) {\n-            fieldCollector.collectChanges(buckets);\n+        // collect changes from composite buckets\n+        final CompositeAggregation compositeAggResults = aggregations.get(COMPOSITE_AGGREGATION_NAME);\n+\n+        // xor: either both must exist or not exist\n+        assert (compositeAggregation == null ^ compositeAggResults == null) == false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3689b71849b32c324833b5f0fa42e0860b180579"}, "originalPosition": 490}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjEzOTU1Ng==", "bodyText": "static", "url": "https://github.com/elastic/elasticsearch/pull/63315#discussion_r506139556", "createdAt": "2020-10-16T07:53:19Z", "author": {"login": "przemekwitek"}, "path": "x-pack/plugin/transform/qa/single-node-tests/src/javaRestTest/java/org/elasticsearch/xpack/transform/integration/continuous/DateHistogramGroupByOtherTimeFieldIT.java", "diffHunk": "@@ -0,0 +1,234 @@\n+package org.elasticsearch.xpack.transform.integration.continuous;\n+\n+import org.elasticsearch.action.search.SearchRequest;\n+import org.elasticsearch.action.search.SearchResponse;\n+import org.elasticsearch.action.support.IndicesOptions;\n+import org.elasticsearch.client.transform.transforms.DestConfig;\n+import org.elasticsearch.client.transform.transforms.SourceConfig;\n+import org.elasticsearch.client.transform.transforms.TransformConfig;\n+import org.elasticsearch.client.transform.transforms.pivot.DateHistogramGroupSource;\n+import org.elasticsearch.client.transform.transforms.pivot.GroupConfig;\n+import org.elasticsearch.client.transform.transforms.pivot.PivotConfig;\n+import org.elasticsearch.client.transform.transforms.pivot.TermsGroupSource;\n+import org.elasticsearch.common.xcontent.support.XContentMapValues;\n+import org.elasticsearch.search.SearchHit;\n+import org.elasticsearch.search.aggregations.AggregatorFactories;\n+import org.elasticsearch.search.aggregations.BucketOrder;\n+import org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramAggregationBuilder;\n+import org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramInterval;\n+import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;\n+import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket;\n+import org.elasticsearch.search.aggregations.bucket.terms.Terms;\n+import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregationBuilder;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+\n+import java.io.IOException;\n+import java.time.ZonedDateTime;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.lessThanOrEqualTo;\n+\n+/**\n+ * Testcase for date histogram group_by on _different_ fields than used for sync\n+ */\n+public class DateHistogramGroupByOtherTimeFieldIT extends ContinuousTestCase {\n+    private static final String NAME = \"continuous-date-histogram-pivot-other-timefield-test\";\n+\n+    private final boolean addGroupByTerms;\n+\n+    public DateHistogramGroupByOtherTimeFieldIT() {\n+        addGroupByTerms = randomBoolean();\n+    }\n+\n+    @Override\n+    public TransformConfig createConfig() {\n+        TransformConfig.Builder transformConfigBuilder = new TransformConfig.Builder();\n+        addCommonBuilderParameters(transformConfigBuilder);\n+        transformConfigBuilder.setSource(new SourceConfig(CONTINUOUS_EVENTS_SOURCE_INDEX));\n+        transformConfigBuilder.setDest(new DestConfig(NAME, INGEST_PIPELINE));\n+        transformConfigBuilder.setId(NAME);\n+        PivotConfig.Builder pivotConfigBuilder = new PivotConfig.Builder();\n+        GroupConfig.Builder groups = new GroupConfig.Builder().groupBy(\n+            \"second\",\n+            new DateHistogramGroupSource.Builder().setField(\"metric-timestamp\")\n+                .setInterval(new DateHistogramGroupSource.FixedInterval(DateHistogramInterval.SECOND))\n+                .build()\n+        );\n+        if (addGroupByTerms) {\n+            groups.groupBy(\"event\", new TermsGroupSource.Builder().setField(\"event\").build());\n+        }\n+        pivotConfigBuilder.setGroups(groups.build());\n+        AggregatorFactories.Builder aggregations = new AggregatorFactories.Builder();\n+        addCommonAggregations(aggregations);\n+\n+        pivotConfigBuilder.setAggregations(aggregations);\n+        transformConfigBuilder.setPivotConfig(pivotConfigBuilder.build());\n+        return transformConfigBuilder.build();\n+    }\n+\n+    @Override\n+    public String getName() {\n+        return NAME;\n+    }\n+\n+    @Override\n+    public void testIteration(int iteration) throws IOException {\n+        SearchRequest searchRequestSource = new SearchRequest(CONTINUOUS_EVENTS_SOURCE_INDEX).allowPartialSearchResults(false)\n+            .indicesOptions(IndicesOptions.LENIENT_EXPAND_OPEN);\n+        SearchSourceBuilder sourceBuilderSource = new SearchSourceBuilder().size(0);\n+        DateHistogramAggregationBuilder bySecond = new DateHistogramAggregationBuilder(\"second\").field(\"metric-timestamp\")\n+            .fixedInterval(DateHistogramInterval.SECOND)\n+            .order(BucketOrder.key(true));\n+\n+        if (addGroupByTerms) {\n+            TermsAggregationBuilder terms = new TermsAggregationBuilder(\"event\").size(1000).field(\"event\").order(BucketOrder.key(true));\n+            bySecond.subAggregation(terms);\n+        }\n+        sourceBuilderSource.aggregation(bySecond);\n+        searchRequestSource.source(sourceBuilderSource);\n+        SearchResponse responseSource = search(searchRequestSource);\n+\n+        SearchRequest searchRequestDest = new SearchRequest(NAME).allowPartialSearchResults(false)\n+            .indicesOptions(IndicesOptions.LENIENT_EXPAND_OPEN);\n+        SearchSourceBuilder sourceBuilderDest = new SearchSourceBuilder().size(10000).sort(\"second\");\n+        if (addGroupByTerms) {\n+            sourceBuilderDest.sort(\"event\");\n+        }\n+\n+        searchRequestDest.source(sourceBuilderDest);\n+        SearchResponse responseDest = search(searchRequestDest);\n+\n+        if (addGroupByTerms) {\n+            assertResultsGroupByDateHistogramAndTerms(iteration, responseSource, responseDest);\n+        } else {\n+            assertResultsGroupByDateHistogram(iteration, responseSource, responseDest);\n+        }\n+    }\n+\n+    private void assertResultsGroupByDateHistogram(int iteration, SearchResponse responseSource, SearchResponse responseDest) {\n+        List<? extends Bucket> buckets = ((Histogram) responseSource.getAggregations().get(\"second\")).getBuckets();\n+        Iterator<? extends Bucket> sourceIterator = buckets.iterator();\n+        Iterator<SearchHit> destIterator = responseDest.getHits().iterator();\n+\n+        while (sourceIterator.hasNext() && destIterator.hasNext()) {\n+            Bucket bucket = sourceIterator.next();\n+            SearchHit searchHit = destIterator.next();\n+            Map<String, Object> source = searchHit.getSourceAsMap();\n+\n+            Long transformBucketKey = (Long) XContentMapValues.extractValue(\"second\", source);\n+\n+            // aggs return buckets with 0 doc_count while composite aggs skip over them\n+            while (bucket.getDocCount() == 0L) {\n+                assertTrue(sourceIterator.hasNext());\n+                bucket = sourceIterator.next();\n+            }\n+            long bucketKey = ((ZonedDateTime) bucket.getKey()).toEpochSecond() * 1000;\n+\n+            // test correctness, the results from the aggregation and the results from the transform should be the same\n+            assertThat(\n+                \"Buckets did not match, source: \" + source + \", expected: \" + bucketKey + \", iteration: \" + iteration,\n+                transformBucketKey,\n+                equalTo(bucketKey)\n+            );\n+            assertThat(\n+                \"Doc count did not match, source: \" + source + \", expected: \" + bucket.getDocCount() + \", iteration: \" + iteration,\n+                XContentMapValues.extractValue(\"count\", source),\n+                equalTo(Double.valueOf(bucket.getDocCount()))\n+            );\n+\n+            // transform should only rewrite documents that require it\n+            assertThat(\n+                \"Ingest run: \"\n+                    + XContentMapValues.extractValue(INGEST_RUN_FIELD, source)\n+                    + \" did not match max run: \"\n+                    + XContentMapValues.extractValue(MAX_RUN_FIELD, source)\n+                    + \", iteration: \"\n+                    + iteration,\n+                // we use a fixed_interval of `1s`, the transform runs every `1s`, a bucket might be recalculated at the next run\n+                // but should NOT be recalculated for the 2nd/3rd/... run\n+                Double.valueOf((Integer) XContentMapValues.extractValue(INGEST_RUN_FIELD, source)) - (Double) XContentMapValues\n+                    .extractValue(MAX_RUN_FIELD, source),\n+                is(lessThanOrEqualTo(1.0))\n+            );\n+\n+        }\n+        assertFalse(sourceIterator.hasNext());\n+        assertFalse(destIterator.hasNext());\n+    }\n+\n+    private void assertResultsGroupByDateHistogramAndTerms(int iteration, SearchResponse responseSource, SearchResponse responseDest) {\n+        List<? extends Bucket> buckets = ((Histogram) responseSource.getAggregations().get(\"second\")).getBuckets();\n+\n+        List<Map<String, Object>> flattenedBuckets = new ArrayList<>();\n+        for (Bucket b : buckets) {\n+            if (b.getDocCount() == 0) {\n+                continue;\n+            }\n+            long second = ((ZonedDateTime) b.getKey()).toEpochSecond() * 1000;\n+            List<? extends Terms.Bucket> terms = ((Terms) b.getAggregations().get(\"event\")).getBuckets();\n+            for (Terms.Bucket t : terms) {\n+                flattenedBuckets.add(flattenedResult(second, t.getKeyAsString(), t.getDocCount()));\n+            }\n+        }\n+\n+        Iterator<Map<String, Object>> sourceIterator = flattenedBuckets.iterator();\n+        Iterator<SearchHit> destIterator = responseDest.getHits().iterator();\n+\n+        while (sourceIterator.hasNext() && destIterator.hasNext()) {\n+            Map<String, Object> bucket = sourceIterator.next();\n+\n+            SearchHit searchHit = destIterator.next();\n+            Map<String, Object> source = searchHit.getSourceAsMap();\n+\n+            Long transformBucketKey = (Long) XContentMapValues.extractValue(\"second\", source);\n+\n+            // test correctness, the results from the aggregation and the results from the transform should be the same\n+            assertThat(\n+                \"Buckets did not match, source: \" + source + \", expected: \" + bucket.get(\"second\") + \", iteration: \" + iteration,\n+                transformBucketKey,\n+                equalTo(bucket.get(\"second\"))\n+            );\n+            assertThat(\n+                \"Doc count did not match, source: \" + source + \", expected: \" + bucket.get(\"count\") + \", iteration: \" + iteration,\n+                XContentMapValues.extractValue(\"count\", source),\n+                equalTo(Double.valueOf(((Long) bucket.get(\"count\"))))\n+            );\n+            assertThat(\n+                \"Term did not match, source: \" + source + \", expected: \" + bucket.get(\"event\") + \", iteration: \" + iteration,\n+                XContentMapValues.extractValue(\"event\", source),\n+                equalTo(bucket.get(\"event\"))\n+            );\n+\n+            // transform should only rewrite documents that require it\n+            assertThat(\n+                \"Ingest run: \"\n+                    + XContentMapValues.extractValue(INGEST_RUN_FIELD, source)\n+                    + \" did not match max run: \"\n+                    + XContentMapValues.extractValue(MAX_RUN_FIELD, source)\n+                    + \", iteration: \"\n+                    + iteration,\n+                // we use a fixed_interval of `1s`, the transform runs every `1s`, a bucket might be recalculated at the next run\n+                // but should NOT be recalculated for the 2nd/3rd/... run\n+                Double.valueOf((Integer) XContentMapValues.extractValue(INGEST_RUN_FIELD, source)) - (Double) XContentMapValues\n+                    .extractValue(MAX_RUN_FIELD, source),\n+                is(lessThanOrEqualTo(1.0))\n+            );\n+        }\n+        assertFalse(sourceIterator.hasNext());\n+        assertFalse(destIterator.hasNext());\n+    }\n+\n+    private Map<String, Object> flattenedResult(long second, String event, long count) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3689b71849b32c324833b5f0fa42e0860b180579"}, "originalPosition": 227}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "23bfac4e91546ebce1a79f74174f29c8c1225f1a", "author": {"user": {"login": "hendrikmuhs", "name": "Hendrik Muhs"}}, "url": "https://github.com/elastic/elasticsearch/commit/23bfac4e91546ebce1a79f74174f29c8c1225f1a", "committedDate": "2020-10-16T09:11:53Z", "message": "apply review suggestion"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4309, "cost": 1, "resetAt": "2021-10-28T18:54:27Z"}}}