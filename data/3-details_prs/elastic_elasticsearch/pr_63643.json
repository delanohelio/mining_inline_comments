{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTAyODExMTQ1", "number": 63643, "title": "Speed up date_histogram without children", "bodyText": "This speeds up date_histogram aggregations without a parent or\nchildren. This is quite common - it's the aggregation that Kibana's Discover\nuses all over the place. Also, we hope to be able to use the same\nmechanism to speed aggs with children one day, but that day isn't today.\nThe kind of speedup we're seeing is fairly substantial in many cases:\n|                              |                                            |  before |   after |    |\n| 90th percentile service time |           date_histogram_calendar_interval | 9266.07 | 1376.13 | ms |\n| 90th percentile service time |   date_histogram_calendar_interval_with_tz | 9217.21 | 1372.67 | ms |\n| 90th percentile service time |              date_histogram_fixed_interval | 8817.36 | 1312.67 | ms |\n| 90th percentile service time |      date_histogram_fixed_interval_with_tz | 8801.71 | 1311.69 | ms | <-- discover's agg\n| 90th percentile service time | date_histogram_fixed_interval_with_metrics | 44660.2 | 43789.5 | ms |\n\nThis uses the work we did in #61467 to precompute the rounding points for\na date_histogram. Now, when we know the rounding points we execute the\ndate_histogram as a range aggregation. This is nice for two reasons:\n\nWe can further rewrite the range aggregation (see below)\nWe don't need to allocate a hash to convert rounding points\nto ordinals.\nWe can send precise cardinality estimates to sub-aggs.\n\nPoints 2 and 3 above are nice, but most of the speed difference comes from\npoint 1. Specifically, we now look into executing range aggregations as\na filters aggregation. Normally the filters aggregation is quite slow\nbut when it doesn't have a parent or any children then we can execute it\n\"filter by filter\" which is significantly faster. So fast, in fact, that\nit is faster than the original date_histogram.\nThe range aggregation is fairly careful in how it rewrites, giving up\non the filters aggregation if it won't collect \"filter by filter\" and\nfalling back to its original execution mechanism.\nSo an aggregation like this:\nPOST _search\n{\n  \"size\": 0,\n  \"query\": {\n    \"range\": {\n      \"dropoff_datetime\": {\n        \"gte\": \"2015-01-01 00:00:00\",\n        \"lt\": \"2016-01-01 00:00:00\"\n      }\n    }\n  },\n  \"aggs\": {\n    \"dropoffs_over_time\": {\n      \"date_histogram\": {\n        \"field\": \"dropoff_datetime\",\n        \"fixed_interval\": \"60d\",\n        \"time_zone\": \"America/New_York\"\n      }\n    }\n  }\n}\n\nis executed like:\nPOST _search\n{\n  \"size\": 0,\n  \"query\": {\n    \"range\": {\n      \"dropoff_datetime\": {\n        \"gte\": \"2015-01-01 00:00:00\",\n        \"lt\": \"2016-01-01 00:00:00\"\n      }\n    }\n  },\n  \"aggs\": {\n    \"dropoffs_over_time\": {\n      \"range\": {\n        \"field\": \"dropoff_datetime\",\n        \"ranges\": [\n          {\"from\": 1415250000000, \"to\": 1420434000000},\n          {\"from\": 1420434000000, \"to\": 1425618000000},\n          {\"from\": 1425618000000, \"to\": 1430798400000},\n          {\"from\": 1430798400000, \"to\": 1435982400000},\n          {\"from\": 1435982400000, \"to\": 1441166400000},\n          {\"from\": 1441166400000, \"to\": 1446350400000},\n          {\"from\": 1446350400000, \"to\": 1451538000000},\n          {\"from\": 1451538000000}\n        ]\n      }\n    }\n  }\n}\n\nWhich in turn is executed like this:\nPOST _search\n{\n  \"size\": 0,\n  \"query\": {\n    \"range\": {\n      \"dropoff_datetime\": {\n        \"gte\": \"2015-01-01 00:00:00\",\n        \"lt\": \"2016-01-01 00:00:00\"\n      }\n    }\n  },\n  \"aggs\": {\n    \"dropoffs_over_time\": {\n      \"filters\": {\n        \"filters\": {\n          \"1\": {\"range\": {\"dropoff_datetime\": {\"gte\": \"2014-12-30 00:00:00\", \"lt\": \"2015-01-05 05:00:00\"}}},\n          \"2\": {\"range\": {\"dropoff_datetime\": {\"gte\": \"2015-01-05 05:00:00\", \"lt\": \"2015-03-06 05:00:00\"}}},\n          \"3\": {\"range\": {\"dropoff_datetime\": {\"gte\": \"2015-03-06 00:00:00\", \"lt\": \"2015-05-05 00:00:00\"}}},\n          \"4\": {\"range\": {\"dropoff_datetime\": {\"gte\": \"2015-05-05 00:00:00\", \"lt\": \"2015-07-04 00:00:00\"}}},\n          \"5\": {\"range\": {\"dropoff_datetime\": {\"gte\": \"2015-07-04 00:00:00\", \"lt\": \"2015-09-02 00:00:00\"}}},\n          \"6\": {\"range\": {\"dropoff_datetime\": {\"gte\": \"2015-09-02 00:00:00\", \"lt\": \"2015-11-01 00:00:00\"}}},\n          \"7\": {\"range\": {\"dropoff_datetime\": {\"gte\": \"2015-11-01 00:00:00\", \"lt\": \"2015-12-31 00:00:00\"}}},\n          \"8\": {\"range\": {\"dropoff_datetime\": {\"gte\": \"2015-12-31 00:00:00\"}}}\n        }\n      }\n    }\n  }\n}\n\nAnd that is faster because we can execute it \"filter by filter\".\nFinally, notice the range query filtering the data. That is required for\nthe data set that I'm using for testing. The \"filter by filter\" collection\nmechanism for the filters agg needs special case handling when the query\nis a range query and the filter is a range query and they are both on\nthe same field. That special case handling \"merges\" the range query.\nWithout it \"filter by filter\" collection is substantially slower. Its still\nquite a bit quicker than the standard filter collection, but not nearly\nas fast as it could be.", "createdAt": "2020-10-13T20:37:20Z", "url": "https://github.com/elastic/elasticsearch/pull/63643", "merged": true, "mergeCommit": {"oid": "7ceed1369dcec5eacd4f023e3b917dea5084b3dc"}, "closed": true, "closedAt": "2020-11-09T19:20:26Z", "author": {"login": "nik9000"}, "timelineItems": {"totalCount": 73, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdR5O2_AH2gAyNTAyODExMTQ1OjNlYjk0YmNmM2E5Mjg1Y2FkZTRiZDhhNDIzZTdiYWZiNzYzMjgzNzc=", "endCursor": "Y3Vyc29yOnYyOpPPAAABda4oa7AH2gAyNTAyODExMTQ1OmZjMTQ0NGQ4MTU3NmNkMDQ1MWYyYzc2YjNhNmI5Y2NjNThhZDc1MGI=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "3eb94bcf3a9285cade4bd8a423e7bafb76328377", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/3eb94bcf3a9285cade4bd8a423e7bafb76328377", "committedDate": "2020-10-12T19:34:46Z", "message": "Execute date_histo agg as date_range agg\n\nWIP"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3f99f0431ed868959b4941f64189c4d87f7a8ef1", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/3f99f0431ed868959b4941f64189c4d87f7a8ef1", "committedDate": "2020-10-12T19:34:46Z", "message": "factor out collector"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6c8cb0bcbd9742d7aa3c2423f71ccde190069478", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/6c8cb0bcbd9742d7aa3c2423f71ccde190069478", "committedDate": "2020-10-12T19:34:46Z", "message": "ordered"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "995ca2428be12b7c3d6bb9b6570a4ccb9ed97d08", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/995ca2428be12b7c3d6bb9b6570a4ccb9ed97d08", "committedDate": "2020-10-12T19:34:46Z", "message": "refactor"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d7becd802ac13a937b90d87489001c4b20bfecd8", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/d7becd802ac13a937b90d87489001c4b20bfecd8", "committedDate": "2020-10-12T19:44:36Z", "message": "Fixup"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0a1987d99b5ff07cdbbe5100cf291cb45cfefd08", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/0a1987d99b5ff07cdbbe5100cf291cb45cfefd08", "committedDate": "2020-10-12T20:17:42Z", "message": "Better name"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1258ad4d88b441a88f315fdcf5984b9349473e38", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/1258ad4d88b441a88f315fdcf5984b9349473e38", "committedDate": "2020-10-12T21:36:44Z", "message": "Experiment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "da02047b005365a94f687994e8426cd921a8c521", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/da02047b005365a94f687994e8426cd921a8c521", "committedDate": "2020-10-12T22:01:26Z", "message": "Rework"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9157f00d32ed4293b6fd9325675d62d72af23c82", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/9157f00d32ed4293b6fd9325675d62d72af23c82", "committedDate": "2020-10-13T12:21:57Z", "message": "Use query"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "20047dc92fc4bf190ede12ecf9759c6cfe0210f8", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/20047dc92fc4bf190ede12ecf9759c6cfe0210f8", "committedDate": "2020-10-13T13:41:08Z", "message": "Super hack"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e69628e7e081daea268a860fb329f8a1b984f44e", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/e69628e7e081daea268a860fb329f8a1b984f44e", "committedDate": "2020-10-13T17:01:09Z", "message": "Shuffle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "607ae5f7c245c05a2414e0ceb50e417ad7c0e67e", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/607ae5f7c245c05a2414e0ceb50e417ad7c0e67e", "committedDate": "2020-10-13T17:38:51Z", "message": "Tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0e68cad023bb5594110e176f53865b889c39be22", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/0e68cad023bb5594110e176f53865b889c39be22", "committedDate": "2020-10-13T18:21:28Z", "message": "look"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9754fb0303803ae7fec06351ca3cbdebd0ee4391", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/9754fb0303803ae7fec06351ca3cbdebd0ee4391", "committedDate": "2020-10-13T18:34:55Z", "message": "no looking"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA3ODExMDUz", "url": "https://github.com/elastic/elasticsearch/pull/63643#pullrequestreview-507811053", "createdAt": "2020-10-13T20:39:23Z", "commit": {"oid": "9754fb0303803ae7fec06351ca3cbdebd0ee4391"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QyMDozOToyM1rOHg4p6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QyMDozOToyM1rOHg4p6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDI0NDcxNA==", "bodyText": "This method and everything in it is kind of shameful but it gives a 2x speed improvement.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r504244714", "createdAt": "2020-10-13T20:39:23Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java", "diffHunk": "@@ -185,16 +235,234 @@ public InternalAggregation buildEmptyAggregation() {\n             buckets.add(bucket);\n         }\n \n-        if (showOtherBucket) {\n+        if (otherBucketKey != null) {\n             InternalFilters.InternalBucket bucket = new InternalFilters.InternalBucket(otherBucketKey, 0, subAggs, keyed);\n             buckets.add(bucket);\n         }\n \n         return new InternalFilters(name, buckets, keyed, metadata());\n     }\n \n-    final long bucketOrd(long owningBucketOrdinal, int filterOrd) {\n-        return owningBucketOrdinal * totalNumKeys + filterOrd;\n+    public abstract boolean collectsInFilterOrder();\n+\n+    private static class FilterOrderAggregator extends FiltersAggregator {\n+        private final Query[] filters;\n+        private Weight[] filterWeights;\n+\n+        public FilterOrderAggregator(\n+            String name,\n+            String[] keys,\n+            Query[] filters,\n+            boolean keyed,\n+            SearchContext context,\n+            Aggregator parent,\n+            CardinalityUpperBound cardinality,\n+            Map<String, Object> metadata\n+        ) throws IOException {\n+            super(name, AggregatorFactories.EMPTY, keys, keyed, null, context, parent, cardinality, metadata);\n+            this.filters = filters;\n+        }\n+\n+        @Override\n+        protected LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n+            if (filterWeights == null) {\n+                filterWeights = buildWeights(context.query(), filters);\n+            }\n+            Bits live = ctx.reader().getLiveDocs();\n+            for (int filterOrd = 0; filterOrd < filters.length; filterOrd++) {\n+                Scorer scorer = filterWeights[filterOrd].scorer(ctx);\n+                if (scorer == null) {\n+                    // the filter doesn't match any docs\n+                    continue;\n+                }\n+                DocIdSetIterator itr = scorer.iterator();\n+                if (live == null) {\n+                    while (itr.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n+                        collectBucket(sub, itr.docID(), filterOrd);\n+                    }\n+                } else {\n+                    while (itr.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n+                        if (live.get(itr.docID())) {\n+                            collectBucket(sub, itr.docID(), filterOrd);\n+                        }\n+                    }\n+                }\n+            }\n+            throw new CollectionTerminatedException();\n+        }\n+\n+        @Override\n+        public boolean collectsInFilterOrder() {\n+            return true;\n+        }\n     }\n \n+    private static class StandardOrderAggregator extends FiltersAggregator {\n+        private final Query[] filters;\n+        private Weight[] filterWeights;\n+\n+        private final int totalNumKeys;\n+\n+        public StandardOrderAggregator(\n+            String name,\n+            AggregatorFactories factories,\n+            String[] keys,\n+            Query[] filters,\n+            boolean keyed,\n+            String otherBucketKey,\n+            SearchContext context,\n+            Aggregator parent,\n+            CardinalityUpperBound cardinality,\n+            Map<String, Object> metadata\n+        ) throws IOException {\n+            super(name, factories, keys, keyed, otherBucketKey, context, parent, cardinality, metadata);\n+            this.filters = filters;\n+            if (otherBucketKey == null) {\n+                this.totalNumKeys = keys.length;\n+            } else {\n+                this.totalNumKeys = keys.length + 1;\n+            }\n+        }\n+\n+        @Override\n+        protected LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n+            if (filterWeights == null) {\n+                filterWeights = buildWeights(new MatchAllDocsQuery(), filters);\n+            }\n+            final Bits[] bits = new Bits[filters.length];\n+            for (int i = 0; i < filters.length; ++i) {\n+                bits[i] = Lucene.asSequentialAccessBits(ctx.reader().maxDoc(), filterWeights[i].scorerSupplier(ctx));\n+            }\n+            return new LeafBucketCollectorBase(sub, null) {\n+                @Override\n+                public void collect(int doc, long bucket) throws IOException {\n+                    boolean matched = false;\n+                    for (int i = 0; i < bits.length; i++) {\n+                        if (bits[i].get(doc)) {\n+                            collectBucket(sub, doc, bucketOrd(bucket, i));\n+                            matched = true;\n+                        }\n+                    }\n+                    if (otherBucketKey != null && false == matched) {\n+                        collectBucket(sub, doc, bucketOrd(bucket, bits.length));\n+                    }\n+                }\n+            };\n+        }\n+\n+        final long bucketOrd(long owningBucketOrdinal, int filterOrd) {\n+            return owningBucketOrdinal * totalNumKeys + filterOrd;\n+        }\n+\n+        @Override\n+        public boolean collectsInFilterOrder() {\n+            return false;\n+        }\n+    }\n+\n+    protected Weight[] buildWeights(Query topLevelQuery, Query filters[]) throws IOException{\n+        Weight[] weights = new Weight[filters.length];\n+        for (int i = 0; i < filters.length; ++i) {\n+            Query filter = filterMatchingBoth(topLevelQuery, filters[i]);\n+            weights[i] = context.searcher().createWeight(context.searcher().rewrite(filter), ScoreMode.COMPLETE_NO_SCORES, 1);\n+        }\n+        return weights;\n+    }\n+\n+    private Query filterMatchingBoth(Query lhs, Query rhs) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9754fb0303803ae7fec06351ca3cbdebd0ee4391"}, "originalPosition": 308}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0750aa8bbe9c8a6bee2d1fa5c44307fa2369fe4b", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/0750aa8bbe9c8a6bee2d1fa5c44307fa2369fe4b", "committedDate": "2020-10-13T20:42:17Z", "message": "tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c57c98a787bc9fab00b042de89a04dd06a937b6d", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/c57c98a787bc9fab00b042de89a04dd06a937b6d", "committedDate": "2020-10-13T21:51:33Z", "message": "Handle unbounded ranges"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "402132781b8530f616ad6474c0b7652a588404c7", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/402132781b8530f616ad6474c0b7652a588404c7", "committedDate": "2020-10-13T21:53:25Z", "message": "Test for max and min"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7a44c21e1c1a24f66aefccbe107e648662665866", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/7a44c21e1c1a24f66aefccbe107e648662665866", "committedDate": "2020-10-14T16:07:02Z", "message": "Merge branch 'master' into date_histo_as_range"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "73aaa455a23286c8275510d09766e96346e5afbb", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/73aaa455a23286c8275510d09766e96346e5afbb", "committedDate": "2020-10-14T17:13:01Z", "message": "Fixup profiler"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b73df70e183e4940051134dedb6b0fbe7d589f9d", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/b73df70e183e4940051134dedb6b0fbe7d589f9d", "committedDate": "2020-10-14T19:57:33Z", "message": "Rate agg\n\nThis is weird"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b7e8dccef147a4916c6f514ee7928218ebde15d3", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/b7e8dccef147a4916c6f514ee7928218ebde15d3", "committedDate": "2020-10-14T20:03:57Z", "message": "WIP"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7c18141d23dc3e7f4dcd5883068c88424147af2f", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/7c18141d23dc3e7f4dcd5883068c88424147af2f", "committedDate": "2020-10-14T20:36:19Z", "message": "Fixup weird formats"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2d04e358e33aeaf528221b900d41854118b37125", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/2d04e358e33aeaf528221b900d41854118b37125", "committedDate": "2020-10-14T20:43:06Z", "message": "Feh"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "102e5263357ef58f73b24333f46dddc5dfb6e52d", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/102e5263357ef58f73b24333f46dddc5dfb6e52d", "committedDate": "2020-10-14T20:45:37Z", "message": "Shift"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e333cbb179ef4d3392e2439aa26ae8702a6c6287", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/e333cbb179ef4d3392e2439aa26ae8702a6c6287", "committedDate": "2020-10-14T21:12:45Z", "message": "Forbidden"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e2fd164096dc653417c16587a43805c5a02e2a90", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/e2fd164096dc653417c16587a43805c5a02e2a90", "committedDate": "2020-10-15T12:59:54Z", "message": "Moar tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "46a10159401aac957f10196e619f3f08b64958d9", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/46a10159401aac957f10196e619f3f08b64958d9", "committedDate": "2020-10-15T13:53:22Z", "message": "test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "deb56842f63ef0827f354f52582c7e830ca5b4ea", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/deb56842f63ef0827f354f52582c7e830ca5b4ea", "committedDate": "2020-10-15T14:14:28Z", "message": "Fixup tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/de96179d12412bf51045607ab1b27e87e6a43d4e", "committedDate": "2020-10-15T15:37:37Z", "message": "precommit"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA4MzUzNDgy", "url": "https://github.com/elastic/elasticsearch/pull/63643#pullrequestreview-508353482", "createdAt": "2020-10-14T13:36:32Z", "commit": {"oid": "402132781b8530f616ad6474c0b7652a588404c7"}, "state": "APPROVED", "comments": {"totalCount": 15, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxMzozNjozMlrOHhTXjA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQxNzo1NTozOVrOHiTe3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY4MjM4MA==", "bodyText": "On my first read, I thought we might round down to null.  How about:\nIf this rounding mechanism precalculates rounding points, this array stores dates such that each\ndate between each entry will be rounded down to that entry.  If the rounding mechanism does not\nsupport the optimization, this array is {@code null}.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r504682380", "createdAt": "2020-10-14T13:36:32Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/common/Rounding.java", "diffHunk": "@@ -291,6 +291,12 @@ public void writeTo(StreamOutput out) throws IOException {\n          * next rounded value in specified units if possible.\n          */\n         double roundingSize(long utcMillis, DateTimeUnit timeUnit);\n+        /**\n+         * An array of dates such that each date between each entry is will\n+         * be rounded down to that entry or {@code null} if this rounding\n+         * mechanism doesn't or can't precalculate these points.\n+         */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "402132781b8530f616ad6474c0b7652a588404c7"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc0NzA0Mg==", "bodyText": "I'm not entirely convinced that the way we're shoehorning GeoDistance into the range framework is a good idea.  The fact that we need this supports that viewpoint, I think.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r504747042", "createdAt": "2020-10-14T14:56:50Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/GeoDistanceRangeAggregatorFactory.java", "diffHunk": "@@ -66,7 +66,7 @@ public static void registerAggregators(ValuesSourceRegistry.Builder builder) {\n                 cardinality,\n                 metadata) -> {\n                 DistanceSource distanceSource = new DistanceSource((ValuesSource.GeoPoint) valuesSource, distanceType, origin, units);\n-                return new RangeAggregator(\n+                return RangeAggregator.buildWithoutAttemptedToAdaptToFilters(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "402132781b8530f616ad6474c0b7652a588404c7"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTY1MzEwOQ==", "bodyText": "This could use some javadoc.  It's not clear from the context that it's intended to be used with the AddaptingAggregator (or, really, wrapping aggs in general, although I don't think we have any others)", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505653109", "createdAt": "2020-10-15T15:51:17Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactories.java", "diffHunk": "@@ -227,6 +227,18 @@ public int countAggregators() {\n         return factories.length;\n     }\n \n+    public AggregatorFactories fixParent(Aggregator fixedParent) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTY1OTUyOQ==", "bodyText": "Bit of javadoc would be nice here.  It looks like the intention is for this to only have the implementations already built in this file, would be good to document that.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505659529", "createdAt": "2020-10-15T15:59:57Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java", "diffHunk": "@@ -45,9 +56,11 @@\n import java.util.List;\n import java.util.Map;\n import java.util.Objects;\n-import java.util.function.Supplier;\n+import java.util.function.BiConsumer;\n \n-public class FiltersAggregator extends BucketsAggregator {\n+import static java.util.Arrays.compareUnsigned;\n+\n+public abstract class FiltersAggregator extends BucketsAggregator {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTY5NTk1Mg==", "bodyText": "Javadoc here too please.  This operates differently from what this method usually does (e.g. doesn't return a LeafBucketCollector), and it would be good to make a note of why that's correct.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505695952", "createdAt": "2020-10-15T16:55:15Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java", "diffHunk": "@@ -185,16 +243,254 @@ public InternalAggregation buildEmptyAggregation() {\n             buckets.add(bucket);\n         }\n \n-        if (showOtherBucket) {\n+        if (otherBucketKey != null) {\n             InternalFilters.InternalBucket bucket = new InternalFilters.InternalBucket(otherBucketKey, 0, subAggs, keyed);\n             buckets.add(bucket);\n         }\n \n         return new InternalFilters(name, buckets, keyed, metadata());\n     }\n \n-    final long bucketOrd(long owningBucketOrdinal, int filterOrd) {\n-        return owningBucketOrdinal * totalNumKeys + filterOrd;\n+    public abstract boolean collectsInFilterOrder();\n+\n+    /**\n+     * Collects results by running each filter against the searcher and doesn't\n+     * build any {@link LeafBucketCollector}s which is generally faster than\n+     * {@link Compatible} but doesn't support when there is a parent aggregator\n+     * or any child aggregators.\n+     */\n+    private static class FilterByFilter extends FiltersAggregator {\n+        private final Query[] filters;\n+        private Weight[] filterWeights;\n+        private int segmentsWithDeletedDocs;\n+\n+        FilterByFilter(\n+            String name,\n+            String[] keys,\n+            Query[] filters,\n+            boolean keyed,\n+            SearchContext context,\n+            Aggregator parent,\n+            CardinalityUpperBound cardinality,\n+            Map<String, Object> metadata\n+        ) throws IOException {\n+            super(name, AggregatorFactories.EMPTY, keys, keyed, null, context, parent, cardinality, metadata);\n+            this.filters = filters;\n+        }\n+\n+        @Override\n+        protected LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 218}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTY5OTAzNA==", "bodyText": "Why do we throw here?", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505699034", "createdAt": "2020-10-15T17:00:01Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java", "diffHunk": "@@ -185,16 +243,254 @@ public InternalAggregation buildEmptyAggregation() {\n             buckets.add(bucket);\n         }\n \n-        if (showOtherBucket) {\n+        if (otherBucketKey != null) {\n             InternalFilters.InternalBucket bucket = new InternalFilters.InternalBucket(otherBucketKey, 0, subAggs, keyed);\n             buckets.add(bucket);\n         }\n \n         return new InternalFilters(name, buckets, keyed, metadata());\n     }\n \n-    final long bucketOrd(long owningBucketOrdinal, int filterOrd) {\n-        return owningBucketOrdinal * totalNumKeys + filterOrd;\n+    public abstract boolean collectsInFilterOrder();\n+\n+    /**\n+     * Collects results by running each filter against the searcher and doesn't\n+     * build any {@link LeafBucketCollector}s which is generally faster than\n+     * {@link Compatible} but doesn't support when there is a parent aggregator\n+     * or any child aggregators.\n+     */\n+    private static class FilterByFilter extends FiltersAggregator {\n+        private final Query[] filters;\n+        private Weight[] filterWeights;\n+        private int segmentsWithDeletedDocs;\n+\n+        FilterByFilter(\n+            String name,\n+            String[] keys,\n+            Query[] filters,\n+            boolean keyed,\n+            SearchContext context,\n+            Aggregator parent,\n+            CardinalityUpperBound cardinality,\n+            Map<String, Object> metadata\n+        ) throws IOException {\n+            super(name, AggregatorFactories.EMPTY, keys, keyed, null, context, parent, cardinality, metadata);\n+            this.filters = filters;\n+        }\n+\n+        @Override\n+        protected LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n+            if (filterWeights == null) {\n+                filterWeights = buildWeights(context.query(), filters);\n+            }\n+            Bits live = ctx.reader().getLiveDocs();\n+            for (int filterOrd = 0; filterOrd < filters.length; filterOrd++) {\n+                Scorer scorer = filterWeights[filterOrd].scorer(ctx);\n+                if (scorer == null) {\n+                    // the filter doesn't match any docs\n+                    continue;\n+                }\n+                DocIdSetIterator itr = scorer.iterator();\n+                if (live == null) {\n+                    while (itr.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n+                        collectBucket(sub, itr.docID(), filterOrd);\n+                    }\n+                } else {\n+                    segmentsWithDeletedDocs++;\n+                    while (itr.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n+                        if (live.get(itr.docID())) {\n+                            collectBucket(sub, itr.docID(), filterOrd);\n+                        }\n+                    }\n+                }\n+            }\n+            throw new CollectionTerminatedException();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 243}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcwMzA0NQ==", "bodyText": "so, this merges two filter queries so they can be performed in one pass?  I know it's a private method, but I still think a bit of documentation for what it does and why that's important would be good.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505703045", "createdAt": "2020-10-15T17:06:33Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java", "diffHunk": "@@ -185,16 +235,234 @@ public InternalAggregation buildEmptyAggregation() {\n             buckets.add(bucket);\n         }\n \n-        if (showOtherBucket) {\n+        if (otherBucketKey != null) {\n             InternalFilters.InternalBucket bucket = new InternalFilters.InternalBucket(otherBucketKey, 0, subAggs, keyed);\n             buckets.add(bucket);\n         }\n \n         return new InternalFilters(name, buckets, keyed, metadata());\n     }\n \n-    final long bucketOrd(long owningBucketOrdinal, int filterOrd) {\n-        return owningBucketOrdinal * totalNumKeys + filterOrd;\n+    public abstract boolean collectsInFilterOrder();\n+\n+    private static class FilterOrderAggregator extends FiltersAggregator {\n+        private final Query[] filters;\n+        private Weight[] filterWeights;\n+\n+        public FilterOrderAggregator(\n+            String name,\n+            String[] keys,\n+            Query[] filters,\n+            boolean keyed,\n+            SearchContext context,\n+            Aggregator parent,\n+            CardinalityUpperBound cardinality,\n+            Map<String, Object> metadata\n+        ) throws IOException {\n+            super(name, AggregatorFactories.EMPTY, keys, keyed, null, context, parent, cardinality, metadata);\n+            this.filters = filters;\n+        }\n+\n+        @Override\n+        protected LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n+            if (filterWeights == null) {\n+                filterWeights = buildWeights(context.query(), filters);\n+            }\n+            Bits live = ctx.reader().getLiveDocs();\n+            for (int filterOrd = 0; filterOrd < filters.length; filterOrd++) {\n+                Scorer scorer = filterWeights[filterOrd].scorer(ctx);\n+                if (scorer == null) {\n+                    // the filter doesn't match any docs\n+                    continue;\n+                }\n+                DocIdSetIterator itr = scorer.iterator();\n+                if (live == null) {\n+                    while (itr.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n+                        collectBucket(sub, itr.docID(), filterOrd);\n+                    }\n+                } else {\n+                    while (itr.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n+                        if (live.get(itr.docID())) {\n+                            collectBucket(sub, itr.docID(), filterOrd);\n+                        }\n+                    }\n+                }\n+            }\n+            throw new CollectionTerminatedException();\n+        }\n+\n+        @Override\n+        public boolean collectsInFilterOrder() {\n+            return true;\n+        }\n     }\n \n+    private static class StandardOrderAggregator extends FiltersAggregator {\n+        private final Query[] filters;\n+        private Weight[] filterWeights;\n+\n+        private final int totalNumKeys;\n+\n+        public StandardOrderAggregator(\n+            String name,\n+            AggregatorFactories factories,\n+            String[] keys,\n+            Query[] filters,\n+            boolean keyed,\n+            String otherBucketKey,\n+            SearchContext context,\n+            Aggregator parent,\n+            CardinalityUpperBound cardinality,\n+            Map<String, Object> metadata\n+        ) throws IOException {\n+            super(name, factories, keys, keyed, otherBucketKey, context, parent, cardinality, metadata);\n+            this.filters = filters;\n+            if (otherBucketKey == null) {\n+                this.totalNumKeys = keys.length;\n+            } else {\n+                this.totalNumKeys = keys.length + 1;\n+            }\n+        }\n+\n+        @Override\n+        protected LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n+            if (filterWeights == null) {\n+                filterWeights = buildWeights(new MatchAllDocsQuery(), filters);\n+            }\n+            final Bits[] bits = new Bits[filters.length];\n+            for (int i = 0; i < filters.length; ++i) {\n+                bits[i] = Lucene.asSequentialAccessBits(ctx.reader().maxDoc(), filterWeights[i].scorerSupplier(ctx));\n+            }\n+            return new LeafBucketCollectorBase(sub, null) {\n+                @Override\n+                public void collect(int doc, long bucket) throws IOException {\n+                    boolean matched = false;\n+                    for (int i = 0; i < bits.length; i++) {\n+                        if (bits[i].get(doc)) {\n+                            collectBucket(sub, doc, bucketOrd(bucket, i));\n+                            matched = true;\n+                        }\n+                    }\n+                    if (otherBucketKey != null && false == matched) {\n+                        collectBucket(sub, doc, bucketOrd(bucket, bits.length));\n+                    }\n+                }\n+            };\n+        }\n+\n+        final long bucketOrd(long owningBucketOrdinal, int filterOrd) {\n+            return owningBucketOrdinal * totalNumKeys + filterOrd;\n+        }\n+\n+        @Override\n+        public boolean collectsInFilterOrder() {\n+            return false;\n+        }\n+    }\n+\n+    protected Weight[] buildWeights(Query topLevelQuery, Query filters[]) throws IOException{\n+        Weight[] weights = new Weight[filters.length];\n+        for (int i = 0; i < filters.length; ++i) {\n+            Query filter = filterMatchingBoth(topLevelQuery, filters[i]);\n+            weights[i] = context.searcher().createWeight(context.searcher().rewrite(filter), ScoreMode.COMPLETE_NO_SCORES, 1);\n+        }\n+        return weights;\n+    }\n+\n+    private Query filterMatchingBoth(Query lhs, Query rhs) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDI0NDcxNA=="}, "originalCommit": {"oid": "9754fb0303803ae7fec06351ca3cbdebd0ee4391"}, "originalPosition": 308}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcwNjAzNQ==", "bodyText": "I think you lost a verb: \"because it doesn't need to the rounding points\"?  Compute, maybe?", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505706035", "createdAt": "2020-10-15T17:10:47Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java", "diffHunk": "@@ -51,6 +62,143 @@\n  * @see Rounding\n  */\n class DateHistogramAggregator extends BucketsAggregator implements SizedBucketAggregator {\n+    /**\n+     * Build an {@link Aggregator} for a {@code date_histogram} aggregation.\n+     * If we can determine the bucket boundaries from\n+     * {@link Rounding.Prepared#fixedRoundingPoints()} we use\n+     * {@link RangeAggregator} to do the actual collecting, otherwise we use\n+     * an specialized {@link DateHistogramAggregator Aggregator} specifically\n+     * for the {@code date_histogram}s. We prefer to delegate to the\n+     * {@linkplain RangeAggregator} because it can sometimes be further\n+     * optimized into a {@link FiltersAggregator}. Even when it can't be\n+     * optimized, it is going to be marginally faster and consume less memory\n+     * than the {@linkplain DateHistogramAggregator} because it doesn't need\n+     * to the rounding points and because it can pass precise cardinality", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcwOTE1OQ==", "bodyText": "This is probably redundant, FYI.  In theory, if hasValues() == false, then we went into the createUnmapped path from the builder.  Harmless to check though.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505709159", "createdAt": "2020-10-15T17:16:03Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java", "diffHunk": "@@ -51,6 +62,143 @@\n  * @see Rounding\n  */\n class DateHistogramAggregator extends BucketsAggregator implements SizedBucketAggregator {\n+    /**\n+     * Build an {@link Aggregator} for a {@code date_histogram} aggregation.\n+     * If we can determine the bucket boundaries from\n+     * {@link Rounding.Prepared#fixedRoundingPoints()} we use\n+     * {@link RangeAggregator} to do the actual collecting, otherwise we use\n+     * an specialized {@link DateHistogramAggregator Aggregator} specifically\n+     * for the {@code date_histogram}s. We prefer to delegate to the\n+     * {@linkplain RangeAggregator} because it can sometimes be further\n+     * optimized into a {@link FiltersAggregator}. Even when it can't be\n+     * optimized, it is going to be marginally faster and consume less memory\n+     * than the {@linkplain DateHistogramAggregator} because it doesn't need\n+     * to the rounding points and because it can pass precise cardinality\n+     * estimates to its child aggregations.\n+     */\n+    public static Aggregator build(\n+        String name,\n+        AggregatorFactories factories,\n+        Rounding rounding,\n+        BucketOrder order,\n+        boolean keyed,\n+        long minDocCount,\n+        @Nullable LongBounds extendedBounds,\n+        @Nullable LongBounds hardBounds,\n+        ValuesSourceConfig valuesSourceConfig,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        Rounding.Prepared preparedRounding = valuesSourceConfig.roundingPreparer().apply(rounding);\n+        Aggregator asRange = adaptIntoRangeOrNull(\n+            name,\n+            factories,\n+            rounding,\n+            preparedRounding,\n+            order,\n+            keyed,\n+            minDocCount,\n+            extendedBounds,\n+            hardBounds,\n+            valuesSourceConfig,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+        if (asRange != null) {\n+            return asRange;\n+        }\n+        return new DateHistogramAggregator(\n+            name,\n+            factories,\n+            rounding,\n+            preparedRounding,\n+            order,\n+            keyed,\n+            minDocCount,\n+            extendedBounds,\n+            hardBounds,\n+            valuesSourceConfig,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+    }\n+\n+    private static FromDateRange adaptIntoRangeOrNull(\n+        String name,\n+        AggregatorFactories factories,\n+        Rounding rounding,\n+        Rounding.Prepared preparedRounding,\n+        BucketOrder order,\n+        boolean keyed,\n+        long minDocCount,\n+        @Nullable LongBounds extendedBounds,\n+        @Nullable LongBounds hardBounds,\n+        ValuesSourceConfig valuesSourceConfig,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        if (hardBounds != null) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.hasValues() == false) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcwOTY4NQ==", "bodyText": "fine for now, but I'm not sure why hard bounds can't be translated into the range aggregation.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505709685", "createdAt": "2020-10-15T17:16:59Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java", "diffHunk": "@@ -51,6 +62,143 @@\n  * @see Rounding\n  */\n class DateHistogramAggregator extends BucketsAggregator implements SizedBucketAggregator {\n+    /**\n+     * Build an {@link Aggregator} for a {@code date_histogram} aggregation.\n+     * If we can determine the bucket boundaries from\n+     * {@link Rounding.Prepared#fixedRoundingPoints()} we use\n+     * {@link RangeAggregator} to do the actual collecting, otherwise we use\n+     * an specialized {@link DateHistogramAggregator Aggregator} specifically\n+     * for the {@code date_histogram}s. We prefer to delegate to the\n+     * {@linkplain RangeAggregator} because it can sometimes be further\n+     * optimized into a {@link FiltersAggregator}. Even when it can't be\n+     * optimized, it is going to be marginally faster and consume less memory\n+     * than the {@linkplain DateHistogramAggregator} because it doesn't need\n+     * to the rounding points and because it can pass precise cardinality\n+     * estimates to its child aggregations.\n+     */\n+    public static Aggregator build(\n+        String name,\n+        AggregatorFactories factories,\n+        Rounding rounding,\n+        BucketOrder order,\n+        boolean keyed,\n+        long minDocCount,\n+        @Nullable LongBounds extendedBounds,\n+        @Nullable LongBounds hardBounds,\n+        ValuesSourceConfig valuesSourceConfig,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        Rounding.Prepared preparedRounding = valuesSourceConfig.roundingPreparer().apply(rounding);\n+        Aggregator asRange = adaptIntoRangeOrNull(\n+            name,\n+            factories,\n+            rounding,\n+            preparedRounding,\n+            order,\n+            keyed,\n+            minDocCount,\n+            extendedBounds,\n+            hardBounds,\n+            valuesSourceConfig,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+        if (asRange != null) {\n+            return asRange;\n+        }\n+        return new DateHistogramAggregator(\n+            name,\n+            factories,\n+            rounding,\n+            preparedRounding,\n+            order,\n+            keyed,\n+            minDocCount,\n+            extendedBounds,\n+            hardBounds,\n+            valuesSourceConfig,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+    }\n+\n+    private static FromDateRange adaptIntoRangeOrNull(\n+        String name,\n+        AggregatorFactories factories,\n+        Rounding rounding,\n+        Rounding.Prepared preparedRounding,\n+        BucketOrder order,\n+        boolean keyed,\n+        long minDocCount,\n+        @Nullable LongBounds extendedBounds,\n+        @Nullable LongBounds hardBounds,\n+        ValuesSourceConfig valuesSourceConfig,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        if (hardBounds != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 124}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcxODIxMA==", "bodyText": "I'm trying to move us away from null values sources.  By default, ValuesSourceConfig will return an empty values source for unmapped fields now.  Some aggregators still override that to be null internally, because of how we handle createUnmapped now.  But in general, we shouldn't be expecting null values sources.  Check ValuesSourceConfig#hasValues() instead.\nAlso, what ValuesSource?", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505718210", "createdAt": "2020-10-15T17:31:45Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java", "diffHunk": "@@ -195,4 +344,95 @@ public double bucketSize(long bucket, Rounding.DateTimeUnit unitSize) {\n             return 1.0;\n         }\n     }\n+\n+    private static class FromDateRange extends AdaptingAggregator implements SizedBucketAggregator {\n+        private final DocValueFormat format;\n+        private final Rounding rounding;\n+        private final Rounding.Prepared preparedRounding;\n+        private final BucketOrder order;\n+        private final long minDocCount;\n+        private final LongBounds extendedBounds;\n+        private final boolean keyed;\n+        private final long[] fixedRoundingPoints;\n+\n+        FromDateRange(\n+            Aggregator parent,\n+            AggregatorFactories subAggregators,\n+            CheckedFunction<AggregatorFactories, Aggregator, IOException> delegate,\n+            DocValueFormat format,\n+            Rounding rounding,\n+            Rounding.Prepared preparedRounding,\n+            BucketOrder order,\n+            long minDocCount,\n+            LongBounds extendedBounds,\n+            boolean keyed,\n+            long[] fixedRoundingPoints\n+        ) throws IOException {\n+            super(parent, subAggregators, delegate);\n+            this.format = format;\n+            this.rounding = rounding;\n+            this.preparedRounding = preparedRounding;\n+            this.order = order;\n+            order.validate(this);\n+            this.minDocCount = minDocCount;\n+            this.extendedBounds = extendedBounds;\n+            this.keyed = keyed;\n+            this.fixedRoundingPoints = fixedRoundingPoints;\n+        }\n+\n+        @Override\n+        protected InternalAggregation adapt(InternalAggregation delegateResult) {\n+            InternalDateRange range = (InternalDateRange) delegateResult;\n+            List<InternalDateHistogram.Bucket> buckets = new ArrayList<>(range.getBuckets().size());\n+            for (InternalDateRange.Bucket rangeBucket : range.getBuckets()) {\n+                if (rangeBucket.getDocCount() > 0) {\n+                    buckets.add(\n+                        new InternalDateHistogram.Bucket(\n+                            rangeBucket.getFrom().toInstant().toEpochMilli(),\n+                            rangeBucket.getDocCount(),\n+                            keyed,\n+                            format,\n+                            rangeBucket.getAggregations()\n+                        )\n+                    );\n+                }\n+            }\n+            CollectionUtil.introSort(buckets, BucketOrder.key(true).comparator());\n+\n+            // value source will be null for unmapped fields", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 257}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcyMDU3Mw==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505720573", "createdAt": "2020-10-15T17:35:15Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/AbstractRangeAggregatorFactory.java", "diffHunk": "@@ -92,8 +91,7 @@ protected Aggregator doCreateInternal(\n             .build(\n                 name,\n                 factories,\n-                (Numeric) config.getValuesSource(),\n-                config.format(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcyMTY2MA==", "bodyText": "Like with filters, I think a note explaining when to subclass this would be helpful.  Especially since there's already a lot of weird reuse patterns in the Range family.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505721660", "createdAt": "2020-10-15T17:37:11Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java", "diffHunk": "@@ -52,7 +63,7 @@\n \n import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n \n-public class RangeAggregator extends BucketsAggregator {\n+public abstract class RangeAggregator extends BucketsAggregator {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcyODcyMA==", "bodyText": "ValuesSourceConfig#getPointReaderOrNull() uses nearly the same set of checks for nearly the same reason - would it make sense to wrap all of these into one predicate on VSConfig?", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505728720", "createdAt": "2020-10-15T17:49:05Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java", "diffHunk": "@@ -215,15 +226,186 @@ public boolean equals(Object obj) {\n         }\n     }\n \n-    final ValuesSource.Numeric valuesSource;\n-    final DocValueFormat format;\n-    final Range[] ranges;\n-    final boolean keyed;\n-    final InternalRange.Factory rangeFactory;\n+    /**\n+     * Build an {@link Aggregator} for a {@code range} aggregation. If the\n+     * {@code ranges} can be converted into filters then it builds a\n+     * {@link FiltersAggregator} and uses that to collect the results\n+     * <strong>if</strong> that aggregator can run in \"filter by filter\"\n+     * collection mode. If it can't then we'll collect the ranges using\n+     * a native {@link RangeAggregator} which is significantly faster\n+     * than the \"compatible\" collection mechanism for the filters agg.\n+     */\n+    public static Aggregator build(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSourceConfig valuesSourceConfig,\n+        InternalRange.Factory<?, ?> rangeFactory,\n+        Range[] ranges,\n+        boolean keyed,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        Aggregator adapted = adaptIntoFiltersOrNull(\n+            name,\n+            factories,\n+            valuesSourceConfig,\n+            rangeFactory,\n+            ranges,\n+            keyed,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+        if (adapted != null) {\n+            return adapted;\n+        }\n+        return buildWithoutAttemptedToAdaptToFilters(\n+            name,\n+            factories,\n+            (ValuesSource.Numeric) valuesSourceConfig.getValuesSource(),\n+            valuesSourceConfig.format(),\n+            rangeFactory,\n+            ranges,\n+            keyed,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+    }\n \n-    final double[] maxTo;\n+    public static Aggregator adaptIntoFiltersOrNull(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSourceConfig valuesSourceConfig,\n+        InternalRange.Factory<?, ?> rangeFactory,\n+        Range[] ranges,\n+        boolean keyed,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        if (valuesSourceConfig.fieldType() == null) {\n+            return null;\n+        }\n+        if (false == valuesSourceConfig.fieldType().isSearchable()) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.missing() != null) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.script() != null) {\n+            return null;\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 129}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTczMjgyOA==", "bodyText": "Just to say it out loud: we don't care that we just built and then threw away an aggregator here, because aggregator creation time is not highly sensitive to performance hits, correct?", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505732828", "createdAt": "2020-10-15T17:55:39Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java", "diffHunk": "@@ -215,15 +226,186 @@ public boolean equals(Object obj) {\n         }\n     }\n \n-    final ValuesSource.Numeric valuesSource;\n-    final DocValueFormat format;\n-    final Range[] ranges;\n-    final boolean keyed;\n-    final InternalRange.Factory rangeFactory;\n+    /**\n+     * Build an {@link Aggregator} for a {@code range} aggregation. If the\n+     * {@code ranges} can be converted into filters then it builds a\n+     * {@link FiltersAggregator} and uses that to collect the results\n+     * <strong>if</strong> that aggregator can run in \"filter by filter\"\n+     * collection mode. If it can't then we'll collect the ranges using\n+     * a native {@link RangeAggregator} which is significantly faster\n+     * than the \"compatible\" collection mechanism for the filters agg.\n+     */\n+    public static Aggregator build(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSourceConfig valuesSourceConfig,\n+        InternalRange.Factory<?, ?> rangeFactory,\n+        Range[] ranges,\n+        boolean keyed,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        Aggregator adapted = adaptIntoFiltersOrNull(\n+            name,\n+            factories,\n+            valuesSourceConfig,\n+            rangeFactory,\n+            ranges,\n+            keyed,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+        if (adapted != null) {\n+            return adapted;\n+        }\n+        return buildWithoutAttemptedToAdaptToFilters(\n+            name,\n+            factories,\n+            (ValuesSource.Numeric) valuesSourceConfig.getValuesSource(),\n+            valuesSourceConfig.format(),\n+            rangeFactory,\n+            ranges,\n+            keyed,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+    }\n \n-    final double[] maxTo;\n+    public static Aggregator adaptIntoFiltersOrNull(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSourceConfig valuesSourceConfig,\n+        InternalRange.Factory<?, ?> rangeFactory,\n+        Range[] ranges,\n+        boolean keyed,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        if (valuesSourceConfig.fieldType() == null) {\n+            return null;\n+        }\n+        if (false == valuesSourceConfig.fieldType().isSearchable()) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.missing() != null) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.script() != null) {\n+            return null;\n+        }\n+        // TODO bail here for runtime fields. They'll be slower this way. Maybe we can somehow look at the Query?\n+        if (valuesSourceConfig.fieldType() instanceof DateFieldType\n+            && ((DateFieldType) valuesSourceConfig.fieldType()).resolution() == Resolution.NANOSECONDS) {\n+            // We don't generate sensible Queries for nanoseconds.\n+            return null;\n+        }\n+        String[] keys = new String[ranges.length];\n+        Query[] filters = new Query[ranges.length];\n+        for (int i = 0; i < ranges.length; i++) {\n+            keys[i] = Integer.toString(i);\n+            /*\n+             * Use the native format on the field rather than the one provided\n+             * on the valuesSourceConfig because the format on the field is what\n+             * we parse. With https://github.com/elastic/elasticsearch/pull/63692\n+             * we can just cast to a long here and it'll be taken as millis.\n+             */\n+            DocValueFormat format = valuesSourceConfig.fieldType().docValueFormat(null, null);\n+            filters[i] = valuesSourceConfig.fieldType()\n+                .rangeQuery(\n+                    ranges[i].from == Double.NEGATIVE_INFINITY ? null : format.format(ranges[i].from),\n+                    ranges[i].to == Double.POSITIVE_INFINITY ? null : format.format(ranges[i].to),\n+                    true,\n+                    false,\n+                    ShapeRelation.CONTAINS,\n+                    null,\n+                    null,\n+                    context.getQueryShardContext()\n+                );\n+        }\n+        RangeAggregator.FromFilters<?> fromFilters = new RangeAggregator.FromFilters<>(\n+            parent,\n+            factories,\n+            subAggregators -> FiltersAggregator.build(\n+                name,\n+                subAggregators,\n+                keys,\n+                filters,\n+                false,\n+                null,\n+                context,\n+                parent,\n+                cardinality,\n+                metadata\n+            ),\n+            valuesSourceConfig.format(),\n+            ranges,\n+            keyed,\n+            rangeFactory\n+        );\n+        if (false == ((FiltersAggregator) fromFilters.delegate()).collectsInFilterOrder()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 179}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4e9c9180286a9de2c358e5ba2adef8f0ab17c36c", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/4e9c9180286a9de2c358e5ba2adef8f0ab17c36c", "committedDate": "2020-10-16T16:24:33Z", "message": "Merge branch 'master' into date_histo_as_range"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f13a8ae89d99e7a8bc448ca734b2ed7c03fde224", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/f13a8ae89d99e7a8bc448ca734b2ed7c03fde224", "committedDate": "2020-10-16T16:25:11Z", "message": "Drop old extra test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6b3324713d5ba6934c3cd72cab56dd3f1eb5d438", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/6b3324713d5ba6934c3cd72cab56dd3f1eb5d438", "committedDate": "2020-10-19T13:21:41Z", "message": "TODO"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "14b49626d5c0f96c6062b3c524069953ad51cc72", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/14b49626d5c0f96c6062b3c524069953ad51cc72", "committedDate": "2020-10-20T13:39:47Z", "message": "Merge branch 'master' into date_histo_as_range"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2fd7e53762cbfc9ded7fae5b79796fe28941d442", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/2fd7e53762cbfc9ded7fae5b79796fe28941d442", "committedDate": "2020-10-20T14:42:09Z", "message": "Don't attempt the optimization if rounding would break it"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "97c35cdb46ee01c6fa4db505107d47b305f88791", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/97c35cdb46ee01c6fa4db505107d47b305f88791", "committedDate": "2020-10-20T16:00:20Z", "message": "move building\n\nThis way we don't have to worry about BigArrays and releasing and stuff."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6c7eaa1ef54adcf49f091459f22885b4e1ed88ec", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/6c7eaa1ef54adcf49f091459f22885b4e1ed88ec", "committedDate": "2020-10-20T16:06:25Z", "message": "Computers are hard"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5b5ac7be4ec09dee13f1d9d211d924098eba48dc", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/5b5ac7be4ec09dee13f1d9d211d924098eba48dc", "committedDate": "2020-10-20T17:18:15Z", "message": "Merge branch 'master' into date_histo_as_range"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "117eb77edc372d3d707dca96dbcdf161e3c33faf", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/117eb77edc372d3d707dca96dbcdf161e3c33faf", "committedDate": "2020-10-20T17:56:19Z", "message": "Moar javadoc"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE0ODI1Nzk5", "url": "https://github.com/elastic/elasticsearch/pull/63643#pullrequestreview-514825799", "createdAt": "2020-10-22T15:00:26Z", "commit": {"oid": "117eb77edc372d3d707dca96dbcdf161e3c33faf"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNTowMDoyNlrOHmmM3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNTowMDoyNlrOHmmM3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDIzMzgyMw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    if ((double) max != max) {\n          \n          \n            \n                    if ((long) ((double) max) != max) {\n          \n      \n    \n    \n  \n\nLet's be explicit about the casting here.  I don't want to have to wonder if some optimization decided to cast the RHS to a double instead of the LHS to a long.  Ditto for other instances of this check.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510233823", "createdAt": "2020-10-22T15:00:26Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java", "diffHunk": "@@ -22,35 +22,186 @@\n import org.apache.lucene.index.SortedNumericDocValues;\n import org.apache.lucene.search.ScoreMode;\n import org.apache.lucene.util.CollectionUtil;\n+import org.elasticsearch.common.CheckedFunction;\n import org.elasticsearch.common.Nullable;\n import org.elasticsearch.common.Rounding;\n+import org.elasticsearch.common.Rounding.DateTimeUnit;\n import org.elasticsearch.common.lease.Releasables;\n import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.aggregations.AdaptingAggregator;\n import org.elasticsearch.search.aggregations.Aggregator;\n import org.elasticsearch.search.aggregations.AggregatorFactories;\n import org.elasticsearch.search.aggregations.BucketOrder;\n import org.elasticsearch.search.aggregations.CardinalityUpperBound;\n import org.elasticsearch.search.aggregations.InternalAggregation;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n import org.elasticsearch.search.aggregations.LeafBucketCollector;\n import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;\n import org.elasticsearch.search.aggregations.bucket.BucketsAggregator;\n+import org.elasticsearch.search.aggregations.bucket.filter.FiltersAggregator;\n+import org.elasticsearch.search.aggregations.bucket.range.InternalDateRange;\n+import org.elasticsearch.search.aggregations.bucket.range.RangeAggregationBuilder;\n+import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator;\n+import org.elasticsearch.search.aggregations.bucket.range.RangeAggregatorSupplier;\n import org.elasticsearch.search.aggregations.bucket.terms.LongKeyedBucketOrds;\n import org.elasticsearch.search.aggregations.support.ValuesSource;\n import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;\n import org.elasticsearch.search.internal.SearchContext;\n \n import java.io.IOException;\n+import java.util.ArrayList;\n import java.util.Collections;\n+import java.util.List;\n import java.util.Map;\n import java.util.function.BiConsumer;\n \n /**\n- * An aggregator for date values. Every date is rounded down using a configured\n- * {@link Rounding}.\n- *\n- * @see Rounding\n+ * Aggregator for {@code date_histogram} that rounds values using\n+ * {@link Rounding}. See {@link FromDateRange} which also aggregates for\n+ * {@code date_histogram} but does so by running a {@code range} aggregation\n+ * over the date and transforming the results. In general\n+ * {@link FromDateRange} is faster than {@link DateHistogramAggregator}\n+ * but {@linkplain DateHistogramAggregator} works when we can't precalculate\n+ * all of the {@link Rounding.Prepared#fixedRoundingPoints() fixed rounding points}.\n  */\n class DateHistogramAggregator extends BucketsAggregator implements SizedBucketAggregator {\n+    /**\n+     * Build an {@link Aggregator} for a {@code date_histogram} aggregation.\n+     * If we can determine the bucket boundaries from\n+     * {@link Rounding.Prepared#fixedRoundingPoints()} we use\n+     * {@link RangeAggregator} to do the actual collecting, otherwise we use\n+     * an specialized {@link DateHistogramAggregator Aggregator} specifically\n+     * for the {@code date_histogram}s. We prefer to delegate to the\n+     * {@linkplain RangeAggregator} because it can sometimes be further\n+     * optimized into a {@link FiltersAggregator}. Even when it can't be\n+     * optimized, it is going to be marginally faster and consume less memory\n+     * than the {@linkplain DateHistogramAggregator} because it doesn't need\n+     * to the rounding points and because it can pass precise cardinality\n+     * estimates to its child aggregations.\n+     */\n+    public static Aggregator build(\n+        String name,\n+        AggregatorFactories factories,\n+        Rounding rounding,\n+        BucketOrder order,\n+        boolean keyed,\n+        long minDocCount,\n+        @Nullable LongBounds extendedBounds,\n+        @Nullable LongBounds hardBounds,\n+        ValuesSourceConfig valuesSourceConfig,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        Rounding.Prepared preparedRounding = valuesSourceConfig.roundingPreparer().apply(rounding);\n+        Aggregator asRange = adaptIntoRangeOrNull(\n+            name,\n+            factories,\n+            rounding,\n+            preparedRounding,\n+            order,\n+            keyed,\n+            minDocCount,\n+            extendedBounds,\n+            hardBounds,\n+            valuesSourceConfig,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+        if (asRange != null) {\n+            return asRange;\n+        }\n+        return new DateHistogramAggregator(\n+            name,\n+            factories,\n+            rounding,\n+            preparedRounding,\n+            order,\n+            keyed,\n+            minDocCount,\n+            extendedBounds,\n+            hardBounds,\n+            valuesSourceConfig,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+    }\n+\n+    private static FromDateRange adaptIntoRangeOrNull(\n+        String name,\n+        AggregatorFactories factories,\n+        Rounding rounding,\n+        Rounding.Prepared preparedRounding,\n+        BucketOrder order,\n+        boolean keyed,\n+        long minDocCount,\n+        @Nullable LongBounds extendedBounds,\n+        @Nullable LongBounds hardBounds,\n+        ValuesSourceConfig valuesSourceConfig,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        if (hardBounds != null) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.hasValues() == false) {\n+            return null;\n+        }\n+        long[] fixedRoundingPoints = preparedRounding.fixedRoundingPoints();\n+        if (fixedRoundingPoints == null) {\n+            return null;\n+        }\n+        // Range aggs use a double to aggregate and we don't want to lose precision.\n+        long max = fixedRoundingPoints[fixedRoundingPoints.length - 1];\n+        if ((double) max != max) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "117eb77edc372d3d707dca96dbcdf161e3c33faf"}, "originalPosition": 146}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "08fee6c3882cb82080ff21baf69bdd314e228df4", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/08fee6c3882cb82080ff21baf69bdd314e228df4", "committedDate": "2020-10-22T15:16:21Z", "message": "Merge branch 'master' into date_histo_as_range"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE0ODY1Mzg1", "url": "https://github.com/elastic/elasticsearch/pull/63643#pullrequestreview-514865385", "createdAt": "2020-10-22T15:39:43Z", "commit": {"oid": "117eb77edc372d3d707dca96dbcdf161e3c33faf"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNTozOTo0M1rOHmoAtQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNTozOTo0M1rOHmoAtQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDI2MzQ3Nw==", "bodyText": "My instinct is that this should be delegated to the ValuesSourceType in some way, but I'm not sure how right now.  Something to think about.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510263477", "createdAt": "2020-10-22T15:39:43Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java", "diffHunk": "@@ -215,15 +235,207 @@ public boolean equals(Object obj) {\n         }\n     }\n \n-    final ValuesSource.Numeric valuesSource;\n-    final DocValueFormat format;\n-    final Range[] ranges;\n-    final boolean keyed;\n-    final InternalRange.Factory rangeFactory;\n+    /**\n+     * Build an {@link Aggregator} for a {@code range} aggregation. If the\n+     * {@code ranges} can be converted into filters then it builds a\n+     * {@link FiltersAggregator} and uses that to collect the results\n+     * <strong>if</strong> that aggregator can run in \"filter by filter\"\n+     * collection mode. If it can't then we'll collect the ranges using\n+     * a native {@link RangeAggregator} which is significantly faster\n+     * than the \"compatible\" collection mechanism for the filters agg.\n+     */\n+    public static Aggregator build(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSourceConfig valuesSourceConfig,\n+        InternalRange.Factory<?, ?> rangeFactory,\n+        Range[] ranges,\n+        boolean keyed,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        Aggregator adapted = adaptIntoFiltersOrNull(\n+            name,\n+            factories,\n+            valuesSourceConfig,\n+            rangeFactory,\n+            ranges,\n+            keyed,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+        if (adapted != null) {\n+            return adapted;\n+        }\n+        return buildWithoutAttemptedToAdaptToFilters(\n+            name,\n+            factories,\n+            (ValuesSource.Numeric) valuesSourceConfig.getValuesSource(),\n+            valuesSourceConfig.format(),\n+            rangeFactory,\n+            ranges,\n+            keyed,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+    }\n+\n+    public static Aggregator adaptIntoFiltersOrNull(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSourceConfig valuesSourceConfig,\n+        InternalRange.Factory<?, ?> rangeFactory,\n+        Range[] ranges,\n+        boolean keyed,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        if (valuesSourceConfig.fieldType() == null) {\n+            return null;\n+        }\n+        if (false == valuesSourceConfig.fieldType().isSearchable()) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.missing() != null) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.script() != null) {\n+            return null;\n+        }\n+        // TODO bail here for runtime fields. They'll be slower this way. Maybe we can somehow look at the Query?\n+        if (valuesSourceConfig.fieldType() instanceof DateFieldType\n+            && ((DateFieldType) valuesSourceConfig.fieldType()).resolution() == Resolution.NANOSECONDS) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "117eb77edc372d3d707dca96dbcdf161e3c33faf"}, "originalPosition": 140}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE0ODcxMDMw", "url": "https://github.com/elastic/elasticsearch/pull/63643#pullrequestreview-514871030", "createdAt": "2020-10-22T15:45:32Z", "commit": {"oid": "117eb77edc372d3d707dca96dbcdf161e3c33faf"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNTo0NTozM1rOHmoRkA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNTo0NTozM1rOHmoRkA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDI2Nzc5Mg==", "bodyText": "Because we've been talking about it all week, I know what 1 << 53 is doing here, but someone coming fresh to this code without the IEEE floating point spec on their mind probably won't know why that number is magic.  I'd suggest making a constant on this class LARGEST_PRECISE_DOUBLE or something like that", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510267792", "createdAt": "2020-10-22T15:45:33Z", "author": {"login": "not-napoleon"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java", "diffHunk": "@@ -215,15 +235,207 @@ public boolean equals(Object obj) {\n         }\n     }\n \n-    final ValuesSource.Numeric valuesSource;\n-    final DocValueFormat format;\n-    final Range[] ranges;\n-    final boolean keyed;\n-    final InternalRange.Factory rangeFactory;\n+    /**\n+     * Build an {@link Aggregator} for a {@code range} aggregation. If the\n+     * {@code ranges} can be converted into filters then it builds a\n+     * {@link FiltersAggregator} and uses that to collect the results\n+     * <strong>if</strong> that aggregator can run in \"filter by filter\"\n+     * collection mode. If it can't then we'll collect the ranges using\n+     * a native {@link RangeAggregator} which is significantly faster\n+     * than the \"compatible\" collection mechanism for the filters agg.\n+     */\n+    public static Aggregator build(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSourceConfig valuesSourceConfig,\n+        InternalRange.Factory<?, ?> rangeFactory,\n+        Range[] ranges,\n+        boolean keyed,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        Aggregator adapted = adaptIntoFiltersOrNull(\n+            name,\n+            factories,\n+            valuesSourceConfig,\n+            rangeFactory,\n+            ranges,\n+            keyed,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+        if (adapted != null) {\n+            return adapted;\n+        }\n+        return buildWithoutAttemptedToAdaptToFilters(\n+            name,\n+            factories,\n+            (ValuesSource.Numeric) valuesSourceConfig.getValuesSource(),\n+            valuesSourceConfig.format(),\n+            rangeFactory,\n+            ranges,\n+            keyed,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+    }\n+\n+    public static Aggregator adaptIntoFiltersOrNull(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSourceConfig valuesSourceConfig,\n+        InternalRange.Factory<?, ?> rangeFactory,\n+        Range[] ranges,\n+        boolean keyed,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        if (valuesSourceConfig.fieldType() == null) {\n+            return null;\n+        }\n+        if (false == valuesSourceConfig.fieldType().isSearchable()) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.missing() != null) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.script() != null) {\n+            return null;\n+        }\n+        // TODO bail here for runtime fields. They'll be slower this way. Maybe we can somehow look at the Query?\n+        if (valuesSourceConfig.fieldType() instanceof DateFieldType\n+            && ((DateFieldType) valuesSourceConfig.fieldType()).resolution() == Resolution.NANOSECONDS) {\n+            // We don't generate sensible Queries for nanoseconds.\n+            return null;\n+        }\n+        boolean wholeNumbersOnly = false == ((ValuesSource.Numeric) valuesSourceConfig.getValuesSource()).isFloatingPoint();\n+        String[] keys = new String[ranges.length];\n+        Query[] filters = new Query[ranges.length];\n+        for (int i = 0; i < ranges.length; i++) {\n+            /*\n+             * If the bounds on the ranges are too high then the `double`s\n+             * that we work with will round differently in the native range\n+             * aggregator than in the filters aggregator. So we can't use\n+             * the filters. That is, if the input data type is a `long` in\n+             * the first place. If it isn't then \n+             */\n+            if (wholeNumbersOnly && ranges[i].from != Double.NEGATIVE_INFINITY && Math.abs(ranges[i].from) > 1L << 53) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "117eb77edc372d3d707dca96dbcdf161e3c33faf"}, "originalPosition": 155}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ce640e3543934de558ad5f3e21b9a26fcfa53f4a", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/ce640e3543934de558ad5f3e21b9a26fcfa53f4a", "committedDate": "2020-10-22T16:38:35Z", "message": "Test cases we can't do it"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "523d420a065bac8298969997a8638a8846bb0870", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/523d420a065bac8298969997a8638a8846bb0870", "committedDate": "2020-10-22T17:12:42Z", "message": "Fix broken test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f1ae9801b38dacebf8913ce06340cfb22f32e31b", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/f1ae9801b38dacebf8913ce06340cfb22f32e31b", "committedDate": "2020-10-22T17:19:09Z", "message": "Zap"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE0ODUyODAw", "url": "https://github.com/elastic/elasticsearch/pull/63643#pullrequestreview-514852800", "createdAt": "2020-10-22T15:27:10Z", "commit": {"oid": "117eb77edc372d3d707dca96dbcdf161e3c33faf"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNToyNzoxMFrOHmnb5Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNzoyMjowM1rOHmsMPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDI1NDA1Mw==", "bodyText": "Looks like I forgot to use this in RangeAggregator!", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510254053", "createdAt": "2020-10-22T15:27:10Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceConfig.java", "diffHunk": "@@ -385,11 +385,17 @@ public boolean hasGlobalOrdinals() {\n      */\n     @Nullable\n     public Function<byte[], Number> getPointReaderOrNull() {\n-        MappedFieldType fieldType = fieldType();\n-        if (fieldType != null && script() == null && missing() == null) {\n-            return fieldType.pointReaderIfPossible();\n-        }\n-        return null;\n+        return alignesWithSearchIndex() ? fieldType().pointReaderIfPossible() : null;\n+    }\n+\n+    /**\n+     * Do {@link ValuesSource}s built by this config line up with the search\n+     * index of the underlying field? This'll only return true if the fields\n+     * is searchable and there aren't missing values or a script to confuse\n+     * the ordering.\n+     */\n+    public boolean alignesWithSearchIndex() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "117eb77edc372d3d707dca96dbcdf161e3c33faf"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDMwMzQxOA==", "bodyText": "I'm going to replace this with a hard comparison which is easier to reason about. max > 1L << 53.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510303418", "createdAt": "2020-10-22T16:35:57Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java", "diffHunk": "@@ -22,35 +22,186 @@\n import org.apache.lucene.index.SortedNumericDocValues;\n import org.apache.lucene.search.ScoreMode;\n import org.apache.lucene.util.CollectionUtil;\n+import org.elasticsearch.common.CheckedFunction;\n import org.elasticsearch.common.Nullable;\n import org.elasticsearch.common.Rounding;\n+import org.elasticsearch.common.Rounding.DateTimeUnit;\n import org.elasticsearch.common.lease.Releasables;\n import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.aggregations.AdaptingAggregator;\n import org.elasticsearch.search.aggregations.Aggregator;\n import org.elasticsearch.search.aggregations.AggregatorFactories;\n import org.elasticsearch.search.aggregations.BucketOrder;\n import org.elasticsearch.search.aggregations.CardinalityUpperBound;\n import org.elasticsearch.search.aggregations.InternalAggregation;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n import org.elasticsearch.search.aggregations.LeafBucketCollector;\n import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;\n import org.elasticsearch.search.aggregations.bucket.BucketsAggregator;\n+import org.elasticsearch.search.aggregations.bucket.filter.FiltersAggregator;\n+import org.elasticsearch.search.aggregations.bucket.range.InternalDateRange;\n+import org.elasticsearch.search.aggregations.bucket.range.RangeAggregationBuilder;\n+import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator;\n+import org.elasticsearch.search.aggregations.bucket.range.RangeAggregatorSupplier;\n import org.elasticsearch.search.aggregations.bucket.terms.LongKeyedBucketOrds;\n import org.elasticsearch.search.aggregations.support.ValuesSource;\n import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;\n import org.elasticsearch.search.internal.SearchContext;\n \n import java.io.IOException;\n+import java.util.ArrayList;\n import java.util.Collections;\n+import java.util.List;\n import java.util.Map;\n import java.util.function.BiConsumer;\n \n /**\n- * An aggregator for date values. Every date is rounded down using a configured\n- * {@link Rounding}.\n- *\n- * @see Rounding\n+ * Aggregator for {@code date_histogram} that rounds values using\n+ * {@link Rounding}. See {@link FromDateRange} which also aggregates for\n+ * {@code date_histogram} but does so by running a {@code range} aggregation\n+ * over the date and transforming the results. In general\n+ * {@link FromDateRange} is faster than {@link DateHistogramAggregator}\n+ * but {@linkplain DateHistogramAggregator} works when we can't precalculate\n+ * all of the {@link Rounding.Prepared#fixedRoundingPoints() fixed rounding points}.\n  */\n class DateHistogramAggregator extends BucketsAggregator implements SizedBucketAggregator {\n+    /**\n+     * Build an {@link Aggregator} for a {@code date_histogram} aggregation.\n+     * If we can determine the bucket boundaries from\n+     * {@link Rounding.Prepared#fixedRoundingPoints()} we use\n+     * {@link RangeAggregator} to do the actual collecting, otherwise we use\n+     * an specialized {@link DateHistogramAggregator Aggregator} specifically\n+     * for the {@code date_histogram}s. We prefer to delegate to the\n+     * {@linkplain RangeAggregator} because it can sometimes be further\n+     * optimized into a {@link FiltersAggregator}. Even when it can't be\n+     * optimized, it is going to be marginally faster and consume less memory\n+     * than the {@linkplain DateHistogramAggregator} because it doesn't need\n+     * to the rounding points and because it can pass precise cardinality\n+     * estimates to its child aggregations.\n+     */\n+    public static Aggregator build(\n+        String name,\n+        AggregatorFactories factories,\n+        Rounding rounding,\n+        BucketOrder order,\n+        boolean keyed,\n+        long minDocCount,\n+        @Nullable LongBounds extendedBounds,\n+        @Nullable LongBounds hardBounds,\n+        ValuesSourceConfig valuesSourceConfig,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        Rounding.Prepared preparedRounding = valuesSourceConfig.roundingPreparer().apply(rounding);\n+        Aggregator asRange = adaptIntoRangeOrNull(\n+            name,\n+            factories,\n+            rounding,\n+            preparedRounding,\n+            order,\n+            keyed,\n+            minDocCount,\n+            extendedBounds,\n+            hardBounds,\n+            valuesSourceConfig,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+        if (asRange != null) {\n+            return asRange;\n+        }\n+        return new DateHistogramAggregator(\n+            name,\n+            factories,\n+            rounding,\n+            preparedRounding,\n+            order,\n+            keyed,\n+            minDocCount,\n+            extendedBounds,\n+            hardBounds,\n+            valuesSourceConfig,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+    }\n+\n+    private static FromDateRange adaptIntoRangeOrNull(\n+        String name,\n+        AggregatorFactories factories,\n+        Rounding rounding,\n+        Rounding.Prepared preparedRounding,\n+        BucketOrder order,\n+        boolean keyed,\n+        long minDocCount,\n+        @Nullable LongBounds extendedBounds,\n+        @Nullable LongBounds hardBounds,\n+        ValuesSourceConfig valuesSourceConfig,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        if (hardBounds != null) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.hasValues() == false) {\n+            return null;\n+        }\n+        long[] fixedRoundingPoints = preparedRounding.fixedRoundingPoints();\n+        if (fixedRoundingPoints == null) {\n+            return null;\n+        }\n+        // Range aggs use a double to aggregate and we don't want to lose precision.\n+        long max = fixedRoundingPoints[fixedRoundingPoints.length - 1];\n+        if ((double) max != max) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDIzMzgyMw=="}, "originalCommit": {"oid": "117eb77edc372d3d707dca96dbcdf161e3c33faf"}, "originalPosition": 146}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDMyNjg0MA==", "bodyText": "I wonder if it'd make sense to shoe horn it via adapting the data source on the way in and the result on the way out. Maybe. I'm not sure. A thing for later, I think.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510326840", "createdAt": "2020-10-22T17:13:37Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/GeoDistanceRangeAggregatorFactory.java", "diffHunk": "@@ -66,7 +66,7 @@ public static void registerAggregators(ValuesSourceRegistry.Builder builder) {\n                 cardinality,\n                 metadata) -> {\n                 DistanceSource distanceSource = new DistanceSource((ValuesSource.GeoPoint) valuesSource, distanceType, origin, units);\n-                return new RangeAggregator(\n+                return RangeAggregator.buildWithoutAttemptedToAdaptToFilters(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc0NzA0Mg=="}, "originalCommit": {"oid": "402132781b8530f616ad6474c0b7652a588404c7"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDMyODg1Mg==", "bodyText": "I copied this from above but I don't think it is accurate either place. I've zapped it.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510328852", "createdAt": "2020-10-22T17:16:54Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java", "diffHunk": "@@ -195,4 +344,95 @@ public double bucketSize(long bucket, Rounding.DateTimeUnit unitSize) {\n             return 1.0;\n         }\n     }\n+\n+    private static class FromDateRange extends AdaptingAggregator implements SizedBucketAggregator {\n+        private final DocValueFormat format;\n+        private final Rounding rounding;\n+        private final Rounding.Prepared preparedRounding;\n+        private final BucketOrder order;\n+        private final long minDocCount;\n+        private final LongBounds extendedBounds;\n+        private final boolean keyed;\n+        private final long[] fixedRoundingPoints;\n+\n+        FromDateRange(\n+            Aggregator parent,\n+            AggregatorFactories subAggregators,\n+            CheckedFunction<AggregatorFactories, Aggregator, IOException> delegate,\n+            DocValueFormat format,\n+            Rounding rounding,\n+            Rounding.Prepared preparedRounding,\n+            BucketOrder order,\n+            long minDocCount,\n+            LongBounds extendedBounds,\n+            boolean keyed,\n+            long[] fixedRoundingPoints\n+        ) throws IOException {\n+            super(parent, subAggregators, delegate);\n+            this.format = format;\n+            this.rounding = rounding;\n+            this.preparedRounding = preparedRounding;\n+            this.order = order;\n+            order.validate(this);\n+            this.minDocCount = minDocCount;\n+            this.extendedBounds = extendedBounds;\n+            this.keyed = keyed;\n+            this.fixedRoundingPoints = fixedRoundingPoints;\n+        }\n+\n+        @Override\n+        protected InternalAggregation adapt(InternalAggregation delegateResult) {\n+            InternalDateRange range = (InternalDateRange) delegateResult;\n+            List<InternalDateHistogram.Bucket> buckets = new ArrayList<>(range.getBuckets().size());\n+            for (InternalDateRange.Bucket rangeBucket : range.getBuckets()) {\n+                if (rangeBucket.getDocCount() > 0) {\n+                    buckets.add(\n+                        new InternalDateHistogram.Bucket(\n+                            rangeBucket.getFrom().toInstant().toEpochMilli(),\n+                            rangeBucket.getDocCount(),\n+                            keyed,\n+                            format,\n+                            rangeBucket.getAggregations()\n+                        )\n+                    );\n+                }\n+            }\n+            CollectionUtil.introSort(buckets, BucketOrder.key(true).comparator());\n+\n+            // value source will be null for unmapped fields", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcxODIxMA=="}, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 257}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDMyOTk5Mw==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510329993", "createdAt": "2020-10-22T17:18:50Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/common/Rounding.java", "diffHunk": "@@ -291,6 +291,12 @@ public void writeTo(StreamOutput out) throws IOException {\n          * next rounded value in specified units if possible.\n          */\n         double roundingSize(long utcMillis, DateTimeUnit timeUnit);\n+        /**\n+         * An array of dates such that each date between each entry is will\n+         * be rounded down to that entry or {@code null} if this rounding\n+         * mechanism doesn't or can't precalculate these points.\n+         */", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY4MjM4MA=="}, "originalCommit": {"oid": "402132781b8530f616ad6474c0b7652a588404c7"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDMzMTUwNA==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510331504", "createdAt": "2020-10-22T17:21:18Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactories.java", "diffHunk": "@@ -227,6 +227,18 @@ public int countAggregators() {\n         return factories.length;\n     }\n \n+    public AggregatorFactories fixParent(Aggregator fixedParent) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTY1MzEwOQ=="}, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDMzMTk2NA==", "bodyText": "\"to round points\" is what I meant.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510331964", "createdAt": "2020-10-22T17:22:03Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java", "diffHunk": "@@ -51,6 +62,143 @@\n  * @see Rounding\n  */\n class DateHistogramAggregator extends BucketsAggregator implements SizedBucketAggregator {\n+    /**\n+     * Build an {@link Aggregator} for a {@code date_histogram} aggregation.\n+     * If we can determine the bucket boundaries from\n+     * {@link Rounding.Prepared#fixedRoundingPoints()} we use\n+     * {@link RangeAggregator} to do the actual collecting, otherwise we use\n+     * an specialized {@link DateHistogramAggregator Aggregator} specifically\n+     * for the {@code date_histogram}s. We prefer to delegate to the\n+     * {@linkplain RangeAggregator} because it can sometimes be further\n+     * optimized into a {@link FiltersAggregator}. Even when it can't be\n+     * optimized, it is going to be marginally faster and consume less memory\n+     * than the {@linkplain DateHistogramAggregator} because it doesn't need\n+     * to the rounding points and because it can pass precise cardinality", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcwNjAzNQ=="}, "originalCommit": {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e"}, "originalPosition": 52}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9da87c50f3b30f192b3830c4803c3f77002032bd", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/9da87c50f3b30f192b3830c4803c3f77002032bd", "committedDate": "2020-10-22T17:40:30Z", "message": "Words"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3d3ea198857f7dd269e1b9e08f9356b1a69243e3", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/3d3ea198857f7dd269e1b9e08f9356b1a69243e3", "committedDate": "2020-10-22T17:42:40Z", "message": "Coment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4291b701930cc85774df23be425225101e7b90c0", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/4291b701930cc85774df23be425225101e7b90c0", "committedDate": "2020-10-22T20:27:26Z", "message": "Add fancy query"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ff41cdbf5a7d6f40c198cad1831a616d07e3bf29", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/ff41cdbf5a7d6f40c198cad1831a616d07e3bf29", "committedDate": "2020-10-26T13:34:51Z", "message": "NOCOMMIT"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a5f00099ad9368fda09bc8284dba1f68544488ad", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/a5f00099ad9368fda09bc8284dba1f68544488ad", "committedDate": "2020-10-28T13:43:58Z", "message": "Merge branch 'master' into date_histo_as_range"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ec5bf41875bd04c13bab742828d0d4062f193834", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/ec5bf41875bd04c13bab742828d0d4062f193834", "committedDate": "2020-10-28T19:15:44Z", "message": "tests\n\nnot enough yet"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6b5c09ad14bd9cce098418dea6b8be0b46b33086", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/6b5c09ad14bd9cce098418dea6b8be0b46b33086", "committedDate": "2020-10-28T21:33:47Z", "message": "missing tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9c26fd879a34d00274bd729f0199737d1b4cd2bd", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/9c26fd879a34d00274bd729f0199737d1b4cd2bd", "committedDate": "2020-10-28T21:39:51Z", "message": "remove!"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c5b04647e1539a481d1db7584bddfd5c74f399bb", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/c5b04647e1539a481d1db7584bddfd5c74f399bb", "committedDate": "2020-10-29T14:01:19Z", "message": "Merge branch 'master' into date_histo_as_range"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "66684f2983799e2430770b8a8daa95349f20ea7e", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/66684f2983799e2430770b8a8daa95349f20ea7e", "committedDate": "2020-10-29T14:27:41Z", "message": "I think this is more normal"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIxNjIwNzQx", "url": "https://github.com/elastic/elasticsearch/pull/63643#pullrequestreview-521620741", "createdAt": "2020-11-02T13:20:54Z", "commit": {"oid": "66684f2983799e2430770b8a8daa95349f20ea7e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxMzoyMDo1NFrOHsEFKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxMzoyMDo1NFrOHsEFKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTk2NjI1MQ==", "bodyText": "I think we don't need to create this weight here but do it lazy in the same way we are doing for singleValue.", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r515966251", "createdAt": "2020-11-02T13:20:54Z", "author": {"login": "iverase"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java", "diffHunk": "@@ -0,0 +1,201 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.search.aggregations.bucket.filter;\n+\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.PointValues;\n+import org.apache.lucene.index.Term;\n+import org.apache.lucene.search.BooleanClause.Occur;\n+import org.apache.lucene.search.BooleanQuery;\n+import org.apache.lucene.search.BulkScorer;\n+import org.apache.lucene.search.Explanation;\n+import org.apache.lucene.search.IndexSearcher;\n+import org.apache.lucene.search.MatchNoDocsQuery;\n+import org.apache.lucene.search.PointRangeQuery;\n+import org.apache.lucene.search.Query;\n+import org.apache.lucene.search.ScoreMode;\n+import org.apache.lucene.search.Scorer;\n+import org.apache.lucene.search.ScorerSupplier;\n+import org.apache.lucene.search.Weight;\n+\n+import java.io.IOException;\n+import java.util.Set;\n+\n+import static java.util.Arrays.compareUnsigned;\n+\n+/**\n+ * Query merging two point in range queries.\n+ */\n+public class MergedPointRangeQuery extends Query {\n+    /**\n+     * Merge two {@linkplain PointRangeQuery}s into a {@linkplain MergedPointRangeQuery}\n+     * that matches points that match both filters.\n+     */\n+    public static Query merge(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        if (lhs.equals(rhs)) {\n+            // Lucky case! The queries were the same so their UNION is just the query itself.\n+            return lhs;\n+        }\n+        if (lhs.getField() != rhs.getField() || lhs.getNumDims() != rhs.getNumDims() || lhs.getBytesPerDim() != rhs.getBytesPerDim()) {\n+            return null;\n+        }\n+        return new MergedPointRangeQuery(lhs, rhs);\n+    }\n+\n+    private final String field;\n+    private final Query delegateForMultiValuedSegments;\n+    private final Query delegateForSingleValuedSegments;\n+\n+    private MergedPointRangeQuery(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        field = lhs.getField();\n+        delegateForMultiValuedSegments = new BooleanQuery.Builder().add(lhs, Occur.MUST).add(rhs, Occur.MUST).build();\n+        int numDims = lhs.getNumDims();\n+        int bytesPerDim = lhs.getBytesPerDim();\n+        this.delegateForSingleValuedSegments = pickDelegateForSingleValuedSegments(\n+            mergeBound(lhs.getLowerPoint(), rhs.getLowerPoint(), numDims, bytesPerDim, true),\n+            mergeBound(lhs.getUpperPoint(), rhs.getUpperPoint(), numDims, bytesPerDim, false),\n+            numDims,\n+            bytesPerDim\n+        );\n+    }\n+\n+    private Query pickDelegateForSingleValuedSegments(byte[] lower, byte[] upper, int numDims, int bytesPerDim) {\n+        // If we ended up with disjoint ranges in any dimension then on single valued segments we can't match any docs.\n+        for (int dim = 0; dim < numDims; dim++) {\n+            int offset = dim * bytesPerDim;\n+            if (compareUnsigned(lower, offset, offset + bytesPerDim, upper, offset, offset + bytesPerDim) > 0) {\n+                return new MatchNoDocsQuery(\"disjoint ranges\");\n+            }\n+        }\n+        // Otherwise on single valued segments we can only match docs the match the UNION of the two ranges.\n+        return new PointRangeQuery(field, lower, upper, numDims) {\n+            @Override\n+            protected String toString(int dimension, byte[] value) {\n+                // Stolen from Lucene's Binary range query. It'd be best to delegate, but the method isn't visible.\n+                StringBuilder sb = new StringBuilder();\n+                sb.append(\"(\");\n+                for (int i = 0; i < value.length; i++) {\n+                    if (i > 0) {\n+                        sb.append(' ');\n+                    }\n+                    sb.append(Integer.toHexString(value[i] & 0xFF));\n+                }\n+                sb.append(')');\n+                return sb.toString();\n+            }\n+        };\n+    }\n+\n+    @Override\n+    public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n+        Weight delegateForMultiValuedSegmentsWeight = delegateForMultiValuedSegments.createWeight(searcher, scoreMode, boost);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "66684f2983799e2430770b8a8daa95349f20ea7e"}, "originalPosition": 108}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIxNjIxMDk2", "url": "https://github.com/elastic/elasticsearch/pull/63643#pullrequestreview-521621096", "createdAt": "2020-11-02T13:21:24Z", "commit": {"oid": "66684f2983799e2430770b8a8daa95349f20ea7e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxMzoyMToyNFrOHsEGNg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxMzoyMToyNFrOHsEGNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTk2NjUxOA==", "bodyText": "point queries always return true so we can probably just return true here?", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r515966518", "createdAt": "2020-11-02T13:21:24Z", "author": {"login": "iverase"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java", "diffHunk": "@@ -0,0 +1,201 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.search.aggregations.bucket.filter;\n+\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.PointValues;\n+import org.apache.lucene.index.Term;\n+import org.apache.lucene.search.BooleanClause.Occur;\n+import org.apache.lucene.search.BooleanQuery;\n+import org.apache.lucene.search.BulkScorer;\n+import org.apache.lucene.search.Explanation;\n+import org.apache.lucene.search.IndexSearcher;\n+import org.apache.lucene.search.MatchNoDocsQuery;\n+import org.apache.lucene.search.PointRangeQuery;\n+import org.apache.lucene.search.Query;\n+import org.apache.lucene.search.ScoreMode;\n+import org.apache.lucene.search.Scorer;\n+import org.apache.lucene.search.ScorerSupplier;\n+import org.apache.lucene.search.Weight;\n+\n+import java.io.IOException;\n+import java.util.Set;\n+\n+import static java.util.Arrays.compareUnsigned;\n+\n+/**\n+ * Query merging two point in range queries.\n+ */\n+public class MergedPointRangeQuery extends Query {\n+    /**\n+     * Merge two {@linkplain PointRangeQuery}s into a {@linkplain MergedPointRangeQuery}\n+     * that matches points that match both filters.\n+     */\n+    public static Query merge(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        if (lhs.equals(rhs)) {\n+            // Lucky case! The queries were the same so their UNION is just the query itself.\n+            return lhs;\n+        }\n+        if (lhs.getField() != rhs.getField() || lhs.getNumDims() != rhs.getNumDims() || lhs.getBytesPerDim() != rhs.getBytesPerDim()) {\n+            return null;\n+        }\n+        return new MergedPointRangeQuery(lhs, rhs);\n+    }\n+\n+    private final String field;\n+    private final Query delegateForMultiValuedSegments;\n+    private final Query delegateForSingleValuedSegments;\n+\n+    private MergedPointRangeQuery(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        field = lhs.getField();\n+        delegateForMultiValuedSegments = new BooleanQuery.Builder().add(lhs, Occur.MUST).add(rhs, Occur.MUST).build();\n+        int numDims = lhs.getNumDims();\n+        int bytesPerDim = lhs.getBytesPerDim();\n+        this.delegateForSingleValuedSegments = pickDelegateForSingleValuedSegments(\n+            mergeBound(lhs.getLowerPoint(), rhs.getLowerPoint(), numDims, bytesPerDim, true),\n+            mergeBound(lhs.getUpperPoint(), rhs.getUpperPoint(), numDims, bytesPerDim, false),\n+            numDims,\n+            bytesPerDim\n+        );\n+    }\n+\n+    private Query pickDelegateForSingleValuedSegments(byte[] lower, byte[] upper, int numDims, int bytesPerDim) {\n+        // If we ended up with disjoint ranges in any dimension then on single valued segments we can't match any docs.\n+        for (int dim = 0; dim < numDims; dim++) {\n+            int offset = dim * bytesPerDim;\n+            if (compareUnsigned(lower, offset, offset + bytesPerDim, upper, offset, offset + bytesPerDim) > 0) {\n+                return new MatchNoDocsQuery(\"disjoint ranges\");\n+            }\n+        }\n+        // Otherwise on single valued segments we can only match docs the match the UNION of the two ranges.\n+        return new PointRangeQuery(field, lower, upper, numDims) {\n+            @Override\n+            protected String toString(int dimension, byte[] value) {\n+                // Stolen from Lucene's Binary range query. It'd be best to delegate, but the method isn't visible.\n+                StringBuilder sb = new StringBuilder();\n+                sb.append(\"(\");\n+                for (int i = 0; i < value.length; i++) {\n+                    if (i > 0) {\n+                        sb.append(' ');\n+                    }\n+                    sb.append(Integer.toHexString(value[i] & 0xFF));\n+                }\n+                sb.append(')');\n+                return sb.toString();\n+            }\n+        };\n+    }\n+\n+    @Override\n+    public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n+        Weight delegateForMultiValuedSegmentsWeight = delegateForMultiValuedSegments.createWeight(searcher, scoreMode, boost);\n+        return new Weight(this) {\n+            Weight mostCompactWeight;\n+\n+            @Override\n+            public boolean isCacheable(LeafReaderContext ctx) {\n+                return delegateForMultiValuedSegmentsWeight.isCacheable(ctx);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "66684f2983799e2430770b8a8daa95349f20ea7e"}, "originalPosition": 114}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIxNjIxNDEz", "url": "https://github.com/elastic/elasticsearch/pull/63643#pullrequestreview-521621413", "createdAt": "2020-11-02T13:21:50Z", "commit": {"oid": "66684f2983799e2430770b8a8daa95349f20ea7e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxMzoyMTo1MFrOHsEHHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxMzoyMTo1MFrOHsEHHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTk2Njc0OQ==", "bodyText": "I think this can be safely a NOOP", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r515966749", "createdAt": "2020-11-02T13:21:50Z", "author": {"login": "iverase"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java", "diffHunk": "@@ -0,0 +1,201 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.search.aggregations.bucket.filter;\n+\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.PointValues;\n+import org.apache.lucene.index.Term;\n+import org.apache.lucene.search.BooleanClause.Occur;\n+import org.apache.lucene.search.BooleanQuery;\n+import org.apache.lucene.search.BulkScorer;\n+import org.apache.lucene.search.Explanation;\n+import org.apache.lucene.search.IndexSearcher;\n+import org.apache.lucene.search.MatchNoDocsQuery;\n+import org.apache.lucene.search.PointRangeQuery;\n+import org.apache.lucene.search.Query;\n+import org.apache.lucene.search.ScoreMode;\n+import org.apache.lucene.search.Scorer;\n+import org.apache.lucene.search.ScorerSupplier;\n+import org.apache.lucene.search.Weight;\n+\n+import java.io.IOException;\n+import java.util.Set;\n+\n+import static java.util.Arrays.compareUnsigned;\n+\n+/**\n+ * Query merging two point in range queries.\n+ */\n+public class MergedPointRangeQuery extends Query {\n+    /**\n+     * Merge two {@linkplain PointRangeQuery}s into a {@linkplain MergedPointRangeQuery}\n+     * that matches points that match both filters.\n+     */\n+    public static Query merge(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        if (lhs.equals(rhs)) {\n+            // Lucky case! The queries were the same so their UNION is just the query itself.\n+            return lhs;\n+        }\n+        if (lhs.getField() != rhs.getField() || lhs.getNumDims() != rhs.getNumDims() || lhs.getBytesPerDim() != rhs.getBytesPerDim()) {\n+            return null;\n+        }\n+        return new MergedPointRangeQuery(lhs, rhs);\n+    }\n+\n+    private final String field;\n+    private final Query delegateForMultiValuedSegments;\n+    private final Query delegateForSingleValuedSegments;\n+\n+    private MergedPointRangeQuery(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        field = lhs.getField();\n+        delegateForMultiValuedSegments = new BooleanQuery.Builder().add(lhs, Occur.MUST).add(rhs, Occur.MUST).build();\n+        int numDims = lhs.getNumDims();\n+        int bytesPerDim = lhs.getBytesPerDim();\n+        this.delegateForSingleValuedSegments = pickDelegateForSingleValuedSegments(\n+            mergeBound(lhs.getLowerPoint(), rhs.getLowerPoint(), numDims, bytesPerDim, true),\n+            mergeBound(lhs.getUpperPoint(), rhs.getUpperPoint(), numDims, bytesPerDim, false),\n+            numDims,\n+            bytesPerDim\n+        );\n+    }\n+\n+    private Query pickDelegateForSingleValuedSegments(byte[] lower, byte[] upper, int numDims, int bytesPerDim) {\n+        // If we ended up with disjoint ranges in any dimension then on single valued segments we can't match any docs.\n+        for (int dim = 0; dim < numDims; dim++) {\n+            int offset = dim * bytesPerDim;\n+            if (compareUnsigned(lower, offset, offset + bytesPerDim, upper, offset, offset + bytesPerDim) > 0) {\n+                return new MatchNoDocsQuery(\"disjoint ranges\");\n+            }\n+        }\n+        // Otherwise on single valued segments we can only match docs the match the UNION of the two ranges.\n+        return new PointRangeQuery(field, lower, upper, numDims) {\n+            @Override\n+            protected String toString(int dimension, byte[] value) {\n+                // Stolen from Lucene's Binary range query. It'd be best to delegate, but the method isn't visible.\n+                StringBuilder sb = new StringBuilder();\n+                sb.append(\"(\");\n+                for (int i = 0; i < value.length; i++) {\n+                    if (i > 0) {\n+                        sb.append(' ');\n+                    }\n+                    sb.append(Integer.toHexString(value[i] & 0xFF));\n+                }\n+                sb.append(')');\n+                return sb.toString();\n+            }\n+        };\n+    }\n+\n+    @Override\n+    public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n+        Weight delegateForMultiValuedSegmentsWeight = delegateForMultiValuedSegments.createWeight(searcher, scoreMode, boost);\n+        return new Weight(this) {\n+            Weight mostCompactWeight;\n+\n+            @Override\n+            public boolean isCacheable(LeafReaderContext ctx) {\n+                return delegateForMultiValuedSegmentsWeight.isCacheable(ctx);\n+            }\n+\n+            @Override\n+            public Scorer scorer(LeafReaderContext context) throws IOException {\n+                ScorerSupplier scorerSupplier = scorerSupplier(context);\n+                if (scorerSupplier == null) {\n+                    return null;\n+                }\n+                return scorerSupplier.get(Long.MAX_VALUE);\n+            }\n+\n+            @Override\n+            public ScorerSupplier scorerSupplier(LeafReaderContext context) throws IOException {\n+                /*\n+                 * If we're sure docs only have a single value for the field\n+                 * we can pick the most compact bounds. If there are multiple values\n+                 * for the field we have to run the boolean query.\n+                 */\n+                PointValues points = context.reader().getPointValues(field);\n+                if (points == null) {\n+                    return null;\n+                }\n+                if (points.size() == points.getDocCount()) {\n+                    // Each doc that has points has exactly one point.\n+                    if (mostCompactWeight == null) {\n+                        mostCompactWeight = delegateForSingleValuedSegments.createWeight(searcher, scoreMode, boost);\n+                    }\n+                    return mostCompactWeight.scorerSupplier(context);\n+                }\n+                return delegateForMultiValuedSegmentsWeight.scorerSupplier(context);\n+            }\n+\n+            @Override\n+            public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n+                return super.bulkScorer(context);\n+            }\n+\n+            @Override\n+            @Deprecated\n+            public void extractTerms(Set<Term> terms) {\n+                delegateForMultiValuedSegmentsWeight.extractTerms(terms);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "66684f2983799e2430770b8a8daa95349f20ea7e"}, "originalPosition": 155}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIxNjIyMDE1", "url": "https://github.com/elastic/elasticsearch/pull/63643#pullrequestreview-521622015", "createdAt": "2020-11-02T13:22:38Z", "commit": {"oid": "66684f2983799e2430770b8a8daa95349f20ea7e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxMzoyMjozOFrOHsEI6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxMzoyMjozOFrOHsEI6w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTk2NzIxMQ==", "bodyText": "We can safely delegate this method to the default implementation?", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r515967211", "createdAt": "2020-11-02T13:22:38Z", "author": {"login": "iverase"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java", "diffHunk": "@@ -0,0 +1,201 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.search.aggregations.bucket.filter;\n+\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.PointValues;\n+import org.apache.lucene.index.Term;\n+import org.apache.lucene.search.BooleanClause.Occur;\n+import org.apache.lucene.search.BooleanQuery;\n+import org.apache.lucene.search.BulkScorer;\n+import org.apache.lucene.search.Explanation;\n+import org.apache.lucene.search.IndexSearcher;\n+import org.apache.lucene.search.MatchNoDocsQuery;\n+import org.apache.lucene.search.PointRangeQuery;\n+import org.apache.lucene.search.Query;\n+import org.apache.lucene.search.ScoreMode;\n+import org.apache.lucene.search.Scorer;\n+import org.apache.lucene.search.ScorerSupplier;\n+import org.apache.lucene.search.Weight;\n+\n+import java.io.IOException;\n+import java.util.Set;\n+\n+import static java.util.Arrays.compareUnsigned;\n+\n+/**\n+ * Query merging two point in range queries.\n+ */\n+public class MergedPointRangeQuery extends Query {\n+    /**\n+     * Merge two {@linkplain PointRangeQuery}s into a {@linkplain MergedPointRangeQuery}\n+     * that matches points that match both filters.\n+     */\n+    public static Query merge(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        if (lhs.equals(rhs)) {\n+            // Lucky case! The queries were the same so their UNION is just the query itself.\n+            return lhs;\n+        }\n+        if (lhs.getField() != rhs.getField() || lhs.getNumDims() != rhs.getNumDims() || lhs.getBytesPerDim() != rhs.getBytesPerDim()) {\n+            return null;\n+        }\n+        return new MergedPointRangeQuery(lhs, rhs);\n+    }\n+\n+    private final String field;\n+    private final Query delegateForMultiValuedSegments;\n+    private final Query delegateForSingleValuedSegments;\n+\n+    private MergedPointRangeQuery(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        field = lhs.getField();\n+        delegateForMultiValuedSegments = new BooleanQuery.Builder().add(lhs, Occur.MUST).add(rhs, Occur.MUST).build();\n+        int numDims = lhs.getNumDims();\n+        int bytesPerDim = lhs.getBytesPerDim();\n+        this.delegateForSingleValuedSegments = pickDelegateForSingleValuedSegments(\n+            mergeBound(lhs.getLowerPoint(), rhs.getLowerPoint(), numDims, bytesPerDim, true),\n+            mergeBound(lhs.getUpperPoint(), rhs.getUpperPoint(), numDims, bytesPerDim, false),\n+            numDims,\n+            bytesPerDim\n+        );\n+    }\n+\n+    private Query pickDelegateForSingleValuedSegments(byte[] lower, byte[] upper, int numDims, int bytesPerDim) {\n+        // If we ended up with disjoint ranges in any dimension then on single valued segments we can't match any docs.\n+        for (int dim = 0; dim < numDims; dim++) {\n+            int offset = dim * bytesPerDim;\n+            if (compareUnsigned(lower, offset, offset + bytesPerDim, upper, offset, offset + bytesPerDim) > 0) {\n+                return new MatchNoDocsQuery(\"disjoint ranges\");\n+            }\n+        }\n+        // Otherwise on single valued segments we can only match docs the match the UNION of the two ranges.\n+        return new PointRangeQuery(field, lower, upper, numDims) {\n+            @Override\n+            protected String toString(int dimension, byte[] value) {\n+                // Stolen from Lucene's Binary range query. It'd be best to delegate, but the method isn't visible.\n+                StringBuilder sb = new StringBuilder();\n+                sb.append(\"(\");\n+                for (int i = 0; i < value.length; i++) {\n+                    if (i > 0) {\n+                        sb.append(' ');\n+                    }\n+                    sb.append(Integer.toHexString(value[i] & 0xFF));\n+                }\n+                sb.append(')');\n+                return sb.toString();\n+            }\n+        };\n+    }\n+\n+    @Override\n+    public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n+        Weight delegateForMultiValuedSegmentsWeight = delegateForMultiValuedSegments.createWeight(searcher, scoreMode, boost);\n+        return new Weight(this) {\n+            Weight mostCompactWeight;\n+\n+            @Override\n+            public boolean isCacheable(LeafReaderContext ctx) {\n+                return delegateForMultiValuedSegmentsWeight.isCacheable(ctx);\n+            }\n+\n+            @Override\n+            public Scorer scorer(LeafReaderContext context) throws IOException {\n+                ScorerSupplier scorerSupplier = scorerSupplier(context);\n+                if (scorerSupplier == null) {\n+                    return null;\n+                }\n+                return scorerSupplier.get(Long.MAX_VALUE);\n+            }\n+\n+            @Override\n+            public ScorerSupplier scorerSupplier(LeafReaderContext context) throws IOException {\n+                /*\n+                 * If we're sure docs only have a single value for the field\n+                 * we can pick the most compact bounds. If there are multiple values\n+                 * for the field we have to run the boolean query.\n+                 */\n+                PointValues points = context.reader().getPointValues(field);\n+                if (points == null) {\n+                    return null;\n+                }\n+                if (points.size() == points.getDocCount()) {\n+                    // Each doc that has points has exactly one point.\n+                    if (mostCompactWeight == null) {\n+                        mostCompactWeight = delegateForSingleValuedSegments.createWeight(searcher, scoreMode, boost);\n+                    }\n+                    return mostCompactWeight.scorerSupplier(context);\n+                }\n+                return delegateForMultiValuedSegmentsWeight.scorerSupplier(context);\n+            }\n+\n+            @Override\n+            public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n+                return super.bulkScorer(context);\n+            }\n+\n+            @Override\n+            @Deprecated\n+            public void extractTerms(Set<Term> terms) {\n+                delegateForMultiValuedSegmentsWeight.extractTerms(terms);\n+            }\n+\n+            @Override\n+            public Explanation explain(LeafReaderContext context, int doc) throws IOException {\n+                return delegateForMultiValuedSegmentsWeight.explain(context, doc);\n+            }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "66684f2983799e2430770b8a8daa95349f20ea7e"}, "originalPosition": 161}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b1c79b24153fc796779dd0e2c0c92963f5ec7843", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/b1c79b24153fc796779dd0e2c0c92963f5ec7843", "committedDate": "2020-11-05T19:50:44Z", "message": "Merge branch 'master' into date_histo_as_range"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "997e9b9948631e6463009961308790e3d414abf1", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/997e9b9948631e6463009961308790e3d414abf1", "committedDate": "2020-11-05T20:01:43Z", "message": "Feedback"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9494447f523c1785230423819e8ccec08349fb28", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/9494447f523c1785230423819e8ccec08349fb28", "committedDate": "2020-11-05T20:43:20Z", "message": "Merge branch 'master' into date_histo_as_range"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bfb10bd614bcfbafb77ff4d5306632a8ce67c00f", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/bfb10bd614bcfbafb77ff4d5306632a8ce67c00f", "committedDate": "2020-11-06T13:45:07Z", "message": "Merge branch 'master' into date_histo_as_range"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1e1b1dd99159fdfca5688210ac62afa2de90ddfb", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/1e1b1dd99159fdfca5688210ac62afa2de90ddfb", "committedDate": "2020-11-06T13:52:09Z", "message": "ConstantScoreWeight"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ca09463ad6846a4579d477713a9d26e54f27bf07", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/ca09463ad6846a4579d477713a9d26e54f27bf07", "committedDate": "2020-11-06T15:15:32Z", "message": "Merge branch 'master' into date_histo_as_range"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI2MDIwODgy", "url": "https://github.com/elastic/elasticsearch/pull/63643#pullrequestreview-526020882", "createdAt": "2020-11-09T08:30:49Z", "commit": {"oid": "ca09463ad6846a4579d477713a9d26e54f27bf07"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwODozMDo0OVrOHvjrVw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwODozMDo0OVrOHvjrVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTYyOTY1NQ==", "bodyText": "I think this is incomplete, we should check delegateForSingleValuedSegments as well?", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r519629655", "createdAt": "2020-11-09T08:30:49Z", "author": {"login": "iverase"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java", "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.search.aggregations.bucket.filter;\n+\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.PointValues;\n+import org.apache.lucene.search.BooleanClause.Occur;\n+import org.apache.lucene.search.BooleanQuery;\n+import org.apache.lucene.search.BulkScorer;\n+import org.apache.lucene.search.ConstantScoreWeight;\n+import org.apache.lucene.search.IndexSearcher;\n+import org.apache.lucene.search.MatchNoDocsQuery;\n+import org.apache.lucene.search.PointRangeQuery;\n+import org.apache.lucene.search.Query;\n+import org.apache.lucene.search.ScoreMode;\n+import org.apache.lucene.search.Scorer;\n+import org.apache.lucene.search.ScorerSupplier;\n+import org.apache.lucene.search.Weight;\n+\n+import java.io.IOException;\n+\n+import static java.util.Arrays.compareUnsigned;\n+\n+/**\n+ * Query merging two point in range queries.\n+ */\n+public class MergedPointRangeQuery extends Query {\n+    /**\n+     * Merge two {@linkplain PointRangeQuery}s into a {@linkplain MergedPointRangeQuery}\n+     * that matches points that match both filters.\n+     */\n+    public static Query merge(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        if (lhs.equals(rhs)) {\n+            // Lucky case! The queries were the same so their UNION is just the query itself.\n+            return lhs;\n+        }\n+        if (lhs.getField() != rhs.getField() || lhs.getNumDims() != rhs.getNumDims() || lhs.getBytesPerDim() != rhs.getBytesPerDim()) {\n+            return null;\n+        }\n+        return new MergedPointRangeQuery(lhs, rhs);\n+    }\n+\n+    private final String field;\n+    private final Query delegateForMultiValuedSegments;\n+    private final Query delegateForSingleValuedSegments;\n+\n+    private MergedPointRangeQuery(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        field = lhs.getField();\n+        delegateForMultiValuedSegments = new BooleanQuery.Builder().add(lhs, Occur.MUST).add(rhs, Occur.MUST).build();\n+        int numDims = lhs.getNumDims();\n+        int bytesPerDim = lhs.getBytesPerDim();\n+        this.delegateForSingleValuedSegments = pickDelegateForSingleValuedSegments(\n+            mergeBound(lhs.getLowerPoint(), rhs.getLowerPoint(), numDims, bytesPerDim, true),\n+            mergeBound(lhs.getUpperPoint(), rhs.getUpperPoint(), numDims, bytesPerDim, false),\n+            numDims,\n+            bytesPerDim\n+        );\n+    }\n+\n+    private Query pickDelegateForSingleValuedSegments(byte[] lower, byte[] upper, int numDims, int bytesPerDim) {\n+        // If we ended up with disjoint ranges in any dimension then on single valued segments we can't match any docs.\n+        for (int dim = 0; dim < numDims; dim++) {\n+            int offset = dim * bytesPerDim;\n+            if (compareUnsigned(lower, offset, offset + bytesPerDim, upper, offset, offset + bytesPerDim) > 0) {\n+                return new MatchNoDocsQuery(\"disjoint ranges\");\n+            }\n+        }\n+        // Otherwise on single valued segments we can only match docs the match the UNION of the two ranges.\n+        return new PointRangeQuery(field, lower, upper, numDims) {\n+            @Override\n+            protected String toString(int dimension, byte[] value) {\n+                // Stolen from Lucene's Binary range query. It'd be best to delegate, but the method isn't visible.\n+                StringBuilder sb = new StringBuilder();\n+                sb.append(\"(\");\n+                for (int i = 0; i < value.length; i++) {\n+                    if (i > 0) {\n+                        sb.append(' ');\n+                    }\n+                    sb.append(Integer.toHexString(value[i] & 0xFF));\n+                }\n+                sb.append(')');\n+                return sb.toString();\n+            }\n+        };\n+    }\n+\n+    @Override\n+    public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n+        return new ConstantScoreWeight(this, boost) {\n+            Weight multiValuedSegmentWeight;\n+            Weight singleValuedSegmentWeight;\n+\n+            @Override\n+            public boolean isCacheable(LeafReaderContext ctx) {\n+                return true;\n+            }\n+\n+            @Override\n+            public Scorer scorer(LeafReaderContext context) throws IOException {\n+                ScorerSupplier scorerSupplier = scorerSupplier(context);\n+                if (scorerSupplier == null) {\n+                    return null;\n+                }\n+                return scorerSupplier.get(Long.MAX_VALUE);\n+            }\n+\n+            @Override\n+            public ScorerSupplier scorerSupplier(LeafReaderContext context) throws IOException {\n+                /*\n+                 * If we're sure docs only have a single value for the field\n+                 * we can pick the most compact bounds. If there are multiple values\n+                 * for the field we have to run the boolean query.\n+                 */\n+                PointValues points = context.reader().getPointValues(field);\n+                if (points == null) {\n+                    return null;\n+                }\n+                if (points.size() == points.getDocCount()) {\n+                    // Each doc that has points has exactly one point.\n+                    return singleValuedSegmentWeight().scorerSupplier(context);\n+                }\n+                return multiValuedSegmentWeight().scorerSupplier(context);\n+            }\n+\n+            @Override\n+            public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n+                PointValues points = context.reader().getPointValues(field);\n+                if (points == null) {\n+                    return null;\n+                }\n+                if (points.size() == points.getDocCount()) {\n+                    // Each doc that has points has exactly one point.\n+                    return singleValuedSegmentWeight().bulkScorer(context);\n+                }\n+                return multiValuedSegmentWeight().bulkScorer(context);\n+            }\n+\n+            private Weight singleValuedSegmentWeight() throws IOException {\n+                if (singleValuedSegmentWeight == null) {\n+                    singleValuedSegmentWeight = delegateForSingleValuedSegments.createWeight(searcher, scoreMode, boost);\n+                }\n+                return singleValuedSegmentWeight;\n+            }\n+\n+            private Weight multiValuedSegmentWeight() throws IOException {\n+                if (multiValuedSegmentWeight == null) {\n+                    multiValuedSegmentWeight = delegateForMultiValuedSegments.createWeight(searcher, scoreMode, boost);\n+                }\n+                return multiValuedSegmentWeight;\n+            }\n+        };\n+    }\n+\n+    /**\n+     * The query used when we have single valued segments.\n+     */\n+    Query delegateForSingleValuedSegments() {\n+        return delegateForSingleValuedSegments;\n+    }\n+\n+    @Override\n+    public String toString(String field) {\n+        return \"MergedPointRange[\" + delegateForMultiValuedSegments.toString(field) + \"]\";\n+    }\n+\n+    @Override\n+    public boolean equals(Object obj) {\n+        if (obj == null || obj.getClass() != getClass()) {\n+            return false;\n+        }\n+        MergedPointRangeQuery other = (MergedPointRangeQuery) obj;\n+        return delegateForMultiValuedSegments.equals(other.delegateForMultiValuedSegments);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca09463ad6846a4579d477713a9d26e54f27bf07"}, "originalPosition": 189}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI2MDIxNTA1", "url": "https://github.com/elastic/elasticsearch/pull/63643#pullrequestreview-526021505", "createdAt": "2020-11-09T08:31:45Z", "commit": {"oid": "ca09463ad6846a4579d477713a9d26e54f27bf07"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwODozMTo0NlrOHvjtXw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwODozMTo0NlrOHvjtXw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTYzMDE3NQ==", "bodyText": "My feeling is that we do not need to implement this method. Note that queries are used in a filter context so we do not need to worry about scores?", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r519630175", "createdAt": "2020-11-09T08:31:46Z", "author": {"login": "iverase"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java", "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.search.aggregations.bucket.filter;\n+\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.PointValues;\n+import org.apache.lucene.search.BooleanClause.Occur;\n+import org.apache.lucene.search.BooleanQuery;\n+import org.apache.lucene.search.BulkScorer;\n+import org.apache.lucene.search.ConstantScoreWeight;\n+import org.apache.lucene.search.IndexSearcher;\n+import org.apache.lucene.search.MatchNoDocsQuery;\n+import org.apache.lucene.search.PointRangeQuery;\n+import org.apache.lucene.search.Query;\n+import org.apache.lucene.search.ScoreMode;\n+import org.apache.lucene.search.Scorer;\n+import org.apache.lucene.search.ScorerSupplier;\n+import org.apache.lucene.search.Weight;\n+\n+import java.io.IOException;\n+\n+import static java.util.Arrays.compareUnsigned;\n+\n+/**\n+ * Query merging two point in range queries.\n+ */\n+public class MergedPointRangeQuery extends Query {\n+    /**\n+     * Merge two {@linkplain PointRangeQuery}s into a {@linkplain MergedPointRangeQuery}\n+     * that matches points that match both filters.\n+     */\n+    public static Query merge(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        if (lhs.equals(rhs)) {\n+            // Lucky case! The queries were the same so their UNION is just the query itself.\n+            return lhs;\n+        }\n+        if (lhs.getField() != rhs.getField() || lhs.getNumDims() != rhs.getNumDims() || lhs.getBytesPerDim() != rhs.getBytesPerDim()) {\n+            return null;\n+        }\n+        return new MergedPointRangeQuery(lhs, rhs);\n+    }\n+\n+    private final String field;\n+    private final Query delegateForMultiValuedSegments;\n+    private final Query delegateForSingleValuedSegments;\n+\n+    private MergedPointRangeQuery(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        field = lhs.getField();\n+        delegateForMultiValuedSegments = new BooleanQuery.Builder().add(lhs, Occur.MUST).add(rhs, Occur.MUST).build();\n+        int numDims = lhs.getNumDims();\n+        int bytesPerDim = lhs.getBytesPerDim();\n+        this.delegateForSingleValuedSegments = pickDelegateForSingleValuedSegments(\n+            mergeBound(lhs.getLowerPoint(), rhs.getLowerPoint(), numDims, bytesPerDim, true),\n+            mergeBound(lhs.getUpperPoint(), rhs.getUpperPoint(), numDims, bytesPerDim, false),\n+            numDims,\n+            bytesPerDim\n+        );\n+    }\n+\n+    private Query pickDelegateForSingleValuedSegments(byte[] lower, byte[] upper, int numDims, int bytesPerDim) {\n+        // If we ended up with disjoint ranges in any dimension then on single valued segments we can't match any docs.\n+        for (int dim = 0; dim < numDims; dim++) {\n+            int offset = dim * bytesPerDim;\n+            if (compareUnsigned(lower, offset, offset + bytesPerDim, upper, offset, offset + bytesPerDim) > 0) {\n+                return new MatchNoDocsQuery(\"disjoint ranges\");\n+            }\n+        }\n+        // Otherwise on single valued segments we can only match docs the match the UNION of the two ranges.\n+        return new PointRangeQuery(field, lower, upper, numDims) {\n+            @Override\n+            protected String toString(int dimension, byte[] value) {\n+                // Stolen from Lucene's Binary range query. It'd be best to delegate, but the method isn't visible.\n+                StringBuilder sb = new StringBuilder();\n+                sb.append(\"(\");\n+                for (int i = 0; i < value.length; i++) {\n+                    if (i > 0) {\n+                        sb.append(' ');\n+                    }\n+                    sb.append(Integer.toHexString(value[i] & 0xFF));\n+                }\n+                sb.append(')');\n+                return sb.toString();\n+            }\n+        };\n+    }\n+\n+    @Override\n+    public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n+        return new ConstantScoreWeight(this, boost) {\n+            Weight multiValuedSegmentWeight;\n+            Weight singleValuedSegmentWeight;\n+\n+            @Override\n+            public boolean isCacheable(LeafReaderContext ctx) {\n+                return true;\n+            }\n+\n+            @Override\n+            public Scorer scorer(LeafReaderContext context) throws IOException {\n+                ScorerSupplier scorerSupplier = scorerSupplier(context);\n+                if (scorerSupplier == null) {\n+                    return null;\n+                }\n+                return scorerSupplier.get(Long.MAX_VALUE);\n+            }\n+\n+            @Override\n+            public ScorerSupplier scorerSupplier(LeafReaderContext context) throws IOException {\n+                /*\n+                 * If we're sure docs only have a single value for the field\n+                 * we can pick the most compact bounds. If there are multiple values\n+                 * for the field we have to run the boolean query.\n+                 */\n+                PointValues points = context.reader().getPointValues(field);\n+                if (points == null) {\n+                    return null;\n+                }\n+                if (points.size() == points.getDocCount()) {\n+                    // Each doc that has points has exactly one point.\n+                    return singleValuedSegmentWeight().scorerSupplier(context);\n+                }\n+                return multiValuedSegmentWeight().scorerSupplier(context);\n+            }\n+\n+            @Override\n+            public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n+                PointValues points = context.reader().getPointValues(field);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca09463ad6846a4579d477713a9d26e54f27bf07"}, "originalPosition": 144}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5b5d6ccf659b2cf24b5a39ef1fbc6b6f0d99a6be", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/5b5d6ccf659b2cf24b5a39ef1fbc6b6f0d99a6be", "committedDate": "2020-11-09T16:35:31Z", "message": "Iter"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI2NDkwMzc5", "url": "https://github.com/elastic/elasticsearch/pull/63643#pullrequestreview-526490379", "createdAt": "2020-11-09T17:31:30Z", "commit": {"oid": "5b5d6ccf659b2cf24b5a39ef1fbc6b6f0d99a6be"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fc1444d81576cd0451f2c76b3a6b9ccc58ad750b", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/fc1444d81576cd0451f2c76b3a6b9ccc58ad750b", "committedDate": "2020-11-09T17:58:06Z", "message": "Merge branch 'master' into date_histo_as_range"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4147, "cost": 1, "resetAt": "2021-10-28T18:54:27Z"}}}