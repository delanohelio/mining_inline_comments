{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDgyODUzNTQ0", "number": 62167, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwOTowODozMVrOEjCGFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwOTozNTo1MlrOEjCvNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA1MTY5OTQyOnYy", "diffSide": "RIGHT", "path": "plugins/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3RetryingInputStream.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwOTowODozMVrOHRKtfg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwOTowODozMVrOHRKtfg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Nzc2MzMyNg==", "bodyText": "See my comment below, I think this isn't necessary potentially.", "url": "https://github.com/elastic/elasticsearch/pull/62167#discussion_r487763326", "createdAt": "2020-09-14T09:08:31Z", "author": {"login": "original-brownbear"}, "path": "plugins/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3RetryingInputStream.java", "diffHunk": "@@ -53,12 +54,14 @@\n     private final long start;\n     private final long end;\n     private final int maxAttempts;\n+    private final List<IOException> failures;\n \n-    private InputStream currentStream;\n+    private S3ObjectInputStream currentStream;\n+    private long currentStreamLastOffset;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "824e8f9ff09b0179c0cb5fc670476698df98df36"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA1MTc1MTIzOnYy", "diffSide": "RIGHT", "path": "plugins/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3RetryingInputStream.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwOToyMTozM1rOHRLMQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQxMTowNDo0MVrOHROtZw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Nzc3MTIwMQ==", "bodyText": "Same here, maybe we should just use our own end and start offsets or use metadata.getContentLength() if we don't have an end instead of going through the indirection of the SDH header parsing here? That seems a lot more straight forward to me and doesn't require us to be scared of random exceptions from SDK misbehavior?\nAlso, then we could just make our life real easy. If eof is set to true or start + currentOffset == currentStreamLastOffset -> close, else abort. No need to even get the length from the metadata because any open ended stream of unknown length we'd read till EOF anyway?", "url": "https://github.com/elastic/elasticsearch/pull/62167#discussion_r487771201", "createdAt": "2020-09-14T09:21:33Z", "author": {"login": "original-brownbear"}, "path": "plugins/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3RetryingInputStream.java", "diffHunk": "@@ -101,12 +106,35 @@ private InputStream openStream() throws IOException {\n         }\n     }\n \n+    private long getStreamLength(final S3Object object) {\n+        final ObjectMetadata metadata = object.getObjectMetadata();\n+        try {\n+            // Returns the content range of the object if response contains the Content-Range header.\n+            final Long[] range = metadata.getContentRange();\n+            if (range != null) {\n+                assert range[1] >= range[0] : range[1] + \" vs \" + range[0];\n+                assert range[0] == start + currentOffset :\n+                    \"Content-Range start value [\" + range[0] + \"] exceeds start [\" + start + \"] + current offset [\" + currentOffset + ']';\n+                assert range[1] == end : \"Content-Range end value [\" + range[1] + \"] exceeds end [\" + end + ']';\n+                return range[1] - range[0] + 1L;\n+            }\n+            return metadata.getContentLength();\n+        } catch (Exception e) {\n+            assert false : e;\n+            return Long.MAX_VALUE - 1L; // assume a large stream so that the underlying stream is aborted on closing, unless eof is reached", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "824e8f9ff09b0179c0cb5fc670476698df98df36"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Nzc5Mzc5Nw==", "bodyText": "No need to even get the length from the metadata because any open ended stream of unknown length we'd read till EOF anyway?\n\nThat's what I'm trying to avoid here; if we know the exact range or if we don't know it, the S3 endpoint should return the content length and we can use it to know if all bytes were really consumed. The exceptional case here should never happen and in this case we set an extra large end which should force anyway the stream to be aborted before closing.", "url": "https://github.com/elastic/elasticsearch/pull/62167#discussion_r487793797", "createdAt": "2020-09-14T09:59:37Z", "author": {"login": "tlrx"}, "path": "plugins/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3RetryingInputStream.java", "diffHunk": "@@ -101,12 +106,35 @@ private InputStream openStream() throws IOException {\n         }\n     }\n \n+    private long getStreamLength(final S3Object object) {\n+        final ObjectMetadata metadata = object.getObjectMetadata();\n+        try {\n+            // Returns the content range of the object if response contains the Content-Range header.\n+            final Long[] range = metadata.getContentRange();\n+            if (range != null) {\n+                assert range[1] >= range[0] : range[1] + \" vs \" + range[0];\n+                assert range[0] == start + currentOffset :\n+                    \"Content-Range start value [\" + range[0] + \"] exceeds start [\" + start + \"] + current offset [\" + currentOffset + ']';\n+                assert range[1] == end : \"Content-Range end value [\" + range[1] + \"] exceeds end [\" + end + ']';\n+                return range[1] - range[0] + 1L;\n+            }\n+            return metadata.getContentLength();\n+        } catch (Exception e) {\n+            assert false : e;\n+            return Long.MAX_VALUE - 1L; // assume a large stream so that the underlying stream is aborted on closing, unless eof is reached", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Nzc3MTIwMQ=="}, "originalCommit": {"oid": "824e8f9ff09b0179c0cb5fc670476698df98df36"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzgyODgzOQ==", "bodyText": "Fair point, I guess I still don't really like that we redundantly store the lengths and offsets here to some degree, but I also just noticed we we do the same in the GCS stream as well. I suppose this approach is the safest for now :) => let's go with it then.", "url": "https://github.com/elastic/elasticsearch/pull/62167#discussion_r487828839", "createdAt": "2020-09-14T11:04:41Z", "author": {"login": "original-brownbear"}, "path": "plugins/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3RetryingInputStream.java", "diffHunk": "@@ -101,12 +106,35 @@ private InputStream openStream() throws IOException {\n         }\n     }\n \n+    private long getStreamLength(final S3Object object) {\n+        final ObjectMetadata metadata = object.getObjectMetadata();\n+        try {\n+            // Returns the content range of the object if response contains the Content-Range header.\n+            final Long[] range = metadata.getContentRange();\n+            if (range != null) {\n+                assert range[1] >= range[0] : range[1] + \" vs \" + range[0];\n+                assert range[0] == start + currentOffset :\n+                    \"Content-Range start value [\" + range[0] + \"] exceeds start [\" + start + \"] + current offset [\" + currentOffset + ']';\n+                assert range[1] == end : \"Content-Range end value [\" + range[1] + \"] exceeds end [\" + end + ']';\n+                return range[1] - range[0] + 1L;\n+            }\n+            return metadata.getContentLength();\n+        } catch (Exception e) {\n+            assert false : e;\n+            return Long.MAX_VALUE - 1L; // assume a large stream so that the underlying stream is aborted on closing, unless eof is reached", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Nzc3MTIwMQ=="}, "originalCommit": {"oid": "824e8f9ff09b0179c0cb5fc670476698df98df36"}, "originalPosition": 77}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA1MTgwNDcxOnYy", "diffSide": "RIGHT", "path": "plugins/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3RetryingInputStream.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwOTozNTo1MlrOHRLs2A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwOTozNTo1MlrOHRLs2A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Nzc3OTU0NA==", "bodyText": "if (eof || start + currentOffset == currentStreamLastOffset) { and drop the conditional from the try { since both cases mean the same to us?", "url": "https://github.com/elastic/elasticsearch/pull/62167#discussion_r487779544", "createdAt": "2020-09-14T09:35:52Z", "author": {"login": "original-brownbear"}, "path": "plugins/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3RetryingInputStream.java", "diffHunk": "@@ -151,24 +180,36 @@ private void reopenStreamOrFail(IOException e) throws IOException {\n         if (failures.size() < MAX_SUPPRESSED_EXCEPTIONS) {\n             failures.add(e);\n         }\n-        try {\n-            Streams.consumeFully(currentStream);\n-        } catch (Exception e2) {\n-            logger.trace(\"Failed to fully consume stream on close\", e);\n-        }\n+        maybeAbort(currentStream);\n         IOUtils.closeWhileHandlingException(currentStream);\n-        currentStream = openStream();\n+        openStream();\n     }\n \n     @Override\n     public void close() throws IOException {\n+        maybeAbort(currentStream);\n         try {\n-            Streams.consumeFully(currentStream);\n+            currentStream.close();\n+        } finally {\n+            closed = true;\n+        }\n+    }\n+\n+    /**\n+     * Abort the {@link S3ObjectInputStream} if it wasn't read completely at the time this method is called,\n+     * suppressing all thrown exceptions.\n+     */\n+    private void maybeAbort(S3ObjectInputStream stream) {\n+        if (eof) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "824e8f9ff09b0179c0cb5fc670476698df98df36"}, "originalPosition": 133}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1593, "cost": 1, "resetAt": "2021-11-12T13:16:51Z"}}}