{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQxNjc1MzI5", "number": 66489, "title": "Reduce memory usage on Azure repository implementation", "bodyText": "This commit moves the upload logic to the repository itself\ninstead of delegating into the SDK.\nMulti-block uploads are done sequentially instead of in parallel\nthat allows to bound the outstanding memory.\nAdditionally the number of i/o threads have been reduced to 1,\nto reduce the memory overhead.\nCloses #66385", "createdAt": "2020-12-17T07:48:10Z", "url": "https://github.com/elastic/elasticsearch/pull/66489", "merged": true, "mergeCommit": {"oid": "02ac68eb8bd7a31dd9fae3dcbb2aeb58e863ef03"}, "closed": true, "closedAt": "2020-12-17T10:09:56Z", "author": {"login": "fcofdez"}, "timelineItems": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdm-ngCgH2gAyNTQxNjc1MzI5OmNhMWMwNDcxOGY3NGFkZmNlNzgwMDE5Mjg4YjEyNGZlMjc2NDMyMjk=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdnAIp0AFqTU1NDQyMTM5NA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "ca1c04718f74adfce780019288b124fe27643229", "author": {"user": {"login": "fcofdez", "name": "Francisco Fern\u00e1ndez Casta\u00f1o"}}, "url": "https://github.com/elastic/elasticsearch/commit/ca1c04718f74adfce780019288b124fe27643229", "committedDate": "2020-12-17T07:43:37Z", "message": "Reduce memory usage on Azure repository implementation\n\nThis commit moves the upload logic to the repository itself\ninstead of delegating into the SDK.\nMulti-block uploads are done sequentially instead of in parallel\nthat allows to bound the outstanding memory.\nAdditionally the number of i/o threads have been reduced to 1,\nto reduce the memory overhead."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU0MzQ4NDU1", "url": "https://github.com/elastic/elasticsearch/pull/66489#pullrequestreview-554348455", "createdAt": "2020-12-17T07:49:11Z", "commit": {"oid": "ca1c04718f74adfce780019288b124fe27643229"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwNzo0OToxMVrOIHoq6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwNzo0OToxMVrOIHoq6w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg3NzI5MQ==", "bodyText": "I think that we can reduce those too, wdyt @original-brownbear?", "url": "https://github.com/elastic/elasticsearch/pull/66489#discussion_r544877291", "createdAt": "2020-12-17T07:49:11Z", "author": {"login": "fcofdez"}, "path": "plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureClientProvider.java", "diffHunk": "@@ -150,7 +150,6 @@ private static ByteBufAllocator createByteBufAllocator() {\n         int tinyCacheSize = PooledByteBufAllocator.defaultTinyCacheSize();\n         int smallCacheSize = PooledByteBufAllocator.defaultSmallCacheSize();\n         int normalCacheSize = PooledByteBufAllocator.defaultNormalCacheSize();\n-        boolean useCacheForAllThreads = PooledByteBufAllocator.defaultUseCacheForAllThreads();\n \n         return new PooledByteBufAllocator(false,\n             nHeapArena,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca1c04718f74adfce780019288b124fe27643229"}, "originalPosition": 16}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU0MzYwMDE1", "url": "https://github.com/elastic/elasticsearch/pull/66489#pullrequestreview-554360015", "createdAt": "2020-12-17T08:08:37Z", "commit": {"oid": "ca1c04718f74adfce780019288b124fe27643229"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwODowODozN1rOIHpTPA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwODo1NzowN1rOIHrF-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg4NzYxMg==", "bodyText": "Yea, can't we just do 1 maybe when we only use one thread now anyway?", "url": "https://github.com/elastic/elasticsearch/pull/66489#discussion_r544887612", "createdAt": "2020-12-17T08:08:37Z", "author": {"login": "original-brownbear"}, "path": "plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureClientProvider.java", "diffHunk": "@@ -150,7 +150,6 @@ private static ByteBufAllocator createByteBufAllocator() {\n         int tinyCacheSize = PooledByteBufAllocator.defaultTinyCacheSize();\n         int smallCacheSize = PooledByteBufAllocator.defaultSmallCacheSize();\n         int normalCacheSize = PooledByteBufAllocator.defaultNormalCacheSize();\n-        boolean useCacheForAllThreads = PooledByteBufAllocator.defaultUseCacheForAllThreads();\n \n         return new PooledByteBufAllocator(false,\n             nHeapArena,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg3NzI5MQ=="}, "originalCommit": {"oid": "ca1c04718f74adfce780019288b124fe27643229"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDkxNjk4NA==", "bodyText": "This still scares me a little. If we only do 64k at a time here, can't we use the Netty memory allocator (or manage  our own set of byte[] and recycle them on doOnComplete or do we still have no guarantees about flushing at that point?", "url": "https://github.com/elastic/elasticsearch/pull/66489#discussion_r544916984", "createdAt": "2020-12-17T08:57:07Z", "author": {"login": "original-brownbear"}, "path": "plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureBlobStore.java", "diffHunk": "@@ -404,12 +403,156 @@ public void writeBlob(String blobName, InputStream inputStream, long blobSize, b\n         logger.trace(() -> new ParameterizedMessage(\"writeBlob({}, stream, {}) - done\", blobName, blobSize));\n     }\n \n-    private ParallelTransferOptions getParallelTransferOptions() {\n-        ParallelTransferOptions parallelTransferOptions = new ParallelTransferOptions();\n-        parallelTransferOptions.setBlockSizeLong(service.getUploadBlockSize())\n-            .setMaxSingleUploadSizeLong(service.getSizeThresholdForMultiBlockUpload())\n-            .setMaxConcurrency(service.getMaxUploadParallelism());\n-        return parallelTransferOptions;\n+    private void executeSingleUpload(String blobName, InputStream inputStream, long blobSize, boolean failIfAlreadyExists) {\n+        SocketAccess.doPrivilegedVoidException(() -> {\n+            final BlobServiceAsyncClient asyncClient = asyncClient();\n+\n+            final BlobAsyncClient blobAsyncClient = asyncClient.getBlobContainerAsyncClient(container).getBlobAsyncClient(blobName);\n+            final BlockBlobAsyncClient blockBlobAsyncClient = blobAsyncClient.getBlockBlobAsyncClient();\n+\n+            final Flux<ByteBuffer> byteBufferFlux =\n+                convertStreamToByteBuffer(inputStream, blobSize, DEFAULT_UPLOAD_BUFFERS_SIZE);\n+            final BlockBlobSimpleUploadOptions options = new BlockBlobSimpleUploadOptions(byteBufferFlux, blobSize);\n+            BlobRequestConditions requestConditions = new BlobRequestConditions();\n+            if (failIfAlreadyExists) {\n+                requestConditions.setIfNoneMatch(\"*\");\n+            }\n+            options.setRequestConditions(requestConditions);\n+            blockBlobAsyncClient.uploadWithResponse(options).block();\n+        });\n+    }\n+\n+    private void executeMultipartUpload(String blobName, InputStream inputStream, long blobSize, boolean failIfAlreadyExists) {\n+        SocketAccess.doPrivilegedVoidException(() -> {\n+            final BlobServiceAsyncClient asyncClient = asyncClient();\n+            final BlobAsyncClient blobAsyncClient = asyncClient.getBlobContainerAsyncClient(container)\n+                .getBlobAsyncClient(blobName);\n+            final BlockBlobAsyncClient blockBlobAsyncClient = blobAsyncClient.getBlockBlobAsyncClient();\n+\n+            final long partSize = getUploadBlockSize();\n+            final Tuple<Long, Long> multiParts = numberOfMultiparts(blobSize, partSize);\n+            final int nbParts = multiParts.v1().intValue();\n+            final long lastPartSize = multiParts.v2();\n+            assert blobSize == (((nbParts - 1) * partSize) + lastPartSize) : \"blobSize does not match multipart sizes\";\n+\n+            final List<String> blockIds = new ArrayList<>(nbParts);\n+            for (int i = 0; i < nbParts; i++) {\n+                final long length = i < nbParts - 1 ? partSize : lastPartSize;\n+                final Flux<ByteBuffer> byteBufferFlux =\n+                    convertStreamToByteBuffer(inputStream, length, DEFAULT_UPLOAD_BUFFERS_SIZE);\n+\n+                final String blockId = UUIDs.base64UUID();\n+                blockBlobAsyncClient.stageBlock(blockId, byteBufferFlux, length).block();\n+                blockIds.add(blockId);\n+            }\n+\n+            blockBlobAsyncClient.commitBlockList(blockIds, failIfAlreadyExists == false).block();\n+        });\n+    }\n+\n+    /**\n+     * Converts the provided input stream into a Flux of ByteBuffer. To avoid having large amounts of outstanding\n+     * memory this Flux reads the InputStream into ByteBuffers of {@code chunkSize} size.\n+     * @param inputStream the InputStream to convert\n+     * @param length the InputStream length\n+     * @param chunkSize the chunk size in bytes\n+     * @return a Flux of ByteBuffers\n+     */\n+    private Flux<ByteBuffer> convertStreamToByteBuffer(InputStream inputStream, long length, int chunkSize) {\n+        assert inputStream.markSupported() : \"An InputStream with mark support was expected\";\n+        // We need to mark the InputStream as it's possible that we need to retry for the same chunk\n+        inputStream.mark(Integer.MAX_VALUE);\n+        return Flux.defer(() -> {\n+            final AtomicLong currentTotalLength = new AtomicLong(0);\n+            try {\n+                inputStream.reset();\n+            } catch (IOException e) {\n+                throw new RuntimeException(e);\n+            }\n+            // This flux is subscribed by a downstream operator that finally queues the\n+            // buffers into netty output queue. Sadly we are not able to get a signal once\n+            // the buffer has been flushed, so we have to allocate those and let the GC to\n+            // reclaim them (see MonoSendMany). Additionally, that very same operator requests\n+            // 128 elements (that's hardcoded) once it's subscribed (later on, it requests\n+            // by 64 elements), that's why we provide 64kb buffers.\n+            return Flux.range(0, (int) Math.ceil((double) length / (double) chunkSize))\n+                .map(i -> i * chunkSize)\n+                .concatMap(pos -> Mono.fromCallable(() -> {\n+                    long count = pos + chunkSize > length ? length - pos : chunkSize;\n+                    int numOfBytesRead = 0;\n+                    int offset = 0;\n+                    int len = (int) count;\n+                    final byte[] buffer = new byte[len];", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca1c04718f74adfce780019288b124fe27643229"}, "originalPosition": 156}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d044af02e6109e7108359a5d521c0a569c92e924", "author": {"user": {"login": "fcofdez", "name": "Francisco Fern\u00e1ndez Casta\u00f1o"}}, "url": "https://github.com/elastic/elasticsearch/commit/d044af02e6109e7108359a5d521c0a569c92e924", "committedDate": "2020-12-17T09:27:55Z", "message": "Reduce the number of heap arenas"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU0NDIxMzk0", "url": "https://github.com/elastic/elasticsearch/pull/66489#pullrequestreview-554421394", "createdAt": "2020-12-17T09:29:44Z", "commit": {"oid": "d044af02e6109e7108359a5d521c0a569c92e924"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4457, "cost": 1, "resetAt": "2021-10-28T18:54:27Z"}}}