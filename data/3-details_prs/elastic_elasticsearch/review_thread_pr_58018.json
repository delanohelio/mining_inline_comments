{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDMzNDQ0NzQ3", "number": 58018, "reviewThreads": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQwMzozOToyM1rOEE6-eg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wN1QwNjo0NzoxNFrOEL8hQA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczNTk2MDI2OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/indices/recovery/PeerRecoveryTargetService.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQwMzozOToyM1rOGi2A9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNlQxNTo1MjowN1rOGtdNSA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE4OTc0OA==", "bodyText": "I think we should use the WRITE threadpool instead of GENERIC for indexing operations.", "url": "https://github.com/elastic/elasticsearch/pull/58018#discussion_r439189748", "createdAt": "2020-06-12T03:39:23Z", "author": {"login": "dnhatn"}, "path": "server/src/main/java/org/elasticsearch/indices/recovery/PeerRecoveryTargetService.java", "diffHunk": "@@ -119,7 +119,7 @@ public PeerRecoveryTargetService(ThreadPool threadPool, TransportService transpo\n             RecoveryCleanFilesRequest::new, new CleanFilesRequestHandler());\n         transportService.registerRequestHandler(Actions.PREPARE_TRANSLOG, ThreadPool.Names.GENERIC,\n                 RecoveryPrepareForTranslogOperationsRequest::new, new PrepareForTranslogOperationsRequestHandler());\n-        transportService.registerRequestHandler(Actions.TRANSLOG_OPS, ThreadPool.Names.GENERIC, RecoveryTranslogOperationsRequest::new,\n+        transportService.registerRequestHandler(Actions.TRANSLOG_OPS, ThreadPool.Names.WRITE, RecoveryTranslogOperationsRequest::new,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a2103a7861330be4de74fe3496e05927a47cd4ba"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDMxNzY0MA==", "bodyText": "I backed out this change in d34db4b. I think we need a separate change for it.", "url": "https://github.com/elastic/elasticsearch/pull/58018#discussion_r450317640", "createdAt": "2020-07-06T15:52:07Z", "author": {"login": "dnhatn"}, "path": "server/src/main/java/org/elasticsearch/indices/recovery/PeerRecoveryTargetService.java", "diffHunk": "@@ -119,7 +119,7 @@ public PeerRecoveryTargetService(ThreadPool threadPool, TransportService transpo\n             RecoveryCleanFilesRequest::new, new CleanFilesRequestHandler());\n         transportService.registerRequestHandler(Actions.PREPARE_TRANSLOG, ThreadPool.Names.GENERIC,\n                 RecoveryPrepareForTranslogOperationsRequest::new, new PrepareForTranslogOperationsRequestHandler());\n-        transportService.registerRequestHandler(Actions.TRANSLOG_OPS, ThreadPool.Names.GENERIC, RecoveryTranslogOperationsRequest::new,\n+        transportService.registerRequestHandler(Actions.TRANSLOG_OPS, ThreadPool.Names.WRITE, RecoveryTranslogOperationsRequest::new,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE4OTc0OA=="}, "originalCommit": {"oid": "a2103a7861330be4de74fe3496e05927a47cd4ba"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczNjcyNzE1OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQwOToyOToyN1rOGi9c6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNDowMDo0OFrOGjFGuw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTMxMTU5NA==", "bodyText": "I'm not sure I understand how this adds parallelism: If we only have one item here in the chunks list, then the MultiChunkTransfer won't do anything for us will it?", "url": "https://github.com/elastic/elasticsearch/pull/58018#discussion_r439311594", "createdAt": "2020-06-12T09:29:27Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java", "diffHunk": "@@ -661,106 +655,122 @@ void phase2(\n             throw new IndexShardClosedException(request.shardId());\n         }\n         logger.trace(\"recovery [phase2]: sending transaction log operations (from [\" + startingSeqNo + \"] to [\" + endingSeqNo + \"]\");\n-\n-        final AtomicInteger skippedOps = new AtomicInteger();\n-        final AtomicInteger totalSentOps = new AtomicInteger();\n-        final AtomicInteger lastBatchCount = new AtomicInteger(); // used to estimate the count of the subsequent batch.\n-        final CheckedSupplier<List<Translog.Operation>, IOException> readNextBatch = () -> {\n-            // We need to synchronized Snapshot#next() because it's called by different threads through sendBatch.\n-            // Even though those calls are not concurrent, Snapshot#next() uses non-synchronized state and is not multi-thread-compatible.\n-            synchronized (snapshot) {\n-                final List<Translog.Operation> ops = lastBatchCount.get() > 0 ? new ArrayList<>(lastBatchCount.get()) : new ArrayList<>();\n-                long batchSizeInBytes = 0L;\n-                Translog.Operation operation;\n-                while ((operation = snapshot.next()) != null) {\n-                    if (shard.state() == IndexShardState.CLOSED) {\n-                        throw new IndexShardClosedException(request.shardId());\n-                    }\n-                    cancellableThreads.checkForCancel();\n-                    final long seqNo = operation.seqNo();\n-                    if (seqNo < startingSeqNo || seqNo > endingSeqNo) {\n-                        skippedOps.incrementAndGet();\n-                        continue;\n-                    }\n-                    ops.add(operation);\n-                    batchSizeInBytes += operation.estimateSize();\n-                    totalSentOps.incrementAndGet();\n-\n-                    // check if this request is past bytes threshold, and if so, send it off\n-                    if (batchSizeInBytes >= chunkSizeInBytes) {\n-                        break;\n-                    }\n-                }\n-                lastBatchCount.set(ops.size());\n-                return ops;\n-            }\n-        };\n-\n         final StopWatch stopWatch = new StopWatch().start();\n-        final ActionListener<Long> batchedListener = ActionListener.map(listener,\n-            targetLocalCheckpoint -> {\n-                assert snapshot.totalOperations() == snapshot.skippedOperations() + skippedOps.get() + totalSentOps.get()\n+        final StepListener<Void> sendListener = new StepListener<>();\n+        final OperationBatchSender sender = new OperationBatchSender(startingSeqNo, endingSeqNo, snapshot, maxSeenAutoIdTimestamp,\n+            maxSeqNoOfUpdatesOrDeletes, retentionLeases, mappingVersion, sendListener);\n+        sendListener.whenComplete(\n+            ignored -> {\n+                final long skippedOps = sender.skippedOps.get();\n+                final int totalSentOps = sender.sentOps.get();\n+                final long targetLocalCheckpoint = sender.targetLocalCheckpoint.get();\n+                assert snapshot.totalOperations() == snapshot.skippedOperations() + skippedOps + totalSentOps\n                     : String.format(Locale.ROOT, \"expected total [%d], overridden [%d], skipped [%d], total sent [%d]\",\n-                    snapshot.totalOperations(), snapshot.skippedOperations(), skippedOps.get(), totalSentOps.get());\n+                    snapshot.totalOperations(), snapshot.skippedOperations(), skippedOps, totalSentOps);\n                 stopWatch.stop();\n                 final TimeValue tookTime = stopWatch.totalTime();\n                 logger.trace(\"recovery [phase2]: took [{}]\", tookTime);\n-                return new SendSnapshotResult(targetLocalCheckpoint, totalSentOps.get(), tookTime);\n-            }\n-        );\n+                listener.onResponse(new SendSnapshotResult(targetLocalCheckpoint, totalSentOps, tookTime));\n+            }, listener::onFailure);\n+        sender.start();\n+    }\n \n-        sendBatch(\n-                readNextBatch,\n-                true,\n-                SequenceNumbers.UNASSIGNED_SEQ_NO,\n-                snapshot.totalOperations(),\n-                maxSeenAutoIdTimestamp,\n-                maxSeqNoOfUpdatesOrDeletes,\n-                retentionLeases,\n-                mappingVersion,\n-                batchedListener);\n+    private static class OperationChunkRequest implements MultiChunkTransfer.ChunkRequest {\n+        final List<Translog.Operation> operations;\n+        final boolean lastChunk;\n+\n+        OperationChunkRequest(List<Translog.Operation> operations, boolean lastChunk) {\n+            this.operations = operations;\n+            this.lastChunk = lastChunk;\n+        }\n+\n+        @Override\n+        public boolean lastChunk() {\n+            return lastChunk;\n+        }\n     }\n \n-    private void sendBatch(\n-            final CheckedSupplier<List<Translog.Operation>, IOException> nextBatch,\n-            final boolean firstBatch,\n-            final long targetLocalCheckpoint,\n-            final int totalTranslogOps,\n-            final long maxSeenAutoIdTimestamp,\n-            final long maxSeqNoOfUpdatesOrDeletes,\n-            final RetentionLeases retentionLeases,\n-            final long mappingVersionOnPrimary,\n-            final ActionListener<Long> listener) throws IOException {\n-        assert ThreadPool.assertCurrentMethodIsNotCalledRecursively();\n-        assert Transports.assertNotTransportThread(RecoverySourceHandler.this + \"[send translog]\");\n-        final List<Translog.Operation> operations = nextBatch.get();\n-        // send the leftover operations or if no operations were sent, request the target to respond with its local checkpoint\n-        if (operations.isEmpty() == false || firstBatch) {\n+    private class OperationBatchSender extends MultiChunkTransfer<Translog.Snapshot, OperationChunkRequest> {\n+        private final long startingSeqNo;\n+        private final long endingSeqNo;\n+        private final Translog.Snapshot snapshot;\n+        private final long maxSeenAutoIdTimestamp;\n+        private final long maxSeqNoOfUpdatesOrDeletes;\n+        private final RetentionLeases retentionLeases;\n+        private final long mappingVersion;\n+        private int lastBatchCount = 0; // used to estimate the count of the subsequent batch.\n+        private final AtomicInteger skippedOps = new AtomicInteger();\n+        private final AtomicInteger sentOps = new AtomicInteger();\n+        private final AtomicLong targetLocalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n+\n+        OperationBatchSender(long startingSeqNo, long endingSeqNo, Translog.Snapshot snapshot, long maxSeenAutoIdTimestamp,\n+                             long maxSeqNoOfUpdatesOrDeletes, RetentionLeases retentionLeases, long mappingVersion,\n+                             ActionListener<Void> listener) {\n+            super(logger, threadPool.getThreadContext(), listener, maxConcurrentFileChunks, List.of(snapshot));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7707c3318111b53b310ce782295d049b06a98d71"}, "originalPosition": 160}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQyMzE4NQ==", "bodyText": "We have a single snapshot, but we can have multiple batches from it. The MultiChunkTransfer can send multiple batches without waiting for the acknowledges from the receiver.", "url": "https://github.com/elastic/elasticsearch/pull/58018#discussion_r439423185", "createdAt": "2020-06-12T13:38:10Z", "author": {"login": "dnhatn"}, "path": "server/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java", "diffHunk": "@@ -661,106 +655,122 @@ void phase2(\n             throw new IndexShardClosedException(request.shardId());\n         }\n         logger.trace(\"recovery [phase2]: sending transaction log operations (from [\" + startingSeqNo + \"] to [\" + endingSeqNo + \"]\");\n-\n-        final AtomicInteger skippedOps = new AtomicInteger();\n-        final AtomicInteger totalSentOps = new AtomicInteger();\n-        final AtomicInteger lastBatchCount = new AtomicInteger(); // used to estimate the count of the subsequent batch.\n-        final CheckedSupplier<List<Translog.Operation>, IOException> readNextBatch = () -> {\n-            // We need to synchronized Snapshot#next() because it's called by different threads through sendBatch.\n-            // Even though those calls are not concurrent, Snapshot#next() uses non-synchronized state and is not multi-thread-compatible.\n-            synchronized (snapshot) {\n-                final List<Translog.Operation> ops = lastBatchCount.get() > 0 ? new ArrayList<>(lastBatchCount.get()) : new ArrayList<>();\n-                long batchSizeInBytes = 0L;\n-                Translog.Operation operation;\n-                while ((operation = snapshot.next()) != null) {\n-                    if (shard.state() == IndexShardState.CLOSED) {\n-                        throw new IndexShardClosedException(request.shardId());\n-                    }\n-                    cancellableThreads.checkForCancel();\n-                    final long seqNo = operation.seqNo();\n-                    if (seqNo < startingSeqNo || seqNo > endingSeqNo) {\n-                        skippedOps.incrementAndGet();\n-                        continue;\n-                    }\n-                    ops.add(operation);\n-                    batchSizeInBytes += operation.estimateSize();\n-                    totalSentOps.incrementAndGet();\n-\n-                    // check if this request is past bytes threshold, and if so, send it off\n-                    if (batchSizeInBytes >= chunkSizeInBytes) {\n-                        break;\n-                    }\n-                }\n-                lastBatchCount.set(ops.size());\n-                return ops;\n-            }\n-        };\n-\n         final StopWatch stopWatch = new StopWatch().start();\n-        final ActionListener<Long> batchedListener = ActionListener.map(listener,\n-            targetLocalCheckpoint -> {\n-                assert snapshot.totalOperations() == snapshot.skippedOperations() + skippedOps.get() + totalSentOps.get()\n+        final StepListener<Void> sendListener = new StepListener<>();\n+        final OperationBatchSender sender = new OperationBatchSender(startingSeqNo, endingSeqNo, snapshot, maxSeenAutoIdTimestamp,\n+            maxSeqNoOfUpdatesOrDeletes, retentionLeases, mappingVersion, sendListener);\n+        sendListener.whenComplete(\n+            ignored -> {\n+                final long skippedOps = sender.skippedOps.get();\n+                final int totalSentOps = sender.sentOps.get();\n+                final long targetLocalCheckpoint = sender.targetLocalCheckpoint.get();\n+                assert snapshot.totalOperations() == snapshot.skippedOperations() + skippedOps + totalSentOps\n                     : String.format(Locale.ROOT, \"expected total [%d], overridden [%d], skipped [%d], total sent [%d]\",\n-                    snapshot.totalOperations(), snapshot.skippedOperations(), skippedOps.get(), totalSentOps.get());\n+                    snapshot.totalOperations(), snapshot.skippedOperations(), skippedOps, totalSentOps);\n                 stopWatch.stop();\n                 final TimeValue tookTime = stopWatch.totalTime();\n                 logger.trace(\"recovery [phase2]: took [{}]\", tookTime);\n-                return new SendSnapshotResult(targetLocalCheckpoint, totalSentOps.get(), tookTime);\n-            }\n-        );\n+                listener.onResponse(new SendSnapshotResult(targetLocalCheckpoint, totalSentOps, tookTime));\n+            }, listener::onFailure);\n+        sender.start();\n+    }\n \n-        sendBatch(\n-                readNextBatch,\n-                true,\n-                SequenceNumbers.UNASSIGNED_SEQ_NO,\n-                snapshot.totalOperations(),\n-                maxSeenAutoIdTimestamp,\n-                maxSeqNoOfUpdatesOrDeletes,\n-                retentionLeases,\n-                mappingVersion,\n-                batchedListener);\n+    private static class OperationChunkRequest implements MultiChunkTransfer.ChunkRequest {\n+        final List<Translog.Operation> operations;\n+        final boolean lastChunk;\n+\n+        OperationChunkRequest(List<Translog.Operation> operations, boolean lastChunk) {\n+            this.operations = operations;\n+            this.lastChunk = lastChunk;\n+        }\n+\n+        @Override\n+        public boolean lastChunk() {\n+            return lastChunk;\n+        }\n     }\n \n-    private void sendBatch(\n-            final CheckedSupplier<List<Translog.Operation>, IOException> nextBatch,\n-            final boolean firstBatch,\n-            final long targetLocalCheckpoint,\n-            final int totalTranslogOps,\n-            final long maxSeenAutoIdTimestamp,\n-            final long maxSeqNoOfUpdatesOrDeletes,\n-            final RetentionLeases retentionLeases,\n-            final long mappingVersionOnPrimary,\n-            final ActionListener<Long> listener) throws IOException {\n-        assert ThreadPool.assertCurrentMethodIsNotCalledRecursively();\n-        assert Transports.assertNotTransportThread(RecoverySourceHandler.this + \"[send translog]\");\n-        final List<Translog.Operation> operations = nextBatch.get();\n-        // send the leftover operations or if no operations were sent, request the target to respond with its local checkpoint\n-        if (operations.isEmpty() == false || firstBatch) {\n+    private class OperationBatchSender extends MultiChunkTransfer<Translog.Snapshot, OperationChunkRequest> {\n+        private final long startingSeqNo;\n+        private final long endingSeqNo;\n+        private final Translog.Snapshot snapshot;\n+        private final long maxSeenAutoIdTimestamp;\n+        private final long maxSeqNoOfUpdatesOrDeletes;\n+        private final RetentionLeases retentionLeases;\n+        private final long mappingVersion;\n+        private int lastBatchCount = 0; // used to estimate the count of the subsequent batch.\n+        private final AtomicInteger skippedOps = new AtomicInteger();\n+        private final AtomicInteger sentOps = new AtomicInteger();\n+        private final AtomicLong targetLocalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n+\n+        OperationBatchSender(long startingSeqNo, long endingSeqNo, Translog.Snapshot snapshot, long maxSeenAutoIdTimestamp,\n+                             long maxSeqNoOfUpdatesOrDeletes, RetentionLeases retentionLeases, long mappingVersion,\n+                             ActionListener<Void> listener) {\n+            super(logger, threadPool.getThreadContext(), listener, maxConcurrentFileChunks, List.of(snapshot));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTMxMTU5NA=="}, "originalCommit": {"oid": "7707c3318111b53b310ce782295d049b06a98d71"}, "originalPosition": 160}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQzNjk4Nw==", "bodyText": "\ud83e\udd26 right, sorry for the noise :) My memory on this class betrayed me.", "url": "https://github.com/elastic/elasticsearch/pull/58018#discussion_r439436987", "createdAt": "2020-06-12T14:00:48Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java", "diffHunk": "@@ -661,106 +655,122 @@ void phase2(\n             throw new IndexShardClosedException(request.shardId());\n         }\n         logger.trace(\"recovery [phase2]: sending transaction log operations (from [\" + startingSeqNo + \"] to [\" + endingSeqNo + \"]\");\n-\n-        final AtomicInteger skippedOps = new AtomicInteger();\n-        final AtomicInteger totalSentOps = new AtomicInteger();\n-        final AtomicInteger lastBatchCount = new AtomicInteger(); // used to estimate the count of the subsequent batch.\n-        final CheckedSupplier<List<Translog.Operation>, IOException> readNextBatch = () -> {\n-            // We need to synchronized Snapshot#next() because it's called by different threads through sendBatch.\n-            // Even though those calls are not concurrent, Snapshot#next() uses non-synchronized state and is not multi-thread-compatible.\n-            synchronized (snapshot) {\n-                final List<Translog.Operation> ops = lastBatchCount.get() > 0 ? new ArrayList<>(lastBatchCount.get()) : new ArrayList<>();\n-                long batchSizeInBytes = 0L;\n-                Translog.Operation operation;\n-                while ((operation = snapshot.next()) != null) {\n-                    if (shard.state() == IndexShardState.CLOSED) {\n-                        throw new IndexShardClosedException(request.shardId());\n-                    }\n-                    cancellableThreads.checkForCancel();\n-                    final long seqNo = operation.seqNo();\n-                    if (seqNo < startingSeqNo || seqNo > endingSeqNo) {\n-                        skippedOps.incrementAndGet();\n-                        continue;\n-                    }\n-                    ops.add(operation);\n-                    batchSizeInBytes += operation.estimateSize();\n-                    totalSentOps.incrementAndGet();\n-\n-                    // check if this request is past bytes threshold, and if so, send it off\n-                    if (batchSizeInBytes >= chunkSizeInBytes) {\n-                        break;\n-                    }\n-                }\n-                lastBatchCount.set(ops.size());\n-                return ops;\n-            }\n-        };\n-\n         final StopWatch stopWatch = new StopWatch().start();\n-        final ActionListener<Long> batchedListener = ActionListener.map(listener,\n-            targetLocalCheckpoint -> {\n-                assert snapshot.totalOperations() == snapshot.skippedOperations() + skippedOps.get() + totalSentOps.get()\n+        final StepListener<Void> sendListener = new StepListener<>();\n+        final OperationBatchSender sender = new OperationBatchSender(startingSeqNo, endingSeqNo, snapshot, maxSeenAutoIdTimestamp,\n+            maxSeqNoOfUpdatesOrDeletes, retentionLeases, mappingVersion, sendListener);\n+        sendListener.whenComplete(\n+            ignored -> {\n+                final long skippedOps = sender.skippedOps.get();\n+                final int totalSentOps = sender.sentOps.get();\n+                final long targetLocalCheckpoint = sender.targetLocalCheckpoint.get();\n+                assert snapshot.totalOperations() == snapshot.skippedOperations() + skippedOps + totalSentOps\n                     : String.format(Locale.ROOT, \"expected total [%d], overridden [%d], skipped [%d], total sent [%d]\",\n-                    snapshot.totalOperations(), snapshot.skippedOperations(), skippedOps.get(), totalSentOps.get());\n+                    snapshot.totalOperations(), snapshot.skippedOperations(), skippedOps, totalSentOps);\n                 stopWatch.stop();\n                 final TimeValue tookTime = stopWatch.totalTime();\n                 logger.trace(\"recovery [phase2]: took [{}]\", tookTime);\n-                return new SendSnapshotResult(targetLocalCheckpoint, totalSentOps.get(), tookTime);\n-            }\n-        );\n+                listener.onResponse(new SendSnapshotResult(targetLocalCheckpoint, totalSentOps, tookTime));\n+            }, listener::onFailure);\n+        sender.start();\n+    }\n \n-        sendBatch(\n-                readNextBatch,\n-                true,\n-                SequenceNumbers.UNASSIGNED_SEQ_NO,\n-                snapshot.totalOperations(),\n-                maxSeenAutoIdTimestamp,\n-                maxSeqNoOfUpdatesOrDeletes,\n-                retentionLeases,\n-                mappingVersion,\n-                batchedListener);\n+    private static class OperationChunkRequest implements MultiChunkTransfer.ChunkRequest {\n+        final List<Translog.Operation> operations;\n+        final boolean lastChunk;\n+\n+        OperationChunkRequest(List<Translog.Operation> operations, boolean lastChunk) {\n+            this.operations = operations;\n+            this.lastChunk = lastChunk;\n+        }\n+\n+        @Override\n+        public boolean lastChunk() {\n+            return lastChunk;\n+        }\n     }\n \n-    private void sendBatch(\n-            final CheckedSupplier<List<Translog.Operation>, IOException> nextBatch,\n-            final boolean firstBatch,\n-            final long targetLocalCheckpoint,\n-            final int totalTranslogOps,\n-            final long maxSeenAutoIdTimestamp,\n-            final long maxSeqNoOfUpdatesOrDeletes,\n-            final RetentionLeases retentionLeases,\n-            final long mappingVersionOnPrimary,\n-            final ActionListener<Long> listener) throws IOException {\n-        assert ThreadPool.assertCurrentMethodIsNotCalledRecursively();\n-        assert Transports.assertNotTransportThread(RecoverySourceHandler.this + \"[send translog]\");\n-        final List<Translog.Operation> operations = nextBatch.get();\n-        // send the leftover operations or if no operations were sent, request the target to respond with its local checkpoint\n-        if (operations.isEmpty() == false || firstBatch) {\n+    private class OperationBatchSender extends MultiChunkTransfer<Translog.Snapshot, OperationChunkRequest> {\n+        private final long startingSeqNo;\n+        private final long endingSeqNo;\n+        private final Translog.Snapshot snapshot;\n+        private final long maxSeenAutoIdTimestamp;\n+        private final long maxSeqNoOfUpdatesOrDeletes;\n+        private final RetentionLeases retentionLeases;\n+        private final long mappingVersion;\n+        private int lastBatchCount = 0; // used to estimate the count of the subsequent batch.\n+        private final AtomicInteger skippedOps = new AtomicInteger();\n+        private final AtomicInteger sentOps = new AtomicInteger();\n+        private final AtomicLong targetLocalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n+\n+        OperationBatchSender(long startingSeqNo, long endingSeqNo, Translog.Snapshot snapshot, long maxSeenAutoIdTimestamp,\n+                             long maxSeqNoOfUpdatesOrDeletes, RetentionLeases retentionLeases, long mappingVersion,\n+                             ActionListener<Void> listener) {\n+            super(logger, threadPool.getThreadContext(), listener, maxConcurrentFileChunks, List.of(snapshot));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTMxMTU5NA=="}, "originalCommit": {"oid": "7707c3318111b53b310ce782295d049b06a98d71"}, "originalPosition": 160}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgwODQ4MzE3OnYy", "diffSide": "RIGHT", "path": "docs/reference/modules/indices/recovery.asciidoc", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNlQyMDo0NjoyMFrOGtmjMA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wN1QxNDo0NzozN1rOGuCIeQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDQ3MDcwNA==", "bodyText": "Maybe reword this a little to something like (though maybe @DaveCTurner can use his native speaker powers here to help with wording :)):\nIncrease the value of this setting with care as higher values will cause increased CPU use, which might negatively impact indexing and search activities.", "url": "https://github.com/elastic/elasticsearch/pull/58018#discussion_r450470704", "createdAt": "2020-07-06T20:46:20Z", "author": {"login": "original-brownbear"}, "path": "docs/reference/modules/indices/recovery.asciidoc", "diffHunk": "@@ -48,3 +48,11 @@ sent in parallel for each recovery. Defaults to `2`.\n +\n You can increase the value of this setting when the recovery of a single shard\n is not reaching the traffic limit set by `indices.recovery.max_bytes_per_sec`.\n+\n+`indices.recovery.max_concurrent_operation_chunks`::\n+(<<cluster-update-settings,Dynamic>>, Expert) Number of operation chunk requests\n+sent in parallel for each recovery. Defaults to `1`.\n++\n+Increasing the value of this setting with care as it can cause recoveries to consume", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f0c2d713e69f2616c5b81749f03b8d676f411552"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDQ4MzYzMA==", "bodyText": "though maybe @DaveCTurner can use his native speaker powers here to help with wording\n\n++", "url": "https://github.com/elastic/elasticsearch/pull/58018#discussion_r450483630", "createdAt": "2020-07-06T21:14:42Z", "author": {"login": "dnhatn"}, "path": "docs/reference/modules/indices/recovery.asciidoc", "diffHunk": "@@ -48,3 +48,11 @@ sent in parallel for each recovery. Defaults to `2`.\n +\n You can increase the value of this setting when the recovery of a single shard\n is not reaching the traffic limit set by `indices.recovery.max_bytes_per_sec`.\n+\n+`indices.recovery.max_concurrent_operation_chunks`::\n+(<<cluster-update-settings,Dynamic>>, Expert) Number of operation chunk requests\n+sent in parallel for each recovery. Defaults to `1`.\n++\n+Increasing the value of this setting with care as it can cause recoveries to consume", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDQ3MDcwNA=="}, "originalCommit": {"oid": "f0c2d713e69f2616c5b81749f03b8d676f411552"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDY0ODYzMA==", "bodyText": "Sure, how about this?\nConcurrently replaying operations during recovery can be very\nresource-intensive and may interfere with indexing, search, and other\nactivities in your cluster. Do not increase this setting without carefully\nverifying that your cluster has the resources available to handle the extra\nload that will result.", "url": "https://github.com/elastic/elasticsearch/pull/58018#discussion_r450648630", "createdAt": "2020-07-07T06:56:41Z", "author": {"login": "DaveCTurner"}, "path": "docs/reference/modules/indices/recovery.asciidoc", "diffHunk": "@@ -48,3 +48,11 @@ sent in parallel for each recovery. Defaults to `2`.\n +\n You can increase the value of this setting when the recovery of a single shard\n is not reaching the traffic limit set by `indices.recovery.max_bytes_per_sec`.\n+\n+`indices.recovery.max_concurrent_operation_chunks`::\n+(<<cluster-update-settings,Dynamic>>, Expert) Number of operation chunk requests\n+sent in parallel for each recovery. Defaults to `1`.\n++\n+Increasing the value of this setting with care as it can cause recoveries to consume", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDQ3MDcwNA=="}, "originalCommit": {"oid": "f0c2d713e69f2616c5b81749f03b8d676f411552"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDkyMjYxNw==", "bodyText": "Thanks David. Applied in e4f1fbf.", "url": "https://github.com/elastic/elasticsearch/pull/58018#discussion_r450922617", "createdAt": "2020-07-07T14:47:37Z", "author": {"login": "dnhatn"}, "path": "docs/reference/modules/indices/recovery.asciidoc", "diffHunk": "@@ -48,3 +48,11 @@ sent in parallel for each recovery. Defaults to `2`.\n +\n You can increase the value of this setting when the recovery of a single shard\n is not reaching the traffic limit set by `indices.recovery.max_bytes_per_sec`.\n+\n+`indices.recovery.max_concurrent_operation_chunks`::\n+(<<cluster-update-settings,Dynamic>>, Expert) Number of operation chunk requests\n+sent in parallel for each recovery. Defaults to `1`.\n++\n+Increasing the value of this setting with care as it can cause recoveries to consume", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDQ3MDcwNA=="}, "originalCommit": {"oid": "f0c2d713e69f2616c5b81749f03b8d676f411552"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgwOTU5OTg4OnYy", "diffSide": "RIGHT", "path": "docs/reference/modules/indices/recovery.asciidoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wN1QwNjo0MTozNlrOGtxCgQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wN1QxNDo0NzowNlrOGuCHBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDY0MjU2MQ==", "bodyText": "Suggest simply indices.recovery.max_concurrent_operations; the fact that operations are sent in chunks (i.e. batches) is a bit too much detail for users here IMO.", "url": "https://github.com/elastic/elasticsearch/pull/58018#discussion_r450642561", "createdAt": "2020-07-07T06:41:36Z", "author": {"login": "DaveCTurner"}, "path": "docs/reference/modules/indices/recovery.asciidoc", "diffHunk": "@@ -48,3 +48,11 @@ sent in parallel for each recovery. Defaults to `2`.\n +\n You can increase the value of this setting when the recovery of a single shard\n is not reaching the traffic limit set by `indices.recovery.max_bytes_per_sec`.\n+\n+`indices.recovery.max_concurrent_operation_chunks`::", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f0c2d713e69f2616c5b81749f03b8d676f411552"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDkyMjI0Nw==", "bodyText": "++, see e4f1fbf.", "url": "https://github.com/elastic/elasticsearch/pull/58018#discussion_r450922247", "createdAt": "2020-07-07T14:47:06Z", "author": {"login": "dnhatn"}, "path": "docs/reference/modules/indices/recovery.asciidoc", "diffHunk": "@@ -48,3 +48,11 @@ sent in parallel for each recovery. Defaults to `2`.\n +\n You can increase the value of this setting when the recovery of a single shard\n is not reaching the traffic limit set by `indices.recovery.max_bytes_per_sec`.\n+\n+`indices.recovery.max_concurrent_operation_chunks`::", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDY0MjU2MQ=="}, "originalCommit": {"oid": "f0c2d713e69f2616c5b81749f03b8d676f411552"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgwOTYxMzQ0OnYy", "diffSide": "RIGHT", "path": "docs/reference/modules/indices/recovery.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wN1QwNjo0NzoxNFrOGtxLDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wN1QwNjo0NzoxNFrOGtxLDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDY0NDc0OQ==", "bodyText": "Similarly:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            (<<cluster-update-settings,Dynamic>>, Expert) Number of operation chunk requests\n          \n          \n            \n            (<<cluster-update-settings,Dynamic>>, Expert) Number of operations", "url": "https://github.com/elastic/elasticsearch/pull/58018#discussion_r450644749", "createdAt": "2020-07-07T06:47:14Z", "author": {"login": "DaveCTurner"}, "path": "docs/reference/modules/indices/recovery.asciidoc", "diffHunk": "@@ -48,3 +48,11 @@ sent in parallel for each recovery. Defaults to `2`.\n +\n You can increase the value of this setting when the recovery of a single shard\n is not reaching the traffic limit set by `indices.recovery.max_bytes_per_sec`.\n+\n+`indices.recovery.max_concurrent_operation_chunks`::\n+(<<cluster-update-settings,Dynamic>>, Expert) Number of operation chunk requests", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f0c2d713e69f2616c5b81749f03b8d676f411552"}, "originalPosition": 6}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1569, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}