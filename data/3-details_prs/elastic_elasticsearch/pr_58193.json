{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM1NDM2OTY2", "number": 58193, "title": "Pipeline Inference Aggregation", "bodyText": "A pipeline aggregation that loads a model and performs inference on the input aggregation results\nConfiguration\nTakes the standard inference_config options  but inference_config is optional in which case the model defaults are used.\n\"classification_agg\": {.    <-- agg name\n    \"inference\": {                <-- this is an inference aggregation\n    \t\"model_id\": \"telco_churn\",   \n        \"inference_config\": {.    <-- optional\n        .....\n        },\n    \t\"buckets_path\": {   \n      \t\t\"account_length\": \"account_length.value\",\n      \t\t\"avg_price\": \"avg_price_agg\"\n     \t....\n    \t}\n    }\n}\n\nbuckets_path serves 2 purposes, it defines the paths to the input aggregations and functions as a field_map when fields have to be renamed to match the model's expected input.\nDetecting Configuration Errors\nA missing field does not necessarily mean the config is incorrect so we will leniently accept that but in some cases a typo will mean a field is missing giving consistently poor results. There are ways to detect mis-configuration:\n\nBucket paths must be valid paths to the aggregations. If not an error is thrown.\nThe element pointed to by the bucket path must be either something that resolves to a number (Number or InternalNumericMetricsAggregation.SingleValue) or a String (String or StringTerms.Bucket)\n\nAccessing script values in the buckets_path\nThe value of most aggregations can be accessed just with the agg name e.g. avg_price_agg but for scripted_metric aggregations value has to be appended to the agg name: scripted_metric_agg.value.\nGap Policy\nOf the 2 possible gap policies only SKIP is valid as INSERT_ZEROS changes the data in a way that did not occur during training.\nIf the bucket hit zero documents (_count == 0) the the bucket is skipped unless the _count field is one of the model's inputs.\nOutput Controversy\nEDIT: There is no controversy we will follow aggregations example and put the regression/classification result in the value field.\nIt is a validation error to specify a different results_field in the config.\nSimilarly the top_classes field cannot be changed from the default\nAggregations usually put the result in the value field but there are exceptions e.g. stats\n\"avg_cost_agg\" : {\n    \"value\" : 165.0\n },\n\nThe ml inference config has a result_field setting which is where we expect the results to appear. However the fact that this field name can change makes it difficult to predict where the result will be when parsing.\nI've chosen to respect the inference config rather than follow the aggregations convention but the price we pay is that parsing the results is very difficult unless you know what the result field is called.\nA config like\n\"regression_agg\": {\n  \"inference\": {\n    \"model_id\": \"foo\",\n    \"inference_config\": {\n      \"regression\": {\n        \"results_field\": \"ml.results\"      <-- the important part\n      }\n    },\n    \"buckets_path\": {\n      \"avg_cost\": \"avg_cost_agg\"\n    }\n  }\n}\n\nWill write a ml.results field inside each bucket rather than value.\n\"regression_agg\" : {\n    \"ml.results\" : 2.0\n}\n\nThis format is not parsable by a static parser because in the format {results_field}: {value} the name of {results_field} is unknown during parsing. Deserialising this format is tricky and has not been implemented yet (the HLRC will required it) but for the sake of readability it is preferable to the ungainly alternative:\n   \"results_field\": \"ml.results\",\n   \"results_value\": 2.0\n}", "createdAt": "2020-06-16T20:00:15Z", "url": "https://github.com/elastic/elasticsearch/pull/58193", "merged": true, "mergeCommit": {"oid": "7daed3b8af5fce3b62e5f9fd9aae0b0b712d72d7"}, "closed": true, "closedAt": "2020-07-02T13:33:03Z", "author": {"login": "davidkyle"}, "timelineItems": {"totalCount": 65, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcr7b9lABqjM0NTA3ODAyMTU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcw8uXuABqjM1MDY1NTcxMjQ=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7156aa711ad12ac15de8ea06ce65efb4d0f10c4e", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/7156aa711ad12ac15de8ea06ce65efb4d0f10c4e", "committedDate": "2020-06-16T18:51:51Z", "message": "precommit"}, "afterCommit": {"oid": "aaedaed32d258431609359baf340cad96c31da73", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/aaedaed32d258431609359baf340cad96c31da73", "committedDate": "2020-06-16T20:39:27Z", "message": "Fix hashcode and tidy up"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6c293d0053f9d5746117483b6176d365f960ec0e", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/6c293d0053f9d5746117483b6176d365f960ec0e", "committedDate": "2020-06-16T20:47:36Z", "message": "checkstyle"}, "afterCommit": {"oid": "e5c2cbc07e9bff54eaa8065f995339e654b60226", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/e5c2cbc07e9bff54eaa8065f995339e654b60226", "committedDate": "2020-06-17T09:55:40Z", "message": "black list rest tests from security tests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMyMzI2Nzcz", "url": "https://github.com/elastic/elasticsearch/pull/58193#pullrequestreview-432326773", "createdAt": "2020-06-17T11:57:08Z", "commit": {"oid": "e5c2cbc07e9bff54eaa8065f995339e654b60226"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxMTo1NzowOFrOGlCY_A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxMjoxNDo1OFrOGlC81Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ4OTY2MA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    return (o == null || getClass() != o.getClass()) == false;\n          \n          \n            \n                    return o != null && getClass() == o.getClass();", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r441489660", "createdAt": "2020-06-17T11:57:08Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ml/inference/trainedmodel/EmptyConfigUpdate.java", "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.ml.inference.trainedmodel;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+\n+import java.io.IOException;\n+\n+/**\n+ * A config update that applies no changes.\n+ * Supports any type of {@link InferenceConfig}\n+ */\n+public class EmptyConfigUpdate implements InferenceConfigUpdate {\n+\n+    public static final ParseField NAME = new ParseField(\"empty\");\n+\n+    private static final ObjectParser<EmptyConfigUpdate, Void> PARSER =\n+        new ObjectParser<>(NAME.getPreferredName(), EmptyConfigUpdate::new);\n+\n+    public static EmptyConfigUpdate fromXContent(XContentParser parser) {\n+        return PARSER.apply(parser, null);\n+    }\n+\n+    public EmptyConfigUpdate() {\n+    }\n+\n+    public EmptyConfigUpdate(StreamInput in) {\n+    }\n+\n+    @Override\n+    public InferenceConfig apply(InferenceConfig originalConfig) {\n+        return originalConfig;\n+    }\n+\n+    @Override\n+    public InferenceConfig toConfig() {\n+        return RegressionConfig.EMPTY_PARAMS;\n+    }\n+\n+    @Override\n+    public boolean isSupported(InferenceConfig config) {\n+        return true;\n+    }\n+\n+    @Override\n+    public String getWriteableName() {\n+        return NAME.getPreferredName();\n+    }\n+\n+    @Override\n+    public void writeTo(StreamOutput out) throws IOException {\n+\n+    }\n+\n+    @Override\n+    public String getName() {\n+        return NAME.getPreferredName();\n+    }\n+\n+    @Override\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        builder.startObject();\n+        builder.endObject();\n+        return builder;\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+        // Equal if o is not null and the same class\n+        return (o == null || getClass() != o.getClass()) == false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5c2cbc07e9bff54eaa8065f995339e654b60226"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ5MTQ4MA==", "bodyText": "None of these are expecting errors in their yaml tests. Why are they part of the blacklist?", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r441491480", "createdAt": "2020-06-17T12:00:33Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/ml/qa/ml-with-security/build.gradle", "diffHunk": "@@ -183,6 +183,9 @@ integTest.runner {\n     'ml/job_groups/Test put job with id that matches an existing group',\n     'ml/job_groups/Test put job with invalid group',\n     'ml/ml_info/Test ml info',\n+    'ml/pipeline_inference/Test pipeline regression simple',\n+    'ml/pipeline_inference/Test pipeline agg referencing a single bucket',\n+    'ml/pipeline_inference/Test all fields missing warning',", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5c2cbc07e9bff54eaa8065f995339e654b60226"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ5NzI2NA==", "bodyText": "Loading the model could take seconds to finish. Depending on the threadpool/context this is bad.\nThis needs to be addressed before merge.\nA potential solution to this would have pipeline aggs satisfy some sort of \"rewrite\" phase. This way the context has time to make an asynchronous call before executing.", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r441497264", "createdAt": "2020-06-17T12:11:56Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InferencePipelineAggregationBuilder.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.inference.aggs;\n+\n+import org.apache.lucene.util.SetOnce;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.LatchedActionListener;\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.ObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ClassificationConfig;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ClassificationConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.EmptyConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.InferenceConfig;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.InferenceConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.LenientlyParsedInferenceConfig;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.RegressionConfig;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.RegressionConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.utils.ExceptionsHelper;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.Model;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.ModelLoadingService;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.CountDownLatch;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.constructorArg;\n+import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregator.Parser.GAP_POLICY;\n+\n+public class InferencePipelineAggregationBuilder extends AbstractPipelineAggregationBuilder<InferencePipelineAggregationBuilder> {\n+\n+    public static String NAME = \"inference\";\n+\n+    public static final ParseField MODEL_ID = new ParseField(\"model_id\");\n+    private static final ParseField INFERENCE_CONFIG = new ParseField(\"inference_config\");\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private static final ConstructingObjectParser<InferencePipelineAggregationBuilder,\n+        Tuple<SetOnce<ModelLoadingService>, String>> PARSER = new ConstructingObjectParser<>(\n+        NAME, false,\n+        (args, context) -> new InferencePipelineAggregationBuilder(context.v2(), context.v1(), (Map<String, String>) args[0])\n+    );\n+\n+    static {\n+        PARSER.declareObject(constructorArg(), (p, c) -> p.mapStrings(), BUCKETS_PATH_FIELD);\n+        PARSER.declareString(InferencePipelineAggregationBuilder::setModelId, MODEL_ID);\n+        PARSER.declareNamedObject(InferencePipelineAggregationBuilder::setInferenceConfig,\n+            (p, c, n) -> p.namedObject(LenientlyParsedInferenceConfig.class, n, c), INFERENCE_CONFIG);\n+        PARSER.declareField(InferencePipelineAggregationBuilder::setGapPolicy, p -> {\n+            if (p.currentToken() == XContentParser.Token.VALUE_STRING) {\n+                return BucketHelpers.GapPolicy.parse(p.text().toLowerCase(Locale.ROOT), p.getTokenLocation());\n+            }\n+            throw new IllegalArgumentException(\n+                \"Unsupported token [\" + p.currentToken() + \"] parsing inference aggregation \" + GAP_POLICY.getPreferredName());\n+        }, GAP_POLICY, ObjectParser.ValueType.STRING);\n+    }\n+\n+    private final Map<String, String> bucketPathMap;\n+    private String modelId;\n+    private InferenceConfig inferenceConfig;\n+    private final SetOnce<ModelLoadingService> modelLoadingService;\n+    private BucketHelpers.GapPolicy gapPolicy = BucketHelpers.GapPolicy.SKIP;\n+\n+    public static InferencePipelineAggregationBuilder parse(SetOnce<ModelLoadingService> modelLoadingService,\n+                                                            String pipelineAggregatorName,\n+                                                            XContentParser parser) {\n+        Tuple<SetOnce<ModelLoadingService>, String> context = new Tuple<>(modelLoadingService, pipelineAggregatorName);\n+        return PARSER.apply(parser, context);\n+    }\n+\n+    public InferencePipelineAggregationBuilder(String name, SetOnce<ModelLoadingService> modelLoadingService,\n+                                               Map<String, String> bucketsPath) {\n+        super(name, NAME, bucketsPath.values().toArray(new String[] {}));\n+        this.modelLoadingService = modelLoadingService;\n+        this.bucketPathMap = bucketsPath;\n+    }\n+\n+    public InferencePipelineAggregationBuilder(StreamInput in, SetOnce<ModelLoadingService> modelLoadingService) throws IOException {\n+        super(in, NAME);\n+        modelId = in.readString();\n+        bucketPathMap = in.readMap(StreamInput::readString, StreamInput::readString);\n+        inferenceConfig = in.readOptionalNamedWriteable(InferenceConfig.class);\n+        gapPolicy = BucketHelpers.GapPolicy.readFrom(in);\n+        this.modelLoadingService = modelLoadingService;\n+    }\n+\n+    void setModelId(String modelId) {\n+        this.modelId = modelId;\n+    }\n+\n+    void setInferenceConfig(InferenceConfig inferenceConfig) {\n+        this.inferenceConfig = inferenceConfig;\n+    }\n+\n+    void setGapPolicy(BucketHelpers.GapPolicy gapPolicy) {\n+        this.gapPolicy = gapPolicy;\n+    }\n+\n+    @Override\n+    protected void validate(ValidationContext context) {\n+        context.validateHasParent(NAME, name);\n+        if (modelId == null) {\n+            context.addValidationError(\"Model Id must be set\");\n+        }\n+        if (gapPolicy != BucketHelpers.GapPolicy.SKIP) {\n+            context.addValidationError(\"gap policy [\" + gapPolicy + \"] in not valid for [\" + NAME + \"] aggregation\");\n+        }\n+    }\n+\n+    @Override\n+    protected void doWriteTo(StreamOutput out) throws IOException {\n+        out.writeString(modelId);\n+        out.writeMap(bucketPathMap, StreamOutput::writeString, StreamOutput::writeString);\n+        out.writeOptionalNamedWriteable(inferenceConfig);\n+        gapPolicy.writeTo(out);\n+    }\n+\n+    @Override\n+    protected PipelineAggregator createInternal(Map<String, Object> metaData) {\n+\n+        SetOnce<Model> model = new SetOnce<>();\n+        SetOnce<Exception> error = new SetOnce<>();\n+        CountDownLatch latch = new CountDownLatch(1);\n+        ActionListener<Model> listener = new LatchedActionListener<>(\n+            ActionListener.wrap(model::set, error::set), latch);\n+\n+        modelLoadingService.get().getModelForSearch(modelId, listener);\n+        try {\n+            // TODO Avoid the blocking wait\n+            latch.await();\n+        } catch (InterruptedException e) {\n+            throw new RuntimeException(\"Inference aggregation interrupted loading model\", e);\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5c2cbc07e9bff54eaa8065f995339e654b60226"}, "originalPosition": 148}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ5ODgzNw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public InferencePipelineAggregator(String name, Map<String,\n          \n          \n            \n                                                   String> bucketPathMap, Map<String, Object> metaData,\n          \n          \n            \n                public InferencePipelineAggregator(String name, \n          \n          \n            \n                                                   Map<String, String> bucketPathMap, \n          \n          \n            \n                                                   Map<String, Object> metaData,", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r441498837", "createdAt": "2020-06-17T12:14:58Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InferencePipelineAggregator.java", "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.inference.aggs;\n+\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.search.aggregations.AggregationExecutionException;\n+import org.elasticsearch.search.aggregations.InternalAggregation;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;\n+import org.elasticsearch.search.aggregations.bucket.MultiBucketsAggregation;\n+import org.elasticsearch.search.aggregations.bucket.terms.StringTerms;\n+import org.elasticsearch.search.aggregations.metrics.InternalNumericMetricsAggregation;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+import org.elasticsearch.search.aggregations.support.AggregationPath;\n+import org.elasticsearch.xpack.core.ml.inference.results.InferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.results.WarningInferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.InferenceConfigUpdate;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.Model;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+\n+\n+public class InferencePipelineAggregator extends PipelineAggregator {\n+\n+    private Map<String, String> bucketPathMap;\n+    private InferenceConfigUpdate configUpdate;\n+    private Model model;\n+\n+    public InferencePipelineAggregator(String name, Map<String,\n+                                       String> bucketPathMap, Map<String, Object> metaData,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5c2cbc07e9bff54eaa8065f995339e654b60226"}, "originalPosition": 40}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMzMTI0MzEw", "url": "https://github.com/elastic/elasticsearch/pull/58193#pullrequestreview-433124310", "createdAt": "2020-06-18T09:54:07Z", "commit": {"oid": "c47d42d5c91ef55490abff9683f7a7c0b1bdde0c"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQwOTo1NDowN1rOGloLgg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQwOTo1NjoyOVrOGloREQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjEwODgwMg==", "bodyText": "I agree with what Ben said about the need to remove the latch await, and if that is done there will hopefully be no need to catch InterruptedException here.  However, if we do still have to catch it, the handler should interrupt the current thread before throwing a different type of exception, because the class that owns the thread (e.g. a thread pool) may be relying on the interrupted status to decide what to do with the thread.", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r442108802", "createdAt": "2020-06-18T09:54:07Z", "author": {"login": "droberts195"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InferencePipelineAggregationBuilder.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.inference.aggs;\n+\n+import org.apache.lucene.util.SetOnce;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.LatchedActionListener;\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.ObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ClassificationConfig;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ClassificationConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.EmptyConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.InferenceConfig;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.InferenceConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.LenientlyParsedInferenceConfig;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.RegressionConfig;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.RegressionConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.utils.ExceptionsHelper;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.Model;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.ModelLoadingService;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.CountDownLatch;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.constructorArg;\n+import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregator.Parser.GAP_POLICY;\n+\n+public class InferencePipelineAggregationBuilder extends AbstractPipelineAggregationBuilder<InferencePipelineAggregationBuilder> {\n+\n+    public static String NAME = \"inference\";\n+\n+    public static final ParseField MODEL_ID = new ParseField(\"model_id\");\n+    private static final ParseField INFERENCE_CONFIG = new ParseField(\"inference_config\");\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private static final ConstructingObjectParser<InferencePipelineAggregationBuilder,\n+        Tuple<SetOnce<ModelLoadingService>, String>> PARSER = new ConstructingObjectParser<>(\n+        NAME, false,\n+        (args, context) -> new InferencePipelineAggregationBuilder(context.v2(), context.v1(), (Map<String, String>) args[0])\n+    );\n+\n+    static {\n+        PARSER.declareObject(constructorArg(), (p, c) -> p.mapStrings(), BUCKETS_PATH_FIELD);\n+        PARSER.declareString(InferencePipelineAggregationBuilder::setModelId, MODEL_ID);\n+        PARSER.declareNamedObject(InferencePipelineAggregationBuilder::setInferenceConfig,\n+            (p, c, n) -> p.namedObject(LenientlyParsedInferenceConfig.class, n, c), INFERENCE_CONFIG);\n+        PARSER.declareField(InferencePipelineAggregationBuilder::setGapPolicy, p -> {\n+            if (p.currentToken() == XContentParser.Token.VALUE_STRING) {\n+                return BucketHelpers.GapPolicy.parse(p.text().toLowerCase(Locale.ROOT), p.getTokenLocation());\n+            }\n+            throw new IllegalArgumentException(\n+                \"Unsupported token [\" + p.currentToken() + \"] parsing inference aggregation \" + GAP_POLICY.getPreferredName());\n+        }, GAP_POLICY, ObjectParser.ValueType.STRING);\n+    }\n+\n+    private final Map<String, String> bucketPathMap;\n+    private String modelId;\n+    private InferenceConfig inferenceConfig;\n+    private final SetOnce<ModelLoadingService> modelLoadingService;\n+    private BucketHelpers.GapPolicy gapPolicy = BucketHelpers.GapPolicy.SKIP;\n+\n+    public static InferencePipelineAggregationBuilder parse(SetOnce<ModelLoadingService> modelLoadingService,\n+                                                            String pipelineAggregatorName,\n+                                                            XContentParser parser) {\n+        Tuple<SetOnce<ModelLoadingService>, String> context = new Tuple<>(modelLoadingService, pipelineAggregatorName);\n+        return PARSER.apply(parser, context);\n+    }\n+\n+    public InferencePipelineAggregationBuilder(String name, SetOnce<ModelLoadingService> modelLoadingService,\n+                                               Map<String, String> bucketsPath) {\n+        super(name, NAME, bucketsPath.values().toArray(new String[] {}));\n+        this.modelLoadingService = modelLoadingService;\n+        this.bucketPathMap = bucketsPath;\n+    }\n+\n+    public InferencePipelineAggregationBuilder(StreamInput in, SetOnce<ModelLoadingService> modelLoadingService) throws IOException {\n+        super(in, NAME);\n+        modelId = in.readString();\n+        bucketPathMap = in.readMap(StreamInput::readString, StreamInput::readString);\n+        inferenceConfig = in.readOptionalNamedWriteable(InferenceConfig.class);\n+        gapPolicy = BucketHelpers.GapPolicy.readFrom(in);\n+        this.modelLoadingService = modelLoadingService;\n+    }\n+\n+    void setModelId(String modelId) {\n+        this.modelId = modelId;\n+    }\n+\n+    void setInferenceConfig(InferenceConfig inferenceConfig) {\n+        this.inferenceConfig = inferenceConfig;\n+    }\n+\n+    void setGapPolicy(BucketHelpers.GapPolicy gapPolicy) {\n+        this.gapPolicy = gapPolicy;\n+    }\n+\n+    @Override\n+    protected void validate(ValidationContext context) {\n+        context.validateHasParent(NAME, name);\n+        if (modelId == null) {\n+            context.addValidationError(\"Model Id must be set\");\n+        }\n+        if (gapPolicy != BucketHelpers.GapPolicy.SKIP) {\n+            context.addValidationError(\"gap policy [\" + gapPolicy + \"] in not valid for [\" + NAME + \"] aggregation\");\n+        }\n+    }\n+\n+    @Override\n+    protected void doWriteTo(StreamOutput out) throws IOException {\n+        out.writeString(modelId);\n+        out.writeMap(bucketPathMap, StreamOutput::writeString, StreamOutput::writeString);\n+        out.writeOptionalNamedWriteable(inferenceConfig);\n+        gapPolicy.writeTo(out);\n+    }\n+\n+    @Override\n+    protected PipelineAggregator createInternal(Map<String, Object> metaData) {\n+\n+        SetOnce<Model> model = new SetOnce<>();\n+        SetOnce<Exception> error = new SetOnce<>();\n+        CountDownLatch latch = new CountDownLatch(1);\n+        ActionListener<Model> listener = new LatchedActionListener<>(\n+            ActionListener.wrap(model::set, error::set), latch);\n+\n+        modelLoadingService.get().getModelForSearch(modelId, listener);\n+        try {\n+            // TODO Avoid the blocking wait\n+            latch.await();\n+        } catch (InterruptedException e) {\n+            throw new RuntimeException(\"Inference aggregation interrupted loading model\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c47d42d5c91ef55490abff9683f7a7c0b1bdde0c"}, "originalPosition": 147}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjExMDIyNQ==", "bodyText": "Most exceptions used in Elasticsearch are already RuntimeExceptions.  It would be better if these weren't wrapped in a generic runtime exception, as that will look less friendly in error messages and especially if it ends up getting transported to a different node.  So it would be better to only wrap the exception if it's not already instanceof RuntimeException.", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r442110225", "createdAt": "2020-06-18T09:56:29Z", "author": {"login": "droberts195"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InferencePipelineAggregationBuilder.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.inference.aggs;\n+\n+import org.apache.lucene.util.SetOnce;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.LatchedActionListener;\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.ObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ClassificationConfig;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ClassificationConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.EmptyConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.InferenceConfig;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.InferenceConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.LenientlyParsedInferenceConfig;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.RegressionConfig;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.RegressionConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.utils.ExceptionsHelper;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.Model;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.ModelLoadingService;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.CountDownLatch;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.constructorArg;\n+import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregator.Parser.GAP_POLICY;\n+\n+public class InferencePipelineAggregationBuilder extends AbstractPipelineAggregationBuilder<InferencePipelineAggregationBuilder> {\n+\n+    public static String NAME = \"inference\";\n+\n+    public static final ParseField MODEL_ID = new ParseField(\"model_id\");\n+    private static final ParseField INFERENCE_CONFIG = new ParseField(\"inference_config\");\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private static final ConstructingObjectParser<InferencePipelineAggregationBuilder,\n+        Tuple<SetOnce<ModelLoadingService>, String>> PARSER = new ConstructingObjectParser<>(\n+        NAME, false,\n+        (args, context) -> new InferencePipelineAggregationBuilder(context.v2(), context.v1(), (Map<String, String>) args[0])\n+    );\n+\n+    static {\n+        PARSER.declareObject(constructorArg(), (p, c) -> p.mapStrings(), BUCKETS_PATH_FIELD);\n+        PARSER.declareString(InferencePipelineAggregationBuilder::setModelId, MODEL_ID);\n+        PARSER.declareNamedObject(InferencePipelineAggregationBuilder::setInferenceConfig,\n+            (p, c, n) -> p.namedObject(LenientlyParsedInferenceConfig.class, n, c), INFERENCE_CONFIG);\n+        PARSER.declareField(InferencePipelineAggregationBuilder::setGapPolicy, p -> {\n+            if (p.currentToken() == XContentParser.Token.VALUE_STRING) {\n+                return BucketHelpers.GapPolicy.parse(p.text().toLowerCase(Locale.ROOT), p.getTokenLocation());\n+            }\n+            throw new IllegalArgumentException(\n+                \"Unsupported token [\" + p.currentToken() + \"] parsing inference aggregation \" + GAP_POLICY.getPreferredName());\n+        }, GAP_POLICY, ObjectParser.ValueType.STRING);\n+    }\n+\n+    private final Map<String, String> bucketPathMap;\n+    private String modelId;\n+    private InferenceConfig inferenceConfig;\n+    private final SetOnce<ModelLoadingService> modelLoadingService;\n+    private BucketHelpers.GapPolicy gapPolicy = BucketHelpers.GapPolicy.SKIP;\n+\n+    public static InferencePipelineAggregationBuilder parse(SetOnce<ModelLoadingService> modelLoadingService,\n+                                                            String pipelineAggregatorName,\n+                                                            XContentParser parser) {\n+        Tuple<SetOnce<ModelLoadingService>, String> context = new Tuple<>(modelLoadingService, pipelineAggregatorName);\n+        return PARSER.apply(parser, context);\n+    }\n+\n+    public InferencePipelineAggregationBuilder(String name, SetOnce<ModelLoadingService> modelLoadingService,\n+                                               Map<String, String> bucketsPath) {\n+        super(name, NAME, bucketsPath.values().toArray(new String[] {}));\n+        this.modelLoadingService = modelLoadingService;\n+        this.bucketPathMap = bucketsPath;\n+    }\n+\n+    public InferencePipelineAggregationBuilder(StreamInput in, SetOnce<ModelLoadingService> modelLoadingService) throws IOException {\n+        super(in, NAME);\n+        modelId = in.readString();\n+        bucketPathMap = in.readMap(StreamInput::readString, StreamInput::readString);\n+        inferenceConfig = in.readOptionalNamedWriteable(InferenceConfig.class);\n+        gapPolicy = BucketHelpers.GapPolicy.readFrom(in);\n+        this.modelLoadingService = modelLoadingService;\n+    }\n+\n+    void setModelId(String modelId) {\n+        this.modelId = modelId;\n+    }\n+\n+    void setInferenceConfig(InferenceConfig inferenceConfig) {\n+        this.inferenceConfig = inferenceConfig;\n+    }\n+\n+    void setGapPolicy(BucketHelpers.GapPolicy gapPolicy) {\n+        this.gapPolicy = gapPolicy;\n+    }\n+\n+    @Override\n+    protected void validate(ValidationContext context) {\n+        context.validateHasParent(NAME, name);\n+        if (modelId == null) {\n+            context.addValidationError(\"Model Id must be set\");\n+        }\n+        if (gapPolicy != BucketHelpers.GapPolicy.SKIP) {\n+            context.addValidationError(\"gap policy [\" + gapPolicy + \"] in not valid for [\" + NAME + \"] aggregation\");\n+        }\n+    }\n+\n+    @Override\n+    protected void doWriteTo(StreamOutput out) throws IOException {\n+        out.writeString(modelId);\n+        out.writeMap(bucketPathMap, StreamOutput::writeString, StreamOutput::writeString);\n+        out.writeOptionalNamedWriteable(inferenceConfig);\n+        gapPolicy.writeTo(out);\n+    }\n+\n+    @Override\n+    protected PipelineAggregator createInternal(Map<String, Object> metaData) {\n+\n+        SetOnce<Model> model = new SetOnce<>();\n+        SetOnce<Exception> error = new SetOnce<>();\n+        CountDownLatch latch = new CountDownLatch(1);\n+        ActionListener<Model> listener = new LatchedActionListener<>(\n+            ActionListener.wrap(model::set, error::set), latch);\n+\n+        modelLoadingService.get().getModelForSearch(modelId, listener);\n+        try {\n+            // TODO Avoid the blocking wait\n+            latch.await();\n+        } catch (InterruptedException e) {\n+            throw new RuntimeException(\"Inference aggregation interrupted loading model\", e);\n+        }\n+\n+        if (error.get() != null) {\n+            throw new RuntimeException(error.get());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c47d42d5c91ef55490abff9683f7a7c0b1bdde0c"}, "originalPosition": 151}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c29c09e57a8bbd9e4ff80908b0c58a6cccc2d5f3", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/c29c09e57a8bbd9e4ff80908b0c58a6cccc2d5f3", "committedDate": "2020-06-18T10:12:12Z", "message": "Simplify equals method\n\nCo-authored-by: Benjamin Trent <ben.w.trent@gmail.com>"}, "afterCommit": {"oid": "5b9ecf60f23823e2c702b1bcbd7b554ef7c0c201", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/5b9ecf60f23823e2c702b1bcbd7b554ef7c0c201", "committedDate": "2020-06-24T11:55:33Z", "message": "Parsed Agg"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5b9ecf60f23823e2c702b1bcbd7b554ef7c0c201", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/5b9ecf60f23823e2c702b1bcbd7b554ef7c0c201", "committedDate": "2020-06-24T11:55:33Z", "message": "Parsed Agg"}, "afterCommit": {"oid": "0022c649b1f54dce121593c8ce26395112993d13", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/0022c649b1f54dce121593c8ce26395112993d13", "committedDate": "2020-06-24T12:33:20Z", "message": "Parsed Agg"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM3NTk0NDk2", "url": "https://github.com/elastic/elasticsearch/pull/58193#pullrequestreview-437594496", "createdAt": "2020-06-25T15:12:09Z", "commit": {"oid": "ab6e17850c7623cfec2ee7f0cdc51f8864d5dcb5"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxNToxMjowOVrOGo_VRg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxNToxMjowOVrOGo_VRg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTYzMzg2Mg==", "bodyText": "It might be good to add a predictedValue() method to the InferenceResults interface. That way you can access it without these castings, especially since all inference results will either have a predicted value or null.", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445633862", "createdAt": "2020-06-25T15:12:09Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InternalInferenceAggregation.java", "diffHunk": "@@ -48,24 +51,44 @@ public InternalAggregation reduce(List<InternalAggregation> aggregations, Reduce\n     }\n \n     @Override\n-    @SuppressWarnings(\"unchecked\")\n     public Object getProperty(List<String> path) {\n-        Map<String, Object> resultMap = this.inferenceResult.writeResultToMap();\n-\n-        for (int i=0; i<path.size() -1; i++) {\n-            Object value = resultMap.get(path.get(i));\n-            if (value == null) {\n-                throw new InvalidAggregationPathException(\"Cannot find an key [\" + path.get(i) + \"] in \" + path);\n-            }\n-\n-            if (value instanceof Map<?, ?>) {\n-                resultMap = (Map<String, Object>)value;\n+        if (path.isEmpty()) {\n+            return this;\n+        } else if (path.size() == 1) {\n+            String field = path.get(0);\n+            if (CommonFields.VALUE.getPreferredName().equals(field)) {\n+                if (inferenceResult instanceof ClassificationInferenceResults) {\n+                    return ((ClassificationInferenceResults)inferenceResult).transformedPredictedValue();\n+                } else if (inferenceResult instanceof SingleValueInferenceResults) {\n+                    return ((SingleValueInferenceResults)inferenceResult).value();\n+                } else {\n+                    return null;\n+                }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ab6e17850c7623cfec2ee7f0cdc51f8864d5dcb5"}, "originalPosition": 38}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM3NTk1NjEz", "url": "https://github.com/elastic/elasticsearch/pull/58193#pullrequestreview-437595613", "createdAt": "2020-06-25T15:13:18Z", "commit": {"oid": "ab6e17850c7623cfec2ee7f0cdc51f8864d5dcb5"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxNToxMzoxOFrOGo_Yhg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxNToxMzoxOFrOGo_Yhg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTYzNDY5NA==", "bodyText": "awe snap! Since we are handling the paths...I wonder if we could handle paths like feature_importance.0.importance and feature_importance.0.feature_name", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445634694", "createdAt": "2020-06-25T15:13:18Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InternalInferenceAggregation.java", "diffHunk": "@@ -48,24 +51,44 @@ public InternalAggregation reduce(List<InternalAggregation> aggregations, Reduce\n     }\n \n     @Override\n-    @SuppressWarnings(\"unchecked\")\n     public Object getProperty(List<String> path) {\n-        Map<String, Object> resultMap = this.inferenceResult.writeResultToMap();\n-\n-        for (int i=0; i<path.size() -1; i++) {\n-            Object value = resultMap.get(path.get(i));\n-            if (value == null) {\n-                throw new InvalidAggregationPathException(\"Cannot find an key [\" + path.get(i) + \"] in \" + path);\n-            }\n-\n-            if (value instanceof Map<?, ?>) {\n-                resultMap = (Map<String, Object>)value;\n+        if (path.isEmpty()) {\n+            return this;\n+        } else if (path.size() == 1) {\n+            String field = path.get(0);\n+            if (CommonFields.VALUE.getPreferredName().equals(field)) {\n+                if (inferenceResult instanceof ClassificationInferenceResults) {\n+                    return ((ClassificationInferenceResults)inferenceResult).transformedPredictedValue();\n+                } else if (inferenceResult instanceof SingleValueInferenceResults) {\n+                    return ((SingleValueInferenceResults)inferenceResult).value();\n+                } else {\n+                    return null;\n+                }\n+            } else if (SingleValueInferenceResults.FEATURE_IMPORTANCE.equals(field)) {\n+                if (inferenceResult instanceof SingleValueInferenceResults) {\n+                    SingleValueInferenceResults valueResult = (SingleValueInferenceResults) inferenceResult;\n+                    return valueResult.getFeatureImportance();\n+                } else {\n+                    return null;\n+                }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ab6e17850c7623cfec2ee7f0cdc51f8864d5dcb5"}, "originalPosition": 45}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM3NTk2NTA4", "url": "https://github.com/elastic/elasticsearch/pull/58193#pullrequestreview-437596508", "createdAt": "2020-06-25T15:14:14Z", "commit": {"oid": "ab6e17850c7623cfec2ee7f0cdc51f8864d5dcb5"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxNToxNDoxNFrOGo_bMg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxNToxNDoxNFrOGo_bMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTYzNTM3OA==", "bodyText": "similar to the feature_importance path. Could we support getting the top class probability?", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445635378", "createdAt": "2020-06-25T15:14:14Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InternalInferenceAggregation.java", "diffHunk": "@@ -48,24 +51,44 @@ public InternalAggregation reduce(List<InternalAggregation> aggregations, Reduce\n     }\n \n     @Override\n-    @SuppressWarnings(\"unchecked\")\n     public Object getProperty(List<String> path) {\n-        Map<String, Object> resultMap = this.inferenceResult.writeResultToMap();\n-\n-        for (int i=0; i<path.size() -1; i++) {\n-            Object value = resultMap.get(path.get(i));\n-            if (value == null) {\n-                throw new InvalidAggregationPathException(\"Cannot find an key [\" + path.get(i) + \"] in \" + path);\n-            }\n-\n-            if (value instanceof Map<?, ?>) {\n-                resultMap = (Map<String, Object>)value;\n+        if (path.isEmpty()) {\n+            return this;\n+        } else if (path.size() == 1) {\n+            String field = path.get(0);\n+            if (CommonFields.VALUE.getPreferredName().equals(field)) {\n+                if (inferenceResult instanceof ClassificationInferenceResults) {\n+                    return ((ClassificationInferenceResults)inferenceResult).transformedPredictedValue();\n+                } else if (inferenceResult instanceof SingleValueInferenceResults) {\n+                    return ((SingleValueInferenceResults)inferenceResult).value();\n+                } else {\n+                    return null;\n+                }\n+            } else if (SingleValueInferenceResults.FEATURE_IMPORTANCE.equals(field)) {\n+                if (inferenceResult instanceof SingleValueInferenceResults) {\n+                    SingleValueInferenceResults valueResult = (SingleValueInferenceResults) inferenceResult;\n+                    return valueResult.getFeatureImportance();\n+                } else {\n+                    return null;\n+                }\n+            } else if (ClassificationConfig.DEFAULT_TOP_CLASSES_RESULTS_FIELD.equals(field)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ab6e17850c7623cfec2ee7f0cdc51f8864d5dcb5"}, "originalPosition": 46}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM3NjM4NDgz", "url": "https://github.com/elastic/elasticsearch/pull/58193#pullrequestreview-437638483", "createdAt": "2020-06-25T16:00:15Z", "commit": {"oid": "285ae40134507395bb022490ab4e09bd24acd439"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxNjowMDoxNVrOGpBW2A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxNjo1MzowMFrOGpDbxQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTY2NzAzMg==", "bodyText": "Since it only accepts skip, you could just remove the option from the parser and hardcode it.", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445667032", "createdAt": "2020-06-25T16:00:15Z", "author": {"login": "polyfractal"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InferencePipelineAggregationBuilder.java", "diffHunk": "@@ -0,0 +1,236 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.inference.aggs;\n+\n+import org.apache.lucene.util.SetOnce;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.LatchedActionListener;\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.ObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ClassificationConfig;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ClassificationConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.InferenceConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ResultsFieldUpdate;\n+import org.elasticsearch.xpack.core.ml.utils.ExceptionsHelper;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.Model;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.ModelLoadingService;\n+\n+import java.io.IOException;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.TreeMap;\n+import java.util.concurrent.CountDownLatch;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.constructorArg;\n+import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregator.Parser.GAP_POLICY;\n+\n+public class InferencePipelineAggregationBuilder extends AbstractPipelineAggregationBuilder<InferencePipelineAggregationBuilder> {\n+\n+    public static String NAME = \"inference\";\n+\n+    public static final ParseField MODEL_ID = new ParseField(\"model_id\");\n+    private static final ParseField INFERENCE_CONFIG = new ParseField(\"inference_config\");\n+\n+    static String AGGREGATIONS_RESULTS_FIELD = \"value\";\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private static final ConstructingObjectParser<InferencePipelineAggregationBuilder,\n+        Tuple<SetOnce<ModelLoadingService>, String>> PARSER = new ConstructingObjectParser<>(\n+        NAME, false,\n+        (args, context) -> new InferencePipelineAggregationBuilder(context.v2(), context.v1(), (Map<String, String>) args[0])\n+    );\n+\n+    static {\n+        PARSER.declareObject(constructorArg(), (p, c) -> p.mapStrings(), BUCKETS_PATH_FIELD);\n+        PARSER.declareString(InferencePipelineAggregationBuilder::setModelId, MODEL_ID);\n+        PARSER.declareNamedObject(InferencePipelineAggregationBuilder::setInferenceConfig,\n+            (p, c, n) -> p.namedObject(InferenceConfigUpdate.class, n, c), INFERENCE_CONFIG);\n+        PARSER.declareField(InferencePipelineAggregationBuilder::setGapPolicy, p -> {\n+            if (p.currentToken() == XContentParser.Token.VALUE_STRING) {\n+                return BucketHelpers.GapPolicy.parse(p.text().toLowerCase(Locale.ROOT), p.getTokenLocation());\n+            }\n+            throw new IllegalArgumentException(\n+                \"Unsupported token [\" + p.currentToken() + \"] parsing inference aggregation \" + GAP_POLICY.getPreferredName());\n+        }, GAP_POLICY, ObjectParser.ValueType.STRING);\n+    }\n+\n+    private final Map<String, String> bucketPathMap;\n+    private String modelId;\n+    private InferenceConfigUpdate inferenceConfig;\n+    private final SetOnce<ModelLoadingService> modelLoadingService;\n+    private BucketHelpers.GapPolicy gapPolicy = BucketHelpers.GapPolicy.SKIP;\n+\n+    public static InferencePipelineAggregationBuilder parse(SetOnce<ModelLoadingService> modelLoadingService,\n+                                                            String pipelineAggregatorName,\n+                                                            XContentParser parser) {\n+        Tuple<SetOnce<ModelLoadingService>, String> context = new Tuple<>(modelLoadingService, pipelineAggregatorName);\n+        return PARSER.apply(parser, context);\n+    }\n+\n+    public InferencePipelineAggregationBuilder(String name, SetOnce<ModelLoadingService> modelLoadingService,\n+                                               Map<String, String> bucketsPath) {\n+        super(name, NAME, new TreeMap<>(bucketsPath).values().toArray(new String[] {}));\n+        this.modelLoadingService = modelLoadingService;\n+        this.bucketPathMap = bucketsPath;\n+    }\n+\n+    public InferencePipelineAggregationBuilder(StreamInput in, SetOnce<ModelLoadingService> modelLoadingService) throws IOException {\n+        super(in, NAME);\n+        modelId = in.readString();\n+        bucketPathMap = in.readMap(StreamInput::readString, StreamInput::readString);\n+        inferenceConfig = in.readOptionalNamedWriteable(InferenceConfigUpdate.class);\n+        gapPolicy = BucketHelpers.GapPolicy.readFrom(in);\n+        this.modelLoadingService = modelLoadingService;\n+    }\n+\n+    void setModelId(String modelId) {\n+        this.modelId = modelId;\n+    }\n+\n+    void setInferenceConfig(InferenceConfigUpdate inferenceConfig) {\n+        this.inferenceConfig = inferenceConfig;\n+    }\n+\n+    void setGapPolicy(BucketHelpers.GapPolicy gapPolicy) {\n+        this.gapPolicy = gapPolicy;\n+    }\n+\n+    @Override\n+    protected void validate(ValidationContext context) {\n+        context.validateHasParent(NAME, name);\n+        if (modelId == null) {\n+            context.addValidationError(\"Model Id must be set\");\n+        }\n+        if (gapPolicy != BucketHelpers.GapPolicy.SKIP) {\n+            context.addValidationError(\"gap policy [\" + gapPolicy + \"] in not valid for [\" + NAME + \"] aggregation\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "285ae40134507395bb022490ab4e09bd24acd439"}, "originalPosition": 120}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTY3NjQwMw==", "bodyText": "I don't know what any of this is doing :), but I wanted to mention that by time this is called a lot of work will have been done on the data nodes.  So if any of this validation can be done earlier (parsing, etc) it would save some work.", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445676403", "createdAt": "2020-06-25T16:14:10Z", "author": {"login": "polyfractal"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InferencePipelineAggregationBuilder.java", "diffHunk": "@@ -0,0 +1,236 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.inference.aggs;\n+\n+import org.apache.lucene.util.SetOnce;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.LatchedActionListener;\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.ObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ClassificationConfig;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ClassificationConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.InferenceConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ResultsFieldUpdate;\n+import org.elasticsearch.xpack.core.ml.utils.ExceptionsHelper;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.Model;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.ModelLoadingService;\n+\n+import java.io.IOException;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.TreeMap;\n+import java.util.concurrent.CountDownLatch;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.constructorArg;\n+import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregator.Parser.GAP_POLICY;\n+\n+public class InferencePipelineAggregationBuilder extends AbstractPipelineAggregationBuilder<InferencePipelineAggregationBuilder> {\n+\n+    public static String NAME = \"inference\";\n+\n+    public static final ParseField MODEL_ID = new ParseField(\"model_id\");\n+    private static final ParseField INFERENCE_CONFIG = new ParseField(\"inference_config\");\n+\n+    static String AGGREGATIONS_RESULTS_FIELD = \"value\";\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private static final ConstructingObjectParser<InferencePipelineAggregationBuilder,\n+        Tuple<SetOnce<ModelLoadingService>, String>> PARSER = new ConstructingObjectParser<>(\n+        NAME, false,\n+        (args, context) -> new InferencePipelineAggregationBuilder(context.v2(), context.v1(), (Map<String, String>) args[0])\n+    );\n+\n+    static {\n+        PARSER.declareObject(constructorArg(), (p, c) -> p.mapStrings(), BUCKETS_PATH_FIELD);\n+        PARSER.declareString(InferencePipelineAggregationBuilder::setModelId, MODEL_ID);\n+        PARSER.declareNamedObject(InferencePipelineAggregationBuilder::setInferenceConfig,\n+            (p, c, n) -> p.namedObject(InferenceConfigUpdate.class, n, c), INFERENCE_CONFIG);\n+        PARSER.declareField(InferencePipelineAggregationBuilder::setGapPolicy, p -> {\n+            if (p.currentToken() == XContentParser.Token.VALUE_STRING) {\n+                return BucketHelpers.GapPolicy.parse(p.text().toLowerCase(Locale.ROOT), p.getTokenLocation());\n+            }\n+            throw new IllegalArgumentException(\n+                \"Unsupported token [\" + p.currentToken() + \"] parsing inference aggregation \" + GAP_POLICY.getPreferredName());\n+        }, GAP_POLICY, ObjectParser.ValueType.STRING);\n+    }\n+\n+    private final Map<String, String> bucketPathMap;\n+    private String modelId;\n+    private InferenceConfigUpdate inferenceConfig;\n+    private final SetOnce<ModelLoadingService> modelLoadingService;\n+    private BucketHelpers.GapPolicy gapPolicy = BucketHelpers.GapPolicy.SKIP;\n+\n+    public static InferencePipelineAggregationBuilder parse(SetOnce<ModelLoadingService> modelLoadingService,\n+                                                            String pipelineAggregatorName,\n+                                                            XContentParser parser) {\n+        Tuple<SetOnce<ModelLoadingService>, String> context = new Tuple<>(modelLoadingService, pipelineAggregatorName);\n+        return PARSER.apply(parser, context);\n+    }\n+\n+    public InferencePipelineAggregationBuilder(String name, SetOnce<ModelLoadingService> modelLoadingService,\n+                                               Map<String, String> bucketsPath) {\n+        super(name, NAME, new TreeMap<>(bucketsPath).values().toArray(new String[] {}));\n+        this.modelLoadingService = modelLoadingService;\n+        this.bucketPathMap = bucketsPath;\n+    }\n+\n+    public InferencePipelineAggregationBuilder(StreamInput in, SetOnce<ModelLoadingService> modelLoadingService) throws IOException {\n+        super(in, NAME);\n+        modelId = in.readString();\n+        bucketPathMap = in.readMap(StreamInput::readString, StreamInput::readString);\n+        inferenceConfig = in.readOptionalNamedWriteable(InferenceConfigUpdate.class);\n+        gapPolicy = BucketHelpers.GapPolicy.readFrom(in);\n+        this.modelLoadingService = modelLoadingService;\n+    }\n+\n+    void setModelId(String modelId) {\n+        this.modelId = modelId;\n+    }\n+\n+    void setInferenceConfig(InferenceConfigUpdate inferenceConfig) {\n+        this.inferenceConfig = inferenceConfig;\n+    }\n+\n+    void setGapPolicy(BucketHelpers.GapPolicy gapPolicy) {\n+        this.gapPolicy = gapPolicy;\n+    }\n+\n+    @Override\n+    protected void validate(ValidationContext context) {\n+        context.validateHasParent(NAME, name);\n+        if (modelId == null) {\n+            context.addValidationError(\"Model Id must be set\");\n+        }\n+        if (gapPolicy != BucketHelpers.GapPolicy.SKIP) {\n+            context.addValidationError(\"gap policy [\" + gapPolicy + \"] in not valid for [\" + NAME + \"] aggregation\");\n+        }\n+    }\n+\n+    @Override\n+    protected void doWriteTo(StreamOutput out) throws IOException {\n+        out.writeString(modelId);\n+        out.writeMap(bucketPathMap, StreamOutput::writeString, StreamOutput::writeString);\n+        out.writeOptionalNamedWriteable(inferenceConfig);\n+        gapPolicy.writeTo(out);\n+    }\n+\n+    @Override\n+    protected PipelineAggregator createInternal(Map<String, Object> metaData) {\n+\n+        SetOnce<Model> model = new SetOnce<>();\n+        SetOnce<Exception> error = new SetOnce<>();\n+        CountDownLatch latch = new CountDownLatch(1);\n+        ActionListener<Model> listener = new LatchedActionListener<>(\n+            ActionListener.wrap(model::set, error::set), latch);\n+\n+        modelLoadingService.get().getModelForSearch(modelId, listener);\n+        try {\n+            // TODO Avoid the blocking wait\n+            latch.await();\n+        } catch (InterruptedException e) {\n+            Thread.currentThread().interrupt();\n+            throw new RuntimeException(\"Inference aggregation interrupted loading model\", e);\n+        }\n+\n+        Exception e = error.get();\n+        if (e != null) {\n+            if (e instanceof RuntimeException) {\n+                throw (RuntimeException)e;\n+            } else {\n+                throw new RuntimeException(error.get());\n+            }\n+        }\n+\n+        InferenceConfigUpdate update = adaptForAggregation(inferenceConfig);\n+\n+        return new InferencePipelineAggregator(name, bucketPathMap, metaData, update, model.get());\n+    }\n+\n+    static InferenceConfigUpdate adaptForAggregation(InferenceConfigUpdate originalUpdate) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "285ae40134507395bb022490ab4e09bd24acd439"}, "originalPosition": 164}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTY3ODYzOQ==", "bodyText": "Can these all be final?", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445678639", "createdAt": "2020-06-25T16:17:32Z", "author": {"login": "polyfractal"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InferencePipelineAggregator.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.inference.aggs;\n+\n+import org.elasticsearch.search.aggregations.AggregationExecutionException;\n+import org.elasticsearch.search.aggregations.InternalAggregation;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;\n+import org.elasticsearch.search.aggregations.bucket.MultiBucketsAggregation;\n+import org.elasticsearch.search.aggregations.bucket.terms.StringTerms;\n+import org.elasticsearch.search.aggregations.metrics.InternalNumericMetricsAggregation;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+import org.elasticsearch.search.aggregations.support.AggregationPath;\n+import org.elasticsearch.xpack.core.ml.inference.results.InferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.results.WarningInferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.InferenceConfigUpdate;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.Model;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+\n+\n+public class InferencePipelineAggregator extends PipelineAggregator {\n+\n+    private Map<String, String> bucketPathMap;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "285ae40134507395bb022490ab4e09bd24acd439"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTY4OTQ5Nw==", "bodyText": "I think we can do bucket.getAggregations().asList().stream()... instead of StreamSupport.  The older pipelines have that laying around because they predated stream on the iterator itself iirc.", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445689497", "createdAt": "2020-06-25T16:34:11Z", "author": {"login": "polyfractal"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InferencePipelineAggregator.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.inference.aggs;\n+\n+import org.elasticsearch.search.aggregations.AggregationExecutionException;\n+import org.elasticsearch.search.aggregations.InternalAggregation;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;\n+import org.elasticsearch.search.aggregations.bucket.MultiBucketsAggregation;\n+import org.elasticsearch.search.aggregations.bucket.terms.StringTerms;\n+import org.elasticsearch.search.aggregations.metrics.InternalNumericMetricsAggregation;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+import org.elasticsearch.search.aggregations.support.AggregationPath;\n+import org.elasticsearch.xpack.core.ml.inference.results.InferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.results.WarningInferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.InferenceConfigUpdate;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.Model;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+\n+\n+public class InferencePipelineAggregator extends PipelineAggregator {\n+\n+    private Map<String, String> bucketPathMap;\n+    private InferenceConfigUpdate configUpdate;\n+    private Model model;\n+\n+    public InferencePipelineAggregator(String name, Map<String,\n+                                       String> bucketPathMap,\n+                                       Map<String, Object> metaData,\n+                                       InferenceConfigUpdate configUpdate,\n+                                       Model model) {\n+        super(name, bucketPathMap.values().toArray(new String[] {}), metaData);\n+        this.bucketPathMap = bucketPathMap;\n+        this.configUpdate = configUpdate;\n+        this.model = model;\n+    }\n+\n+    @SuppressWarnings({\"rawtypes\", \"unchecked\"})\n+    @Override\n+    public InternalAggregation reduce(InternalAggregation aggregation, InternalAggregation.ReduceContext reduceContext) {\n+\n+        InternalMultiBucketAggregation<InternalMultiBucketAggregation, InternalMultiBucketAggregation.InternalBucket> originalAgg =\n+            (InternalMultiBucketAggregation<InternalMultiBucketAggregation, InternalMultiBucketAggregation.InternalBucket>) aggregation;\n+        List<? extends InternalMultiBucketAggregation.InternalBucket> buckets = originalAgg.getBuckets();\n+\n+        List<InternalMultiBucketAggregation.InternalBucket> newBuckets = new ArrayList<>();\n+        for (InternalMultiBucketAggregation.InternalBucket bucket : buckets) {\n+            Map<String, Object> inputFields = new HashMap<>();\n+\n+            if (bucket.getDocCount() == 0) {\n+                // ignore this empty bucket unless the doc count is used\n+                if (bucketPathMap.containsKey(\"_count\") == false) {\n+                    newBuckets.add(bucket);\n+                    continue;\n+                }\n+            }\n+\n+            for (Map.Entry<String, String> entry : bucketPathMap.entrySet()) {\n+                String aggName = entry.getKey();\n+                String bucketPath = entry.getValue();\n+                Object propertyValue = resolveBucketValue(originalAgg, bucket, bucketPath);\n+\n+                if (propertyValue instanceof Number) {\n+                    double doubleVal = ((Number) propertyValue).doubleValue();\n+                    // NaN or infinite values indicate a missing value\n+                    if (Double.isFinite(doubleVal)) {\n+                        inputFields.put(aggName, doubleVal);\n+                    }\n+                } else if (propertyValue instanceof InternalNumericMetricsAggregation.SingleValue) {\n+                    double doubleVal = ((InternalNumericMetricsAggregation.SingleValue) propertyValue).value();\n+                    if (Double.isFinite(doubleVal)) {\n+                        inputFields.put(aggName, doubleVal);\n+                    }\n+                } else if (propertyValue instanceof StringTerms.Bucket) {\n+                    StringTerms.Bucket b = (StringTerms.Bucket) propertyValue;\n+                    inputFields.put(aggName, b.getKeyAsString());\n+                } else if (propertyValue instanceof String) {\n+                    inputFields.put(aggName, propertyValue);\n+                } else if (propertyValue != null) {\n+                    // Doubles, String terms or null are valid, any other type is an error\n+                    throw invalidAggTypeError(bucketPath, propertyValue);\n+                }\n+            }\n+\n+\n+            InferenceResults inference;\n+            try {\n+                 inference = model.infer(inputFields, configUpdate);\n+            } catch (Exception e) {\n+                inference = new WarningInferenceResults(e.getMessage());\n+            }\n+\n+            final List<InternalAggregation> aggs = StreamSupport.stream(bucket.getAggregations().spliterator(), false).map(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "285ae40134507395bb022490ab4e09bd24acd439"}, "originalPosition": 104}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTY5ODI4Nw==", "bodyText": "Just an FYI if you care: there's currently no validation that the user doesn't enter the same bucket path multiple times with different keys.  It's not generally a problem elsewhere (and sometimes useful), but since your requirements might be different, wanted to mention it :)", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445698287", "createdAt": "2020-06-25T16:48:33Z", "author": {"login": "polyfractal"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InferencePipelineAggregator.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.inference.aggs;\n+\n+import org.elasticsearch.search.aggregations.AggregationExecutionException;\n+import org.elasticsearch.search.aggregations.InternalAggregation;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;\n+import org.elasticsearch.search.aggregations.bucket.MultiBucketsAggregation;\n+import org.elasticsearch.search.aggregations.bucket.terms.StringTerms;\n+import org.elasticsearch.search.aggregations.metrics.InternalNumericMetricsAggregation;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+import org.elasticsearch.search.aggregations.support.AggregationPath;\n+import org.elasticsearch.xpack.core.ml.inference.results.InferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.results.WarningInferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.InferenceConfigUpdate;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.Model;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+\n+\n+public class InferencePipelineAggregator extends PipelineAggregator {\n+\n+    private Map<String, String> bucketPathMap;\n+    private InferenceConfigUpdate configUpdate;\n+    private Model model;\n+\n+    public InferencePipelineAggregator(String name, Map<String,\n+                                       String> bucketPathMap,\n+                                       Map<String, Object> metaData,\n+                                       InferenceConfigUpdate configUpdate,\n+                                       Model model) {\n+        super(name, bucketPathMap.values().toArray(new String[] {}), metaData);\n+        this.bucketPathMap = bucketPathMap;\n+        this.configUpdate = configUpdate;\n+        this.model = model;\n+    }\n+\n+    @SuppressWarnings({\"rawtypes\", \"unchecked\"})\n+    @Override\n+    public InternalAggregation reduce(InternalAggregation aggregation, InternalAggregation.ReduceContext reduceContext) {\n+\n+        InternalMultiBucketAggregation<InternalMultiBucketAggregation, InternalMultiBucketAggregation.InternalBucket> originalAgg =\n+            (InternalMultiBucketAggregation<InternalMultiBucketAggregation, InternalMultiBucketAggregation.InternalBucket>) aggregation;\n+        List<? extends InternalMultiBucketAggregation.InternalBucket> buckets = originalAgg.getBuckets();\n+\n+        List<InternalMultiBucketAggregation.InternalBucket> newBuckets = new ArrayList<>();\n+        for (InternalMultiBucketAggregation.InternalBucket bucket : buckets) {\n+            Map<String, Object> inputFields = new HashMap<>();\n+\n+            if (bucket.getDocCount() == 0) {\n+                // ignore this empty bucket unless the doc count is used\n+                if (bucketPathMap.containsKey(\"_count\") == false) {\n+                    newBuckets.add(bucket);\n+                    continue;\n+                }\n+            }\n+\n+            for (Map.Entry<String, String> entry : bucketPathMap.entrySet()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "285ae40134507395bb022490ab4e09bd24acd439"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTcwMTA2MQ==", "bodyText": "Another Note: NaN or infinite could also mean the agg actually generate a NaN/Infinite.  Unfortunately there's not a good way of knowing without inspecting the underlying agg itself and seeing if it's using its placeholder value.\nOther pipelines don't particularly care, all NaN/Infinite are bad, so it's never been a problem.  But YMMV :)", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445701061", "createdAt": "2020-06-25T16:53:00Z", "author": {"login": "polyfractal"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InferencePipelineAggregator.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.inference.aggs;\n+\n+import org.elasticsearch.search.aggregations.AggregationExecutionException;\n+import org.elasticsearch.search.aggregations.InternalAggregation;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;\n+import org.elasticsearch.search.aggregations.bucket.MultiBucketsAggregation;\n+import org.elasticsearch.search.aggregations.bucket.terms.StringTerms;\n+import org.elasticsearch.search.aggregations.metrics.InternalNumericMetricsAggregation;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+import org.elasticsearch.search.aggregations.support.AggregationPath;\n+import org.elasticsearch.xpack.core.ml.inference.results.InferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.results.WarningInferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.InferenceConfigUpdate;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.Model;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+\n+\n+public class InferencePipelineAggregator extends PipelineAggregator {\n+\n+    private Map<String, String> bucketPathMap;\n+    private InferenceConfigUpdate configUpdate;\n+    private Model model;\n+\n+    public InferencePipelineAggregator(String name, Map<String,\n+                                       String> bucketPathMap,\n+                                       Map<String, Object> metaData,\n+                                       InferenceConfigUpdate configUpdate,\n+                                       Model model) {\n+        super(name, bucketPathMap.values().toArray(new String[] {}), metaData);\n+        this.bucketPathMap = bucketPathMap;\n+        this.configUpdate = configUpdate;\n+        this.model = model;\n+    }\n+\n+    @SuppressWarnings({\"rawtypes\", \"unchecked\"})\n+    @Override\n+    public InternalAggregation reduce(InternalAggregation aggregation, InternalAggregation.ReduceContext reduceContext) {\n+\n+        InternalMultiBucketAggregation<InternalMultiBucketAggregation, InternalMultiBucketAggregation.InternalBucket> originalAgg =\n+            (InternalMultiBucketAggregation<InternalMultiBucketAggregation, InternalMultiBucketAggregation.InternalBucket>) aggregation;\n+        List<? extends InternalMultiBucketAggregation.InternalBucket> buckets = originalAgg.getBuckets();\n+\n+        List<InternalMultiBucketAggregation.InternalBucket> newBuckets = new ArrayList<>();\n+        for (InternalMultiBucketAggregation.InternalBucket bucket : buckets) {\n+            Map<String, Object> inputFields = new HashMap<>();\n+\n+            if (bucket.getDocCount() == 0) {\n+                // ignore this empty bucket unless the doc count is used\n+                if (bucketPathMap.containsKey(\"_count\") == false) {\n+                    newBuckets.add(bucket);\n+                    continue;\n+                }\n+            }\n+\n+            for (Map.Entry<String, String> entry : bucketPathMap.entrySet()) {\n+                String aggName = entry.getKey();\n+                String bucketPath = entry.getValue();\n+                Object propertyValue = resolveBucketValue(originalAgg, bucket, bucketPath);\n+\n+                if (propertyValue instanceof Number) {\n+                    double doubleVal = ((Number) propertyValue).doubleValue();\n+                    // NaN or infinite values indicate a missing value", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "285ae40134507395bb022490ab4e09bd24acd439"}, "originalPosition": 76}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM3NjY1Mzg3", "url": "https://github.com/elastic/elasticsearch/pull/58193#pullrequestreview-437665387", "createdAt": "2020-06-25T16:32:17Z", "commit": {"oid": "285ae40134507395bb022490ab4e09bd24acd439"}, "state": "COMMENTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxNjozMjoxN1rOGpCqEQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxNzo0NjozMlrOGpFUIA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTY4ODMzNw==", "bodyText": "Seems to be only accessed in tests.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public PredictionFieldType getPredictionFieldType() {\n          \n          \n            \n                PredictionFieldType getPredictionFieldType() {", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445688337", "createdAt": "2020-06-25T16:32:17Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ml/inference/results/ClassificationInferenceResults.java", "diffHunk": "@@ -85,6 +84,10 @@ public String getClassificationLabel() {\n         return topClasses;\n     }\n \n+    public PredictionFieldType getPredictionFieldType() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "285ae40134507395bb022490ab4e09bd24acd439"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTY5NjY2Mg==", "bodyText": "Suggested change", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445696662", "createdAt": "2020-06-25T16:46:04Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ml/inference/trainedmodel/ClassificationConfigUpdate.java", "diffHunk": "@@ -96,6 +96,8 @@ public ClassificationConfigUpdate(Integer numTopClasses,\n         }\n         this.numTopFeatureImportanceValues = featureImportance;\n         this.predictionFieldType = predictionFieldType;\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "285ae40134507395bb022490ab4e09bd24acd439"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTcxMTYzMw==", "bodyText": "I know it will mean more LoC, but this method is not extensible and only exists for a single thing/path.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                InferenceConfigUpdate duplicateWithResultsField(String resultsField);\n          \n          \n            \n                Builder newBuilder();\n          \n          \n            \n            \n          \n          \n            \n                interface Builder<T extends Builder, U extends InferenceConfigUpdate> {\n          \n          \n            \n            \n          \n          \n            \n                    U build();\n          \n          \n            \n            \n          \n          \n            \n                    T setResultsField(String resultsField);\n          \n          \n            \n            \n          \n          \n            \n                }\n          \n      \n    \n    \n  \n\nThen each builder satisfies this interface and needs a copy ctor accepting their appropriate update object.\nThe code to replace the resultsField name is now\nInferenceConfigUpdate newUpdate = update.newBuilder().setResultsField(\"foo\").build();\n\nThis seems more general + more clean than a one off method.", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445711633", "createdAt": "2020-06-25T17:11:09Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ml/inference/trainedmodel/InferenceConfigUpdate.java", "diffHunk": "@@ -6,14 +6,48 @@\n package org.elasticsearch.xpack.core.ml.inference.trainedmodel;\n \n import org.elasticsearch.common.io.stream.NamedWriteable;\n+import org.elasticsearch.xpack.core.ml.inference.TrainedModelConfig;\n+import org.elasticsearch.xpack.core.ml.inference.results.WarningInferenceResults;\n+import org.elasticsearch.xpack.core.ml.utils.ExceptionsHelper;\n import org.elasticsearch.xpack.core.ml.utils.NamedXContentObject;\n \n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n \n public interface InferenceConfigUpdate extends NamedXContentObject, NamedWriteable {\n+    Set<String> RESERVED_ML_FIELD_NAMES = new HashSet<>(Arrays.asList(\n+        WarningInferenceResults.WARNING.getPreferredName(),\n+        TrainedModelConfig.MODEL_ID.getPreferredName()));\n \n     InferenceConfig apply(InferenceConfig originalConfig);\n \n     InferenceConfig toConfig();\n \n     boolean isSupported(InferenceConfig config);\n+\n+    String getResultsField();\n+\n+    InferenceConfigUpdate duplicateWithResultsField(String resultsField);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "285ae40134507395bb022490ab4e09bd24acd439"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTcxOTA3Mw==", "bodyText": "These types of validations seem like they belong in the agg parser.\nMaybe we should always parse the named objects into a ResultsFieldUpdate object or something.\nAt a minimum, these validations should occur before we attempt to load the model.", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445719073", "createdAt": "2020-06-25T17:24:43Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InferencePipelineAggregationBuilder.java", "diffHunk": "@@ -0,0 +1,236 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.inference.aggs;\n+\n+import org.apache.lucene.util.SetOnce;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.LatchedActionListener;\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.ObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ClassificationConfig;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ClassificationConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.InferenceConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ResultsFieldUpdate;\n+import org.elasticsearch.xpack.core.ml.utils.ExceptionsHelper;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.Model;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.ModelLoadingService;\n+\n+import java.io.IOException;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.TreeMap;\n+import java.util.concurrent.CountDownLatch;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.constructorArg;\n+import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregator.Parser.GAP_POLICY;\n+\n+public class InferencePipelineAggregationBuilder extends AbstractPipelineAggregationBuilder<InferencePipelineAggregationBuilder> {\n+\n+    public static String NAME = \"inference\";\n+\n+    public static final ParseField MODEL_ID = new ParseField(\"model_id\");\n+    private static final ParseField INFERENCE_CONFIG = new ParseField(\"inference_config\");\n+\n+    static String AGGREGATIONS_RESULTS_FIELD = \"value\";\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private static final ConstructingObjectParser<InferencePipelineAggregationBuilder,\n+        Tuple<SetOnce<ModelLoadingService>, String>> PARSER = new ConstructingObjectParser<>(\n+        NAME, false,\n+        (args, context) -> new InferencePipelineAggregationBuilder(context.v2(), context.v1(), (Map<String, String>) args[0])\n+    );\n+\n+    static {\n+        PARSER.declareObject(constructorArg(), (p, c) -> p.mapStrings(), BUCKETS_PATH_FIELD);\n+        PARSER.declareString(InferencePipelineAggregationBuilder::setModelId, MODEL_ID);\n+        PARSER.declareNamedObject(InferencePipelineAggregationBuilder::setInferenceConfig,\n+            (p, c, n) -> p.namedObject(InferenceConfigUpdate.class, n, c), INFERENCE_CONFIG);\n+        PARSER.declareField(InferencePipelineAggregationBuilder::setGapPolicy, p -> {\n+            if (p.currentToken() == XContentParser.Token.VALUE_STRING) {\n+                return BucketHelpers.GapPolicy.parse(p.text().toLowerCase(Locale.ROOT), p.getTokenLocation());\n+            }\n+            throw new IllegalArgumentException(\n+                \"Unsupported token [\" + p.currentToken() + \"] parsing inference aggregation \" + GAP_POLICY.getPreferredName());\n+        }, GAP_POLICY, ObjectParser.ValueType.STRING);\n+    }\n+\n+    private final Map<String, String> bucketPathMap;\n+    private String modelId;\n+    private InferenceConfigUpdate inferenceConfig;\n+    private final SetOnce<ModelLoadingService> modelLoadingService;\n+    private BucketHelpers.GapPolicy gapPolicy = BucketHelpers.GapPolicy.SKIP;\n+\n+    public static InferencePipelineAggregationBuilder parse(SetOnce<ModelLoadingService> modelLoadingService,\n+                                                            String pipelineAggregatorName,\n+                                                            XContentParser parser) {\n+        Tuple<SetOnce<ModelLoadingService>, String> context = new Tuple<>(modelLoadingService, pipelineAggregatorName);\n+        return PARSER.apply(parser, context);\n+    }\n+\n+    public InferencePipelineAggregationBuilder(String name, SetOnce<ModelLoadingService> modelLoadingService,\n+                                               Map<String, String> bucketsPath) {\n+        super(name, NAME, new TreeMap<>(bucketsPath).values().toArray(new String[] {}));\n+        this.modelLoadingService = modelLoadingService;\n+        this.bucketPathMap = bucketsPath;\n+    }\n+\n+    public InferencePipelineAggregationBuilder(StreamInput in, SetOnce<ModelLoadingService> modelLoadingService) throws IOException {\n+        super(in, NAME);\n+        modelId = in.readString();\n+        bucketPathMap = in.readMap(StreamInput::readString, StreamInput::readString);\n+        inferenceConfig = in.readOptionalNamedWriteable(InferenceConfigUpdate.class);\n+        gapPolicy = BucketHelpers.GapPolicy.readFrom(in);\n+        this.modelLoadingService = modelLoadingService;\n+    }\n+\n+    void setModelId(String modelId) {\n+        this.modelId = modelId;\n+    }\n+\n+    void setInferenceConfig(InferenceConfigUpdate inferenceConfig) {\n+        this.inferenceConfig = inferenceConfig;\n+    }\n+\n+    void setGapPolicy(BucketHelpers.GapPolicy gapPolicy) {\n+        this.gapPolicy = gapPolicy;\n+    }\n+\n+    @Override\n+    protected void validate(ValidationContext context) {\n+        context.validateHasParent(NAME, name);\n+        if (modelId == null) {\n+            context.addValidationError(\"Model Id must be set\");\n+        }\n+        if (gapPolicy != BucketHelpers.GapPolicy.SKIP) {\n+            context.addValidationError(\"gap policy [\" + gapPolicy + \"] in not valid for [\" + NAME + \"] aggregation\");\n+        }\n+    }\n+\n+    @Override\n+    protected void doWriteTo(StreamOutput out) throws IOException {\n+        out.writeString(modelId);\n+        out.writeMap(bucketPathMap, StreamOutput::writeString, StreamOutput::writeString);\n+        out.writeOptionalNamedWriteable(inferenceConfig);\n+        gapPolicy.writeTo(out);\n+    }\n+\n+    @Override\n+    protected PipelineAggregator createInternal(Map<String, Object> metaData) {\n+\n+        SetOnce<Model> model = new SetOnce<>();\n+        SetOnce<Exception> error = new SetOnce<>();\n+        CountDownLatch latch = new CountDownLatch(1);\n+        ActionListener<Model> listener = new LatchedActionListener<>(\n+            ActionListener.wrap(model::set, error::set), latch);\n+\n+        modelLoadingService.get().getModelForSearch(modelId, listener);\n+        try {\n+            // TODO Avoid the blocking wait\n+            latch.await();\n+        } catch (InterruptedException e) {\n+            Thread.currentThread().interrupt();\n+            throw new RuntimeException(\"Inference aggregation interrupted loading model\", e);\n+        }\n+\n+        Exception e = error.get();\n+        if (e != null) {\n+            if (e instanceof RuntimeException) {\n+                throw (RuntimeException)e;\n+            } else {\n+                throw new RuntimeException(error.get());\n+            }\n+        }\n+\n+        InferenceConfigUpdate update = adaptForAggregation(inferenceConfig);\n+\n+        return new InferencePipelineAggregator(name, bucketPathMap, metaData, update, model.get());\n+    }\n+\n+    static InferenceConfigUpdate adaptForAggregation(InferenceConfigUpdate originalUpdate) {\n+        InferenceConfigUpdate updated;\n+        if (originalUpdate == null) {\n+            updated = new ResultsFieldUpdate(AGGREGATIONS_RESULTS_FIELD);\n+        } else {\n+            if (originalUpdate instanceof ClassificationConfigUpdate) {\n+                ClassificationConfigUpdate classUpdate = (ClassificationConfigUpdate)originalUpdate;\n+\n+                // error if the top classes result field is set and not equal to the only acceptable value\n+                String topClassesField = classUpdate.getTopClassesResultsField();\n+                if (Strings.isNullOrEmpty(topClassesField) == false &&\n+                    ClassificationConfig.DEFAULT_TOP_CLASSES_RESULTS_FIELD.equals(topClassesField) == false) {\n+                    throw ExceptionsHelper.badRequestException(\"setting option [{}] to [{}] is not valid for inference aggregations\",\n+                        ClassificationConfig.DEFAULT_TOP_CLASSES_RESULTS_FIELD, topClassesField);\n+                }\n+            }\n+\n+            // error if the results field is set and not equal to the only acceptable value\n+            String resultsField = originalUpdate.getResultsField();\n+            if (Strings.isNullOrEmpty(resultsField) == false && AGGREGATIONS_RESULTS_FIELD.equals(resultsField) == false) {\n+                throw ExceptionsHelper.badRequestException(\"setting option [{}] to [{}] is not valid for inference aggregations\",\n+                    ClassificationConfig.RESULTS_FIELD.getPreferredName(), resultsField);\n+            }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "285ae40134507395bb022490ab4e09bd24acd439"}, "originalPosition": 186}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTcxOTcxNg==", "bodyText": "Suggested change", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445719716", "createdAt": "2020-06-25T17:25:47Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InferencePipelineAggregator.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.inference.aggs;\n+\n+import org.elasticsearch.search.aggregations.AggregationExecutionException;\n+import org.elasticsearch.search.aggregations.InternalAggregation;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;\n+import org.elasticsearch.search.aggregations.bucket.MultiBucketsAggregation;\n+import org.elasticsearch.search.aggregations.bucket.terms.StringTerms;\n+import org.elasticsearch.search.aggregations.metrics.InternalNumericMetricsAggregation;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+import org.elasticsearch.search.aggregations.support.AggregationPath;\n+import org.elasticsearch.xpack.core.ml.inference.results.InferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.results.WarningInferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.InferenceConfigUpdate;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.Model;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+\n+\n+public class InferencePipelineAggregator extends PipelineAggregator {\n+\n+    private Map<String, String> bucketPathMap;\n+    private InferenceConfigUpdate configUpdate;\n+    private Model model;\n+\n+    public InferencePipelineAggregator(String name, Map<String,\n+                                       String> bucketPathMap,\n+                                       Map<String, Object> metaData,\n+                                       InferenceConfigUpdate configUpdate,\n+                                       Model model) {\n+        super(name, bucketPathMap.values().toArray(new String[] {}), metaData);\n+        this.bucketPathMap = bucketPathMap;\n+        this.configUpdate = configUpdate;\n+        this.model = model;\n+    }\n+\n+    @SuppressWarnings({\"rawtypes\", \"unchecked\"})\n+    @Override\n+    public InternalAggregation reduce(InternalAggregation aggregation, InternalAggregation.ReduceContext reduceContext) {\n+\n+        InternalMultiBucketAggregation<InternalMultiBucketAggregation, InternalMultiBucketAggregation.InternalBucket> originalAgg =\n+            (InternalMultiBucketAggregation<InternalMultiBucketAggregation, InternalMultiBucketAggregation.InternalBucket>) aggregation;\n+        List<? extends InternalMultiBucketAggregation.InternalBucket> buckets = originalAgg.getBuckets();\n+\n+        List<InternalMultiBucketAggregation.InternalBucket> newBuckets = new ArrayList<>();\n+        for (InternalMultiBucketAggregation.InternalBucket bucket : buckets) {\n+            Map<String, Object> inputFields = new HashMap<>();\n+\n+            if (bucket.getDocCount() == 0) {\n+                // ignore this empty bucket unless the doc count is used\n+                if (bucketPathMap.containsKey(\"_count\") == false) {\n+                    newBuckets.add(bucket);\n+                    continue;\n+                }\n+            }\n+\n+            for (Map.Entry<String, String> entry : bucketPathMap.entrySet()) {\n+                String aggName = entry.getKey();\n+                String bucketPath = entry.getValue();\n+                Object propertyValue = resolveBucketValue(originalAgg, bucket, bucketPath);\n+\n+                if (propertyValue instanceof Number) {\n+                    double doubleVal = ((Number) propertyValue).doubleValue();\n+                    // NaN or infinite values indicate a missing value\n+                    if (Double.isFinite(doubleVal)) {\n+                        inputFields.put(aggName, doubleVal);\n+                    }\n+                } else if (propertyValue instanceof InternalNumericMetricsAggregation.SingleValue) {\n+                    double doubleVal = ((InternalNumericMetricsAggregation.SingleValue) propertyValue).value();\n+                    if (Double.isFinite(doubleVal)) {\n+                        inputFields.put(aggName, doubleVal);\n+                    }\n+                } else if (propertyValue instanceof StringTerms.Bucket) {\n+                    StringTerms.Bucket b = (StringTerms.Bucket) propertyValue;\n+                    inputFields.put(aggName, b.getKeyAsString());\n+                } else if (propertyValue instanceof String) {\n+                    inputFields.put(aggName, propertyValue);\n+                } else if (propertyValue != null) {\n+                    // Doubles, String terms or null are valid, any other type is an error\n+                    throw invalidAggTypeError(bucketPath, propertyValue);\n+                }\n+            }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "285ae40134507395bb022490ab4e09bd24acd439"}, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTcyMDEwNg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                             inference = model.infer(inputFields, configUpdate);\n          \n          \n            \n                            inference = model.infer(inputFields, configUpdate);", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445720106", "createdAt": "2020-06-25T17:26:29Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InferencePipelineAggregator.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.inference.aggs;\n+\n+import org.elasticsearch.search.aggregations.AggregationExecutionException;\n+import org.elasticsearch.search.aggregations.InternalAggregation;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;\n+import org.elasticsearch.search.aggregations.bucket.MultiBucketsAggregation;\n+import org.elasticsearch.search.aggregations.bucket.terms.StringTerms;\n+import org.elasticsearch.search.aggregations.metrics.InternalNumericMetricsAggregation;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+import org.elasticsearch.search.aggregations.support.AggregationPath;\n+import org.elasticsearch.xpack.core.ml.inference.results.InferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.results.WarningInferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.InferenceConfigUpdate;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.Model;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+\n+\n+public class InferencePipelineAggregator extends PipelineAggregator {\n+\n+    private Map<String, String> bucketPathMap;\n+    private InferenceConfigUpdate configUpdate;\n+    private Model model;\n+\n+    public InferencePipelineAggregator(String name, Map<String,\n+                                       String> bucketPathMap,\n+                                       Map<String, Object> metaData,\n+                                       InferenceConfigUpdate configUpdate,\n+                                       Model model) {\n+        super(name, bucketPathMap.values().toArray(new String[] {}), metaData);\n+        this.bucketPathMap = bucketPathMap;\n+        this.configUpdate = configUpdate;\n+        this.model = model;\n+    }\n+\n+    @SuppressWarnings({\"rawtypes\", \"unchecked\"})\n+    @Override\n+    public InternalAggregation reduce(InternalAggregation aggregation, InternalAggregation.ReduceContext reduceContext) {\n+\n+        InternalMultiBucketAggregation<InternalMultiBucketAggregation, InternalMultiBucketAggregation.InternalBucket> originalAgg =\n+            (InternalMultiBucketAggregation<InternalMultiBucketAggregation, InternalMultiBucketAggregation.InternalBucket>) aggregation;\n+        List<? extends InternalMultiBucketAggregation.InternalBucket> buckets = originalAgg.getBuckets();\n+\n+        List<InternalMultiBucketAggregation.InternalBucket> newBuckets = new ArrayList<>();\n+        for (InternalMultiBucketAggregation.InternalBucket bucket : buckets) {\n+            Map<String, Object> inputFields = new HashMap<>();\n+\n+            if (bucket.getDocCount() == 0) {\n+                // ignore this empty bucket unless the doc count is used\n+                if (bucketPathMap.containsKey(\"_count\") == false) {\n+                    newBuckets.add(bucket);\n+                    continue;\n+                }\n+            }\n+\n+            for (Map.Entry<String, String> entry : bucketPathMap.entrySet()) {\n+                String aggName = entry.getKey();\n+                String bucketPath = entry.getValue();\n+                Object propertyValue = resolveBucketValue(originalAgg, bucket, bucketPath);\n+\n+                if (propertyValue instanceof Number) {\n+                    double doubleVal = ((Number) propertyValue).doubleValue();\n+                    // NaN or infinite values indicate a missing value\n+                    if (Double.isFinite(doubleVal)) {\n+                        inputFields.put(aggName, doubleVal);\n+                    }\n+                } else if (propertyValue instanceof InternalNumericMetricsAggregation.SingleValue) {\n+                    double doubleVal = ((InternalNumericMetricsAggregation.SingleValue) propertyValue).value();\n+                    if (Double.isFinite(doubleVal)) {\n+                        inputFields.put(aggName, doubleVal);\n+                    }\n+                } else if (propertyValue instanceof StringTerms.Bucket) {\n+                    StringTerms.Bucket b = (StringTerms.Bucket) propertyValue;\n+                    inputFields.put(aggName, b.getKeyAsString());\n+                } else if (propertyValue instanceof String) {\n+                    inputFields.put(aggName, propertyValue);\n+                } else if (propertyValue != null) {\n+                    // Doubles, String terms or null are valid, any other type is an error\n+                    throw invalidAggTypeError(bucketPath, propertyValue);\n+                }\n+            }\n+\n+\n+            InferenceResults inference;\n+            try {\n+                 inference = model.infer(inputFields, configUpdate);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "285ae40134507395bb022490ab4e09bd24acd439"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTcyNzMyMg==", "bodyText": "Its difficult to fully grok with all these branches. Any way to use the fact that these branches are all returning out of the function?\nSomething like\nif (predicate) {\n   return \"foo\";\n}\nif (predicate2) {\n   return \"bar\";\n}\nreturn null;", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445727322", "createdAt": "2020-06-25T17:39:02Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InternalInferenceAggregation.java", "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.inference.aggs;\n+\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.search.aggregations.InternalAggregation;\n+import org.elasticsearch.search.aggregations.InvalidAggregationPathException;\n+import org.elasticsearch.xpack.core.ml.inference.results.ClassificationInferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.results.InferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.results.SingleValueInferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ClassificationConfig;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+public class InternalInferenceAggregation extends InternalAggregation {\n+\n+    private final InferenceResults inferenceResult;\n+\n+    protected InternalInferenceAggregation(String name, Map<String, Object> metadata,\n+                                           InferenceResults inferenceResult) {\n+        super(name, metadata);\n+        this.inferenceResult = inferenceResult;\n+    }\n+\n+    public InternalInferenceAggregation(StreamInput in) throws IOException {\n+        super(in);\n+        inferenceResult = in.readNamedWriteable(InferenceResults.class);\n+    }\n+\n+    public InferenceResults getInferenceResult() {\n+        return inferenceResult;\n+    }\n+\n+    @Override\n+    protected void doWriteTo(StreamOutput out) throws IOException {\n+        out.writeNamedWriteable(inferenceResult);\n+    }\n+\n+    @Override\n+    public InternalAggregation reduce(List<InternalAggregation> aggregations, ReduceContext reduceContext) {\n+        throw new UnsupportedOperationException(\"Reducing an inference aggregation is not supported\");\n+    }\n+\n+    @Override\n+    public Object getProperty(List<String> path) {\n+        if (path.isEmpty()) {\n+            return this;\n+        } else if (path.size() == 1) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "285ae40134507395bb022490ab4e09bd24acd439"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTczMTg3Mg==", "bodyText": "I do not see where this is used or referenced.", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445731872", "createdAt": "2020-06-25T17:46:32Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/ParsedInference.java", "diffHunk": "@@ -0,0 +1,131 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.inference.aggs;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.ObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParseException;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+import org.elasticsearch.search.aggregations.ParsedAggregation;\n+import org.elasticsearch.xpack.core.ml.inference.results.FeatureImportance;\n+import org.elasticsearch.xpack.core.ml.inference.results.SingleValueInferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.results.TopClassEntry;\n+import org.elasticsearch.xpack.core.ml.inference.results.WarningInferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ClassificationConfig;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+import static org.elasticsearch.xpack.core.ml.inference.results.SingleValueInferenceResults.FEATURE_IMPORTANCE;\n+\n+\n+/**\n+ * There isn't enough information in toXContent representation of the\n+ * {@link org.elasticsearch.xpack.core.ml.inference.results.InferenceResults}\n+ * objects to fully reconstruct them. In particular, depending on which\n+ * fields are written (result value, feature importance) it is not possible to\n+ * distinguish between a Regression result and a Classification result.\n+ *\n+ * This class parses the union all possible fields that may be written by\n+ * InferenceResults.\n+ *\n+ * The warning field is mutually exclusive with all the other fields.\n+ */\n+public class ParsedInference extends ParsedAggregation {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "285ae40134507395bb022490ab4e09bd24acd439"}, "originalPosition": 41}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM5MzI3NTcy", "url": "https://github.com/elastic/elasticsearch/pull/58193#pullrequestreview-439327572", "createdAt": "2020-06-29T16:50:10Z", "commit": {"oid": "208f223a49286bb39992dd52edc2ad1e901d8881"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "53af0bc3634f387f4185614da503d51e41db70c2", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/53af0bc3634f387f4185614da503d51e41db70c2", "committedDate": "2020-06-29T22:13:57Z", "message": "Remove the GapPolicy"}, "afterCommit": {"oid": "8b5cbb0268d18996ea2153e29499e116f3d71a20", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/8b5cbb0268d18996ea2153e29499e116f3d71a20", "committedDate": "2020-06-30T08:04:03Z", "message": "Remove the GapPolicy"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "24b9238bd0c5524d389bb3022f5d258576197687", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/24b9238bd0c5524d389bb3022f5d258576197687", "committedDate": "2020-06-30T08:13:00Z", "message": "Fix compilation after rebase"}, "afterCommit": {"oid": "5e8c63896d7ed268c0bbdd74d7a084badac1ae76", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/5e8c63896d7ed268c0bbdd74d7a084badac1ae76", "committedDate": "2020-06-30T15:33:14Z", "message": "First pass at docs"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQwNTg4MTU1", "url": "https://github.com/elastic/elasticsearch/pull/58193#pullrequestreview-440588155", "createdAt": "2020-07-01T06:47:38Z", "commit": {"oid": "5e8c63896d7ed268c0bbdd74d7a084badac1ae76"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQwNjo0NzozOFrOGrY6oQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQwNjo1NzowMlrOGrZKow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE1MDE3Nw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \n          \n          \n            \n            \n          \n          \n            \n            [[inference-bucket-agg-syntax]]", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r448150177", "createdAt": "2020-07-01T06:47:38Z", "author": {"login": "szabosteve"}, "path": "docs/reference/aggregations/pipeline/inference-bucket-aggregation.asciidoc", "diffHunk": "@@ -0,0 +1,75 @@\n+[role=\"xpack\"]\n+[testenv=\"basic\"]\n+[[search-aggregations-pipeline-inference-bucket-aggregation]]\n+=== Inference Bucket Aggregation\n+\n+A parent pipeline aggregation which loads a pre-trained model and performs inference on the\n+collated result field from the parent bucket aggregation.\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e8c63896d7ed268c0bbdd74d7a084badac1ae76"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE1Mzc4Mg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ==== Configuration options for Inference Models\n          \n          \n            \n            ==== Configuration options for {infer} models", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r448153782", "createdAt": "2020-07-01T06:55:53Z", "author": {"login": "szabosteve"}, "path": "docs/reference/aggregations/pipeline/inference-bucket-aggregation.asciidoc", "diffHunk": "@@ -0,0 +1,75 @@\n+[role=\"xpack\"]\n+[testenv=\"basic\"]\n+[[search-aggregations-pipeline-inference-bucket-aggregation]]\n+=== Inference Bucket Aggregation\n+\n+A parent pipeline aggregation which loads a pre-trained model and performs inference on the\n+collated result field from the parent bucket aggregation.\n+\n+==== Syntax\n+\n+A `inference` aggregation looks like this in isolation:\n+\n+[source,js]\n+--------------------------------------------------\n+{\n+    \"inference\": {\n+        \"model_id\": \"a_model_for_inference\", <1>\n+        \"inference_config\": { <2>\n+            \"regression_config\": {\n+                \"num_top_feature_importance_values\": 2\n+            }\n+        },\n+        \"buckets_path\": {\n+            \"avg_cost\": \"avg_agg\", <3>\n+            \"max_cost\": \"max_agg\"\n+        }\n+    }\n+}\n+--------------------------------------------------\n+// NOTCONSOLE\n+<1> The ID of model to use.\n+<2> The optional inference config which overrides the model's default settings\n+<3> Map the value of `avg_agg` to the model's input field `avg_cost`\n+\n+[[inference-bucket-params]]\n+.`inference` Parameters\n+[options=\"header\"]\n+|===\n+|Parameter Name |Description |Required |Default Value\n+| `model_id`         | The ID of the model to load and infer against       | Required  | -\n+| `inference_config` | Contains the inference type and its options. There are two types: <<inference-agg-regression-opt,`regression`>> and <<inference-agg-classification-opt,`classification`>>  | Optional | -\n+| `buckets_path`     | Defines the paths to the input aggregations and maps the aggregation names to the field names expected by the model.\n+See <<buckets-path-syntax>> for more details | Required       | -\n+|===\n+\n+\n+==== Configuration options for Inference Models", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e8c63896d7ed268c0bbdd74d7a084badac1ae76"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE1NDEzOA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ===== Configuration options for {regression-cap} Models\n          \n          \n            \n            ===== Configuration options for {regression} models", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r448154138", "createdAt": "2020-07-01T06:56:42Z", "author": {"login": "szabosteve"}, "path": "docs/reference/aggregations/pipeline/inference-bucket-aggregation.asciidoc", "diffHunk": "@@ -0,0 +1,75 @@\n+[role=\"xpack\"]\n+[testenv=\"basic\"]\n+[[search-aggregations-pipeline-inference-bucket-aggregation]]\n+=== Inference Bucket Aggregation\n+\n+A parent pipeline aggregation which loads a pre-trained model and performs inference on the\n+collated result field from the parent bucket aggregation.\n+\n+==== Syntax\n+\n+A `inference` aggregation looks like this in isolation:\n+\n+[source,js]\n+--------------------------------------------------\n+{\n+    \"inference\": {\n+        \"model_id\": \"a_model_for_inference\", <1>\n+        \"inference_config\": { <2>\n+            \"regression_config\": {\n+                \"num_top_feature_importance_values\": 2\n+            }\n+        },\n+        \"buckets_path\": {\n+            \"avg_cost\": \"avg_agg\", <3>\n+            \"max_cost\": \"max_agg\"\n+        }\n+    }\n+}\n+--------------------------------------------------\n+// NOTCONSOLE\n+<1> The ID of model to use.\n+<2> The optional inference config which overrides the model's default settings\n+<3> Map the value of `avg_agg` to the model's input field `avg_cost`\n+\n+[[inference-bucket-params]]\n+.`inference` Parameters\n+[options=\"header\"]\n+|===\n+|Parameter Name |Description |Required |Default Value\n+| `model_id`         | The ID of the model to load and infer against       | Required  | -\n+| `inference_config` | Contains the inference type and its options. There are two types: <<inference-agg-regression-opt,`regression`>> and <<inference-agg-classification-opt,`classification`>>  | Optional | -\n+| `buckets_path`     | Defines the paths to the input aggregations and maps the aggregation names to the field names expected by the model.\n+See <<buckets-path-syntax>> for more details | Required       | -\n+|===\n+\n+\n+==== Configuration options for Inference Models\n+The `inference_config` setting is optional and usaully isn't required as the pre-trained models come equipped with sensible defaults.\n+In the context of aggregations some options can overridden for each of the 2 types of model.\n+\n+[discrete]\n+[[inference-agg-regression-opt]]\n+===== Configuration options for {regression-cap} Models", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e8c63896d7ed268c0bbdd74d7a084badac1ae76"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE1NDI3NQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ===== Configuration options for {classification-cap} Models\n          \n          \n            \n            ===== Configuration options for {classification} models", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r448154275", "createdAt": "2020-07-01T06:57:02Z", "author": {"login": "szabosteve"}, "path": "docs/reference/aggregations/pipeline/inference-bucket-aggregation.asciidoc", "diffHunk": "@@ -0,0 +1,75 @@\n+[role=\"xpack\"]\n+[testenv=\"basic\"]\n+[[search-aggregations-pipeline-inference-bucket-aggregation]]\n+=== Inference Bucket Aggregation\n+\n+A parent pipeline aggregation which loads a pre-trained model and performs inference on the\n+collated result field from the parent bucket aggregation.\n+\n+==== Syntax\n+\n+A `inference` aggregation looks like this in isolation:\n+\n+[source,js]\n+--------------------------------------------------\n+{\n+    \"inference\": {\n+        \"model_id\": \"a_model_for_inference\", <1>\n+        \"inference_config\": { <2>\n+            \"regression_config\": {\n+                \"num_top_feature_importance_values\": 2\n+            }\n+        },\n+        \"buckets_path\": {\n+            \"avg_cost\": \"avg_agg\", <3>\n+            \"max_cost\": \"max_agg\"\n+        }\n+    }\n+}\n+--------------------------------------------------\n+// NOTCONSOLE\n+<1> The ID of model to use.\n+<2> The optional inference config which overrides the model's default settings\n+<3> Map the value of `avg_agg` to the model's input field `avg_cost`\n+\n+[[inference-bucket-params]]\n+.`inference` Parameters\n+[options=\"header\"]\n+|===\n+|Parameter Name |Description |Required |Default Value\n+| `model_id`         | The ID of the model to load and infer against       | Required  | -\n+| `inference_config` | Contains the inference type and its options. There are two types: <<inference-agg-regression-opt,`regression`>> and <<inference-agg-classification-opt,`classification`>>  | Optional | -\n+| `buckets_path`     | Defines the paths to the input aggregations and maps the aggregation names to the field names expected by the model.\n+See <<buckets-path-syntax>> for more details | Required       | -\n+|===\n+\n+\n+==== Configuration options for Inference Models\n+The `inference_config` setting is optional and usaully isn't required as the pre-trained models come equipped with sensible defaults.\n+In the context of aggregations some options can overridden for each of the 2 types of model.\n+\n+[discrete]\n+[[inference-agg-regression-opt]]\n+===== Configuration options for {regression-cap} Models\n+\n+`num_top_feature_importance_values`::\n+(Optional, integer)\n+include::{es-repo-dir}/ml/ml-shared.asciidoc[tag=inference-config-regression-num-top-feature-importance-values]\n+\n+[discrete]\n+[[inference-agg-classification-opt]]\n+===== Configuration options for {classification-cap} Models", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e8c63896d7ed268c0bbdd74d7a084badac1ae76"}, "originalPosition": 61}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a855f1fc3dceac17c4775e46d0985881afd04eb7", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/a855f1fc3dceac17c4775e46d0985881afd04eb7", "committedDate": "2020-07-01T07:47:52Z", "message": "Update docs/reference/aggregations/pipeline/inference-bucket-aggregation.asciidoc\n\nCo-authored-by: Istv\u00e1n Zolt\u00e1n Szab\u00f3 <istvan.szabo@elastic.co>"}, "afterCommit": {"oid": "5c1c43a3ff92453719b7232a30160ece39064b4e", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/5c1c43a3ff92453719b7232a30160ece39064b4e", "committedDate": "2020-07-01T14:07:31Z", "message": "Update docs/reference/aggregations/pipeline/inference-bucket-aggregation.asciidoc\n\nCo-authored-by: Istv\u00e1n Zolt\u00e1n Szab\u00f3 <istvan.szabo@elastic.co>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQxMDkyNjA5", "url": "https://github.com/elastic/elasticsearch/pull/58193#pullrequestreview-441092609", "createdAt": "2020-07-01T18:09:49Z", "commit": {"oid": "a474b2e90b4ea1940bd717ad7e446251b3b829fc"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQxODowOTo1MFrOGrwW2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQxODowOTo1MFrOGrwW2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODUzNDIzNQ==", "bodyText": "I'm ok to merge this, but I'll work immediately on replacing it with a rewrite. I don't think we can release this code.", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r448534235", "createdAt": "2020-07-01T18:09:50Z", "author": {"login": "nik9000"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InferencePipelineAggregationBuilder.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.inference.aggs;\n+\n+import org.apache.lucene.util.SetOnce;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.LatchedActionListener;\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ClassificationConfig;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ClassificationConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.InferenceConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ResultsFieldUpdate;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.LocalModel;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.ModelLoadingService;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.TreeMap;\n+import java.util.concurrent.CountDownLatch;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.constructorArg;\n+\n+public class InferencePipelineAggregationBuilder extends AbstractPipelineAggregationBuilder<InferencePipelineAggregationBuilder> {\n+\n+    public static String NAME = \"inference\";\n+\n+    public static final ParseField MODEL_ID = new ParseField(\"model_id\");\n+    private static final ParseField INFERENCE_CONFIG = new ParseField(\"inference_config\");\n+\n+    static String AGGREGATIONS_RESULTS_FIELD = \"value\";\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private static final ConstructingObjectParser<InferencePipelineAggregationBuilder,\n+        Tuple<SetOnce<ModelLoadingService>, String>> PARSER = new ConstructingObjectParser<>(\n+        NAME, false,\n+        (args, context) -> new InferencePipelineAggregationBuilder(context.v2(), context.v1(), (Map<String, String>) args[0])\n+    );\n+\n+    static {\n+        PARSER.declareObject(constructorArg(), (p, c) -> p.mapStrings(), BUCKETS_PATH_FIELD);\n+        PARSER.declareString(InferencePipelineAggregationBuilder::setModelId, MODEL_ID);\n+        PARSER.declareNamedObject(InferencePipelineAggregationBuilder::setInferenceConfig,\n+            (p, c, n) -> p.namedObject(InferenceConfigUpdate.class, n, c), INFERENCE_CONFIG);\n+    }\n+\n+    private final Map<String, String> bucketPathMap;\n+    private String modelId;\n+    private InferenceConfigUpdate inferenceConfig;\n+    private final SetOnce<ModelLoadingService> modelLoadingService;\n+\n+    public static InferencePipelineAggregationBuilder parse(SetOnce<ModelLoadingService> modelLoadingService,\n+                                                            String pipelineAggregatorName,\n+                                                            XContentParser parser) {\n+        Tuple<SetOnce<ModelLoadingService>, String> context = new Tuple<>(modelLoadingService, pipelineAggregatorName);\n+        return PARSER.apply(parser, context);\n+    }\n+\n+    public InferencePipelineAggregationBuilder(String name, SetOnce<ModelLoadingService> modelLoadingService,\n+                                               Map<String, String> bucketsPath) {\n+        super(name, NAME, new TreeMap<>(bucketsPath).values().toArray(new String[] {}));\n+        this.modelLoadingService = modelLoadingService;\n+        this.bucketPathMap = bucketsPath;\n+    }\n+\n+    public InferencePipelineAggregationBuilder(StreamInput in, SetOnce<ModelLoadingService> modelLoadingService) throws IOException {\n+        super(in, NAME);\n+        modelId = in.readString();\n+        bucketPathMap = in.readMap(StreamInput::readString, StreamInput::readString);\n+        inferenceConfig = in.readOptionalNamedWriteable(InferenceConfigUpdate.class);\n+        this.modelLoadingService = modelLoadingService;\n+    }\n+\n+    void setModelId(String modelId) {\n+        this.modelId = modelId;\n+    }\n+\n+    void setInferenceConfig(InferenceConfigUpdate inferenceConfig) {\n+        this.inferenceConfig = inferenceConfig;\n+    }\n+\n+    @Override\n+    protected void validate(ValidationContext context) {\n+        context.validateHasParent(NAME, name);\n+        if (modelId == null) {\n+            context.addValidationError(\"[model_id] must be set\");\n+        }\n+\n+        if (inferenceConfig != null) {\n+            // error if the results field is set and not equal to the only acceptable value\n+            String resultsField = inferenceConfig.getResultsField();\n+            if (Strings.isNullOrEmpty(resultsField) == false && AGGREGATIONS_RESULTS_FIELD.equals(resultsField) == false) {\n+                context.addValidationError(\"setting option [\" + ClassificationConfig.RESULTS_FIELD.getPreferredName()\n+                    + \"] to [\" + resultsField + \"] is not valid for inference aggregations\");\n+            }\n+\n+            if (inferenceConfig instanceof ClassificationConfigUpdate) {\n+                ClassificationConfigUpdate classUpdate = (ClassificationConfigUpdate)inferenceConfig;\n+\n+                // error if the top classes result field is set and not equal to the only acceptable value\n+                String topClassesField = classUpdate.getTopClassesResultsField();\n+                if (Strings.isNullOrEmpty(topClassesField) == false &&\n+                    ClassificationConfig.DEFAULT_TOP_CLASSES_RESULTS_FIELD.equals(topClassesField) == false) {\n+                    context.addValidationError(\"setting option [\" + ClassificationConfig.DEFAULT_TOP_CLASSES_RESULTS_FIELD\n+                        + \"] to [\" + topClassesField + \"] is not valid for inference aggregations\");\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected void doWriteTo(StreamOutput out) throws IOException {\n+        out.writeString(modelId);\n+        out.writeMap(bucketPathMap, StreamOutput::writeString, StreamOutput::writeString);\n+        out.writeOptionalNamedWriteable(inferenceConfig);\n+    }\n+\n+    @Override\n+    protected PipelineAggregator createInternal(Map<String, Object> metaData) {\n+\n+        SetOnce<LocalModel> model = new SetOnce<>();\n+        SetOnce<Exception> error = new SetOnce<>();\n+        CountDownLatch latch = new CountDownLatch(1);\n+        ActionListener<LocalModel> listener = new LatchedActionListener<>(\n+            ActionListener.wrap(model::set, error::set), latch);\n+\n+        modelLoadingService.get().getModelForSearch(modelId, listener);\n+        try {\n+            // TODO Avoid the blocking wait\n+            latch.await();\n+        } catch (InterruptedException e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a474b2e90b4ea1940bd717ad7e446251b3b829fc"}, "originalPosition": 144}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dda58a2b05e9760534b9ba380868668ee2675929", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/dda58a2b05e9760534b9ba380868668ee2675929", "committedDate": "2020-07-02T10:42:38Z", "message": "inference pipeline agg classes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a89424875a135cf32953121962e0f1dc7d6313dd", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/a89424875a135cf32953121962e0f1dc7d6313dd", "committedDate": "2020-07-02T10:42:38Z", "message": "Add empty config update\n\n# Conflicts:\n#\tx-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InferencePipelineAggregationBuilder.java"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a0fbf7538693f56db3ab77ae43d8e0d1d5c67033", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/a0fbf7538693f56db3ab77ae43d8e0d1d5c67033", "committedDate": "2020-07-02T10:42:38Z", "message": "Add yml test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4682eab2e15e47875e5e004b7fd52512559dbed1", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/4682eab2e15e47875e5e004b7fd52512559dbed1", "committedDate": "2020-07-02T10:42:38Z", "message": "Add InternalInferenceAggregation"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "32f49091b2bef1835246676aa80030c577b3c13a", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/32f49091b2bef1835246676aa80030c577b3c13a", "committedDate": "2020-07-02T10:42:38Z", "message": "Fixes for types that aren\u2019t a double"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1eb6f5feda6941bb8bf7cab29fc30df88c0fa6e4", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/1eb6f5feda6941bb8bf7cab29fc30df88c0fa6e4", "committedDate": "2020-07-02T10:42:38Z", "message": "Implement getProperty"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c26c08b8acbaec2cd5f1b5fbd1656b695bb36b78", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/c26c08b8acbaec2cd5f1b5fbd1656b695bb36b78", "committedDate": "2020-07-02T10:42:38Z", "message": "Some tidying up"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1196d46d5e89eeee47bd9a399cd7793201635f0c", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/1196d46d5e89eeee47bd9a399cd7793201635f0c", "committedDate": "2020-07-02T10:42:38Z", "message": "Insert zeros gap policy is not valid"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d494d268555b5e139073c059bfcb6c3b092275b0", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/d494d268555b5e139073c059bfcb6c3b092275b0", "committedDate": "2020-07-02T10:42:38Z", "message": "rework skip bucket"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b3a235cf1c2ca6ab6c2f5553c2b1704604c0d26d", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/b3a235cf1c2ca6ab6c2f5553c2b1704604c0d26d", "committedDate": "2020-07-02T10:42:39Z", "message": "Make inference results implement toxcontent\n\n# Conflicts:\n#\tx-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ml/inference/results/SingleValueInferenceResults.java\n#\tx-pack/plugin/core/src/test/java/org/elasticsearch/xpack/core/ml/inference/results/WarningInferenceResultsTests.java\n#\tx-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InternalInferenceAggregation.java"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "507f3878656239e8f8d8f3fe87cd549ed9989285", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/507f3878656239e8f8d8f3fe87cd549ed9989285", "committedDate": "2020-07-02T10:42:39Z", "message": "Add Internal Inference agg tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f322c972d5bc4a1d1f4985750d6cc06b87488048", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/f322c972d5bc4a1d1f4985750d6cc06b87488048", "committedDate": "2020-07-02T10:42:39Z", "message": "Handle bucket terms"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "440d7c6453d632179a9c1afeae6f2eca70a35a6d", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/440d7c6453d632179a9c1afeae6f2eca70a35a6d", "committedDate": "2020-07-02T10:42:39Z", "message": "Delete broken test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4b609e811feedf16a1f439aafb35877311109c2b", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/4b609e811feedf16a1f439aafb35877311109c2b", "committedDate": "2020-07-02T10:42:39Z", "message": "precommit"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c3e943a0e37a4eede8396b7c4bf5018b0bfaf40d", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/c3e943a0e37a4eede8396b7c4bf5018b0bfaf40d", "committedDate": "2020-07-02T10:42:39Z", "message": "Fix hashcode and tidy up"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "de4f8e6e0a2c18f2f48096798c4881356292f70f", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/de4f8e6e0a2c18f2f48096798c4881356292f70f", "committedDate": "2020-07-02T10:42:39Z", "message": "checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e67cae3f68e6dece4a0a5ccf9a22a80c555dcb35", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/e67cae3f68e6dece4a0a5ccf9a22a80c555dcb35", "committedDate": "2020-07-02T10:42:39Z", "message": "black list rest tests from security tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c4848043c2dc2bfda4c943038e183c8f1c8312d5", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/c4848043c2dc2bfda4c943038e183c8f1c8312d5", "committedDate": "2020-07-02T10:42:39Z", "message": "Fix ml with security yml tests\n\nCheck an ml endpoint was called"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "158b99a674a4232b9c039e4c924ad4ab418b8583", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/158b99a674a4232b9c039e4c924ad4ab418b8583", "committedDate": "2020-07-02T10:42:39Z", "message": "un block ml with security tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "011e6f5a525c4f460c7ea625552992160452bfd5", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/011e6f5a525c4f460c7ea625552992160452bfd5", "committedDate": "2020-07-02T10:42:39Z", "message": "Don't swallow interrupt"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1c793d0cbeac14922aa1d6ac127aab39e776db30", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/1c793d0cbeac14922aa1d6ac127aab39e776db30", "committedDate": "2020-07-02T10:42:39Z", "message": "Move config unique field name check"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "db8133a8dda7ae573810df32fd895615784b4f2e", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/db8133a8dda7ae573810df32fd895615784b4f2e", "committedDate": "2020-07-02T10:42:39Z", "message": "Do not allow setting the results_field in agg"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c44b1e4873ed38a316e8483443f44343ad4107b5", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/c44b1e4873ed38a316e8483443f44343ad4107b5", "committedDate": "2020-07-02T10:42:39Z", "message": "Results field update WIP"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8f3d57e81e5f3e3d71c351f95a8cefbda732a581", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/8f3d57e81e5f3e3d71c351f95a8cefbda732a581", "committedDate": "2020-07-02T10:42:39Z", "message": "Revert \"Results field update WIP\"\n\nThis reverts commit 2b00456f18105fec0f8d670468f12df5d72d63f0."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "da5c99ebb8e4661f73a076cd478fc92d1ead0dd9", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/da5c99ebb8e4661f73a076cd478fc92d1ead0dd9", "committedDate": "2020-07-02T10:42:39Z", "message": "Add results field update"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e3c33a61f30c7afed0decf71dc1c0bb108fe7847", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/e3c33a61f30c7afed0decf71dc1c0bb108fe7847", "committedDate": "2020-07-02T10:42:39Z", "message": "delete unused update class"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e1119d01ad1e3f403a662050ac965a95e509d3c6", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/e1119d01ad1e3f403a662050ac965a95e509d3c6", "committedDate": "2020-07-02T10:42:39Z", "message": "Extract TopClassEntry"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b7631cb47b821b96db6db4178e2ae079b318967e", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/b7631cb47b821b96db6db4178e2ae079b318967e", "committedDate": "2020-07-02T10:42:39Z", "message": "Fix serialisation test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f9b136ff8fddc0dd11ae16cab35478a95044dc69", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/f9b136ff8fddc0dd11ae16cab35478a95044dc69", "committedDate": "2020-07-02T10:42:39Z", "message": "Feature importance parser"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1033b48bba31663b8b26b0222bc52857bd13f2f4", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/1033b48bba31663b8b26b0222bc52857bd13f2f4", "committedDate": "2020-07-02T10:42:39Z", "message": "Parsed Agg"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4bbc76125747121c101e684a0b259987c0e0f45d", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/4bbc76125747121c101e684a0b259987c0e0f45d", "committedDate": "2020-07-02T10:42:39Z", "message": "Remove defaults in config update"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ac2b2ac66cc531e1adc1e1902ec26f14f124b430", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/ac2b2ac66cc531e1adc1e1902ec26f14f124b430", "committedDate": "2020-07-02T10:42:39Z", "message": "Remove getProperty method on results"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fad3e1484f0d58341ea7239267895ef901ec91a0", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/fad3e1484f0d58341ea7239267895ef901ec91a0", "committedDate": "2020-07-02T10:42:39Z", "message": "Reject setting the top_classes field"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e6f8bf60e9bd045c2f283f516514a33beb5a0ee5", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/e6f8bf60e9bd045c2f283f516514a33beb5a0ee5", "committedDate": "2020-07-02T10:42:39Z", "message": "tidy up"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9e5fafdda827fc1fe2f84107d241f267f20178dc", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/9e5fafdda827fc1fe2f84107d241f267f20178dc", "committedDate": "2020-07-02T10:42:39Z", "message": "checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "756e8a08724d9d07032514c46739c79cce2f2ff5", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/756e8a08724d9d07032514c46739c79cce2f2ff5", "committedDate": "2020-07-02T10:42:39Z", "message": "Add predictedValue method"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1c499d503d46772cbc76ad4bcbe581cc8114db0f", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/1c499d503d46772cbc76ad4bcbe581cc8114db0f", "committedDate": "2020-07-02T10:42:39Z", "message": "re work the awful duplicate method"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "27e2f19a512bf60a6351b8339fb431240f455fcd", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/27e2f19a512bf60a6351b8339fb431240f455fcd", "committedDate": "2020-07-02T10:42:39Z", "message": "Move validation to the validate method"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ab80263c26896eb07d5556f30b5efecd8c2a70f6", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/ab80263c26896eb07d5556f30b5efecd8c2a70f6", "committedDate": "2020-07-02T10:42:40Z", "message": "tidy up"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "14184fa9cdab966ed0757a2aaa14aa10283a053d", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/14184fa9cdab966ed0757a2aaa14aa10283a053d", "committedDate": "2020-07-02T10:42:40Z", "message": "Remove the GapPolicy"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "04444b7812db207e0c910aa01dd2877cb968fe91", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/04444b7812db207e0c910aa01dd2877cb968fe91", "committedDate": "2020-07-02T10:42:40Z", "message": "Fix compilation after rebase"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "113b0dddef698964b7911a0c748c216f09f19b27", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/113b0dddef698964b7911a0c748c216f09f19b27", "committedDate": "2020-07-02T10:42:40Z", "message": "First pass at docs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4c08a66536aa2657bca24695224432f2c154839c", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/4c08a66536aa2657bca24695224432f2c154839c", "committedDate": "2020-07-02T10:42:40Z", "message": "Update docs/reference/aggregations/pipeline/inference-bucket-aggregation.asciidoc\n\nCo-authored-by: Istv\u00e1n Zolt\u00e1n Szab\u00f3 <istvan.szabo@elastic.co>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "153d3997be94c25a9fef0d2aa8699fe1f8ed65e1", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/153d3997be94c25a9fef0d2aa8699fe1f8ed65e1", "committedDate": "2020-07-02T10:42:40Z", "message": "Update docs/reference/aggregations/pipeline/inference-bucket-aggregation.asciidoc\n\nCo-authored-by: Istv\u00e1n Zolt\u00e1n Szab\u00f3 <istvan.szabo@elastic.co>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a9b637f4afeae1d99e83c7cf07e0cbe19cd7ad6b", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/a9b637f4afeae1d99e83c7cf07e0cbe19cd7ad6b", "committedDate": "2020-07-02T10:42:40Z", "message": "Update docs/reference/aggregations/pipeline/inference-bucket-aggregation.asciidoc\n\nCo-authored-by: Istv\u00e1n Zolt\u00e1n Szab\u00f3 <istvan.szabo@elastic.co>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e32457a6453a6f01b6b3aa99151ee5681e74fa52", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/e32457a6453a6f01b6b3aa99151ee5681e74fa52", "committedDate": "2020-07-02T10:42:40Z", "message": "Update docs/reference/aggregations/pipeline/inference-bucket-aggregation.asciidoc\n\nCo-authored-by: Istv\u00e1n Zolt\u00e1n Szab\u00f3 <istvan.szabo@elastic.co>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8cdd8ae61e9ba55106aa429f780497be9a8f638d", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/8cdd8ae61e9ba55106aa429f780497be9a8f638d", "committedDate": "2020-07-02T10:42:40Z", "message": "Resolve conflict"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a474b2e90b4ea1940bd717ad7e446251b3b829fc", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/a474b2e90b4ea1940bd717ad7e446251b3b829fc", "committedDate": "2020-07-01T14:38:07Z", "message": "Resolve conflict"}, "afterCommit": {"oid": "8cdd8ae61e9ba55106aa429f780497be9a8f638d", "author": {"user": {"login": "davidkyle", "name": "David Kyle"}}, "url": "https://github.com/elastic/elasticsearch/commit/8cdd8ae61e9ba55106aa429f780497be9a8f638d", "committedDate": "2020-07-02T10:42:40Z", "message": "Resolve conflict"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 545, "cost": 1, "resetAt": "2021-10-28T18:54:27Z"}}}