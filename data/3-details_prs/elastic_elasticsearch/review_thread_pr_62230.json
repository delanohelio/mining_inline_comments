{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDgzODk5NjMx", "number": 62230, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxNzoxNDoxOVrOEiNw4A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQxNTo1NzozM1rOEjNS-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0MzEyNTQ0OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/fetch/subphase/highlight/FastVectorHighlighter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxNzoxNDoxOVrOHP96SA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xM1QxMDo0NzowNFrOHQ7cvA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjUwNTAzMg==", "bodyText": "Double-checking that none of this logic changed, you just removed the try/ catch? The git diff is probably just unintuitive.", "url": "https://github.com/elastic/elasticsearch/pull/62230#discussion_r486505032", "createdAt": "2020-09-10T17:14:19Z", "author": {"login": "jtibshirani"}, "path": "server/src/main/java/org/elasticsearch/search/fetch/subphase/highlight/FastVectorHighlighter.java", "diffHunk": "@@ -87,126 +86,119 @@ public HighlightField highlight(FieldHighlightContext fieldContext) {\n             hitContext.cache().put(CACHE_KEY, new HighlighterEntry());\n         }\n         HighlighterEntry cache = (HighlighterEntry) hitContext.cache().get(CACHE_KEY);\n-\n-        try {\n-            FieldHighlightEntry entry = cache.fields.get(fieldType);\n-            if (entry == null) {\n-                FragListBuilder fragListBuilder;\n-                BaseFragmentsBuilder fragmentsBuilder;\n-\n-                final BoundaryScanner boundaryScanner = getBoundaryScanner(field);\n-                if (field.fieldOptions().numberOfFragments() == 0) {\n-                    fragListBuilder = new SingleFragListBuilder();\n-\n+        FieldHighlightEntry entry = cache.fields.get(fieldType);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7966f1e4c2baedca0f5252745f5876cdf8fdfac5"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzUxMzI3Ng==", "bodyText": "Yes, that's correct.", "url": "https://github.com/elastic/elasticsearch/pull/62230#discussion_r487513276", "createdAt": "2020-09-13T10:47:04Z", "author": {"login": "romseygeek"}, "path": "server/src/main/java/org/elasticsearch/search/fetch/subphase/highlight/FastVectorHighlighter.java", "diffHunk": "@@ -87,126 +86,119 @@ public HighlightField highlight(FieldHighlightContext fieldContext) {\n             hitContext.cache().put(CACHE_KEY, new HighlighterEntry());\n         }\n         HighlighterEntry cache = (HighlighterEntry) hitContext.cache().get(CACHE_KEY);\n-\n-        try {\n-            FieldHighlightEntry entry = cache.fields.get(fieldType);\n-            if (entry == null) {\n-                FragListBuilder fragListBuilder;\n-                BaseFragmentsBuilder fragmentsBuilder;\n-\n-                final BoundaryScanner boundaryScanner = getBoundaryScanner(field);\n-                if (field.fieldOptions().numberOfFragments() == 0) {\n-                    fragListBuilder = new SingleFragListBuilder();\n-\n+        FieldHighlightEntry entry = cache.fields.get(fieldType);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjUwNTAzMg=="}, "originalCommit": {"oid": "7966f1e4c2baedca0f5252745f5876cdf8fdfac5"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0MzE2NTI1OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/fetch/subphase/highlight/PlainHighlighter.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxNzoyNToxOFrOHP-Tvg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xM1QxMDo0ODo0MlrOHQ7dOw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjUxMTU1MA==", "bodyText": "Does this change the behavior? I'm not sure if we previously swallowed InvalidTokenOffsetsException.", "url": "https://github.com/elastic/elasticsearch/pull/62230#discussion_r486511550", "createdAt": "2020-09-10T17:25:18Z", "author": {"login": "jtibshirani"}, "path": "server/src/main/java/org/elasticsearch/search/fetch/subphase/highlight/PlainHighlighter.java", "diffHunk": "@@ -111,56 +109,42 @@ public HighlightField highlight(FieldHighlightContext fieldContext) {\n         }\n         final int maxAnalyzedOffset = context.getIndexSettings().getHighlightMaxAnalyzedOffset();\n \n-        try {\n-            textsToHighlight = HighlightUtils.loadFieldValues(fieldType, hitContext, fieldContext.forceSource);\n+        textsToHighlight = HighlightUtils.loadFieldValues(fieldType, hitContext, fieldContext.forceSource);\n \n-            for (Object textToHighlight : textsToHighlight) {\n-                String text = convertFieldValue(fieldType, textToHighlight);\n-                int textLength = text.length();\n-                if (keywordIgnoreAbove != null  && textLength > keywordIgnoreAbove) {\n-                    continue; // skip highlighting keyword terms that were ignored during indexing\n-                }\n-                if (textLength > maxAnalyzedOffset) {\n-                    throw new IllegalArgumentException(\n-                        \"The length of [\" + fieldContext.fieldName + \"] field of [\" + hitContext.hit().getId() +\n-                            \"] doc of [\" + context.index().getName() + \"] index \" +\n-                            \"has exceeded [\" + maxAnalyzedOffset + \"] - maximum allowed to be analyzed for highlighting. \" +\n-                            \"This maximum can be set by changing the [\" + IndexSettings.MAX_ANALYZED_OFFSET_SETTING.getKey() +\n-                            \"] index level setting. \" + \"For large texts, indexing with offsets or term vectors, and highlighting \" +\n-                            \"with unified or fvh highlighter is recommended!\");\n-                }\n+        for (Object textToHighlight : textsToHighlight) {\n+            String text = convertFieldValue(fieldType, textToHighlight);\n+            int textLength = text.length();\n+            if (keywordIgnoreAbove != null && textLength > keywordIgnoreAbove) {\n+                continue; // skip highlighting keyword terms that were ignored during indexing\n+            }\n+            if (textLength > maxAnalyzedOffset) {\n+                throw new IllegalArgumentException(\n+                    \"The length of [\" + fieldContext.fieldName + \"] field of [\" + hitContext.hit().getId() +\n+                        \"] doc of [\" + context.index().getName() + \"] index \" +\n+                        \"has exceeded [\" + maxAnalyzedOffset + \"] - maximum allowed to be analyzed for highlighting. \" +\n+                        \"This maximum can be set by changing the [\" + IndexSettings.MAX_ANALYZED_OFFSET_SETTING.getKey() +\n+                        \"] index level setting. \" + \"For large texts, indexing with offsets or term vectors, and highlighting \" +\n+                        \"with unified or fvh highlighter is recommended!\");\n+            }\n \n-                try (TokenStream tokenStream = analyzer.tokenStream(fieldType.name(), text)) {\n-                    if (!tokenStream.hasAttribute(CharTermAttribute.class) || !tokenStream.hasAttribute(OffsetAttribute.class)) {\n-                        // can't perform highlighting if the stream has no terms (binary token stream) or no offsets\n-                        continue;\n-                    }\n-                    TextFragment[] bestTextFragments = entry.getBestTextFragments(tokenStream, text, false, numberOfFragments);\n-                    for (TextFragment bestTextFragment : bestTextFragments) {\n-                        if (bestTextFragment != null && bestTextFragment.getScore() > 0) {\n-                            fragsList.add(bestTextFragment);\n-                        }\n+            try (TokenStream tokenStream = analyzer.tokenStream(fieldType.name(), text)) {\n+                if (!tokenStream.hasAttribute(CharTermAttribute.class) || !tokenStream.hasAttribute(OffsetAttribute.class)) {\n+                    // can't perform highlighting if the stream has no terms (binary token stream) or no offsets\n+                    continue;\n+                }\n+                TextFragment[] bestTextFragments = entry.getBestTextFragments(tokenStream, text, false, numberOfFragments);\n+                for (TextFragment bestTextFragment : bestTextFragments) {\n+                    if (bestTextFragment != null && bestTextFragment.getScore() > 0) {\n+                        fragsList.add(bestTextFragment);\n                     }\n                 }\n-            }\n-        } catch (Exception e) {\n-            if (ExceptionsHelper.unwrap(e, BytesRefHash.MaxBytesLengthExceededException.class) != null) {\n-                // this can happen if for example a field is not_analyzed and ignore_above option is set.\n-                // the field will be ignored when indexing but the huge term is still in the source and\n-                // the plain highlighter will parse the source and try to analyze it.\n-                return null;\n-            } else {\n-                throw new FetchPhaseExecutionException(fieldContext.shardTarget,\n-                    \"Failed to highlight field [\" + fieldContext.fieldName + \"]\", e);\n+            } catch (InvalidTokenOffsetsException | BytesRefHash.MaxBytesLengthExceededException e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7966f1e4c2baedca0f5252745f5876cdf8fdfac5"}, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzUxMzQwMw==", "bodyText": "Well spotted, we need to rethrow InvalidTokenOffsetsException.  I've changed the outer loop in FetchPhase to catch generic Exceptions and rethrow them as FetchPhaseExecutionExceptions, so that we also properly handle this case as well as various other places in highlighting code that throw IllegalArgumentException.  There's a lot more cleanup that can be done around the highlighters, I think!", "url": "https://github.com/elastic/elasticsearch/pull/62230#discussion_r487513403", "createdAt": "2020-09-13T10:48:42Z", "author": {"login": "romseygeek"}, "path": "server/src/main/java/org/elasticsearch/search/fetch/subphase/highlight/PlainHighlighter.java", "diffHunk": "@@ -111,56 +109,42 @@ public HighlightField highlight(FieldHighlightContext fieldContext) {\n         }\n         final int maxAnalyzedOffset = context.getIndexSettings().getHighlightMaxAnalyzedOffset();\n \n-        try {\n-            textsToHighlight = HighlightUtils.loadFieldValues(fieldType, hitContext, fieldContext.forceSource);\n+        textsToHighlight = HighlightUtils.loadFieldValues(fieldType, hitContext, fieldContext.forceSource);\n \n-            for (Object textToHighlight : textsToHighlight) {\n-                String text = convertFieldValue(fieldType, textToHighlight);\n-                int textLength = text.length();\n-                if (keywordIgnoreAbove != null  && textLength > keywordIgnoreAbove) {\n-                    continue; // skip highlighting keyword terms that were ignored during indexing\n-                }\n-                if (textLength > maxAnalyzedOffset) {\n-                    throw new IllegalArgumentException(\n-                        \"The length of [\" + fieldContext.fieldName + \"] field of [\" + hitContext.hit().getId() +\n-                            \"] doc of [\" + context.index().getName() + \"] index \" +\n-                            \"has exceeded [\" + maxAnalyzedOffset + \"] - maximum allowed to be analyzed for highlighting. \" +\n-                            \"This maximum can be set by changing the [\" + IndexSettings.MAX_ANALYZED_OFFSET_SETTING.getKey() +\n-                            \"] index level setting. \" + \"For large texts, indexing with offsets or term vectors, and highlighting \" +\n-                            \"with unified or fvh highlighter is recommended!\");\n-                }\n+        for (Object textToHighlight : textsToHighlight) {\n+            String text = convertFieldValue(fieldType, textToHighlight);\n+            int textLength = text.length();\n+            if (keywordIgnoreAbove != null && textLength > keywordIgnoreAbove) {\n+                continue; // skip highlighting keyword terms that were ignored during indexing\n+            }\n+            if (textLength > maxAnalyzedOffset) {\n+                throw new IllegalArgumentException(\n+                    \"The length of [\" + fieldContext.fieldName + \"] field of [\" + hitContext.hit().getId() +\n+                        \"] doc of [\" + context.index().getName() + \"] index \" +\n+                        \"has exceeded [\" + maxAnalyzedOffset + \"] - maximum allowed to be analyzed for highlighting. \" +\n+                        \"This maximum can be set by changing the [\" + IndexSettings.MAX_ANALYZED_OFFSET_SETTING.getKey() +\n+                        \"] index level setting. \" + \"For large texts, indexing with offsets or term vectors, and highlighting \" +\n+                        \"with unified or fvh highlighter is recommended!\");\n+            }\n \n-                try (TokenStream tokenStream = analyzer.tokenStream(fieldType.name(), text)) {\n-                    if (!tokenStream.hasAttribute(CharTermAttribute.class) || !tokenStream.hasAttribute(OffsetAttribute.class)) {\n-                        // can't perform highlighting if the stream has no terms (binary token stream) or no offsets\n-                        continue;\n-                    }\n-                    TextFragment[] bestTextFragments = entry.getBestTextFragments(tokenStream, text, false, numberOfFragments);\n-                    for (TextFragment bestTextFragment : bestTextFragments) {\n-                        if (bestTextFragment != null && bestTextFragment.getScore() > 0) {\n-                            fragsList.add(bestTextFragment);\n-                        }\n+            try (TokenStream tokenStream = analyzer.tokenStream(fieldType.name(), text)) {\n+                if (!tokenStream.hasAttribute(CharTermAttribute.class) || !tokenStream.hasAttribute(OffsetAttribute.class)) {\n+                    // can't perform highlighting if the stream has no terms (binary token stream) or no offsets\n+                    continue;\n+                }\n+                TextFragment[] bestTextFragments = entry.getBestTextFragments(tokenStream, text, false, numberOfFragments);\n+                for (TextFragment bestTextFragment : bestTextFragments) {\n+                    if (bestTextFragment != null && bestTextFragment.getScore() > 0) {\n+                        fragsList.add(bestTextFragment);\n                     }\n                 }\n-            }\n-        } catch (Exception e) {\n-            if (ExceptionsHelper.unwrap(e, BytesRefHash.MaxBytesLengthExceededException.class) != null) {\n-                // this can happen if for example a field is not_analyzed and ignore_above option is set.\n-                // the field will be ignored when indexing but the huge term is still in the source and\n-                // the plain highlighter will parse the source and try to analyze it.\n-                return null;\n-            } else {\n-                throw new FetchPhaseExecutionException(fieldContext.shardTarget,\n-                    \"Failed to highlight field [\" + fieldContext.fieldName + \"]\", e);\n+            } catch (InvalidTokenOffsetsException | BytesRefHash.MaxBytesLengthExceededException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjUxMTU1MA=="}, "originalCommit": {"oid": "7966f1e4c2baedca0f5252745f5876cdf8fdfac5"}, "originalPosition": 106}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA1MzUzNDY3OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/fetch/subphase/highlight/PlainHighlighter.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQxNTo1NzozM1rOHRb6OQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQxNjoyOToyMlrOHRdLpw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODA0NTExMw==", "bodyText": "Sorry if I'm missing something, why does InvalidTokenOffsetsException need special treatment?\nAlso maybe we can keep the comment explaining why we're skipping BytesRefHash.MaxBytesLengthExceededException?", "url": "https://github.com/elastic/elasticsearch/pull/62230#discussion_r488045113", "createdAt": "2020-09-14T15:57:33Z", "author": {"login": "jtibshirani"}, "path": "server/src/main/java/org/elasticsearch/search/fetch/subphase/highlight/PlainHighlighter.java", "diffHunk": "@@ -111,56 +109,44 @@ public HighlightField highlight(FieldHighlightContext fieldContext) {\n         }\n         final int maxAnalyzedOffset = context.getIndexSettings().getHighlightMaxAnalyzedOffset();\n \n-        try {\n-            textsToHighlight = HighlightUtils.loadFieldValues(fieldType, hitContext, fieldContext.forceSource);\n+        textsToHighlight = HighlightUtils.loadFieldValues(fieldType, hitContext, fieldContext.forceSource);\n \n-            for (Object textToHighlight : textsToHighlight) {\n-                String text = convertFieldValue(fieldType, textToHighlight);\n-                int textLength = text.length();\n-                if (keywordIgnoreAbove != null  && textLength > keywordIgnoreAbove) {\n-                    continue; // skip highlighting keyword terms that were ignored during indexing\n-                }\n-                if (textLength > maxAnalyzedOffset) {\n-                    throw new IllegalArgumentException(\n-                        \"The length of [\" + fieldContext.fieldName + \"] field of [\" + hitContext.hit().getId() +\n-                            \"] doc of [\" + context.index().getName() + \"] index \" +\n-                            \"has exceeded [\" + maxAnalyzedOffset + \"] - maximum allowed to be analyzed for highlighting. \" +\n-                            \"This maximum can be set by changing the [\" + IndexSettings.MAX_ANALYZED_OFFSET_SETTING.getKey() +\n-                            \"] index level setting. \" + \"For large texts, indexing with offsets or term vectors, and highlighting \" +\n-                            \"with unified or fvh highlighter is recommended!\");\n-                }\n+        for (Object textToHighlight : textsToHighlight) {\n+            String text = convertFieldValue(fieldType, textToHighlight);\n+            int textLength = text.length();\n+            if (keywordIgnoreAbove != null && textLength > keywordIgnoreAbove) {\n+                continue; // skip highlighting keyword terms that were ignored during indexing\n+            }\n+            if (textLength > maxAnalyzedOffset) {\n+                throw new IllegalArgumentException(\n+                    \"The length of [\" + fieldContext.fieldName + \"] field of [\" + hitContext.hit().getId() +\n+                        \"] doc of [\" + context.index().getName() + \"] index \" +\n+                        \"has exceeded [\" + maxAnalyzedOffset + \"] - maximum allowed to be analyzed for highlighting. \" +\n+                        \"This maximum can be set by changing the [\" + IndexSettings.MAX_ANALYZED_OFFSET_SETTING.getKey() +\n+                        \"] index level setting. \" + \"For large texts, indexing with offsets or term vectors, and highlighting \" +\n+                        \"with unified or fvh highlighter is recommended!\");\n+            }\n \n-                try (TokenStream tokenStream = analyzer.tokenStream(fieldType.name(), text)) {\n-                    if (!tokenStream.hasAttribute(CharTermAttribute.class) || !tokenStream.hasAttribute(OffsetAttribute.class)) {\n-                        // can't perform highlighting if the stream has no terms (binary token stream) or no offsets\n-                        continue;\n-                    }\n-                    TextFragment[] bestTextFragments = entry.getBestTextFragments(tokenStream, text, false, numberOfFragments);\n-                    for (TextFragment bestTextFragment : bestTextFragments) {\n-                        if (bestTextFragment != null && bestTextFragment.getScore() > 0) {\n-                            fragsList.add(bestTextFragment);\n-                        }\n+            try (TokenStream tokenStream = analyzer.tokenStream(fieldType.name(), text)) {\n+                if (!tokenStream.hasAttribute(CharTermAttribute.class) || !tokenStream.hasAttribute(OffsetAttribute.class)) {\n+                    // can't perform highlighting if the stream has no terms (binary token stream) or no offsets\n+                    continue;\n+                }\n+                TextFragment[] bestTextFragments = entry.getBestTextFragments(tokenStream, text, false, numberOfFragments);\n+                for (TextFragment bestTextFragment : bestTextFragments) {\n+                    if (bestTextFragment != null && bestTextFragment.getScore() > 0) {\n+                        fragsList.add(bestTextFragment);\n                     }\n                 }\n-            }\n-        } catch (Exception e) {\n-            if (ExceptionsHelper.unwrap(e, BytesRefHash.MaxBytesLengthExceededException.class) != null) {\n-                // this can happen if for example a field is not_analyzed and ignore_above option is set.\n-                // the field will be ignored when indexing but the huge term is still in the source and\n-                // the plain highlighter will parse the source and try to analyze it.\n-                return null;\n-            } else {\n-                throw new FetchPhaseExecutionException(fieldContext.shardTarget,\n-                    \"Failed to highlight field [\" + fieldContext.fieldName + \"]\", e);\n+            } catch (BytesRefHash.MaxBytesLengthExceededException e) {\n+                // ignore and continue to the next value", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "25051501b4327146d84397d8189429028e444018"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODA2Mzg0MA==", "bodyText": "We need special treatment because Highlighter#getBestTextFragments is declared as throwing InvalidTokenOffsetsException, which just extends plain Exception.  It doesn't feel correct rethrowing as an IOException (because it isn't one!) so wrapping in an IllegalArgumentException seems like a happy compromise.  It will be caught and translated to a FetchPhaseExecutionException by FetchPhase.\n++ to add the explanatory comment back", "url": "https://github.com/elastic/elasticsearch/pull/62230#discussion_r488063840", "createdAt": "2020-09-14T16:25:55Z", "author": {"login": "romseygeek"}, "path": "server/src/main/java/org/elasticsearch/search/fetch/subphase/highlight/PlainHighlighter.java", "diffHunk": "@@ -111,56 +109,44 @@ public HighlightField highlight(FieldHighlightContext fieldContext) {\n         }\n         final int maxAnalyzedOffset = context.getIndexSettings().getHighlightMaxAnalyzedOffset();\n \n-        try {\n-            textsToHighlight = HighlightUtils.loadFieldValues(fieldType, hitContext, fieldContext.forceSource);\n+        textsToHighlight = HighlightUtils.loadFieldValues(fieldType, hitContext, fieldContext.forceSource);\n \n-            for (Object textToHighlight : textsToHighlight) {\n-                String text = convertFieldValue(fieldType, textToHighlight);\n-                int textLength = text.length();\n-                if (keywordIgnoreAbove != null  && textLength > keywordIgnoreAbove) {\n-                    continue; // skip highlighting keyword terms that were ignored during indexing\n-                }\n-                if (textLength > maxAnalyzedOffset) {\n-                    throw new IllegalArgumentException(\n-                        \"The length of [\" + fieldContext.fieldName + \"] field of [\" + hitContext.hit().getId() +\n-                            \"] doc of [\" + context.index().getName() + \"] index \" +\n-                            \"has exceeded [\" + maxAnalyzedOffset + \"] - maximum allowed to be analyzed for highlighting. \" +\n-                            \"This maximum can be set by changing the [\" + IndexSettings.MAX_ANALYZED_OFFSET_SETTING.getKey() +\n-                            \"] index level setting. \" + \"For large texts, indexing with offsets or term vectors, and highlighting \" +\n-                            \"with unified or fvh highlighter is recommended!\");\n-                }\n+        for (Object textToHighlight : textsToHighlight) {\n+            String text = convertFieldValue(fieldType, textToHighlight);\n+            int textLength = text.length();\n+            if (keywordIgnoreAbove != null && textLength > keywordIgnoreAbove) {\n+                continue; // skip highlighting keyword terms that were ignored during indexing\n+            }\n+            if (textLength > maxAnalyzedOffset) {\n+                throw new IllegalArgumentException(\n+                    \"The length of [\" + fieldContext.fieldName + \"] field of [\" + hitContext.hit().getId() +\n+                        \"] doc of [\" + context.index().getName() + \"] index \" +\n+                        \"has exceeded [\" + maxAnalyzedOffset + \"] - maximum allowed to be analyzed for highlighting. \" +\n+                        \"This maximum can be set by changing the [\" + IndexSettings.MAX_ANALYZED_OFFSET_SETTING.getKey() +\n+                        \"] index level setting. \" + \"For large texts, indexing with offsets or term vectors, and highlighting \" +\n+                        \"with unified or fvh highlighter is recommended!\");\n+            }\n \n-                try (TokenStream tokenStream = analyzer.tokenStream(fieldType.name(), text)) {\n-                    if (!tokenStream.hasAttribute(CharTermAttribute.class) || !tokenStream.hasAttribute(OffsetAttribute.class)) {\n-                        // can't perform highlighting if the stream has no terms (binary token stream) or no offsets\n-                        continue;\n-                    }\n-                    TextFragment[] bestTextFragments = entry.getBestTextFragments(tokenStream, text, false, numberOfFragments);\n-                    for (TextFragment bestTextFragment : bestTextFragments) {\n-                        if (bestTextFragment != null && bestTextFragment.getScore() > 0) {\n-                            fragsList.add(bestTextFragment);\n-                        }\n+            try (TokenStream tokenStream = analyzer.tokenStream(fieldType.name(), text)) {\n+                if (!tokenStream.hasAttribute(CharTermAttribute.class) || !tokenStream.hasAttribute(OffsetAttribute.class)) {\n+                    // can't perform highlighting if the stream has no terms (binary token stream) or no offsets\n+                    continue;\n+                }\n+                TextFragment[] bestTextFragments = entry.getBestTextFragments(tokenStream, text, false, numberOfFragments);\n+                for (TextFragment bestTextFragment : bestTextFragments) {\n+                    if (bestTextFragment != null && bestTextFragment.getScore() > 0) {\n+                        fragsList.add(bestTextFragment);\n                     }\n                 }\n-            }\n-        } catch (Exception e) {\n-            if (ExceptionsHelper.unwrap(e, BytesRefHash.MaxBytesLengthExceededException.class) != null) {\n-                // this can happen if for example a field is not_analyzed and ignore_above option is set.\n-                // the field will be ignored when indexing but the huge term is still in the source and\n-                // the plain highlighter will parse the source and try to analyze it.\n-                return null;\n-            } else {\n-                throw new FetchPhaseExecutionException(fieldContext.shardTarget,\n-                    \"Failed to highlight field [\" + fieldContext.fieldName + \"]\", e);\n+            } catch (BytesRefHash.MaxBytesLengthExceededException e) {\n+                // ignore and continue to the next value", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODA0NTExMw=="}, "originalCommit": {"oid": "25051501b4327146d84397d8189429028e444018"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODA2NTk1OQ==", "bodyText": "Got it, thanks! I had missed that it was a checked exception.", "url": "https://github.com/elastic/elasticsearch/pull/62230#discussion_r488065959", "createdAt": "2020-09-14T16:29:22Z", "author": {"login": "jtibshirani"}, "path": "server/src/main/java/org/elasticsearch/search/fetch/subphase/highlight/PlainHighlighter.java", "diffHunk": "@@ -111,56 +109,44 @@ public HighlightField highlight(FieldHighlightContext fieldContext) {\n         }\n         final int maxAnalyzedOffset = context.getIndexSettings().getHighlightMaxAnalyzedOffset();\n \n-        try {\n-            textsToHighlight = HighlightUtils.loadFieldValues(fieldType, hitContext, fieldContext.forceSource);\n+        textsToHighlight = HighlightUtils.loadFieldValues(fieldType, hitContext, fieldContext.forceSource);\n \n-            for (Object textToHighlight : textsToHighlight) {\n-                String text = convertFieldValue(fieldType, textToHighlight);\n-                int textLength = text.length();\n-                if (keywordIgnoreAbove != null  && textLength > keywordIgnoreAbove) {\n-                    continue; // skip highlighting keyword terms that were ignored during indexing\n-                }\n-                if (textLength > maxAnalyzedOffset) {\n-                    throw new IllegalArgumentException(\n-                        \"The length of [\" + fieldContext.fieldName + \"] field of [\" + hitContext.hit().getId() +\n-                            \"] doc of [\" + context.index().getName() + \"] index \" +\n-                            \"has exceeded [\" + maxAnalyzedOffset + \"] - maximum allowed to be analyzed for highlighting. \" +\n-                            \"This maximum can be set by changing the [\" + IndexSettings.MAX_ANALYZED_OFFSET_SETTING.getKey() +\n-                            \"] index level setting. \" + \"For large texts, indexing with offsets or term vectors, and highlighting \" +\n-                            \"with unified or fvh highlighter is recommended!\");\n-                }\n+        for (Object textToHighlight : textsToHighlight) {\n+            String text = convertFieldValue(fieldType, textToHighlight);\n+            int textLength = text.length();\n+            if (keywordIgnoreAbove != null && textLength > keywordIgnoreAbove) {\n+                continue; // skip highlighting keyword terms that were ignored during indexing\n+            }\n+            if (textLength > maxAnalyzedOffset) {\n+                throw new IllegalArgumentException(\n+                    \"The length of [\" + fieldContext.fieldName + \"] field of [\" + hitContext.hit().getId() +\n+                        \"] doc of [\" + context.index().getName() + \"] index \" +\n+                        \"has exceeded [\" + maxAnalyzedOffset + \"] - maximum allowed to be analyzed for highlighting. \" +\n+                        \"This maximum can be set by changing the [\" + IndexSettings.MAX_ANALYZED_OFFSET_SETTING.getKey() +\n+                        \"] index level setting. \" + \"For large texts, indexing with offsets or term vectors, and highlighting \" +\n+                        \"with unified or fvh highlighter is recommended!\");\n+            }\n \n-                try (TokenStream tokenStream = analyzer.tokenStream(fieldType.name(), text)) {\n-                    if (!tokenStream.hasAttribute(CharTermAttribute.class) || !tokenStream.hasAttribute(OffsetAttribute.class)) {\n-                        // can't perform highlighting if the stream has no terms (binary token stream) or no offsets\n-                        continue;\n-                    }\n-                    TextFragment[] bestTextFragments = entry.getBestTextFragments(tokenStream, text, false, numberOfFragments);\n-                    for (TextFragment bestTextFragment : bestTextFragments) {\n-                        if (bestTextFragment != null && bestTextFragment.getScore() > 0) {\n-                            fragsList.add(bestTextFragment);\n-                        }\n+            try (TokenStream tokenStream = analyzer.tokenStream(fieldType.name(), text)) {\n+                if (!tokenStream.hasAttribute(CharTermAttribute.class) || !tokenStream.hasAttribute(OffsetAttribute.class)) {\n+                    // can't perform highlighting if the stream has no terms (binary token stream) or no offsets\n+                    continue;\n+                }\n+                TextFragment[] bestTextFragments = entry.getBestTextFragments(tokenStream, text, false, numberOfFragments);\n+                for (TextFragment bestTextFragment : bestTextFragments) {\n+                    if (bestTextFragment != null && bestTextFragment.getScore() > 0) {\n+                        fragsList.add(bestTextFragment);\n                     }\n                 }\n-            }\n-        } catch (Exception e) {\n-            if (ExceptionsHelper.unwrap(e, BytesRefHash.MaxBytesLengthExceededException.class) != null) {\n-                // this can happen if for example a field is not_analyzed and ignore_above option is set.\n-                // the field will be ignored when indexing but the huge term is still in the source and\n-                // the plain highlighter will parse the source and try to analyze it.\n-                return null;\n-            } else {\n-                throw new FetchPhaseExecutionException(fieldContext.shardTarget,\n-                    \"Failed to highlight field [\" + fieldContext.fieldName + \"]\", e);\n+            } catch (BytesRefHash.MaxBytesLengthExceededException e) {\n+                // ignore and continue to the next value", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODA0NTExMw=="}, "originalCommit": {"oid": "25051501b4327146d84397d8189429028e444018"}, "originalPosition": 107}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1622, "cost": 1, "resetAt": "2021-11-12T13:16:51Z"}}}