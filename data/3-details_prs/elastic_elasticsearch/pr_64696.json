{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE2NzE4MTYx", "number": 64696, "title": "Allow searchable snapshot cache service to periodically fsync cache files", "bodyText": "This pull request changes the searchable snapshot's CacheService so that it now periodically fsync cache files using the method introduced in #64201.\nThe synchronization is executed every 60 seconds by default (this interval can be changed using a new xpack.searchable.snapshot.cache.sync_interval setting). It is only executed on data nodes that have at least one searchable snapshot assigned. On a data node cache file fsyncs are serialized and executed on a per-shard basis where the order of shards is undefined.", "createdAt": "2020-11-06T12:38:02Z", "url": "https://github.com/elastic/elasticsearch/pull/64696", "merged": true, "mergeCommit": {"oid": "8d28c3594770095a0652a8cf0ca9953414df264e"}, "closed": true, "closedAt": "2020-11-23T11:02:23Z", "author": {"login": "tlrx"}, "timelineItems": {"totalCount": 39, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdZ13_YAH2gAyNTE2NzE4MTYxOjAxMmVlZTJhYTZlZDcyMjFjYTI1ZmY5MWM2MzdkYmMzODFiNTA4ZTM=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdfSaihAH2gAyNTE2NzE4MTYxOjEwMjE2ZWE0MGFmOTczNjNiNDc2NDZmZmUzYmNhZGIzODRmMGZiZGU=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "012eee2aa6ed7221ca25ff91c637dbc381b508e3", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/012eee2aa6ed7221ca25ff91c637dbc381b508e3", "committedDate": "2020-11-06T12:11:28Z", "message": "Periodically fsync searchable snapshots cache files"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI1MTE0MzQx", "url": "https://github.com/elastic/elasticsearch/pull/64696#pullrequestreview-525114341", "createdAt": "2020-11-06T12:39:30Z", "commit": {"oid": "012eee2aa6ed7221ca25ff91c637dbc381b508e3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxMjozOTozMFrOHusf7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxMjozOTozMFrOHusf7g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODcyNTYxNA==", "bodyText": "\ud83e\udd26", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r518725614", "createdAt": "2020-11-06T12:39:30Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/index/store/cache/CacheFileTests.java", "diffHunk": "@@ -335,7 +335,7 @@ public void onEviction(CacheFile evictedCacheFile) {\n     public static void assertNumberOfFSyncs(final Path path, final Matcher<Long> matcher) {\n         final FSyncTrackingFileSystemProvider provider = (FSyncTrackingFileSystemProvider) path.getFileSystem().provider();\n         final AtomicLong fsyncCounter = provider.files.get(path);\n-        assertThat(\"File [\" + path + \"] was never fsynced\", notNullValue());\n+        assertThat(\"File [\" + path + \"] was never fsynced\", fsyncCounter, notNullValue());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "012eee2aa6ed7221ca25ff91c637dbc381b508e3"}, "originalPosition": 5}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "499f12bf15c81235ac24f7b6915cc0264258b680", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/499f12bf15c81235ac24f7b6915cc0264258b680", "committedDate": "2020-11-09T16:58:57Z", "message": "Merge branch 'master' into periodic-fsync"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "31299c1a14f827892eb97b38228f68f77baea1ce", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/31299c1a14f827892eb97b38228f68f77baea1ce", "committedDate": "2020-11-09T17:02:08Z", "message": "Fix spotless"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI2ODg5MDA5", "url": "https://github.com/elastic/elasticsearch/pull/64696#pullrequestreview-526889009", "createdAt": "2020-11-10T06:20:42Z", "commit": {"oid": "31299c1a14f827892eb97b38228f68f77baea1ce"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQwNjoyMDo0M1rOHwNm-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQwNzo0MDozNFrOHwPmTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDMxNjY2NQ==", "bodyText": "I wonder if it was better to not instantiate the CacheService at all on non-data nodes? Having it support non-data nodes without the cacheSyncTask seems counter intuitive (unless there is a good reason).\nThat would allow removing the asserts/ifs on cacheSyncTask != null.", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r520316665", "createdAt": "2020-11-10T06:20:43Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -52,31 +82,63 @@\n         Setting.Property.NodeScope\n     );\n \n+    public static final TimeValue MIN_SNAPSHOT_CACHE_SYNC_INTERVAL = TimeValue.timeValueSeconds(10L);\n+    public static final Setting<TimeValue> SNAPSHOT_CACHE_SYNC_INTERVAL_SETTING = Setting.timeSetting(\n+        SETTINGS_PREFIX + \"sync_interval\",\n+        TimeValue.timeValueSeconds(60L),                        // default\n+        MIN_SNAPSHOT_CACHE_SYNC_INTERVAL,                       // min\n+        Setting.Property.NodeScope,\n+        Setting.Property.Dynamic\n+    );\n+\n+    private static final Logger logger = LogManager.getLogger(CacheService.class);\n+\n+    private final ClusterService clusterService;\n+    private final NodeEnvironment nodeEnvironment;\n+    private final ThreadPool threadPool;\n+    private final CacheSynchronizationTask cacheSyncTask;\n     private final Cache<CacheKey, CacheFile> cache;\n     private final ByteSizeValue cacheSize;\n     private final Runnable cacheCleaner;\n     private final ByteSizeValue rangeSize;\n \n-    public CacheService(final Runnable cacheCleaner, final Settings settings) {\n-        this(cacheCleaner, SNAPSHOT_CACHE_SIZE_SETTING.get(settings), SNAPSHOT_CACHE_RANGE_SIZE_SETTING.get(settings));\n-    }\n-\n-    // exposed for tests\n-    public CacheService(final Runnable cacheCleaner, final ByteSizeValue cacheSize, final ByteSizeValue rangeSize) {\n-        this.cacheSize = Objects.requireNonNull(cacheSize);\n+    public CacheService(\n+        final Settings settings,\n+        final ClusterService clusterService,\n+        final ThreadPool threadPool,\n+        final NodeEnvironment nodeEnvironment,\n+        final Runnable cacheCleaner\n+    ) {\n+        this.clusterService = Objects.requireNonNull(clusterService);\n+        this.nodeEnvironment = Objects.requireNonNull(nodeEnvironment);\n+        this.threadPool = Objects.requireNonNull(threadPool);\n+        this.cacheSize = SNAPSHOT_CACHE_SIZE_SETTING.get(settings);\n         this.cacheCleaner = Objects.requireNonNull(cacheCleaner);\n-        this.rangeSize = Objects.requireNonNull(rangeSize);\n+        this.rangeSize = SNAPSHOT_CACHE_RANGE_SIZE_SETTING.get(settings);\n         this.cache = CacheBuilder.<CacheKey, CacheFile>builder()\n             .setMaximumWeight(cacheSize.getBytes())\n             .weigher((key, entry) -> entry.getLength())\n             // NORELEASE This does not immediately free space on disk, as cache file are only deleted when all index inputs\n             // are done with reading/writing the cache file\n             .removalListener(notification -> IOUtils.closeWhileHandlingException(() -> notification.getValue().startEviction()))\n             .build();\n+\n+        if (DiscoveryNode.isDataNode(settings)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "31299c1a14f827892eb97b38228f68f77baea1ce"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDMzMTM0MA==", "bodyText": "I think the reason we need this is because cacheSyncTask.cancel does not promise to not reschedule until rescheduleIfNecessary is called again?\nI wonder if that is a bug we should fix in AbstractAsyncTask? Can certainly be done in a follow-up (or separate PR).", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r520331340", "createdAt": "2020-11-10T06:55:48Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -141,4 +206,176 @@ public void removeFromCache(final Predicate<CacheKey> predicate) {\n         }\n         cache.refresh();\n     }\n+\n+    // used in tests\n+    CacheSynchronizationTask getCacheSyncTask() {\n+        return cacheSyncTask;\n+    }\n+\n+    void setCacheSyncInterval(TimeValue interval) {\n+        assert cacheSyncTask != null;\n+        cacheSyncTask.setInterval(interval);\n+    }\n+\n+    /**\n+     * Reschedule the {@link CacheSynchronizationTask} if the local data node is hosting searchable snapshot shards.\n+     */\n+    @Override\n+    public void clusterChanged(ClusterChangedEvent event) {\n+        assert cacheSyncTask != null;\n+\n+        if (event.routingTableChanged()) {\n+            final ClusterState clusterState = event.state();\n+            final DiscoveryNode localNode = clusterState.getNodes().getLocalNode();\n+            assert localNode.isDataNode();\n+\n+            final boolean shouldSynchronize = hasSearchableSnapshotShards(clusterState, localNode.getId());\n+            cacheSyncTask.allowReschedule.set(shouldSynchronize);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "31299c1a14f827892eb97b38228f68f77baea1ce"}, "originalPosition": 177}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDMzMjE0Ng==", "bodyText": "I think this could mean that we will not start fsync'ing during initialization/recovery of the first shard(s) on a starting node?", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r520332146", "createdAt": "2020-11-10T06:58:06Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -141,4 +206,176 @@ public void removeFromCache(final Predicate<CacheKey> predicate) {\n         }\n         cache.refresh();\n     }\n+\n+    // used in tests\n+    CacheSynchronizationTask getCacheSyncTask() {\n+        return cacheSyncTask;\n+    }\n+\n+    void setCacheSyncInterval(TimeValue interval) {\n+        assert cacheSyncTask != null;\n+        cacheSyncTask.setInterval(interval);\n+    }\n+\n+    /**\n+     * Reschedule the {@link CacheSynchronizationTask} if the local data node is hosting searchable snapshot shards.\n+     */\n+    @Override\n+    public void clusterChanged(ClusterChangedEvent event) {\n+        assert cacheSyncTask != null;\n+\n+        if (event.routingTableChanged()) {\n+            final ClusterState clusterState = event.state();\n+            final DiscoveryNode localNode = clusterState.getNodes().getLocalNode();\n+            assert localNode.isDataNode();\n+\n+            final boolean shouldSynchronize = hasSearchableSnapshotShards(clusterState, localNode.getId());\n+            cacheSyncTask.allowReschedule.set(shouldSynchronize);\n+\n+            if (shouldSynchronize == false) {\n+                logger.trace(\"canceling cache synchronization task (no searchable snapshots shard(s) assigned to local node)\");\n+                cacheSyncTask.cancel();\n+\n+            } else if (cacheSyncTask.isScheduled() == false) {\n+                logger.trace(\"scheduling cache synchronization task (searchable snapshots shard(s) assigned to local node)\");\n+                cacheSyncTask.rescheduleIfNecessary();\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Synchronize the cache files and dirs.\n+     *\n+     * This method iterates over all the searchable snapshot shards assigned to the local node in order to execute {@link CacheFile#fsync()}\n+     * on cache entries belonging to shards. When at least one {@link CacheFile#fsync()} call returns a non empty set of completed ranges\n+     * this method also fsync the shard's snapshot cache directory, which is the parent directory of the cache entries. Note that this\n+     * method is best effort as cache entries might be evicted during iterations and cache files/dirs removed from disk.\n+     */\n+    protected synchronized void synchronizeCache() {\n+        if (lifecycleState() != Lifecycle.State.STARTED) {\n+            return;\n+        }\n+        final ClusterState clusterState = clusterService.state();\n+        final RoutingNode routingNode = clusterState.getRoutingNodes().node(clusterState.getNodes().getLocalNodeId());\n+        assert routingNode != null;\n+\n+        final long startTimeNanos = threadPool.relativeTimeInNanos();\n+        for (ShardRouting shardRouting : routingNode) {\n+            if (shardRouting.active()) {\n+                final IndexMetadata indexMetadata = clusterState.metadata().getIndexSafe(shardRouting.index());\n+                final Settings indexSettings = indexMetadata.getSettings();\n+                if (isSearchableSnapshotStore(indexSettings)) {\n+                    final ShardId shardId = shardRouting.shardId();\n+                    final SnapshotId snapshotId = new SnapshotId(\n+                        SNAPSHOT_SNAPSHOT_NAME_SETTING.get(indexSettings),\n+                        SNAPSHOT_SNAPSHOT_ID_SETTING.get(indexSettings)\n+                    );\n+                    final IndexId indexId = new IndexId(\n+                        SNAPSHOT_INDEX_NAME_SETTING.get(indexSettings),\n+                        SNAPSHOT_INDEX_ID_SETTING.get(indexSettings)\n+                    );\n+\n+                    boolean syncDirectory = false;\n+                    for (Tuple<CacheKey, CacheFile> entry : cache.entries()) {\n+                        final CacheKey cacheKey = entry.v1();\n+                        if (cacheKey.belongsTo(snapshotId, indexId, shardId)) {\n+                            final CacheFile cacheFile = entry.v2();\n+                            try {\n+                                final SortedSet<Tuple<Long, Long>> ranges = cacheFile.fsync();\n+                                if (ranges.isEmpty() == false) {\n+                                    logger.trace(\n+                                        \"{} cache file [{}] synchronized with [{}] completed range(s)\",\n+                                        shardId,\n+                                        cacheFile.getFile().getFileName(),\n+                                        ranges.size()\n+                                    );\n+                                    syncDirectory = true;\n+                                    // TODO Index searchable snapshot shard information + cache file ranges in Lucene\n+                                }\n+                            } catch (Exception e) {\n+                                logger.warn(\n+                                    () -> new ParameterizedMessage(\n+                                        \"{} failed to fsync cache file [{}]\",\n+                                        shardId,\n+                                        cacheFile.getFile().getFileName()\n+                                    ),\n+                                    e\n+                                );\n+                            }\n+                        }\n+                    }\n+\n+                    if (syncDirectory) {\n+                        assert IndexMetadata.INDEX_DATA_PATH_SETTING.exists(indexSettings) == false;\n+                        for (Path shardPath : nodeEnvironment.availableShardPaths(shardId)) {\n+                            final Path snapshotCacheDir = resolveSnapshotCache(shardPath).resolve(snapshotId.getUUID());\n+                            if (Files.exists(snapshotCacheDir)) {\n+                                try {\n+                                    IOUtils.fsync(snapshotCacheDir, true, false);\n+                                    logger.trace(\"{} cache directory [{}] synchronized\", shardId, snapshotCacheDir);\n+                                } catch (Exception e) {\n+                                    logger.warn(\n+                                        () -> new ParameterizedMessage(\n+                                            \"{} failed to synchronize cache directory [{}]\",\n+                                            shardId,\n+                                            snapshotCacheDir\n+                                        ),\n+                                        e\n+                                    );\n+                                }\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+        if (logger.isDebugEnabled()) {\n+            final long elapsedNanos = threadPool.relativeTimeInNanos() - startTimeNanos;\n+            logger.debug(\"cache files synchronized in [{}]\", TimeValue.timeValueNanos(elapsedNanos));\n+        }\n+    }\n+\n+    static boolean hasSearchableSnapshotShards(final ClusterState clusterState, final String nodeId) {\n+        final RoutingNode routingNode = clusterState.getRoutingNodes().node(nodeId);\n+        if (routingNode != null) {\n+            for (ShardRouting shardRouting : routingNode) {\n+                if (shardRouting.active()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "31299c1a14f827892eb97b38228f68f77baea1ce"}, "originalPosition": 286}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDMzMjY1MA==", "bodyText": "I think we also want to fsync initializing shards?", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r520332650", "createdAt": "2020-11-10T06:59:27Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -141,4 +206,176 @@ public void removeFromCache(final Predicate<CacheKey> predicate) {\n         }\n         cache.refresh();\n     }\n+\n+    // used in tests\n+    CacheSynchronizationTask getCacheSyncTask() {\n+        return cacheSyncTask;\n+    }\n+\n+    void setCacheSyncInterval(TimeValue interval) {\n+        assert cacheSyncTask != null;\n+        cacheSyncTask.setInterval(interval);\n+    }\n+\n+    /**\n+     * Reschedule the {@link CacheSynchronizationTask} if the local data node is hosting searchable snapshot shards.\n+     */\n+    @Override\n+    public void clusterChanged(ClusterChangedEvent event) {\n+        assert cacheSyncTask != null;\n+\n+        if (event.routingTableChanged()) {\n+            final ClusterState clusterState = event.state();\n+            final DiscoveryNode localNode = clusterState.getNodes().getLocalNode();\n+            assert localNode.isDataNode();\n+\n+            final boolean shouldSynchronize = hasSearchableSnapshotShards(clusterState, localNode.getId());\n+            cacheSyncTask.allowReschedule.set(shouldSynchronize);\n+\n+            if (shouldSynchronize == false) {\n+                logger.trace(\"canceling cache synchronization task (no searchable snapshots shard(s) assigned to local node)\");\n+                cacheSyncTask.cancel();\n+\n+            } else if (cacheSyncTask.isScheduled() == false) {\n+                logger.trace(\"scheduling cache synchronization task (searchable snapshots shard(s) assigned to local node)\");\n+                cacheSyncTask.rescheduleIfNecessary();\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Synchronize the cache files and dirs.\n+     *\n+     * This method iterates over all the searchable snapshot shards assigned to the local node in order to execute {@link CacheFile#fsync()}\n+     * on cache entries belonging to shards. When at least one {@link CacheFile#fsync()} call returns a non empty set of completed ranges\n+     * this method also fsync the shard's snapshot cache directory, which is the parent directory of the cache entries. Note that this\n+     * method is best effort as cache entries might be evicted during iterations and cache files/dirs removed from disk.\n+     */\n+    protected synchronized void synchronizeCache() {\n+        if (lifecycleState() != Lifecycle.State.STARTED) {\n+            return;\n+        }\n+        final ClusterState clusterState = clusterService.state();\n+        final RoutingNode routingNode = clusterState.getRoutingNodes().node(clusterState.getNodes().getLocalNodeId());\n+        assert routingNode != null;\n+\n+        final long startTimeNanos = threadPool.relativeTimeInNanos();\n+        for (ShardRouting shardRouting : routingNode) {\n+            if (shardRouting.active()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "31299c1a14f827892eb97b38228f68f77baea1ce"}, "originalPosition": 208}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDMzNTQ5Nw==", "bodyText": "This seems like an n^2 algorithm. Since we iterate the cache entries, could we not just fsync the cache files individually as long as they are in the cache?", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r520335497", "createdAt": "2020-11-10T07:07:12Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -141,4 +206,176 @@ public void removeFromCache(final Predicate<CacheKey> predicate) {\n         }\n         cache.refresh();\n     }\n+\n+    // used in tests\n+    CacheSynchronizationTask getCacheSyncTask() {\n+        return cacheSyncTask;\n+    }\n+\n+    void setCacheSyncInterval(TimeValue interval) {\n+        assert cacheSyncTask != null;\n+        cacheSyncTask.setInterval(interval);\n+    }\n+\n+    /**\n+     * Reschedule the {@link CacheSynchronizationTask} if the local data node is hosting searchable snapshot shards.\n+     */\n+    @Override\n+    public void clusterChanged(ClusterChangedEvent event) {\n+        assert cacheSyncTask != null;\n+\n+        if (event.routingTableChanged()) {\n+            final ClusterState clusterState = event.state();\n+            final DiscoveryNode localNode = clusterState.getNodes().getLocalNode();\n+            assert localNode.isDataNode();\n+\n+            final boolean shouldSynchronize = hasSearchableSnapshotShards(clusterState, localNode.getId());\n+            cacheSyncTask.allowReschedule.set(shouldSynchronize);\n+\n+            if (shouldSynchronize == false) {\n+                logger.trace(\"canceling cache synchronization task (no searchable snapshots shard(s) assigned to local node)\");\n+                cacheSyncTask.cancel();\n+\n+            } else if (cacheSyncTask.isScheduled() == false) {\n+                logger.trace(\"scheduling cache synchronization task (searchable snapshots shard(s) assigned to local node)\");\n+                cacheSyncTask.rescheduleIfNecessary();\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Synchronize the cache files and dirs.\n+     *\n+     * This method iterates over all the searchable snapshot shards assigned to the local node in order to execute {@link CacheFile#fsync()}\n+     * on cache entries belonging to shards. When at least one {@link CacheFile#fsync()} call returns a non empty set of completed ranges\n+     * this method also fsync the shard's snapshot cache directory, which is the parent directory of the cache entries. Note that this\n+     * method is best effort as cache entries might be evicted during iterations and cache files/dirs removed from disk.\n+     */\n+    protected synchronized void synchronizeCache() {\n+        if (lifecycleState() != Lifecycle.State.STARTED) {\n+            return;\n+        }\n+        final ClusterState clusterState = clusterService.state();\n+        final RoutingNode routingNode = clusterState.getRoutingNodes().node(clusterState.getNodes().getLocalNodeId());\n+        assert routingNode != null;\n+\n+        final long startTimeNanos = threadPool.relativeTimeInNanos();\n+        for (ShardRouting shardRouting : routingNode) {\n+            if (shardRouting.active()) {\n+                final IndexMetadata indexMetadata = clusterState.metadata().getIndexSafe(shardRouting.index());\n+                final Settings indexSettings = indexMetadata.getSettings();\n+                if (isSearchableSnapshotStore(indexSettings)) {\n+                    final ShardId shardId = shardRouting.shardId();\n+                    final SnapshotId snapshotId = new SnapshotId(\n+                        SNAPSHOT_SNAPSHOT_NAME_SETTING.get(indexSettings),\n+                        SNAPSHOT_SNAPSHOT_ID_SETTING.get(indexSettings)\n+                    );\n+                    final IndexId indexId = new IndexId(\n+                        SNAPSHOT_INDEX_NAME_SETTING.get(indexSettings),\n+                        SNAPSHOT_INDEX_ID_SETTING.get(indexSettings)\n+                    );\n+\n+                    boolean syncDirectory = false;\n+                    for (Tuple<CacheKey, CacheFile> entry : cache.entries()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "31299c1a14f827892eb97b38228f68f77baea1ce"}, "originalPosition": 223}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDM0ODIyNA==", "bodyText": "I think this is true. In fact, it looks like the lru-chain would be unsafely published with no happens-before to the reader.\nI think the iteration of the entries done in this PR is thus unsafe, at least it might skip some entries.\nI think the same applies to iterating over keys(), which we do a few places.\nTogether with other comments in this PR, this makes me think that perhaps it was easier to let CacheFile's that need fsync register with the CacheService (or a registry in between)?", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r520348224", "createdAt": "2020-11-10T07:38:30Z", "author": {"login": "henningandersen"}, "path": "server/src/main/java/org/elasticsearch/common/cache/Cache.java", "diffHunk": "@@ -650,6 +650,36 @@ public void remove() {\n         };\n     }\n \n+    /**\n+     * An LRU sequencing of the entries in the cache. This sequence is not protected from mutations\n+     * to the cache (except for {@link Iterator#remove()}. The result of iteration under any other mutation is\n+     * undefined.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "31299c1a14f827892eb97b38228f68f77baea1ce"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDM0OTI2Mg==", "bodyText": "I wonder if we need to couple this to cluster state updates? Instead we could either trigger on the presence of any cache files or as proposed in another comment an explicit fsync-needed registration?", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r520349262", "createdAt": "2020-11-10T07:40:34Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -141,4 +206,176 @@ public void removeFromCache(final Predicate<CacheKey> predicate) {\n         }\n         cache.refresh();\n     }\n+\n+    // used in tests\n+    CacheSynchronizationTask getCacheSyncTask() {\n+        return cacheSyncTask;\n+    }\n+\n+    void setCacheSyncInterval(TimeValue interval) {\n+        assert cacheSyncTask != null;\n+        cacheSyncTask.setInterval(interval);\n+    }\n+\n+    /**\n+     * Reschedule the {@link CacheSynchronizationTask} if the local data node is hosting searchable snapshot shards.\n+     */\n+    @Override\n+    public void clusterChanged(ClusterChangedEvent event) {\n+        assert cacheSyncTask != null;\n+\n+        if (event.routingTableChanged()) {\n+            final ClusterState clusterState = event.state();\n+            final DiscoveryNode localNode = clusterState.getNodes().getLocalNode();\n+            assert localNode.isDataNode();\n+\n+            final boolean shouldSynchronize = hasSearchableSnapshotShards(clusterState, localNode.getId());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "31299c1a14f827892eb97b38228f68f77baea1ce"}, "originalPosition": 176}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI3MDQzODA3", "url": "https://github.com/elastic/elasticsearch/pull/64696#pullrequestreview-527043807", "createdAt": "2020-11-10T10:07:16Z", "commit": {"oid": "31299c1a14f827892eb97b38228f68f77baea1ce"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQxMDowNzoxNlrOHwVIbQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQxMDo1Nzo1MlrOHwXI_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDQzOTkxNw==", "bodyText": "++ I think not instantiating it would be nice.\nMild worry on this :) <- it might be a little cumbersome to get it right due to transport actions? (haven't checked this here in detail but I remember trying to do a similar thing elsewhere and it turned out to be tricky)", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r520439917", "createdAt": "2020-11-10T10:07:16Z", "author": {"login": "original-brownbear"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -52,31 +82,63 @@\n         Setting.Property.NodeScope\n     );\n \n+    public static final TimeValue MIN_SNAPSHOT_CACHE_SYNC_INTERVAL = TimeValue.timeValueSeconds(10L);\n+    public static final Setting<TimeValue> SNAPSHOT_CACHE_SYNC_INTERVAL_SETTING = Setting.timeSetting(\n+        SETTINGS_PREFIX + \"sync_interval\",\n+        TimeValue.timeValueSeconds(60L),                        // default\n+        MIN_SNAPSHOT_CACHE_SYNC_INTERVAL,                       // min\n+        Setting.Property.NodeScope,\n+        Setting.Property.Dynamic\n+    );\n+\n+    private static final Logger logger = LogManager.getLogger(CacheService.class);\n+\n+    private final ClusterService clusterService;\n+    private final NodeEnvironment nodeEnvironment;\n+    private final ThreadPool threadPool;\n+    private final CacheSynchronizationTask cacheSyncTask;\n     private final Cache<CacheKey, CacheFile> cache;\n     private final ByteSizeValue cacheSize;\n     private final Runnable cacheCleaner;\n     private final ByteSizeValue rangeSize;\n \n-    public CacheService(final Runnable cacheCleaner, final Settings settings) {\n-        this(cacheCleaner, SNAPSHOT_CACHE_SIZE_SETTING.get(settings), SNAPSHOT_CACHE_RANGE_SIZE_SETTING.get(settings));\n-    }\n-\n-    // exposed for tests\n-    public CacheService(final Runnable cacheCleaner, final ByteSizeValue cacheSize, final ByteSizeValue rangeSize) {\n-        this.cacheSize = Objects.requireNonNull(cacheSize);\n+    public CacheService(\n+        final Settings settings,\n+        final ClusterService clusterService,\n+        final ThreadPool threadPool,\n+        final NodeEnvironment nodeEnvironment,\n+        final Runnable cacheCleaner\n+    ) {\n+        this.clusterService = Objects.requireNonNull(clusterService);\n+        this.nodeEnvironment = Objects.requireNonNull(nodeEnvironment);\n+        this.threadPool = Objects.requireNonNull(threadPool);\n+        this.cacheSize = SNAPSHOT_CACHE_SIZE_SETTING.get(settings);\n         this.cacheCleaner = Objects.requireNonNull(cacheCleaner);\n-        this.rangeSize = Objects.requireNonNull(rangeSize);\n+        this.rangeSize = SNAPSHOT_CACHE_RANGE_SIZE_SETTING.get(settings);\n         this.cache = CacheBuilder.<CacheKey, CacheFile>builder()\n             .setMaximumWeight(cacheSize.getBytes())\n             .weigher((key, entry) -> entry.getLength())\n             // NORELEASE This does not immediately free space on disk, as cache file are only deleted when all index inputs\n             // are done with reading/writing the cache file\n             .removalListener(notification -> IOUtils.closeWhileHandlingException(() -> notification.getValue().startEviction()))\n             .build();\n+\n+        if (DiscoveryNode.isDataNode(settings)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDMxNjY2NQ=="}, "originalCommit": {"oid": "31299c1a14f827892eb97b38228f68f77baea1ce"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDQ3MjgzMA==", "bodyText": "++, I would also think there's no need to be tricky here. We already have all the tricky logic for handling a cache file's life-cycle in CacheFile => I think I like the idea of just iterating over all CacheFile here combined with Henning's other point of having the CacheFile register itself for fsync in some form. That seems like it wouldn't add new complexities around synchronization and life-cycle beyond what we already have in CacheFile?", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r520472830", "createdAt": "2020-11-10T10:57:52Z", "author": {"login": "original-brownbear"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -141,4 +206,176 @@ public void removeFromCache(final Predicate<CacheKey> predicate) {\n         }\n         cache.refresh();\n     }\n+\n+    // used in tests\n+    CacheSynchronizationTask getCacheSyncTask() {\n+        return cacheSyncTask;\n+    }\n+\n+    void setCacheSyncInterval(TimeValue interval) {\n+        assert cacheSyncTask != null;\n+        cacheSyncTask.setInterval(interval);\n+    }\n+\n+    /**\n+     * Reschedule the {@link CacheSynchronizationTask} if the local data node is hosting searchable snapshot shards.\n+     */\n+    @Override\n+    public void clusterChanged(ClusterChangedEvent event) {\n+        assert cacheSyncTask != null;\n+\n+        if (event.routingTableChanged()) {\n+            final ClusterState clusterState = event.state();\n+            final DiscoveryNode localNode = clusterState.getNodes().getLocalNode();\n+            assert localNode.isDataNode();\n+\n+            final boolean shouldSynchronize = hasSearchableSnapshotShards(clusterState, localNode.getId());\n+            cacheSyncTask.allowReschedule.set(shouldSynchronize);\n+\n+            if (shouldSynchronize == false) {\n+                logger.trace(\"canceling cache synchronization task (no searchable snapshots shard(s) assigned to local node)\");\n+                cacheSyncTask.cancel();\n+\n+            } else if (cacheSyncTask.isScheduled() == false) {\n+                logger.trace(\"scheduling cache synchronization task (searchable snapshots shard(s) assigned to local node)\");\n+                cacheSyncTask.rescheduleIfNecessary();\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Synchronize the cache files and dirs.\n+     *\n+     * This method iterates over all the searchable snapshot shards assigned to the local node in order to execute {@link CacheFile#fsync()}\n+     * on cache entries belonging to shards. When at least one {@link CacheFile#fsync()} call returns a non empty set of completed ranges\n+     * this method also fsync the shard's snapshot cache directory, which is the parent directory of the cache entries. Note that this\n+     * method is best effort as cache entries might be evicted during iterations and cache files/dirs removed from disk.\n+     */\n+    protected synchronized void synchronizeCache() {\n+        if (lifecycleState() != Lifecycle.State.STARTED) {\n+            return;\n+        }\n+        final ClusterState clusterState = clusterService.state();\n+        final RoutingNode routingNode = clusterState.getRoutingNodes().node(clusterState.getNodes().getLocalNodeId());\n+        assert routingNode != null;\n+\n+        final long startTimeNanos = threadPool.relativeTimeInNanos();\n+        for (ShardRouting shardRouting : routingNode) {\n+            if (shardRouting.active()) {\n+                final IndexMetadata indexMetadata = clusterState.metadata().getIndexSafe(shardRouting.index());\n+                final Settings indexSettings = indexMetadata.getSettings();\n+                if (isSearchableSnapshotStore(indexSettings)) {\n+                    final ShardId shardId = shardRouting.shardId();\n+                    final SnapshotId snapshotId = new SnapshotId(\n+                        SNAPSHOT_SNAPSHOT_NAME_SETTING.get(indexSettings),\n+                        SNAPSHOT_SNAPSHOT_ID_SETTING.get(indexSettings)\n+                    );\n+                    final IndexId indexId = new IndexId(\n+                        SNAPSHOT_INDEX_NAME_SETTING.get(indexSettings),\n+                        SNAPSHOT_INDEX_ID_SETTING.get(indexSettings)\n+                    );\n+\n+                    boolean syncDirectory = false;\n+                    for (Tuple<CacheKey, CacheFile> entry : cache.entries()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDMzNTQ5Nw=="}, "originalCommit": {"oid": "31299c1a14f827892eb97b38228f68f77baea1ce"}, "originalPosition": 223}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bf0e6600fc90cf1c0fbdde3e591ce53e7dcdaaa4", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/bf0e6600fc90cf1c0fbdde3e591ce53e7dcdaaa4", "committedDate": "2020-11-12T11:39:31Z", "message": "Merge branch 'master' into periodic-fsync"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9085f81a55fcb04b673705afad62a70bf54225d3", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/9085f81a55fcb04b673705afad62a70bf54225d3", "committedDate": "2020-11-13T09:20:39Z", "message": "Merge branch 'master' into periodic-fsync"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fb10c6fef5c3f6d13f1960f764212f09d9a3ea18", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/fb10c6fef5c3f6d13f1960f764212f09d9a3ea18", "committedDate": "2020-11-16T11:46:45Z", "message": "use registering mechanism to sync cache files"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f5d95597e8f27871f73cd1b9d2c83ae3f86da87d", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/f5d95597e8f27871f73cd1b9d2c83ae3f86da87d", "committedDate": "2020-11-16T11:47:22Z", "message": "Merge branch 'master' into periodic-fsync"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMxMjkzMzIy", "url": "https://github.com/elastic/elasticsearch/pull/64696#pullrequestreview-531293322", "createdAt": "2020-11-16T12:55:09Z", "commit": {"oid": "f5d95597e8f27871f73cd1b9d2c83ae3f86da87d"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQxMjo1NTowOVrOHz9gHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQxMzoyODowM1rOHz-rpA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDI0NzA2OQ==", "bodyText": "This looks unused now?", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r524247069", "createdAt": "2020-11-16T12:55:09Z", "author": {"login": "henningandersen"}, "path": "server/src/main/java/org/elasticsearch/common/cache/Cache.java", "diffHunk": "@@ -650,6 +650,36 @@ public void remove() {\n         };\n     }\n \n+    /**\n+     * An LRU sequencing of the entries in the cache. This sequence is not protected from mutations\n+     * to the cache (except for {@link Iterator#remove()}. The result of iteration under any other mutation is\n+     * undefined.\n+     *\n+     * @return an LRU-ordered {@link Iterable} over the entries in the cache\n+     */\n+    public Iterable<Tuple<K,V>> entries() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f5d95597e8f27871f73cd1b9d2c83ae3f86da87d"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDI1MjI4Mg==", "bodyText": "Looks like this might as well be a method (or inlined)?", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r524252282", "createdAt": "2020-11-16T13:04:27Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -52,40 +69,65 @@\n         Setting.Property.NodeScope\n     );\n \n+    public static final TimeValue MIN_SNAPSHOT_CACHE_SYNC_INTERVAL = TimeValue.timeValueSeconds(10L);\n+    public static final Setting<TimeValue> SNAPSHOT_CACHE_SYNC_INTERVAL_SETTING = Setting.timeSetting(\n+        SETTINGS_PREFIX + \"sync_interval\",\n+        TimeValue.timeValueSeconds(60L),                        // default\n+        MIN_SNAPSHOT_CACHE_SYNC_INTERVAL,                       // min\n+        Setting.Property.NodeScope,\n+        Setting.Property.Dynamic\n+    );\n+\n+    private static final Supplier<Set<CacheFile>> SUPPLIER_OF_CACHE_FILES_TO_SYNC = ConcurrentCollections::newConcurrentSet;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f5d95597e8f27871f73cd1b9d2c83ae3f86da87d"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDI1MzcxMQ==", "bodyText": "I think this is a leftover from last iteration? Since it is a final field initialized in constructor, I would prefer not to have this assert.", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r524253711", "createdAt": "2020-11-16T13:06:47Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -141,4 +183,123 @@ public void removeFromCache(final Predicate<CacheKey> predicate) {\n         }\n         cache.refresh();\n     }\n+\n+    void setCacheSyncInterval(TimeValue interval) {\n+        assert cacheSyncTask != null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f5d95597e8f27871f73cd1b9d2c83ae3f86da87d"}, "originalPosition": 141}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDI2MDM4OA==", "bodyText": "There is a race condition here, since we first get the set and then add to it. But the synchronizeCache method might read the set and check that it is empty before the add is done here, risking that we miss an fsync. Same is true for the non-empty case, in that it could start iterating over the set before the add is executed.\nMaybe we can use a queue instead? Since CacheFile ensures it is only registered once, we might not need the set semantics except when removing a cache file (where we could just iterate the queue, seems ok, could even just leave it in the queue).", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r524260388", "createdAt": "2020-11-16T13:18:09Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -141,4 +183,123 @@ public void removeFromCache(final Predicate<CacheKey> predicate) {\n         }\n         cache.refresh();\n     }\n+\n+    void setCacheSyncInterval(TimeValue interval) {\n+        assert cacheSyncTask != null;\n+        cacheSyncTask.setInterval(interval);\n+    }\n+\n+    /**\n+     * This method is invoked when a {@link CacheFile} notifies the current {@link CacheService} that it needs to be fsync on disk.\n+     * <p>\n+     * It adds the {@link CacheFile} instance to current set of cache files to synchronize.\n+     *\n+     * @param cacheFile the instance that needs to be fsync\n+     */\n+    void onCacheFileUpdate(CacheFile cacheFile) {\n+        final boolean added = cacheFilesToSyncRef.get().add(cacheFile);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f5d95597e8f27871f73cd1b9d2c83ae3f86da87d"}, "originalPosition": 153}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDI2NjQwNA==", "bodyText": "If the compareAndSet above does not succeed, should we then perhaps fail when running tests (using an assert)?", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r524266404", "createdAt": "2020-11-16T13:28:03Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CacheFile.java", "diffHunk": "@@ -444,12 +461,12 @@ boolean needsFsync() {\n                         assert completedRanges != null;\n                         assert completedRanges.isEmpty() == false;\n \n-                        IOUtils.fsync(file, false, false); // TODO don't forget to fsync parent directory\n+                        IOUtils.fsync(file, false, false);\n                         success = true;\n                         return completedRanges;\n                     } finally {\n                         if (success == false) {\n-                            needsFsync.set(true);\n+                            markAsNeedsFSync();\n                         }\n                     }\n                 }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f5d95597e8f27871f73cd1b9d2c83ae3f86da87d"}, "originalPosition": 66}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "684e01ec740fa9c69b20c19d902305df8231e480", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/684e01ec740fa9c69b20c19d902305df8231e480", "committedDate": "2020-11-16T15:14:15Z", "message": "Use a queue"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMxNTI0MTY5", "url": "https://github.com/elastic/elasticsearch/pull/64696#pullrequestreview-531524169", "createdAt": "2020-11-16T17:04:41Z", "commit": {"oid": "684e01ec740fa9c69b20c19d902305df8231e480"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQxNzowNDo0MVrOH0Ieog==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMDowNzoxMVrOH0Pa6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDQyNjkxNA==", "bodyText": "Looks like we're only ever passing null here in tests, maybe cleaner to just pass the noop consumer in tests than having this conditional?", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r524426914", "createdAt": "2020-11-16T17:04:41Z", "author": {"login": "original-brownbear"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CacheFile.java", "diffHunk": "@@ -117,10 +124,11 @@ protected void closeInternal() {\n     @Nullable\n     private volatile FileChannelReference channelRef;\n \n-    public CacheFile(String description, long length, Path file) {\n+    public CacheFile(String description, long length, Path file, @Nullable Consumer<CacheFile> fsyncListener) {\n         this.tracker = new SparseFileTracker(file.toString(), length);\n         this.description = Objects.requireNonNull(description);\n         this.file = Objects.requireNonNull(file);\n+        this.needsFsyncListener = fsyncListener != null ? fsyncListener : cacheFile -> {};", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "684e01ec740fa9c69b20c19d902305df8231e480"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDUzMjM0OA==", "bodyText": "I wonder if we should be this heroic? I guess I could see us running into an IOException here when exceeding the FD limit, but beyond that it seems there's very little valid reasons to keep going with a cache file after we fail to fsync it?\nShould we have a notion of forcefully dropping such a file from the cache since we can't trust it any longer?", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r524532348", "createdAt": "2020-11-16T19:52:14Z", "author": {"login": "original-brownbear"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -141,4 +195,125 @@ public void removeFromCache(final Predicate<CacheKey> predicate) {\n         }\n         cache.refresh();\n     }\n+\n+    void setCacheSyncInterval(TimeValue interval) {\n+        cacheSyncTask.setInterval(interval);\n+    }\n+\n+    private void setMaxCacheFilesToSyncAtOnce(int maxCacheFilesToSyncAtOnce) {\n+        this.maxCacheFilesToSyncAtOnce = maxCacheFilesToSyncAtOnce;\n+    }\n+\n+    /**\n+     * This method is invoked when a {@link CacheFile} notifies the current {@link CacheService} that it needs to be fsync on disk.\n+     * <p>\n+     * It adds the {@link CacheFile} instance to current set of cache files to synchronize.\n+     *\n+     * @param cacheFile the instance that needs to be fsync\n+     */\n+    void onCacheFileUpdate(CacheFile cacheFile) {\n+        cacheFilesToSync.offer(cacheFile);\n+    }\n+\n+    /**\n+     * This method is invoked after a {@link CacheFile} is evicted from the cache.\n+     * <p>\n+     * It notifies the {@link CacheFile}'s eviction listeners that the instance is evicted. It also removes the instance from the current\n+     * queue of cache files to synchronize if the instance is referenced there.\n+     *\n+     * @param cacheFile the evicted instance\n+     */\n+    void onCacheFileRemoval(CacheFile cacheFile) {\n+        IOUtils.closeWhileHandlingException(cacheFile::startEviction);\n+        cacheFilesToSync.remove(cacheFile);\n+    }\n+\n+    // used in tests\n+    boolean isCacheFileToSync(CacheFile cacheFile) {\n+        return cacheFilesToSync.contains(cacheFile);\n+    }\n+\n+    /**\n+     * Synchronize the cache files and their parent directories on disk.\n+     *\n+     * This method synchronizes the cache files that have been updated since the last time the method was invoked. To be able to do this,\n+     * the cache files must notify the {@link CacheService} when they need to be fsync. When a {@link CacheFile} notifies the service the\n+     * {@link CacheFile} instance is added to the current queue of cache files to synchronize referenced by {@link #cacheFilesToSync}.\n+     *\n+     * Cache files are serially synchronized using the {@link CacheFile#fsync()} method. When the {@link CacheFile#fsync()} call returns a\n+     * non empty set of completed ranges this method also fsync the shard's snapshot cache directory, which is the parent directory of the\n+     * cache entry. Note that cache files might be evicted during the synchronization.\n+     */\n+    protected void synchronizeCache() {\n+        long count = 0L;\n+        final Set<Path> cacheDirs = new HashSet<>();\n+        final long startTimeNanos = threadPool.relativeTimeInNanos();\n+        final int maxCacheFilesToSync = this.maxCacheFilesToSyncAtOnce;\n+        for (long i = 0L; i < maxCacheFilesToSync; i++) {\n+            if (lifecycleState() != Lifecycle.State.STARTED) {\n+                logger.debug(\"stopping cache synchronization (cache service is closing)\");\n+                break;\n+            }\n+            final CacheFile cacheFile = cacheFilesToSync.poll();\n+            if (cacheFile == null) {\n+                logger.debug(\"stopping cache synchronization (no more cache files to fsync)\");\n+                break;\n+            }\n+            final Path cacheFilePath = cacheFile.getFile();\n+            try {\n+                final SortedSet<Tuple<Long, Long>> ranges = cacheFile.fsync();\n+                if (ranges.isEmpty() == false) {\n+                    logger.trace(\"cache file [{}] synchronized with [{}] completed range(s)\", cacheFilePath.getFileName(), ranges.size());\n+                    final Path cacheDir = cacheFilePath.toAbsolutePath().getParent();\n+                    if (cacheDirs.add(cacheDir)) {\n+                        try {\n+                            IOUtils.fsync(cacheDir, true, false);\n+                            logger.trace(\"cache directory [{}] synchronized\", cacheDir);\n+                        } catch (Exception e) {\n+                            logger.warn(() -> new ParameterizedMessage(\"failed to synchronize cache directory [{}]\", cacheDir), e);\n+                        }\n+                    }\n+                    // TODO Index searchable snapshot shard information + cache file ranges in Lucene\n+                    count += 1L;\n+                }\n+            } catch (Exception e) {\n+                logger.warn(() -> new ParameterizedMessage(\"failed to fsync cache file [{}]\", cacheFilePath.getFileName()), e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "684e01ec740fa9c69b20c19d902305df8231e480"}, "originalPosition": 232}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDUzMzU1OA==", "bodyText": "Same as the other comment, maybe even more pronounced here: what does it mean if we fail to fsync the directory? Doesn't it at least mean that we failed to fsync the file as well? (it's certainly not guaranteed to be safely persisted if it was just created on many Linux FS)", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r524533558", "createdAt": "2020-11-16T19:54:18Z", "author": {"login": "original-brownbear"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -141,4 +195,125 @@ public void removeFromCache(final Predicate<CacheKey> predicate) {\n         }\n         cache.refresh();\n     }\n+\n+    void setCacheSyncInterval(TimeValue interval) {\n+        cacheSyncTask.setInterval(interval);\n+    }\n+\n+    private void setMaxCacheFilesToSyncAtOnce(int maxCacheFilesToSyncAtOnce) {\n+        this.maxCacheFilesToSyncAtOnce = maxCacheFilesToSyncAtOnce;\n+    }\n+\n+    /**\n+     * This method is invoked when a {@link CacheFile} notifies the current {@link CacheService} that it needs to be fsync on disk.\n+     * <p>\n+     * It adds the {@link CacheFile} instance to current set of cache files to synchronize.\n+     *\n+     * @param cacheFile the instance that needs to be fsync\n+     */\n+    void onCacheFileUpdate(CacheFile cacheFile) {\n+        cacheFilesToSync.offer(cacheFile);\n+    }\n+\n+    /**\n+     * This method is invoked after a {@link CacheFile} is evicted from the cache.\n+     * <p>\n+     * It notifies the {@link CacheFile}'s eviction listeners that the instance is evicted. It also removes the instance from the current\n+     * queue of cache files to synchronize if the instance is referenced there.\n+     *\n+     * @param cacheFile the evicted instance\n+     */\n+    void onCacheFileRemoval(CacheFile cacheFile) {\n+        IOUtils.closeWhileHandlingException(cacheFile::startEviction);\n+        cacheFilesToSync.remove(cacheFile);\n+    }\n+\n+    // used in tests\n+    boolean isCacheFileToSync(CacheFile cacheFile) {\n+        return cacheFilesToSync.contains(cacheFile);\n+    }\n+\n+    /**\n+     * Synchronize the cache files and their parent directories on disk.\n+     *\n+     * This method synchronizes the cache files that have been updated since the last time the method was invoked. To be able to do this,\n+     * the cache files must notify the {@link CacheService} when they need to be fsync. When a {@link CacheFile} notifies the service the\n+     * {@link CacheFile} instance is added to the current queue of cache files to synchronize referenced by {@link #cacheFilesToSync}.\n+     *\n+     * Cache files are serially synchronized using the {@link CacheFile#fsync()} method. When the {@link CacheFile#fsync()} call returns a\n+     * non empty set of completed ranges this method also fsync the shard's snapshot cache directory, which is the parent directory of the\n+     * cache entry. Note that cache files might be evicted during the synchronization.\n+     */\n+    protected void synchronizeCache() {\n+        long count = 0L;\n+        final Set<Path> cacheDirs = new HashSet<>();\n+        final long startTimeNanos = threadPool.relativeTimeInNanos();\n+        final int maxCacheFilesToSync = this.maxCacheFilesToSyncAtOnce;\n+        for (long i = 0L; i < maxCacheFilesToSync; i++) {\n+            if (lifecycleState() != Lifecycle.State.STARTED) {\n+                logger.debug(\"stopping cache synchronization (cache service is closing)\");\n+                break;\n+            }\n+            final CacheFile cacheFile = cacheFilesToSync.poll();\n+            if (cacheFile == null) {\n+                logger.debug(\"stopping cache synchronization (no more cache files to fsync)\");\n+                break;\n+            }\n+            final Path cacheFilePath = cacheFile.getFile();\n+            try {\n+                final SortedSet<Tuple<Long, Long>> ranges = cacheFile.fsync();\n+                if (ranges.isEmpty() == false) {\n+                    logger.trace(\"cache file [{}] synchronized with [{}] completed range(s)\", cacheFilePath.getFileName(), ranges.size());\n+                    final Path cacheDir = cacheFilePath.toAbsolutePath().getParent();\n+                    if (cacheDirs.add(cacheDir)) {\n+                        try {\n+                            IOUtils.fsync(cacheDir, true, false);\n+                            logger.trace(\"cache directory [{}] synchronized\", cacheDir);\n+                        } catch (Exception e) {\n+                            logger.warn(() -> new ParameterizedMessage(\"failed to synchronize cache directory [{}]\", cacheDir), e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "684e01ec740fa9c69b20c19d902305df8231e480"}, "originalPosition": 225}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDUzNTIzMw==", "bodyText": "Should we first remove the file from the queue and then evict to have less of a race here with concurrent fsync runs?", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r524535233", "createdAt": "2020-11-16T19:57:14Z", "author": {"login": "original-brownbear"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -141,4 +195,125 @@ public void removeFromCache(final Predicate<CacheKey> predicate) {\n         }\n         cache.refresh();\n     }\n+\n+    void setCacheSyncInterval(TimeValue interval) {\n+        cacheSyncTask.setInterval(interval);\n+    }\n+\n+    private void setMaxCacheFilesToSyncAtOnce(int maxCacheFilesToSyncAtOnce) {\n+        this.maxCacheFilesToSyncAtOnce = maxCacheFilesToSyncAtOnce;\n+    }\n+\n+    /**\n+     * This method is invoked when a {@link CacheFile} notifies the current {@link CacheService} that it needs to be fsync on disk.\n+     * <p>\n+     * It adds the {@link CacheFile} instance to current set of cache files to synchronize.\n+     *\n+     * @param cacheFile the instance that needs to be fsync\n+     */\n+    void onCacheFileUpdate(CacheFile cacheFile) {\n+        cacheFilesToSync.offer(cacheFile);\n+    }\n+\n+    /**\n+     * This method is invoked after a {@link CacheFile} is evicted from the cache.\n+     * <p>\n+     * It notifies the {@link CacheFile}'s eviction listeners that the instance is evicted. It also removes the instance from the current\n+     * queue of cache files to synchronize if the instance is referenced there.\n+     *\n+     * @param cacheFile the evicted instance\n+     */\n+    void onCacheFileRemoval(CacheFile cacheFile) {\n+        IOUtils.closeWhileHandlingException(cacheFile::startEviction);\n+        cacheFilesToSync.remove(cacheFile);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "684e01ec740fa9c69b20c19d902305df8231e480"}, "originalPosition": 180}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDU0MDY0OQ==", "bodyText": "Also, calling remove (O(n)) on a ConcurrentLinkedQueue feels like it may bring trouble if we're dealing with a large number of files queued up? Do we even need to do this when we could simply skip evicted files when polling the fsync queue?", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r524540649", "createdAt": "2020-11-16T20:07:11Z", "author": {"login": "original-brownbear"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -141,4 +195,125 @@ public void removeFromCache(final Predicate<CacheKey> predicate) {\n         }\n         cache.refresh();\n     }\n+\n+    void setCacheSyncInterval(TimeValue interval) {\n+        cacheSyncTask.setInterval(interval);\n+    }\n+\n+    private void setMaxCacheFilesToSyncAtOnce(int maxCacheFilesToSyncAtOnce) {\n+        this.maxCacheFilesToSyncAtOnce = maxCacheFilesToSyncAtOnce;\n+    }\n+\n+    /**\n+     * This method is invoked when a {@link CacheFile} notifies the current {@link CacheService} that it needs to be fsync on disk.\n+     * <p>\n+     * It adds the {@link CacheFile} instance to current set of cache files to synchronize.\n+     *\n+     * @param cacheFile the instance that needs to be fsync\n+     */\n+    void onCacheFileUpdate(CacheFile cacheFile) {\n+        cacheFilesToSync.offer(cacheFile);\n+    }\n+\n+    /**\n+     * This method is invoked after a {@link CacheFile} is evicted from the cache.\n+     * <p>\n+     * It notifies the {@link CacheFile}'s eviction listeners that the instance is evicted. It also removes the instance from the current\n+     * queue of cache files to synchronize if the instance is referenced there.\n+     *\n+     * @param cacheFile the evicted instance\n+     */\n+    void onCacheFileRemoval(CacheFile cacheFile) {\n+        IOUtils.closeWhileHandlingException(cacheFile::startEviction);\n+        cacheFilesToSync.remove(cacheFile);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "684e01ec740fa9c69b20c19d902305df8231e480"}, "originalPosition": 180}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6b238d6e76b0ad8445feae40ef7076611d88a5c8", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/6b238d6e76b0ad8445feae40ef7076611d88a5c8", "committedDate": "2020-11-17T08:13:11Z", "message": "revert changes in Cache"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "eabe56292858d6a23a4b78d77eba1b6b6bf998f9", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/eabe56292858d6a23a4b78d77eba1b6b6bf998f9", "committedDate": "2020-11-17T08:14:06Z", "message": "do not remove from queue on eviction"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "af47aaa052d82b6033de699956a575ffd780ab73", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/af47aaa052d82b6033de699956a575ffd780ab73", "committedDate": "2020-11-17T08:17:57Z", "message": "noop fsyncListener"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMyMTcwNTgx", "url": "https://github.com/elastic/elasticsearch/pull/64696#pullrequestreview-532170581", "createdAt": "2020-11-17T09:27:33Z", "commit": {"oid": "af47aaa052d82b6033de699956a575ffd780ab73"}, "state": "COMMENTED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwOToyNzozM1rOH0ryGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QxMToxNDozOFrOH0wCLQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTAwNTMzNw==", "bodyText": "It would be nice to ensure we never loop further than the set of cache files already in the queue at the beginning of this method. Can we cap the number of iterations by the size of the queue before we start any fsync'ing? It is O(n) on the linked-queue implementation, but I think that is fine.\nIf we just continue looping, we risk an oscillating effect of writing another block of data and then fsync'ing it multiple times, resulting in much more fsync'ing than desired.", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r525005337", "createdAt": "2020-11-17T09:27:33Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -141,4 +195,123 @@ public void removeFromCache(final Predicate<CacheKey> predicate) {\n         }\n         cache.refresh();\n     }\n+\n+    void setCacheSyncInterval(TimeValue interval) {\n+        cacheSyncTask.setInterval(interval);\n+    }\n+\n+    private void setMaxCacheFilesToSyncAtOnce(int maxCacheFilesToSyncAtOnce) {\n+        this.maxCacheFilesToSyncAtOnce = maxCacheFilesToSyncAtOnce;\n+    }\n+\n+    /**\n+     * This method is invoked when a {@link CacheFile} notifies the current {@link CacheService} that it needs to be fsync on disk.\n+     * <p>\n+     * It adds the {@link CacheFile} instance to current set of cache files to synchronize.\n+     *\n+     * @param cacheFile the instance that needs to be fsync\n+     */\n+    void onCacheFileUpdate(CacheFile cacheFile) {\n+        cacheFilesToSync.offer(cacheFile);\n+    }\n+\n+    /**\n+     * This method is invoked after a {@link CacheFile} is evicted from the cache.\n+     * <p>\n+     * It notifies the {@link CacheFile}'s eviction listeners that the instance is evicted.d\n+     *\n+     * @param cacheFile the evicted instance\n+     */\n+    void onCacheFileRemoval(CacheFile cacheFile) {\n+        IOUtils.closeWhileHandlingException(cacheFile::startEviction);\n+    }\n+\n+    // used in tests\n+    boolean isCacheFileToSync(CacheFile cacheFile) {\n+        return cacheFilesToSync.contains(cacheFile);\n+    }\n+\n+    /**\n+     * Synchronize the cache files and their parent directories on disk.\n+     *\n+     * This method synchronizes the cache files that have been updated since the last time the method was invoked. To be able to do this,\n+     * the cache files must notify the {@link CacheService} when they need to be fsync. When a {@link CacheFile} notifies the service the\n+     * {@link CacheFile} instance is added to the current queue of cache files to synchronize referenced by {@link #cacheFilesToSync}.\n+     *\n+     * Cache files are serially synchronized using the {@link CacheFile#fsync()} method. When the {@link CacheFile#fsync()} call returns a\n+     * non empty set of completed ranges this method also fsync the shard's snapshot cache directory, which is the parent directory of the\n+     * cache entry. Note that cache files might be evicted during the synchronization.\n+     */\n+    protected void synchronizeCache() {\n+        long count = 0L;\n+        final Set<Path> cacheDirs = new HashSet<>();\n+        final long startTimeNanos = threadPool.relativeTimeInNanos();\n+        final int maxCacheFilesToSync = this.maxCacheFilesToSyncAtOnce;\n+        for (long i = 0L; i < maxCacheFilesToSync; i++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "af47aaa052d82b6033de699956a575ffd780ab73"}, "originalPosition": 202}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTAwODI2OQ==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r525008269", "createdAt": "2020-11-17T09:31:43Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CacheFile.java", "diffHunk": "@@ -444,12 +461,12 @@ boolean needsFsync() {\n                         assert completedRanges != null;\n                         assert completedRanges.isEmpty() == false;\n \n-                        IOUtils.fsync(file, false, false); // TODO don't forget to fsync parent directory\n+                        IOUtils.fsync(file, false, false);\n                         success = true;\n                         return completedRanges;\n                     } finally {\n                         if (success == false) {\n-                            needsFsync.set(true);\n+                            markAsNeedsFSync();\n                         }\n                     }\n                 }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDI2NjQwNA=="}, "originalCommit": {"oid": "f5d95597e8f27871f73cd1b9d2c83ae3f86da87d"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTAzOTAzOA==", "bodyText": "I think we only expect IOException here. Perhaps we can assert that to ensure tests will fail?\nSimilar assert a few lines up.", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r525039038", "createdAt": "2020-11-17T10:17:32Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -141,4 +195,123 @@ public void removeFromCache(final Predicate<CacheKey> predicate) {\n         }\n         cache.refresh();\n     }\n+\n+    void setCacheSyncInterval(TimeValue interval) {\n+        cacheSyncTask.setInterval(interval);\n+    }\n+\n+    private void setMaxCacheFilesToSyncAtOnce(int maxCacheFilesToSyncAtOnce) {\n+        this.maxCacheFilesToSyncAtOnce = maxCacheFilesToSyncAtOnce;\n+    }\n+\n+    /**\n+     * This method is invoked when a {@link CacheFile} notifies the current {@link CacheService} that it needs to be fsync on disk.\n+     * <p>\n+     * It adds the {@link CacheFile} instance to current set of cache files to synchronize.\n+     *\n+     * @param cacheFile the instance that needs to be fsync\n+     */\n+    void onCacheFileUpdate(CacheFile cacheFile) {\n+        cacheFilesToSync.offer(cacheFile);\n+    }\n+\n+    /**\n+     * This method is invoked after a {@link CacheFile} is evicted from the cache.\n+     * <p>\n+     * It notifies the {@link CacheFile}'s eviction listeners that the instance is evicted.d\n+     *\n+     * @param cacheFile the evicted instance\n+     */\n+    void onCacheFileRemoval(CacheFile cacheFile) {\n+        IOUtils.closeWhileHandlingException(cacheFile::startEviction);\n+    }\n+\n+    // used in tests\n+    boolean isCacheFileToSync(CacheFile cacheFile) {\n+        return cacheFilesToSync.contains(cacheFile);\n+    }\n+\n+    /**\n+     * Synchronize the cache files and their parent directories on disk.\n+     *\n+     * This method synchronizes the cache files that have been updated since the last time the method was invoked. To be able to do this,\n+     * the cache files must notify the {@link CacheService} when they need to be fsync. When a {@link CacheFile} notifies the service the\n+     * {@link CacheFile} instance is added to the current queue of cache files to synchronize referenced by {@link #cacheFilesToSync}.\n+     *\n+     * Cache files are serially synchronized using the {@link CacheFile#fsync()} method. When the {@link CacheFile#fsync()} call returns a\n+     * non empty set of completed ranges this method also fsync the shard's snapshot cache directory, which is the parent directory of the\n+     * cache entry. Note that cache files might be evicted during the synchronization.\n+     */\n+    protected void synchronizeCache() {\n+        long count = 0L;\n+        final Set<Path> cacheDirs = new HashSet<>();\n+        final long startTimeNanos = threadPool.relativeTimeInNanos();\n+        final int maxCacheFilesToSync = this.maxCacheFilesToSyncAtOnce;\n+        for (long i = 0L; i < maxCacheFilesToSync; i++) {\n+            if (lifecycleState() != Lifecycle.State.STARTED) {\n+                logger.debug(\"stopping cache synchronization (cache service is closing)\");\n+                break;\n+            }\n+            final CacheFile cacheFile = cacheFilesToSync.poll();\n+            if (cacheFile == null) {\n+                logger.debug(\"stopping cache synchronization (no more cache files to fsync)\");\n+                break;\n+            }\n+            final Path cacheFilePath = cacheFile.getFile();\n+            try {\n+                final SortedSet<Tuple<Long, Long>> ranges = cacheFile.fsync();\n+                if (ranges.isEmpty() == false) {\n+                    logger.trace(\"cache file [{}] synchronized with [{}] completed range(s)\", cacheFilePath.getFileName(), ranges.size());\n+                    final Path cacheDir = cacheFilePath.toAbsolutePath().getParent();\n+                    if (cacheDirs.add(cacheDir)) {\n+                        try {\n+                            IOUtils.fsync(cacheDir, true, false);\n+                            logger.trace(\"cache directory [{}] synchronized\", cacheDir);\n+                        } catch (Exception e) {\n+                            logger.warn(() -> new ParameterizedMessage(\"failed to synchronize cache directory [{}]\", cacheDir), e);\n+                        }\n+                    }\n+                    // TODO Index searchable snapshot shard information + cache file ranges in Lucene\n+                    count += 1L;\n+                }\n+            } catch (Exception e) {\n+                logger.warn(() -> new ParameterizedMessage(\"failed to fsync cache file [{}]\", cacheFilePath.getFileName()), e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "af47aaa052d82b6033de699956a575ffd780ab73"}, "originalPosition": 230}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTA1MTg1Mw==", "bodyText": "nit: I find it nicer to just pass in a Runnable here, to make it clear that a CacheFile can only request an fsync of itself.", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r525051853", "createdAt": "2020-11-17T10:37:14Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CacheFile.java", "diffHunk": "@@ -117,10 +124,11 @@ protected void closeInternal() {\n     @Nullable\n     private volatile FileChannelReference channelRef;\n \n-    public CacheFile(String description, long length, Path file) {\n+    public CacheFile(String description, long length, Path file, Consumer<CacheFile> fsyncListener) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "af47aaa052d82b6033de699956a575ffd780ab73"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTA1MzcxNA==", "bodyText": "This will add it back on the queue, so we continually try to fsync a bad file. I saw the comments Armin made in other places about this, we can tackle this in the same follow-up.", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r525053714", "createdAt": "2020-11-17T10:40:09Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CacheFile.java", "diffHunk": "@@ -444,12 +461,12 @@ boolean needsFsync() {\n                         assert completedRanges != null;\n                         assert completedRanges.isEmpty() == false;\n \n-                        IOUtils.fsync(file, false, false); // TODO don't forget to fsync parent directory\n+                        IOUtils.fsync(file, false, false);\n                         success = true;\n                         return completedRanges;\n                     } finally {\n                         if (success == false) {\n-                            needsFsync.set(true);\n+                            markAsNeedsFSync();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "af47aaa052d82b6033de699956a575ffd780ab73"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTA1NTE0NA==", "bodyText": "I wonder if we should lower this to 1 second.\nI can imagine us using this for a poor-mans rate limiter - set the interval to 1 second and number of files to sync to 10 on a spinning disk setup in case fsync'ing causes issues.", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r525055144", "createdAt": "2020-11-17T10:42:16Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -52,40 +68,78 @@\n         Setting.Property.NodeScope\n     );\n \n+    public static final TimeValue MIN_SNAPSHOT_CACHE_SYNC_INTERVAL = TimeValue.timeValueSeconds(10L);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "af47aaa052d82b6033de699956a575ffd780ab73"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTA2MDU4Nw==", "bodyText": "These fields are unused. Seems like you intended the rootDir to have significance?", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r525060587", "createdAt": "2020-11-17T10:50:53Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/index/store/cache/TestUtils.java", "diffHunk": "@@ -245,4 +291,46 @@ public void putAsync(\n             listener.onResponse(null);\n         }\n     }\n+\n+    /**\n+     * A {@link FileSystemProvider} that counts the number of times the method {@link FileChannel#force(boolean)} is executed on every\n+     * files.\n+     */\n+    public static class FSyncTrackingFileSystemProvider extends FilterFileSystemProvider {\n+\n+        private final Map<Path, AtomicInteger> files = new ConcurrentHashMap<>();\n+        private final FileSystem delegateInstance;\n+        private final Path rootDir;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "af47aaa052d82b6033de699956a575ffd780ab73"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTA3MTE0Ng==", "bodyText": "Would be good to verify that once we evicted, we do not fsync these CacheFiles anymore.", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r525071146", "createdAt": "2020-11-17T11:08:14Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheServiceTests.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.PathUtils;\n+import org.elasticsearch.common.io.PathUtilsForTesting;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.common.util.set.Sets;\n+import org.elasticsearch.index.Index;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.index.store.cache.CacheFile;\n+import org.elasticsearch.index.store.cache.CacheKey;\n+import org.elasticsearch.index.store.cache.TestUtils.FSyncTrackingFileSystemProvider;\n+import org.elasticsearch.repositories.IndexId;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.xpack.searchablesnapshots.AbstractSearchableSnapshotsTestCase;\n+import org.junit.AfterClass;\n+import org.junit.BeforeClass;\n+\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.HashMap;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.SortedSet;\n+\n+import static org.elasticsearch.index.store.cache.TestUtils.randomPopulateAndReads;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.is;\n+\n+public class CacheServiceTests extends AbstractSearchableSnapshotsTestCase {\n+\n+    private static FSyncTrackingFileSystemProvider fileSystemProvider;\n+\n+    @BeforeClass\n+    public static void installFileSystem() {\n+        fileSystemProvider = new FSyncTrackingFileSystemProvider(PathUtils.getDefaultFileSystem(), createTempDir());\n+        PathUtilsForTesting.installMock(fileSystemProvider.getFileSystem(null));\n+    }\n+\n+    @AfterClass\n+    public static void removeFileSystem() {\n+        PathUtilsForTesting.teardown();\n+    }\n+\n+    public void testCacheSynchronization() throws Exception {\n+        final int numShards = randomIntBetween(1, 3);\n+        final Index index = new Index(randomAlphaOfLength(5).toLowerCase(Locale.ROOT), UUIDs.randomBase64UUID(random()));\n+        final SnapshotId snapshotId = new SnapshotId(\"_snapshot_name\", UUIDs.randomBase64UUID(random()));\n+        final IndexId indexId = new IndexId(\"_index_name\", UUIDs.randomBase64UUID(random()));\n+\n+        logger.debug(\"--> creating shard cache directories on disk\");\n+        final Path[] shardsCacheDirs = new Path[numShards];\n+        for (int i = 0; i < numShards; i++) {\n+            final Path shardDataPath = randomFrom(nodeEnvironment.availableShardPaths(new ShardId(index, i)));\n+            assertFalse(Files.exists(shardDataPath));\n+\n+            logger.debug(\"--> creating directories [{}] for shard [{}]\", shardDataPath.toAbsolutePath(), i);\n+            shardsCacheDirs[i] = Files.createDirectories(CacheService.resolveSnapshotCache(shardDataPath).resolve(snapshotId.getUUID()));\n+        }\n+\n+        try (CacheService cacheService = defaultCacheService()) {\n+            cacheService.start();\n+\n+            logger.debug(\"--> setting large cache sync interval (explicit cache synchronization calls in test)\");\n+            cacheService.setCacheSyncInterval(TimeValue.timeValueMillis(Long.MAX_VALUE));\n+\n+            // Keep a count of the number of writes for every cache file existing in the cache\n+            final Map<CacheKey, Tuple<CacheFile, Integer>> previous = new HashMap<>();\n+\n+            for (int iteration = 0; iteration < between(1, 10); iteration++) {\n+\n+                final Map<CacheKey, Tuple<CacheFile, Integer>> updates = new HashMap<>();\n+\n+                logger.trace(\"--> more random reads/writes from existing cache files\");\n+                for (Map.Entry<CacheKey, Tuple<CacheFile, Integer>> cacheEntry : randomSubsetOf(previous.entrySet())) {\n+                    final CacheKey cacheKey = cacheEntry.getKey();\n+                    final CacheFile cacheFile = cacheEntry.getValue().v1();\n+\n+                    final CacheFile.EvictionListener listener = evictedCacheFile -> {};\n+                    cacheFile.acquire(listener);\n+\n+                    final SortedSet<Tuple<Long, Long>> newCacheRanges = randomPopulateAndReads(cacheFile);\n+                    assertThat(cacheService.isCacheFileToSync(cacheFile), is(newCacheRanges.isEmpty() == false));\n+                    if (newCacheRanges.isEmpty() == false) {\n+                        final int numberOfWrites = cacheEntry.getValue().v2() + 1;\n+                        updates.put(cacheKey, Tuple.tuple(cacheFile, numberOfWrites));\n+                    }\n+                    cacheFile.release(listener);\n+                }\n+\n+                logger.trace(\"--> creating new cache files and randomly read/write them\");\n+                for (int i = 0; i < between(1, 25); i++) {\n+                    final ShardId shardId = new ShardId(index, randomIntBetween(0, numShards - 1));\n+                    final String fileName = String.format(Locale.ROOT, \"file_%d_%d\", iteration, i);\n+                    final CacheKey cacheKey = new CacheKey(snapshotId, indexId, shardId, fileName);\n+                    final CacheFile cacheFile = cacheService.get(cacheKey, randomIntBetween(1, 10_000), shardsCacheDirs[shardId.id()]);\n+\n+                    final CacheFile.EvictionListener listener = evictedCacheFile -> {};\n+                    cacheFile.acquire(listener);\n+\n+                    final SortedSet<Tuple<Long, Long>> newRanges = randomPopulateAndReads(cacheFile);\n+                    assertThat(cacheService.isCacheFileToSync(cacheFile), is(newRanges.isEmpty() == false));\n+                    updates.put(cacheKey, Tuple.tuple(cacheFile, newRanges.isEmpty() ? 0 : 1));\n+                    cacheFile.release(listener);\n+                }\n+\n+                logger.trace(\"--> evicting random cache files\");\n+                for (CacheKey evictedCacheKey : randomSubsetOf(Sets.union(previous.keySet(), updates.keySet()))) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "af47aaa052d82b6033de699956a575ffd780ab73"}, "originalPosition": 115}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTA3NDk4OQ==", "bodyText": "I think we should bias towards smaller values, perhaps using scaledRandomInt?", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r525074989", "createdAt": "2020-11-17T11:14:38Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/xpack/searchablesnapshots/AbstractSearchableSnapshotsTestCase.java", "diffHunk": "@@ -63,19 +101,27 @@ protected CacheService randomCacheService() {\n         if (randomBoolean()) {\n             cacheSettings.put(CacheService.SNAPSHOT_CACHE_RANGE_SIZE_SETTING.getKey(), randomCacheRangeSize());\n         }\n-        return new CacheService(AbstractSearchableSnapshotsTestCase::noOpCacheCleaner, cacheSettings.build());\n+        if (randomBoolean()) {\n+            cacheSettings.put(\n+                CacheService.SNAPSHOT_CACHE_SYNC_INTERVAL_SETTING.getKey(),\n+                TimeValue.timeValueSeconds(randomLongBetween(MIN_SNAPSHOT_CACHE_SYNC_INTERVAL.getSeconds(), Long.MAX_VALUE))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "af47aaa052d82b6033de699956a575ffd780ab73"}, "originalPosition": 98}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "437c8ea0c64f9d3777762e19f96ec2ecc7f22b4d", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/437c8ea0c64f9d3777762e19f96ec2ecc7f22b4d", "committedDate": "2020-11-17T12:21:12Z", "message": "limit iterations"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c20bf75e9f58a71cfcc624b2cb230f3167c0c6b7", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/c20bf75e9f58a71cfcc624b2cb230f3167c0c6b7", "committedDate": "2020-11-17T12:21:12Z", "message": "assert IOE"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "365812d53aebf7023cdd8010f430274de0930247", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/365812d53aebf7023cdd8010f430274de0930247", "committedDate": "2020-11-17T12:21:12Z", "message": "min 1s"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8fe920be20ba2478f45e2940c688b9ab6ded559f", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/8fe920be20ba2478f45e2940c688b9ab6ded559f", "committedDate": "2020-11-17T12:23:44Z", "message": "scaledRandomIntBetween(1, 120)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "74f728f51f097f2129314ffdcc64bdc9eeebc554", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/74f728f51f097f2129314ffdcc64bdc9eeebc554", "committedDate": "2020-11-17T13:30:22Z", "message": "Use runnable"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5091a5db7dfd9a9cbb84dbbbf6ac61cc49ff97b0", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/5091a5db7dfd9a9cbb84dbbbf6ac61cc49ff97b0", "committedDate": "2020-11-17T13:48:20Z", "message": "mutualize FSyncTrackingFileSystemProvider"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f69dde9e9e911a02fbf728a378e65da8b56cba7b", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/f69dde9e9e911a02fbf728a378e65da8b56cba7b", "committedDate": "2020-11-17T14:32:35Z", "message": "check evictions"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "395845dff77748e3c873d8c5e0e2d3f5e7bfec97", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/395845dff77748e3c873d8c5e0e2d3f5e7bfec97", "committedDate": "2020-11-17T15:03:40Z", "message": "use atomic long to count cache files"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMyNDk5OTIw", "url": "https://github.com/elastic/elasticsearch/pull/64696#pullrequestreview-532499920", "createdAt": "2020-11-17T15:38:15Z", "commit": {"oid": "395845dff77748e3c873d8c5e0e2d3f5e7bfec97"}, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QxNTozODoxNlrOH07VEQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QxNTo0MDozOVrOH07coA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTI2MDA0OQ==", "bodyText": "NIT: we seem to generally wrap parameters in [{}] when logging?", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r525260049", "createdAt": "2020-11-17T15:38:16Z", "author": {"login": "original-brownbear"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -141,4 +202,129 @@ public void removeFromCache(final Predicate<CacheKey> predicate) {\n         }\n         cache.refresh();\n     }\n+\n+    void setCacheSyncInterval(TimeValue interval) {\n+        cacheSyncTask.setInterval(interval);\n+    }\n+\n+    private void setMaxCacheFilesToSyncAtOnce(int maxCacheFilesToSyncAtOnce) {\n+        this.maxCacheFilesToSyncAtOnce = maxCacheFilesToSyncAtOnce;\n+    }\n+\n+    /**\n+     * This method is invoked when a {@link CacheFile} notifies the current {@link CacheService} that it needs to be fsync on disk.\n+     * <p>\n+     * It adds the {@link CacheFile} instance to current set of cache files to synchronize.\n+     *\n+     * @param cacheFile the instance that needs to be fsync\n+     */\n+    void onCacheFileUpdate(CacheFile cacheFile) {\n+        assert cacheFile != null;\n+        cacheFilesToSync.offer(cacheFile);\n+        numberOfCacheFilesToSync.incrementAndGet();\n+    }\n+\n+    /**\n+     * This method is invoked after a {@link CacheFile} is evicted from the cache.\n+     * <p>\n+     * It notifies the {@link CacheFile}'s eviction listeners that the instance is evicted.\n+     *\n+     * @param cacheFile the evicted instance\n+     */\n+    void onCacheFileRemoval(CacheFile cacheFile) {\n+        IOUtils.closeWhileHandlingException(cacheFile::startEviction);\n+    }\n+\n+    // used in tests\n+    boolean isCacheFileToSync(CacheFile cacheFile) {\n+        return cacheFilesToSync.contains(cacheFile);\n+    }\n+\n+    /**\n+     * Synchronize the cache files and their parent directories on disk.\n+     *\n+     * This method synchronizes the cache files that have been updated since the last time the method was invoked. To be able to do this,\n+     * the cache files must notify the {@link CacheService} when they need to be fsync. When a {@link CacheFile} notifies the service the\n+     * {@link CacheFile} instance is added to the current queue of cache files to synchronize referenced by {@link #cacheFilesToSync}.\n+     *\n+     * Cache files are serially synchronized using the {@link CacheFile#fsync()} method. When the {@link CacheFile#fsync()} call returns a\n+     * non empty set of completed ranges this method also fsync the shard's snapshot cache directory, which is the parent directory of the\n+     * cache entry. Note that cache files might be evicted during the synchronization.\n+     */\n+    protected void synchronizeCache() {\n+        long count = 0L;\n+        final Set<Path> cacheDirs = new HashSet<>();\n+        final long startTimeNanos = threadPool.relativeTimeInNanos();\n+        final long maxCacheFilesToSync = Math.min(numberOfCacheFilesToSync.get(), this.maxCacheFilesToSyncAtOnce);\n+        for (long i = 0L; i < maxCacheFilesToSync; i++) {\n+            if (lifecycleState() != Lifecycle.State.STARTED) {\n+                logger.debug(\"stopping cache synchronization (cache service is closing)\");\n+                break;\n+            }\n+            final CacheFile cacheFile = cacheFilesToSync.poll();\n+            if (cacheFile == null) {\n+                logger.debug(\"stopping cache synchronization (no more cache files to fsync)\");\n+                break;\n+            }\n+            final long value = numberOfCacheFilesToSync.decrementAndGet();\n+            assert value >= 0 : value;\n+            final Path cacheFilePath = cacheFile.getFile();\n+            try {\n+                final SortedSet<Tuple<Long, Long>> ranges = cacheFile.fsync();\n+                if (ranges.isEmpty() == false) {\n+                    logger.trace(\"cache file [{}] synchronized with [{}] completed range(s)\", cacheFilePath.getFileName(), ranges.size());\n+                    final Path cacheDir = cacheFilePath.toAbsolutePath().getParent();\n+                    if (cacheDirs.add(cacheDir)) {\n+                        try {\n+                            IOUtils.fsync(cacheDir, true, false);\n+                            logger.trace(\"cache directory [{}] synchronized\", cacheDir);\n+                        } catch (Exception e) {\n+                            assert e instanceof IOException : e;\n+                            logger.warn(() -> new ParameterizedMessage(\"failed to synchronize cache directory [{}]\", cacheDir), e);\n+                        }\n+                    }\n+                    // TODO Index searchable snapshot shard information + cache file ranges in Lucene\n+                    count += 1L;\n+                }\n+            } catch (Exception e) {\n+                assert e instanceof IOException : e;\n+                logger.warn(() -> new ParameterizedMessage(\"failed to fsync cache file [{}]\", cacheFilePath.getFileName()), e);\n+            }\n+        }\n+        if (logger.isDebugEnabled()) {\n+            final long elapsedNanos = threadPool.relativeTimeInNanos() - startTimeNanos;\n+            logger.debug(\n+                \"cache files synchronization is done ({} cache files synchronized in {})\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "395845dff77748e3c873d8c5e0e2d3f5e7bfec97"}, "originalPosition": 249}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTI2MTk4NA==", "bodyText": "Could we theoretically assert that the service has been stopped here? (it seems the only way for the queue to be reduced is for this method to run and it should only ever run once at any point in time)", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r525261984", "createdAt": "2020-11-17T15:40:39Z", "author": {"login": "original-brownbear"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -141,4 +202,129 @@ public void removeFromCache(final Predicate<CacheKey> predicate) {\n         }\n         cache.refresh();\n     }\n+\n+    void setCacheSyncInterval(TimeValue interval) {\n+        cacheSyncTask.setInterval(interval);\n+    }\n+\n+    private void setMaxCacheFilesToSyncAtOnce(int maxCacheFilesToSyncAtOnce) {\n+        this.maxCacheFilesToSyncAtOnce = maxCacheFilesToSyncAtOnce;\n+    }\n+\n+    /**\n+     * This method is invoked when a {@link CacheFile} notifies the current {@link CacheService} that it needs to be fsync on disk.\n+     * <p>\n+     * It adds the {@link CacheFile} instance to current set of cache files to synchronize.\n+     *\n+     * @param cacheFile the instance that needs to be fsync\n+     */\n+    void onCacheFileUpdate(CacheFile cacheFile) {\n+        assert cacheFile != null;\n+        cacheFilesToSync.offer(cacheFile);\n+        numberOfCacheFilesToSync.incrementAndGet();\n+    }\n+\n+    /**\n+     * This method is invoked after a {@link CacheFile} is evicted from the cache.\n+     * <p>\n+     * It notifies the {@link CacheFile}'s eviction listeners that the instance is evicted.\n+     *\n+     * @param cacheFile the evicted instance\n+     */\n+    void onCacheFileRemoval(CacheFile cacheFile) {\n+        IOUtils.closeWhileHandlingException(cacheFile::startEviction);\n+    }\n+\n+    // used in tests\n+    boolean isCacheFileToSync(CacheFile cacheFile) {\n+        return cacheFilesToSync.contains(cacheFile);\n+    }\n+\n+    /**\n+     * Synchronize the cache files and their parent directories on disk.\n+     *\n+     * This method synchronizes the cache files that have been updated since the last time the method was invoked. To be able to do this,\n+     * the cache files must notify the {@link CacheService} when they need to be fsync. When a {@link CacheFile} notifies the service the\n+     * {@link CacheFile} instance is added to the current queue of cache files to synchronize referenced by {@link #cacheFilesToSync}.\n+     *\n+     * Cache files are serially synchronized using the {@link CacheFile#fsync()} method. When the {@link CacheFile#fsync()} call returns a\n+     * non empty set of completed ranges this method also fsync the shard's snapshot cache directory, which is the parent directory of the\n+     * cache entry. Note that cache files might be evicted during the synchronization.\n+     */\n+    protected void synchronizeCache() {\n+        long count = 0L;\n+        final Set<Path> cacheDirs = new HashSet<>();\n+        final long startTimeNanos = threadPool.relativeTimeInNanos();\n+        final long maxCacheFilesToSync = Math.min(numberOfCacheFilesToSync.get(), this.maxCacheFilesToSyncAtOnce);\n+        for (long i = 0L; i < maxCacheFilesToSync; i++) {\n+            if (lifecycleState() != Lifecycle.State.STARTED) {\n+                logger.debug(\"stopping cache synchronization (cache service is closing)\");\n+                break;\n+            }\n+            final CacheFile cacheFile = cacheFilesToSync.poll();\n+            if (cacheFile == null) {\n+                logger.debug(\"stopping cache synchronization (no more cache files to fsync)\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "395845dff77748e3c873d8c5e0e2d3f5e7bfec97"}, "originalPosition": 218}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "082b76db40bd1076b57905433caff29247dc4859", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/082b76db40bd1076b57905433caff29247dc4859", "committedDate": "2020-11-18T09:09:09Z", "message": "[] + synchronized assert"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMzMzAwOTAw", "url": "https://github.com/elastic/elasticsearch/pull/64696#pullrequestreview-533300900", "createdAt": "2020-11-18T10:25:29Z", "commit": {"oid": "082b76db40bd1076b57905433caff29247dc4859"}, "state": "COMMENTED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQxMDoyNToyOVrOH1m8GQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQxMToyMTo0MlrOH1pHWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk3NDU1Mw==", "bodyText": "nit: I would find it more readable with an explicit lock object. It also reduces the risk of/if someone synchronizing another method in this class", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r525974553", "createdAt": "2020-11-18T10:25:29Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -251,7 +251,7 @@ boolean isCacheFileToSync(CacheFile cacheFile) {\n      * non empty set of completed ranges this method also fsync the shard's snapshot cache directory, which is the parent directory of the\n      * cache entry. Note that cache files might be evicted during the synchronization.\n      */\n-    protected void synchronizeCache() {\n+    protected synchronized void synchronizeCache() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "082b76db40bd1076b57905433caff29247dc4859"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk4NzIyMA==", "bodyText": "Should we also wait (with timeout) for any ongoing cache sync task to complete?", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r525987220", "createdAt": "2020-11-18T10:44:59Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -52,40 +71,80 @@\n         Setting.Property.NodeScope\n     );\n \n+    public static final TimeValue MIN_SNAPSHOT_CACHE_SYNC_INTERVAL = TimeValue.timeValueSeconds(1L);\n+    public static final Setting<TimeValue> SNAPSHOT_CACHE_SYNC_INTERVAL_SETTING = Setting.timeSetting(\n+        SETTINGS_PREFIX + \"sync_interval\",\n+        TimeValue.timeValueSeconds(60L),                        // default\n+        MIN_SNAPSHOT_CACHE_SYNC_INTERVAL,                       // min\n+        Setting.Property.NodeScope,\n+        Setting.Property.Dynamic\n+    );\n+\n+    public static final Setting<Integer> SNAPSHOT_CACHE_MAX_FILES_TO_SYNC_AT_ONCE_SETTING = Setting.intSetting(\n+        SETTINGS_PREFIX + \"max_files_to_sync\",\n+        10_000,                                                 // default\n+        0,                                                      // min\n+        Integer.MAX_VALUE,                                      // min\n+        Setting.Property.NodeScope,\n+        Setting.Property.Dynamic\n+    );\n+\n+    private static final Logger logger = LogManager.getLogger(CacheService.class);\n+\n+    private final ThreadPool threadPool;\n+    private final ConcurrentLinkedQueue<CacheFile> cacheFilesToSync;\n+    private final AtomicLong numberOfCacheFilesToSync;\n+    private final CacheSynchronizationTask cacheSyncTask;\n     private final Cache<CacheKey, CacheFile> cache;\n     private final ByteSizeValue cacheSize;\n     private final Runnable cacheCleaner;\n     private final ByteSizeValue rangeSize;\n \n-    public CacheService(final Runnable cacheCleaner, final Settings settings) {\n-        this(cacheCleaner, SNAPSHOT_CACHE_SIZE_SETTING.get(settings), SNAPSHOT_CACHE_RANGE_SIZE_SETTING.get(settings));\n-    }\n+    private volatile int maxCacheFilesToSyncAtOnce;\n \n-    // exposed for tests\n-    public CacheService(final Runnable cacheCleaner, final ByteSizeValue cacheSize, final ByteSizeValue rangeSize) {\n-        this.cacheSize = Objects.requireNonNull(cacheSize);\n+    public CacheService(\n+        final Settings settings,\n+        final ClusterService clusterService,\n+        final ThreadPool threadPool,\n+        final Runnable cacheCleaner\n+    ) {\n+        this.threadPool = Objects.requireNonNull(threadPool);\n+        this.cacheSize = SNAPSHOT_CACHE_SIZE_SETTING.get(settings);\n         this.cacheCleaner = Objects.requireNonNull(cacheCleaner);\n-        this.rangeSize = Objects.requireNonNull(rangeSize);\n+        this.rangeSize = SNAPSHOT_CACHE_RANGE_SIZE_SETTING.get(settings);\n         this.cache = CacheBuilder.<CacheKey, CacheFile>builder()\n             .setMaximumWeight(cacheSize.getBytes())\n             .weigher((key, entry) -> entry.getLength())\n             // NORELEASE This does not immediately free space on disk, as cache file are only deleted when all index inputs\n             // are done with reading/writing the cache file\n-            .removalListener(notification -> IOUtils.closeWhileHandlingException(() -> notification.getValue().startEviction()))\n+            .removalListener(notification -> onCacheFileRemoval(notification.getValue()))\n             .build();\n+        this.numberOfCacheFilesToSync = new AtomicLong();\n+        this.cacheFilesToSync = new ConcurrentLinkedQueue<>();\n+        final ClusterSettings clusterSettings = clusterService.getClusterSettings();\n+        this.maxCacheFilesToSyncAtOnce = SNAPSHOT_CACHE_MAX_FILES_TO_SYNC_AT_ONCE_SETTING.get(settings);\n+        clusterSettings.addSettingsUpdateConsumer(SNAPSHOT_CACHE_MAX_FILES_TO_SYNC_AT_ONCE_SETTING, this::setMaxCacheFilesToSyncAtOnce);\n+        this.cacheSyncTask = new CacheSynchronizationTask(threadPool, SNAPSHOT_CACHE_SYNC_INTERVAL_SETTING.get(settings));\n+        clusterSettings.addSettingsUpdateConsumer(SNAPSHOT_CACHE_SYNC_INTERVAL_SETTING, this::setCacheSyncInterval);\n     }\n \n     public static Path getShardCachePath(ShardPath shardPath) {\n-        return shardPath.getDataPath().resolve(\"snapshot_cache\");\n+        return resolveSnapshotCache(shardPath.getDataPath());\n+    }\n+\n+    static Path resolveSnapshotCache(Path path) {\n+        return path.resolve(\"snapshot_cache\");\n     }\n \n     @Override\n     protected void doStart() {\n+        cacheSyncTask.rescheduleIfNecessary();\n         cacheCleaner.run();\n     }\n \n     @Override\n     protected void doStop() {\n+        cacheSyncTask.close();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "082b76db40bd1076b57905433caff29247dc4859"}, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk4ODM3NQ==", "bodyText": "I think this can be private?", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r525988375", "createdAt": "2020-11-18T10:46:46Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -141,4 +202,127 @@ public void removeFromCache(final Predicate<CacheKey> predicate) {\n         }\n         cache.refresh();\n     }\n+\n+    void setCacheSyncInterval(TimeValue interval) {\n+        cacheSyncTask.setInterval(interval);\n+    }\n+\n+    private void setMaxCacheFilesToSyncAtOnce(int maxCacheFilesToSyncAtOnce) {\n+        this.maxCacheFilesToSyncAtOnce = maxCacheFilesToSyncAtOnce;\n+    }\n+\n+    /**\n+     * This method is invoked when a {@link CacheFile} notifies the current {@link CacheService} that it needs to be fsync on disk.\n+     * <p>\n+     * It adds the {@link CacheFile} instance to current set of cache files to synchronize.\n+     *\n+     * @param cacheFile the instance that needs to be fsync\n+     */\n+    void onCacheFileUpdate(CacheFile cacheFile) {\n+        assert cacheFile != null;\n+        cacheFilesToSync.offer(cacheFile);\n+        numberOfCacheFilesToSync.incrementAndGet();\n+    }\n+\n+    /**\n+     * This method is invoked after a {@link CacheFile} is evicted from the cache.\n+     * <p>\n+     * It notifies the {@link CacheFile}'s eviction listeners that the instance is evicted.\n+     *\n+     * @param cacheFile the evicted instance\n+     */\n+    void onCacheFileRemoval(CacheFile cacheFile) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "082b76db40bd1076b57905433caff29247dc4859"}, "originalPosition": 186}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk4ODQyNg==", "bodyText": "I think this can be private?", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r525988426", "createdAt": "2020-11-18T10:46:50Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -141,4 +202,127 @@ public void removeFromCache(final Predicate<CacheKey> predicate) {\n         }\n         cache.refresh();\n     }\n+\n+    void setCacheSyncInterval(TimeValue interval) {\n+        cacheSyncTask.setInterval(interval);\n+    }\n+\n+    private void setMaxCacheFilesToSyncAtOnce(int maxCacheFilesToSyncAtOnce) {\n+        this.maxCacheFilesToSyncAtOnce = maxCacheFilesToSyncAtOnce;\n+    }\n+\n+    /**\n+     * This method is invoked when a {@link CacheFile} notifies the current {@link CacheService} that it needs to be fsync on disk.\n+     * <p>\n+     * It adds the {@link CacheFile} instance to current set of cache files to synchronize.\n+     *\n+     * @param cacheFile the instance that needs to be fsync\n+     */\n+    void onCacheFileUpdate(CacheFile cacheFile) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "082b76db40bd1076b57905433caff29247dc4859"}, "originalPosition": 173}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk5MDk5NQ==", "bodyText": "Do we need to fsync the directory except for the first time we fsync a file created in the directory? Perhaps we should do that when creating the file instead?\nThis can be handled in a follow-up if you prefer (and agree).", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r525990995", "createdAt": "2020-11-18T10:50:53Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -141,4 +202,127 @@ public void removeFromCache(final Predicate<CacheKey> predicate) {\n         }\n         cache.refresh();\n     }\n+\n+    void setCacheSyncInterval(TimeValue interval) {\n+        cacheSyncTask.setInterval(interval);\n+    }\n+\n+    private void setMaxCacheFilesToSyncAtOnce(int maxCacheFilesToSyncAtOnce) {\n+        this.maxCacheFilesToSyncAtOnce = maxCacheFilesToSyncAtOnce;\n+    }\n+\n+    /**\n+     * This method is invoked when a {@link CacheFile} notifies the current {@link CacheService} that it needs to be fsync on disk.\n+     * <p>\n+     * It adds the {@link CacheFile} instance to current set of cache files to synchronize.\n+     *\n+     * @param cacheFile the instance that needs to be fsync\n+     */\n+    void onCacheFileUpdate(CacheFile cacheFile) {\n+        assert cacheFile != null;\n+        cacheFilesToSync.offer(cacheFile);\n+        numberOfCacheFilesToSync.incrementAndGet();\n+    }\n+\n+    /**\n+     * This method is invoked after a {@link CacheFile} is evicted from the cache.\n+     * <p>\n+     * It notifies the {@link CacheFile}'s eviction listeners that the instance is evicted.\n+     *\n+     * @param cacheFile the evicted instance\n+     */\n+    void onCacheFileRemoval(CacheFile cacheFile) {\n+        IOUtils.closeWhileHandlingException(cacheFile::startEviction);\n+    }\n+\n+    // used in tests\n+    boolean isCacheFileToSync(CacheFile cacheFile) {\n+        return cacheFilesToSync.contains(cacheFile);\n+    }\n+\n+    /**\n+     * Synchronize the cache files and their parent directories on disk.\n+     *\n+     * This method synchronizes the cache files that have been updated since the last time the method was invoked. To be able to do this,\n+     * the cache files must notify the {@link CacheService} when they need to be fsync. When a {@link CacheFile} notifies the service the\n+     * {@link CacheFile} instance is added to the current queue of cache files to synchronize referenced by {@link #cacheFilesToSync}.\n+     *\n+     * Cache files are serially synchronized using the {@link CacheFile#fsync()} method. When the {@link CacheFile#fsync()} call returns a\n+     * non empty set of completed ranges this method also fsync the shard's snapshot cache directory, which is the parent directory of the\n+     * cache entry. Note that cache files might be evicted during the synchronization.\n+     */\n+    protected synchronized void synchronizeCache() {\n+        long count = 0L;\n+        final Set<Path> cacheDirs = new HashSet<>();\n+        final long startTimeNanos = threadPool.relativeTimeInNanos();\n+        final long maxCacheFilesToSync = Math.min(numberOfCacheFilesToSync.get(), this.maxCacheFilesToSyncAtOnce);\n+        for (long i = 0L; i < maxCacheFilesToSync; i++) {\n+            if (lifecycleState() != Lifecycle.State.STARTED) {\n+                logger.debug(\"stopping cache synchronization (cache service is closing)\");\n+                break;\n+            }\n+            final CacheFile cacheFile = cacheFilesToSync.poll();\n+            assert cacheFile != null;\n+\n+            final long value = numberOfCacheFilesToSync.decrementAndGet();\n+            assert value >= 0 : value;\n+            final Path cacheFilePath = cacheFile.getFile();\n+            try {\n+                final SortedSet<Tuple<Long, Long>> ranges = cacheFile.fsync();\n+                if (ranges.isEmpty() == false) {\n+                    logger.trace(\"cache file [{}] synchronized with [{}] completed range(s)\", cacheFilePath.getFileName(), ranges.size());\n+                    final Path cacheDir = cacheFilePath.toAbsolutePath().getParent();\n+                    if (cacheDirs.add(cacheDir)) {\n+                        try {\n+                            IOUtils.fsync(cacheDir, true, false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "082b76db40bd1076b57905433caff29247dc4859"}, "originalPosition": 229}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk5NTI5OQ==", "bodyText": "Can we move this into a method on the file system provider so that it just reads:\nprovider.tearDown();\n\nhere?", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r525995299", "createdAt": "2020-11-18T10:57:31Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/index/store/cache/CacheFileTests.java", "diffHunk": "@@ -271,23 +300,30 @@ public void testFSyncFailure() throws Exception {\n                 final SortedSet<Tuple<Long, Long>> expectedCompletedRanges = randomPopulateAndReads(cacheFile);\n                 if (expectedCompletedRanges.isEmpty() == false) {\n                     assertTrue(cacheFile.needsFsync());\n-                    expectThrows(IOException.class, cacheFile::fsync);\n+                    assertTrue(needsFSyncCalled.getAndSet(false));\n+                    IOException exception = expectThrows(IOException.class, cacheFile::fsync);\n+                    assertThat(exception.getMessage(), containsString(\"simulated\"));\n+                    assertTrue(cacheFile.needsFsync());\n+                    assertTrue(needsFSyncCalled.getAndSet(false));\n                 } else {\n                     assertFalse(cacheFile.needsFsync());\n                     final SortedSet<Tuple<Long, Long>> completedRanges = cacheFile.fsync();\n                     assertTrue(completedRanges.isEmpty());\n                 }\n-                assertNumberOfFSyncs(cacheFile.getFile(), equalTo(0L));\n+                assertNumberOfFSyncs(cacheFile.getFile(), equalTo(0));\n \n-                fileSystem.failFSyncs.set(false);\n+                fileSystem.failFSyncs(false);\n \n                 final SortedSet<Tuple<Long, Long>> completedRanges = cacheFile.fsync();\n                 assertArrayEquals(completedRanges.toArray(Tuple[]::new), expectedCompletedRanges.toArray(Tuple[]::new));\n-                assertNumberOfFSyncs(cacheFile.getFile(), equalTo(expectedCompletedRanges.isEmpty() ? 0L : 1L));\n+                assertNumberOfFSyncs(cacheFile.getFile(), equalTo(expectedCompletedRanges.isEmpty() ? 0 : 1));\n                 assertFalse(cacheFile.needsFsync());\n+                assertFalse(needsFSyncCalled.get());\n             } finally {\n                 cacheFile.release(listener);\n             }\n+        } finally {\n+            PathUtilsForTesting.installMock(fileSystem.getDelegateInstance());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "082b76db40bd1076b57905433caff29247dc4859"}, "originalPosition": 241}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjAwMDg5Mw==", "bodyText": "I think we should do this before cacheService.start() to avoid a race condition of the schedule task \"just started to run\" before we change this, which could interfere with the testing below.", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r526000893", "createdAt": "2020-11-18T11:06:23Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheServiceTests.java", "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.PathUtils;\n+import org.elasticsearch.common.io.PathUtilsForTesting;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.common.util.set.Sets;\n+import org.elasticsearch.index.Index;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.index.store.cache.CacheFile;\n+import org.elasticsearch.index.store.cache.CacheKey;\n+import org.elasticsearch.index.store.cache.TestUtils.FSyncTrackingFileSystemProvider;\n+import org.elasticsearch.repositories.IndexId;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.xpack.searchablesnapshots.AbstractSearchableSnapshotsTestCase;\n+import org.junit.AfterClass;\n+import org.junit.BeforeClass;\n+\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.HashMap;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.SortedSet;\n+\n+import static org.elasticsearch.index.store.cache.TestUtils.randomPopulateAndReads;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.is;\n+\n+public class CacheServiceTests extends AbstractSearchableSnapshotsTestCase {\n+\n+    private static FSyncTrackingFileSystemProvider fileSystemProvider;\n+\n+    @BeforeClass\n+    public static void installFileSystem() {\n+        fileSystemProvider = new FSyncTrackingFileSystemProvider(PathUtils.getDefaultFileSystem(), createTempDir());\n+        PathUtilsForTesting.installMock(fileSystemProvider.getFileSystem(null));\n+    }\n+\n+    @AfterClass\n+    public static void removeFileSystem() {\n+        PathUtilsForTesting.teardown();\n+    }\n+\n+    public void testCacheSynchronization() throws Exception {\n+        final int numShards = randomIntBetween(1, 3);\n+        final Index index = new Index(randomAlphaOfLength(5).toLowerCase(Locale.ROOT), UUIDs.randomBase64UUID(random()));\n+        final SnapshotId snapshotId = new SnapshotId(\"_snapshot_name\", UUIDs.randomBase64UUID(random()));\n+        final IndexId indexId = new IndexId(\"_index_name\", UUIDs.randomBase64UUID(random()));\n+\n+        logger.debug(\"--> creating shard cache directories on disk\");\n+        final Path[] shardsCacheDirs = new Path[numShards];\n+        for (int i = 0; i < numShards; i++) {\n+            final Path shardDataPath = randomFrom(nodeEnvironment.availableShardPaths(new ShardId(index, i)));\n+            assertFalse(Files.exists(shardDataPath));\n+\n+            logger.debug(\"--> creating directories [{}] for shard [{}]\", shardDataPath.toAbsolutePath(), i);\n+            shardsCacheDirs[i] = Files.createDirectories(CacheService.resolveSnapshotCache(shardDataPath).resolve(snapshotId.getUUID()));\n+        }\n+\n+        try (CacheService cacheService = defaultCacheService()) {\n+            cacheService.start();\n+\n+            logger.debug(\"--> setting large cache sync interval (explicit cache synchronization calls in test)\");\n+            cacheService.setCacheSyncInterval(TimeValue.timeValueMillis(Long.MAX_VALUE));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "082b76db40bd1076b57905433caff29247dc4859"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjAwNDE4Nw==", "bodyText": "This looks like it is a near identical copy from CacheFileTests, but the original method has not been removed, I think it should?", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r526004187", "createdAt": "2020-11-18T11:11:49Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/index/store/cache/TestUtils.java", "diffHunk": "@@ -22,20 +27,62 @@\n import java.io.FileNotFoundException;\n import java.io.IOException;\n import java.io.InputStream;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.FileSystem;\n+import java.nio.file.OpenOption;\n+import java.nio.file.Path;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.spi.FileSystemProvider;\n+import java.util.ArrayList;\n import java.util.Comparator;\n import java.util.List;\n import java.util.Map;\n+import java.util.Set;\n import java.util.SortedSet;\n import java.util.TreeSet;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n \n+import static java.util.Collections.synchronizedNavigableSet;\n+import static org.apache.lucene.util.LuceneTestCase.random;\n+import static org.elasticsearch.common.settings.Settings.builder;\n+import static org.elasticsearch.node.Node.NODE_NAME_SETTING;\n+import static org.elasticsearch.test.ESTestCase.between;\n+import static org.elasticsearch.test.ESTestCase.randomLongBetween;\n import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.toIntBytes;\n import static org.hamcrest.MatcherAssert.assertThat;\n import static org.hamcrest.Matchers.equalTo;\n+import static org.junit.Assert.assertTrue;\n import static org.mockito.Mockito.mock;\n \n public final class TestUtils {\n     private TestUtils() {}\n \n+    public static SortedSet<Tuple<Long, Long>> randomPopulateAndReads(final CacheFile cacheFile) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "082b76db40bd1076b57905433caff29247dc4859"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjAxMDIwMg==", "bodyText": "That the tests work with this confused me for a while - but CacheFile uses deleteIfExists, which delegates to the original file system rather than going here. We need to keep the fsync counters for the check that we no longer fsync after evict to work.\nI wonder if this override of delete is at all necessary?", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r526010202", "createdAt": "2020-11-18T11:21:42Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/index/store/cache/TestUtils.java", "diffHunk": "@@ -245,4 +292,62 @@ public void putAsync(\n             listener.onResponse(null);\n         }\n     }\n+\n+    /**\n+     * A {@link FileSystemProvider} that counts the number of times the method {@link FileChannel#force(boolean)} is executed on every\n+     * files.\n+     */\n+    public static class FSyncTrackingFileSystemProvider extends FilterFileSystemProvider {\n+\n+        private final Map<Path, AtomicInteger> files = new ConcurrentHashMap<>();\n+        private final AtomicBoolean failFSyncs = new AtomicBoolean();\n+        private final FileSystem delegateInstance;\n+        private final Path rootDir;\n+\n+        public FSyncTrackingFileSystemProvider(FileSystem delegate, Path rootDir) {\n+            super(\"fsynccounting://\", delegate);\n+            this.rootDir = new FilterPath(rootDir, this.fileSystem);\n+            this.delegateInstance = delegate;\n+        }\n+\n+        public FileSystem getDelegateInstance() {\n+            return delegateInstance;\n+        }\n+\n+        public void failFSyncs(boolean shouldFail) {\n+            failFSyncs.set(shouldFail);\n+        }\n+\n+        public Path resolve(String other) {\n+            return rootDir.resolve(other);\n+        }\n+\n+        @Nullable\n+        public Integer getNumberOfFSyncs(Path path) {\n+            final AtomicInteger counter = files.get(path);\n+            return counter != null ? counter.get() : null;\n+        }\n+\n+        @Override\n+        public FileChannel newFileChannel(Path path, Set<? extends OpenOption> options, FileAttribute<?>... attrs) throws IOException {\n+            final AtomicInteger counter = files.computeIfAbsent(path, p -> new AtomicInteger(0));\n+            return new FilterFileChannel(delegate.newFileChannel(toDelegate(path), options, attrs)) {\n+\n+                @Override\n+                public void force(boolean metaData) throws IOException {\n+                    if (failFSyncs.get()) {\n+                        throw new IOException(\"simulated\");\n+                    }\n+                    super.force(metaData);\n+                    counter.incrementAndGet();\n+                }\n+            };\n+        }\n+\n+        @Override\n+        public void delete(Path path) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "082b76db40bd1076b57905433caff29247dc4859"}, "originalPosition": 145}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ea34570bfaf68a928b29187c32fa1f3686b85fd9", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/ea34570bfaf68a928b29187c32fa1f3686b85fd9", "committedDate": "2020-11-20T09:44:30Z", "message": "private"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "54d051d8829b323fe7748daee37ac5ff5aefd170", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/54d051d8829b323fe7748daee37ac5ff5aefd170", "committedDate": "2020-11-20T10:02:03Z", "message": "provider.tearDown()"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e8ad9be817caba9d9f7ff4ea33d60ee56cfbae53", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/e8ad9be817caba9d9f7ff4ea33d60ee56cfbae53", "committedDate": "2020-11-20T10:07:05Z", "message": "randomPopulateAndReads"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6066924795a61fb689cf0b2740e17074019b5eeb", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/6066924795a61fb689cf0b2740e17074019b5eeb", "committedDate": "2020-11-20T10:08:50Z", "message": "set interval before start"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e9721c43b8c24cb74475e7ecbd1fb2e4f5feda38", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/e9721c43b8c24cb74475e7ecbd1fb2e4f5feda38", "committedDate": "2020-11-20T10:52:19Z", "message": "deleteIfExists"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bd1ce13f170c3482ea7d78a09f75857ce3b2bc46", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/bd1ce13f170c3482ea7d78a09f75857ce3b2bc46", "committedDate": "2020-11-20T13:16:31Z", "message": "lock & waitfor termination"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "46adb3c4344e330a099a1642da31b25a668a5a34", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/46adb3c4344e330a099a1642da31b25a668a5a34", "committedDate": "2020-11-20T13:17:53Z", "message": "Merge branch 'master' into periodic-fsync"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "04a4d77ed3762dd581bd8b6d57eaef88ef322af3", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/04a4d77ed3762dd581bd8b6d57eaef88ef322af3", "committedDate": "2020-11-20T13:24:36Z", "message": "missing close"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "db24043323d0cd4ac10470d839ace3ea4293a7a5", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/db24043323d0cd4ac10470d839ace3ea4293a7a5", "committedDate": "2020-11-23T08:22:21Z", "message": "Merge branch 'master' into periodic-fsync"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM2MzAyMzI2", "url": "https://github.com/elastic/elasticsearch/pull/64696#pullrequestreview-536302326", "createdAt": "2020-11-23T09:39:25Z", "commit": {"oid": "db24043323d0cd4ac10470d839ace3ea4293a7a5"}, "state": "APPROVED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QwOTozOToyNlrOH4FgEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QwOTo0NjoyN1rOH4FxJA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODU3MjQzMg==", "bodyText": "Perhaps just 10 seconds default, since we only need to wait for one fsync and if it takes more than 10s to do one, we really want to continue shutting down the node anyway? The other doStop timeouts that I found (did not search thoroughly though) are in the 10-30s range.", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r528572432", "createdAt": "2020-11-23T09:39:26Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -73,28 +74,37 @@\n \n     public static final TimeValue MIN_SNAPSHOT_CACHE_SYNC_INTERVAL = TimeValue.timeValueSeconds(1L);\n     public static final Setting<TimeValue> SNAPSHOT_CACHE_SYNC_INTERVAL_SETTING = Setting.timeSetting(\n-        SETTINGS_PREFIX + \"sync_interval\",\n+        SETTINGS_PREFIX + \"sync.interval\",\n         TimeValue.timeValueSeconds(60L),                        // default\n         MIN_SNAPSHOT_CACHE_SYNC_INTERVAL,                       // min\n         Setting.Property.NodeScope,\n         Setting.Property.Dynamic\n     );\n \n     public static final Setting<Integer> SNAPSHOT_CACHE_MAX_FILES_TO_SYNC_AT_ONCE_SETTING = Setting.intSetting(\n-        SETTINGS_PREFIX + \"max_files_to_sync\",\n+        SETTINGS_PREFIX + \"sync.max_files\",\n         10_000,                                                 // default\n         0,                                                      // min\n-        Integer.MAX_VALUE,                                      // min\n+        Integer.MAX_VALUE,                                      // max\n         Setting.Property.NodeScope,\n         Setting.Property.Dynamic\n     );\n \n+    public static final Setting<TimeValue> SNAPSHOT_CACHE_SYNC_SHUTDOWN_TIMEOUT = Setting.timeSetting(\n+        SETTINGS_PREFIX + \"sync.shutdown_timeout\",\n+        TimeValue.timeValueSeconds(60L),                        // default", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "db24043323d0cd4ac10470d839ace3ea4293a7a5"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODU3NTg0Nw==", "bodyText": "I think we need to also do cacheSyncTask.close() and cache.invalidateAll() in this case? Possibly better to surround the tryLock with a separate try catch for InterruptedException.", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r528575847", "createdAt": "2020-11-23T09:44:51Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -144,8 +156,22 @@ protected void doStart() {\n \n     @Override\n     protected void doStop() {\n-        cacheSyncTask.close();\n-        cache.invalidateAll();\n+        boolean acquired = false;\n+        try {\n+            acquired = cacheSyncLock.tryLock(cacheSyncStopTimeout.duration(), cacheSyncStopTimeout.timeUnit());\n+            if (acquired == false) {\n+                logger.warn(\"failed to acquire cache sync lock in [{}], cache might be partially persisted\", cacheSyncStopTimeout);\n+            }\n+            cacheSyncTask.close();\n+            cache.invalidateAll();\n+        } catch (InterruptedException e) {\n+            Thread.currentThread().interrupt();\n+            logger.warn(\"interrupted while waiting for cache sync lock\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "db24043323d0cd4ac10470d839ace3ea4293a7a5"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODU3NjgwNA==", "bodyText": "I would prefer to keep this inside the loop to ensure we break out as soon as possible when shutting down.", "url": "https://github.com/elastic/elasticsearch/pull/64696#discussion_r528576804", "createdAt": "2020-11-23T09:46:27Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -251,51 +277,62 @@ boolean isCacheFileToSync(CacheFile cacheFile) {\n      * non empty set of completed ranges this method also fsync the shard's snapshot cache directory, which is the parent directory of the\n      * cache entry. Note that cache files might be evicted during the synchronization.\n      */\n-    protected synchronized void synchronizeCache() {\n-        long count = 0L;\n-        final Set<Path> cacheDirs = new HashSet<>();\n-        final long startTimeNanos = threadPool.relativeTimeInNanos();\n-        final long maxCacheFilesToSync = Math.min(numberOfCacheFilesToSync.get(), this.maxCacheFilesToSyncAtOnce);\n-        for (long i = 0L; i < maxCacheFilesToSync; i++) {\n+    protected void synchronizeCache() {\n+        cacheSyncLock.lock();\n+        try {\n             if (lifecycleState() != Lifecycle.State.STARTED) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "db24043323d0cd4ac10470d839ace3ea4293a7a5"}, "originalPosition": 120}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "10216ea40af97363b47646ffe3bcadb384f0fbde", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/10216ea40af97363b47646ffe3bcadb384f0fbde", "committedDate": "2020-11-23T10:16:10Z", "message": "nits"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1350, "cost": 1, "resetAt": "2021-10-28T18:54:27Z"}}}