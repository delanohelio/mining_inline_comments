{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ3MDg5MDk0", "number": 59341, "title": "Clean up a few of vwh's rough edges", "bodyText": "This cleans up a few rough edged in the variable_width_histogram,\nmostly found by @wwang500:\n\nSetting its tuning parameters in an unexpected order could cause the\nrequest to fail.\nWe checked that the maximum number of buckets was both less than\n50000 and MAX_BUCKETS. This drops the 50000.\nFixes a divide by 0 that can occur of the shard_size is 1.\nFixes a divide by 0 that can occur if the shard_size * 3 overflows\na signed int.\nRequires shard_size * 3 / 4 to be at least buckets. If it is less\nthan buckets we will very consistently return fewer buckets than\nrequested. For the most part we expect folks to leave it at the\ndefault. If they change it, we expect it to be much bigger than\nbuckets.\nAllocate a smaller mergeMap in when initially bucketing requests\nthat don't use the entire shard_size * 3 / 4. Its just a waste.\nDefault shard_size to 10 * buckets rather than 100. It looks\nlike that was our intention the whole time. And it feels like it'd\nkeep the algorithm humming along more smoothly.\nDefault the initial_buffer to min(10 * shard_size, 50000) like\nwe've documented it rather than 5000. Like the point above, this\nfeels like the right thing to do to keep the algorithm happy.", "createdAt": "2020-07-09T20:55:09Z", "url": "https://github.com/elastic/elasticsearch/pull/59341", "merged": true, "mergeCommit": {"oid": "27efb5f3b80a1ed4d1669de2b68ab359101db770"}, "closed": true, "closedAt": "2020-07-17T17:39:28Z", "author": {"login": "nik9000"}, "timelineItems": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABczVWjyAH2gAyNDQ3MDg5MDk0OmFiZDE1YTdjMDk3NDM5NWMyMTFiNDlhM2JhZWNkNGI2ZTY3ZjE0NWM=", "endCursor": "Y3Vyc29yOnYyOpPPAAABc12fNjAH2gAyNDQ3MDg5MDk0OjM5YzRhMzgwZWU2ZGUwMGJhYzNhMTI0ODFkMzhjZWQzY2FkMGU3MTI=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "abd15a7c0974395c211b49a3baecd4b6e67f145c", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/abd15a7c0974395c211b49a3baecd4b6e67f145c", "committedDate": "2020-07-09T20:48:52Z", "message": "Clean up a few of vwh's rough edges\n\nThis cleans up a few rough edged in the `variable_width_histogram`,\nmostly found by @wwang500:\n1. Setting its tuning parameters in an unexpected order could cause the\n   request to fail.\n2. We checked that the maximum number of buckets was both less than\n   50000 and MAX_BUCKETS. This drops the 50000.\n3. Fixes a divide by 0 that can occur of the `shard_size` is 1.\n4. Fixes a divide by 0 that can occur if the `shard_size * 3` overflows\n   a signed int.\n5. Requires `shard_size * 3 / 4` to be at least `buckets`. If it is less\n   than `buckets` we will very consistently return fewer buckets than\n   requested. For the most part we expect folks to leave it at the\n   default. If they change it, we expect it to be much bigger than\n   `buckets`.\n6. Allocate a smaller `mergeMap` in when initially bucketing requests\n   that don't use the entire `shard_size * 3 / 4`. Its just a waste.\n7. Default `shard_size` to `10 * buckets` rather than `100`. It *looks*\n   like that was our intention the whole time. And it feels like it'd\n   keep the algorithm humming along more smoothly.\n8. Default the `initial_buffer` to `min(10 * shard_size, 50000)` like\n   we've documented it rather than `5000`. Like the point above, this\n   feels like the right thing to do to keep the algorithm happy."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ2MTAwNjM2", "url": "https://github.com/elastic/elasticsearch/pull/59341#pullrequestreview-446100636", "createdAt": "2020-07-10T03:33:10Z", "commit": {"oid": "abd15a7c0974395c211b49a3baecd4b6e67f145c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQwMzozMzoxMFrOGvowlA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQwMzozMzoxMFrOGvowlA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjYwNDA1Mg==", "bodyText": "\ud83d\udc4d good idea!", "url": "https://github.com/elastic/elasticsearch/pull/59341#discussion_r452604052", "createdAt": "2020-07-10T03:33:10Z", "author": {"login": "jamesdorfman"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/VariableWidthHistogramAggregator.java", "diffHunk": "@@ -265,7 +264,7 @@ private void bucketBufferedDocs(final DoubleArray buffer, final int bufferSize,\n                 }\n             }\n \n-            mergeBuckets(mergeMap, numBuckets);\n+            mergeBuckets(mergeMap, bucketOrd + 1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "abd15a7c0974395c211b49a3baecd4b6e67f145c"}, "originalPosition": 24}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "66ebadc8fbda911cf228f132a463c6fe03693fbe", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/66ebadc8fbda911cf228f132a463c6fe03693fbe", "committedDate": "2020-07-13T17:42:49Z", "message": "Merge branch 'master' into variable_width_histo_cleanup_request"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUwMTk5NDcy", "url": "https://github.com/elastic/elasticsearch/pull/59341#pullrequestreview-450199472", "createdAt": "2020-07-16T20:40:05Z", "commit": {"oid": "66ebadc8fbda911cf228f132a463c6fe03693fbe"}, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQyMDo0MDowNlrOGy79lg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQyMDo0MTo1NVrOGy8BrA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA2NDQwNg==", "bodyText": "This is pretty late in the process... is there a reason we can't check these when parsing the request?  I don't see anything that needs the shard context but I might be missing something?", "url": "https://github.com/elastic/elasticsearch/pull/59341#discussion_r456064406", "createdAt": "2020-07-16T20:40:06Z", "author": {"login": "polyfractal"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/VariableWidthHistogramAggregationBuilder.java", "diffHunk": "@@ -149,12 +158,32 @@ protected ValuesSourceAggregatorFactory innerBuild(QueryShardContext queryShardC\n                                                        ValuesSourceConfig config,\n                                                        AggregatorFactory parent,\n                                                        AggregatorFactories.Builder subFactoriesBuilder) throws IOException {\n-\n         Settings settings = queryShardContext.getIndexSettings().getNodeSettings();\n         int maxBuckets = MultiBucketConsumerService.MAX_BUCKET_SETTING.get(settings);\n         if (numBuckets > maxBuckets) {\n-            throw new IllegalArgumentException(NUM_BUCKETS_FIELD.getPreferredName()+\n-                \" must be less than \" + maxBuckets);\n+            throw new IllegalArgumentException(NUM_BUCKETS_FIELD.getPreferredName() + \" must be less than \" + maxBuckets);\n+        }\n+        int initialBuffer = getInitialBuffer();\n+        int shardSize = getShardSize();\n+        if (initialBuffer < numBuckets) {\n+            // If numBuckets buckets are being returned, then at least that many must be stored in memory\n+            throw new IllegalArgumentException(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "66ebadc8fbda911cf228f132a463c6fe03693fbe"}, "originalPosition": 115}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA2NTQ1Mg==", "bodyText": "Side note for us to think about in the future: I wonder if we should escalate this sort of check to BucketsAggregator or something?  E.g. it doesn't make sense for any of the aggs to request > max_buckets and we can determine that up front at parsing.  Potentially save some heartache.\nLess of an issue since we've increased the threshold but still.", "url": "https://github.com/elastic/elasticsearch/pull/59341#discussion_r456065452", "createdAt": "2020-07-16T20:41:55Z", "author": {"login": "polyfractal"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/VariableWidthHistogramAggregationBuilder.java", "diffHunk": "@@ -149,12 +158,32 @@ protected ValuesSourceAggregatorFactory innerBuild(QueryShardContext queryShardC\n                                                        ValuesSourceConfig config,\n                                                        AggregatorFactory parent,\n                                                        AggregatorFactories.Builder subFactoriesBuilder) throws IOException {\n-\n         Settings settings = queryShardContext.getIndexSettings().getNodeSettings();\n         int maxBuckets = MultiBucketConsumerService.MAX_BUCKET_SETTING.get(settings);\n         if (numBuckets > maxBuckets) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "66ebadc8fbda911cf228f132a463c6fe03693fbe"}, "originalPosition": 106}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "39c4a380ee6de00bac3a12481d38ced3cad0e712", "author": {"user": {"login": "elasticmachine", "name": "Elastic Machine"}}, "url": "https://github.com/elastic/elasticsearch/commit/39c4a380ee6de00bac3a12481d38ced3cad0e712", "committedDate": "2020-07-17T16:33:02Z", "message": "Merge branch 'master' into variable_width_histo_cleanup_request"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2148, "cost": 1, "resetAt": "2021-10-28T18:54:27Z"}}}