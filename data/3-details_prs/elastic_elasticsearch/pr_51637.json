{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzY4Njc0OTY4", "number": 51637, "title": "Add cache directory low-level instrumentation ", "bodyText": "This pull request is a first step to add instrumentation to the CacheDirectory added in #50693.\nIt adds a new mutable IndexInputStats object that allows to track various information about how CacheBufferedIndexInput interacts with the underlying cache file to satisfy the read operations. It keep tracks of small/large forward/backward seekings as well as the total number of bytes read from the IndexInput and the total number of bytes read/written from/to the CacheFile.\nNote that the stats do not reflect the exact usage that Lucene does of the IndexInputs opened through a CacheDirectory: IndexInputStats is not aware of the read operations that are directly served at a higher level by the internal BufferedIndexInput's buffer. Instead it tracks what really hit the disk which is, I think, what is the most interesting for us at this stage.\nThis pull request does not expose the information through a REST API, this will be done in a follow up pull request once the low level instrumentation is validated.", "createdAt": "2020-01-29T17:19:18Z", "url": "https://github.com/elastic/elasticsearch/pull/51637", "merged": true, "mergeCommit": {"oid": "229b953f1b9b8caa1dfce719760954cdca2c5c26"}, "closed": true, "closedAt": "2020-01-31T16:04:30Z", "author": {"login": "tlrx"}, "timelineItems": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABb_JBBOgH2gAyMzY4Njc0OTY4OjQwOWU3MjBlMTJiOGQyODRkOTU2Nzk1MThkM2I2YzFjNDJkNzM1NWE=", "endCursor": "Y3Vyc29yOnYyOpPPAAABb_v5QTAFqTM1MTUzNDQ2OQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "409e720e12b8d284d95679518d3b6c1c42d7355a", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/409e720e12b8d284d95679518d3b6c1c42d7355a", "committedDate": "2020-01-29T17:02:25Z", "message": "Add IndexInputStats"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5702eb9b1bbc1bd5c1893412d0471fdf5e64d918", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/5702eb9b1bbc1bd5c1893412d0471fdf5e64d918", "committedDate": "2020-01-31T11:17:44Z", "message": "Merge branch 'feature/searchable-snapshots' into add-instrumentation-step-1"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUxNDMwODU4", "url": "https://github.com/elastic/elasticsearch/pull/51637#pullrequestreview-351430858", "createdAt": "2020-01-31T10:58:36Z", "commit": {"oid": "409e720e12b8d284d95679518d3b6c1c42d7355a"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxMDo1ODozNlrOFkH_oA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxMToyNjoyMlrOFkIpmA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzQyNDAzMg==", "bodyText": "I think we should not count cases where delta == 0 as a seek. I remember seeing some no-op calls to seekInternal() when looking at a separate issue, but I think we can make them have no effect on the underlying stream and therefore shouldn't be counted here.", "url": "https://github.com/elastic/elasticsearch/pull/51637#discussion_r373424032", "createdAt": "2020-01-31T10:58:36Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/IndexInputStats.java", "diffHunk": "@@ -0,0 +1,167 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.elasticsearch.xpack.searchablesnapshots.cache.CacheDirectory.CacheBufferedIndexInput;\n+\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.LongAdder;\n+import java.util.function.LongConsumer;\n+\n+/**\n+ * {@link IndexInputStats} records stats for a given {@link CacheBufferedIndexInput}.\n+ */\n+public class IndexInputStats {\n+\n+    static final double SEEKING_THRESHOLD = 0.25d;\n+\n+    private final long fileLength;\n+\n+    private final LongAdder opened = new LongAdder();\n+    private final LongAdder closed = new LongAdder();\n+\n+    private final Counter forwardSmallSeeks = new Counter();\n+    private final Counter backwardSmallSeeks = new Counter();\n+\n+    private final Counter forwardLargeSeeks = new Counter();\n+    private final Counter backwardLargeSeeks = new Counter();\n+\n+    private final Counter contiguousReads = new Counter();\n+    private final Counter nonContiguousReads = new Counter();\n+\n+    private final Counter directBytesRead = new Counter();\n+\n+    private final Counter cachedBytesRead = new Counter();\n+    private final Counter cachedBytesWritten = new Counter();\n+\n+    public IndexInputStats(long fileLength) {\n+        this.fileLength = fileLength;\n+    }\n+\n+    public void incrementOpenCount() {\n+        opened.increment();\n+    }\n+\n+    public void incrementCloseCount() {\n+        closed.increment();\n+    }\n+\n+    public void addCachedBytesRead(int bytesRead) {\n+        cachedBytesRead.add(bytesRead);\n+    }\n+\n+    public void addCachedBytesWritten(int bytesWritten) {\n+        cachedBytesWritten.add(bytesWritten);\n+    }\n+\n+    public void addDirectBytesRead(int bytesRead) {\n+        directBytesRead.add(bytesRead);\n+    }\n+\n+    public void incrementBytesRead(long previousPosition, long currentPosition, int bytesRead) {\n+        LongConsumer incBytesRead = (previousPosition == currentPosition) ? contiguousReads::add : nonContiguousReads::add;\n+        incBytesRead.accept(bytesRead);\n+    }\n+\n+    public void incrementSeeks(long currentPosition, long newPosition) {\n+        final long delta = newPosition - currentPosition;\n+        final double threshold = fileLength * SEEKING_THRESHOLD;\n+        LongConsumer incSeekCount;\n+        if (delta >= 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "409e720e12b8d284d95679518d3b6c1c42d7355a"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzQyNTI1MA==", "bodyText": "Should we track the number of times we open an input on the inner Directory (both here and in readDirectly)? This results in a readBlob call which may be charged on a per-request basis.", "url": "https://github.com/elastic/elasticsearch/pull/51637#discussion_r373425250", "createdAt": "2020-01-31T11:01:34Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheDirectory.java", "diffHunk": "@@ -188,33 +206,40 @@ protected void readInternal(final byte[] buffer, final int offset, final int len\n                     if (e instanceof AlreadyClosedException || (e.getCause() != null && e.getCause() instanceof AlreadyClosedException)) {\n                         try {\n                             // cache file was evicted during the range fetching, read bytes directly from source\n-                            bytesRead += readDirectly(pos, pos + len, buffer, off);\n+                            bytesRead = readDirectly(pos, pos + len, buffer, off);\n+                            stats.addDirectBytesRead(bytesRead);\n                             continue;\n                         } catch (Exception inner) {\n                             e.addSuppressed(inner);\n                         }\n                     }\n                     throw new IOException(\"Fail to read data from cache\", e);\n \n+                } finally {\n+                    totalBytesRead += bytesRead;\n                 }\n             }\n-            assert bytesRead == length : \"partial read operation, read [\" + bytesRead + \"] bytes of [\" + length + \"]\";\n+            assert totalBytesRead == length : \"partial read operation, read [\" + totalBytesRead + \"] bytes of [\" + length + \"]\";\n+            stats.incrementBytesRead(lastReadPosition, position, totalBytesRead);\n+            lastReadPosition = position + totalBytesRead;\n         }\n \n         int readCacheFile(FileChannel fc, long end, long position, byte[] buffer, int offset, long length) throws IOException {\n             assert assertFileChannelOpen(fc);\n-            return Channels.readFromFileChannel(fc, position, buffer, offset, Math.toIntExact(Math.min(length, end - position)));\n+            int bytesRead = Channels.readFromFileChannel(fc, position, buffer, offset, Math.toIntExact(Math.min(length, end - position)));\n+            stats.addCachedBytesRead(bytesRead);\n+            return bytesRead;\n         }\n \n         @SuppressForbidden(reason = \"Use positional writes on purpose\")\n         void writeCacheFile(FileChannel fc, long start, long end) throws IOException {\n             assert assertFileChannelOpen(fc);\n             final byte[] copyBuffer = new byte[Math.toIntExact(Math.min(COPY_BUFFER_SIZE, end - start))];\n+            int bytesCopied = 0;\n             try (IndexInput input = in.openInput(cacheFileReference.getFileName(), ioContext)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "409e720e12b8d284d95679518d3b6c1c42d7355a"}, "originalPosition": 155}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzQyODQyMg==", "bodyText": "Also, I think we should add a TRACE-level log recording the specific range we're copying here (and in readDirectly) because I think there will be situations where we need to see the details of the access pattern.", "url": "https://github.com/elastic/elasticsearch/pull/51637#discussion_r373428422", "createdAt": "2020-01-31T11:09:18Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheDirectory.java", "diffHunk": "@@ -188,33 +206,40 @@ protected void readInternal(final byte[] buffer, final int offset, final int len\n                     if (e instanceof AlreadyClosedException || (e.getCause() != null && e.getCause() instanceof AlreadyClosedException)) {\n                         try {\n                             // cache file was evicted during the range fetching, read bytes directly from source\n-                            bytesRead += readDirectly(pos, pos + len, buffer, off);\n+                            bytesRead = readDirectly(pos, pos + len, buffer, off);\n+                            stats.addDirectBytesRead(bytesRead);\n                             continue;\n                         } catch (Exception inner) {\n                             e.addSuppressed(inner);\n                         }\n                     }\n                     throw new IOException(\"Fail to read data from cache\", e);\n \n+                } finally {\n+                    totalBytesRead += bytesRead;\n                 }\n             }\n-            assert bytesRead == length : \"partial read operation, read [\" + bytesRead + \"] bytes of [\" + length + \"]\";\n+            assert totalBytesRead == length : \"partial read operation, read [\" + totalBytesRead + \"] bytes of [\" + length + \"]\";\n+            stats.incrementBytesRead(lastReadPosition, position, totalBytesRead);\n+            lastReadPosition = position + totalBytesRead;\n         }\n \n         int readCacheFile(FileChannel fc, long end, long position, byte[] buffer, int offset, long length) throws IOException {\n             assert assertFileChannelOpen(fc);\n-            return Channels.readFromFileChannel(fc, position, buffer, offset, Math.toIntExact(Math.min(length, end - position)));\n+            int bytesRead = Channels.readFromFileChannel(fc, position, buffer, offset, Math.toIntExact(Math.min(length, end - position)));\n+            stats.addCachedBytesRead(bytesRead);\n+            return bytesRead;\n         }\n \n         @SuppressForbidden(reason = \"Use positional writes on purpose\")\n         void writeCacheFile(FileChannel fc, long start, long end) throws IOException {\n             assert assertFileChannelOpen(fc);\n             final byte[] copyBuffer = new byte[Math.toIntExact(Math.min(COPY_BUFFER_SIZE, end - start))];\n+            int bytesCopied = 0;\n             try (IndexInput input = in.openInput(cacheFileReference.getFileName(), ioContext)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "409e720e12b8d284d95679518d3b6c1c42d7355a"}, "originalPosition": 155}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzQzNDc3Ng==", "bodyText": "One of the big open questions about seeking is whether we are seeking forwards in small enough steps that we should have just requested a larger contiguous chunk of the blob up-front. Today the implementation is based on fixed-size chunks, so I think it makes sense to set a fixed-size threshold for what we consider to be a \"large\" seek. I would guess something like 8MB or 16MB would be a good start.\nWe could of course move to a chunk size that is proportional to the size of the underlying file, and in that case it makes more sense for the threshold to be proportional to the file size too.", "url": "https://github.com/elastic/elasticsearch/pull/51637#discussion_r373434776", "createdAt": "2020-01-31T11:26:22Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/IndexInputStats.java", "diffHunk": "@@ -0,0 +1,167 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.elasticsearch.xpack.searchablesnapshots.cache.CacheDirectory.CacheBufferedIndexInput;\n+\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.LongAdder;\n+import java.util.function.LongConsumer;\n+\n+/**\n+ * {@link IndexInputStats} records stats for a given {@link CacheBufferedIndexInput}.\n+ */\n+public class IndexInputStats {\n+\n+    static final double SEEKING_THRESHOLD = 0.25d;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "409e720e12b8d284d95679518d3b6c1c42d7355a"}, "originalPosition": 19}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cba3fc3af84d0f58f9bd69473d70ada39b8fd8ba", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/cba3fc3af84d0f58f9bd69473d70ada39b8fd8ba", "committedDate": "2020-01-31T11:35:15Z", "message": "Fix compilation issues after merging master"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3f8b3a4f7ccfb51c8cea8771d0e3baefd0d2111f", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/3f8b3a4f7ccfb51c8cea8771d0e3baefd0d2111f", "committedDate": "2020-01-31T11:51:46Z", "message": "Ignore seek to same position"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "208b14f3161859c6f6d31da70e9499165c12f534", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/208b14f3161859c6f6d31da70e9499165c12f534", "committedDate": "2020-01-31T12:42:38Z", "message": "add innner opening counter + log traces"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "51c4f43a5753d38d81beabb83024c9268b99913d", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/51c4f43a5753d38d81beabb83024c9268b99913d", "committedDate": "2020-01-31T13:17:18Z", "message": "Seek threshold in MB"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUxNTAzMTgy", "url": "https://github.com/elastic/elasticsearch/pull/51637#pullrequestreview-351503182", "createdAt": "2020-01-31T13:28:23Z", "commit": {"oid": "51c4f43a5753d38d81beabb83024c9268b99913d"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxMzoyODoyM1rOFkLTUw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxMzozMDoxNVrOFkLWUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzQ3ODIyNw==", "bodyText": "Ah, I think this will not include the shard ID so we might struggle to interpret the logs when there are lots of shards to search. I think logging cacheFileReference itself gives us everything we need.", "url": "https://github.com/elastic/elasticsearch/pull/51637#discussion_r373478227", "createdAt": "2020-01-31T13:28:23Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheDirectory.java", "diffHunk": "@@ -202,33 +224,43 @@ protected void readInternal(final byte[] buffer, final int offset, final int len\n                     if (e instanceof AlreadyClosedException || (e.getCause() != null && e.getCause() instanceof AlreadyClosedException)) {\n                         try {\n                             // cache file was evicted during the range fetching, read bytes directly from source\n-                            bytesRead += readDirectly(pos, pos + len, buffer, off);\n+                            bytesRead = readDirectly(pos, pos + len, buffer, off);\n                             continue;\n                         } catch (Exception inner) {\n                             e.addSuppressed(inner);\n                         }\n                     }\n                     throw new IOException(\"Fail to read data from cache\", e);\n \n+                } finally {\n+                    totalBytesRead += bytesRead;\n                 }\n             }\n-            assert bytesRead == length : \"partial read operation, read [\" + bytesRead + \"] bytes of [\" + length + \"]\";\n+            assert totalBytesRead == length : \"partial read operation, read [\" + totalBytesRead + \"] bytes of [\" + length + \"]\";\n+            stats.incrementBytesRead(lastReadPosition, position, totalBytesRead);\n+            lastReadPosition = position + totalBytesRead;\n         }\n \n         int readCacheFile(FileChannel fc, long end, long position, byte[] buffer, int offset, long length) throws IOException {\n             assert assertFileChannelOpen(fc);\n-            return Channels.readFromFileChannel(fc, position, buffer, offset, Math.toIntExact(Math.min(length, end - position)));\n+            int bytesRead = Channels.readFromFileChannel(fc, position, buffer, offset, Math.toIntExact(Math.min(length, end - position)));\n+            stats.addCachedBytesRead(bytesRead);\n+            return bytesRead;\n         }\n \n         @SuppressForbidden(reason = \"Use positional writes on purpose\")\n         void writeCacheFile(FileChannel fc, long start, long end) throws IOException {\n             assert assertFileChannelOpen(fc);\n+            final String fileName = cacheFileReference.getFileName();\n             final byte[] copyBuffer = new byte[Math.toIntExact(Math.min(COPY_BUFFER_SIZE, end - start))];\n-            try (IndexInput input = in.openInput(cacheFileReference.getFileName(), ioContext)) {\n+            logger.trace(() -> new ParameterizedMessage(\"writing range [{}-{}] of file [{}] to cache file\", start, end, fileName));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "51c4f43a5753d38d81beabb83024c9268b99913d"}, "originalPosition": 173}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzQ3ODk5Mg==", "bodyText": "Oh yes you're absolutely right. To be addressed elsewhere.", "url": "https://github.com/elastic/elasticsearch/pull/51637#discussion_r373478992", "createdAt": "2020-01-31T13:30:15Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheDirectory.java", "diffHunk": "@@ -188,33 +206,40 @@ protected void readInternal(final byte[] buffer, final int offset, final int len\n                     if (e instanceof AlreadyClosedException || (e.getCause() != null && e.getCause() instanceof AlreadyClosedException)) {\n                         try {\n                             // cache file was evicted during the range fetching, read bytes directly from source\n-                            bytesRead += readDirectly(pos, pos + len, buffer, off);\n+                            bytesRead = readDirectly(pos, pos + len, buffer, off);\n+                            stats.addDirectBytesRead(bytesRead);\n                             continue;\n                         } catch (Exception inner) {\n                             e.addSuppressed(inner);\n                         }\n                     }\n                     throw new IOException(\"Fail to read data from cache\", e);\n \n+                } finally {\n+                    totalBytesRead += bytesRead;\n                 }\n             }\n-            assert bytesRead == length : \"partial read operation, read [\" + bytesRead + \"] bytes of [\" + length + \"]\";\n+            assert totalBytesRead == length : \"partial read operation, read [\" + totalBytesRead + \"] bytes of [\" + length + \"]\";\n+            stats.incrementBytesRead(lastReadPosition, position, totalBytesRead);\n+            lastReadPosition = position + totalBytesRead;\n         }\n \n         int readCacheFile(FileChannel fc, long end, long position, byte[] buffer, int offset, long length) throws IOException {\n             assert assertFileChannelOpen(fc);\n-            return Channels.readFromFileChannel(fc, position, buffer, offset, Math.toIntExact(Math.min(length, end - position)));\n+            int bytesRead = Channels.readFromFileChannel(fc, position, buffer, offset, Math.toIntExact(Math.min(length, end - position)));\n+            stats.addCachedBytesRead(bytesRead);\n+            return bytesRead;\n         }\n \n         @SuppressForbidden(reason = \"Use positional writes on purpose\")\n         void writeCacheFile(FileChannel fc, long start, long end) throws IOException {\n             assert assertFileChannelOpen(fc);\n             final byte[] copyBuffer = new byte[Math.toIntExact(Math.min(COPY_BUFFER_SIZE, end - start))];\n+            int bytesCopied = 0;\n             try (IndexInput input = in.openInput(cacheFileReference.getFileName(), ioContext)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzQyNTI1MA=="}, "originalCommit": {"oid": "409e720e12b8d284d95679518d3b6c1c42d7355a"}, "originalPosition": 155}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUxNTA1MzMy", "url": "https://github.com/elastic/elasticsearch/pull/51637#pullrequestreview-351505332", "createdAt": "2020-01-31T13:32:23Z", "commit": {"oid": "51c4f43a5753d38d81beabb83024c9268b99913d"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9de84533884a13e397406ed16aa5638cccb8ea9a", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/9de84533884a13e397406ed16aa5638cccb8ea9a", "committedDate": "2020-01-31T13:53:37Z", "message": "adapt logging"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUxNTIyMTE5", "url": "https://github.com/elastic/elasticsearch/pull/51637#pullrequestreview-351522119", "createdAt": "2020-01-31T14:00:55Z", "commit": {"oid": "9de84533884a13e397406ed16aa5638cccb8ea9a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxNDowMDo1NVrOFkMLWg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxNDowMDo1NVrOFkMLWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzQ5MjU3MA==", "bodyText": "Sorry, here too :)", "url": "https://github.com/elastic/elasticsearch/pull/51637#discussion_r373492570", "createdAt": "2020-01-31T14:00:55Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheDirectory.java", "diffHunk": "@@ -279,10 +312,13 @@ public String toString() {\n         }\n \n         private int readDirectly(long start, long end, byte[] buffer, int offset) throws IOException {\n+            final String fileName = cacheFileReference.getFileName();\n             final byte[] copyBuffer = new byte[Math.toIntExact(Math.min(COPY_BUFFER_SIZE, end - start))];\n+            logger.trace(() -> new ParameterizedMessage(\"direct reading of range [{}-{}] from file [{}]\", start, end, fileName));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9de84533884a13e397406ed16aa5638cccb8ea9a"}, "originalPosition": 214}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "64fe3eb482a6f89ed13c9eaf06886c0682a33ea2", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/64fe3eb482a6f89ed13c9eaf06886c0682a33ea2", "committedDate": "2020-01-31T14:13:06Z", "message": "adapt logging (again)"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUxNTM0NDY5", "url": "https://github.com/elastic/elasticsearch/pull/51637#pullrequestreview-351534469", "createdAt": "2020-01-31T14:20:14Z", "commit": {"oid": "64fe3eb482a6f89ed13c9eaf06886c0682a33ea2"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3058, "cost": 1, "resetAt": "2021-10-28T18:54:27Z"}}}