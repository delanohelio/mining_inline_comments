{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDA5MzgxMDgx", "number": 55793, "reviewThreads": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QwOTo1NTowNVrOD2l-5g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNFQxMzozOTowOFrOD4_hQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4NTcyMDA2OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QwOTo1NTowNVrOGMawBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxMDoxMjoxM1rOGMbcmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTY3NDM3NQ==", "bodyText": "Why wrap this in a SizeBlockingQueue?", "url": "https://github.com/elastic/elasticsearch/pull/55793#discussion_r415674375", "createdAt": "2020-04-27T09:55:05Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -361,68 +367,95 @@ private void prewarmCache() {\n             final Executor executor = threadPool.executor(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME);\n             logger.debug(\"{} warming shard cache for [{}] files\", shardId, cacheFiles.size());\n \n+            final BlockingQueue<CheckedRunnable<Exception>> queue = new SizeBlockingQueue<>(new LinkedBlockingQueue<>(), Integer.MAX_VALUE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9743806ad5a396d88079be890ef43b84b2815ca0"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTY4NTc4NQ==", "bodyText": "Mostly to prevent the queue to have more than Integer.MAX_VALUE elements, which could maybe happen for a large number of files with very low chunk size?", "url": "https://github.com/elastic/elasticsearch/pull/55793#discussion_r415685785", "createdAt": "2020-04-27T10:12:13Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -361,68 +367,95 @@ private void prewarmCache() {\n             final Executor executor = threadPool.executor(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME);\n             logger.debug(\"{} warming shard cache for [{}] files\", shardId, cacheFiles.size());\n \n+            final BlockingQueue<CheckedRunnable<Exception>> queue = new SizeBlockingQueue<>(new LinkedBlockingQueue<>(), Integer.MAX_VALUE);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTY3NDM3NQ=="}, "originalCommit": {"oid": "9743806ad5a396d88079be890ef43b84b2815ca0"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4NTczMDE5OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QwOTo1NzoxOFrOGMa18A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQxMToxMDoxOVrOGOkmmw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTY3NTg4OA==", "bodyText": "should this not always be true? Why should this queue ever overflow?", "url": "https://github.com/elastic/elasticsearch/pull/55793#discussion_r415675888", "createdAt": "2020-04-27T09:57:18Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -361,68 +367,95 @@ private void prewarmCache() {\n             final Executor executor = threadPool.executor(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME);\n             logger.debug(\"{} warming shard cache for [{}] files\", shardId, cacheFiles.size());\n \n+            final BlockingQueue<CheckedRunnable<Exception>> queue = new SizeBlockingQueue<>(new LinkedBlockingQueue<>(), Integer.MAX_VALUE);\n             for (BlobStoreIndexShardSnapshot.FileInfo cacheFile : cacheFiles) {\n                 final String fileName = cacheFile.physicalName();\n                 try {\n                     final IndexInput input = openInput(fileName, CachedBlobContainerIndexInput.CACHE_WARMING_CONTEXT);\n                     assert input instanceof CachedBlobContainerIndexInput : \"expected cached index input but got \" + input.getClass();\n \n                     final long numberOfParts = cacheFile.numberOfParts();\n-                    final CountDown countDown = new CountDown(Math.toIntExact(numberOfParts));\n-                    for (long p = 0; p < numberOfParts; p++) {\n-                        final int part = Math.toIntExact(p);\n-                        // TODO use multiple workers to warm each part instead of filling the thread pool\n-                        executor.execute(new AbstractRunnable() {\n-                            @Override\n-                            protected void doRun() throws Exception {\n-                                ensureOpen();\n-\n-                                logger.trace(\"warming cache for [{}] part [{}/{}]\", fileName, part, numberOfParts);\n-                                final long startTimeInNanos = statsCurrentTimeNanosSupplier.getAsLong();\n-\n-                                final CachedBlobContainerIndexInput cachedIndexInput = (CachedBlobContainerIndexInput) input.clone();\n-                                final int bytesRead = cachedIndexInput.prefetchPart(part); // TODO does not include any rate limitation\n-                                assert bytesRead == cacheFile.partBytes(part);\n-\n-                                logger.trace(\n-                                    () -> new ParameterizedMessage(\n-                                        \"part [{}/{}] of [{}] warmed in [{}] ms\",\n-                                        part,\n-                                        numberOfParts,\n-                                        fileName,\n-                                        TimeValue.timeValueNanos(statsCurrentTimeNanosSupplier.getAsLong() - startTimeInNanos).millis()\n-                                    )\n-                                );\n-                            }\n-\n-                            @Override\n-                            public void onFailure(Exception e) {\n-                                logger.trace(\n-                                    () -> new ParameterizedMessage(\n-                                        \"failed to warm cache for [{}] part [{}/{}]\",\n-                                        fileName,\n-                                        part,\n-                                        numberOfParts\n-                                    ),\n-                                    e\n-                                );\n-                            }\n-\n-                            @Override\n-                            public void onAfter() {\n-                                if (countDown.countDown()) {\n-                                    IOUtils.closeWhileHandlingException(input);\n+                    if (queue.remainingCapacity() >= numberOfParts) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9743806ad5a396d88079be890ef43b84b2815ca0"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTgyNjc0NA==", "bodyText": "Do we just silently ignore if we have more than Integer.MAX_VALUE items? Should we log a big fat warning at that point instead?", "url": "https://github.com/elastic/elasticsearch/pull/55793#discussion_r415826744", "createdAt": "2020-04-27T13:46:59Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -361,68 +367,95 @@ private void prewarmCache() {\n             final Executor executor = threadPool.executor(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME);\n             logger.debug(\"{} warming shard cache for [{}] files\", shardId, cacheFiles.size());\n \n+            final BlockingQueue<CheckedRunnable<Exception>> queue = new SizeBlockingQueue<>(new LinkedBlockingQueue<>(), Integer.MAX_VALUE);\n             for (BlobStoreIndexShardSnapshot.FileInfo cacheFile : cacheFiles) {\n                 final String fileName = cacheFile.physicalName();\n                 try {\n                     final IndexInput input = openInput(fileName, CachedBlobContainerIndexInput.CACHE_WARMING_CONTEXT);\n                     assert input instanceof CachedBlobContainerIndexInput : \"expected cached index input but got \" + input.getClass();\n \n                     final long numberOfParts = cacheFile.numberOfParts();\n-                    final CountDown countDown = new CountDown(Math.toIntExact(numberOfParts));\n-                    for (long p = 0; p < numberOfParts; p++) {\n-                        final int part = Math.toIntExact(p);\n-                        // TODO use multiple workers to warm each part instead of filling the thread pool\n-                        executor.execute(new AbstractRunnable() {\n-                            @Override\n-                            protected void doRun() throws Exception {\n-                                ensureOpen();\n-\n-                                logger.trace(\"warming cache for [{}] part [{}/{}]\", fileName, part, numberOfParts);\n-                                final long startTimeInNanos = statsCurrentTimeNanosSupplier.getAsLong();\n-\n-                                final CachedBlobContainerIndexInput cachedIndexInput = (CachedBlobContainerIndexInput) input.clone();\n-                                final int bytesRead = cachedIndexInput.prefetchPart(part); // TODO does not include any rate limitation\n-                                assert bytesRead == cacheFile.partBytes(part);\n-\n-                                logger.trace(\n-                                    () -> new ParameterizedMessage(\n-                                        \"part [{}/{}] of [{}] warmed in [{}] ms\",\n-                                        part,\n-                                        numberOfParts,\n-                                        fileName,\n-                                        TimeValue.timeValueNanos(statsCurrentTimeNanosSupplier.getAsLong() - startTimeInNanos).millis()\n-                                    )\n-                                );\n-                            }\n-\n-                            @Override\n-                            public void onFailure(Exception e) {\n-                                logger.trace(\n-                                    () -> new ParameterizedMessage(\n-                                        \"failed to warm cache for [{}] part [{}/{}]\",\n-                                        fileName,\n-                                        part,\n-                                        numberOfParts\n-                                    ),\n-                                    e\n-                                );\n-                            }\n-\n-                            @Override\n-                            public void onAfter() {\n-                                if (countDown.countDown()) {\n-                                    IOUtils.closeWhileHandlingException(input);\n+                    if (queue.remainingCapacity() >= numberOfParts) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTY3NTg4OA=="}, "originalCommit": {"oid": "9743806ad5a396d88079be890ef43b84b2815ca0"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzkzMjk1NQ==", "bodyText": "++ the fact that org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardSnapshot.FileInfo#numberOfParts is a long is weird to begin with here. But practically speaking there's no way we could ever exceed the queue size here (before we get here, writing/reading the necessary metadata would have OOMed already). I'd just remove this check and assume that if the queue ever throws IllegalStateException there's some other bug at play.", "url": "https://github.com/elastic/elasticsearch/pull/55793#discussion_r417932955", "createdAt": "2020-04-30T11:10:19Z", "author": {"login": "original-brownbear"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -361,68 +367,95 @@ private void prewarmCache() {\n             final Executor executor = threadPool.executor(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME);\n             logger.debug(\"{} warming shard cache for [{}] files\", shardId, cacheFiles.size());\n \n+            final BlockingQueue<CheckedRunnable<Exception>> queue = new SizeBlockingQueue<>(new LinkedBlockingQueue<>(), Integer.MAX_VALUE);\n             for (BlobStoreIndexShardSnapshot.FileInfo cacheFile : cacheFiles) {\n                 final String fileName = cacheFile.physicalName();\n                 try {\n                     final IndexInput input = openInput(fileName, CachedBlobContainerIndexInput.CACHE_WARMING_CONTEXT);\n                     assert input instanceof CachedBlobContainerIndexInput : \"expected cached index input but got \" + input.getClass();\n \n                     final long numberOfParts = cacheFile.numberOfParts();\n-                    final CountDown countDown = new CountDown(Math.toIntExact(numberOfParts));\n-                    for (long p = 0; p < numberOfParts; p++) {\n-                        final int part = Math.toIntExact(p);\n-                        // TODO use multiple workers to warm each part instead of filling the thread pool\n-                        executor.execute(new AbstractRunnable() {\n-                            @Override\n-                            protected void doRun() throws Exception {\n-                                ensureOpen();\n-\n-                                logger.trace(\"warming cache for [{}] part [{}/{}]\", fileName, part, numberOfParts);\n-                                final long startTimeInNanos = statsCurrentTimeNanosSupplier.getAsLong();\n-\n-                                final CachedBlobContainerIndexInput cachedIndexInput = (CachedBlobContainerIndexInput) input.clone();\n-                                final int bytesRead = cachedIndexInput.prefetchPart(part); // TODO does not include any rate limitation\n-                                assert bytesRead == cacheFile.partBytes(part);\n-\n-                                logger.trace(\n-                                    () -> new ParameterizedMessage(\n-                                        \"part [{}/{}] of [{}] warmed in [{}] ms\",\n-                                        part,\n-                                        numberOfParts,\n-                                        fileName,\n-                                        TimeValue.timeValueNanos(statsCurrentTimeNanosSupplier.getAsLong() - startTimeInNanos).millis()\n-                                    )\n-                                );\n-                            }\n-\n-                            @Override\n-                            public void onFailure(Exception e) {\n-                                logger.trace(\n-                                    () -> new ParameterizedMessage(\n-                                        \"failed to warm cache for [{}] part [{}/{}]\",\n-                                        fileName,\n-                                        part,\n-                                        numberOfParts\n-                                    ),\n-                                    e\n-                                );\n-                            }\n-\n-                            @Override\n-                            public void onAfter() {\n-                                if (countDown.countDown()) {\n-                                    IOUtils.closeWhileHandlingException(input);\n+                    if (queue.remainingCapacity() >= numberOfParts) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTY3NTg4OA=="}, "originalCommit": {"oid": "9743806ad5a396d88079be890ef43b84b2815ca0"}, "originalPosition": 91}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4NTc0MjY0OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxMDowMDowMFrOGMa9Nw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxMDoxMzoyNFrOGMbfhw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTY3Nzc1MQ==", "bodyText": "do we need to drain the queue and close input here?", "url": "https://github.com/elastic/elasticsearch/pull/55793#discussion_r415677751", "createdAt": "2020-04-27T10:00:00Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -361,68 +367,95 @@ private void prewarmCache() {\n             final Executor executor = threadPool.executor(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME);\n             logger.debug(\"{} warming shard cache for [{}] files\", shardId, cacheFiles.size());\n \n+            final BlockingQueue<CheckedRunnable<Exception>> queue = new SizeBlockingQueue<>(new LinkedBlockingQueue<>(), Integer.MAX_VALUE);\n             for (BlobStoreIndexShardSnapshot.FileInfo cacheFile : cacheFiles) {\n                 final String fileName = cacheFile.physicalName();\n                 try {\n                     final IndexInput input = openInput(fileName, CachedBlobContainerIndexInput.CACHE_WARMING_CONTEXT);\n                     assert input instanceof CachedBlobContainerIndexInput : \"expected cached index input but got \" + input.getClass();\n \n                     final long numberOfParts = cacheFile.numberOfParts();\n-                    final CountDown countDown = new CountDown(Math.toIntExact(numberOfParts));\n-                    for (long p = 0; p < numberOfParts; p++) {\n-                        final int part = Math.toIntExact(p);\n-                        // TODO use multiple workers to warm each part instead of filling the thread pool\n-                        executor.execute(new AbstractRunnable() {\n-                            @Override\n-                            protected void doRun() throws Exception {\n-                                ensureOpen();\n-\n-                                logger.trace(\"warming cache for [{}] part [{}/{}]\", fileName, part, numberOfParts);\n-                                final long startTimeInNanos = statsCurrentTimeNanosSupplier.getAsLong();\n-\n-                                final CachedBlobContainerIndexInput cachedIndexInput = (CachedBlobContainerIndexInput) input.clone();\n-                                final int bytesRead = cachedIndexInput.prefetchPart(part); // TODO does not include any rate limitation\n-                                assert bytesRead == cacheFile.partBytes(part);\n-\n-                                logger.trace(\n-                                    () -> new ParameterizedMessage(\n-                                        \"part [{}/{}] of [{}] warmed in [{}] ms\",\n-                                        part,\n-                                        numberOfParts,\n-                                        fileName,\n-                                        TimeValue.timeValueNanos(statsCurrentTimeNanosSupplier.getAsLong() - startTimeInNanos).millis()\n-                                    )\n-                                );\n-                            }\n-\n-                            @Override\n-                            public void onFailure(Exception e) {\n-                                logger.trace(\n-                                    () -> new ParameterizedMessage(\n-                                        \"failed to warm cache for [{}] part [{}/{}]\",\n-                                        fileName,\n-                                        part,\n-                                        numberOfParts\n-                                    ),\n-                                    e\n-                                );\n-                            }\n-\n-                            @Override\n-                            public void onAfter() {\n-                                if (countDown.countDown()) {\n-                                    IOUtils.closeWhileHandlingException(input);\n+                    if (queue.remainingCapacity() >= numberOfParts) {\n+                        final CountDown countDown = new CountDown(Math.toIntExact(numberOfParts));\n+                        for (long p = 0; p < numberOfParts; p++) {\n+                            final int part = Math.toIntExact(p);\n+                            queue.add(() -> {\n+                                try {\n+                                    ensureOpen();\n+\n+                                    logger.trace(\"warming cache for [{}] part [{}/{}]\", fileName, part, numberOfParts);\n+                                    final long startTimeInNanos = statsCurrentTimeNanosSupplier.getAsLong();\n+\n+                                    final CachedBlobContainerIndexInput cachedIndexInput = (CachedBlobContainerIndexInput) input.clone();\n+                                    final int bytesRead = cachedIndexInput.prefetchPart(part); // TODO does not include any rate limitation\n+                                    assert bytesRead == cacheFile.partBytes(part);\n+\n+                                    logger.trace(\n+                                        () -> new ParameterizedMessage(\n+                                            \"part [{}/{}] of [{}] warmed in [{}] ms\",\n+                                            part,\n+                                            numberOfParts,\n+                                            fileName,\n+                                            TimeValue.timeValueNanos(statsCurrentTimeNanosSupplier.getAsLong() - startTimeInNanos).millis()\n+                                        )\n+                                    );\n+                                } catch (Exception e) {\n+                                    logger.trace(\n+                                        () -> new ParameterizedMessage(\n+                                            \"failed to warm cache for [{}] part [{}/{}]\",\n+                                            fileName,\n+                                            part,\n+                                            numberOfParts\n+                                        ),\n+                                        e\n+                                    );\n+                                    if (e instanceof AlreadyClosedException\n+                                        || (e.getCause() != null && e.getCause() instanceof AlreadyClosedException)) {\n+                                        return; // don't rethrow in case of cache eviction or directory closing\n+                                    }\n+                                    throw e;\n+                                } finally {\n+                                    if (countDown.countDown()) {\n+                                        IOUtils.closeWhileHandlingException(input);\n+                                    }\n                                 }\n-                            }\n-                        });\n+                            });\n+                        }\n                     }\n                 } catch (IOException e) {\n-                    logger.trace(() -> new ParameterizedMessage(\"failed to warm cache for [{}]\", fileName), e);\n+                    logger.trace(() -> new ParameterizedMessage(\"unable to schedule cache prewarming for [{}]\", fileName), e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9743806ad5a396d88079be890ef43b84b2815ca0"}, "originalPosition": 142}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTY4NjUzNQ==", "bodyText": "Right - I checked again and the only situation I see is the directory being close so it makes sense to drain the queue. I'll address that.", "url": "https://github.com/elastic/elasticsearch/pull/55793#discussion_r415686535", "createdAt": "2020-04-27T10:13:24Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -361,68 +367,95 @@ private void prewarmCache() {\n             final Executor executor = threadPool.executor(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME);\n             logger.debug(\"{} warming shard cache for [{}] files\", shardId, cacheFiles.size());\n \n+            final BlockingQueue<CheckedRunnable<Exception>> queue = new SizeBlockingQueue<>(new LinkedBlockingQueue<>(), Integer.MAX_VALUE);\n             for (BlobStoreIndexShardSnapshot.FileInfo cacheFile : cacheFiles) {\n                 final String fileName = cacheFile.physicalName();\n                 try {\n                     final IndexInput input = openInput(fileName, CachedBlobContainerIndexInput.CACHE_WARMING_CONTEXT);\n                     assert input instanceof CachedBlobContainerIndexInput : \"expected cached index input but got \" + input.getClass();\n \n                     final long numberOfParts = cacheFile.numberOfParts();\n-                    final CountDown countDown = new CountDown(Math.toIntExact(numberOfParts));\n-                    for (long p = 0; p < numberOfParts; p++) {\n-                        final int part = Math.toIntExact(p);\n-                        // TODO use multiple workers to warm each part instead of filling the thread pool\n-                        executor.execute(new AbstractRunnable() {\n-                            @Override\n-                            protected void doRun() throws Exception {\n-                                ensureOpen();\n-\n-                                logger.trace(\"warming cache for [{}] part [{}/{}]\", fileName, part, numberOfParts);\n-                                final long startTimeInNanos = statsCurrentTimeNanosSupplier.getAsLong();\n-\n-                                final CachedBlobContainerIndexInput cachedIndexInput = (CachedBlobContainerIndexInput) input.clone();\n-                                final int bytesRead = cachedIndexInput.prefetchPart(part); // TODO does not include any rate limitation\n-                                assert bytesRead == cacheFile.partBytes(part);\n-\n-                                logger.trace(\n-                                    () -> new ParameterizedMessage(\n-                                        \"part [{}/{}] of [{}] warmed in [{}] ms\",\n-                                        part,\n-                                        numberOfParts,\n-                                        fileName,\n-                                        TimeValue.timeValueNanos(statsCurrentTimeNanosSupplier.getAsLong() - startTimeInNanos).millis()\n-                                    )\n-                                );\n-                            }\n-\n-                            @Override\n-                            public void onFailure(Exception e) {\n-                                logger.trace(\n-                                    () -> new ParameterizedMessage(\n-                                        \"failed to warm cache for [{}] part [{}/{}]\",\n-                                        fileName,\n-                                        part,\n-                                        numberOfParts\n-                                    ),\n-                                    e\n-                                );\n-                            }\n-\n-                            @Override\n-                            public void onAfter() {\n-                                if (countDown.countDown()) {\n-                                    IOUtils.closeWhileHandlingException(input);\n+                    if (queue.remainingCapacity() >= numberOfParts) {\n+                        final CountDown countDown = new CountDown(Math.toIntExact(numberOfParts));\n+                        for (long p = 0; p < numberOfParts; p++) {\n+                            final int part = Math.toIntExact(p);\n+                            queue.add(() -> {\n+                                try {\n+                                    ensureOpen();\n+\n+                                    logger.trace(\"warming cache for [{}] part [{}/{}]\", fileName, part, numberOfParts);\n+                                    final long startTimeInNanos = statsCurrentTimeNanosSupplier.getAsLong();\n+\n+                                    final CachedBlobContainerIndexInput cachedIndexInput = (CachedBlobContainerIndexInput) input.clone();\n+                                    final int bytesRead = cachedIndexInput.prefetchPart(part); // TODO does not include any rate limitation\n+                                    assert bytesRead == cacheFile.partBytes(part);\n+\n+                                    logger.trace(\n+                                        () -> new ParameterizedMessage(\n+                                            \"part [{}/{}] of [{}] warmed in [{}] ms\",\n+                                            part,\n+                                            numberOfParts,\n+                                            fileName,\n+                                            TimeValue.timeValueNanos(statsCurrentTimeNanosSupplier.getAsLong() - startTimeInNanos).millis()\n+                                        )\n+                                    );\n+                                } catch (Exception e) {\n+                                    logger.trace(\n+                                        () -> new ParameterizedMessage(\n+                                            \"failed to warm cache for [{}] part [{}/{}]\",\n+                                            fileName,\n+                                            part,\n+                                            numberOfParts\n+                                        ),\n+                                        e\n+                                    );\n+                                    if (e instanceof AlreadyClosedException\n+                                        || (e.getCause() != null && e.getCause() instanceof AlreadyClosedException)) {\n+                                        return; // don't rethrow in case of cache eviction or directory closing\n+                                    }\n+                                    throw e;\n+                                } finally {\n+                                    if (countDown.countDown()) {\n+                                        IOUtils.closeWhileHandlingException(input);\n+                                    }\n                                 }\n-                            }\n-                        });\n+                            });\n+                        }\n                     }\n                 } catch (IOException e) {\n-                    logger.trace(() -> new ParameterizedMessage(\"failed to warm cache for [{}]\", fileName), e);\n+                    logger.trace(() -> new ParameterizedMessage(\"unable to schedule cache prewarming for [{}]\", fileName), e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTY3Nzc1MQ=="}, "originalCommit": {"oid": "9743806ad5a396d88079be890ef43b84b2815ca0"}, "originalPosition": 142}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4NTc1NDEzOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxMDowMjoyNFrOGMbDrQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxMDoxNTo1NVrOGMblxw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTY3OTQwNQ==", "bodyText": "If scheduling fails here, how do we guarantee that IndexInput is closed?", "url": "https://github.com/elastic/elasticsearch/pull/55793#discussion_r415679405", "createdAt": "2020-04-27T10:02:24Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -361,68 +367,95 @@ private void prewarmCache() {\n             final Executor executor = threadPool.executor(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME);\n             logger.debug(\"{} warming shard cache for [{}] files\", shardId, cacheFiles.size());\n \n+            final BlockingQueue<CheckedRunnable<Exception>> queue = new SizeBlockingQueue<>(new LinkedBlockingQueue<>(), Integer.MAX_VALUE);\n             for (BlobStoreIndexShardSnapshot.FileInfo cacheFile : cacheFiles) {\n                 final String fileName = cacheFile.physicalName();\n                 try {\n                     final IndexInput input = openInput(fileName, CachedBlobContainerIndexInput.CACHE_WARMING_CONTEXT);\n                     assert input instanceof CachedBlobContainerIndexInput : \"expected cached index input but got \" + input.getClass();\n \n                     final long numberOfParts = cacheFile.numberOfParts();\n-                    final CountDown countDown = new CountDown(Math.toIntExact(numberOfParts));\n-                    for (long p = 0; p < numberOfParts; p++) {\n-                        final int part = Math.toIntExact(p);\n-                        // TODO use multiple workers to warm each part instead of filling the thread pool\n-                        executor.execute(new AbstractRunnable() {\n-                            @Override\n-                            protected void doRun() throws Exception {\n-                                ensureOpen();\n-\n-                                logger.trace(\"warming cache for [{}] part [{}/{}]\", fileName, part, numberOfParts);\n-                                final long startTimeInNanos = statsCurrentTimeNanosSupplier.getAsLong();\n-\n-                                final CachedBlobContainerIndexInput cachedIndexInput = (CachedBlobContainerIndexInput) input.clone();\n-                                final int bytesRead = cachedIndexInput.prefetchPart(part); // TODO does not include any rate limitation\n-                                assert bytesRead == cacheFile.partBytes(part);\n-\n-                                logger.trace(\n-                                    () -> new ParameterizedMessage(\n-                                        \"part [{}/{}] of [{}] warmed in [{}] ms\",\n-                                        part,\n-                                        numberOfParts,\n-                                        fileName,\n-                                        TimeValue.timeValueNanos(statsCurrentTimeNanosSupplier.getAsLong() - startTimeInNanos).millis()\n-                                    )\n-                                );\n-                            }\n-\n-                            @Override\n-                            public void onFailure(Exception e) {\n-                                logger.trace(\n-                                    () -> new ParameterizedMessage(\n-                                        \"failed to warm cache for [{}] part [{}/{}]\",\n-                                        fileName,\n-                                        part,\n-                                        numberOfParts\n-                                    ),\n-                                    e\n-                                );\n-                            }\n-\n-                            @Override\n-                            public void onAfter() {\n-                                if (countDown.countDown()) {\n-                                    IOUtils.closeWhileHandlingException(input);\n+                    if (queue.remainingCapacity() >= numberOfParts) {\n+                        final CountDown countDown = new CountDown(Math.toIntExact(numberOfParts));\n+                        for (long p = 0; p < numberOfParts; p++) {\n+                            final int part = Math.toIntExact(p);\n+                            queue.add(() -> {\n+                                try {\n+                                    ensureOpen();\n+\n+                                    logger.trace(\"warming cache for [{}] part [{}/{}]\", fileName, part, numberOfParts);\n+                                    final long startTimeInNanos = statsCurrentTimeNanosSupplier.getAsLong();\n+\n+                                    final CachedBlobContainerIndexInput cachedIndexInput = (CachedBlobContainerIndexInput) input.clone();\n+                                    final int bytesRead = cachedIndexInput.prefetchPart(part); // TODO does not include any rate limitation\n+                                    assert bytesRead == cacheFile.partBytes(part);\n+\n+                                    logger.trace(\n+                                        () -> new ParameterizedMessage(\n+                                            \"part [{}/{}] of [{}] warmed in [{}] ms\",\n+                                            part,\n+                                            numberOfParts,\n+                                            fileName,\n+                                            TimeValue.timeValueNanos(statsCurrentTimeNanosSupplier.getAsLong() - startTimeInNanos).millis()\n+                                        )\n+                                    );\n+                                } catch (Exception e) {\n+                                    logger.trace(\n+                                        () -> new ParameterizedMessage(\n+                                            \"failed to warm cache for [{}] part [{}/{}]\",\n+                                            fileName,\n+                                            part,\n+                                            numberOfParts\n+                                        ),\n+                                        e\n+                                    );\n+                                    if (e instanceof AlreadyClosedException\n+                                        || (e.getCause() != null && e.getCause() instanceof AlreadyClosedException)) {\n+                                        return; // don't rethrow in case of cache eviction or directory closing\n+                                    }\n+                                    throw e;\n+                                } finally {\n+                                    if (countDown.countDown()) {\n+                                        IOUtils.closeWhileHandlingException(input);\n+                                    }\n                                 }\n-                            }\n-                        });\n+                            });\n+                        }\n                     }\n                 } catch (IOException e) {\n-                    logger.trace(() -> new ParameterizedMessage(\"failed to warm cache for [{}]\", fileName), e);\n+                    logger.trace(() -> new ParameterizedMessage(\"unable to schedule cache prewarming for [{}]\", fileName), e);\n                 }\n             }\n+\n+            // Start as many workers as fit into the searchable snapshot pool at once at the most\n+            final int workers = Math.min(threadPool.info(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME).getMax(), queue.size());\n+            for (int i = 0; i < workers; ++i) {\n+                executeNextWarmer(executor, queue);\n+            }\n         }\n     }\n \n+    private void executeNextWarmer(final Executor executor, final BlockingQueue<CheckedRunnable<Exception>> queue) {\n+        executor.execute(new AbstractRunnable() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9743806ad5a396d88079be890ef43b84b2815ca0"}, "originalPosition": 155}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTY4ODEzNQ==", "bodyText": "Arghl right... you saw nothing. I'll address that.", "url": "https://github.com/elastic/elasticsearch/pull/55793#discussion_r415688135", "createdAt": "2020-04-27T10:15:55Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -361,68 +367,95 @@ private void prewarmCache() {\n             final Executor executor = threadPool.executor(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME);\n             logger.debug(\"{} warming shard cache for [{}] files\", shardId, cacheFiles.size());\n \n+            final BlockingQueue<CheckedRunnable<Exception>> queue = new SizeBlockingQueue<>(new LinkedBlockingQueue<>(), Integer.MAX_VALUE);\n             for (BlobStoreIndexShardSnapshot.FileInfo cacheFile : cacheFiles) {\n                 final String fileName = cacheFile.physicalName();\n                 try {\n                     final IndexInput input = openInput(fileName, CachedBlobContainerIndexInput.CACHE_WARMING_CONTEXT);\n                     assert input instanceof CachedBlobContainerIndexInput : \"expected cached index input but got \" + input.getClass();\n \n                     final long numberOfParts = cacheFile.numberOfParts();\n-                    final CountDown countDown = new CountDown(Math.toIntExact(numberOfParts));\n-                    for (long p = 0; p < numberOfParts; p++) {\n-                        final int part = Math.toIntExact(p);\n-                        // TODO use multiple workers to warm each part instead of filling the thread pool\n-                        executor.execute(new AbstractRunnable() {\n-                            @Override\n-                            protected void doRun() throws Exception {\n-                                ensureOpen();\n-\n-                                logger.trace(\"warming cache for [{}] part [{}/{}]\", fileName, part, numberOfParts);\n-                                final long startTimeInNanos = statsCurrentTimeNanosSupplier.getAsLong();\n-\n-                                final CachedBlobContainerIndexInput cachedIndexInput = (CachedBlobContainerIndexInput) input.clone();\n-                                final int bytesRead = cachedIndexInput.prefetchPart(part); // TODO does not include any rate limitation\n-                                assert bytesRead == cacheFile.partBytes(part);\n-\n-                                logger.trace(\n-                                    () -> new ParameterizedMessage(\n-                                        \"part [{}/{}] of [{}] warmed in [{}] ms\",\n-                                        part,\n-                                        numberOfParts,\n-                                        fileName,\n-                                        TimeValue.timeValueNanos(statsCurrentTimeNanosSupplier.getAsLong() - startTimeInNanos).millis()\n-                                    )\n-                                );\n-                            }\n-\n-                            @Override\n-                            public void onFailure(Exception e) {\n-                                logger.trace(\n-                                    () -> new ParameterizedMessage(\n-                                        \"failed to warm cache for [{}] part [{}/{}]\",\n-                                        fileName,\n-                                        part,\n-                                        numberOfParts\n-                                    ),\n-                                    e\n-                                );\n-                            }\n-\n-                            @Override\n-                            public void onAfter() {\n-                                if (countDown.countDown()) {\n-                                    IOUtils.closeWhileHandlingException(input);\n+                    if (queue.remainingCapacity() >= numberOfParts) {\n+                        final CountDown countDown = new CountDown(Math.toIntExact(numberOfParts));\n+                        for (long p = 0; p < numberOfParts; p++) {\n+                            final int part = Math.toIntExact(p);\n+                            queue.add(() -> {\n+                                try {\n+                                    ensureOpen();\n+\n+                                    logger.trace(\"warming cache for [{}] part [{}/{}]\", fileName, part, numberOfParts);\n+                                    final long startTimeInNanos = statsCurrentTimeNanosSupplier.getAsLong();\n+\n+                                    final CachedBlobContainerIndexInput cachedIndexInput = (CachedBlobContainerIndexInput) input.clone();\n+                                    final int bytesRead = cachedIndexInput.prefetchPart(part); // TODO does not include any rate limitation\n+                                    assert bytesRead == cacheFile.partBytes(part);\n+\n+                                    logger.trace(\n+                                        () -> new ParameterizedMessage(\n+                                            \"part [{}/{}] of [{}] warmed in [{}] ms\",\n+                                            part,\n+                                            numberOfParts,\n+                                            fileName,\n+                                            TimeValue.timeValueNanos(statsCurrentTimeNanosSupplier.getAsLong() - startTimeInNanos).millis()\n+                                        )\n+                                    );\n+                                } catch (Exception e) {\n+                                    logger.trace(\n+                                        () -> new ParameterizedMessage(\n+                                            \"failed to warm cache for [{}] part [{}/{}]\",\n+                                            fileName,\n+                                            part,\n+                                            numberOfParts\n+                                        ),\n+                                        e\n+                                    );\n+                                    if (e instanceof AlreadyClosedException\n+                                        || (e.getCause() != null && e.getCause() instanceof AlreadyClosedException)) {\n+                                        return; // don't rethrow in case of cache eviction or directory closing\n+                                    }\n+                                    throw e;\n+                                } finally {\n+                                    if (countDown.countDown()) {\n+                                        IOUtils.closeWhileHandlingException(input);\n+                                    }\n                                 }\n-                            }\n-                        });\n+                            });\n+                        }\n                     }\n                 } catch (IOException e) {\n-                    logger.trace(() -> new ParameterizedMessage(\"failed to warm cache for [{}]\", fileName), e);\n+                    logger.trace(() -> new ParameterizedMessage(\"unable to schedule cache prewarming for [{}]\", fileName), e);\n                 }\n             }\n+\n+            // Start as many workers as fit into the searchable snapshot pool at once at the most\n+            final int workers = Math.min(threadPool.info(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME).getMax(), queue.size());\n+            for (int i = 0; i < workers; ++i) {\n+                executeNextWarmer(executor, queue);\n+            }\n         }\n     }\n \n+    private void executeNextWarmer(final Executor executor, final BlockingQueue<CheckedRunnable<Exception>> queue) {\n+        executor.execute(new AbstractRunnable() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTY3OTQwNQ=="}, "originalCommit": {"oid": "9743806ad5a396d88079be890ef43b84b2815ca0"}, "originalPosition": 155}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4NjcwOTExOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxMzozODoyNlrOGMjoQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxMzo1OTowMVrOGMkpXA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTgxOTg0MQ==", "bodyText": "Call warmer.onFailure here?", "url": "https://github.com/elastic/elasticsearch/pull/55793#discussion_r415819841", "createdAt": "2020-04-27T13:38:26Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -393,33 +403,64 @@ protected void doRun() throws Exception {\n                                         TimeValue.timeValueNanos(statsCurrentTimeNanosSupplier.getAsLong() - startTimeInNanos).millis()\n                                     )\n                                 );\n+                            }));\n+                        }\n+                        enqueued = true;\n+                    }\n+                } catch (Exception e) {\n+                    logger.warn(\n+                        () -> new ParameterizedMessage(\"unable to open file [{}], cancelling prewarming for shard [{}]\", fileName, shardId),\n+                        e\n+                    );\n+                    if (queue.size() > 0) {\n+                        try {\n+                            ActionRunnable<Void> warmer;\n+                            while ((warmer = queue.poll(0L, TimeUnit.MILLISECONDS)) != null) {\n+                                warmer.onFailure(new IOException(\"Shard cache prewarming cancelled\"));\n                             }\n+                        } catch (InterruptedException ie) {\n+                            Thread.currentThread().interrupt();\n+                        }\n+                    }\n+                } finally {\n+                    if (enqueued == false) {\n+                        IOUtils.closeWhileHandlingException(input);\n+                    }\n+                }\n+            }\n \n-                            @Override\n-                            public void onFailure(Exception e) {\n-                                logger.trace(\n-                                    () -> new ParameterizedMessage(\n-                                        \"failed to warm cache for [{}] part [{}/{}]\",\n-                                        fileName,\n-                                        part,\n-                                        numberOfParts\n-                                    ),\n-                                    e\n-                                );\n-                            }\n+            // Start as many workers as fit into the searchable snapshot pool at once at the most\n+            final int workers = Math.min(threadPool.info(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME).getMax(), queue.size());\n+            for (int i = 0; i < workers; ++i) {\n+                executeNextWarmer(executor, queue);\n+            }\n+        }\n+    }\n \n-                            @Override\n-                            public void onAfter() {\n-                                if (countDown.countDown()) {\n-                                    IOUtils.closeWhileHandlingException(input);\n-                                }\n-                            }\n-                        });\n+    private void executeNextWarmer(final Executor executor, final BlockingQueue<ActionRunnable<Void>> queue) {\n+        try {\n+            final ActionRunnable<?> warmer = queue.poll(0L, TimeUnit.MILLISECONDS);\n+            if (warmer != null) {\n+                executor.execute(new AbstractRunnable() {\n+                    @Override\n+                    protected void doRun() {\n+                        warmer.run();\n+                        executeNextWarmer(executor, queue);\n                     }\n-                } catch (IOException e) {\n-                    logger.trace(() -> new ParameterizedMessage(\"failed to warm cache for [{}]\", fileName), e);\n-                }\n+\n+                    @Override\n+                    public void onRejection(Exception e) {\n+                        warmer.onRejection(e);\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        logger.warn(\"failed to schedule next file part to prewarm in cache\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "01ca9f711d4c3297890a601f93b7aadc2701e2d3"}, "originalPosition": 150}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTgzNjUwOA==", "bodyText": "I don't think we need to delegate here: warmer.run() already calls warmer.onFailure if something went wrong when executing warmer.doRun() and the only case that we want to delegate to is the case of rejection.", "url": "https://github.com/elastic/elasticsearch/pull/55793#discussion_r415836508", "createdAt": "2020-04-27T13:59:01Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -393,33 +403,64 @@ protected void doRun() throws Exception {\n                                         TimeValue.timeValueNanos(statsCurrentTimeNanosSupplier.getAsLong() - startTimeInNanos).millis()\n                                     )\n                                 );\n+                            }));\n+                        }\n+                        enqueued = true;\n+                    }\n+                } catch (Exception e) {\n+                    logger.warn(\n+                        () -> new ParameterizedMessage(\"unable to open file [{}], cancelling prewarming for shard [{}]\", fileName, shardId),\n+                        e\n+                    );\n+                    if (queue.size() > 0) {\n+                        try {\n+                            ActionRunnable<Void> warmer;\n+                            while ((warmer = queue.poll(0L, TimeUnit.MILLISECONDS)) != null) {\n+                                warmer.onFailure(new IOException(\"Shard cache prewarming cancelled\"));\n                             }\n+                        } catch (InterruptedException ie) {\n+                            Thread.currentThread().interrupt();\n+                        }\n+                    }\n+                } finally {\n+                    if (enqueued == false) {\n+                        IOUtils.closeWhileHandlingException(input);\n+                    }\n+                }\n+            }\n \n-                            @Override\n-                            public void onFailure(Exception e) {\n-                                logger.trace(\n-                                    () -> new ParameterizedMessage(\n-                                        \"failed to warm cache for [{}] part [{}/{}]\",\n-                                        fileName,\n-                                        part,\n-                                        numberOfParts\n-                                    ),\n-                                    e\n-                                );\n-                            }\n+            // Start as many workers as fit into the searchable snapshot pool at once at the most\n+            final int workers = Math.min(threadPool.info(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME).getMax(), queue.size());\n+            for (int i = 0; i < workers; ++i) {\n+                executeNextWarmer(executor, queue);\n+            }\n+        }\n+    }\n \n-                            @Override\n-                            public void onAfter() {\n-                                if (countDown.countDown()) {\n-                                    IOUtils.closeWhileHandlingException(input);\n-                                }\n-                            }\n-                        });\n+    private void executeNextWarmer(final Executor executor, final BlockingQueue<ActionRunnable<Void>> queue) {\n+        try {\n+            final ActionRunnable<?> warmer = queue.poll(0L, TimeUnit.MILLISECONDS);\n+            if (warmer != null) {\n+                executor.execute(new AbstractRunnable() {\n+                    @Override\n+                    protected void doRun() {\n+                        warmer.run();\n+                        executeNextWarmer(executor, queue);\n                     }\n-                } catch (IOException e) {\n-                    logger.trace(() -> new ParameterizedMessage(\"failed to warm cache for [{}]\", fileName), e);\n-                }\n+\n+                    @Override\n+                    public void onRejection(Exception e) {\n+                        warmer.onRejection(e);\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        logger.warn(\"failed to schedule next file part to prewarm in cache\", e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTgxOTg0MQ=="}, "originalCommit": {"oid": "01ca9f711d4c3297890a601f93b7aadc2701e2d3"}, "originalPosition": 150}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4NjcyMjQyOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxMzo0MDo1MlrOGMjv7A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxNDozNDoxM1rOGMma2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTgyMTgwNA==", "bodyText": "This never rethrows the exception?", "url": "https://github.com/elastic/elasticsearch/pull/55793#discussion_r415821804", "createdAt": "2020-04-27T13:40:52Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -393,33 +403,64 @@ protected void doRun() throws Exception {\n                                         TimeValue.timeValueNanos(statsCurrentTimeNanosSupplier.getAsLong() - startTimeInNanos).millis()\n                                     )\n                                 );\n+                            }));\n+                        }\n+                        enqueued = true;\n+                    }\n+                } catch (Exception e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "01ca9f711d4c3297890a601f93b7aadc2701e2d3"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTg0NjQwOQ==", "bodyText": "If I understand correctly rethrowing here would fail the shard which I think it's something we should try to avoid in case of prewarming. Maybe we should rethrow only non-AlreadyClosedException, so that we don't fail the shard if the node is closing.", "url": "https://github.com/elastic/elasticsearch/pull/55793#discussion_r415846409", "createdAt": "2020-04-27T14:11:25Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -393,33 +403,64 @@ protected void doRun() throws Exception {\n                                         TimeValue.timeValueNanos(statsCurrentTimeNanosSupplier.getAsLong() - startTimeInNanos).millis()\n                                     )\n                                 );\n+                            }));\n+                        }\n+                        enqueued = true;\n+                    }\n+                } catch (Exception e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTgyMTgwNA=="}, "originalCommit": {"oid": "01ca9f711d4c3297890a601f93b7aadc2701e2d3"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTg2NTU2Mw==", "bodyText": "Something like 92bcbcb", "url": "https://github.com/elastic/elasticsearch/pull/55793#discussion_r415865563", "createdAt": "2020-04-27T14:34:13Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -393,33 +403,64 @@ protected void doRun() throws Exception {\n                                         TimeValue.timeValueNanos(statsCurrentTimeNanosSupplier.getAsLong() - startTimeInNanos).millis()\n                                     )\n                                 );\n+                            }));\n+                        }\n+                        enqueued = true;\n+                    }\n+                } catch (Exception e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTgyMTgwNA=="}, "originalCommit": {"oid": "01ca9f711d4c3297890a601f93b7aadc2701e2d3"}, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4NjcyMzYxOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxMzo0MTowNlrOGMjwoQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxNDoxNToyMlrOGMlc1g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTgyMTk4NQ==", "bodyText": "This just swallows the exception?", "url": "https://github.com/elastic/elasticsearch/pull/55793#discussion_r415821985", "createdAt": "2020-04-27T13:41:06Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -393,33 +403,64 @@ protected void doRun() throws Exception {\n                                         TimeValue.timeValueNanos(statsCurrentTimeNanosSupplier.getAsLong() - startTimeInNanos).millis()\n                                     )\n                                 );\n+                            }));\n+                        }\n+                        enqueued = true;\n+                    }\n+                } catch (Exception e) {\n+                    logger.warn(\n+                        () -> new ParameterizedMessage(\"unable to open file [{}], cancelling prewarming for shard [{}]\", fileName, shardId),\n+                        e\n+                    );\n+                    if (queue.size() > 0) {\n+                        try {\n+                            ActionRunnable<Void> warmer;\n+                            while ((warmer = queue.poll(0L, TimeUnit.MILLISECONDS)) != null) {\n+                                warmer.onFailure(new IOException(\"Shard cache prewarming cancelled\"));\n                             }\n+                        } catch (InterruptedException ie) {\n+                            Thread.currentThread().interrupt();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "01ca9f711d4c3297890a601f93b7aadc2701e2d3"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTg0OTY4Ng==", "bodyText": "I don't think we need to do more than logging here so I pushed b6b90c4", "url": "https://github.com/elastic/elasticsearch/pull/55793#discussion_r415849686", "createdAt": "2020-04-27T14:15:22Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -393,33 +403,64 @@ protected void doRun() throws Exception {\n                                         TimeValue.timeValueNanos(statsCurrentTimeNanosSupplier.getAsLong() - startTimeInNanos).millis()\n                                     )\n                                 );\n+                            }));\n+                        }\n+                        enqueued = true;\n+                    }\n+                } catch (Exception e) {\n+                    logger.warn(\n+                        () -> new ParameterizedMessage(\"unable to open file [{}], cancelling prewarming for shard [{}]\", fileName, shardId),\n+                        e\n+                    );\n+                    if (queue.size() > 0) {\n+                        try {\n+                            ActionRunnable<Void> warmer;\n+                            while ((warmer = queue.poll(0L, TimeUnit.MILLISECONDS)) != null) {\n+                                warmer.onFailure(new IOException(\"Shard cache prewarming cancelled\"));\n                             }\n+                        } catch (InterruptedException ie) {\n+                            Thread.currentThread().interrupt();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTgyMTk4NQ=="}, "originalCommit": {"oid": "01ca9f711d4c3297890a601f93b7aadc2701e2d3"}, "originalPosition": 92}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4NjcyNDQ3OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxMzo0MToxNVrOGMjxFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxNDoxNToxOVrOGMlclw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTgyMjEwMA==", "bodyText": "This just swallows the exception?", "url": "https://github.com/elastic/elasticsearch/pull/55793#discussion_r415822100", "createdAt": "2020-04-27T13:41:15Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -393,33 +403,64 @@ protected void doRun() throws Exception {\n                                         TimeValue.timeValueNanos(statsCurrentTimeNanosSupplier.getAsLong() - startTimeInNanos).millis()\n                                     )\n                                 );\n+                            }));\n+                        }\n+                        enqueued = true;\n+                    }\n+                } catch (Exception e) {\n+                    logger.warn(\n+                        () -> new ParameterizedMessage(\"unable to open file [{}], cancelling prewarming for shard [{}]\", fileName, shardId),\n+                        e\n+                    );\n+                    if (queue.size() > 0) {\n+                        try {\n+                            ActionRunnable<Void> warmer;\n+                            while ((warmer = queue.poll(0L, TimeUnit.MILLISECONDS)) != null) {\n+                                warmer.onFailure(new IOException(\"Shard cache prewarming cancelled\"));\n                             }\n+                        } catch (InterruptedException ie) {\n+                            Thread.currentThread().interrupt();\n+                        }\n+                    }\n+                } finally {\n+                    if (enqueued == false) {\n+                        IOUtils.closeWhileHandlingException(input);\n+                    }\n+                }\n+            }\n \n-                            @Override\n-                            public void onFailure(Exception e) {\n-                                logger.trace(\n-                                    () -> new ParameterizedMessage(\n-                                        \"failed to warm cache for [{}] part [{}/{}]\",\n-                                        fileName,\n-                                        part,\n-                                        numberOfParts\n-                                    ),\n-                                    e\n-                                );\n-                            }\n+            // Start as many workers as fit into the searchable snapshot pool at once at the most\n+            final int workers = Math.min(threadPool.info(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME).getMax(), queue.size());\n+            for (int i = 0; i < workers; ++i) {\n+                executeNextWarmer(executor, queue);\n+            }\n+        }\n+    }\n \n-                            @Override\n-                            public void onAfter() {\n-                                if (countDown.countDown()) {\n-                                    IOUtils.closeWhileHandlingException(input);\n-                                }\n-                            }\n-                        });\n+    private void executeNextWarmer(final Executor executor, final BlockingQueue<ActionRunnable<Void>> queue) {\n+        try {\n+            final ActionRunnable<?> warmer = queue.poll(0L, TimeUnit.MILLISECONDS);\n+            if (warmer != null) {\n+                executor.execute(new AbstractRunnable() {\n+                    @Override\n+                    protected void doRun() {\n+                        warmer.run();\n+                        executeNextWarmer(executor, queue);\n                     }\n-                } catch (IOException e) {\n-                    logger.trace(() -> new ParameterizedMessage(\"failed to warm cache for [{}]\", fileName), e);\n-                }\n+\n+                    @Override\n+                    public void onRejection(Exception e) {\n+                        warmer.onRejection(e);\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        logger.warn(\"failed to schedule next file part to prewarm in cache\", e);\n+                    }\n+                });\n             }\n+        } catch (InterruptedException e) {\n+            Thread.currentThread().interrupt();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "01ca9f711d4c3297890a601f93b7aadc2701e2d3"}, "originalPosition": 155}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTg0OTYyMw==", "bodyText": "I don't think we need to do more than logging here so I pushed b6b90c4", "url": "https://github.com/elastic/elasticsearch/pull/55793#discussion_r415849623", "createdAt": "2020-04-27T14:15:19Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -393,33 +403,64 @@ protected void doRun() throws Exception {\n                                         TimeValue.timeValueNanos(statsCurrentTimeNanosSupplier.getAsLong() - startTimeInNanos).millis()\n                                     )\n                                 );\n+                            }));\n+                        }\n+                        enqueued = true;\n+                    }\n+                } catch (Exception e) {\n+                    logger.warn(\n+                        () -> new ParameterizedMessage(\"unable to open file [{}], cancelling prewarming for shard [{}]\", fileName, shardId),\n+                        e\n+                    );\n+                    if (queue.size() > 0) {\n+                        try {\n+                            ActionRunnable<Void> warmer;\n+                            while ((warmer = queue.poll(0L, TimeUnit.MILLISECONDS)) != null) {\n+                                warmer.onFailure(new IOException(\"Shard cache prewarming cancelled\"));\n                             }\n+                        } catch (InterruptedException ie) {\n+                            Thread.currentThread().interrupt();\n+                        }\n+                    }\n+                } finally {\n+                    if (enqueued == false) {\n+                        IOUtils.closeWhileHandlingException(input);\n+                    }\n+                }\n+            }\n \n-                            @Override\n-                            public void onFailure(Exception e) {\n-                                logger.trace(\n-                                    () -> new ParameterizedMessage(\n-                                        \"failed to warm cache for [{}] part [{}/{}]\",\n-                                        fileName,\n-                                        part,\n-                                        numberOfParts\n-                                    ),\n-                                    e\n-                                );\n-                            }\n+            // Start as many workers as fit into the searchable snapshot pool at once at the most\n+            final int workers = Math.min(threadPool.info(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME).getMax(), queue.size());\n+            for (int i = 0; i < workers; ++i) {\n+                executeNextWarmer(executor, queue);\n+            }\n+        }\n+    }\n \n-                            @Override\n-                            public void onAfter() {\n-                                if (countDown.countDown()) {\n-                                    IOUtils.closeWhileHandlingException(input);\n-                                }\n-                            }\n-                        });\n+    private void executeNextWarmer(final Executor executor, final BlockingQueue<ActionRunnable<Void>> queue) {\n+        try {\n+            final ActionRunnable<?> warmer = queue.poll(0L, TimeUnit.MILLISECONDS);\n+            if (warmer != null) {\n+                executor.execute(new AbstractRunnable() {\n+                    @Override\n+                    protected void doRun() {\n+                        warmer.run();\n+                        executeNextWarmer(executor, queue);\n                     }\n-                } catch (IOException e) {\n-                    logger.trace(() -> new ParameterizedMessage(\"failed to warm cache for [{}]\", fileName), e);\n-                }\n+\n+                    @Override\n+                    public void onRejection(Exception e) {\n+                        warmer.onRejection(e);\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        logger.warn(\"failed to schedule next file part to prewarm in cache\", e);\n+                    }\n+                });\n             }\n+        } catch (InterruptedException e) {\n+            Thread.currentThread().interrupt();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTgyMjEwMA=="}, "originalCommit": {"oid": "01ca9f711d4c3297890a601f93b7aadc2701e2d3"}, "originalPosition": 155}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4Njc4MDIzOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxMzo1MTo0MlrOGMkR1A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxNDoxNzoxOFrOGMljOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTgzMDQ4NA==", "bodyText": "why not pass the original exception here?", "url": "https://github.com/elastic/elasticsearch/pull/55793#discussion_r415830484", "createdAt": "2020-04-27T13:51:42Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -393,33 +403,64 @@ protected void doRun() throws Exception {\n                                         TimeValue.timeValueNanos(statsCurrentTimeNanosSupplier.getAsLong() - startTimeInNanos).millis()\n                                     )\n                                 );\n+                            }));\n+                        }\n+                        enqueued = true;\n+                    }\n+                } catch (Exception e) {\n+                    logger.warn(\n+                        () -> new ParameterizedMessage(\"unable to open file [{}], cancelling prewarming for shard [{}]\", fileName, shardId),\n+                        e\n+                    );\n+                    if (queue.size() > 0) {\n+                        try {\n+                            ActionRunnable<Void> warmer;\n+                            while ((warmer = queue.poll(0L, TimeUnit.MILLISECONDS)) != null) {\n+                                warmer.onFailure(new IOException(\"Shard cache prewarming cancelled\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "01ca9f711d4c3297890a601f93b7aadc2701e2d3"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTg1MTMyMA==", "bodyText": "Right - I pushed e0f8874", "url": "https://github.com/elastic/elasticsearch/pull/55793#discussion_r415851320", "createdAt": "2020-04-27T14:17:18Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -393,33 +403,64 @@ protected void doRun() throws Exception {\n                                         TimeValue.timeValueNanos(statsCurrentTimeNanosSupplier.getAsLong() - startTimeInNanos).millis()\n                                     )\n                                 );\n+                            }));\n+                        }\n+                        enqueued = true;\n+                    }\n+                } catch (Exception e) {\n+                    logger.warn(\n+                        () -> new ParameterizedMessage(\"unable to open file [{}], cancelling prewarming for shard [{}]\", fileName, shardId),\n+                        e\n+                    );\n+                    if (queue.size() > 0) {\n+                        try {\n+                            ActionRunnable<Void> warmer;\n+                            while ((warmer = queue.poll(0L, TimeUnit.MILLISECONDS)) != null) {\n+                                warmer.onFailure(new IOException(\"Shard cache prewarming cancelled\"));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTgzMDQ4NA=="}, "originalCommit": {"oid": "01ca9f711d4c3297890a601f93b7aadc2701e2d3"}, "originalPosition": 89}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwMDQ3NjE3OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQxMToyMzozNlrOGOk-uw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQxMToyMzozNlrOGOk-uw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzkzOTEzMQ==", "bodyText": "Do we actually need this? If my understanding is correct, we are only ever opening any resources on the worker threads (which haven't yet started here), so why do we need to fail the runnables here here?", "url": "https://github.com/elastic/elasticsearch/pull/55793#discussion_r417939131", "createdAt": "2020-04-30T11:23:36Z", "author": {"login": "original-brownbear"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -392,33 +404,73 @@ protected void doRun() throws Exception {\n                                         TimeValue.timeValueNanos(statsCurrentTimeNanosSupplier.getAsLong() - startTimeInNanos).millis()\n                                     )\n                                 );\n+                            }));\n+                        }\n+                        enqueued = true;\n+                    } else {\n+                        logger.warn(\"{} too many files ({}) to warm in cache, skipping file [{}]\", shardId, queue.size(), fileName);\n+                    }\n+                } catch (Exception e) {\n+                    logger.warn(\n+                        () -> new ParameterizedMessage(\"unable to open file [{}], cancelling prewarming for shard [{}]\", fileName, shardId),\n+                        e\n+                    );\n+                    if (queue.size() > 0) {\n+                        try {\n+                            ActionRunnable<Void> warmer;\n+                            while ((warmer = queue.poll(0L, TimeUnit.MILLISECONDS)) != null) {\n+                                warmer.onFailure(e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d22c34ace60a052294000abc1d9d400f404d66a8"}, "originalPosition": 100}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwMDUxMDEwOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQxMTozNDozNFrOGOlT6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQxMTozNDozNFrOGOlT6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzk0NDU1NA==", "bodyText": "Why do our own queuing here? Can't we just push all the tasks in queue onto the executor and let it poll tasks from its internal queue when we're using the full pool anyway?", "url": "https://github.com/elastic/elasticsearch/pull/55793#discussion_r417944554", "createdAt": "2020-04-30T11:34:34Z", "author": {"login": "original-brownbear"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -392,33 +404,73 @@ protected void doRun() throws Exception {\n                                         TimeValue.timeValueNanos(statsCurrentTimeNanosSupplier.getAsLong() - startTimeInNanos).millis()\n                                     )\n                                 );\n+                            }));\n+                        }\n+                        enqueued = true;\n+                    } else {\n+                        logger.warn(\"{} too many files ({}) to warm in cache, skipping file [{}]\", shardId, queue.size(), fileName);\n+                    }\n+                } catch (Exception e) {\n+                    logger.warn(\n+                        () -> new ParameterizedMessage(\"unable to open file [{}], cancelling prewarming for shard [{}]\", fileName, shardId),\n+                        e\n+                    );\n+                    if (queue.size() > 0) {\n+                        try {\n+                            ActionRunnable<Void> warmer;\n+                            while ((warmer = queue.poll(0L, TimeUnit.MILLISECONDS)) != null) {\n+                                warmer.onFailure(e);\n                             }\n+                        } catch (InterruptedException ie) {\n+                            Thread.currentThread().interrupt();\n+                            logger.warn(() -> new ParameterizedMessage(\"{} shard cache warming has been interrupted\", shardId), ie);\n+                        }\n+                    }\n+                    if (e instanceof AlreadyClosedException || (e.getCause() != null && e.getCause() instanceof AlreadyClosedException)) {\n+                        break;\n+                    } else {\n+                        throw new ElasticsearchException(\"Exception when warming cache for shard \" + shardId, e);\n+                    }\n+                } finally {\n+                    if (enqueued == false) {\n+                        IOUtils.closeWhileHandlingException(input);\n+                    }\n+                }\n+            }\n \n-                            @Override\n-                            public void onFailure(Exception e) {\n-                                logger.trace(\n-                                    () -> new ParameterizedMessage(\n-                                        \"failed to warm cache for [{}] part [{}/{}]\",\n-                                        fileName,\n-                                        part,\n-                                        numberOfParts\n-                                    ),\n-                                    e\n-                                );\n-                            }\n+            // Start as many workers as fit into the searchable snapshot pool at once at the most\n+            final int workers = Math.min(threadPool.info(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME).getMax(), queue.size());\n+            for (int i = 0; i < workers; ++i) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d22c34ace60a052294000abc1d9d400f404d66a8"}, "originalPosition": 133}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxMDM0MjgwOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNFQxMTowNTo1MVrOGP7sKA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNFQxMzo0MzozN1rOGQA9Sw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTM1OTc4NA==", "bodyText": "NIT:\nI guess technically we could\nfinal BlobStoreIndexShardSnapshot.FileInfo file = queue.poll(0L, TimeUnit.MILLISECONDS);\nif (file == null) {\n    return;\n}\nbefore branching off to the executor at the beginning of this method. That way we save at least one pointless task on the scheduler for each shard (and an indent level in the abstract runnable :)).", "url": "https://github.com/elastic/elasticsearch/pull/55793#discussion_r419359784", "createdAt": "2020-05-04T11:05:51Z", "author": {"login": "original-brownbear"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -352,74 +357,71 @@ public String toString() {\n \n     private void prewarmCache() {\n         if (prewarmCache) {\n-            final List<BlobStoreIndexShardSnapshot.FileInfo> cacheFiles = snapshot().indexFiles()\n+            final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> queue = snapshot().indexFiles()\n                 .stream()\n                 .filter(file -> file.metadata().hashEqualsContents() == false)\n                 .filter(file -> isExcludedFromCache(file.physicalName()) == false)\n-                .collect(Collectors.toList());\n+                .collect(Collectors.toCollection(LinkedBlockingQueue::new));\n \n             final Executor executor = threadPool.executor(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME);\n-            logger.debug(\"{} warming shard cache for [{}] files\", shardId, cacheFiles.size());\n+            logger.debug(\"{} warming shard cache for [{}] files\", shardId, queue.size());\n \n-            for (BlobStoreIndexShardSnapshot.FileInfo cacheFile : cacheFiles) {\n-                final String fileName = cacheFile.physicalName();\n-                try {\n-                    final IndexInput input = openInput(fileName, CachedBlobContainerIndexInput.CACHE_WARMING_CONTEXT);\n+            // Start as many workers as fit into the searchable snapshot pool at once at the most\n+            final int workers = Math.min(threadPool.info(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME).getMax(), queue.size());\n+            for (int i = 0; i < workers; ++i) {\n+                prewarmNextFile(executor, queue);\n+            }\n+        }\n+    }\n+\n+    private void prewarmNextFile(final Executor executor, final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> queue) {\n+        executor.execute(new AbstractRunnable() {\n+            @Override\n+            protected void doRun() throws Exception {\n+                final BlobStoreIndexShardSnapshot.FileInfo file = queue.poll(0L, TimeUnit.MILLISECONDS);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d9d5b6ac8894771b0b6a1de023960c9bd03d3b8"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTM2ODE0OA==", "bodyText": "Sure, I pushed dc6d484", "url": "https://github.com/elastic/elasticsearch/pull/55793#discussion_r419368148", "createdAt": "2020-05-04T11:25:01Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -352,74 +357,71 @@ public String toString() {\n \n     private void prewarmCache() {\n         if (prewarmCache) {\n-            final List<BlobStoreIndexShardSnapshot.FileInfo> cacheFiles = snapshot().indexFiles()\n+            final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> queue = snapshot().indexFiles()\n                 .stream()\n                 .filter(file -> file.metadata().hashEqualsContents() == false)\n                 .filter(file -> isExcludedFromCache(file.physicalName()) == false)\n-                .collect(Collectors.toList());\n+                .collect(Collectors.toCollection(LinkedBlockingQueue::new));\n \n             final Executor executor = threadPool.executor(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME);\n-            logger.debug(\"{} warming shard cache for [{}] files\", shardId, cacheFiles.size());\n+            logger.debug(\"{} warming shard cache for [{}] files\", shardId, queue.size());\n \n-            for (BlobStoreIndexShardSnapshot.FileInfo cacheFile : cacheFiles) {\n-                final String fileName = cacheFile.physicalName();\n-                try {\n-                    final IndexInput input = openInput(fileName, CachedBlobContainerIndexInput.CACHE_WARMING_CONTEXT);\n+            // Start as many workers as fit into the searchable snapshot pool at once at the most\n+            final int workers = Math.min(threadPool.info(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME).getMax(), queue.size());\n+            for (int i = 0; i < workers; ++i) {\n+                prewarmNextFile(executor, queue);\n+            }\n+        }\n+    }\n+\n+    private void prewarmNextFile(final Executor executor, final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> queue) {\n+        executor.execute(new AbstractRunnable() {\n+            @Override\n+            protected void doRun() throws Exception {\n+                final BlobStoreIndexShardSnapshot.FileInfo file = queue.poll(0L, TimeUnit.MILLISECONDS);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTM1OTc4NA=="}, "originalCommit": {"oid": "0d9d5b6ac8894771b0b6a1de023960c9bd03d3b8"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQxNDYwOQ==", "bodyText": "Sorry I should've added another line here to the snippet, I was thinking about this:\nfinal BlobStoreIndexShardSnapshot.FileInfo file = queue.poll(0L, TimeUnit.MILLISECONDS);\nif (file == null) {\n    return;\n}\nexecutor.execute(new AbstractRunnable() {\ni.e. doing the polling from the queue on the calling thread. This way:\n\nWe don't have to create as many tasks as we have threads to begin with. (e.g. if we're only warming 5 files, we're still creating 32 threads on the pool of which 27 will just not do anything and just poll a null)\nWe don't waste another task at the end of each file as well. Currently if 32 threads had work at some point, each of them will add one needless task that only polls a null at the end of the warming of a shard.\n\nHope that makes sense? :)", "url": "https://github.com/elastic/elasticsearch/pull/55793#discussion_r419414609", "createdAt": "2020-05-04T12:56:14Z", "author": {"login": "original-brownbear"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -352,74 +357,71 @@ public String toString() {\n \n     private void prewarmCache() {\n         if (prewarmCache) {\n-            final List<BlobStoreIndexShardSnapshot.FileInfo> cacheFiles = snapshot().indexFiles()\n+            final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> queue = snapshot().indexFiles()\n                 .stream()\n                 .filter(file -> file.metadata().hashEqualsContents() == false)\n                 .filter(file -> isExcludedFromCache(file.physicalName()) == false)\n-                .collect(Collectors.toList());\n+                .collect(Collectors.toCollection(LinkedBlockingQueue::new));\n \n             final Executor executor = threadPool.executor(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME);\n-            logger.debug(\"{} warming shard cache for [{}] files\", shardId, cacheFiles.size());\n+            logger.debug(\"{} warming shard cache for [{}] files\", shardId, queue.size());\n \n-            for (BlobStoreIndexShardSnapshot.FileInfo cacheFile : cacheFiles) {\n-                final String fileName = cacheFile.physicalName();\n-                try {\n-                    final IndexInput input = openInput(fileName, CachedBlobContainerIndexInput.CACHE_WARMING_CONTEXT);\n+            // Start as many workers as fit into the searchable snapshot pool at once at the most\n+            final int workers = Math.min(threadPool.info(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME).getMax(), queue.size());\n+            for (int i = 0; i < workers; ++i) {\n+                prewarmNextFile(executor, queue);\n+            }\n+        }\n+    }\n+\n+    private void prewarmNextFile(final Executor executor, final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> queue) {\n+        executor.execute(new AbstractRunnable() {\n+            @Override\n+            protected void doRun() throws Exception {\n+                final BlobStoreIndexShardSnapshot.FileInfo file = queue.poll(0L, TimeUnit.MILLISECONDS);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTM1OTc4NA=="}, "originalCommit": {"oid": "0d9d5b6ac8894771b0b6a1de023960c9bd03d3b8"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQ0NjA5MQ==", "bodyText": "My bad, I've been too quick reading your comment. I pushed 1bffb66", "url": "https://github.com/elastic/elasticsearch/pull/55793#discussion_r419446091", "createdAt": "2020-05-04T13:43:37Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -352,74 +357,71 @@ public String toString() {\n \n     private void prewarmCache() {\n         if (prewarmCache) {\n-            final List<BlobStoreIndexShardSnapshot.FileInfo> cacheFiles = snapshot().indexFiles()\n+            final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> queue = snapshot().indexFiles()\n                 .stream()\n                 .filter(file -> file.metadata().hashEqualsContents() == false)\n                 .filter(file -> isExcludedFromCache(file.physicalName()) == false)\n-                .collect(Collectors.toList());\n+                .collect(Collectors.toCollection(LinkedBlockingQueue::new));\n \n             final Executor executor = threadPool.executor(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME);\n-            logger.debug(\"{} warming shard cache for [{}] files\", shardId, cacheFiles.size());\n+            logger.debug(\"{} warming shard cache for [{}] files\", shardId, queue.size());\n \n-            for (BlobStoreIndexShardSnapshot.FileInfo cacheFile : cacheFiles) {\n-                final String fileName = cacheFile.physicalName();\n-                try {\n-                    final IndexInput input = openInput(fileName, CachedBlobContainerIndexInput.CACHE_WARMING_CONTEXT);\n+            // Start as many workers as fit into the searchable snapshot pool at once at the most\n+            final int workers = Math.min(threadPool.info(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME).getMax(), queue.size());\n+            for (int i = 0; i < workers; ++i) {\n+                prewarmNextFile(executor, queue);\n+            }\n+        }\n+    }\n+\n+    private void prewarmNextFile(final Executor executor, final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> queue) {\n+        executor.execute(new AbstractRunnable() {\n+            @Override\n+            protected void doRun() throws Exception {\n+                final BlobStoreIndexShardSnapshot.FileInfo file = queue.poll(0L, TimeUnit.MILLISECONDS);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTM1OTc4NA=="}, "originalCommit": {"oid": "0d9d5b6ac8894771b0b6a1de023960c9bd03d3b8"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxMDM1NjQ2OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNFQxMToxMDo0OVrOGP70Yg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNFQxMToyNToxNFrOGP8NQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTM2MTg5MA==", "bodyText": "I'm wondering if we should just special case numberOfParts == 1 here? It's the by far most common case and we could simply run it straight on the current thread instead of creating another task. Currently, we're creating twice as many tasks on the pool as necessary with this change.\nNo need for a lot of code here even, could just keep it short and use the direct executor service if numberOfParts == 1 and be happy with it?", "url": "https://github.com/elastic/elasticsearch/pull/55793#discussion_r419361890", "createdAt": "2020-05-04T11:10:49Z", "author": {"login": "original-brownbear"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -352,74 +357,71 @@ public String toString() {\n \n     private void prewarmCache() {\n         if (prewarmCache) {\n-            final List<BlobStoreIndexShardSnapshot.FileInfo> cacheFiles = snapshot().indexFiles()\n+            final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> queue = snapshot().indexFiles()\n                 .stream()\n                 .filter(file -> file.metadata().hashEqualsContents() == false)\n                 .filter(file -> isExcludedFromCache(file.physicalName()) == false)\n-                .collect(Collectors.toList());\n+                .collect(Collectors.toCollection(LinkedBlockingQueue::new));\n \n             final Executor executor = threadPool.executor(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME);\n-            logger.debug(\"{} warming shard cache for [{}] files\", shardId, cacheFiles.size());\n+            logger.debug(\"{} warming shard cache for [{}] files\", shardId, queue.size());\n \n-            for (BlobStoreIndexShardSnapshot.FileInfo cacheFile : cacheFiles) {\n-                final String fileName = cacheFile.physicalName();\n-                try {\n-                    final IndexInput input = openInput(fileName, CachedBlobContainerIndexInput.CACHE_WARMING_CONTEXT);\n+            // Start as many workers as fit into the searchable snapshot pool at once at the most\n+            final int workers = Math.min(threadPool.info(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME).getMax(), queue.size());\n+            for (int i = 0; i < workers; ++i) {\n+                prewarmNextFile(executor, queue);\n+            }\n+        }\n+    }\n+\n+    private void prewarmNextFile(final Executor executor, final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> queue) {\n+        executor.execute(new AbstractRunnable() {\n+            @Override\n+            protected void doRun() throws Exception {\n+                final BlobStoreIndexShardSnapshot.FileInfo file = queue.poll(0L, TimeUnit.MILLISECONDS);\n+                if (file != null) {\n+                    final IndexInput input = openInput(file.physicalName(), CachedBlobContainerIndexInput.CACHE_WARMING_CONTEXT);\n                     assert input instanceof CachedBlobContainerIndexInput : \"expected cached index input but got \" + input.getClass();\n \n-                    final long numberOfParts = cacheFile.numberOfParts();\n-                    final CountDown countDown = new CountDown(Math.toIntExact(numberOfParts));\n-                    for (long p = 0; p < numberOfParts; p++) {\n-                        final int part = Math.toIntExact(p);\n-                        // TODO use multiple workers to warm each part instead of filling the thread pool\n-                        executor.execute(new AbstractRunnable() {\n-                            @Override\n-                            protected void doRun() throws Exception {\n-                                ensureOpen();\n-\n-                                logger.trace(\"warming cache for [{}] part [{}/{}]\", fileName, part, numberOfParts);\n-                                final long startTimeInNanos = statsCurrentTimeNanosSupplier.getAsLong();\n-\n-                                final CachedBlobContainerIndexInput cachedIndexInput = (CachedBlobContainerIndexInput) input.clone();\n-                                cachedIndexInput.prefetchPart(part); // TODO does not include any rate limitation\n-\n-                                logger.trace(\n-                                    () -> new ParameterizedMessage(\n-                                        \"part [{}/{}] of [{}] warmed in [{}] ms\",\n-                                        part,\n-                                        numberOfParts,\n-                                        fileName,\n-                                        TimeValue.timeValueNanos(statsCurrentTimeNanosSupplier.getAsLong() - startTimeInNanos).millis()\n-                                    )\n-                                );\n-                            }\n-\n-                            @Override\n-                            public void onFailure(Exception e) {\n-                                logger.trace(\n-                                    () -> new ParameterizedMessage(\n-                                        \"failed to warm cache for [{}] part [{}/{}]\",\n-                                        fileName,\n-                                        part,\n-                                        numberOfParts\n-                                    ),\n-                                    e\n-                                );\n-                            }\n-\n-                            @Override\n-                            public void onAfter() {\n-                                if (countDown.countDown()) {\n-                                    IOUtils.closeWhileHandlingException(input);\n-                                }\n-                            }\n-                        });\n+                    final int numberOfParts = Math.toIntExact(file.numberOfParts());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d9d5b6ac8894771b0b6a1de023960c9bd03d3b8"}, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTM2ODI1OA==", "bodyText": "Makes perfect sense, I pushed 47b2f0e", "url": "https://github.com/elastic/elasticsearch/pull/55793#discussion_r419368258", "createdAt": "2020-05-04T11:25:14Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -352,74 +357,71 @@ public String toString() {\n \n     private void prewarmCache() {\n         if (prewarmCache) {\n-            final List<BlobStoreIndexShardSnapshot.FileInfo> cacheFiles = snapshot().indexFiles()\n+            final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> queue = snapshot().indexFiles()\n                 .stream()\n                 .filter(file -> file.metadata().hashEqualsContents() == false)\n                 .filter(file -> isExcludedFromCache(file.physicalName()) == false)\n-                .collect(Collectors.toList());\n+                .collect(Collectors.toCollection(LinkedBlockingQueue::new));\n \n             final Executor executor = threadPool.executor(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME);\n-            logger.debug(\"{} warming shard cache for [{}] files\", shardId, cacheFiles.size());\n+            logger.debug(\"{} warming shard cache for [{}] files\", shardId, queue.size());\n \n-            for (BlobStoreIndexShardSnapshot.FileInfo cacheFile : cacheFiles) {\n-                final String fileName = cacheFile.physicalName();\n-                try {\n-                    final IndexInput input = openInput(fileName, CachedBlobContainerIndexInput.CACHE_WARMING_CONTEXT);\n+            // Start as many workers as fit into the searchable snapshot pool at once at the most\n+            final int workers = Math.min(threadPool.info(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME).getMax(), queue.size());\n+            for (int i = 0; i < workers; ++i) {\n+                prewarmNextFile(executor, queue);\n+            }\n+        }\n+    }\n+\n+    private void prewarmNextFile(final Executor executor, final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> queue) {\n+        executor.execute(new AbstractRunnable() {\n+            @Override\n+            protected void doRun() throws Exception {\n+                final BlobStoreIndexShardSnapshot.FileInfo file = queue.poll(0L, TimeUnit.MILLISECONDS);\n+                if (file != null) {\n+                    final IndexInput input = openInput(file.physicalName(), CachedBlobContainerIndexInput.CACHE_WARMING_CONTEXT);\n                     assert input instanceof CachedBlobContainerIndexInput : \"expected cached index input but got \" + input.getClass();\n \n-                    final long numberOfParts = cacheFile.numberOfParts();\n-                    final CountDown countDown = new CountDown(Math.toIntExact(numberOfParts));\n-                    for (long p = 0; p < numberOfParts; p++) {\n-                        final int part = Math.toIntExact(p);\n-                        // TODO use multiple workers to warm each part instead of filling the thread pool\n-                        executor.execute(new AbstractRunnable() {\n-                            @Override\n-                            protected void doRun() throws Exception {\n-                                ensureOpen();\n-\n-                                logger.trace(\"warming cache for [{}] part [{}/{}]\", fileName, part, numberOfParts);\n-                                final long startTimeInNanos = statsCurrentTimeNanosSupplier.getAsLong();\n-\n-                                final CachedBlobContainerIndexInput cachedIndexInput = (CachedBlobContainerIndexInput) input.clone();\n-                                cachedIndexInput.prefetchPart(part); // TODO does not include any rate limitation\n-\n-                                logger.trace(\n-                                    () -> new ParameterizedMessage(\n-                                        \"part [{}/{}] of [{}] warmed in [{}] ms\",\n-                                        part,\n-                                        numberOfParts,\n-                                        fileName,\n-                                        TimeValue.timeValueNanos(statsCurrentTimeNanosSupplier.getAsLong() - startTimeInNanos).millis()\n-                                    )\n-                                );\n-                            }\n-\n-                            @Override\n-                            public void onFailure(Exception e) {\n-                                logger.trace(\n-                                    () -> new ParameterizedMessage(\n-                                        \"failed to warm cache for [{}] part [{}/{}]\",\n-                                        fileName,\n-                                        part,\n-                                        numberOfParts\n-                                    ),\n-                                    e\n-                                );\n-                            }\n-\n-                            @Override\n-                            public void onAfter() {\n-                                if (countDown.countDown()) {\n-                                    IOUtils.closeWhileHandlingException(input);\n-                                }\n-                            }\n-                        });\n+                    final int numberOfParts = Math.toIntExact(file.numberOfParts());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTM2MTg5MA=="}, "originalCommit": {"oid": "0d9d5b6ac8894771b0b6a1de023960c9bd03d3b8"}, "originalPosition": 113}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxMDgwNzY2OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNFQxMzoyNDozNlrOGQAH7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNFQxMzoyNDozNlrOGQAH7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQzMjQzMQ==", "bodyText": "Not sure it's a big deal but I guess we could always use the current thread for one part:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                warmExecutor.execute(ActionRunnable.run(listener, () -> {\n          \n          \n            \n                                 final Executor warmExecutor = (p < numberOfParts - 1) ? executor : EsExecutors.newDirectExecutorService();\n          \n          \n            \n                                warmExecutor.execute(ActionRunnable.run(listener, () -> {", "url": "https://github.com/elastic/elasticsearch/pull/55793#discussion_r419432431", "createdAt": "2020-05-04T13:24:36Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -352,76 +358,78 @@ public String toString() {\n \n     private void prewarmCache() {\n         if (prewarmCache) {\n-            final List<BlobStoreIndexShardSnapshot.FileInfo> cacheFiles = snapshot().indexFiles()\n+            final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> queue = snapshot().indexFiles()\n                 .stream()\n                 .filter(file -> file.metadata().hashEqualsContents() == false)\n                 .filter(file -> isExcludedFromCache(file.physicalName()) == false)\n-                .collect(Collectors.toList());\n+                .collect(Collectors.toCollection(LinkedBlockingQueue::new));\n \n             final Executor executor = threadPool.executor(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME);\n-            logger.debug(\"{} warming shard cache for [{}] files\", shardId, cacheFiles.size());\n-\n-            for (BlobStoreIndexShardSnapshot.FileInfo cacheFile : cacheFiles) {\n-                final String fileName = cacheFile.physicalName();\n-                try {\n-                    final IndexInput input = openInput(fileName, CachedBlobContainerIndexInput.CACHE_WARMING_CONTEXT);\n-                    assert input instanceof CachedBlobContainerIndexInput : \"expected cached index input but got \" + input.getClass();\n-\n-                    final long numberOfParts = cacheFile.numberOfParts();\n-                    final CountDown countDown = new CountDown(Math.toIntExact(numberOfParts));\n-                    for (long p = 0; p < numberOfParts; p++) {\n-                        final int part = Math.toIntExact(p);\n-                        // TODO use multiple workers to warm each part instead of filling the thread pool\n-                        executor.execute(new AbstractRunnable() {\n-                            @Override\n-                            protected void doRun() throws Exception {\n-                                ensureOpen();\n-\n-                                logger.trace(\"warming cache for [{}] part [{}/{}]\", fileName, part, numberOfParts);\n-                                final long startTimeInNanos = statsCurrentTimeNanosSupplier.getAsLong();\n-\n-                                final CachedBlobContainerIndexInput cachedIndexInput = (CachedBlobContainerIndexInput) input.clone();\n-                                cachedIndexInput.prefetchPart(part); // TODO does not include any rate limitation\n-\n-                                logger.trace(\n-                                    () -> new ParameterizedMessage(\n-                                        \"part [{}/{}] of [{}] warmed in [{}] ms\",\n-                                        part,\n-                                        numberOfParts,\n-                                        fileName,\n-                                        TimeValue.timeValueNanos(statsCurrentTimeNanosSupplier.getAsLong() - startTimeInNanos).millis()\n-                                    )\n-                                );\n-                            }\n-\n-                            @Override\n-                            public void onFailure(Exception e) {\n-                                logger.trace(\n-                                    () -> new ParameterizedMessage(\n-                                        \"failed to warm cache for [{}] part [{}/{}]\",\n-                                        fileName,\n-                                        part,\n-                                        numberOfParts\n-                                    ),\n-                                    e\n-                                );\n-                            }\n-\n-                            @Override\n-                            public void onAfter() {\n-                                if (countDown.countDown()) {\n-                                    IOUtils.closeWhileHandlingException(input);\n-                                }\n-                            }\n-                        });\n-                    }\n-                } catch (IOException e) {\n-                    logger.trace(() -> new ParameterizedMessage(\"failed to warm cache for [{}]\", fileName), e);\n-                }\n+            logger.debug(\"{} warming shard cache for [{}] files\", shardId, queue.size());\n+\n+            // Start as many workers as fit into the searchable snapshot pool at once at the most\n+            final int workers = Math.min(threadPool.info(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME).getMax(), queue.size());\n+            for (int i = 0; i < workers; ++i) {\n+                prewarmNextFile(executor, queue);\n             }\n         }\n     }\n \n+    private void prewarmNextFile(final Executor executor, final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> queue) {\n+        executor.execute(new AbstractRunnable() {\n+            @Override\n+            protected void doRun() throws Exception {\n+                final BlobStoreIndexShardSnapshot.FileInfo file = queue.poll(0L, TimeUnit.MILLISECONDS);\n+                if (file == null) {\n+                    return;\n+                }\n+\n+                final IndexInput input = openInput(file.physicalName(), CachedBlobContainerIndexInput.CACHE_WARMING_CONTEXT);\n+                assert input instanceof CachedBlobContainerIndexInput : \"expected cached index input but got \" + input.getClass();\n+\n+                final int numberOfParts = Math.toIntExact(file.numberOfParts());\n+                final GroupedActionListener<Void> listener = new GroupedActionListener<>(\n+                    ActionListener.runAfter(\n+                        ActionListener.wrap(() -> IOUtils.closeWhileHandlingException(input)),\n+                        () -> prewarmNextFile(executor, queue)\n+                    ),\n+                    numberOfParts\n+                );\n+\n+                // if the file to prewarm is composed of a single part then it is prewarmed using the current thread\n+                final Executor warmExecutor = (numberOfParts > 1) ? executor : EsExecutors.newDirectExecutorService();\n+\n+                for (int p = 0; p < numberOfParts; p++) {\n+                    final int part = p;\n+                    warmExecutor.execute(ActionRunnable.run(listener, () -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fffc4cf388fbe10dc184b5a823bedf76144bd371"}, "originalPosition": 138}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxMDg0MDcxOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNFQxMzozMToyMlrOGQAbXg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNFQxNToxNDo0NFrOGQFBUw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQzNzQwNg==", "bodyText": "I think this means we use IOUtils.closeWhileHandlingException even when not handling an exception, which sounds wrong to me.", "url": "https://github.com/elastic/elasticsearch/pull/55793#discussion_r419437406", "createdAt": "2020-05-04T13:31:22Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -352,76 +358,78 @@ public String toString() {\n \n     private void prewarmCache() {\n         if (prewarmCache) {\n-            final List<BlobStoreIndexShardSnapshot.FileInfo> cacheFiles = snapshot().indexFiles()\n+            final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> queue = snapshot().indexFiles()\n                 .stream()\n                 .filter(file -> file.metadata().hashEqualsContents() == false)\n                 .filter(file -> isExcludedFromCache(file.physicalName()) == false)\n-                .collect(Collectors.toList());\n+                .collect(Collectors.toCollection(LinkedBlockingQueue::new));\n \n             final Executor executor = threadPool.executor(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME);\n-            logger.debug(\"{} warming shard cache for [{}] files\", shardId, cacheFiles.size());\n-\n-            for (BlobStoreIndexShardSnapshot.FileInfo cacheFile : cacheFiles) {\n-                final String fileName = cacheFile.physicalName();\n-                try {\n-                    final IndexInput input = openInput(fileName, CachedBlobContainerIndexInput.CACHE_WARMING_CONTEXT);\n-                    assert input instanceof CachedBlobContainerIndexInput : \"expected cached index input but got \" + input.getClass();\n-\n-                    final long numberOfParts = cacheFile.numberOfParts();\n-                    final CountDown countDown = new CountDown(Math.toIntExact(numberOfParts));\n-                    for (long p = 0; p < numberOfParts; p++) {\n-                        final int part = Math.toIntExact(p);\n-                        // TODO use multiple workers to warm each part instead of filling the thread pool\n-                        executor.execute(new AbstractRunnable() {\n-                            @Override\n-                            protected void doRun() throws Exception {\n-                                ensureOpen();\n-\n-                                logger.trace(\"warming cache for [{}] part [{}/{}]\", fileName, part, numberOfParts);\n-                                final long startTimeInNanos = statsCurrentTimeNanosSupplier.getAsLong();\n-\n-                                final CachedBlobContainerIndexInput cachedIndexInput = (CachedBlobContainerIndexInput) input.clone();\n-                                cachedIndexInput.prefetchPart(part); // TODO does not include any rate limitation\n-\n-                                logger.trace(\n-                                    () -> new ParameterizedMessage(\n-                                        \"part [{}/{}] of [{}] warmed in [{}] ms\",\n-                                        part,\n-                                        numberOfParts,\n-                                        fileName,\n-                                        TimeValue.timeValueNanos(statsCurrentTimeNanosSupplier.getAsLong() - startTimeInNanos).millis()\n-                                    )\n-                                );\n-                            }\n-\n-                            @Override\n-                            public void onFailure(Exception e) {\n-                                logger.trace(\n-                                    () -> new ParameterizedMessage(\n-                                        \"failed to warm cache for [{}] part [{}/{}]\",\n-                                        fileName,\n-                                        part,\n-                                        numberOfParts\n-                                    ),\n-                                    e\n-                                );\n-                            }\n-\n-                            @Override\n-                            public void onAfter() {\n-                                if (countDown.countDown()) {\n-                                    IOUtils.closeWhileHandlingException(input);\n-                                }\n-                            }\n-                        });\n-                    }\n-                } catch (IOException e) {\n-                    logger.trace(() -> new ParameterizedMessage(\"failed to warm cache for [{}]\", fileName), e);\n-                }\n+            logger.debug(\"{} warming shard cache for [{}] files\", shardId, queue.size());\n+\n+            // Start as many workers as fit into the searchable snapshot pool at once at the most\n+            final int workers = Math.min(threadPool.info(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME).getMax(), queue.size());\n+            for (int i = 0; i < workers; ++i) {\n+                prewarmNextFile(executor, queue);\n             }\n         }\n     }\n \n+    private void prewarmNextFile(final Executor executor, final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> queue) {\n+        executor.execute(new AbstractRunnable() {\n+            @Override\n+            protected void doRun() throws Exception {\n+                final BlobStoreIndexShardSnapshot.FileInfo file = queue.poll(0L, TimeUnit.MILLISECONDS);\n+                if (file == null) {\n+                    return;\n+                }\n+\n+                final IndexInput input = openInput(file.physicalName(), CachedBlobContainerIndexInput.CACHE_WARMING_CONTEXT);\n+                assert input instanceof CachedBlobContainerIndexInput : \"expected cached index input but got \" + input.getClass();\n+\n+                final int numberOfParts = Math.toIntExact(file.numberOfParts());\n+                final GroupedActionListener<Void> listener = new GroupedActionListener<>(\n+                    ActionListener.runAfter(\n+                        ActionListener.wrap(() -> IOUtils.closeWhileHandlingException(input)),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fffc4cf388fbe10dc184b5a823bedf76144bd371"}, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTUxMjY1OQ==", "bodyText": "Good point, I changed that.", "url": "https://github.com/elastic/elasticsearch/pull/55793#discussion_r419512659", "createdAt": "2020-05-04T15:14:44Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -352,76 +358,78 @@ public String toString() {\n \n     private void prewarmCache() {\n         if (prewarmCache) {\n-            final List<BlobStoreIndexShardSnapshot.FileInfo> cacheFiles = snapshot().indexFiles()\n+            final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> queue = snapshot().indexFiles()\n                 .stream()\n                 .filter(file -> file.metadata().hashEqualsContents() == false)\n                 .filter(file -> isExcludedFromCache(file.physicalName()) == false)\n-                .collect(Collectors.toList());\n+                .collect(Collectors.toCollection(LinkedBlockingQueue::new));\n \n             final Executor executor = threadPool.executor(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME);\n-            logger.debug(\"{} warming shard cache for [{}] files\", shardId, cacheFiles.size());\n-\n-            for (BlobStoreIndexShardSnapshot.FileInfo cacheFile : cacheFiles) {\n-                final String fileName = cacheFile.physicalName();\n-                try {\n-                    final IndexInput input = openInput(fileName, CachedBlobContainerIndexInput.CACHE_WARMING_CONTEXT);\n-                    assert input instanceof CachedBlobContainerIndexInput : \"expected cached index input but got \" + input.getClass();\n-\n-                    final long numberOfParts = cacheFile.numberOfParts();\n-                    final CountDown countDown = new CountDown(Math.toIntExact(numberOfParts));\n-                    for (long p = 0; p < numberOfParts; p++) {\n-                        final int part = Math.toIntExact(p);\n-                        // TODO use multiple workers to warm each part instead of filling the thread pool\n-                        executor.execute(new AbstractRunnable() {\n-                            @Override\n-                            protected void doRun() throws Exception {\n-                                ensureOpen();\n-\n-                                logger.trace(\"warming cache for [{}] part [{}/{}]\", fileName, part, numberOfParts);\n-                                final long startTimeInNanos = statsCurrentTimeNanosSupplier.getAsLong();\n-\n-                                final CachedBlobContainerIndexInput cachedIndexInput = (CachedBlobContainerIndexInput) input.clone();\n-                                cachedIndexInput.prefetchPart(part); // TODO does not include any rate limitation\n-\n-                                logger.trace(\n-                                    () -> new ParameterizedMessage(\n-                                        \"part [{}/{}] of [{}] warmed in [{}] ms\",\n-                                        part,\n-                                        numberOfParts,\n-                                        fileName,\n-                                        TimeValue.timeValueNanos(statsCurrentTimeNanosSupplier.getAsLong() - startTimeInNanos).millis()\n-                                    )\n-                                );\n-                            }\n-\n-                            @Override\n-                            public void onFailure(Exception e) {\n-                                logger.trace(\n-                                    () -> new ParameterizedMessage(\n-                                        \"failed to warm cache for [{}] part [{}/{}]\",\n-                                        fileName,\n-                                        part,\n-                                        numberOfParts\n-                                    ),\n-                                    e\n-                                );\n-                            }\n-\n-                            @Override\n-                            public void onAfter() {\n-                                if (countDown.countDown()) {\n-                                    IOUtils.closeWhileHandlingException(input);\n-                                }\n-                            }\n-                        });\n-                    }\n-                } catch (IOException e) {\n-                    logger.trace(() -> new ParameterizedMessage(\"failed to warm cache for [{}]\", fileName), e);\n-                }\n+            logger.debug(\"{} warming shard cache for [{}] files\", shardId, queue.size());\n+\n+            // Start as many workers as fit into the searchable snapshot pool at once at the most\n+            final int workers = Math.min(threadPool.info(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME).getMax(), queue.size());\n+            for (int i = 0; i < workers; ++i) {\n+                prewarmNextFile(executor, queue);\n             }\n         }\n     }\n \n+    private void prewarmNextFile(final Executor executor, final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> queue) {\n+        executor.execute(new AbstractRunnable() {\n+            @Override\n+            protected void doRun() throws Exception {\n+                final BlobStoreIndexShardSnapshot.FileInfo file = queue.poll(0L, TimeUnit.MILLISECONDS);\n+                if (file == null) {\n+                    return;\n+                }\n+\n+                final IndexInput input = openInput(file.physicalName(), CachedBlobContainerIndexInput.CACHE_WARMING_CONTEXT);\n+                assert input instanceof CachedBlobContainerIndexInput : \"expected cached index input but got \" + input.getClass();\n+\n+                final int numberOfParts = Math.toIntExact(file.numberOfParts());\n+                final GroupedActionListener<Void> listener = new GroupedActionListener<>(\n+                    ActionListener.runAfter(\n+                        ActionListener.wrap(() -> IOUtils.closeWhileHandlingException(input)),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQzNzQwNg=="}, "originalCommit": {"oid": "fffc4cf388fbe10dc184b5a823bedf76144bd371"}, "originalPosition": 127}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxMDg3NTU0OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNFQxMzozOTowOFrOGQAwZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNFQxNTo0NDowNVrOGQGSvA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQ0Mjc4OQ==", "bodyText": "IIUC this means we wait for all parts of this file to be warmed before starting on the next file, which sounds like it might under-utilise the available threads. Could we use something cleverer than a Queue<FileInfo> to track the progress of the work in terms of parts that keeps all the threads busy until the very end?", "url": "https://github.com/elastic/elasticsearch/pull/55793#discussion_r419442789", "createdAt": "2020-05-04T13:39:08Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -352,76 +358,78 @@ public String toString() {\n \n     private void prewarmCache() {\n         if (prewarmCache) {\n-            final List<BlobStoreIndexShardSnapshot.FileInfo> cacheFiles = snapshot().indexFiles()\n+            final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> queue = snapshot().indexFiles()\n                 .stream()\n                 .filter(file -> file.metadata().hashEqualsContents() == false)\n                 .filter(file -> isExcludedFromCache(file.physicalName()) == false)\n-                .collect(Collectors.toList());\n+                .collect(Collectors.toCollection(LinkedBlockingQueue::new));\n \n             final Executor executor = threadPool.executor(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME);\n-            logger.debug(\"{} warming shard cache for [{}] files\", shardId, cacheFiles.size());\n-\n-            for (BlobStoreIndexShardSnapshot.FileInfo cacheFile : cacheFiles) {\n-                final String fileName = cacheFile.physicalName();\n-                try {\n-                    final IndexInput input = openInput(fileName, CachedBlobContainerIndexInput.CACHE_WARMING_CONTEXT);\n-                    assert input instanceof CachedBlobContainerIndexInput : \"expected cached index input but got \" + input.getClass();\n-\n-                    final long numberOfParts = cacheFile.numberOfParts();\n-                    final CountDown countDown = new CountDown(Math.toIntExact(numberOfParts));\n-                    for (long p = 0; p < numberOfParts; p++) {\n-                        final int part = Math.toIntExact(p);\n-                        // TODO use multiple workers to warm each part instead of filling the thread pool\n-                        executor.execute(new AbstractRunnable() {\n-                            @Override\n-                            protected void doRun() throws Exception {\n-                                ensureOpen();\n-\n-                                logger.trace(\"warming cache for [{}] part [{}/{}]\", fileName, part, numberOfParts);\n-                                final long startTimeInNanos = statsCurrentTimeNanosSupplier.getAsLong();\n-\n-                                final CachedBlobContainerIndexInput cachedIndexInput = (CachedBlobContainerIndexInput) input.clone();\n-                                cachedIndexInput.prefetchPart(part); // TODO does not include any rate limitation\n-\n-                                logger.trace(\n-                                    () -> new ParameterizedMessage(\n-                                        \"part [{}/{}] of [{}] warmed in [{}] ms\",\n-                                        part,\n-                                        numberOfParts,\n-                                        fileName,\n-                                        TimeValue.timeValueNanos(statsCurrentTimeNanosSupplier.getAsLong() - startTimeInNanos).millis()\n-                                    )\n-                                );\n-                            }\n-\n-                            @Override\n-                            public void onFailure(Exception e) {\n-                                logger.trace(\n-                                    () -> new ParameterizedMessage(\n-                                        \"failed to warm cache for [{}] part [{}/{}]\",\n-                                        fileName,\n-                                        part,\n-                                        numberOfParts\n-                                    ),\n-                                    e\n-                                );\n-                            }\n-\n-                            @Override\n-                            public void onAfter() {\n-                                if (countDown.countDown()) {\n-                                    IOUtils.closeWhileHandlingException(input);\n-                                }\n-                            }\n-                        });\n-                    }\n-                } catch (IOException e) {\n-                    logger.trace(() -> new ParameterizedMessage(\"failed to warm cache for [{}]\", fileName), e);\n-                }\n+            logger.debug(\"{} warming shard cache for [{}] files\", shardId, queue.size());\n+\n+            // Start as many workers as fit into the searchable snapshot pool at once at the most\n+            final int workers = Math.min(threadPool.info(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME).getMax(), queue.size());\n+            for (int i = 0; i < workers; ++i) {\n+                prewarmNextFile(executor, queue);\n             }\n         }\n     }\n \n+    private void prewarmNextFile(final Executor executor, final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> queue) {\n+        executor.execute(new AbstractRunnable() {\n+            @Override\n+            protected void doRun() throws Exception {\n+                final BlobStoreIndexShardSnapshot.FileInfo file = queue.poll(0L, TimeUnit.MILLISECONDS);\n+                if (file == null) {\n+                    return;\n+                }\n+\n+                final IndexInput input = openInput(file.physicalName(), CachedBlobContainerIndexInput.CACHE_WARMING_CONTEXT);\n+                assert input instanceof CachedBlobContainerIndexInput : \"expected cached index input but got \" + input.getClass();\n+\n+                final int numberOfParts = Math.toIntExact(file.numberOfParts());\n+                final GroupedActionListener<Void> listener = new GroupedActionListener<>(\n+                    ActionListener.runAfter(\n+                        ActionListener.wrap(() -> IOUtils.closeWhileHandlingException(input)),\n+                        () -> prewarmNextFile(executor, queue)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fffc4cf388fbe10dc184b5a823bedf76144bd371"}, "originalPosition": 128}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTUxNDczOQ==", "bodyText": "I agree this is not optimal. The alternative I see is to enqueue file parts instead of file and then have workers polling each part to warm. I pushed 1aea980, it's a bit less readable IMO but still more than what I did in previous iterations. Let me know what you think.", "url": "https://github.com/elastic/elasticsearch/pull/55793#discussion_r419514739", "createdAt": "2020-05-04T15:17:44Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -352,76 +358,78 @@ public String toString() {\n \n     private void prewarmCache() {\n         if (prewarmCache) {\n-            final List<BlobStoreIndexShardSnapshot.FileInfo> cacheFiles = snapshot().indexFiles()\n+            final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> queue = snapshot().indexFiles()\n                 .stream()\n                 .filter(file -> file.metadata().hashEqualsContents() == false)\n                 .filter(file -> isExcludedFromCache(file.physicalName()) == false)\n-                .collect(Collectors.toList());\n+                .collect(Collectors.toCollection(LinkedBlockingQueue::new));\n \n             final Executor executor = threadPool.executor(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME);\n-            logger.debug(\"{} warming shard cache for [{}] files\", shardId, cacheFiles.size());\n-\n-            for (BlobStoreIndexShardSnapshot.FileInfo cacheFile : cacheFiles) {\n-                final String fileName = cacheFile.physicalName();\n-                try {\n-                    final IndexInput input = openInput(fileName, CachedBlobContainerIndexInput.CACHE_WARMING_CONTEXT);\n-                    assert input instanceof CachedBlobContainerIndexInput : \"expected cached index input but got \" + input.getClass();\n-\n-                    final long numberOfParts = cacheFile.numberOfParts();\n-                    final CountDown countDown = new CountDown(Math.toIntExact(numberOfParts));\n-                    for (long p = 0; p < numberOfParts; p++) {\n-                        final int part = Math.toIntExact(p);\n-                        // TODO use multiple workers to warm each part instead of filling the thread pool\n-                        executor.execute(new AbstractRunnable() {\n-                            @Override\n-                            protected void doRun() throws Exception {\n-                                ensureOpen();\n-\n-                                logger.trace(\"warming cache for [{}] part [{}/{}]\", fileName, part, numberOfParts);\n-                                final long startTimeInNanos = statsCurrentTimeNanosSupplier.getAsLong();\n-\n-                                final CachedBlobContainerIndexInput cachedIndexInput = (CachedBlobContainerIndexInput) input.clone();\n-                                cachedIndexInput.prefetchPart(part); // TODO does not include any rate limitation\n-\n-                                logger.trace(\n-                                    () -> new ParameterizedMessage(\n-                                        \"part [{}/{}] of [{}] warmed in [{}] ms\",\n-                                        part,\n-                                        numberOfParts,\n-                                        fileName,\n-                                        TimeValue.timeValueNanos(statsCurrentTimeNanosSupplier.getAsLong() - startTimeInNanos).millis()\n-                                    )\n-                                );\n-                            }\n-\n-                            @Override\n-                            public void onFailure(Exception e) {\n-                                logger.trace(\n-                                    () -> new ParameterizedMessage(\n-                                        \"failed to warm cache for [{}] part [{}/{}]\",\n-                                        fileName,\n-                                        part,\n-                                        numberOfParts\n-                                    ),\n-                                    e\n-                                );\n-                            }\n-\n-                            @Override\n-                            public void onAfter() {\n-                                if (countDown.countDown()) {\n-                                    IOUtils.closeWhileHandlingException(input);\n-                                }\n-                            }\n-                        });\n-                    }\n-                } catch (IOException e) {\n-                    logger.trace(() -> new ParameterizedMessage(\"failed to warm cache for [{}]\", fileName), e);\n-                }\n+            logger.debug(\"{} warming shard cache for [{}] files\", shardId, queue.size());\n+\n+            // Start as many workers as fit into the searchable snapshot pool at once at the most\n+            final int workers = Math.min(threadPool.info(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME).getMax(), queue.size());\n+            for (int i = 0; i < workers; ++i) {\n+                prewarmNextFile(executor, queue);\n             }\n         }\n     }\n \n+    private void prewarmNextFile(final Executor executor, final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> queue) {\n+        executor.execute(new AbstractRunnable() {\n+            @Override\n+            protected void doRun() throws Exception {\n+                final BlobStoreIndexShardSnapshot.FileInfo file = queue.poll(0L, TimeUnit.MILLISECONDS);\n+                if (file == null) {\n+                    return;\n+                }\n+\n+                final IndexInput input = openInput(file.physicalName(), CachedBlobContainerIndexInput.CACHE_WARMING_CONTEXT);\n+                assert input instanceof CachedBlobContainerIndexInput : \"expected cached index input but got \" + input.getClass();\n+\n+                final int numberOfParts = Math.toIntExact(file.numberOfParts());\n+                final GroupedActionListener<Void> listener = new GroupedActionListener<>(\n+                    ActionListener.runAfter(\n+                        ActionListener.wrap(() -> IOUtils.closeWhileHandlingException(input)),\n+                        () -> prewarmNextFile(executor, queue)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQ0Mjc4OQ=="}, "originalCommit": {"oid": "fffc4cf388fbe10dc184b5a823bedf76144bd371"}, "originalPosition": 128}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTUzMzUwMA==", "bodyText": "Yes I like that solution, thanks for the extra iteration. I don't think it's made a material difference to readability but I do think it's clearer now that the tasks are the warming of the individual parts rather than the files; also I just noticed that previously we would bound the worker pool size by the number of files rather than the number of parts, which may have resulted in less concurrency than we wanted -- this is now addressed.", "url": "https://github.com/elastic/elasticsearch/pull/55793#discussion_r419533500", "createdAt": "2020-05-04T15:44:05Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -352,76 +358,78 @@ public String toString() {\n \n     private void prewarmCache() {\n         if (prewarmCache) {\n-            final List<BlobStoreIndexShardSnapshot.FileInfo> cacheFiles = snapshot().indexFiles()\n+            final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> queue = snapshot().indexFiles()\n                 .stream()\n                 .filter(file -> file.metadata().hashEqualsContents() == false)\n                 .filter(file -> isExcludedFromCache(file.physicalName()) == false)\n-                .collect(Collectors.toList());\n+                .collect(Collectors.toCollection(LinkedBlockingQueue::new));\n \n             final Executor executor = threadPool.executor(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME);\n-            logger.debug(\"{} warming shard cache for [{}] files\", shardId, cacheFiles.size());\n-\n-            for (BlobStoreIndexShardSnapshot.FileInfo cacheFile : cacheFiles) {\n-                final String fileName = cacheFile.physicalName();\n-                try {\n-                    final IndexInput input = openInput(fileName, CachedBlobContainerIndexInput.CACHE_WARMING_CONTEXT);\n-                    assert input instanceof CachedBlobContainerIndexInput : \"expected cached index input but got \" + input.getClass();\n-\n-                    final long numberOfParts = cacheFile.numberOfParts();\n-                    final CountDown countDown = new CountDown(Math.toIntExact(numberOfParts));\n-                    for (long p = 0; p < numberOfParts; p++) {\n-                        final int part = Math.toIntExact(p);\n-                        // TODO use multiple workers to warm each part instead of filling the thread pool\n-                        executor.execute(new AbstractRunnable() {\n-                            @Override\n-                            protected void doRun() throws Exception {\n-                                ensureOpen();\n-\n-                                logger.trace(\"warming cache for [{}] part [{}/{}]\", fileName, part, numberOfParts);\n-                                final long startTimeInNanos = statsCurrentTimeNanosSupplier.getAsLong();\n-\n-                                final CachedBlobContainerIndexInput cachedIndexInput = (CachedBlobContainerIndexInput) input.clone();\n-                                cachedIndexInput.prefetchPart(part); // TODO does not include any rate limitation\n-\n-                                logger.trace(\n-                                    () -> new ParameterizedMessage(\n-                                        \"part [{}/{}] of [{}] warmed in [{}] ms\",\n-                                        part,\n-                                        numberOfParts,\n-                                        fileName,\n-                                        TimeValue.timeValueNanos(statsCurrentTimeNanosSupplier.getAsLong() - startTimeInNanos).millis()\n-                                    )\n-                                );\n-                            }\n-\n-                            @Override\n-                            public void onFailure(Exception e) {\n-                                logger.trace(\n-                                    () -> new ParameterizedMessage(\n-                                        \"failed to warm cache for [{}] part [{}/{}]\",\n-                                        fileName,\n-                                        part,\n-                                        numberOfParts\n-                                    ),\n-                                    e\n-                                );\n-                            }\n-\n-                            @Override\n-                            public void onAfter() {\n-                                if (countDown.countDown()) {\n-                                    IOUtils.closeWhileHandlingException(input);\n-                                }\n-                            }\n-                        });\n-                    }\n-                } catch (IOException e) {\n-                    logger.trace(() -> new ParameterizedMessage(\"failed to warm cache for [{}]\", fileName), e);\n-                }\n+            logger.debug(\"{} warming shard cache for [{}] files\", shardId, queue.size());\n+\n+            // Start as many workers as fit into the searchable snapshot pool at once at the most\n+            final int workers = Math.min(threadPool.info(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME).getMax(), queue.size());\n+            for (int i = 0; i < workers; ++i) {\n+                prewarmNextFile(executor, queue);\n             }\n         }\n     }\n \n+    private void prewarmNextFile(final Executor executor, final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> queue) {\n+        executor.execute(new AbstractRunnable() {\n+            @Override\n+            protected void doRun() throws Exception {\n+                final BlobStoreIndexShardSnapshot.FileInfo file = queue.poll(0L, TimeUnit.MILLISECONDS);\n+                if (file == null) {\n+                    return;\n+                }\n+\n+                final IndexInput input = openInput(file.physicalName(), CachedBlobContainerIndexInput.CACHE_WARMING_CONTEXT);\n+                assert input instanceof CachedBlobContainerIndexInput : \"expected cached index input but got \" + input.getClass();\n+\n+                final int numberOfParts = Math.toIntExact(file.numberOfParts());\n+                final GroupedActionListener<Void> listener = new GroupedActionListener<>(\n+                    ActionListener.runAfter(\n+                        ActionListener.wrap(() -> IOUtils.closeWhileHandlingException(input)),\n+                        () -> prewarmNextFile(executor, queue)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQ0Mjc4OQ=="}, "originalCommit": {"oid": "fffc4cf388fbe10dc184b5a823bedf76144bd371"}, "originalPosition": 128}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2606, "cost": 1, "resetAt": "2021-11-12T13:16:51Z"}}}