{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDcyNTkwMjcz", "number": 61484, "title": "Write deprecation logs to a data stream", "bodyText": "Closes #46106. Implement a new log4j appender for deprecation logging, in order to write logs to a dedicated data stream. This is controlled by a new setting, cluster.deprecation_indexing.enabled.\nDepends on #61474.\nTest by running ./gradlew run and then:\nAUTH=\"elastic-admin:elastic-password\"\n\n# Enable new setting\ncurl -u $AUTH -XPUT \\\n  --data '{ \"transient\": { \"cluster.deprecation_indexing.enabled\": true } }' \\\n  http://localhost:9200/_cluster/settings \n\n# Trigger deprecation warning\ncurl -u $AUTH http://localhost:9200/_flush/synced?pretty\n\n# Show indexed deprecation messages\ncurl -u $AUTH http://localhost:9200/logs-deprecation-elasticsearch/_search?pretty", "createdAt": "2020-08-24T15:15:28Z", "url": "https://github.com/elastic/elasticsearch/pull/61484", "merged": true, "mergeCommit": {"oid": "dce2ef9f5a273b2097853b8e49fb0d3dbd17588e"}, "closed": true, "closedAt": "2020-09-03T13:49:57Z", "author": {"login": "pugnascotia"}, "timelineItems": {"totalCount": 15, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdCUZpgAH2gAyNDcyNTkwMjczOjEyNmYwY2FiZjc0NTBhNmYzY2NlN2FjNjVhOTEwNTlkOTE5NDYzMjg=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdL83AJgFqTQ5NTMzMjI5Ng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "126f0cabf7450a6f3cce7ac65a91059d91946328", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/126f0cabf7450a6f3cce7ac65a91059d91946328", "committedDate": "2020-08-25T10:11:12Z", "message": "Allow deprecation logs to be indexed"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9125b651a0da12a5cfe54cba5613e8334b55db01", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/9125b651a0da12a5cfe54cba5613e8334b55db01", "committedDate": "2020-08-24T15:05:56Z", "message": "Allow deprecation logs to be indexed"}, "afterCommit": {"oid": "126f0cabf7450a6f3cce7ac65a91059d91946328", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/126f0cabf7450a6f3cce7ac65a91059d91946328", "committedDate": "2020-08-25T10:11:12Z", "message": "Allow deprecation logs to be indexed"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc3MTA4MTQ5", "url": "https://github.com/elastic/elasticsearch/pull/61484#pullrequestreview-477108149", "createdAt": "2020-08-27T21:48:57Z", "commit": {"oid": "126f0cabf7450a6f3cce7ac65a91059d91946328"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QyMTo0ODo1N1rOHIicMQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QyMTo0ODo1N1rOHIicMQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODcxNDkyOQ==", "bodyText": "add can indeed block, but only when the max concurrent requests are met and when the number of documents or size of docs are are exceeded (as opposed to the timed flush). By default the number of concurrent requests is 1, document count is 1000 and size is 5MB... so if any of these conditions are met by default within the default flush interval of 10s it will block on add.\nRather forking here I would suggest to tweak the bulk processor to avoid blocking.\nThe rationale here is that if the for some reason the index is not writable for a very long period of time, you can end up with a large number of threads from the GENERIC pool. This is further compounded by the fact that some (409) write exceptions are retried by default, leaving the thread alive 5s+ by default.  I understand this is protected by the deprecation throttling..so maybe it is moot point once things are warmed up.\nI 100% agree with the motivation here to ensure that we avoid blocking here, however I think we can better address this by 1) increasing the concurrent requests for the bulk processor (maybe = core count with min 2 ?) 2) disable the bulk processor retry, or configure the retry to be less forgiving (as to consume a concurrent request slot for less time) 3) bump the document count and size default up high so that the flush scheduler thread will be much more likely to be thing that is blocked if anything is blocked. 4) (optionally) add a new configuration that will not try to execute a request if the max concurrent requests are met. eliminating the potential to even block the flush thread.", "url": "https://github.com/elastic/elasticsearch/pull/61484#discussion_r478714929", "createdAt": "2020-08-27T21:48:57Z", "author": {"login": "jakelandis"}, "path": "x-pack/plugin/deprecation/src/main/java/org/elasticsearch/xpack/deprecation/logging/DeprecationIndexingComponent.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.deprecation.logging;\n+\n+import co.elastic.logging.log4j2.EcsLayout;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.core.LoggerContext;\n+import org.apache.logging.log4j.core.config.Configuration;\n+import org.elasticsearch.action.bulk.BulkProcessor;\n+import org.elasticsearch.action.bulk.BulkRequest;\n+import org.elasticsearch.action.bulk.BulkResponse;\n+import org.elasticsearch.action.index.IndexRequest;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.ClusterChangedEvent;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ClusterStateListener;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.logging.ECSJsonLayout;\n+import org.elasticsearch.common.logging.Loggers;\n+import org.elasticsearch.common.logging.RateLimitingFilter;\n+import org.elasticsearch.common.settings.Setting;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.xpack.core.ClientHelper;\n+\n+import java.util.function.Consumer;\n+\n+/**\n+ * This component manages the construction and lifecycle of the {@link DeprecationIndexingAppender}.\n+ * It also starts and stops the appender\n+ */\n+public class DeprecationIndexingComponent extends AbstractLifecycleComponent implements ClusterStateListener {\n+    private static final Logger logger = LogManager.getLogger(DeprecationIndexingComponent.class);\n+\n+    public static final Setting<Boolean> WRITE_DEPRECATION_LOGS_TO_INDEX = Setting.boolSetting(\n+        \"cluster.deprecation_indexing.enabled\",\n+        false,\n+        Setting.Property.NodeScope,\n+        Setting.Property.Dynamic\n+    );\n+\n+    private final DeprecationIndexingAppender appender;\n+    private final BulkProcessor processor;\n+    private final RateLimitingFilter filter;\n+\n+    public DeprecationIndexingComponent(ThreadPool threadPool, Client client) {\n+        this.processor = getBulkProcessor(new OriginSettingClient(client, ClientHelper.DEPRECATION_ORIGIN));\n+        final Consumer<IndexRequest> consumer = buildIndexRequestConsumer(threadPool);\n+\n+        final LoggerContext context = (LoggerContext) LogManager.getContext(false);\n+        final Configuration configuration = context.getConfiguration();\n+\n+        final EcsLayout ecsLayout = ECSJsonLayout.newBuilder().setType(\"deprecation\").setConfiguration(configuration).build();\n+\n+        this.filter = new RateLimitingFilter();\n+        this.appender = new DeprecationIndexingAppender(\"deprecation_indexing_appender\", filter, ecsLayout, consumer);\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        this.appender.start();\n+        Loggers.addAppender(LogManager.getLogger(\"org.elasticsearch.deprecation\"), this.appender);\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        Loggers.removeAppender(LogManager.getLogger(\"org.elasticsearch.deprecation\"), this.appender);\n+        this.appender.stop();\n+    }\n+\n+    @Override\n+    protected void doClose() {\n+        this.processor.close();\n+    }\n+\n+    /**\n+     * Listens for changes to the cluster state, in order to know whether to toggle indexing\n+     * and to set the cluster UUID and node ID. These can't be set in the constructor because\n+     * the initial cluster state won't be set yet.\n+     *\n+     * @param event the cluster state event to process\n+     */\n+    @Override\n+    public void clusterChanged(ClusterChangedEvent event) {\n+        final ClusterState state = event.state();\n+        final boolean newEnabled = WRITE_DEPRECATION_LOGS_TO_INDEX.get(state.getMetadata().settings());\n+        if (appender.isEnabled() != newEnabled) {\n+            // We've flipped from disabled to enabled. Make sure we start with a clean cache of\n+            // previously-seen keys, otherwise we won't index anything.\n+            if (newEnabled) {\n+                this.filter.reset();\n+            }\n+            appender.setEnabled(newEnabled);\n+        }\n+    }\n+\n+    /**\n+     * Constructs a {@link Consumer} that knows what to do with the {@link IndexRequest} instances that the\n+     * {@link DeprecationIndexingAppender} creates. This logic is separated from the service in order to make\n+     * testing significantly easier, and to separate concerns.\n+     * <p>\n+     * Writes are done via {@link BulkProcessor}, which handles batching up writes and retries.\n+     *\n+     * @param threadPool due to <a href=\"https://github.com/elastic/elasticsearch/issues/50440\">#50440</a>,\n+     *                   extra care must be taken to avoid blocking the thread that writes a deprecation message.\n+     * @return           a consumer that accepts an index request and handles all the details of writing it\n+     *                   into the cluster\n+     */\n+    private Consumer<IndexRequest> buildIndexRequestConsumer(ThreadPool threadPool) {\n+        return indexRequest -> {\n+            try {\n+                // TODO: remove the threadpool wrapping when the .add call is non-blocking\n+                // (it can currently execute the bulk request occasionally)\n+                // see: https://github.com/elastic/elasticsearch/issues/50440\n+                threadPool.executor(ThreadPool.Names.GENERIC).execute(() -> this.processor.add(indexRequest));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "126f0cabf7450a6f3cce7ac65a91059d91946328"}, "originalPosition": 121}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c9da4af18bfa96496fe447e66fc730aabff97f2a", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/c9da4af18bfa96496fe447e66fc730aabff97f2a", "committedDate": "2020-09-01T11:12:43Z", "message": "Merge remote-tracking branch 'upstream/master' into 46106-index-deprecation-logs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e8429ab200034bf6b55a6a78909e22668900e7cd", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/e8429ab200034bf6b55a6a78909e22668900e7cd", "committedDate": "2020-09-01T12:10:35Z", "message": "Reconfigure bulk processor after review"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgwMTE1NTMy", "url": "https://github.com/elastic/elasticsearch/pull/61484#pullrequestreview-480115532", "createdAt": "2020-09-01T21:06:26Z", "commit": {"oid": "e8429ab200034bf6b55a6a78909e22668900e7cd"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQyMTowNjoyNlrOHLIPew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQyMTowNjoyNlrOHLIPew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQzMTQxOQ==", "bodyText": "we actually want the bulk actions and bulk size to be large values with a reasonably short flush interval. setting them to -1 will cause the execution to happen one at a time (removing the bulk from the bulk processor) and more likely to happen on the calling thread. setting to large values allows the bulks to build up until the flush takes over and submits the request.\nAlso, even with flush it does block a little bit while mutating the bulk request, but that should be a very fast operation.", "url": "https://github.com/elastic/elasticsearch/pull/61484#discussion_r481431419", "createdAt": "2020-09-01T21:06:26Z", "author": {"login": "jakelandis"}, "path": "x-pack/plugin/deprecation/src/main/java/org/elasticsearch/xpack/deprecation/logging/DeprecationIndexingComponent.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.deprecation.logging;\n+\n+import co.elastic.logging.log4j2.EcsLayout;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.core.LoggerContext;\n+import org.apache.logging.log4j.core.config.Configuration;\n+import org.elasticsearch.action.bulk.BackoffPolicy;\n+import org.elasticsearch.action.bulk.BulkProcessor;\n+import org.elasticsearch.action.bulk.BulkRequest;\n+import org.elasticsearch.action.bulk.BulkResponse;\n+import org.elasticsearch.action.index.IndexRequest;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.ClusterChangedEvent;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ClusterStateListener;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.logging.ECSJsonLayout;\n+import org.elasticsearch.common.logging.Loggers;\n+import org.elasticsearch.common.logging.RateLimitingFilter;\n+import org.elasticsearch.common.settings.Setting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.common.util.concurrent.EsExecutors;\n+import org.elasticsearch.xpack.core.ClientHelper;\n+\n+import java.util.function.Consumer;\n+\n+/**\n+ * This component manages the construction and lifecycle of the {@link DeprecationIndexingAppender}.\n+ * It also starts and stops the appender\n+ */\n+public class DeprecationIndexingComponent extends AbstractLifecycleComponent implements ClusterStateListener {\n+    private static final Logger logger = LogManager.getLogger(DeprecationIndexingComponent.class);\n+\n+    public static final Setting<Boolean> WRITE_DEPRECATION_LOGS_TO_INDEX = Setting.boolSetting(\n+        \"cluster.deprecation_indexing.enabled\",\n+        false,\n+        Setting.Property.NodeScope,\n+        Setting.Property.Dynamic\n+    );\n+\n+    private final DeprecationIndexingAppender appender;\n+    private final BulkProcessor processor;\n+    private final RateLimitingFilter filter;\n+\n+    public DeprecationIndexingComponent(Client client, Settings settings) {\n+        this.processor = getBulkProcessor(new OriginSettingClient(client, ClientHelper.DEPRECATION_ORIGIN), settings);\n+        final Consumer<IndexRequest> consumer = this.processor::add;\n+\n+        final LoggerContext context = (LoggerContext) LogManager.getContext(false);\n+        final Configuration configuration = context.getConfiguration();\n+\n+        final EcsLayout ecsLayout = ECSJsonLayout.newBuilder().setType(\"deprecation\").setConfiguration(configuration).build();\n+\n+        this.filter = new RateLimitingFilter();\n+        this.appender = new DeprecationIndexingAppender(\"deprecation_indexing_appender\", filter, ecsLayout, consumer);\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        this.appender.start();\n+        Loggers.addAppender(LogManager.getLogger(\"org.elasticsearch.deprecation\"), this.appender);\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        Loggers.removeAppender(LogManager.getLogger(\"org.elasticsearch.deprecation\"), this.appender);\n+        this.appender.stop();\n+    }\n+\n+    @Override\n+    protected void doClose() {\n+        this.processor.close();\n+    }\n+\n+    /**\n+     * Listens for changes to the cluster state, in order to know whether to toggle indexing\n+     * and to set the cluster UUID and node ID. These can't be set in the constructor because\n+     * the initial cluster state won't be set yet.\n+     *\n+     * @param event the cluster state event to process\n+     */\n+    @Override\n+    public void clusterChanged(ClusterChangedEvent event) {\n+        final ClusterState state = event.state();\n+        final boolean newEnabled = WRITE_DEPRECATION_LOGS_TO_INDEX.get(state.getMetadata().settings());\n+        if (appender.isEnabled() != newEnabled) {\n+            // We've flipped from disabled to enabled. Make sure we start with a clean cache of\n+            // previously-seen keys, otherwise we won't index anything.\n+            if (newEnabled) {\n+                this.filter.reset();\n+            }\n+            appender.setEnabled(newEnabled);\n+        }\n+    }\n+\n+    /**\n+     * Constructs a bulk processor for writing documents\n+     * @param client the client to use\n+     * @param settings the settings to use\n+     * @return an initialised bulk processor\n+     */\n+    private BulkProcessor getBulkProcessor(Client client, Settings settings) {\n+        final OriginSettingClient originSettingClient = new OriginSettingClient(client, ClientHelper.DEPRECATION_ORIGIN);\n+        final BulkProcessor.Listener listener = new DeprecationBulkListener();\n+\n+        // This configuration disables the size count and size thresholds,\n+        // and instead uses a scheduled flush only. This means that calling\n+        // processor.add() will not block the calling thread.\n+        return BulkProcessor.builder(originSettingClient::bulk, listener)\n+            .setBackoffPolicy(BackoffPolicy.exponentialBackoff(TimeValue.timeValueMillis(1000), 3))\n+            .setConcurrentRequests(Math.max(2, EsExecutors.allocatedProcessors(settings)))\n+            .setBulkActions(-1)\n+            .setBulkSize(new ByteSizeValue(-1, ByteSizeUnit.BYTES))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e8429ab200034bf6b55a6a78909e22668900e7cd"}, "originalPosition": 124}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e3c6c824fbe1b0ab4321a40b4d6dfabfb57a0601", "author": {"user": {"login": "jakelandis", "name": "Jake Landis"}}, "url": "https://github.com/elastic/elasticsearch/commit/e3c6c824fbe1b0ab4321a40b4d6dfabfb57a0601", "committedDate": "2020-09-01T21:27:31Z", "message": "update gradle for new testing plugins"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgwMTQ1NzUw", "url": "https://github.com/elastic/elasticsearch/pull/61484#pullrequestreview-480145750", "createdAt": "2020-09-01T21:57:06Z", "commit": {"oid": "e8429ab200034bf6b55a6a78909e22668900e7cd"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQyMTo1NzowNlrOHLJrCw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQyMTo1NzowNlrOHLJrCw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ1NDg1OQ==", "bodyText": "this is going to match the default logs-- template which defines the following mappings:\nhttps://github.com/elastic/elasticsearch/blob/master/x-pack/plugin/core/src/main/resources/logs-mappings.json\nWe should make sure that we populate the predefined mappings for this template. (datastream, ecs, host, etc.)", "url": "https://github.com/elastic/elasticsearch/pull/61484#discussion_r481454859", "createdAt": "2020-09-01T21:57:06Z", "author": {"login": "jakelandis"}, "path": "x-pack/plugin/deprecation/src/main/java/org/elasticsearch/xpack/deprecation/logging/DeprecationIndexingAppender.java", "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.deprecation.logging;\n+\n+import org.apache.logging.log4j.core.Appender;\n+import org.apache.logging.log4j.core.Core;\n+import org.apache.logging.log4j.core.Filter;\n+import org.apache.logging.log4j.core.Layout;\n+import org.apache.logging.log4j.core.LogEvent;\n+import org.apache.logging.log4j.core.appender.AbstractAppender;\n+import org.apache.logging.log4j.core.config.plugins.Plugin;\n+import org.elasticsearch.action.DocWriteRequest;\n+import org.elasticsearch.action.index.IndexRequest;\n+import org.elasticsearch.common.xcontent.XContentType;\n+\n+import java.util.Objects;\n+import java.util.function.Consumer;\n+\n+/**\n+ * This log4j appender writes deprecation log messages to an index. It does not perform the actual\n+ * writes, but instead constructs an {@link IndexRequest} for the log message and passes that\n+ * to a callback.\n+ */\n+@Plugin(name = \"DeprecationIndexingAppender\", category = Core.CATEGORY_NAME, elementType = Appender.ELEMENT_TYPE)\n+public class DeprecationIndexingAppender extends AbstractAppender {\n+    public static final String DEPRECATION_MESSAGES_DATA_STREAM = \"logs-deprecation-elasticsearch\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e8429ab200034bf6b55a6a78909e22668900e7cd"}, "originalPosition": 30}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgwMTQ4MDE4", "url": "https://github.com/elastic/elasticsearch/pull/61484#pullrequestreview-480148018", "createdAt": "2020-09-01T22:01:41Z", "commit": {"oid": "e8429ab200034bf6b55a6a78909e22668900e7cd"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "035bd6e92f879b1c0d4d3b0854022846ef2d7994", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/035bd6e92f879b1c0d4d3b0854022846ef2d7994", "committedDate": "2020-09-02T12:21:40Z", "message": "Merge pull request #3 from jakelandis/46106-index-deprecation-logs-jake\n\nupdate gradle config for new testing plugins"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgwOTI4NTQ4", "url": "https://github.com/elastic/elasticsearch/pull/61484#pullrequestreview-480928548", "createdAt": "2020-09-02T15:14:37Z", "commit": {"oid": "035bd6e92f879b1c0d4d3b0854022846ef2d7994"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e4ad3893efca5cd50c1944d376eec1a9c380b828", "author": {"user": {"login": "pugnascotia", "name": "Rory Hunter"}}, "url": "https://github.com/elastic/elasticsearch/commit/e4ad3893efca5cd50c1944d376eec1a9c380b828", "committedDate": "2020-09-02T16:01:34Z", "message": "Include more fields in DeprecatedMessage"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgwOTczNjg4", "url": "https://github.com/elastic/elasticsearch/pull/61484#pullrequestreview-480973688", "createdAt": "2020-09-02T16:02:33Z", "commit": {"oid": "e4ad3893efca5cd50c1944d376eec1a9c380b828"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxNjowMjozM1rOHL2W4w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxNjowMjozM1rOHL2W4w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE4Njk3OQ==", "bodyText": "@jakelandis how do these look to you?", "url": "https://github.com/elastic/elasticsearch/pull/61484#discussion_r482186979", "createdAt": "2020-09-02T16:02:33Z", "author": {"login": "pugnascotia"}, "path": "server/src/main/java/org/elasticsearch/common/logging/DeprecatedMessage.java", "diffHunk": "@@ -40,10 +41,14 @@ public static ESLogMessage of(String key, String xOpaqueId, String messagePatter\n             @Override\n             public String toString() {\n                 return ParameterizedMessage.format(messagePattern, args);\n-\n             }\n         };\n+\n         return new ESLogMessage(messagePattern, args)\n+            .field(\"data_stream.type\", \"logs\")\n+            .field(\"data_stream.datatype\", \"deprecation\")\n+            .field(\"data_stream.namespace\", \"elasticsearch\")\n+            .field(\"ecs.version\", ECS_VERSION)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e4ad3893efca5cd50c1944d376eec1a9c380b828"}, "originalPosition": 20}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgxNTg3MzIx", "url": "https://github.com/elastic/elasticsearch/pull/61484#pullrequestreview-481587321", "createdAt": "2020-09-03T07:56:41Z", "commit": {"oid": "e4ad3893efca5cd50c1944d376eec1a9c380b828"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk1MzMyMjk2", "url": "https://github.com/elastic/elasticsearch/pull/61484#pullrequestreview-495332296", "createdAt": "2020-09-24T08:24:47Z", "commit": {"oid": "e4ad3893efca5cd50c1944d376eec1a9c380b828"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwODoyNDo0N1rOHXPUmQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwODoyNDo0N1rOHXPUmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDEzMDMyOQ==", "bodyText": "@pugnascotia This should be data_stream.dataset to be aligned with the indexing strategy.\nI would also propose to keep the namespace as default and use deprecation.elasticsearch as the dataset name. Only important thing is that the dataset does not contain a  -.", "url": "https://github.com/elastic/elasticsearch/pull/61484#discussion_r494130329", "createdAt": "2020-09-24T08:24:47Z", "author": {"login": "ruflin"}, "path": "server/src/main/java/org/elasticsearch/common/logging/DeprecatedMessage.java", "diffHunk": "@@ -40,10 +41,14 @@ public static ESLogMessage of(String key, String xOpaqueId, String messagePatter\n             @Override\n             public String toString() {\n                 return ParameterizedMessage.format(messagePattern, args);\n-\n             }\n         };\n+\n         return new ESLogMessage(messagePattern, args)\n+            .field(\"data_stream.type\", \"logs\")\n+            .field(\"data_stream.datatype\", \"deprecation\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e4ad3893efca5cd50c1944d376eec1a9c380b828"}, "originalPosition": 18}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4687, "cost": 1, "resetAt": "2021-10-28T18:54:27Z"}}}