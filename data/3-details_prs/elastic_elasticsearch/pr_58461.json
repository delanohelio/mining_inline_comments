{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM4ODY0NDUw", "number": 58461, "title": "Executes incremental reduce in the search thread pool", "bodyText": "This change forks the execution of partial reduces in the coordinating node to the search thread pool.\nIt also ensures that partial reduces are executed sequentially and asynchronously in order to limit the\nmemory and cpu that a single search request can use but also to avoid blocking a network thread.\nIf a partial reduce fails with an exception, the search request is cancelled and the reporting of the error is\ndelayed to the start of the fetch phase (when the final reduce is performed). This ensures that we cleanup the\nin-flight search requests before returning an error to the user.\nCloses #53411\nRelates #51857", "createdAt": "2020-06-23T22:51:53Z", "url": "https://github.com/elastic/elasticsearch/pull/58461", "merged": true, "mergeCommit": {"oid": "13e20046625e4897dfdc5c363e493d40b776fadd"}, "closed": true, "closedAt": "2020-07-28T09:35:37Z", "author": {"login": "jimczi"}, "timelineItems": {"totalCount": 22, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcuNgJwgH2gAyNDM4ODY0NDUwOjZhNGVjMGZmNjQyZTU5ZGYzY2IxZmM5MmVkYTRlODg5OTkzOWYzMTc=", "endCursor": "Y3Vyc29yOnYyOpPPAAABc5Rwx_AH2gAyNDM4ODY0NDUwOmUzMzdlZTdiZjc1YmFkZWYxYjJjMDZmMjk2OTc2YWRjMDI1OGY1ZDM=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "6a4ec0ff642e59df3cb1fc92eda4e8899939f317", "author": {"user": {"login": "jimczi", "name": "Jim Ferenczi"}}, "url": "https://github.com/elastic/elasticsearch/commit/6a4ec0ff642e59df3cb1fc92eda4e8899939f317", "committedDate": "2020-06-23T22:50:29Z", "message": "Executes incremental reduce in the search thread pool\n\nThis change forks the execution of partial\nreduces in the coordinating node to the search thread pool.\nIt also ensures that partial reduces are executed sequentially\nand asynchronously in order to limit the memory and cpu that a\nsingle search request can use but also to avoid blocking a\nnetwork thread.\nIf a partial reduce fails with an exception, the search\nrequest is cancelled and the reporting of the error is\ndelayed to the start of the fetch phase (when the final\nreduce is performed). This ensures that we cleanup the\nin-flight search requests before returning an error to\nthe user.\n\nCloses #53411\nRelates #51857"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "baa10e1deebbf6dd9252bb9bfeaf69b01fd9d782", "author": {"user": {"login": "jimczi", "name": "Jim Ferenczi"}}, "url": "https://github.com/elastic/elasticsearch/commit/baa10e1deebbf6dd9252bb9bfeaf69b01fd9d782", "committedDate": "2020-06-23T23:17:37Z", "message": "take into  account the last reduced result when computing the buffer size"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ee22e3ff6c8d5dd22cace2a4915f9dce9a9c5c01", "author": {"user": {"login": "jimczi", "name": "Jim Ferenczi"}}, "url": "https://github.com/elastic/elasticsearch/commit/ee22e3ff6c8d5dd22cace2a4915f9dce9a9c5c01", "committedDate": "2020-06-23T23:40:25Z", "message": "use a consistent ordering on final reduce"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM3MjkzNjY2", "url": "https://github.com/elastic/elasticsearch/pull/58461#pullrequestreview-437293666", "createdAt": "2020-06-25T08:49:59Z", "commit": {"oid": "ee22e3ff6c8d5dd22cace2a4915f9dce9a9c5c01"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQwODo0OTo1OVrOGoxaAA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQwODo1MzowMFrOGoxhRQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQwNTY5Ng==", "bodyText": "Was this class only extracted from SearchPhaseController or what changes have been made to it? Maybe for easier reviewing it could have stayed where it was, at least initially? Would that be possible?", "url": "https://github.com/elastic/elasticsearch/pull/58461#discussion_r445405696", "createdAt": "2020-06-25T08:49:59Z", "author": {"login": "javanna"}, "path": "server/src/main/java/org/elasticsearch/action/search/QueryPhaseResultConsumer.java", "diffHunk": "@@ -0,0 +1,343 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.search;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.lucene.search.TopDocs;\n+import org.elasticsearch.action.search.SearchPhaseController.TopDocsStats;\n+import org.elasticsearch.common.io.stream.DelayableWriteable;\n+import org.elasticsearch.common.io.stream.DelayableWriteable.Serialized;\n+import org.elasticsearch.common.io.stream.NamedWriteableRegistry;\n+import org.elasticsearch.common.lucene.search.TopDocsAndMaxScore;\n+import org.elasticsearch.common.util.concurrent.AbstractRunnable;\n+import org.elasticsearch.search.SearchPhaseResult;\n+import org.elasticsearch.search.SearchShardTarget;\n+import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContextBuilder;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.query.QuerySearchResult;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Consumer;\n+\n+import static org.elasticsearch.action.search.SearchPhaseController.mergeTopDocs;\n+import static org.elasticsearch.action.search.SearchPhaseController.setShardIndex;\n+\n+/**\n+ * A {@link ArraySearchPhaseResults} implementation that incrementally reduces aggregation results\n+ * as shard results are consumed.\n+ * This implementation can be configured to batch up a certain amount of results and reduce\n+ * them asynchronously in the provided {@link Executor} iff the buffer is exhausted.\n+ */\n+class QueryPhaseResultConsumer extends ArraySearchPhaseResults<SearchPhaseResult> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee22e3ff6c8d5dd22cace2a4915f9dce9a9c5c01"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQwNjkzNw==", "bodyText": "Maybe add some prefix to the message? I am always afraid that the exc message may be null or not so understandable, with a prefix we would know for sure where the cancel comes from.", "url": "https://github.com/elastic/elasticsearch/pull/58461#discussion_r445406937", "createdAt": "2020-06-25T08:51:57Z", "author": {"login": "javanna"}, "path": "server/src/main/java/org/elasticsearch/action/search/TransportSearchAction.java", "diffHunk": "@@ -634,6 +641,14 @@ public void run() {\n         }\n     }\n \n+    private void cancelTask(SearchTask task, Exception exc) {\n+        CancelTasksRequest req = new CancelTasksRequest()\n+            .setTaskId(new TaskId(client.getLocalNodeId(), task.getId()))\n+            .setReason(exc.getMessage());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee22e3ff6c8d5dd22cace2a4915f9dce9a9c5c01"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQwNzU1Nw==", "bodyText": "why do we need to do this compared to before?", "url": "https://github.com/elastic/elasticsearch/pull/58461#discussion_r445407557", "createdAt": "2020-06-25T08:53:00Z", "author": {"login": "javanna"}, "path": "server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java", "diffHunk": "@@ -138,6 +140,28 @@ public void execute(SearchContext context) {\n                 fieldsVisitor = new CustomFieldsVisitor(storedToRequestedFields.keySet(), loadSource);\n             }\n         }\n+        int[] docIds = new int[context.docIdsToLoadSize()];\n+        int[] sortedDocIds = new int[context.docIdsToLoadSize()];\n+        int[] indices = new int[context.docIdsToLoadSize()];\n+        Sorter sorter = new IntroSorter() {\n+            @Override\n+            protected void swap(int i, int j) {\n+                int left = docIds[i];\n+                docIds[i] = docIds[j];\n+                docIds[j] = left;\n+            }\n+\n+            @Override\n+            protected void setPivot(int i) {\n+\n+            }\n+\n+            @Override\n+            protected int comparePivot(int j) {\n+                return 0;\n+            }\n+        };", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee22e3ff6c8d5dd22cace2a4915f9dce9a9c5c01"}, "originalPosition": 33}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cbd97762d04acf411eb8831b9774a8ebba701a55", "author": {"user": {"login": "jimczi", "name": "Jim Ferenczi"}}, "url": "https://github.com/elastic/elasticsearch/commit/cbd97762d04acf411eb8831b9774a8ebba701a55", "committedDate": "2020-06-25T10:14:09Z", "message": "remove unrelated change"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "74850ee738ba34d43302af424a6900c4cdb92858", "author": {"user": {"login": "jimczi", "name": "Jim Ferenczi"}}, "url": "https://github.com/elastic/elasticsearch/commit/74850ee738ba34d43302af424a6900c4cdb92858", "committedDate": "2020-07-02T20:00:06Z", "message": "Merge branch 'master' into partial_reduce_thread_pool"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b381f3b420ee6f99eae54545f68cdcb57ee62fc8", "author": {"user": {"login": "jimczi", "name": "Jim Ferenczi"}}, "url": "https://github.com/elastic/elasticsearch/commit/b381f3b420ee6f99eae54545f68cdcb57ee62fc8", "committedDate": "2020-07-02T20:38:19Z", "message": "fix test compliation"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c24f38eb3a97cacdafe572901ddb7a8c7f5937eb", "author": {"user": {"login": "jimczi", "name": "Jim Ferenczi"}}, "url": "https://github.com/elastic/elasticsearch/commit/c24f38eb3a97cacdafe572901ddb7a8c7f5937eb", "committedDate": "2020-07-03T12:58:31Z", "message": "Merge branch 'master' into partial_reduce_thread_pool"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8cb330b4130d3d86da19d7a8ad24fcfdf47e0163", "author": {"user": {"login": "jimczi", "name": "Jim Ferenczi"}}, "url": "https://github.com/elastic/elasticsearch/commit/8cb330b4130d3d86da19d7a8ad24fcfdf47e0163", "committedDate": "2020-07-03T13:14:14Z", "message": "fix another test"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ3NDA0MTMy", "url": "https://github.com/elastic/elasticsearch/pull/58461#pullrequestreview-447404132", "createdAt": "2020-07-13T16:21:01Z", "commit": {"oid": "8cb330b4130d3d86da19d7a8ad24fcfdf47e0163"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxNjoyMTowMVrOGwv-Ag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxNjo0MToyOFrOGwwxAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc3MDc1NA==", "bodyText": "s/shouldExecuteImmediatly/executeNextImmediately/?", "url": "https://github.com/elastic/elasticsearch/pull/58461#discussion_r453770754", "createdAt": "2020-07-13T16:21:01Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/action/search/QueryPhaseResultConsumer.java", "diffHunk": "@@ -0,0 +1,343 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.search;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.lucene.search.TopDocs;\n+import org.elasticsearch.action.search.SearchPhaseController.TopDocsStats;\n+import org.elasticsearch.common.io.stream.DelayableWriteable;\n+import org.elasticsearch.common.io.stream.DelayableWriteable.Serialized;\n+import org.elasticsearch.common.io.stream.NamedWriteableRegistry;\n+import org.elasticsearch.common.lucene.search.TopDocsAndMaxScore;\n+import org.elasticsearch.common.util.concurrent.AbstractRunnable;\n+import org.elasticsearch.search.SearchPhaseResult;\n+import org.elasticsearch.search.SearchShardTarget;\n+import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContextBuilder;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.query.QuerySearchResult;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Consumer;\n+\n+import static org.elasticsearch.action.search.SearchPhaseController.mergeTopDocs;\n+import static org.elasticsearch.action.search.SearchPhaseController.setShardIndex;\n+\n+/**\n+ * A {@link ArraySearchPhaseResults} implementation that incrementally reduces aggregation results\n+ * as shard results are consumed.\n+ * This implementation can be configured to batch up a certain amount of results and reduce\n+ * them asynchronously in the provided {@link Executor} iff the buffer is exhausted.\n+ */\n+class QueryPhaseResultConsumer extends ArraySearchPhaseResults<SearchPhaseResult> {\n+    private static final Logger logger = LogManager.getLogger(SearchPhaseController.class);\n+\n+    private final Executor executor;\n+    private final SearchPhaseController controller;\n+    private final SearchProgressListener progressListener;\n+    private final ReduceContextBuilder aggReduceContextBuilder;\n+    private final NamedWriteableRegistry namedWriteableRegistry;\n+\n+    private final int topNSize;\n+    private final int bufferSize;\n+    private final boolean hasTopDocs;\n+    private final boolean hasAggs;\n+    private final boolean performFinalReduce;\n+\n+    private final TopDocsStats topDocsStats;\n+    private volatile int numReducePhases;\n+    private List<SearchShard> processedShards = new ArrayList<>();\n+    private boolean hasPartialReduce;\n+    private volatile TopDocs reducedTopDocs;\n+    private volatile Serialized<InternalAggregations> reducedAggs;\n+\n+    private final List<QuerySearchResult> buffer = new ArrayList<>();\n+    private final ArrayDeque<MergeTask> queue = new ArrayDeque<>();\n+    private final AtomicReference<MergeTask> runningTask = new AtomicReference<>();\n+    private final AtomicReference<Exception> fatalFailure = new AtomicReference<>();\n+\n+    private final Consumer<Exception> onPartialMergeFailure;\n+\n+    private volatile long aggsMaxBufferSize;\n+    private volatile long aggsCurrentBufferSize;\n+\n+    /**\n+     * Creates a {@link QueryPhaseResultConsumer} that incrementally reduces aggregation results\n+     * as shard results are consumed.\n+     */\n+    QueryPhaseResultConsumer(Executor executor,\n+                             SearchPhaseController controller,\n+                             SearchProgressListener progressListener,\n+                             ReduceContextBuilder aggReduceContextBuilder,\n+                             NamedWriteableRegistry namedWriteableRegistry,\n+                             int expectedResultSize,\n+                             int bufferSize,\n+                             boolean hasTopDocs,\n+                             boolean hasAggs,\n+                             int trackTotalHitsUpTo,\n+                             int topNSize,\n+                             boolean performFinalReduce,\n+                             Consumer<Exception> onPartialMergeFailure) {\n+        super(expectedResultSize);\n+        this.executor = executor;\n+        this.controller = controller;\n+        this.progressListener = progressListener;\n+        this.aggReduceContextBuilder = aggReduceContextBuilder;\n+        this.namedWriteableRegistry = namedWriteableRegistry;\n+        this.topNSize = topNSize;\n+        this.bufferSize = bufferSize;\n+        this.hasTopDocs = hasTopDocs;\n+        this.hasAggs = hasAggs;\n+        this.performFinalReduce = performFinalReduce;\n+        this.topDocsStats = new TopDocsStats(trackTotalHitsUpTo);\n+        this.onPartialMergeFailure = onPartialMergeFailure;\n+    }\n+\n+    public int getNumReducePhases() {\n+        return numReducePhases;\n+    }\n+\n+    @Override\n+    void consumeResult(SearchPhaseResult result, Runnable next) {\n+        super.consumeResult(result, () -> {});\n+        QuerySearchResult querySearchResult = result.queryResult();\n+        consumeInternal(querySearchResult, next);\n+        progressListener.notifyQueryResult(querySearchResult.getShardIndex());\n+    }\n+\n+    private void consumeInternal(QuerySearchResult result, Runnable next) {\n+        boolean shouldExecuteImmediatly = true;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8cb330b4130d3d86da19d7a8ad24fcfdf47e0163"}, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc3MTU2Mg==", "bodyText": "I think it is worth a comment about why you can't use reducedAggs != null. I believe the reason for that is that you want to set this to true before the reduced aggs are ready.", "url": "https://github.com/elastic/elasticsearch/pull/58461#discussion_r453771562", "createdAt": "2020-07-13T16:22:22Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/action/search/QueryPhaseResultConsumer.java", "diffHunk": "@@ -0,0 +1,343 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.search;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.lucene.search.TopDocs;\n+import org.elasticsearch.action.search.SearchPhaseController.TopDocsStats;\n+import org.elasticsearch.common.io.stream.DelayableWriteable;\n+import org.elasticsearch.common.io.stream.DelayableWriteable.Serialized;\n+import org.elasticsearch.common.io.stream.NamedWriteableRegistry;\n+import org.elasticsearch.common.lucene.search.TopDocsAndMaxScore;\n+import org.elasticsearch.common.util.concurrent.AbstractRunnable;\n+import org.elasticsearch.search.SearchPhaseResult;\n+import org.elasticsearch.search.SearchShardTarget;\n+import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContextBuilder;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.query.QuerySearchResult;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Consumer;\n+\n+import static org.elasticsearch.action.search.SearchPhaseController.mergeTopDocs;\n+import static org.elasticsearch.action.search.SearchPhaseController.setShardIndex;\n+\n+/**\n+ * A {@link ArraySearchPhaseResults} implementation that incrementally reduces aggregation results\n+ * as shard results are consumed.\n+ * This implementation can be configured to batch up a certain amount of results and reduce\n+ * them asynchronously in the provided {@link Executor} iff the buffer is exhausted.\n+ */\n+class QueryPhaseResultConsumer extends ArraySearchPhaseResults<SearchPhaseResult> {\n+    private static final Logger logger = LogManager.getLogger(SearchPhaseController.class);\n+\n+    private final Executor executor;\n+    private final SearchPhaseController controller;\n+    private final SearchProgressListener progressListener;\n+    private final ReduceContextBuilder aggReduceContextBuilder;\n+    private final NamedWriteableRegistry namedWriteableRegistry;\n+\n+    private final int topNSize;\n+    private final int bufferSize;\n+    private final boolean hasTopDocs;\n+    private final boolean hasAggs;\n+    private final boolean performFinalReduce;\n+\n+    private final TopDocsStats topDocsStats;\n+    private volatile int numReducePhases;\n+    private List<SearchShard> processedShards = new ArrayList<>();\n+    private boolean hasPartialReduce;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8cb330b4130d3d86da19d7a8ad24fcfdf47e0163"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc3NjcxMg==", "bodyText": "Do you see the task being consumed twice? It is probably worth a comment explaining why.", "url": "https://github.com/elastic/elasticsearch/pull/58461#discussion_r453776712", "createdAt": "2020-07-13T16:30:10Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/action/search/QueryPhaseResultConsumer.java", "diffHunk": "@@ -0,0 +1,343 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.search;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.lucene.search.TopDocs;\n+import org.elasticsearch.action.search.SearchPhaseController.TopDocsStats;\n+import org.elasticsearch.common.io.stream.DelayableWriteable;\n+import org.elasticsearch.common.io.stream.DelayableWriteable.Serialized;\n+import org.elasticsearch.common.io.stream.NamedWriteableRegistry;\n+import org.elasticsearch.common.lucene.search.TopDocsAndMaxScore;\n+import org.elasticsearch.common.util.concurrent.AbstractRunnable;\n+import org.elasticsearch.search.SearchPhaseResult;\n+import org.elasticsearch.search.SearchShardTarget;\n+import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContextBuilder;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.query.QuerySearchResult;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Consumer;\n+\n+import static org.elasticsearch.action.search.SearchPhaseController.mergeTopDocs;\n+import static org.elasticsearch.action.search.SearchPhaseController.setShardIndex;\n+\n+/**\n+ * A {@link ArraySearchPhaseResults} implementation that incrementally reduces aggregation results\n+ * as shard results are consumed.\n+ * This implementation can be configured to batch up a certain amount of results and reduce\n+ * them asynchronously in the provided {@link Executor} iff the buffer is exhausted.\n+ */\n+class QueryPhaseResultConsumer extends ArraySearchPhaseResults<SearchPhaseResult> {\n+    private static final Logger logger = LogManager.getLogger(SearchPhaseController.class);\n+\n+    private final Executor executor;\n+    private final SearchPhaseController controller;\n+    private final SearchProgressListener progressListener;\n+    private final ReduceContextBuilder aggReduceContextBuilder;\n+    private final NamedWriteableRegistry namedWriteableRegistry;\n+\n+    private final int topNSize;\n+    private final int bufferSize;\n+    private final boolean hasTopDocs;\n+    private final boolean hasAggs;\n+    private final boolean performFinalReduce;\n+\n+    private final TopDocsStats topDocsStats;\n+    private volatile int numReducePhases;\n+    private List<SearchShard> processedShards = new ArrayList<>();\n+    private boolean hasPartialReduce;\n+    private volatile TopDocs reducedTopDocs;\n+    private volatile Serialized<InternalAggregations> reducedAggs;\n+\n+    private final List<QuerySearchResult> buffer = new ArrayList<>();\n+    private final ArrayDeque<MergeTask> queue = new ArrayDeque<>();\n+    private final AtomicReference<MergeTask> runningTask = new AtomicReference<>();\n+    private final AtomicReference<Exception> fatalFailure = new AtomicReference<>();\n+\n+    private final Consumer<Exception> onPartialMergeFailure;\n+\n+    private volatile long aggsMaxBufferSize;\n+    private volatile long aggsCurrentBufferSize;\n+\n+    /**\n+     * Creates a {@link QueryPhaseResultConsumer} that incrementally reduces aggregation results\n+     * as shard results are consumed.\n+     */\n+    QueryPhaseResultConsumer(Executor executor,\n+                             SearchPhaseController controller,\n+                             SearchProgressListener progressListener,\n+                             ReduceContextBuilder aggReduceContextBuilder,\n+                             NamedWriteableRegistry namedWriteableRegistry,\n+                             int expectedResultSize,\n+                             int bufferSize,\n+                             boolean hasTopDocs,\n+                             boolean hasAggs,\n+                             int trackTotalHitsUpTo,\n+                             int topNSize,\n+                             boolean performFinalReduce,\n+                             Consumer<Exception> onPartialMergeFailure) {\n+        super(expectedResultSize);\n+        this.executor = executor;\n+        this.controller = controller;\n+        this.progressListener = progressListener;\n+        this.aggReduceContextBuilder = aggReduceContextBuilder;\n+        this.namedWriteableRegistry = namedWriteableRegistry;\n+        this.topNSize = topNSize;\n+        this.bufferSize = bufferSize;\n+        this.hasTopDocs = hasTopDocs;\n+        this.hasAggs = hasAggs;\n+        this.performFinalReduce = performFinalReduce;\n+        this.topDocsStats = new TopDocsStats(trackTotalHitsUpTo);\n+        this.onPartialMergeFailure = onPartialMergeFailure;\n+    }\n+\n+    public int getNumReducePhases() {\n+        return numReducePhases;\n+    }\n+\n+    @Override\n+    void consumeResult(SearchPhaseResult result, Runnable next) {\n+        super.consumeResult(result, () -> {});\n+        QuerySearchResult querySearchResult = result.queryResult();\n+        consumeInternal(querySearchResult, next);\n+        progressListener.notifyQueryResult(querySearchResult.getShardIndex());\n+    }\n+\n+    private void consumeInternal(QuerySearchResult result, Runnable next) {\n+        boolean shouldExecuteImmediatly = true;\n+        synchronized (this) {\n+            if (hasFailure() || result.isNull()) {\n+                SearchShardTarget target = result.getSearchShardTarget();\n+                processedShards.add(new SearchShard(target.getClusterAlias(), target.getShardId()));\n+                result.consumeAll();\n+            } else {\n+                int size = buffer.size() + (hasPartialReduce ? 1 : 0);\n+                if (size == bufferSize) {\n+                    hasPartialReduce = true;\n+                    shouldExecuteImmediatly = false;\n+                    MergeTask task = new MergeTask(buffer, next);\n+                    buffer.clear();\n+                    tryExecute(task);\n+                }\n+                buffer.add(result);\n+            }\n+        }\n+        if (shouldExecuteImmediatly) {\n+            next.run();\n+        }\n+    }\n+\n+    @Override\n+    SearchPhaseController.ReducedQueryPhase reduce() throws Exception {\n+        if (hasPendingMerges()) {\n+            throw new AssertionError(\"partial reduce in-flight\");\n+        } else if (hasFailure()) {\n+            throw fatalFailure.get();\n+        }\n+\n+        logger.trace(\"aggs final reduction [{}] max [{}]\", aggsCurrentBufferSize, aggsMaxBufferSize);\n+        Collections.sort(buffer, Comparator.comparingInt(QuerySearchResult::getShardIndex));\n+        SearchPhaseController.ReducedQueryPhase reducePhase = controller.reducedQueryPhase(results.asList(), consumeAggs(buffer),\n+            consumeTopDocs(buffer), topDocsStats, numReducePhases, false, aggReduceContextBuilder, performFinalReduce);\n+        progressListener.notifyFinalReduce(SearchProgressListener.buildSearchShards(results.asList()),\n+            reducePhase.totalHits, reducePhase.aggregations, reducePhase.numReducePhases);\n+        return reducePhase;\n+    }\n+\n+\n+    private void partialReduce(List<QuerySearchResult> toConsume) {\n+        final List<TopDocs> topDocsToConsume;\n+        final List<InternalAggregations> aggsToConsume;\n+        synchronized (this) {\n+            if (hasFailure()) {\n+                return;\n+            }\n+            topDocsToConsume = consumeTopDocs(toConsume);\n+            aggsToConsume = consumeAggs(toConsume);\n+\n+        }\n+        final TopDocs newTopDocs;\n+        if (hasTopDocs) {\n+            newTopDocs = mergeTopDocs(topDocsToConsume,\n+                // we have to merge here in the same way we collect on a shard\n+                topNSize, 0);\n+        } else {\n+            newTopDocs = null;\n+        }\n+\n+        final Serialized<InternalAggregations> newAggs;\n+        if (hasAggs) {\n+            InternalAggregations result = InternalAggregations.topLevelReduce(aggsToConsume,\n+                aggReduceContextBuilder.forPartialReduction());\n+            newAggs = DelayableWriteable.referencing(result).asSerialized(InternalAggregations::readFrom, namedWriteableRegistry);\n+            long previousBufferSize = aggsCurrentBufferSize;\n+            aggsCurrentBufferSize = newAggs.ramBytesUsed();\n+            aggsMaxBufferSize = Math.max(aggsCurrentBufferSize, aggsMaxBufferSize);\n+            logger.trace(\"aggs partial reduction [{}->{}] max [{}]\",\n+                previousBufferSize, aggsCurrentBufferSize, aggsMaxBufferSize);\n+        } else {\n+            newAggs = null;\n+        }\n+        synchronized (this) {\n+            if (hasFailure())  {\n+                return;\n+            }\n+            reducedTopDocs = newTopDocs;\n+            reducedAggs = newAggs;\n+            ++ numReducePhases;\n+            for (QuerySearchResult result : toConsume) {\n+                SearchShardTarget target = result.getSearchShardTarget();\n+                processedShards.add(new SearchShard(target.getClusterAlias(), target.getShardId()));\n+            }\n+            progressListener.onPartialReduce(processedShards, topDocsStats.getTotalHits(), reducedAggs, numReducePhases);\n+        }\n+    }\n+\n+    private List<InternalAggregations> consumeAggs(List<QuerySearchResult> results) {\n+        if (hasAggs) {\n+            List<InternalAggregations> aggsList = new ArrayList<>();\n+            if (reducedAggs != null) {\n+                aggsList.add(reducedAggs.expand());\n+                reducedAggs = null;\n+            }\n+            for (QuerySearchResult result : results) {\n+                aggsList.add(result.consumeAggs().expand());\n+            }\n+            return aggsList;\n+        } else {\n+            return null;\n+        }\n+    }\n+\n+    private List<TopDocs> consumeTopDocs(List<QuerySearchResult> results) {\n+        if (hasTopDocs) {\n+            List<TopDocs> topDocsList = new ArrayList<>();\n+            if (reducedTopDocs != null)  {\n+                topDocsList.add(reducedTopDocs);\n+            }\n+            for (QuerySearchResult result : results) {\n+                TopDocsAndMaxScore topDocs = result.consumeTopDocs();\n+                topDocsStats.add(topDocs, result.searchTimedOut(), result.terminatedEarly());\n+                setShardIndex(topDocs.topDocs, result.getShardIndex());\n+                topDocsList.add(topDocs.topDocs);\n+            }\n+            return topDocsList;\n+        } else {\n+            return null;\n+        }\n+    }\n+\n+    private synchronized void onFatalFailure(Exception exc) {\n+        onPartialMergeFailure.accept(exc);\n+        fatalFailure.compareAndSet(null, exc);\n+        MergeTask task = runningTask.get();\n+        if (task != null) {\n+            task.cancel();\n+        }\n+        runningTask.compareAndSet(task, null);\n+        queue.stream().forEach(MergeTask::cancel);\n+        queue.clear();\n+        reducedTopDocs = null;\n+        reducedAggs = null;\n+    }\n+\n+    private boolean hasFailure() {\n+        return fatalFailure.get() != null;\n+    }\n+\n+    synchronized boolean hasPendingMerges() {\n+        return queue.isEmpty() == false || runningTask.get() != null;\n+    }\n+\n+    synchronized void tryExecute(MergeTask task) {\n+        queue.add(task);\n+        tryExecuteNext();\n+    }\n+\n+    private void tryExecuteNext() {\n+        final MergeTask task;\n+        synchronized (this) {\n+            if (queue.isEmpty()\n+                    || runningTask.get() != null\n+                    || hasFailure()) {\n+                return;\n+            }\n+            runningTask.compareAndSet(null, queue.peek());\n+            task = queue.poll();\n+        }\n+        executor.execute(new AbstractRunnable() {\n+            @Override\n+            protected void doRun() throws Exception {\n+                List<QuerySearchResult> toConsume = task.consumeBuffer();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8cb330b4130d3d86da19d7a8ad24fcfdf47e0163"}, "originalPosition": 296}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc4MzgwOQ==", "bodyText": "Yeah. This looks so much bigger than before and I don't think it would have made review a ton easier to keep it where it was.", "url": "https://github.com/elastic/elasticsearch/pull/58461#discussion_r453783809", "createdAt": "2020-07-13T16:41:28Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/action/search/QueryPhaseResultConsumer.java", "diffHunk": "@@ -0,0 +1,343 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.search;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.lucene.search.TopDocs;\n+import org.elasticsearch.action.search.SearchPhaseController.TopDocsStats;\n+import org.elasticsearch.common.io.stream.DelayableWriteable;\n+import org.elasticsearch.common.io.stream.DelayableWriteable.Serialized;\n+import org.elasticsearch.common.io.stream.NamedWriteableRegistry;\n+import org.elasticsearch.common.lucene.search.TopDocsAndMaxScore;\n+import org.elasticsearch.common.util.concurrent.AbstractRunnable;\n+import org.elasticsearch.search.SearchPhaseResult;\n+import org.elasticsearch.search.SearchShardTarget;\n+import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContextBuilder;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.query.QuerySearchResult;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Consumer;\n+\n+import static org.elasticsearch.action.search.SearchPhaseController.mergeTopDocs;\n+import static org.elasticsearch.action.search.SearchPhaseController.setShardIndex;\n+\n+/**\n+ * A {@link ArraySearchPhaseResults} implementation that incrementally reduces aggregation results\n+ * as shard results are consumed.\n+ * This implementation can be configured to batch up a certain amount of results and reduce\n+ * them asynchronously in the provided {@link Executor} iff the buffer is exhausted.\n+ */\n+class QueryPhaseResultConsumer extends ArraySearchPhaseResults<SearchPhaseResult> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQwNTY5Ng=="}, "originalCommit": {"oid": "ee22e3ff6c8d5dd22cace2a4915f9dce9a9c5c01"}, "originalPosition": 55}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4b5d157540de29c8b601315038017cf4be0fa6ff", "author": {"user": {"login": "jimczi", "name": "Jim Ferenczi"}}, "url": "https://github.com/elastic/elasticsearch/commit/4b5d157540de29c8b601315038017cf4be0fa6ff", "committedDate": "2020-07-21T11:56:22Z", "message": "Merge branch 'master' into partial_reduce_thread_pool"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1fa48801f433588a8a0de2e64851c1e145cf0640", "author": {"user": {"login": "jimczi", "name": "Jim Ferenczi"}}, "url": "https://github.com/elastic/elasticsearch/commit/1fa48801f433588a8a0de2e64851c1e145cf0640", "committedDate": "2020-07-27T13:55:50Z", "message": "address review + cleanup"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a2331f4bc6e5bae50ae32103f329a2de7fa7e2bd", "author": {"user": {"login": "jimczi", "name": "Jim Ferenczi"}}, "url": "https://github.com/elastic/elasticsearch/commit/a2331f4bc6e5bae50ae32103f329a2de7fa7e2bd", "committedDate": "2020-07-27T14:05:27Z", "message": "Merge branch 'master' into partial_reduce_thread_pool"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9f5d075c154ab9be7f5b90189272acd839f54256", "author": {"user": {"login": "jimczi", "name": "Jim Ferenczi"}}, "url": "https://github.com/elastic/elasticsearch/commit/9f5d075c154ab9be7f5b90189272acd839f54256", "committedDate": "2020-07-27T14:29:28Z", "message": "restore consistent ordering on reduce"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c15a1e6d0915f4c7cc98ea45f43f9fd054620c79", "author": {"user": {"login": "jimczi", "name": "Jim Ferenczi"}}, "url": "https://github.com/elastic/elasticsearch/commit/c15a1e6d0915f4c7cc98ea45f43f9fd054620c79", "committedDate": "2020-07-27T14:59:17Z", "message": "handle null buffer"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU1ODc0MDI0", "url": "https://github.com/elastic/elasticsearch/pull/58461#pullrequestreview-455874024", "createdAt": "2020-07-27T15:08:10Z", "commit": {"oid": "c15a1e6d0915f4c7cc98ea45f43f9fd054620c79"}, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNTowODoxMFrOG3m1iA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNToxMjozMVrOG3nBjA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDk2MTE2MA==", "bodyText": "Did you keep the same class here because folks are used to setting the logger for SearchPhaseController? If so, its worth a comment. If no, I'd switch this to QueryPhaseResultConsumer.", "url": "https://github.com/elastic/elasticsearch/pull/58461#discussion_r460961160", "createdAt": "2020-07-27T15:08:10Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/action/search/QueryPhaseResultConsumer.java", "diffHunk": "@@ -0,0 +1,411 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.search;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.lucene.search.TopDocs;\n+import org.elasticsearch.action.search.SearchPhaseController.TopDocsStats;\n+import org.elasticsearch.common.io.stream.DelayableWriteable;\n+import org.elasticsearch.common.io.stream.DelayableWriteable.Serialized;\n+import org.elasticsearch.common.io.stream.NamedWriteableRegistry;\n+import org.elasticsearch.common.lucene.search.TopDocsAndMaxScore;\n+import org.elasticsearch.common.util.concurrent.AbstractRunnable;\n+import org.elasticsearch.search.SearchPhaseResult;\n+import org.elasticsearch.search.SearchShardTarget;\n+import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContextBuilder;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.query.QuerySearchResult;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Consumer;\n+\n+import static org.elasticsearch.action.search.SearchPhaseController.mergeTopDocs;\n+import static org.elasticsearch.action.search.SearchPhaseController.setShardIndex;\n+\n+/**\n+ * A {@link ArraySearchPhaseResults} implementation that incrementally reduces aggregation results\n+ * as shard results are consumed.\n+ * This implementation can be configured to batch up a certain amount of results and reduce\n+ * them asynchronously in the provided {@link Executor} iff the buffer is exhausted.\n+ */\n+class QueryPhaseResultConsumer extends ArraySearchPhaseResults<SearchPhaseResult> {\n+    private static final Logger logger = LogManager.getLogger(SearchPhaseController.class);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c15a1e6d0915f4c7cc98ea45f43f9fd054620c79"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDk2NDIzNg==", "bodyText": "I think it is more clear to write this is DelayableWriteable.Serialized<InternalAggregations>.", "url": "https://github.com/elastic/elasticsearch/pull/58461#discussion_r460964236", "createdAt": "2020-07-27T15:12:31Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/action/search/QueryPhaseResultConsumer.java", "diffHunk": "@@ -0,0 +1,411 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.search;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.lucene.search.TopDocs;\n+import org.elasticsearch.action.search.SearchPhaseController.TopDocsStats;\n+import org.elasticsearch.common.io.stream.DelayableWriteable;\n+import org.elasticsearch.common.io.stream.DelayableWriteable.Serialized;\n+import org.elasticsearch.common.io.stream.NamedWriteableRegistry;\n+import org.elasticsearch.common.lucene.search.TopDocsAndMaxScore;\n+import org.elasticsearch.common.util.concurrent.AbstractRunnable;\n+import org.elasticsearch.search.SearchPhaseResult;\n+import org.elasticsearch.search.SearchShardTarget;\n+import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContextBuilder;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.query.QuerySearchResult;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Consumer;\n+\n+import static org.elasticsearch.action.search.SearchPhaseController.mergeTopDocs;\n+import static org.elasticsearch.action.search.SearchPhaseController.setShardIndex;\n+\n+/**\n+ * A {@link ArraySearchPhaseResults} implementation that incrementally reduces aggregation results\n+ * as shard results are consumed.\n+ * This implementation can be configured to batch up a certain amount of results and reduce\n+ * them asynchronously in the provided {@link Executor} iff the buffer is exhausted.\n+ */\n+class QueryPhaseResultConsumer extends ArraySearchPhaseResults<SearchPhaseResult> {\n+    private static final Logger logger = LogManager.getLogger(SearchPhaseController.class);\n+\n+    private final Executor executor;\n+    private final SearchPhaseController controller;\n+    private final SearchProgressListener progressListener;\n+    private final ReduceContextBuilder aggReduceContextBuilder;\n+    private final NamedWriteableRegistry namedWriteableRegistry;\n+\n+    private final int topNSize;\n+    private final boolean hasTopDocs;\n+    private final boolean hasAggs;\n+    private final boolean performFinalReduce;\n+\n+    private final PendingMerges pendingMerges;\n+    private final Consumer<Exception> onPartialMergeFailure;\n+\n+    private volatile long aggsMaxBufferSize;\n+    private volatile long aggsCurrentBufferSize;\n+\n+    /**\n+     * Creates a {@link QueryPhaseResultConsumer} that incrementally reduces aggregation results\n+     * as shard results are consumed.\n+     */\n+    QueryPhaseResultConsumer(Executor executor,\n+                             SearchPhaseController controller,\n+                             SearchProgressListener progressListener,\n+                             ReduceContextBuilder aggReduceContextBuilder,\n+                             NamedWriteableRegistry namedWriteableRegistry,\n+                             int expectedResultSize,\n+                             int bufferSize,\n+                             boolean hasTopDocs,\n+                             boolean hasAggs,\n+                             int trackTotalHitsUpTo,\n+                             int topNSize,\n+                             boolean performFinalReduce,\n+                             Consumer<Exception> onPartialMergeFailure) {\n+        super(expectedResultSize);\n+        this.executor = executor;\n+        this.controller = controller;\n+        this.progressListener = progressListener;\n+        this.aggReduceContextBuilder = aggReduceContextBuilder;\n+        this.namedWriteableRegistry = namedWriteableRegistry;\n+        this.topNSize = topNSize;\n+        this.pendingMerges = new PendingMerges(bufferSize, trackTotalHitsUpTo);\n+        this.hasTopDocs = hasTopDocs;\n+        this.hasAggs = hasAggs;\n+        this.performFinalReduce = performFinalReduce;\n+        this.onPartialMergeFailure = onPartialMergeFailure;\n+    }\n+\n+    @Override\n+    void consumeResult(SearchPhaseResult result, Runnable next) {\n+        super.consumeResult(result, () -> {});\n+        QuerySearchResult querySearchResult = result.queryResult();\n+        pendingMerges.consume(querySearchResult, next);\n+        progressListener.notifyQueryResult(querySearchResult.getShardIndex());\n+    }\n+\n+    @Override\n+    SearchPhaseController.ReducedQueryPhase reduce() throws Exception {\n+        if (pendingMerges.hasPendingMerges()) {\n+            throw new AssertionError(\"partial reduce in-flight\");\n+        } else if (pendingMerges.hasFailure()) {\n+            throw pendingMerges.getFailure();\n+        }\n+\n+        logger.trace(\"aggs final reduction [{}] max [{}]\", aggsCurrentBufferSize, aggsMaxBufferSize);\n+        // ensure consistent ordering\n+        pendingMerges.sortBuffer();\n+        final TopDocsStats topDocsStats = pendingMerges.consumeTopDocsStats();\n+        final List<TopDocs> topDocsList = pendingMerges.consumeTopDocs();\n+        final List<InternalAggregations> aggsList = pendingMerges.consumeAggs();\n+        SearchPhaseController.ReducedQueryPhase reducePhase = controller.reducedQueryPhase(results.asList(), aggsList,\n+            topDocsList, topDocsStats, pendingMerges.numReducePhases, false, aggReduceContextBuilder, performFinalReduce);\n+        progressListener.notifyFinalReduce(SearchProgressListener.buildSearchShards(results.asList()),\n+            reducePhase.totalHits, reducePhase.aggregations, reducePhase.numReducePhases);\n+        return reducePhase;\n+    }\n+\n+    private MergeResult partialReduce(MergeTask task,\n+                                      TopDocsStats topDocsStats,\n+                                      MergeResult lastMerge,\n+                                      int numReducePhases) {\n+        final QuerySearchResult[] toConsume = task.consumeBuffer();\n+        if (toConsume == null) {\n+            // the task is cancelled\n+            return null;\n+        }\n+        // ensure consistent ordering\n+        Arrays.sort(toConsume, Comparator.comparingInt(QuerySearchResult::getShardIndex));\n+\n+        for (QuerySearchResult result : toConsume) {\n+            topDocsStats.add(result.topDocs(), result.searchTimedOut(), result.terminatedEarly());\n+        }\n+\n+        final TopDocs newTopDocs;\n+        if (hasTopDocs) {\n+            List<TopDocs> topDocsList = new ArrayList<>();\n+            if (lastMerge != null) {\n+                topDocsList.add(lastMerge.reducedTopDocs);\n+            }\n+            for (QuerySearchResult result : toConsume) {\n+                TopDocsAndMaxScore topDocs = result.consumeTopDocs();\n+                setShardIndex(topDocs.topDocs, result.getShardIndex());\n+                topDocsList.add(topDocs.topDocs);\n+            }\n+            newTopDocs = mergeTopDocs(topDocsList,\n+                // we have to merge here in the same way we collect on a shard\n+                topNSize, 0);\n+        } else {\n+            newTopDocs = null;\n+        }\n+\n+        final Serialized<InternalAggregations> newAggs;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c15a1e6d0915f4c7cc98ea45f43f9fd054620c79"}, "originalPosition": 170}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "065adca4ace02d0d313d583c09e8c874dcc91a5e", "author": {"user": {"login": "jimczi", "name": "Jim Ferenczi"}}, "url": "https://github.com/elastic/elasticsearch/commit/065adca4ace02d0d313d583c09e8c874dcc91a5e", "committedDate": "2020-07-27T16:44:13Z", "message": "handle empty results in progress listener"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a6c4ff4545a5423f6f53f92e07fb35e3e2ff189a", "author": {"user": {"login": "jimczi", "name": "Jim Ferenczi"}}, "url": "https://github.com/elastic/elasticsearch/commit/a6c4ff4545a5423f6f53f92e07fb35e3e2ff189a", "committedDate": "2020-07-27T16:46:24Z", "message": "address review"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "139a8bf1678f1e9b91739a307f73a17ee8d1ae16", "author": {"user": {"login": "jimczi", "name": "Jim Ferenczi"}}, "url": "https://github.com/elastic/elasticsearch/commit/139a8bf1678f1e9b91739a307f73a17ee8d1ae16", "committedDate": "2020-07-27T17:38:55Z", "message": "add more logging in tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f06c1f09f455acd5598e8d64edc75871aced59f5", "author": {"user": {"login": "jimczi", "name": "Jim Ferenczi"}}, "url": "https://github.com/elastic/elasticsearch/commit/f06c1f09f455acd5598e8d64edc75871aced59f5", "committedDate": "2020-07-27T17:57:53Z", "message": "Merge branch 'master' into partial_reduce_thread_pool"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "16d68df1fb37e16c315e33d68eed97ba748ac035", "author": {"user": {"login": "jimczi", "name": "Jim Ferenczi"}}, "url": "https://github.com/elastic/elasticsearch/commit/16d68df1fb37e16c315e33d68eed97ba748ac035", "committedDate": "2020-07-27T18:37:39Z", "message": "fix race condition for tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e337ee7bf75badef1b2c06f296976adc0258f5d3", "author": {"user": {"login": "jimczi", "name": "Jim Ferenczi"}}, "url": "https://github.com/elastic/elasticsearch/commit/e337ee7bf75badef1b2c06f296976adc0258f5d3", "committedDate": "2020-07-28T08:01:26Z", "message": "remove spurious log"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 377, "cost": 1, "resetAt": "2021-10-28T18:54:27Z"}}}