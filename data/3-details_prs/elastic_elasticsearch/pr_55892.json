{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDEwMzMyMjU5", "number": 55892, "title": "[ML] lay ground work for handling >1 result indices", "bodyText": "This commit removes all but one reference to getInitialResultsIndexName.\nThis is to support more than one result index for a single job.", "createdAt": "2020-04-28T20:05:45Z", "url": "https://github.com/elastic/elasticsearch/pull/55892", "merged": true, "mergeCommit": {"oid": "b82f6591d93bf3a1ddc1380dfaa06233ad590d80"}, "closed": true, "closedAt": "2020-05-05T11:55:20Z", "author": {"login": "benwtrent"}, "timelineItems": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABccJfeOgH2gAyNDEwMzMyMjU5Ojc4M2EzMWZkMzM0YzRhODE3NmQ2MjVlMTk0ZWE1MzU1OThiZDVhZjc=", "endCursor": "Y3Vyc29yOnYyOpPPAAABceQkICgFqTQwNTYyMDY0NA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "783a31fd334c4a8176d625e194ea535598bd5af7", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/783a31fd334c4a8176d625e194ea535598bd5af7", "committedDate": "2020-04-28T19:59:29Z", "message": "[ML] lay ground work for handling >1 result indices"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5de6ef1cdf7dd02a789c8f55798761ff93901184", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/5de6ef1cdf7dd02a789c8f55798761ff93901184", "committedDate": "2020-04-29T10:15:13Z", "message": "adding tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "74fc0f2b4d6b5af760d46e07638c6c85c8e181df", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/74fc0f2b4d6b5af760d46e07638c6c85c8e181df", "committedDate": "2020-04-29T12:11:32Z", "message": "fixing tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0eea1214036b72f797b820ff4e8fd95de23ccda3", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/0eea1214036b72f797b820ff4e8fd95de23ccda3", "committedDate": "2020-04-29T12:12:00Z", "message": "Merge remote-tracking branch 'upstream/master' into feature/ml-refactor-for-multi-result-index"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "943181d92e4a57c834fd7cb7a1501f3b42c9561e", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/943181d92e4a57c834fd7cb7a1501f3b42c9561e", "committedDate": "2020-04-29T13:21:49Z", "message": "do not delete shared index"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9658bc9fd4b83d992b1f43c153809f31ef9ee794", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/9658bc9fd4b83d992b1f43c153809f31ef9ee794", "committedDate": "2020-04-29T14:23:20Z", "message": "fixing expansion parameters"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "338eb26824660d4256151aaffa4f9eac229ffaea", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/338eb26824660d4256151aaffa4f9eac229ffaea", "committedDate": "2020-04-30T11:34:50Z", "message": "fixing test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7c8dc854ed2fbb5905cac6613d1143e0ed02869a", "author": {"user": {"login": "elasticmachine", "name": "Elastic Machine"}}, "url": "https://github.com/elastic/elasticsearch/commit/7c8dc854ed2fbb5905cac6613d1143e0ed02869a", "committedDate": "2020-04-30T11:35:12Z", "message": "Merge branch 'master' into feature/ml-refactor-for-multi-result-index"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA0OTAxNjg2", "url": "https://github.com/elastic/elasticsearch/pull/55892#pullrequestreview-404901686", "createdAt": "2020-05-04T11:26:40Z", "commit": {"oid": "7c8dc854ed2fbb5905cac6613d1143e0ed02869a"}, "state": "COMMENTED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNFQxMToyNjo0MVrOGP8PxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNFQxMzo1NTowNFrOGQBdWQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTM2ODkwMA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    // check that the default shared index still exist but is empty\n          \n          \n            \n                    // check that the default shared index still exists but is empty", "url": "https://github.com/elastic/elasticsearch/pull/55892#discussion_r419368900", "createdAt": "2020-05-04T11:26:41Z", "author": {"login": "przemekwitek"}, "path": "x-pack/plugin/ml/qa/native-multi-node-tests/src/test/java/org/elasticsearch/xpack/ml/integration/MlJobIT.java", "diffHunk": "@@ -689,21 +689,17 @@ public void testMultiIndexDelete() throws Exception {\n \n         refreshAllIndices();\n \n-        // check that the indices still exist but are empty\n+        // check that the default shared index still exist but is empty", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c8dc854ed2fbb5905cac6613d1143e0ed02869a"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQzMzg5Mw==", "bodyText": "static?", "url": "https://github.com/elastic/elasticsearch/pull/55892#discussion_r419433893", "createdAt": "2020-05-04T13:26:31Z", "author": {"login": "przemekwitek"}, "path": "x-pack/plugin/ml/src/test/java/org/elasticsearch/xpack/ml/integration/JobStorageDeletionTaskIT.java", "diffHunk": "@@ -46,4 +107,124 @@ public void testUnrelatedIndexNotTouched() throws Exception {\n \n         disableIndexBlock(UNRELATED_INDEX, IndexMetadata.SETTING_READ_ONLY);\n     }\n+\n+    public void testDeleteDedicatedJobWithDataInShared() throws Exception {\n+        internalCluster().ensureAtLeastNumDataNodes(1);\n+        ensureStableCluster(1);\n+        String jobIdDedicated = \"delete-test-job-dedicated\";\n+\n+        Job.Builder job = createJob(jobIdDedicated, new ByteSizeValue(2, ByteSizeUnit.MB))\n+            .setResultsIndexName(\"delete-test-job-dedicated\");\n+        client().execute(PutJobAction.INSTANCE, new PutJobAction.Request(job)).actionGet();\n+        client().execute(OpenJobAction.INSTANCE, new OpenJobAction.Request(job.getId())).actionGet();\n+        String dedicatedIndex = job.build().getInitialResultsIndexName();\n+        awaitJobOpenedAndAssigned(job.getId(), null);\n+        createBuckets(jobIdDedicated, 1, 10);\n+\n+        String jobIdShared = \"delete-test-job-shared\";\n+        job = createJob(jobIdShared, new ByteSizeValue(2, ByteSizeUnit.MB));\n+        client().execute(PutJobAction.INSTANCE, new PutJobAction.Request(job)).actionGet();\n+        client().execute(OpenJobAction.INSTANCE, new OpenJobAction.Request(job.getId())).actionGet();\n+        awaitJobOpenedAndAssigned(job.getId(), null);\n+        createBuckets(jobIdShared, 1, 10);\n+\n+        // Manually switching over alias info\n+        IndicesAliasesRequest aliasesRequest = new IndicesAliasesRequest();\n+        aliasesRequest.addAliasAction(IndicesAliasesRequest.AliasActions\n+                .add()\n+                .alias(AnomalyDetectorsIndex.jobResultsAliasedName(jobIdDedicated))\n+                .isHidden(true)\n+                .index(AnomalyDetectorsIndex.jobResultsIndexPrefix() + \"shared\")\n+                .writeIndex(false)\n+                .filter(QueryBuilders.boolQuery().filter(QueryBuilders.termQuery(Job.ID.getPreferredName(), jobIdDedicated))))\n+            .addAliasAction(IndicesAliasesRequest.AliasActions\n+                .add()\n+                .alias(AnomalyDetectorsIndex.resultsWriteAlias(jobIdDedicated))\n+                .index(AnomalyDetectorsIndex.jobResultsIndexPrefix() + \"shared\")\n+                .isHidden(true)\n+                .writeIndex(true))\n+            .addAliasAction(IndicesAliasesRequest.AliasActions\n+                .remove()\n+                .alias(AnomalyDetectorsIndex.resultsWriteAlias(jobIdDedicated))\n+                .index(dedicatedIndex));\n+\n+        client().admin().indices().aliases(aliasesRequest).actionGet();\n+\n+        createBuckets(jobIdDedicated, 11, 10);\n+        client().admin().indices().prepareRefresh(AnomalyDetectorsIndex.jobResultsIndexPrefix() + \"*\").get();\n+        AtomicReference<QueryPage<Bucket>> bucketHandler = new AtomicReference<>();\n+        AtomicReference<Exception> failureHandler = new AtomicReference<>();\n+        blockingCall(listener ->  jobResultsProvider.buckets(jobIdDedicated,\n+            new BucketsQueryBuilder().from(0).size(22),\n+            listener::onResponse,\n+            listener::onFailure,\n+            client()), bucketHandler, failureHandler);\n+        assertThat(failureHandler.get(), is(nullValue()));\n+        assertThat(bucketHandler.get().count(), equalTo(22L));\n+\n+        DeleteJobAction.Request deleteJobRequest = new DeleteJobAction.Request(jobIdDedicated);\n+        deleteJobRequest.setForce(true);\n+        client().execute(DeleteJobAction.INSTANCE, deleteJobRequest).get();\n+\n+        client().admin().indices().prepareRefresh(AnomalyDetectorsIndex.jobResultsIndexPrefix() + \"*\").get();\n+        // Make sure our shared index job is OK\n+        bucketHandler = new AtomicReference<>();\n+        failureHandler = new AtomicReference<>();\n+        blockingCall(listener ->  jobResultsProvider.buckets(jobIdShared,\n+            new BucketsQueryBuilder().from(0).size(21),\n+            listener::onResponse,\n+            listener::onFailure,\n+            client()), bucketHandler, failureHandler);\n+        assertThat(failureHandler.get(), is(nullValue()));\n+        assertThat(bucketHandler.get().count(), equalTo(11L));\n+\n+        // Make sure dedicated index is gone\n+        assertThat(client().admin()\n+            .indices()\n+            .prepareGetIndex()\n+            .setIndices(dedicatedIndex)\n+            .setIndicesOptions(IndicesOptions.LENIENT_EXPAND_OPEN)\n+            .get()\n+            .indices().length, equalTo(0));\n+\n+        // Make sure all results referencing the dedicated job are gone\n+        assertThat(client().prepareSearch()\n+            .setIndices(AnomalyDetectorsIndex.jobResultsIndexPrefix() + \"*\")\n+            .setIndicesOptions(IndicesOptions.lenientExpandOpenHidden())\n+            .setTrackTotalHits(true)\n+            .setSize(0)\n+            .setSource(SearchSourceBuilder.searchSource()\n+                .query(QueryBuilders.boolQuery().filter(QueryBuilders.termQuery(Job.ID.getPreferredName(), jobIdDedicated))))\n+            .get()\n+            .getHits()\n+            .getTotalHits()\n+            .value, equalTo(0L));\n+    }\n+\n+    private void createBuckets(String jobId, int from, int count) {\n+        JobResultsPersister.Builder builder = jobResultsPersister.bulkPersisterBuilder(jobId, () -> true);\n+        for (int i = from; i <= count + from; ++i) {\n+            Bucket bucket = new Bucket(jobId, new Date(bucketSpan * i), bucketSpan);\n+            builder.persistBucket(bucket);\n+        }\n+        builder.executeRequest();\n+    }\n+\n+    protected <T> void blockingCall(Consumer<ActionListener<T>> function, AtomicReference<T> response,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c8dc854ed2fbb5905cac6613d1143e0ed02869a"}, "originalPosition": 191}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQzNTY1OA==", "bodyText": "static?", "url": "https://github.com/elastic/elasticsearch/pull/55892#discussion_r419435658", "createdAt": "2020-05-04T13:29:00Z", "author": {"login": "przemekwitek"}, "path": "x-pack/plugin/ml/src/test/java/org/elasticsearch/xpack/ml/integration/JobStorageDeletionTaskIT.java", "diffHunk": "@@ -5,22 +5,83 @@\n  */\n package org.elasticsearch.xpack.ml.integration;\n \n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.admin.indices.alias.IndicesAliasesRequest;\n+import org.elasticsearch.action.support.IndicesOptions;\n+import org.elasticsearch.client.OriginSettingClient;\n import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.routing.OperationRouting;\n+import org.elasticsearch.cluster.service.ClusterApplierService;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.cluster.service.MasterService;\n+import org.elasticsearch.common.settings.ClusterSettings;\n+import org.elasticsearch.common.settings.Settings;\n import org.elasticsearch.common.unit.ByteSizeUnit;\n import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.index.query.QueryBuilders;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.xpack.core.ClientHelper;\n+import org.elasticsearch.xpack.core.action.util.QueryPage;\n import org.elasticsearch.xpack.core.ml.action.DeleteJobAction;\n import org.elasticsearch.xpack.core.ml.action.OpenJobAction;\n import org.elasticsearch.xpack.core.ml.action.PutJobAction;\n+import org.elasticsearch.xpack.core.ml.job.config.AnalysisConfig;\n import org.elasticsearch.xpack.core.ml.job.config.Job;\n+import org.elasticsearch.xpack.core.ml.job.persistence.AnomalyDetectorsIndex;\n+import org.elasticsearch.xpack.core.ml.job.results.Bucket;\n+import org.elasticsearch.xpack.ml.inference.ingest.InferenceProcessor;\n+import org.elasticsearch.xpack.ml.job.persistence.BucketsQueryBuilder;\n+import org.elasticsearch.xpack.ml.job.persistence.JobResultsPersister;\n+import org.elasticsearch.xpack.ml.job.persistence.JobResultsProvider;\n+import org.elasticsearch.xpack.ml.notifications.AnomalyDetectionAuditor;\n import org.elasticsearch.xpack.ml.support.BaseMlIntegTestCase;\n+import org.elasticsearch.xpack.ml.utils.persistence.ResultsPersisterService;\n+import org.junit.Before;\n+\n+import java.util.Arrays;\n+import java.util.Date;\n+import java.util.HashSet;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Consumer;\n+\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.nullValue;\n+import static org.mockito.Mockito.mock;\n \n /**\n  * Test that ML does not touch unnecessary indices when removing job index aliases\n  */\n public class JobStorageDeletionTaskIT extends BaseMlIntegTestCase {\n \n+    private long bucketSpan = AnalysisConfig.Builder.DEFAULT_BUCKET_SPAN.getMillis();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c8dc854ed2fbb5905cac6613d1143e0ed02869a"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQzNzk5NA==", "bodyText": "Could you replace literal string with jobIdDedicated here?", "url": "https://github.com/elastic/elasticsearch/pull/55892#discussion_r419437994", "createdAt": "2020-05-04T13:32:09Z", "author": {"login": "przemekwitek"}, "path": "x-pack/plugin/ml/src/test/java/org/elasticsearch/xpack/ml/integration/JobStorageDeletionTaskIT.java", "diffHunk": "@@ -46,4 +107,124 @@ public void testUnrelatedIndexNotTouched() throws Exception {\n \n         disableIndexBlock(UNRELATED_INDEX, IndexMetadata.SETTING_READ_ONLY);\n     }\n+\n+    public void testDeleteDedicatedJobWithDataInShared() throws Exception {\n+        internalCluster().ensureAtLeastNumDataNodes(1);\n+        ensureStableCluster(1);\n+        String jobIdDedicated = \"delete-test-job-dedicated\";\n+\n+        Job.Builder job = createJob(jobIdDedicated, new ByteSizeValue(2, ByteSizeUnit.MB))\n+            .setResultsIndexName(\"delete-test-job-dedicated\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c8dc854ed2fbb5905cac6613d1143e0ed02869a"}, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQzOTY4Mw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    aliasesRequest.addAliasAction(IndicesAliasesRequest.AliasActions\n          \n          \n            \n                    aliasesRequest\n          \n          \n            \n                        .addAliasAction(IndicesAliasesRequest.AliasActions", "url": "https://github.com/elastic/elasticsearch/pull/55892#discussion_r419439683", "createdAt": "2020-05-04T13:34:36Z", "author": {"login": "przemekwitek"}, "path": "x-pack/plugin/ml/src/test/java/org/elasticsearch/xpack/ml/integration/JobStorageDeletionTaskIT.java", "diffHunk": "@@ -46,4 +107,124 @@ public void testUnrelatedIndexNotTouched() throws Exception {\n \n         disableIndexBlock(UNRELATED_INDEX, IndexMetadata.SETTING_READ_ONLY);\n     }\n+\n+    public void testDeleteDedicatedJobWithDataInShared() throws Exception {\n+        internalCluster().ensureAtLeastNumDataNodes(1);\n+        ensureStableCluster(1);\n+        String jobIdDedicated = \"delete-test-job-dedicated\";\n+\n+        Job.Builder job = createJob(jobIdDedicated, new ByteSizeValue(2, ByteSizeUnit.MB))\n+            .setResultsIndexName(\"delete-test-job-dedicated\");\n+        client().execute(PutJobAction.INSTANCE, new PutJobAction.Request(job)).actionGet();\n+        client().execute(OpenJobAction.INSTANCE, new OpenJobAction.Request(job.getId())).actionGet();\n+        String dedicatedIndex = job.build().getInitialResultsIndexName();\n+        awaitJobOpenedAndAssigned(job.getId(), null);\n+        createBuckets(jobIdDedicated, 1, 10);\n+\n+        String jobIdShared = \"delete-test-job-shared\";\n+        job = createJob(jobIdShared, new ByteSizeValue(2, ByteSizeUnit.MB));\n+        client().execute(PutJobAction.INSTANCE, new PutJobAction.Request(job)).actionGet();\n+        client().execute(OpenJobAction.INSTANCE, new OpenJobAction.Request(job.getId())).actionGet();\n+        awaitJobOpenedAndAssigned(job.getId(), null);\n+        createBuckets(jobIdShared, 1, 10);\n+\n+        // Manually switching over alias info\n+        IndicesAliasesRequest aliasesRequest = new IndicesAliasesRequest();\n+        aliasesRequest.addAliasAction(IndicesAliasesRequest.AliasActions", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c8dc854ed2fbb5905cac6613d1143e0ed02869a"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQ0NzU1Mw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            for(String indexName : indexNames.get()) {\n          \n          \n            \n                            for (String indexName : indexNames.get()) {", "url": "https://github.com/elastic/elasticsearch/pull/55892#discussion_r419447553", "createdAt": "2020-05-04T13:45:38Z", "author": {"login": "przemekwitek"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/action/TransportDeleteJobAction.java", "diffHunk": "@@ -328,72 +330,97 @@ private void deleteJobDocuments(ParentTaskAssigningClient parentTaskClient, Stri\n                 },\n                 failureHandler);\n \n-        // Step 6. If we have any hits, that means we are NOT the only job on these indices, and should not delete the indices.\n-        // If we do not have any hits, we can drop the indices and then skip the DBQ and alias deletion.\n-        ActionListener<SearchResponse> customIndexSearchHandler = ActionListener.wrap(\n-                searchResponse -> {\n-                    if (searchResponse == null || searchResponse.getHits().getTotalHits().value > 0) {\n-                        deleteByQueryExecutor.onResponse(true); // We need to run DBQ and alias deletion\n-                    } else {\n-                        logger.info(\"Running DELETE Index on [\" + String.join(\", \", indexNames.get()) + \"] for job [\" + jobId + \"]\");\n-                        DeleteIndexRequest request = new DeleteIndexRequest(indexNames.get());\n-                        request.indicesOptions(IndicesOptions.lenientExpandOpen());\n-                        // If we have deleted the index, then we don't need to delete the aliases or run the DBQ\n-                        executeAsyncWithOrigin(\n-                                parentTaskClient.threadPool().getThreadContext(),\n-                                ML_ORIGIN,\n-                                request,\n-                                ActionListener.<AcknowledgedResponse>wrap(\n-                                        response -> deleteByQueryExecutor.onResponse(false), // skip DBQ && Alias\n-                                        failureHandler),\n-                                parentTaskClient.admin().indices()::delete);\n-                    }\n-                },\n-                failure -> {\n-                    if (ExceptionsHelper.unwrapCause(failure) instanceof IndexNotFoundException) { // assume the index is already deleted\n-                        deleteByQueryExecutor.onResponse(false); // skip DBQ && Alias\n-                    } else {\n-                        failureHandler.accept(failure);\n-                    }\n-                }\n+        // Step 6. Handle each multi-search response. There should be one response for each underlying index.\n+        // For each underlying index that contains results ONLY for the current job, we will delete that index.\n+        // If there exists at least 1 index that has another job's results, we will run DBQ.\n+        ActionListener<MultiSearchResponse> customIndexSearchHandler = ActionListener.wrap(\n+           multiSearchResponse -> {\n+               if (multiSearchResponse == null) {\n+                   deleteByQueryExecutor.onResponse(true); // We need to run DBQ and alias deletion\n+                   return;\n+               }\n+               String defaultSharedIndex = AnomalyDetectorsIndexFields.RESULTS_INDEX_PREFIX +\n+                   AnomalyDetectorsIndexFields.RESULTS_INDEX_DEFAULT;\n+               List<String> indicesToDelete = new ArrayList<>();\n+               boolean needToRunDBQTemp = false;\n+               assert multiSearchResponse.getResponses().length == indexNames.get().length;\n+               int i = 0;\n+               for (MultiSearchResponse.Item item : multiSearchResponse.getResponses()) {\n+                   if (item.isFailure()) {\n+                       ++i;\n+                       if (ExceptionsHelper.unwrapCause(item.getFailure()) instanceof IndexNotFoundException) {\n+                           // index is already deleted, no need to take action against it\n+                           continue;\n+                       } else {\n+                           failureHandler.accept(item.getFailure());\n+                           return;\n+                       }\n+                   }\n+                   SearchResponse searchResponse = item.getResponse();\n+                   if (searchResponse.getHits().getTotalHits().value > 0 || indexNames.get()[i].equals(defaultSharedIndex)) {\n+                       ++i;\n+                       needToRunDBQTemp = true;\n+                   } else {\n+                       indicesToDelete.add(indexNames.get()[i++]);\n+                   }\n+               }\n+               final boolean needToRunDBQ = needToRunDBQTemp;\n+               if (indicesToDelete.isEmpty()) {\n+                   deleteByQueryExecutor.onResponse(needToRunDBQ);\n+                   return;\n+               }\n+               logger.info(\"[{}] deleting the following indices directly {}\", jobId, indicesToDelete);\n+               DeleteIndexRequest request = new DeleteIndexRequest(indicesToDelete.toArray(String[]::new));\n+               request.indicesOptions(IndicesOptions.lenientExpandOpenHidden());\n+               executeAsyncWithOrigin(\n+                   parentTaskClient.threadPool().getThreadContext(),\n+                   ML_ORIGIN,\n+                   request,\n+                   ActionListener.<AcknowledgedResponse>wrap(\n+                       response -> deleteByQueryExecutor.onResponse(needToRunDBQ), // only run DBQ if there is a shared index\n+                       failureHandler),\n+                   parentTaskClient.admin().indices()::delete);\n+           },\n+           failure -> {\n+               if (ExceptionsHelper.unwrapCause(failure) instanceof IndexNotFoundException) { // assume the index is already deleted\n+                   deleteByQueryExecutor.onResponse(false); // skip DBQ && Alias\n+               } else {\n+                   failureHandler.accept(failure);\n+               }\n+           }\n         );\n \n-        // Step 5. Determine if we are on shared indices by looking at whether the initial index was \".ml-anomalies-shared\"\n-        // or whether the indices that the job's results alias points to contain any documents from other jobs.\n-        // TODO: this check is currently assuming that a job's results indices are either ALL shared or ALL\n-        // dedicated to the job.  We have considered functionality like rolling jobs that generate large\n-        // volumes of results from shared to dedicated indices.  On deletion such a job would have a mix of\n-        // shared indices requiring DBQ and dedicated indices that could be simply dropped.  The current\n-        // functionality would apply DBQ to all these indices, which is safe but suboptimal.  So this functionality\n-        // should be revisited when we add rolling results index functionality, especially if we add the ability\n-        // to switch a job over to a dedicated index for future results.\n+        // Step 5. If we successfully find a job, gather information about its result indices.\n+        // This will execute a multi-search action for every concrete index behind the job results alias.\n+        // If there are no concrete indices, take no action and go to the next step.\n         ActionListener<Job.Builder> getJobHandler = ActionListener.wrap(\n-                builder -> {\n-                    Job job = builder.build();\n-                    indexNames.set(indexNameExpressionResolver.concreteIndexNames(clusterService.state(),\n-                        IndicesOptions.lenientExpandOpen(), AnomalyDetectorsIndex.jobResultsAliasedName(jobId)));\n-                    // The job may no longer be using the initial shared index, but if it started off on a\n-                    // shared index then it will still be on a shared index even if it's been reindexed\n-                    if (job.getInitialResultsIndexName()\n-                            .equals(AnomalyDetectorsIndexFields.RESULTS_INDEX_PREFIX + AnomalyDetectorsIndexFields.RESULTS_INDEX_DEFAULT)) {\n-                        // don't bother searching the index any further, we are on the default shared\n-                        customIndexSearchHandler.onResponse(null);\n-                    } else if (indexNames.get().length == 0) {\n-                        // don't bother searching the index any further - it's already been closed or deleted\n-                        customIndexSearchHandler.onResponse(null);\n-                    } else {\n-                        SearchSourceBuilder source = new SearchSourceBuilder()\n-                                .size(1)\n-                                .trackTotalHits(true)\n-                                .query(QueryBuilders.boolQuery().filter(\n-                                        QueryBuilders.boolQuery().mustNot(QueryBuilders.termQuery(Job.ID.getPreferredName(), jobId))));\n-\n-                        SearchRequest searchRequest = new SearchRequest(indexNames.get());\n-                        searchRequest.source(source);\n-                        executeAsyncWithOrigin(parentTaskClient, ML_ORIGIN, SearchAction.INSTANCE, searchRequest, customIndexSearchHandler);\n-                    }\n-                },\n-                failureHandler\n+            builder -> {\n+                indexNames.set(indexNameExpressionResolver.concreteIndexNames(clusterService.state(),\n+                    IndicesOptions.lenientExpandOpen(), AnomalyDetectorsIndex.jobResultsAliasedName(jobId)));\n+                if (indexNames.get().length == 0) {\n+                    // don't bother searching the index any further - it's already been closed or deleted\n+                    customIndexSearchHandler.onResponse(null);\n+                    return;\n+                }\n+                MultiSearchRequest multiSearchRequest = new MultiSearchRequest();\n+                // It is important that the requests are in the same order as the index names.\n+                // This is because responses are ordered according to their request's.\n+                for(String indexName : indexNames.get()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c8dc854ed2fbb5905cac6613d1143e0ed02869a"}, "originalPosition": 177}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQ0Nzg0NQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            // This is because responses are ordered according to their request's.\n          \n          \n            \n                            // This is because responses are ordered according to their requests.", "url": "https://github.com/elastic/elasticsearch/pull/55892#discussion_r419447845", "createdAt": "2020-05-04T13:46:02Z", "author": {"login": "przemekwitek"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/action/TransportDeleteJobAction.java", "diffHunk": "@@ -328,72 +330,97 @@ private void deleteJobDocuments(ParentTaskAssigningClient parentTaskClient, Stri\n                 },\n                 failureHandler);\n \n-        // Step 6. If we have any hits, that means we are NOT the only job on these indices, and should not delete the indices.\n-        // If we do not have any hits, we can drop the indices and then skip the DBQ and alias deletion.\n-        ActionListener<SearchResponse> customIndexSearchHandler = ActionListener.wrap(\n-                searchResponse -> {\n-                    if (searchResponse == null || searchResponse.getHits().getTotalHits().value > 0) {\n-                        deleteByQueryExecutor.onResponse(true); // We need to run DBQ and alias deletion\n-                    } else {\n-                        logger.info(\"Running DELETE Index on [\" + String.join(\", \", indexNames.get()) + \"] for job [\" + jobId + \"]\");\n-                        DeleteIndexRequest request = new DeleteIndexRequest(indexNames.get());\n-                        request.indicesOptions(IndicesOptions.lenientExpandOpen());\n-                        // If we have deleted the index, then we don't need to delete the aliases or run the DBQ\n-                        executeAsyncWithOrigin(\n-                                parentTaskClient.threadPool().getThreadContext(),\n-                                ML_ORIGIN,\n-                                request,\n-                                ActionListener.<AcknowledgedResponse>wrap(\n-                                        response -> deleteByQueryExecutor.onResponse(false), // skip DBQ && Alias\n-                                        failureHandler),\n-                                parentTaskClient.admin().indices()::delete);\n-                    }\n-                },\n-                failure -> {\n-                    if (ExceptionsHelper.unwrapCause(failure) instanceof IndexNotFoundException) { // assume the index is already deleted\n-                        deleteByQueryExecutor.onResponse(false); // skip DBQ && Alias\n-                    } else {\n-                        failureHandler.accept(failure);\n-                    }\n-                }\n+        // Step 6. Handle each multi-search response. There should be one response for each underlying index.\n+        // For each underlying index that contains results ONLY for the current job, we will delete that index.\n+        // If there exists at least 1 index that has another job's results, we will run DBQ.\n+        ActionListener<MultiSearchResponse> customIndexSearchHandler = ActionListener.wrap(\n+           multiSearchResponse -> {\n+               if (multiSearchResponse == null) {\n+                   deleteByQueryExecutor.onResponse(true); // We need to run DBQ and alias deletion\n+                   return;\n+               }\n+               String defaultSharedIndex = AnomalyDetectorsIndexFields.RESULTS_INDEX_PREFIX +\n+                   AnomalyDetectorsIndexFields.RESULTS_INDEX_DEFAULT;\n+               List<String> indicesToDelete = new ArrayList<>();\n+               boolean needToRunDBQTemp = false;\n+               assert multiSearchResponse.getResponses().length == indexNames.get().length;\n+               int i = 0;\n+               for (MultiSearchResponse.Item item : multiSearchResponse.getResponses()) {\n+                   if (item.isFailure()) {\n+                       ++i;\n+                       if (ExceptionsHelper.unwrapCause(item.getFailure()) instanceof IndexNotFoundException) {\n+                           // index is already deleted, no need to take action against it\n+                           continue;\n+                       } else {\n+                           failureHandler.accept(item.getFailure());\n+                           return;\n+                       }\n+                   }\n+                   SearchResponse searchResponse = item.getResponse();\n+                   if (searchResponse.getHits().getTotalHits().value > 0 || indexNames.get()[i].equals(defaultSharedIndex)) {\n+                       ++i;\n+                       needToRunDBQTemp = true;\n+                   } else {\n+                       indicesToDelete.add(indexNames.get()[i++]);\n+                   }\n+               }\n+               final boolean needToRunDBQ = needToRunDBQTemp;\n+               if (indicesToDelete.isEmpty()) {\n+                   deleteByQueryExecutor.onResponse(needToRunDBQ);\n+                   return;\n+               }\n+               logger.info(\"[{}] deleting the following indices directly {}\", jobId, indicesToDelete);\n+               DeleteIndexRequest request = new DeleteIndexRequest(indicesToDelete.toArray(String[]::new));\n+               request.indicesOptions(IndicesOptions.lenientExpandOpenHidden());\n+               executeAsyncWithOrigin(\n+                   parentTaskClient.threadPool().getThreadContext(),\n+                   ML_ORIGIN,\n+                   request,\n+                   ActionListener.<AcknowledgedResponse>wrap(\n+                       response -> deleteByQueryExecutor.onResponse(needToRunDBQ), // only run DBQ if there is a shared index\n+                       failureHandler),\n+                   parentTaskClient.admin().indices()::delete);\n+           },\n+           failure -> {\n+               if (ExceptionsHelper.unwrapCause(failure) instanceof IndexNotFoundException) { // assume the index is already deleted\n+                   deleteByQueryExecutor.onResponse(false); // skip DBQ && Alias\n+               } else {\n+                   failureHandler.accept(failure);\n+               }\n+           }\n         );\n \n-        // Step 5. Determine if we are on shared indices by looking at whether the initial index was \".ml-anomalies-shared\"\n-        // or whether the indices that the job's results alias points to contain any documents from other jobs.\n-        // TODO: this check is currently assuming that a job's results indices are either ALL shared or ALL\n-        // dedicated to the job.  We have considered functionality like rolling jobs that generate large\n-        // volumes of results from shared to dedicated indices.  On deletion such a job would have a mix of\n-        // shared indices requiring DBQ and dedicated indices that could be simply dropped.  The current\n-        // functionality would apply DBQ to all these indices, which is safe but suboptimal.  So this functionality\n-        // should be revisited when we add rolling results index functionality, especially if we add the ability\n-        // to switch a job over to a dedicated index for future results.\n+        // Step 5. If we successfully find a job, gather information about its result indices.\n+        // This will execute a multi-search action for every concrete index behind the job results alias.\n+        // If there are no concrete indices, take no action and go to the next step.\n         ActionListener<Job.Builder> getJobHandler = ActionListener.wrap(\n-                builder -> {\n-                    Job job = builder.build();\n-                    indexNames.set(indexNameExpressionResolver.concreteIndexNames(clusterService.state(),\n-                        IndicesOptions.lenientExpandOpen(), AnomalyDetectorsIndex.jobResultsAliasedName(jobId)));\n-                    // The job may no longer be using the initial shared index, but if it started off on a\n-                    // shared index then it will still be on a shared index even if it's been reindexed\n-                    if (job.getInitialResultsIndexName()\n-                            .equals(AnomalyDetectorsIndexFields.RESULTS_INDEX_PREFIX + AnomalyDetectorsIndexFields.RESULTS_INDEX_DEFAULT)) {\n-                        // don't bother searching the index any further, we are on the default shared\n-                        customIndexSearchHandler.onResponse(null);\n-                    } else if (indexNames.get().length == 0) {\n-                        // don't bother searching the index any further - it's already been closed or deleted\n-                        customIndexSearchHandler.onResponse(null);\n-                    } else {\n-                        SearchSourceBuilder source = new SearchSourceBuilder()\n-                                .size(1)\n-                                .trackTotalHits(true)\n-                                .query(QueryBuilders.boolQuery().filter(\n-                                        QueryBuilders.boolQuery().mustNot(QueryBuilders.termQuery(Job.ID.getPreferredName(), jobId))));\n-\n-                        SearchRequest searchRequest = new SearchRequest(indexNames.get());\n-                        searchRequest.source(source);\n-                        executeAsyncWithOrigin(parentTaskClient, ML_ORIGIN, SearchAction.INSTANCE, searchRequest, customIndexSearchHandler);\n-                    }\n-                },\n-                failureHandler\n+            builder -> {\n+                indexNames.set(indexNameExpressionResolver.concreteIndexNames(clusterService.state(),\n+                    IndicesOptions.lenientExpandOpen(), AnomalyDetectorsIndex.jobResultsAliasedName(jobId)));\n+                if (indexNames.get().length == 0) {\n+                    // don't bother searching the index any further - it's already been closed or deleted\n+                    customIndexSearchHandler.onResponse(null);\n+                    return;\n+                }\n+                MultiSearchRequest multiSearchRequest = new MultiSearchRequest();\n+                // It is important that the requests are in the same order as the index names.\n+                // This is because responses are ordered according to their request's.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c8dc854ed2fbb5905cac6613d1143e0ed02869a"}, "originalPosition": 176}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQ1MjEwNg==", "bodyText": "I'm not sure how it looks like in the rest of our codebase but I think assert should only be used to assert on the invariants implied directly by the code.\nIn this case we examine a response from an external service which can have a bug and therefore cause the assertion to fail. I don't know how such an exception would behave.\nWDYT?", "url": "https://github.com/elastic/elasticsearch/pull/55892#discussion_r419452106", "createdAt": "2020-05-04T13:51:51Z", "author": {"login": "przemekwitek"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/action/TransportDeleteJobAction.java", "diffHunk": "@@ -328,72 +330,97 @@ private void deleteJobDocuments(ParentTaskAssigningClient parentTaskClient, Stri\n                 },\n                 failureHandler);\n \n-        // Step 6. If we have any hits, that means we are NOT the only job on these indices, and should not delete the indices.\n-        // If we do not have any hits, we can drop the indices and then skip the DBQ and alias deletion.\n-        ActionListener<SearchResponse> customIndexSearchHandler = ActionListener.wrap(\n-                searchResponse -> {\n-                    if (searchResponse == null || searchResponse.getHits().getTotalHits().value > 0) {\n-                        deleteByQueryExecutor.onResponse(true); // We need to run DBQ and alias deletion\n-                    } else {\n-                        logger.info(\"Running DELETE Index on [\" + String.join(\", \", indexNames.get()) + \"] for job [\" + jobId + \"]\");\n-                        DeleteIndexRequest request = new DeleteIndexRequest(indexNames.get());\n-                        request.indicesOptions(IndicesOptions.lenientExpandOpen());\n-                        // If we have deleted the index, then we don't need to delete the aliases or run the DBQ\n-                        executeAsyncWithOrigin(\n-                                parentTaskClient.threadPool().getThreadContext(),\n-                                ML_ORIGIN,\n-                                request,\n-                                ActionListener.<AcknowledgedResponse>wrap(\n-                                        response -> deleteByQueryExecutor.onResponse(false), // skip DBQ && Alias\n-                                        failureHandler),\n-                                parentTaskClient.admin().indices()::delete);\n-                    }\n-                },\n-                failure -> {\n-                    if (ExceptionsHelper.unwrapCause(failure) instanceof IndexNotFoundException) { // assume the index is already deleted\n-                        deleteByQueryExecutor.onResponse(false); // skip DBQ && Alias\n-                    } else {\n-                        failureHandler.accept(failure);\n-                    }\n-                }\n+        // Step 6. Handle each multi-search response. There should be one response for each underlying index.\n+        // For each underlying index that contains results ONLY for the current job, we will delete that index.\n+        // If there exists at least 1 index that has another job's results, we will run DBQ.\n+        ActionListener<MultiSearchResponse> customIndexSearchHandler = ActionListener.wrap(\n+           multiSearchResponse -> {\n+               if (multiSearchResponse == null) {\n+                   deleteByQueryExecutor.onResponse(true); // We need to run DBQ and alias deletion\n+                   return;\n+               }\n+               String defaultSharedIndex = AnomalyDetectorsIndexFields.RESULTS_INDEX_PREFIX +\n+                   AnomalyDetectorsIndexFields.RESULTS_INDEX_DEFAULT;\n+               List<String> indicesToDelete = new ArrayList<>();\n+               boolean needToRunDBQTemp = false;\n+               assert multiSearchResponse.getResponses().length == indexNames.get().length;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c8dc854ed2fbb5905cac6613d1143e0ed02869a"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQ1NDI5Nw==", "bodyText": "You seem to increment i in both branches. Could the increment statement be factored out into the line 366?", "url": "https://github.com/elastic/elasticsearch/pull/55892#discussion_r419454297", "createdAt": "2020-05-04T13:55:04Z", "author": {"login": "przemekwitek"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/action/TransportDeleteJobAction.java", "diffHunk": "@@ -328,72 +330,97 @@ private void deleteJobDocuments(ParentTaskAssigningClient parentTaskClient, Stri\n                 },\n                 failureHandler);\n \n-        // Step 6. If we have any hits, that means we are NOT the only job on these indices, and should not delete the indices.\n-        // If we do not have any hits, we can drop the indices and then skip the DBQ and alias deletion.\n-        ActionListener<SearchResponse> customIndexSearchHandler = ActionListener.wrap(\n-                searchResponse -> {\n-                    if (searchResponse == null || searchResponse.getHits().getTotalHits().value > 0) {\n-                        deleteByQueryExecutor.onResponse(true); // We need to run DBQ and alias deletion\n-                    } else {\n-                        logger.info(\"Running DELETE Index on [\" + String.join(\", \", indexNames.get()) + \"] for job [\" + jobId + \"]\");\n-                        DeleteIndexRequest request = new DeleteIndexRequest(indexNames.get());\n-                        request.indicesOptions(IndicesOptions.lenientExpandOpen());\n-                        // If we have deleted the index, then we don't need to delete the aliases or run the DBQ\n-                        executeAsyncWithOrigin(\n-                                parentTaskClient.threadPool().getThreadContext(),\n-                                ML_ORIGIN,\n-                                request,\n-                                ActionListener.<AcknowledgedResponse>wrap(\n-                                        response -> deleteByQueryExecutor.onResponse(false), // skip DBQ && Alias\n-                                        failureHandler),\n-                                parentTaskClient.admin().indices()::delete);\n-                    }\n-                },\n-                failure -> {\n-                    if (ExceptionsHelper.unwrapCause(failure) instanceof IndexNotFoundException) { // assume the index is already deleted\n-                        deleteByQueryExecutor.onResponse(false); // skip DBQ && Alias\n-                    } else {\n-                        failureHandler.accept(failure);\n-                    }\n-                }\n+        // Step 6. Handle each multi-search response. There should be one response for each underlying index.\n+        // For each underlying index that contains results ONLY for the current job, we will delete that index.\n+        // If there exists at least 1 index that has another job's results, we will run DBQ.\n+        ActionListener<MultiSearchResponse> customIndexSearchHandler = ActionListener.wrap(\n+           multiSearchResponse -> {\n+               if (multiSearchResponse == null) {\n+                   deleteByQueryExecutor.onResponse(true); // We need to run DBQ and alias deletion\n+                   return;\n+               }\n+               String defaultSharedIndex = AnomalyDetectorsIndexFields.RESULTS_INDEX_PREFIX +\n+                   AnomalyDetectorsIndexFields.RESULTS_INDEX_DEFAULT;\n+               List<String> indicesToDelete = new ArrayList<>();\n+               boolean needToRunDBQTemp = false;\n+               assert multiSearchResponse.getResponses().length == indexNames.get().length;\n+               int i = 0;\n+               for (MultiSearchResponse.Item item : multiSearchResponse.getResponses()) {\n+                   if (item.isFailure()) {\n+                       ++i;\n+                       if (ExceptionsHelper.unwrapCause(item.getFailure()) instanceof IndexNotFoundException) {\n+                           // index is already deleted, no need to take action against it\n+                           continue;\n+                       } else {\n+                           failureHandler.accept(item.getFailure());\n+                           return;\n+                       }\n+                   }\n+                   SearchResponse searchResponse = item.getResponse();\n+                   if (searchResponse.getHits().getTotalHits().value > 0 || indexNames.get()[i].equals(defaultSharedIndex)) {\n+                       ++i;\n+                       needToRunDBQTemp = true;\n+                   } else {\n+                       indicesToDelete.add(indexNames.get()[i++]);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c8dc854ed2fbb5905cac6613d1143e0ed02869a"}, "originalPosition": 98}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f31e13fb4f9be614bc3f417eb772b3a938deb466", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/f31e13fb4f9be614bc3f417eb772b3a938deb466", "committedDate": "2020-05-04T19:00:49Z", "message": "Merge branch 'master' into feature/ml-refactor-for-multi-result-index"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7555ea7d8e4d9f3aa6adf14616c62662b56fa7b1", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/7555ea7d8e4d9f3aa6adf14616c62662b56fa7b1", "committedDate": "2020-05-04T19:13:35Z", "message": "Addressing PR comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA1NjIwNjQ0", "url": "https://github.com/elastic/elasticsearch/pull/55892#pullrequestreview-405620644", "createdAt": "2020-05-05T09:20:08Z", "commit": {"oid": "7555ea7d8e4d9f3aa6adf14616c62662b56fa7b1"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQwOToyMDowOVrOGQhGpA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQwOToyMDowOVrOGQhGpA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTk3Mjc3Mg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                private static long bucketSpan = AnalysisConfig.Builder.DEFAULT_BUCKET_SPAN.getMillis();\n          \n          \n            \n                private static long BUCKET_SPAN = AnalysisConfig.Builder.DEFAULT_BUCKET_SPAN.getMillis();", "url": "https://github.com/elastic/elasticsearch/pull/55892#discussion_r419972772", "createdAt": "2020-05-05T09:20:09Z", "author": {"login": "przemekwitek"}, "path": "x-pack/plugin/ml/src/test/java/org/elasticsearch/xpack/ml/integration/JobStorageDeletionTaskIT.java", "diffHunk": "@@ -5,22 +5,80 @@\n  */\n package org.elasticsearch.xpack.ml.integration;\n \n+import org.elasticsearch.action.admin.indices.alias.IndicesAliasesRequest;\n+import org.elasticsearch.action.support.IndicesOptions;\n+import org.elasticsearch.client.OriginSettingClient;\n import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.routing.OperationRouting;\n+import org.elasticsearch.cluster.service.ClusterApplierService;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.cluster.service.MasterService;\n+import org.elasticsearch.common.settings.ClusterSettings;\n+import org.elasticsearch.common.settings.Settings;\n import org.elasticsearch.common.unit.ByteSizeUnit;\n import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.index.query.QueryBuilders;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.xpack.core.ClientHelper;\n+import org.elasticsearch.xpack.core.action.util.QueryPage;\n import org.elasticsearch.xpack.core.ml.action.DeleteJobAction;\n import org.elasticsearch.xpack.core.ml.action.OpenJobAction;\n import org.elasticsearch.xpack.core.ml.action.PutJobAction;\n+import org.elasticsearch.xpack.core.ml.job.config.AnalysisConfig;\n import org.elasticsearch.xpack.core.ml.job.config.Job;\n+import org.elasticsearch.xpack.core.ml.job.persistence.AnomalyDetectorsIndex;\n+import org.elasticsearch.xpack.core.ml.job.results.Bucket;\n+import org.elasticsearch.xpack.ml.inference.ingest.InferenceProcessor;\n+import org.elasticsearch.xpack.ml.job.persistence.BucketsQueryBuilder;\n+import org.elasticsearch.xpack.ml.job.persistence.JobResultsPersister;\n+import org.elasticsearch.xpack.ml.job.persistence.JobResultsProvider;\n+import org.elasticsearch.xpack.ml.notifications.AnomalyDetectionAuditor;\n import org.elasticsearch.xpack.ml.support.BaseMlIntegTestCase;\n+import org.elasticsearch.xpack.ml.utils.persistence.ResultsPersisterService;\n+import org.junit.Before;\n+\n+import java.util.Arrays;\n+import java.util.Date;\n+import java.util.HashSet;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.nullValue;\n+import static org.mockito.Mockito.mock;\n \n /**\n  * Test that ML does not touch unnecessary indices when removing job index aliases\n  */\n public class JobStorageDeletionTaskIT extends BaseMlIntegTestCase {\n \n+    private static long bucketSpan = AnalysisConfig.Builder.DEFAULT_BUCKET_SPAN.getMillis();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7555ea7d8e4d9f3aa6adf14616c62662b56fa7b1"}, "originalPosition": 53}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 245, "cost": 1, "resetAt": "2021-10-28T18:54:27Z"}}}