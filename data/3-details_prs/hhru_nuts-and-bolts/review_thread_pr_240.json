{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDEwOTk3ODE3", "number": 240, "reviewThreads": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQxNjowNzoxNVrOD4aJpg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQxMjoyMToyMFrOD5yjqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwNDc1MzAyOnYy", "diffSide": "RIGHT", "path": "nab-kafka/src/main/java/ru/hh/nab/kafka/consumer/ConsumeStrategyInternal.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQxNjowNzoxNVrOGPN7OQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQxNjowNzoxNVrOGPN7OQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODYwOTk3Nw==", "bodyText": "\u041f\u0440\u0438\u0448\u043b\u043e\u0441\u044c \u0432\u0432\u0435\u0441\u0442\u0438 \u043a\u043b\u0430\u0441\u0441, \u0447\u0442\u043e\u0431\u044b \u043f\u0440\u043e\u043a\u0438\u043d\u0443\u0442\u044c \u0432\u044b\u0437\u043e\u0432 kafkaConsumer.onMessagesBatch \u0432 \u0441\u043f\u0440\u0438\u043d\u0433\u043e\u0432\u044b\u0439 KafkaContainer:\n\u041a\u043e\u043d\u0442\u0435\u0439\u043d\u0435\u0440 \u043d\u0443\u0436\u0435\u043d \u0447\u0442\u043e\u0431\u044b \u0441\u043e\u0437\u0434\u0430\u0442\u044c  \u043d\u0430\u0448 kafkaConsumer, \u043f\u0440\u0438 \u044d\u0442\u043e\u043c \u043e\u043d \u0434\u043e\u043b\u0436\u0435\u043d \u043e\u0431\u0440\u0430\u0449\u0430\u0442\u044c\u0441\u044f \u043a \u043a\u043e\u043d\u0441\u0443\u043c\u0435\u0440\u0443", "url": "https://github.com/hhru/nuts-and-bolts/pull/240#discussion_r418609977", "createdAt": "2020-05-01T16:07:15Z", "author": {"login": "bokshitsky"}, "path": "nab-kafka/src/main/java/ru/hh/nab/kafka/consumer/ConsumeStrategyInternal.java", "diffHunk": "@@ -0,0 +1,20 @@\n+package ru.hh.nab.kafka.consumer;\n+\n+import java.util.List;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+\n+\n+class ConsumeStrategyInternal<T> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "39517a063d745afe0e33c80b6883fe649b367fba"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwNDc1NTg5OnYy", "diffSide": "RIGHT", "path": "nab-kafka/src/main/java/ru/hh/nab/kafka/consumer/SeekToFirstNotAckedMessageErrorHandler.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQxNjowODoxM1rOGPN9CA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQxNToyMDoxNlrOGRYS9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODYxMDQ0MA==", "bodyText": "\u041b\u043e\u0433\u0438\u043a\u0430 backoffa \u0441\u043a\u043e\u043f\u0438\u0440\u043e\u0432\u0430\u043d\u0430 \u0438\u0437 SeekToCurrentBatchErrorHandler \u043f\u043e\u043b\u043d\u043e\u0441\u0442\u044c\u044e, \u0434\u0430\u0436\u0435 \u043d\u0435 \u0447\u0438\u0442\u0430\u044f\n\u041c\u0435\u043d\u044f\u0435\u0442\u0441\u044f \u043b\u043e\u0433\u0438\u043a\u0430 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0444\u0435\u0439\u043b\u0430 - \u043f\u0430\u0434\u0430\u0435\u043c \u043d\u0435 \u0432 \u043d\u0430\u0447\u0430\u043b\u043e \u0431\u0430\u0442\u0447\u0430, \u0430 \u043d\u0430 \u0442\u043e \u043c\u0435\u0441\u0442\u043e \u043a\u043e\u0442\u043e\u0440\u043e\u0435 ack-\u043d\u0443\u043b\u0438 \u0432 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0439 \u0440\u0430\u0437", "url": "https://github.com/hhru/nuts-and-bolts/pull/240#discussion_r418610440", "createdAt": "2020-05-01T16:08:13Z", "author": {"login": "bokshitsky"}, "path": "nab-kafka/src/main/java/ru/hh/nab/kafka/consumer/SeekToFirstNotAckedMessageErrorHandler.java", "diffHunk": "@@ -0,0 +1,92 @@\n+package ru.hh.nab.kafka.consumer;\n+\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Optional;\n+import static java.util.stream.Collectors.toMap;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+import org.springframework.kafka.KafkaException;\n+import org.springframework.kafka.listener.ContainerAwareBatchErrorHandler;\n+import org.springframework.kafka.listener.MessageListenerContainer;\n+import org.springframework.util.backoff.BackOff;\n+import org.springframework.util.backoff.BackOffExecution;\n+\n+class SeekToFirstNotAckedMessageErrorHandler<T> implements ContainerAwareBatchErrorHandler {\n+\n+  private final ThreadLocal<BackOffExecution> backOffs = new ThreadLocal<>();\n+\n+  private final ThreadLocal<Long> lastInterval = new ThreadLocal<>();\n+\n+  private final BackOff backOff;\n+  private volatile KafkaConsumer<T> kafkaConsumer;\n+\n+  public SeekToFirstNotAckedMessageErrorHandler(BackOff backOff) {\n+    this.backOff = backOff;\n+  }\n+\n+  public void setKafkaConsumer(KafkaConsumer<T> kafkaConsumer) {\n+    this.kafkaConsumer = kafkaConsumer;\n+  }\n+\n+  @Override\n+  public void handle(Exception thrownException, ConsumerRecords<?, ?> data, Consumer<?, ?> consumer,\n+                     MessageListenerContainer container) {\n+\n+    List<ConsumerRecord<String, T>> currentBatch = kafkaConsumer.getCurrentBatch();\n+    if (!currentBatch.isEmpty()) {\n+      LinkedHashMap<TopicPartition, OffsetAndMetadata> offsetsToSeek = currentBatch.stream().collect(toMap(\n+          record -> new TopicPartition(record.topic(), record.partition()),\n+          record -> new OffsetAndMetadata(record.offset()),\n+          (offset1, offset2) -> offset1,\n+          LinkedHashMap::new\n+      ));\n+\n+      Optional.ofNullable(kafkaConsumer.getLastAckedBatchRecord()).ifPresent(lastAckedBatchRecord -> {\n+        for (ConsumerRecord<String, T> record : kafkaConsumer.getCurrentBatch()) {\n+          offsetsToSeek.put(new TopicPartition(record.topic(), record.partition()), new OffsetAndMetadata(record.offset() + 1));\n+          if (record == lastAckedBatchRecord) {\n+            break;\n+          }\n+        }\n+      });\n+\n+      offsetsToSeek.forEach(consumer::seek);\n+    }\n+\n+    if (this.backOff != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "39517a063d745afe0e33c80b6883fe649b367fba"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDcyOTA3OQ==", "bodyText": "\u043c\u0431 \u0443\u043d\u0430\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0442\u043e\u0433\u0434\u0430 \u043e\u0442 SeekToCurrentBatchErrorHandler?", "url": "https://github.com/hhru/nuts-and-bolts/pull/240#discussion_r420729079", "createdAt": "2020-05-06T11:48:34Z", "author": {"login": "Iskuskov"}, "path": "nab-kafka/src/main/java/ru/hh/nab/kafka/consumer/SeekToFirstNotAckedMessageErrorHandler.java", "diffHunk": "@@ -0,0 +1,92 @@\n+package ru.hh.nab.kafka.consumer;\n+\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Optional;\n+import static java.util.stream.Collectors.toMap;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+import org.springframework.kafka.KafkaException;\n+import org.springframework.kafka.listener.ContainerAwareBatchErrorHandler;\n+import org.springframework.kafka.listener.MessageListenerContainer;\n+import org.springframework.util.backoff.BackOff;\n+import org.springframework.util.backoff.BackOffExecution;\n+\n+class SeekToFirstNotAckedMessageErrorHandler<T> implements ContainerAwareBatchErrorHandler {\n+\n+  private final ThreadLocal<BackOffExecution> backOffs = new ThreadLocal<>();\n+\n+  private final ThreadLocal<Long> lastInterval = new ThreadLocal<>();\n+\n+  private final BackOff backOff;\n+  private volatile KafkaConsumer<T> kafkaConsumer;\n+\n+  public SeekToFirstNotAckedMessageErrorHandler(BackOff backOff) {\n+    this.backOff = backOff;\n+  }\n+\n+  public void setKafkaConsumer(KafkaConsumer<T> kafkaConsumer) {\n+    this.kafkaConsumer = kafkaConsumer;\n+  }\n+\n+  @Override\n+  public void handle(Exception thrownException, ConsumerRecords<?, ?> data, Consumer<?, ?> consumer,\n+                     MessageListenerContainer container) {\n+\n+    List<ConsumerRecord<String, T>> currentBatch = kafkaConsumer.getCurrentBatch();\n+    if (!currentBatch.isEmpty()) {\n+      LinkedHashMap<TopicPartition, OffsetAndMetadata> offsetsToSeek = currentBatch.stream().collect(toMap(\n+          record -> new TopicPartition(record.topic(), record.partition()),\n+          record -> new OffsetAndMetadata(record.offset()),\n+          (offset1, offset2) -> offset1,\n+          LinkedHashMap::new\n+      ));\n+\n+      Optional.ofNullable(kafkaConsumer.getLastAckedBatchRecord()).ifPresent(lastAckedBatchRecord -> {\n+        for (ConsumerRecord<String, T> record : kafkaConsumer.getCurrentBatch()) {\n+          offsetsToSeek.put(new TopicPartition(record.topic(), record.partition()), new OffsetAndMetadata(record.offset() + 1));\n+          if (record == lastAckedBatchRecord) {\n+            break;\n+          }\n+        }\n+      });\n+\n+      offsetsToSeek.forEach(consumer::seek);\n+    }\n+\n+    if (this.backOff != null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODYxMDQ0MA=="}, "originalCommit": {"oid": "39517a063d745afe0e33c80b6883fe649b367fba"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDgzOTUwNA==", "bodyText": "\u041e\u0442 \u043d\u0435\u0433\u043e \u043d\u0435 \u0431\u0443\u0434\u0435\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u043d\u0438 \u043e\u0434\u0438\u043d \u043c\u0435\u0442\u043e\u0434, \u0432\u0440\u043e\u0434\u0435 \u0441\u043c\u044b\u0441\u043b\u0430 \u043d\u0435\u0442 \u0442\u043e\u0433\u0434\u0430", "url": "https://github.com/hhru/nuts-and-bolts/pull/240#discussion_r420839504", "createdAt": "2020-05-06T14:32:55Z", "author": {"login": "bokshitsky"}, "path": "nab-kafka/src/main/java/ru/hh/nab/kafka/consumer/SeekToFirstNotAckedMessageErrorHandler.java", "diffHunk": "@@ -0,0 +1,92 @@\n+package ru.hh.nab.kafka.consumer;\n+\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Optional;\n+import static java.util.stream.Collectors.toMap;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+import org.springframework.kafka.KafkaException;\n+import org.springframework.kafka.listener.ContainerAwareBatchErrorHandler;\n+import org.springframework.kafka.listener.MessageListenerContainer;\n+import org.springframework.util.backoff.BackOff;\n+import org.springframework.util.backoff.BackOffExecution;\n+\n+class SeekToFirstNotAckedMessageErrorHandler<T> implements ContainerAwareBatchErrorHandler {\n+\n+  private final ThreadLocal<BackOffExecution> backOffs = new ThreadLocal<>();\n+\n+  private final ThreadLocal<Long> lastInterval = new ThreadLocal<>();\n+\n+  private final BackOff backOff;\n+  private volatile KafkaConsumer<T> kafkaConsumer;\n+\n+  public SeekToFirstNotAckedMessageErrorHandler(BackOff backOff) {\n+    this.backOff = backOff;\n+  }\n+\n+  public void setKafkaConsumer(KafkaConsumer<T> kafkaConsumer) {\n+    this.kafkaConsumer = kafkaConsumer;\n+  }\n+\n+  @Override\n+  public void handle(Exception thrownException, ConsumerRecords<?, ?> data, Consumer<?, ?> consumer,\n+                     MessageListenerContainer container) {\n+\n+    List<ConsumerRecord<String, T>> currentBatch = kafkaConsumer.getCurrentBatch();\n+    if (!currentBatch.isEmpty()) {\n+      LinkedHashMap<TopicPartition, OffsetAndMetadata> offsetsToSeek = currentBatch.stream().collect(toMap(\n+          record -> new TopicPartition(record.topic(), record.partition()),\n+          record -> new OffsetAndMetadata(record.offset()),\n+          (offset1, offset2) -> offset1,\n+          LinkedHashMap::new\n+      ));\n+\n+      Optional.ofNullable(kafkaConsumer.getLastAckedBatchRecord()).ifPresent(lastAckedBatchRecord -> {\n+        for (ConsumerRecord<String, T> record : kafkaConsumer.getCurrentBatch()) {\n+          offsetsToSeek.put(new TopicPartition(record.topic(), record.partition()), new OffsetAndMetadata(record.offset() + 1));\n+          if (record == lastAckedBatchRecord) {\n+            break;\n+          }\n+        }\n+      });\n+\n+      offsetsToSeek.forEach(consumer::seek);\n+    }\n+\n+    if (this.backOff != null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODYxMDQ0MA=="}, "originalCommit": {"oid": "39517a063d745afe0e33c80b6883fe649b367fba"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDg3NzA0Ng==", "bodyText": "\u0434\u0430, \u044f \u0447\u0442\u043e-\u0442\u043e \u043f\u043e\u0434\u0443\u043c\u0430\u043b, \u0447\u0442\u043e \u0442\u0430\u043c handle() \u043c\u043e\u0436\u043d\u043e \u0432\u044b\u0442\u0430\u0449\u0438\u0442\u044c, \u043d\u043e \u0442\u0430\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u0447\u0430\u0441\u0442\u044c \u043d\u0430\u043c \u043d\u0443\u0436\u043d\u0430", "url": "https://github.com/hhru/nuts-and-bolts/pull/240#discussion_r420877046", "createdAt": "2020-05-06T15:20:16Z", "author": {"login": "Iskuskov"}, "path": "nab-kafka/src/main/java/ru/hh/nab/kafka/consumer/SeekToFirstNotAckedMessageErrorHandler.java", "diffHunk": "@@ -0,0 +1,92 @@\n+package ru.hh.nab.kafka.consumer;\n+\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Optional;\n+import static java.util.stream.Collectors.toMap;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+import org.springframework.kafka.KafkaException;\n+import org.springframework.kafka.listener.ContainerAwareBatchErrorHandler;\n+import org.springframework.kafka.listener.MessageListenerContainer;\n+import org.springframework.util.backoff.BackOff;\n+import org.springframework.util.backoff.BackOffExecution;\n+\n+class SeekToFirstNotAckedMessageErrorHandler<T> implements ContainerAwareBatchErrorHandler {\n+\n+  private final ThreadLocal<BackOffExecution> backOffs = new ThreadLocal<>();\n+\n+  private final ThreadLocal<Long> lastInterval = new ThreadLocal<>();\n+\n+  private final BackOff backOff;\n+  private volatile KafkaConsumer<T> kafkaConsumer;\n+\n+  public SeekToFirstNotAckedMessageErrorHandler(BackOff backOff) {\n+    this.backOff = backOff;\n+  }\n+\n+  public void setKafkaConsumer(KafkaConsumer<T> kafkaConsumer) {\n+    this.kafkaConsumer = kafkaConsumer;\n+  }\n+\n+  @Override\n+  public void handle(Exception thrownException, ConsumerRecords<?, ?> data, Consumer<?, ?> consumer,\n+                     MessageListenerContainer container) {\n+\n+    List<ConsumerRecord<String, T>> currentBatch = kafkaConsumer.getCurrentBatch();\n+    if (!currentBatch.isEmpty()) {\n+      LinkedHashMap<TopicPartition, OffsetAndMetadata> offsetsToSeek = currentBatch.stream().collect(toMap(\n+          record -> new TopicPartition(record.topic(), record.partition()),\n+          record -> new OffsetAndMetadata(record.offset()),\n+          (offset1, offset2) -> offset1,\n+          LinkedHashMap::new\n+      ));\n+\n+      Optional.ofNullable(kafkaConsumer.getLastAckedBatchRecord()).ifPresent(lastAckedBatchRecord -> {\n+        for (ConsumerRecord<String, T> record : kafkaConsumer.getCurrentBatch()) {\n+          offsetsToSeek.put(new TopicPartition(record.topic(), record.partition()), new OffsetAndMetadata(record.offset() + 1));\n+          if (record == lastAckedBatchRecord) {\n+            break;\n+          }\n+        }\n+      });\n+\n+      offsetsToSeek.forEach(consumer::seek);\n+    }\n+\n+    if (this.backOff != null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODYxMDQ0MA=="}, "originalCommit": {"oid": "39517a063d745afe0e33c80b6883fe649b367fba"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwNDc2MDE4OnYy", "diffSide": "RIGHT", "path": "nab-kafka/src/main/java/ru/hh/nab/kafka/consumer/KafkaConsumer.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQxNjowOTo0MFrOGPN_pg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQxNDozMDowMVrOGRV3tg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODYxMTExMA==", "bodyText": "\u0421\u0435\u0439\u0447\u0430\u0441 \u0437\u0430\u0445\u0430\u0440\u0434\u043a\u043e\u0434\u0438\u043b KafkaInternalTopicAck \u0447\u0442\u043e\u0431\u044b \u043f\u043e\u0447\u0438\u043d\u0438\u0442\u044c \u0432 \u0442\u0435\u043a\u0443\u0449\u0438\u0445 \u043a\u043e\u043d\u0441\u0443\u043c\u0435\u0440\u0430\u0445 \u0441\u0435\u043c\u0430\u043d\u0442\u0438\u043a\u0443, \u043d\u043e \u0432\u043e\u043e\u0431\u0449\u0435 \u044d\u0442\u043e \u0437\u0430\u0445\u043e\u0434 \u043d\u0430 \u0442\u043e, \u0447\u0442\u043e\u0431\u044b \u043d\u0430\u0443\u0447\u0438\u0442\u044c nab-\u043a\u0430\u0444\u043a\u0443 \u0445\u0440\u0430\u043d\u0438\u0442\u044c \u043e\u0444\u0444\u0441\u0435\u0442\u044b \u0432 \u043b\u044e\u0431\u043e\u043c \u043d\u0443\u0436\u043d\u043e\u043c \u043c\u0435\u0441\u0442\u0435, \u0430 \u043d\u0435 \u0442\u043e\u043b\u044c\u043a\u043e \u0432 \u0441\u0430\u043c\u043e\u0439 \u043a\u0430\u0444\u043a\u0435", "url": "https://github.com/hhru/nuts-and-bolts/pull/240#discussion_r418611110", "createdAt": "2020-05-01T16:09:40Z", "author": {"login": "bokshitsky"}, "path": "nab-kafka/src/main/java/ru/hh/nab/kafka/consumer/KafkaConsumer.java", "diffHunk": "@@ -1,12 +1,66 @@\n package ru.hh.nab.kafka.consumer;\n \n-import org.apache.kafka.common.TopicPartition;\n import java.util.Collection;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.TopicPartition;\n+import org.springframework.kafka.listener.AbstractMessageListenerContainer;\n+\n+public class KafkaConsumer<T> {\n+\n+  private final AbstractMessageListenerContainer<String, T> springKafkaContainer;\n+\n+  private final ThreadLocal<List<ConsumerRecord<String, T>>> currentBatch = new InheritableThreadLocal<>();\n+  private final ThreadLocal<ConsumerRecord<String, T>> lastAckedBatchRecord = new InheritableThreadLocal<>();\n+  private final ConsumeStrategy<T> consumeStrategy;\n+\n+  public KafkaConsumer(AbstractMessageListenerContainer<String, T> springKafkaContainer, ConsumeStrategy<T> consumeStrategy) {\n+    this.springKafkaContainer = springKafkaContainer;\n+    this.consumeStrategy = consumeStrategy;\n+  }\n+\n+  void start() {\n+    springKafkaContainer.start();\n+  }\n+\n+  void stop(Runnable callback) {\n+    springKafkaContainer.stop(callback);\n+  }\n+\n+  void stop() {\n+    springKafkaContainer.stop();\n+  }\n+\n+  public Collection<TopicPartition> getAssignedPartitions() {\n+    return springKafkaContainer.getAssignedPartitions();\n+  }\n+\n+  public ConsumerRecord<String, T> getLastAckedBatchRecord() {\n+    return lastAckedBatchRecord.get();\n+  }\n \n-public interface KafkaConsumer {\n+  public List<ConsumerRecord<String, T>> getCurrentBatch() {\n+    return currentBatch.get();\n+  }\n \n-  void stopConsumer();\n+  public void setLastAckedBatchRecord(ConsumerRecord<String, T> record) {\n+    lastAckedBatchRecord.set(record);\n+  }\n \n-  Collection<TopicPartition> getAssignedPartitions();\n+  public void onMessagesBatch(List<ConsumerRecord<String, T>> messages, Consumer<?, ?> consumer) {\n+    List<ConsumerRecord<String, T>> sortedBatch = messages.stream().sorted(\n+        Comparator.comparing((Function<ConsumerRecord<String, T>, String>) ConsumerRecord::topic)\n+            .thenComparingInt(ConsumerRecord::partition)\n+            .thenComparingLong(ConsumerRecord::offset)\n+    ).collect(Collectors.toUnmodifiableList());\n \n+    lastAckedBatchRecord.remove();\n+    currentBatch.set(sortedBatch);\n+    Ack<T> ack = new KafkaInternalTopicAck<>(this, consumer);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "39517a063d745afe0e33c80b6883fe649b367fba"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc0OTI4MQ==", "bodyText": "\u0442\u0438\u043f\u043e \u0432 \u0431\u0430\u0437\u0443 \u043a\u0443\u0434\u0430-\u043d\u0438\u0431\u0443\u0434\u044c?", "url": "https://github.com/hhru/nuts-and-bolts/pull/240#discussion_r420749281", "createdAt": "2020-05-06T12:25:55Z", "author": {"login": "Iskuskov"}, "path": "nab-kafka/src/main/java/ru/hh/nab/kafka/consumer/KafkaConsumer.java", "diffHunk": "@@ -1,12 +1,66 @@\n package ru.hh.nab.kafka.consumer;\n \n-import org.apache.kafka.common.TopicPartition;\n import java.util.Collection;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.TopicPartition;\n+import org.springframework.kafka.listener.AbstractMessageListenerContainer;\n+\n+public class KafkaConsumer<T> {\n+\n+  private final AbstractMessageListenerContainer<String, T> springKafkaContainer;\n+\n+  private final ThreadLocal<List<ConsumerRecord<String, T>>> currentBatch = new InheritableThreadLocal<>();\n+  private final ThreadLocal<ConsumerRecord<String, T>> lastAckedBatchRecord = new InheritableThreadLocal<>();\n+  private final ConsumeStrategy<T> consumeStrategy;\n+\n+  public KafkaConsumer(AbstractMessageListenerContainer<String, T> springKafkaContainer, ConsumeStrategy<T> consumeStrategy) {\n+    this.springKafkaContainer = springKafkaContainer;\n+    this.consumeStrategy = consumeStrategy;\n+  }\n+\n+  void start() {\n+    springKafkaContainer.start();\n+  }\n+\n+  void stop(Runnable callback) {\n+    springKafkaContainer.stop(callback);\n+  }\n+\n+  void stop() {\n+    springKafkaContainer.stop();\n+  }\n+\n+  public Collection<TopicPartition> getAssignedPartitions() {\n+    return springKafkaContainer.getAssignedPartitions();\n+  }\n+\n+  public ConsumerRecord<String, T> getLastAckedBatchRecord() {\n+    return lastAckedBatchRecord.get();\n+  }\n \n-public interface KafkaConsumer {\n+  public List<ConsumerRecord<String, T>> getCurrentBatch() {\n+    return currentBatch.get();\n+  }\n \n-  void stopConsumer();\n+  public void setLastAckedBatchRecord(ConsumerRecord<String, T> record) {\n+    lastAckedBatchRecord.set(record);\n+  }\n \n-  Collection<TopicPartition> getAssignedPartitions();\n+  public void onMessagesBatch(List<ConsumerRecord<String, T>> messages, Consumer<?, ?> consumer) {\n+    List<ConsumerRecord<String, T>> sortedBatch = messages.stream().sorted(\n+        Comparator.comparing((Function<ConsumerRecord<String, T>, String>) ConsumerRecord::topic)\n+            .thenComparingInt(ConsumerRecord::partition)\n+            .thenComparingLong(ConsumerRecord::offset)\n+    ).collect(Collectors.toUnmodifiableList());\n \n+    lastAckedBatchRecord.remove();\n+    currentBatch.set(sortedBatch);\n+    Ack<T> ack = new KafkaInternalTopicAck<>(this, consumer);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODYxMTExMA=="}, "originalCommit": {"oid": "39517a063d745afe0e33c80b6883fe649b367fba"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDgzNzMwMg==", "bodyText": "\u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 - \u0434\u0430, \u0435\u0441\u043b\u0438 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u0439 \u0442\u043e\u0436\u0435 \u0441\u043e\u0441\u0442\u043e\u0438\u0442 \u0432 \u0437\u0430\u043f\u0438\u0441\u0438 \u0438\u0445 \u0432 \u0431\u0430\u0437\u0443 - \u043f\u043e\u043b\u0443\u0447\u0438\u043c exactlyOnce", "url": "https://github.com/hhru/nuts-and-bolts/pull/240#discussion_r420837302", "createdAt": "2020-05-06T14:30:01Z", "author": {"login": "bokshitsky"}, "path": "nab-kafka/src/main/java/ru/hh/nab/kafka/consumer/KafkaConsumer.java", "diffHunk": "@@ -1,12 +1,66 @@\n package ru.hh.nab.kafka.consumer;\n \n-import org.apache.kafka.common.TopicPartition;\n import java.util.Collection;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.TopicPartition;\n+import org.springframework.kafka.listener.AbstractMessageListenerContainer;\n+\n+public class KafkaConsumer<T> {\n+\n+  private final AbstractMessageListenerContainer<String, T> springKafkaContainer;\n+\n+  private final ThreadLocal<List<ConsumerRecord<String, T>>> currentBatch = new InheritableThreadLocal<>();\n+  private final ThreadLocal<ConsumerRecord<String, T>> lastAckedBatchRecord = new InheritableThreadLocal<>();\n+  private final ConsumeStrategy<T> consumeStrategy;\n+\n+  public KafkaConsumer(AbstractMessageListenerContainer<String, T> springKafkaContainer, ConsumeStrategy<T> consumeStrategy) {\n+    this.springKafkaContainer = springKafkaContainer;\n+    this.consumeStrategy = consumeStrategy;\n+  }\n+\n+  void start() {\n+    springKafkaContainer.start();\n+  }\n+\n+  void stop(Runnable callback) {\n+    springKafkaContainer.stop(callback);\n+  }\n+\n+  void stop() {\n+    springKafkaContainer.stop();\n+  }\n+\n+  public Collection<TopicPartition> getAssignedPartitions() {\n+    return springKafkaContainer.getAssignedPartitions();\n+  }\n+\n+  public ConsumerRecord<String, T> getLastAckedBatchRecord() {\n+    return lastAckedBatchRecord.get();\n+  }\n \n-public interface KafkaConsumer {\n+  public List<ConsumerRecord<String, T>> getCurrentBatch() {\n+    return currentBatch.get();\n+  }\n \n-  void stopConsumer();\n+  public void setLastAckedBatchRecord(ConsumerRecord<String, T> record) {\n+    lastAckedBatchRecord.set(record);\n+  }\n \n-  Collection<TopicPartition> getAssignedPartitions();\n+  public void onMessagesBatch(List<ConsumerRecord<String, T>> messages, Consumer<?, ?> consumer) {\n+    List<ConsumerRecord<String, T>> sortedBatch = messages.stream().sorted(\n+        Comparator.comparing((Function<ConsumerRecord<String, T>, String>) ConsumerRecord::topic)\n+            .thenComparingInt(ConsumerRecord::partition)\n+            .thenComparingLong(ConsumerRecord::offset)\n+    ).collect(Collectors.toUnmodifiableList());\n \n+    lastAckedBatchRecord.remove();\n+    currentBatch.set(sortedBatch);\n+    Ack<T> ack = new KafkaInternalTopicAck<>(this, consumer);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODYxMTExMA=="}, "originalCommit": {"oid": "39517a063d745afe0e33c80b6883fe649b367fba"}, "originalPosition": 67}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwNDc3ODM1OnYy", "diffSide": "LEFT", "path": "nab-kafka/src/main/java/ru/hh/nab/kafka/consumer/DefaultConsumerFactory.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQxNjoxNjozMFrOGPOLZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQxNToxNDoyMlrOGRYBOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODYxNDExNw==", "bodyText": "\u041f\u0440\u043e\u0431\u043b\u0435\u043c\u0430 \u044d\u0442\u043e\u0439 \u0448\u0442\u0443\u043a\u0438 \u0431\u044b\u043b\u0430 \u0432 \u0442\u043e\u043c, \u0447\u0442\u043e \u0434\u0430\u0436\u0435 \u0441\u0434\u0435\u043b\u0430\u0432 \u0433\u043b\u043e\u0431\u0430\u043b\u044c\u043d\u044b\u0439 Ack (\u0437\u0430\u043f\u0438\u0441\u0430\u0432 \u043e\u0444\u0444\u0441\u0435\u0442\u044b \u0432 \u043a\u0430\u0444\u043a\u0443) - \u043b\u043e\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u0443\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u0438 \u043f\u0440\u0438 \u043b\u044e\u0431\u043e\u0439 \u043e\u0448\u0438\u0431\u043a\u0435 \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u043b\u0438\u0441\u044c \u0432 \u043d\u0430\u0447\u0430\u043b\u043e \u0431\u0430\u0442\u0447\u0430 (seek \u0432\u043d\u0443\u0442\u0440\u0438 SeekToCurrentBatchErrorHandler), \u0438 \u043e\u043d\u0438 \u043e\u0431\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u043b\u0438\u0441\u044c \u0441\u043d\u043e\u0432\u0430 (\u0441\u043b\u043e\u043c\u0430\u043d\u043d\u044b\u0439 at-most-once)\n\u0412 \u044d\u0442\u043e\u0439 \u0432\u0435\u0442\u043a\u0435 \u043c\u044b 1) \u0437\u0430\u043c\u0435\u043d\u044f\u0435\u043c \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044e errorHandler - \u0434\u0435\u043b\u0430\u0435\u043c seek \u043d\u0435 \u0432 \u043d\u0430\u0447\u0430\u043b\u043e \u0431\u0430\u0442\u0447\u0430, \u0430 \u043a \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u044e \u0438\u0434\u0443\u0449\u0435\u043c\u0443 \u0441\u0440\u0430\u0437\u0443 \u0437\u0430 \u0442\u0435\u043c, \u0447\u0442\u043e \u043c\u044b \u0430\u043a\u043d\u0443\u043b\u0438) 2) \u0434\u0435\u043b\u0430\u0435\u043c \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u0430\u043a\u043d\u0443\u0442\u044c \u043d\u0435 \u0432\u0435\u0441\u044c \u0431\u0430\u0442\u0447\u0430, \u0430 \u043a\u0430\u043a\u043e\u0435-\u0442\u043e \u043a\u043e\u043d\u043a\u0440\u0435\u043d\u043e\u0435 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u0435 + \u0434\u0435\u043b\u0430\u0435\u043c \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u044d\u0442\u043e \u043b\u043e\u043a\u0430\u043b\u044c\u043d\u043e - \u0447\u0442\u043e \u043f\u043e\u043c\u043e\u0436\u0435\u0442 \u0441\u043d\u0438\u0437\u0438\u0442\u044c \u0434\u0443\u0431\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043f\u0440\u0438 at-least-once \u0441\u0435\u043c\u0430\u043d\u0442\u0438\u043a\u0435", "url": "https://github.com/hhru/nuts-and-bolts/pull/240#discussion_r418614117", "createdAt": "2020-05-01T16:16:30Z", "author": {"login": "bokshitsky"}, "path": "nab-kafka/src/main/java/ru/hh/nab/kafka/consumer/DefaultConsumerFactory.java", "diffHunk": "@@ -38,68 +38,52 @@ public DefaultConsumerFactory(ConfigProvider configProvider,\n     this.statsDSender = statsDSender;\n   }\n \n-  public <T> KafkaConsumer subscribe(String topicName,\n-                                     String operationName,\n-                                     Class<T> messageClass,\n-                                     ConsumeStrategy<T> messageConsumer) {\n-\n+  public <T> KafkaConsumer<T> subscribe(String topicName,\n+                                        String operationName,\n+                                        Class<T> messageClass,\n+                                        ConsumeStrategy<T> consumeStrategy) {\n     ConsumerFactory<String, T> consumerFactory = getSpringConsumerFactory(topicName, messageClass);\n-\n     ConsumerGroupId consumerGroupId = new ConsumerGroupId(configProvider.getServiceName(), topicName, operationName);\n+\n+    ConsumeStrategyInternal<T> consumeStrategyInternal = new ConsumeStrategyInternal<>();\n+\n     ContainerProperties containerProperties = getSpringConsumerContainerProperties(\n         consumerGroupId,\n-        adaptToSpring(monitor(consumerGroupId, messageConsumer))\n+        (BatchConsumerAwareMessageListener<String, T>) consumeStrategyInternal::invokeOnConsumer,\n+        topicName\n     );\n \n-    var container = getSpringMessageListenerContainer(consumerFactory, containerProperties, topicName);\n-    container.start();\n+    SeekToFirstNotAckedMessageErrorHandler<T> errorHandler = getBatchErrorHandler(topicName);\n+    var container = getSpringMessageListenerContainer(consumerFactory, containerProperties, errorHandler);\n \n-    return new KafkaConsumer() {\n-      @Override\n-      public void stopConsumer() {\n-        container.stop();\n-      }\n+    KafkaConsumer<T> kafkaConsumer = new KafkaConsumer<>(container, monitor(consumerGroupId, consumeStrategy));\n+    consumeStrategyInternal.setKafkaConsumer(kafkaConsumer);\n+    errorHandler.setKafkaConsumer(kafkaConsumer);\n \n-      @Override\n-      public Collection<TopicPartition> getAssignedPartitions() {\n-        return container.getAssignedPartitions();\n-      }\n-    };\n+    kafkaConsumer.start();\n+    return kafkaConsumer;\n   }\n \n   private <T> ConsumeStrategy<T> monitor(ConsumerGroupId consumerGroupId, ConsumeStrategy<T> consumeStrategy) {\n     return new MonitoringConsumeStrategy<>(statsDSender, consumerGroupId, consumeStrategy);\n   }\n \n-  private <T> BatchAcknowledgingMessageListener<String, T> adaptToSpring(ConsumeStrategy<T> consumeStrategy) {\n-    return (data, acknowledgment) -> consumeStrategy.onMessagesBatch(data, new Ack() {\n-      @Override\n-      public void acknowledge() {\n-        acknowledgment.acknowledge();\n-      }\n-\n-      @Override\n-      public void nack(int index, long sleep) {\n-        acknowledgment.nack(index, sleep);\n-      }\n-    });\n-  }\n-\n   private <T> ConcurrentMessageListenerContainer<String, T> getSpringMessageListenerContainer(ConsumerFactory<String, T> consumerFactory,\n                                                                                               ContainerProperties containerProperties,\n-                                                                                              String topicName) {\n+                                                                                              BatchErrorHandler errorHandler) {\n     var container = new ConcurrentMessageListenerContainer<>(consumerFactory, containerProperties);\n-    SeekToCurrentBatchErrorHandler errorHandler = new SeekToCurrentBatchErrorHandler();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "39517a063d745afe0e33c80b6883fe649b367fba"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc5NzY5Nw==", "bodyText": "\u0422\u0430\u043a \u043f\u043e\u043d\u0438\u043c\u0430\u044e, \u0447\u0442\u043e \u0442\u0430\u043a \u0441\u0434\u0435\u043b\u0430\u043d\u043e \u043d\u0435\u0441\u043f\u0440\u043e\u0441\u0442\u0430 (\u0442\u043e\u0447\u043d\u0435\u0435 \u043d\u0435 \u0441\u0434\u0435\u043b\u0430\u043d\u043e), \u0432 \u0434\u043e\u043a\u0430\u0445 \u0442\u0430\u043a\u043e\u0435:\n\nthe SeekToCurrentBatchErrorHandler has no mechanism to recover after a certain number of failures. One reason for this is there is no guarantee that, when a batch is redelivered, the batch has the same number of records and/or the redelivered records are in the same order. It is impossible, therefore, to maintain retry state for a batch.", "url": "https://github.com/hhru/nuts-and-bolts/pull/240#discussion_r420797697", "createdAt": "2020-05-06T13:39:27Z", "author": {"login": "Iskuskov"}, "path": "nab-kafka/src/main/java/ru/hh/nab/kafka/consumer/DefaultConsumerFactory.java", "diffHunk": "@@ -38,68 +38,52 @@ public DefaultConsumerFactory(ConfigProvider configProvider,\n     this.statsDSender = statsDSender;\n   }\n \n-  public <T> KafkaConsumer subscribe(String topicName,\n-                                     String operationName,\n-                                     Class<T> messageClass,\n-                                     ConsumeStrategy<T> messageConsumer) {\n-\n+  public <T> KafkaConsumer<T> subscribe(String topicName,\n+                                        String operationName,\n+                                        Class<T> messageClass,\n+                                        ConsumeStrategy<T> consumeStrategy) {\n     ConsumerFactory<String, T> consumerFactory = getSpringConsumerFactory(topicName, messageClass);\n-\n     ConsumerGroupId consumerGroupId = new ConsumerGroupId(configProvider.getServiceName(), topicName, operationName);\n+\n+    ConsumeStrategyInternal<T> consumeStrategyInternal = new ConsumeStrategyInternal<>();\n+\n     ContainerProperties containerProperties = getSpringConsumerContainerProperties(\n         consumerGroupId,\n-        adaptToSpring(monitor(consumerGroupId, messageConsumer))\n+        (BatchConsumerAwareMessageListener<String, T>) consumeStrategyInternal::invokeOnConsumer,\n+        topicName\n     );\n \n-    var container = getSpringMessageListenerContainer(consumerFactory, containerProperties, topicName);\n-    container.start();\n+    SeekToFirstNotAckedMessageErrorHandler<T> errorHandler = getBatchErrorHandler(topicName);\n+    var container = getSpringMessageListenerContainer(consumerFactory, containerProperties, errorHandler);\n \n-    return new KafkaConsumer() {\n-      @Override\n-      public void stopConsumer() {\n-        container.stop();\n-      }\n+    KafkaConsumer<T> kafkaConsumer = new KafkaConsumer<>(container, monitor(consumerGroupId, consumeStrategy));\n+    consumeStrategyInternal.setKafkaConsumer(kafkaConsumer);\n+    errorHandler.setKafkaConsumer(kafkaConsumer);\n \n-      @Override\n-      public Collection<TopicPartition> getAssignedPartitions() {\n-        return container.getAssignedPartitions();\n-      }\n-    };\n+    kafkaConsumer.start();\n+    return kafkaConsumer;\n   }\n \n   private <T> ConsumeStrategy<T> monitor(ConsumerGroupId consumerGroupId, ConsumeStrategy<T> consumeStrategy) {\n     return new MonitoringConsumeStrategy<>(statsDSender, consumerGroupId, consumeStrategy);\n   }\n \n-  private <T> BatchAcknowledgingMessageListener<String, T> adaptToSpring(ConsumeStrategy<T> consumeStrategy) {\n-    return (data, acknowledgment) -> consumeStrategy.onMessagesBatch(data, new Ack() {\n-      @Override\n-      public void acknowledge() {\n-        acknowledgment.acknowledge();\n-      }\n-\n-      @Override\n-      public void nack(int index, long sleep) {\n-        acknowledgment.nack(index, sleep);\n-      }\n-    });\n-  }\n-\n   private <T> ConcurrentMessageListenerContainer<String, T> getSpringMessageListenerContainer(ConsumerFactory<String, T> consumerFactory,\n                                                                                               ContainerProperties containerProperties,\n-                                                                                              String topicName) {\n+                                                                                              BatchErrorHandler errorHandler) {\n     var container = new ConcurrentMessageListenerContainer<>(consumerFactory, containerProperties);\n-    SeekToCurrentBatchErrorHandler errorHandler = new SeekToCurrentBatchErrorHandler();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODYxNDExNw=="}, "originalCommit": {"oid": "39517a063d745afe0e33c80b6883fe649b367fba"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDgzMjM2NQ==", "bodyText": "\u0412\u0420\u043e\u0434\u0435 \u043a\u043e\u043c\u043c\u0435\u043d\u0442 \u0443 \u043d\u0438\u0445 \u0447\u0443\u0442\u043a\u0430 \u043f\u0440\u043e \u0434\u0440\u0443\u0433\u043e\u0435 - \u043f\u0440\u043e \u0442\u043e, \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u0441\u0447\u0438\u0442\u0430\u0442\u044c \u043a\u043e\u043b-\u0432\u043e \u0444\u0435\u0439\u043b\u043e\u0432 \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u043e\u0435", "url": "https://github.com/hhru/nuts-and-bolts/pull/240#discussion_r420832365", "createdAt": "2020-05-06T14:23:47Z", "author": {"login": "bokshitsky"}, "path": "nab-kafka/src/main/java/ru/hh/nab/kafka/consumer/DefaultConsumerFactory.java", "diffHunk": "@@ -38,68 +38,52 @@ public DefaultConsumerFactory(ConfigProvider configProvider,\n     this.statsDSender = statsDSender;\n   }\n \n-  public <T> KafkaConsumer subscribe(String topicName,\n-                                     String operationName,\n-                                     Class<T> messageClass,\n-                                     ConsumeStrategy<T> messageConsumer) {\n-\n+  public <T> KafkaConsumer<T> subscribe(String topicName,\n+                                        String operationName,\n+                                        Class<T> messageClass,\n+                                        ConsumeStrategy<T> consumeStrategy) {\n     ConsumerFactory<String, T> consumerFactory = getSpringConsumerFactory(topicName, messageClass);\n-\n     ConsumerGroupId consumerGroupId = new ConsumerGroupId(configProvider.getServiceName(), topicName, operationName);\n+\n+    ConsumeStrategyInternal<T> consumeStrategyInternal = new ConsumeStrategyInternal<>();\n+\n     ContainerProperties containerProperties = getSpringConsumerContainerProperties(\n         consumerGroupId,\n-        adaptToSpring(monitor(consumerGroupId, messageConsumer))\n+        (BatchConsumerAwareMessageListener<String, T>) consumeStrategyInternal::invokeOnConsumer,\n+        topicName\n     );\n \n-    var container = getSpringMessageListenerContainer(consumerFactory, containerProperties, topicName);\n-    container.start();\n+    SeekToFirstNotAckedMessageErrorHandler<T> errorHandler = getBatchErrorHandler(topicName);\n+    var container = getSpringMessageListenerContainer(consumerFactory, containerProperties, errorHandler);\n \n-    return new KafkaConsumer() {\n-      @Override\n-      public void stopConsumer() {\n-        container.stop();\n-      }\n+    KafkaConsumer<T> kafkaConsumer = new KafkaConsumer<>(container, monitor(consumerGroupId, consumeStrategy));\n+    consumeStrategyInternal.setKafkaConsumer(kafkaConsumer);\n+    errorHandler.setKafkaConsumer(kafkaConsumer);\n \n-      @Override\n-      public Collection<TopicPartition> getAssignedPartitions() {\n-        return container.getAssignedPartitions();\n-      }\n-    };\n+    kafkaConsumer.start();\n+    return kafkaConsumer;\n   }\n \n   private <T> ConsumeStrategy<T> monitor(ConsumerGroupId consumerGroupId, ConsumeStrategy<T> consumeStrategy) {\n     return new MonitoringConsumeStrategy<>(statsDSender, consumerGroupId, consumeStrategy);\n   }\n \n-  private <T> BatchAcknowledgingMessageListener<String, T> adaptToSpring(ConsumeStrategy<T> consumeStrategy) {\n-    return (data, acknowledgment) -> consumeStrategy.onMessagesBatch(data, new Ack() {\n-      @Override\n-      public void acknowledge() {\n-        acknowledgment.acknowledge();\n-      }\n-\n-      @Override\n-      public void nack(int index, long sleep) {\n-        acknowledgment.nack(index, sleep);\n-      }\n-    });\n-  }\n-\n   private <T> ConcurrentMessageListenerContainer<String, T> getSpringMessageListenerContainer(ConsumerFactory<String, T> consumerFactory,\n                                                                                               ContainerProperties containerProperties,\n-                                                                                              String topicName) {\n+                                                                                              BatchErrorHandler errorHandler) {\n     var container = new ConcurrentMessageListenerContainer<>(consumerFactory, containerProperties);\n-    SeekToCurrentBatchErrorHandler errorHandler = new SeekToCurrentBatchErrorHandler();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODYxNDExNw=="}, "originalCommit": {"oid": "39517a063d745afe0e33c80b6883fe649b367fba"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDgzOTM3OQ==", "bodyText": "\u044d\u0442\u043e \u0434\u0430, \u044f \u0441\u043a\u043e\u0440\u0435\u0435 \u043f\u0440\u043e \u0441\u0430\u043c\u0443 \u043c\u0435\u0445\u0430\u043d\u0438\u043a\u0443", "url": "https://github.com/hhru/nuts-and-bolts/pull/240#discussion_r420839379", "createdAt": "2020-05-06T14:32:45Z", "author": {"login": "Iskuskov"}, "path": "nab-kafka/src/main/java/ru/hh/nab/kafka/consumer/DefaultConsumerFactory.java", "diffHunk": "@@ -38,68 +38,52 @@ public DefaultConsumerFactory(ConfigProvider configProvider,\n     this.statsDSender = statsDSender;\n   }\n \n-  public <T> KafkaConsumer subscribe(String topicName,\n-                                     String operationName,\n-                                     Class<T> messageClass,\n-                                     ConsumeStrategy<T> messageConsumer) {\n-\n+  public <T> KafkaConsumer<T> subscribe(String topicName,\n+                                        String operationName,\n+                                        Class<T> messageClass,\n+                                        ConsumeStrategy<T> consumeStrategy) {\n     ConsumerFactory<String, T> consumerFactory = getSpringConsumerFactory(topicName, messageClass);\n-\n     ConsumerGroupId consumerGroupId = new ConsumerGroupId(configProvider.getServiceName(), topicName, operationName);\n+\n+    ConsumeStrategyInternal<T> consumeStrategyInternal = new ConsumeStrategyInternal<>();\n+\n     ContainerProperties containerProperties = getSpringConsumerContainerProperties(\n         consumerGroupId,\n-        adaptToSpring(monitor(consumerGroupId, messageConsumer))\n+        (BatchConsumerAwareMessageListener<String, T>) consumeStrategyInternal::invokeOnConsumer,\n+        topicName\n     );\n \n-    var container = getSpringMessageListenerContainer(consumerFactory, containerProperties, topicName);\n-    container.start();\n+    SeekToFirstNotAckedMessageErrorHandler<T> errorHandler = getBatchErrorHandler(topicName);\n+    var container = getSpringMessageListenerContainer(consumerFactory, containerProperties, errorHandler);\n \n-    return new KafkaConsumer() {\n-      @Override\n-      public void stopConsumer() {\n-        container.stop();\n-      }\n+    KafkaConsumer<T> kafkaConsumer = new KafkaConsumer<>(container, monitor(consumerGroupId, consumeStrategy));\n+    consumeStrategyInternal.setKafkaConsumer(kafkaConsumer);\n+    errorHandler.setKafkaConsumer(kafkaConsumer);\n \n-      @Override\n-      public Collection<TopicPartition> getAssignedPartitions() {\n-        return container.getAssignedPartitions();\n-      }\n-    };\n+    kafkaConsumer.start();\n+    return kafkaConsumer;\n   }\n \n   private <T> ConsumeStrategy<T> monitor(ConsumerGroupId consumerGroupId, ConsumeStrategy<T> consumeStrategy) {\n     return new MonitoringConsumeStrategy<>(statsDSender, consumerGroupId, consumeStrategy);\n   }\n \n-  private <T> BatchAcknowledgingMessageListener<String, T> adaptToSpring(ConsumeStrategy<T> consumeStrategy) {\n-    return (data, acknowledgment) -> consumeStrategy.onMessagesBatch(data, new Ack() {\n-      @Override\n-      public void acknowledge() {\n-        acknowledgment.acknowledge();\n-      }\n-\n-      @Override\n-      public void nack(int index, long sleep) {\n-        acknowledgment.nack(index, sleep);\n-      }\n-    });\n-  }\n-\n   private <T> ConcurrentMessageListenerContainer<String, T> getSpringMessageListenerContainer(ConsumerFactory<String, T> consumerFactory,\n                                                                                               ContainerProperties containerProperties,\n-                                                                                              String topicName) {\n+                                                                                              BatchErrorHandler errorHandler) {\n     var container = new ConcurrentMessageListenerContainer<>(consumerFactory, containerProperties);\n-    SeekToCurrentBatchErrorHandler errorHandler = new SeekToCurrentBatchErrorHandler();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODYxNDExNw=="}, "originalCommit": {"oid": "39517a063d745afe0e33c80b6883fe649b367fba"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDg3MjUwNg==", "bodyText": "\u041d\u0443 \u0441\u0443\u0434\u044f \u043f\u043e \u0442\u0435\u0441\u0442\u0430\u043c - \u043c\u0435\u0445\u0430\u043d\u0438\u043a\u0430 \u0443 \u043d\u0430\u0441 \u0432\u0440\u043e\u0434\u0435 \u0440\u0430\u0431\u043e\u0447\u0430\u044f", "url": "https://github.com/hhru/nuts-and-bolts/pull/240#discussion_r420872506", "createdAt": "2020-05-06T15:14:22Z", "author": {"login": "bokshitsky"}, "path": "nab-kafka/src/main/java/ru/hh/nab/kafka/consumer/DefaultConsumerFactory.java", "diffHunk": "@@ -38,68 +38,52 @@ public DefaultConsumerFactory(ConfigProvider configProvider,\n     this.statsDSender = statsDSender;\n   }\n \n-  public <T> KafkaConsumer subscribe(String topicName,\n-                                     String operationName,\n-                                     Class<T> messageClass,\n-                                     ConsumeStrategy<T> messageConsumer) {\n-\n+  public <T> KafkaConsumer<T> subscribe(String topicName,\n+                                        String operationName,\n+                                        Class<T> messageClass,\n+                                        ConsumeStrategy<T> consumeStrategy) {\n     ConsumerFactory<String, T> consumerFactory = getSpringConsumerFactory(topicName, messageClass);\n-\n     ConsumerGroupId consumerGroupId = new ConsumerGroupId(configProvider.getServiceName(), topicName, operationName);\n+\n+    ConsumeStrategyInternal<T> consumeStrategyInternal = new ConsumeStrategyInternal<>();\n+\n     ContainerProperties containerProperties = getSpringConsumerContainerProperties(\n         consumerGroupId,\n-        adaptToSpring(monitor(consumerGroupId, messageConsumer))\n+        (BatchConsumerAwareMessageListener<String, T>) consumeStrategyInternal::invokeOnConsumer,\n+        topicName\n     );\n \n-    var container = getSpringMessageListenerContainer(consumerFactory, containerProperties, topicName);\n-    container.start();\n+    SeekToFirstNotAckedMessageErrorHandler<T> errorHandler = getBatchErrorHandler(topicName);\n+    var container = getSpringMessageListenerContainer(consumerFactory, containerProperties, errorHandler);\n \n-    return new KafkaConsumer() {\n-      @Override\n-      public void stopConsumer() {\n-        container.stop();\n-      }\n+    KafkaConsumer<T> kafkaConsumer = new KafkaConsumer<>(container, monitor(consumerGroupId, consumeStrategy));\n+    consumeStrategyInternal.setKafkaConsumer(kafkaConsumer);\n+    errorHandler.setKafkaConsumer(kafkaConsumer);\n \n-      @Override\n-      public Collection<TopicPartition> getAssignedPartitions() {\n-        return container.getAssignedPartitions();\n-      }\n-    };\n+    kafkaConsumer.start();\n+    return kafkaConsumer;\n   }\n \n   private <T> ConsumeStrategy<T> monitor(ConsumerGroupId consumerGroupId, ConsumeStrategy<T> consumeStrategy) {\n     return new MonitoringConsumeStrategy<>(statsDSender, consumerGroupId, consumeStrategy);\n   }\n \n-  private <T> BatchAcknowledgingMessageListener<String, T> adaptToSpring(ConsumeStrategy<T> consumeStrategy) {\n-    return (data, acknowledgment) -> consumeStrategy.onMessagesBatch(data, new Ack() {\n-      @Override\n-      public void acknowledge() {\n-        acknowledgment.acknowledge();\n-      }\n-\n-      @Override\n-      public void nack(int index, long sleep) {\n-        acknowledgment.nack(index, sleep);\n-      }\n-    });\n-  }\n-\n   private <T> ConcurrentMessageListenerContainer<String, T> getSpringMessageListenerContainer(ConsumerFactory<String, T> consumerFactory,\n                                                                                               ContainerProperties containerProperties,\n-                                                                                              String topicName) {\n+                                                                                              BatchErrorHandler errorHandler) {\n     var container = new ConcurrentMessageListenerContainer<>(consumerFactory, containerProperties);\n-    SeekToCurrentBatchErrorHandler errorHandler = new SeekToCurrentBatchErrorHandler();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODYxNDExNw=="}, "originalCommit": {"oid": "39517a063d745afe0e33c80b6883fe649b367fba"}, "originalPosition": 101}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwNDc4NzU2OnYy", "diffSide": "RIGHT", "path": "nab-kafka/src/main/java/ru/hh/nab/kafka/consumer/Ack.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQxNjoyMDoxMFrOGPORTA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQxMjozNDoxMFrOGRQzOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODYxNTYyOA==", "bodyText": "\u043d\u0430\u0437\u0432\u0430\u043b seek \u0432 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0438\u0435 \u0441 \u0442\u0435\u043c, \u0447\u0442\u043e \u0434\u0435\u043b\u0430\u0435\u043c \u043d\u0430\u0442\u0438\u0432\u043d\u044b\u0439 consumer\n\u043f\u043e \u0444\u0430\u043a\u0442\u0443 \u044d\u0442\u0430 \u0448\u0442\u0443\u043a\u0430 \u043c\u043e\u0436\u0435\u0442 \u043f\u0440\u0438\u0433\u043e\u0434\u0438\u0442\u044c\u0441\u044f, \u043a\u043e\u0433\u0434\u0430 \u0445\u043e\u0442\u0438\u043c \u0441\u0434\u0435\u043b\u0430\u0442\u044c atLeastOnce:\nhandle batch{\n  batch.forEach(m->{\n    process(m)\n   *ack.seek(m)*\n})\nack.acknowledge()\n}\n\n\u0422.\u0435. \u0433\u043b\u043e\u0431\u0430\u043b\u044c\u043d\u044b\u0439 ack \u0432 \u043a\u0430\u0444\u043a\u0443 \u043c\u044b \u043d\u0435 \u0434\u0435\u043b\u0430\u0435\u043c, \u043d\u043e \u0441\u0434\u0432\u0438\u0433\u0430\u0435\u043c \u0443\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u044c \u0432 \u043f\u0430\u043c\u044f\u0442\u0438 \u0442\u0435\u043a\u0443\u0449\u0435\u0433\u043e \u043a\u043e\u043d\u0441\u0443\u043c\u0435\u0440\u0430 - \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u0435 \u0437\u0430\u0434\u0443\u0431\u043b\u0438\u0440\u0443\u0435\u0442\u0441\u044f \u0442\u043e\u043b\u044c\u043a\u043e \u0435\u0441\u043b\u0438 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u0435 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u0435 \u0441\u043e\u0432\u0441\u0435\u043c \u0441\u043b\u043e\u043c\u0430\u043d\u043d\u043e\u0435 \u0438 \u043d\u0435 \u0447\u0438\u043d\u0438\u0442\u0441\u044f (\u0430 \u043d\u0435 \u043b\u043e\u0432\u0438\u0442 \u0442\u0430\u0439\u043c\u0430\u0443\u0442 \u0433\u0434\u0435-\u043d\u0438\u0431\u0443\u0434\u044c)", "url": "https://github.com/hhru/nuts-and-bolts/pull/240#discussion_r418615628", "createdAt": "2020-05-01T16:20:10Z", "author": {"login": "bokshitsky"}, "path": "nab-kafka/src/main/java/ru/hh/nab/kafka/consumer/Ack.java", "diffHunk": "@@ -1,26 +1,13 @@\n package ru.hh.nab.kafka.consumer;\n \n-public interface Ack {\n-  /**\n-   * Invoked when the record or batch for which the acknowledgment has been created has\n-   * been processed. Calling this method implies that all the previous messages in the\n-   * partition have been processed already.\n-   */\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+\n+public interface Ack<T> {\n+\n   void acknowledge();\n \n-  /**\n-   * Negatively acknowledge the record at an index in a batch - commit the offset(s) of\n-   * records before the index and re-seek the partitions so that the record at the index\n-   * and subsequent records will be redelivered after the sleep time. Must be called on\n-   * the consumer thread.\n-   * <p>\n-   * <b>When using group management,\n-   * {@code sleep + time spent processing the records before the index} must be less\n-   * than the consumer {@code max.poll.interval.ms} property, to avoid a rebalance.</b>\n-   *\n-   * @param index the index of the failed record in the batch.\n-   * @param sleepMs the time to sleep.\n-   * @since 2.3\n-   */\n-  void nack(int index, long sleepMs);\n+  void acknowledge(ConsumerRecord<String, T> lastProcessedRecord);\n+\n+  void seek(ConsumerRecord<String, T> lastProcessedRecord);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "39517a063d745afe0e33c80b6883fe649b367fba"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc1NDIzMg==", "bodyText": "\u043d\u0430 \u0441\u0430\u043c\u043e\u043c \u0434\u0435\u043b\u0435 \u043e\u043d\u043e \u043d\u0435 \u0440\u0430\u0431\u043e\u0442\u0430\u043b\u043e \u0434\u0430\u0436\u0435 \u043a\u043e\u0433\u0434\u0430 \u043f\u044b\u0442\u0430\u043b\u0438\u0441\u044c \u0441\u0434\u0435\u043b\u0430\u0442\u044c atMostOnce \u0432 \u0432\u0438\u0434\u0435\nhandle batch{\n  ack.acknowledge()\n  batch.forEach(m->{\n    process(m)\n})\n}", "url": "https://github.com/hhru/nuts-and-bolts/pull/240#discussion_r420754232", "createdAt": "2020-05-06T12:34:10Z", "author": {"login": "Iskuskov"}, "path": "nab-kafka/src/main/java/ru/hh/nab/kafka/consumer/Ack.java", "diffHunk": "@@ -1,26 +1,13 @@\n package ru.hh.nab.kafka.consumer;\n \n-public interface Ack {\n-  /**\n-   * Invoked when the record or batch for which the acknowledgment has been created has\n-   * been processed. Calling this method implies that all the previous messages in the\n-   * partition have been processed already.\n-   */\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+\n+public interface Ack<T> {\n+\n   void acknowledge();\n \n-  /**\n-   * Negatively acknowledge the record at an index in a batch - commit the offset(s) of\n-   * records before the index and re-seek the partitions so that the record at the index\n-   * and subsequent records will be redelivered after the sleep time. Must be called on\n-   * the consumer thread.\n-   * <p>\n-   * <b>When using group management,\n-   * {@code sleep + time spent processing the records before the index} must be less\n-   * than the consumer {@code max.poll.interval.ms} property, to avoid a rebalance.</b>\n-   *\n-   * @param index the index of the failed record in the batch.\n-   * @param sleepMs the time to sleep.\n-   * @since 2.3\n-   */\n-  void nack(int index, long sleepMs);\n+  void acknowledge(ConsumerRecord<String, T> lastProcessedRecord);\n+\n+  void seek(ConsumerRecord<String, T> lastProcessedRecord);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODYxNTYyOA=="}, "originalCommit": {"oid": "39517a063d745afe0e33c80b6883fe649b367fba"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwNDc5ODY3OnYy", "diffSide": "RIGHT", "path": "nab-tests/src/test/java/ru/hh/nab/kafka/consumer/ConsumerRecoveryAfterFailTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQxNjoyNDoyOVrOGPOYWw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQxNjoyNDoyOVrOGPOYWw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODYxNzQzNQ==", "bodyText": "\u041d\u0435\u0441\u043c\u043e\u0442\u0440\u044f \u043d\u0430 \u0442\u0430\u0439\u043c\u0430\u0443\u0442\u044b \u0432\u044b\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u0435 \u0432 \u0442\u0435\u0441\u0442\u0430\u0445, \u0442\u0435\u0441\u0442\u044b \u0432 \u0446\u0435\u043b\u043e\u043c \u0431\u0435\u0433\u0443\u0442 \u0431\u044b\u0441\u0442\u0440\u043e", "url": "https://github.com/hhru/nuts-and-bolts/pull/240#discussion_r418617435", "createdAt": "2020-05-01T16:24:29Z", "author": {"login": "bokshitsky"}, "path": "nab-tests/src/test/java/ru/hh/nab/kafka/consumer/ConsumerRecoveryAfterFailTest.java", "diffHunk": "@@ -0,0 +1,232 @@\n+package ru.hh.nab.kafka.consumer;\n+\n+import java.util.ArrayList;\n+import static java.util.Collections.synchronizedList;\n+import static java.util.Collections.synchronizedSet;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.function.Function;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static java.util.stream.Collectors.toSet;\n+import java.util.stream.Stream;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import static org.awaitility.Awaitility.await;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNotEquals;\n+import static org.junit.Assert.assertTrue;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.test.context.ContextConfiguration;\n+import ru.hh.nab.kafka.KafkaTestConfig;\n+\n+@ContextConfiguration(classes = {KafkaTestConfig.class})\n+public class ConsumerRecoveryAfterFailTest extends KafkaConsumerTestbase {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "39517a063d745afe0e33c80b6883fe649b367fba"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwNDgxMjg0OnYy", "diffSide": "RIGHT", "path": "nab-tests/src/test/java/ru/hh/nab/kafka/KafkaTestConfig.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQxNjoyOTozNVrOGPOhag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQxNjoyOTozNVrOGPOhag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODYxOTc1NA==", "bodyText": "\u0415\u0441\u043b\u0438 \u0442\u0435\u0441\u0442\u0438\u0442\u044c \u043d\u0430 1-\u0439 \u043f\u0430\u0440\u0442\u0438\u0446\u0438\u0438 - \u043c\u043e\u0436\u043d\u043e \u043d\u0435 \u043f\u043e\u0439\u043c\u0430\u0442\u044c \u043a\u0430\u043a\u0438\u0445-\u0442\u043e \u043f\u0440\u043e\u0431\u043b\u0435\u043c", "url": "https://github.com/hhru/nuts-and-bolts/pull/240#discussion_r418619754", "createdAt": "2020-05-01T16:29:35Z", "author": {"login": "bokshitsky"}, "path": "nab-tests/src/test/java/ru/hh/nab/kafka/KafkaTestConfig.java", "diffHunk": "@@ -25,7 +26,7 @@\n \n   @Bean\n   public TestKafkaWithJsonMessages testKafka() {\n-    return KafkaTestUtils.startKafkaWithJsonMessages(new ObjectMapper());\n+    return KafkaTestUtils.startKafkaWithJsonMessages(new ObjectMapper(), Map.of(\"num.partitions\", \"5\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "39517a063d745afe0e33c80b6883fe649b367fba"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxODk3MDgwOnYy", "diffSide": "RIGHT", "path": "nab-tests/src/test/java/ru/hh/nab/kafka/consumer/KafkaConsumerTestbase.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQxMDo1ODowMFrOGRNwqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQxMDo1ODowMFrOGRNwqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDcwNDQyNQ==", "bodyText": "\u043d\u0435 \u0445\u043e\u0447\u0435\u0448\u044c TestBase?", "url": "https://github.com/hhru/nuts-and-bolts/pull/240#discussion_r420704425", "createdAt": "2020-05-06T10:58:00Z", "author": {"login": "Iskuskov"}, "path": "nab-tests/src/test/java/ru/hh/nab/kafka/consumer/KafkaConsumerTestbase.java", "diffHunk": "@@ -0,0 +1,40 @@\n+package ru.hh.nab.kafka.consumer;\n+\n+import java.util.UUID;\n+import java.util.concurrent.TimeUnit;\n+import javax.inject.Inject;\n+import static org.awaitility.Awaitility.await;\n+import static org.junit.Assert.assertEquals;\n+import org.junit.Before;\n+import org.springframework.test.context.junit4.AbstractJUnit4SpringContextTests;\n+import ru.hh.kafka.test.TestKafkaWithJsonMessages;\n+\n+public abstract class KafkaConsumerTestbase extends AbstractJUnit4SpringContextTests {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "39517a063d745afe0e33c80b6883fe649b367fba"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxODk5ODk3OnYy", "diffSide": "RIGHT", "path": "nab-tests/src/test/java/ru/hh/nab/kafka/consumer/ConsumerRecoveryAfterFailTest.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQxMTowNjo1N1rOGROB8g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQxNToxNToxNlrOGRYEFA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDcwODg1MA==", "bodyText": "\u043f\u043e \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u044f\u043c \u0442\u0435\u0441\u0442\u043e\u0432 - \u043e\u0431\u044b\u0447\u043d\u043e \u0432\u0440\u043e\u0434\u0435 \u0432 should-\u043d\u043e\u0442\u0430\u0446\u0438\u0438 (\u0440\u044f\u0434\u043e\u043c \u0432 KafkaConsumerFactoryTest \u0442\u043e\u0436\u0435 \u0442\u0430\u043a)", "url": "https://github.com/hhru/nuts-and-bolts/pull/240#discussion_r420708850", "createdAt": "2020-05-06T11:06:57Z", "author": {"login": "Iskuskov"}, "path": "nab-tests/src/test/java/ru/hh/nab/kafka/consumer/ConsumerRecoveryAfterFailTest.java", "diffHunk": "@@ -0,0 +1,232 @@\n+package ru.hh.nab.kafka.consumer;\n+\n+import java.util.ArrayList;\n+import static java.util.Collections.synchronizedList;\n+import static java.util.Collections.synchronizedSet;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.function.Function;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static java.util.stream.Collectors.toSet;\n+import java.util.stream.Stream;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import static org.awaitility.Awaitility.await;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNotEquals;\n+import static org.junit.Assert.assertTrue;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.test.context.ContextConfiguration;\n+import ru.hh.nab.kafka.KafkaTestConfig;\n+\n+@ContextConfiguration(classes = {KafkaTestConfig.class})\n+public class ConsumerRecoveryAfterFailTest extends KafkaConsumerTestbase {\n+\n+  private static AtomicInteger ID_SEQUENCE = new AtomicInteger(0);\n+  private List<String> processedMessages;\n+  private KafkaConsumer<String> consumer;\n+\n+  @Before\n+  public void setUp() {\n+    processedMessages = synchronizedList(new ArrayList<>());\n+  }\n+\n+  @Test\n+  public void testNoGlobalAckPerformed() throws InterruptedException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "39517a063d745afe0e33c80b6883fe649b367fba"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDgwMTU0NQ==", "bodyText": "\u0441\u043c\u043e\u0442\u0440\u0438 \u0441\u0430\u043c, \u043a\u0430\u043a \u0443\u0434\u043e\u0431\u043d\u0435\u0435", "url": "https://github.com/hhru/nuts-and-bolts/pull/240#discussion_r420801545", "createdAt": "2020-05-06T13:44:24Z", "author": {"login": "Iskuskov"}, "path": "nab-tests/src/test/java/ru/hh/nab/kafka/consumer/ConsumerRecoveryAfterFailTest.java", "diffHunk": "@@ -0,0 +1,232 @@\n+package ru.hh.nab.kafka.consumer;\n+\n+import java.util.ArrayList;\n+import static java.util.Collections.synchronizedList;\n+import static java.util.Collections.synchronizedSet;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.function.Function;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static java.util.stream.Collectors.toSet;\n+import java.util.stream.Stream;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import static org.awaitility.Awaitility.await;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNotEquals;\n+import static org.junit.Assert.assertTrue;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.test.context.ContextConfiguration;\n+import ru.hh.nab.kafka.KafkaTestConfig;\n+\n+@ContextConfiguration(classes = {KafkaTestConfig.class})\n+public class ConsumerRecoveryAfterFailTest extends KafkaConsumerTestbase {\n+\n+  private static AtomicInteger ID_SEQUENCE = new AtomicInteger(0);\n+  private List<String> processedMessages;\n+  private KafkaConsumer<String> consumer;\n+\n+  @Before\n+  public void setUp() {\n+    processedMessages = synchronizedList(new ArrayList<>());\n+  }\n+\n+  @Test\n+  public void testNoGlobalAckPerformed() throws InterruptedException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDcwODg1MA=="}, "originalCommit": {"oid": "39517a063d745afe0e33c80b6883fe649b367fba"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDg3MzIzNg==", "bodyText": "\u041d\u0443 \u0442\u0443\u0442  \u0441\u043b\u043e\u0436\u043d\u043e \u043a\u043e\u0440\u0440\u0435\u043a\u0442\u043d\u043e should-\u043d\u043e\u0442\u0430\u0446\u0438\u044e \u0441\u043e\u0441\u0442\u0430\u0432\u0438\u0442\u044c - \u0442\u0435\u0441\u0442\u0438\u043c \"\u0442\u0438\u043f\u0430 \u0441\u0435\u043c\u0430\u043d\u0442\u0438\u043a\u0443\"", "url": "https://github.com/hhru/nuts-and-bolts/pull/240#discussion_r420873236", "createdAt": "2020-05-06T15:15:16Z", "author": {"login": "bokshitsky"}, "path": "nab-tests/src/test/java/ru/hh/nab/kafka/consumer/ConsumerRecoveryAfterFailTest.java", "diffHunk": "@@ -0,0 +1,232 @@\n+package ru.hh.nab.kafka.consumer;\n+\n+import java.util.ArrayList;\n+import static java.util.Collections.synchronizedList;\n+import static java.util.Collections.synchronizedSet;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.function.Function;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static java.util.stream.Collectors.toSet;\n+import java.util.stream.Stream;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import static org.awaitility.Awaitility.await;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNotEquals;\n+import static org.junit.Assert.assertTrue;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.test.context.ContextConfiguration;\n+import ru.hh.nab.kafka.KafkaTestConfig;\n+\n+@ContextConfiguration(classes = {KafkaTestConfig.class})\n+public class ConsumerRecoveryAfterFailTest extends KafkaConsumerTestbase {\n+\n+  private static AtomicInteger ID_SEQUENCE = new AtomicInteger(0);\n+  private List<String> processedMessages;\n+  private KafkaConsumer<String> consumer;\n+\n+  @Before\n+  public void setUp() {\n+    processedMessages = synchronizedList(new ArrayList<>());\n+  }\n+\n+  @Test\n+  public void testNoGlobalAckPerformed() throws InterruptedException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDcwODg1MA=="}, "originalCommit": {"oid": "39517a063d745afe0e33c80b6883fe649b367fba"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxOTAwMjUwOnYy", "diffSide": "RIGHT", "path": "nab-tests/src/test/java/ru/hh/nab/kafka/consumer/ConsumerRecoveryAfterFailTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQxMTowODoxMVrOGROEJw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQxMTowODoxMVrOGROEJw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDcwOTQxNQ==", "bodyText": ") {", "url": "https://github.com/hhru/nuts-and-bolts/pull/240#discussion_r420709415", "createdAt": "2020-05-06T11:08:11Z", "author": {"login": "Iskuskov"}, "path": "nab-tests/src/test/java/ru/hh/nab/kafka/consumer/ConsumerRecoveryAfterFailTest.java", "diffHunk": "@@ -0,0 +1,232 @@\n+package ru.hh.nab.kafka.consumer;\n+\n+import java.util.ArrayList;\n+import static java.util.Collections.synchronizedList;\n+import static java.util.Collections.synchronizedSet;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.function.Function;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static java.util.stream.Collectors.toSet;\n+import java.util.stream.Stream;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import static org.awaitility.Awaitility.await;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNotEquals;\n+import static org.junit.Assert.assertTrue;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.test.context.ContextConfiguration;\n+import ru.hh.nab.kafka.KafkaTestConfig;\n+\n+@ContextConfiguration(classes = {KafkaTestConfig.class})\n+public class ConsumerRecoveryAfterFailTest extends KafkaConsumerTestbase {\n+\n+  private static AtomicInteger ID_SEQUENCE = new AtomicInteger(0);\n+  private List<String> processedMessages;\n+  private KafkaConsumer<String> consumer;\n+\n+  @Before\n+  public void setUp() {\n+    processedMessages = synchronizedList(new ArrayList<>());\n+  }\n+\n+  @Test\n+  public void testNoGlobalAckPerformed() throws InterruptedException {\n+    putMessagesIntoKafka(117);\n+\n+    startConsumer((messages, ack) -> messages.forEach(m -> processedMessages.add(m.value())));\n+    assertProcessedMessagesCount(117);\n+\n+    consumeAllRemainingMessages();\n+    assertProcessedMessagesCount(234);\n+    assertUniqueProcessedMessagesCount(117);\n+  }\n+\n+  @Test\n+  public void testSeek() throws InterruptedException {\n+    putMessagesIntoKafka(117);\n+    AtomicBoolean failed = new AtomicBoolean(false);\n+    startConsumer((messages, ack) -> messages.forEach(m -> {\n+      processedMessages.add(m.value());\n+      if (processedMessages.size() == 40){", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "39517a063d745afe0e33c80b6883fe649b367fba"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxOTA3NTIxOnYy", "diffSide": "RIGHT", "path": "nab-kafka/src/main/java/ru/hh/nab/kafka/consumer/DefaultConsumerFactory.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQxMTozMjo1NFrOGROx2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQxMTozMjo1NFrOGROx2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDcyMTExNQ==", "bodyText": "\u041c\u043d\u0435 \u043a\u0430\u0436\u0435\u0442\u0441\u044f \u043c\u043e\u0436\u043d\u043e \u0442\u0443\u0442 \u043f\u0435\u0440\u0435\u043f\u0438\u0441\u0430\u0442\u044c \u0442\u0430\u043a, \u0447\u0442\u043e\u0431\u044b \u043d\u0435 \u043d\u0443\u0436\u043d\u043e \u0431\u044b\u043b\u043e \u0434\u0435\u043b\u0430\u0442\u044c \u043f\u043e\u0442\u043e\u043c setKafkaConsumer, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 \u0432\u043d\u0435\u0441\u044f \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0435 containerProperties \u0432\u043d\u0443\u0442\u0440\u044c \u0442\u0432\u043e\u0435\u0433\u043e KafkaConsumer. \u0417\u0430\u043e\u0434\u043d\u043e \u0431\u0443\u0434\u0435\u0442 \u0447\u0438\u0442\u0430\u0435\u043c\u0435\u0439 \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u043a\u043e\u043d\u0441\u044c\u044e\u043c\u0435\u0440\u0430", "url": "https://github.com/hhru/nuts-and-bolts/pull/240#discussion_r420721115", "createdAt": "2020-05-06T11:32:54Z", "author": {"login": "Aulust"}, "path": "nab-kafka/src/main/java/ru/hh/nab/kafka/consumer/DefaultConsumerFactory.java", "diffHunk": "@@ -38,68 +38,52 @@ public DefaultConsumerFactory(ConfigProvider configProvider,\n     this.statsDSender = statsDSender;\n   }\n \n-  public <T> KafkaConsumer subscribe(String topicName,\n-                                     String operationName,\n-                                     Class<T> messageClass,\n-                                     ConsumeStrategy<T> messageConsumer) {\n-\n+  public <T> KafkaConsumer<T> subscribe(String topicName,\n+                                        String operationName,\n+                                        Class<T> messageClass,\n+                                        ConsumeStrategy<T> consumeStrategy) {\n     ConsumerFactory<String, T> consumerFactory = getSpringConsumerFactory(topicName, messageClass);\n-\n     ConsumerGroupId consumerGroupId = new ConsumerGroupId(configProvider.getServiceName(), topicName, operationName);\n+\n+    ConsumeStrategyInternal<T> consumeStrategyInternal = new ConsumeStrategyInternal<>();\n+\n     ContainerProperties containerProperties = getSpringConsumerContainerProperties(\n         consumerGroupId,\n-        adaptToSpring(monitor(consumerGroupId, messageConsumer))\n+        (BatchConsumerAwareMessageListener<String, T>) consumeStrategyInternal::invokeOnConsumer,\n+        topicName\n     );\n \n-    var container = getSpringMessageListenerContainer(consumerFactory, containerProperties, topicName);\n-    container.start();\n+    SeekToFirstNotAckedMessageErrorHandler<T> errorHandler = getBatchErrorHandler(topicName);\n+    var container = getSpringMessageListenerContainer(consumerFactory, containerProperties, errorHandler);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "39517a063d745afe0e33c80b6883fe649b367fba"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxOTIzNzUzOnYy", "diffSide": "RIGHT", "path": "nab-kafka/src/main/java/ru/hh/nab/kafka/consumer/KafkaInternalTopicAck.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQxMjoyMToyMFrOGRQVuw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQxNTowNToxMVrOGRXlkA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc0NjY4Mw==", "bodyText": "\u0432\u0440\u043e\u0434\u0435 \u0431\u044b \u043d\u0435\u0442 \u0433\u0430\u0440\u0430\u043d\u0442\u0438\u0439, \u0447\u0442\u043e \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u0435 \u0441\u043c\u0435\u0449\u0435\u043d\u0438\u0435 \u043d\u0430 \u0435\u0434\u0438\u043d\u0438\u0446\u0443 \u0431\u043e\u043b\u044c\u0448\u0435", "url": "https://github.com/hhru/nuts-and-bolts/pull/240#discussion_r420746683", "createdAt": "2020-05-06T12:21:20Z", "author": {"login": "Iskuskov"}, "path": "nab-kafka/src/main/java/ru/hh/nab/kafka/consumer/KafkaInternalTopicAck.java", "diffHunk": "@@ -0,0 +1,59 @@\n+package ru.hh.nab.kafka.consumer;\n+\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+\n+public class KafkaInternalTopicAck<T> implements Ack<T> {\n+\n+  private final KafkaConsumer<T> kafkaConsumer;\n+  private final Consumer<?, ?> consumer;\n+  private Integer lastCommittedIndex = null;\n+\n+  public KafkaInternalTopicAck(KafkaConsumer<T> kafkaConsumer,\n+                               Consumer<?, ?> consumer) {\n+    this.kafkaConsumer = kafkaConsumer;\n+    this.consumer = consumer;\n+  }\n+\n+  @Override\n+  public void acknowledge() {\n+    List<ConsumerRecord<String, T>> currentBatch = kafkaConsumer.getCurrentBatch();\n+    if (!currentBatch.isEmpty()) {\n+      ConsumerRecord<String, T> lastRecord = currentBatch.get(currentBatch.size() - 1);\n+      acknowledge(lastRecord);\n+    }\n+  }\n+\n+  @Override\n+  public void acknowledge(ConsumerRecord<String, T> processedMessage) {\n+    List<ConsumerRecord<String, T>> currentBatch = kafkaConsumer.getCurrentBatch();\n+    if (lastCommittedIndex != null && lastCommittedIndex >= currentBatch.size()) {\n+      return;\n+    }\n+\n+    Integer startIndex = Optional.ofNullable(lastCommittedIndex).map(lci -> lci + 1).orElse(0);\n+    LinkedHashMap<TopicPartition, OffsetAndMetadata> offsetsToCommit = Stream.concat(\n+        currentBatch.subList(startIndex, currentBatch.size()).stream().takeWhile(message -> message != processedMessage),\n+        Stream.of(processedMessage)\n+    ).collect(Collectors.toMap(\n+        message -> new TopicPartition(message.topic(), message.partition()),\n+        message -> new OffsetAndMetadata(message.offset() + 1),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "39517a063d745afe0e33c80b6883fe649b367fba"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDgzMzgwNw==", "bodyText": "\u042d\u0442\u043e \u0437\u043d\u0430\u0447\u0438\u0442 \"\u0432 \u0441\u043b\u0435\u0434 \u0440\u0430\u0437 \u043d\u0430\u0447\u0438\u043d\u0430\u0442\u044c \u0437\u0430\u043f\u0440\u0430\u0448\u0438\u0432\u0430\u0442 \u043d\u0430 \u0435\u0434\u0438\u043d\u0438\u0446\u0443 \u0431\u043e\u043b\u044c\u0448\u0435\" (\u0435\u0441\u043b\u0438 \u0442\u0430\u043c \u043d\u0435 \u0431\u0443\u0434\u0435\u0442 \u0437\u0430\u043f\u0438\u0441\u0438 - \u0437\u043d\u0430\u0447\u0438\u0442 \u0431\u0443\u0434\u0435\u0442 \u0431\u0440\u0430\u0442\u044c\u0441\u044f \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0430\u044f)\n\u0412 \u0441\u044b\u0440\u0446\u0430\u0445 springKafka \u043a\u0430\u043a \u0440\u0430\u0437 \u0442\u0430\u043a\u043e\u0435 \u0438 \u043f\u0440\u043e\u0438\u0441\u0445\u043e\u0434\u0438\u0442", "url": "https://github.com/hhru/nuts-and-bolts/pull/240#discussion_r420833807", "createdAt": "2020-05-06T14:25:35Z", "author": {"login": "bokshitsky"}, "path": "nab-kafka/src/main/java/ru/hh/nab/kafka/consumer/KafkaInternalTopicAck.java", "diffHunk": "@@ -0,0 +1,59 @@\n+package ru.hh.nab.kafka.consumer;\n+\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+\n+public class KafkaInternalTopicAck<T> implements Ack<T> {\n+\n+  private final KafkaConsumer<T> kafkaConsumer;\n+  private final Consumer<?, ?> consumer;\n+  private Integer lastCommittedIndex = null;\n+\n+  public KafkaInternalTopicAck(KafkaConsumer<T> kafkaConsumer,\n+                               Consumer<?, ?> consumer) {\n+    this.kafkaConsumer = kafkaConsumer;\n+    this.consumer = consumer;\n+  }\n+\n+  @Override\n+  public void acknowledge() {\n+    List<ConsumerRecord<String, T>> currentBatch = kafkaConsumer.getCurrentBatch();\n+    if (!currentBatch.isEmpty()) {\n+      ConsumerRecord<String, T> lastRecord = currentBatch.get(currentBatch.size() - 1);\n+      acknowledge(lastRecord);\n+    }\n+  }\n+\n+  @Override\n+  public void acknowledge(ConsumerRecord<String, T> processedMessage) {\n+    List<ConsumerRecord<String, T>> currentBatch = kafkaConsumer.getCurrentBatch();\n+    if (lastCommittedIndex != null && lastCommittedIndex >= currentBatch.size()) {\n+      return;\n+    }\n+\n+    Integer startIndex = Optional.ofNullable(lastCommittedIndex).map(lci -> lci + 1).orElse(0);\n+    LinkedHashMap<TopicPartition, OffsetAndMetadata> offsetsToCommit = Stream.concat(\n+        currentBatch.subList(startIndex, currentBatch.size()).stream().takeWhile(message -> message != processedMessage),\n+        Stream.of(processedMessage)\n+    ).collect(Collectors.toMap(\n+        message -> new TopicPartition(message.topic(), message.partition()),\n+        message -> new OffsetAndMetadata(message.offset() + 1),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc0NjY4Mw=="}, "originalCommit": {"oid": "39517a063d745afe0e33c80b6883fe649b367fba"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDg2NTQyNA==", "bodyText": "\u0434\u0430, \u0432\u043e\u0442 \u043f\u0440\u043e \u044d\u0442\u043e \u0438 \u0432\u043e\u043f\u0440\u043e\u0441, \u0442\u043e\u0433\u0434\u0430 \u043e\u043a", "url": "https://github.com/hhru/nuts-and-bolts/pull/240#discussion_r420865424", "createdAt": "2020-05-06T15:05:11Z", "author": {"login": "Iskuskov"}, "path": "nab-kafka/src/main/java/ru/hh/nab/kafka/consumer/KafkaInternalTopicAck.java", "diffHunk": "@@ -0,0 +1,59 @@\n+package ru.hh.nab.kafka.consumer;\n+\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+\n+public class KafkaInternalTopicAck<T> implements Ack<T> {\n+\n+  private final KafkaConsumer<T> kafkaConsumer;\n+  private final Consumer<?, ?> consumer;\n+  private Integer lastCommittedIndex = null;\n+\n+  public KafkaInternalTopicAck(KafkaConsumer<T> kafkaConsumer,\n+                               Consumer<?, ?> consumer) {\n+    this.kafkaConsumer = kafkaConsumer;\n+    this.consumer = consumer;\n+  }\n+\n+  @Override\n+  public void acknowledge() {\n+    List<ConsumerRecord<String, T>> currentBatch = kafkaConsumer.getCurrentBatch();\n+    if (!currentBatch.isEmpty()) {\n+      ConsumerRecord<String, T> lastRecord = currentBatch.get(currentBatch.size() - 1);\n+      acknowledge(lastRecord);\n+    }\n+  }\n+\n+  @Override\n+  public void acknowledge(ConsumerRecord<String, T> processedMessage) {\n+    List<ConsumerRecord<String, T>> currentBatch = kafkaConsumer.getCurrentBatch();\n+    if (lastCommittedIndex != null && lastCommittedIndex >= currentBatch.size()) {\n+      return;\n+    }\n+\n+    Integer startIndex = Optional.ofNullable(lastCommittedIndex).map(lci -> lci + 1).orElse(0);\n+    LinkedHashMap<TopicPartition, OffsetAndMetadata> offsetsToCommit = Stream.concat(\n+        currentBatch.subList(startIndex, currentBatch.size()).stream().takeWhile(message -> message != processedMessage),\n+        Stream.of(processedMessage)\n+    ).collect(Collectors.toMap(\n+        message -> new TopicPartition(message.topic(), message.partition()),\n+        message -> new OffsetAndMetadata(message.offset() + 1),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc0NjY4Mw=="}, "originalCommit": {"oid": "39517a063d745afe0e33c80b6883fe649b367fba"}, "originalPosition": 47}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4234, "cost": 1, "resetAt": "2021-11-12T20:44:06Z"}}}