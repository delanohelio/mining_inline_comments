{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDkwNjkwMzYx", "number": 6044, "title": "Support for multi-threaded Group By reducer for SQL.", "bodyText": "The existing implementation of Broker reduce phase is single-threaded.\nFor group-by queries where large response are being sent back from multiple servers,\nthis could become a bottlenect.\nGiven that brokers are generally light on CPU usage, making the reduce phase\nmulti-threaded would be a good idea to boost performance. This PR adds a multi-threaded\nimplementation for the Group-By reducer for SQL.\n\n\nAdded an executor service in BrokerReduceService that can be used by multi-threaded\nexecution of the broker reduce phase. This is initialized with number of threads as:\nRuntime.getRuntime().availableProcessors().\n\n\nAdded a broker side config to specify max number of threads per query to be used for reduce phase.\npinot.broker.max.reduce.threads. This has a same default value as server side combine phase:\nMath.max(1, Math.min(10, Runtime.getRuntime().availableProcessors() / 2))\nFor reverting to single threaded reduce, set this config to 1.\n\n\nThe GroupByDataTableReducer uses the following algorithm for determining number of\nthreads to use in reduce phase (per query):\n\nIf there are less than 2 data tables to reduce, it uses single threaded run.\nElse, it uses `Math.min(pinot.broker.max.reduce.threads, numDataTables).\n\n\n\nFor testing, explicitly sets num threads to reduce to be > 1 to ensure functional\ncorrectness is tested.\n\n\nDescription\nAdd a description of your PR here.\nA good description should include pointers to an issue or design document, etc.\nUpgrade Notes\nDoes this PR prevent a zero down-time upgrade? (Assume upgrade order: Controller, Broker, Server, Minion)\n\n Yes (Please label as backward-incompat, and complete the section below on Release Notes)\n\nDoes this PR fix a zero-downtime upgrade introduced earlier?\n\n Yes (Please label this as backward-incompat, and complete the section below on Release Notes)\n\nDoes this PR otherwise need attention when creating release notes? Things to consider:\n\nNew configuration options\nDeprecation of configurations\nSignature changes to public methods/interfaces\nNew plugins added or old plugins removed\n\n\n Yes (Please label this PR as release-notes and complete the section on Release Notes)\n\nRelease Notes\nIf you have tagged this as either backward-incompat or release-notes,\nyou MUST add text here that you would like to see appear in release notes of the\nnext release.\nIf you have a series of commits adding or enabling a feature, then\nadd this section only in final commit that marks the feature completed.\nRefer to earlier release notes to see examples of text\nDocumentation\nIf you have introduced a new feature or configuration, please add it to the documentation as well.\nSee https://docs.pinot.apache.org/developers/developers-and-contributors/update-document", "createdAt": "2020-09-22T04:56:18Z", "url": "https://github.com/apache/pinot/pull/6044", "merged": true, "mergeCommit": {"oid": "a910f5da2487a711bd9d4d82a3fe72d951aeb685"}, "closed": true, "closedAt": "2020-10-14T03:20:09Z", "author": {"login": "mayankshriv"}, "timelineItems": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdLQz8OgBqjM3OTE0ODU0NTg=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdSToTAgBqjM4NzQzODg1MTE=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "23671076b5cadfff756baf3cccb78aef4d74c91c", "author": {"user": {"login": "mayankshriv", "name": "Mayank Shrivastava"}}, "url": "https://github.com/apache/pinot/commit/23671076b5cadfff756baf3cccb78aef4d74c91c", "committedDate": "2020-09-22T04:54:34Z", "message": "Support for multi-threaded Group By reducer for SQL.\n\nThe existing implementation of Broker reduce phase is single-threaded.\nFor group-by queries where large response are being sent back from multiple servers,\nthis could become a bottlenect.\n\nGiven that brokers are generally light on CPU usage, making the reduce phase\nmulti-threaded would be a good idea to boost performance. This PR adds a multi-threaded\nimplementation for the Group-By reducer for SQL.\n\n- Added an executor service in BrokerReduceService that can be used by the reduce phase.\n\n- In this PR, the executor service defaults to have a single thread, until the performance\n  impact can be studied under various conditions (eg high qps, where brokers have higher\n  CPU usage).\n\n- Added a broker side config to specify the number of threads to be used for reduce phase.\n  `pinot.broker.num.reduce.threads`\n\n- For testing, explicitly sets num threads to reduce to be > 1 to ensure functional\n  correctness is tested."}, "afterCommit": {"oid": "0192716f7e55f8c0a367b16056c240ceadce64b3", "author": {"user": {"login": "mayankshriv", "name": "Mayank Shrivastava"}}, "url": "https://github.com/apache/pinot/commit/0192716f7e55f8c0a367b16056c240ceadce64b3", "committedDate": "2020-09-22T05:05:22Z", "message": "Support for multi-threaded Group By reducer for SQL.\n\nThe existing implementation of Broker reduce phase is single-threaded.\nFor group-by queries where large response are being sent back from multiple servers,\nthis could become a bottlenect.\n\nGiven that brokers are generally light on CPU usage, making the reduce phase\nmulti-threaded would be a good idea to boost performance. This PR adds a multi-threaded\nimplementation for the Group-By reducer for SQL.\n\n- Added an executor service in BrokerReduceService that can be used by the reduce phase.\n\n- In this PR, the executor service defaults to have a single thread, until the performance\n  impact can be studied under various conditions (eg high qps, where brokers have higher\n  CPU usage).\n\n- Added a broker side config to specify the number of threads to be used for reduce phase.\n  `pinot.broker.num.reduce.threads`\n\n- For testing, explicitly sets num threads to reduce to be > 1 to ensure functional\n  correctness is tested."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "0192716f7e55f8c0a367b16056c240ceadce64b3", "author": {"user": {"login": "mayankshriv", "name": "Mayank Shrivastava"}}, "url": "https://github.com/apache/pinot/commit/0192716f7e55f8c0a367b16056c240ceadce64b3", "committedDate": "2020-09-22T05:05:22Z", "message": "Support for multi-threaded Group By reducer for SQL.\n\nThe existing implementation of Broker reduce phase is single-threaded.\nFor group-by queries where large response are being sent back from multiple servers,\nthis could become a bottlenect.\n\nGiven that brokers are generally light on CPU usage, making the reduce phase\nmulti-threaded would be a good idea to boost performance. This PR adds a multi-threaded\nimplementation for the Group-By reducer for SQL.\n\n- Added an executor service in BrokerReduceService that can be used by the reduce phase.\n\n- In this PR, the executor service defaults to have a single thread, until the performance\n  impact can be studied under various conditions (eg high qps, where brokers have higher\n  CPU usage).\n\n- Added a broker side config to specify the number of threads to be used for reduce phase.\n  `pinot.broker.num.reduce.threads`\n\n- For testing, explicitly sets num threads to reduce to be > 1 to ensure functional\n  correctness is tested."}, "afterCommit": {"oid": "8b72494adf2992c66fd30bafada7acd4b308d6d4", "author": {"user": {"login": "mayankshriv", "name": "Mayank Shrivastava"}}, "url": "https://github.com/apache/pinot/commit/8b72494adf2992c66fd30bafada7acd4b308d6d4", "committedDate": "2020-09-22T05:08:00Z", "message": "Support for multi-threaded Group By reducer for SQL.\n\nThe existing implementation of Broker reduce phase is single-threaded.\nFor group-by queries where large response are being sent back from multiple servers,\nthis could become a bottlenect.\n\nGiven that brokers are generally light on CPU usage, making the reduce phase\nmulti-threaded would be a good idea to boost performance. This PR adds a multi-threaded\nimplementation for the Group-By reducer for SQL.\n\n- Added an executor service in BrokerReduceService that can be used by the reduce phase.\n\n- In this PR, the executor service defaults to have a single thread, until the performance\n  impact can be studied under various conditions (eg high qps, where brokers have higher\n  CPU usage).\n\n- Added a broker side config to specify the number of threads to be used for reduce phase.\n  `pinot.broker.num.reduce.threads`\n\n- For testing, explicitly sets num threads to reduce to be > 1 to ensure functional\n  correctness is tested."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkzMTQ2OTQ4", "url": "https://github.com/apache/pinot/pull/6044#pullrequestreview-493146948", "createdAt": "2020-09-22T06:07:10Z", "commit": {"oid": "8b72494adf2992c66fd30bafada7acd4b308d6d4"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkzNjE3NDAy", "url": "https://github.com/apache/pinot/pull/6044#pullrequestreview-493617402", "createdAt": "2020-09-22T15:59:08Z", "commit": {"oid": "8b72494adf2992c66fd30bafada7acd4b308d6d4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQxNTo1OTowOFrOHWBZ6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQxNTo1OTowOFrOHWBZ6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mjg1MzczNg==", "bodyText": "Actually, on second thought, the default value of 1 is not good, as it will make reduce across concurrent queries as sequential. Moreover, if we add more threads, then it may cause contention in case of high qps use cases.\nWhile we tune this, perhaps the behavior should be:\n\nIf config not explicitly specified, then preserve current behavior without executor service, or perhaps using MoreExecutors.newDirectExecutorService() that uses the calling thread to execute the Runnable.\nIf config specified, use executor service with num threads specified in the config.\n\nThoughts @kishoreg  @Jackie-Jiang ?\n(I have updated the PR with the approach above).", "url": "https://github.com/apache/pinot/pull/6044#discussion_r492853736", "createdAt": "2020-09-22T15:59:08Z", "author": {"login": "mayankshriv"}, "path": "pinot-common/src/main/java/org/apache/pinot/common/utils/CommonConstants.java", "diffHunk": "@@ -161,6 +161,10 @@\n     public static final double DEFAULT_BROKER_MIN_RESOURCE_PERCENT_FOR_START = 100.0;\n     public static final String CONFIG_OF_ENABLE_QUERY_LIMIT_OVERRIDE = \"pinot.broker.enable.query.limit.override\";\n \n+    // Config for number of threads to use for Broker reduce-phase.\n+    public static final String CONFIG_OF_NUM_REDUCE_THREADS = \"pinot.broker.num.reduce.threads\";\n+    public static final int DEFAULT_NUM_REDUCE_THREADS = 1; // TBD: Change to a more appropriate default (eg numCores).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b72494adf2992c66fd30bafada7acd4b308d6d4"}, "originalPosition": 6}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "8b72494adf2992c66fd30bafada7acd4b308d6d4", "author": {"user": {"login": "mayankshriv", "name": "Mayank Shrivastava"}}, "url": "https://github.com/apache/pinot/commit/8b72494adf2992c66fd30bafada7acd4b308d6d4", "committedDate": "2020-09-22T05:08:00Z", "message": "Support for multi-threaded Group By reducer for SQL.\n\nThe existing implementation of Broker reduce phase is single-threaded.\nFor group-by queries where large response are being sent back from multiple servers,\nthis could become a bottlenect.\n\nGiven that brokers are generally light on CPU usage, making the reduce phase\nmulti-threaded would be a good idea to boost performance. This PR adds a multi-threaded\nimplementation for the Group-By reducer for SQL.\n\n- Added an executor service in BrokerReduceService that can be used by the reduce phase.\n\n- In this PR, the executor service defaults to have a single thread, until the performance\n  impact can be studied under various conditions (eg high qps, where brokers have higher\n  CPU usage).\n\n- Added a broker side config to specify the number of threads to be used for reduce phase.\n  `pinot.broker.num.reduce.threads`\n\n- For testing, explicitly sets num threads to reduce to be > 1 to ensure functional\n  correctness is tested."}, "afterCommit": {"oid": "3e13eb46ffb1b57587d7116de043ef79d0a6afe9", "author": {"user": {"login": "mayankshriv", "name": "Mayank Shrivastava"}}, "url": "https://github.com/apache/pinot/commit/3e13eb46ffb1b57587d7116de043ef79d0a6afe9", "committedDate": "2020-09-22T16:22:19Z", "message": "Support for multi-threaded Group By reducer for SQL.\n\nThe existing implementation of Broker reduce phase is single-threaded.\nFor group-by queries where large response are being sent back from multiple servers,\nthis could become a bottlenect.\n\nGiven that brokers are generally light on CPU usage, making the reduce phase\nmulti-threaded would be a good idea to boost performance. This PR adds a multi-threaded\nimplementation for the Group-By reducer for SQL.\n\n- Added an executor service in BrokerReduceService that can be used by multi-threaded\n  execution of the broker reduce phase.\n\n- Added a broker side config to specify the number of threads to be used for reduce phase.\n  `pinot.broker.num.reduce.threads`\n\n- The default behavior in this PR is to use `MoreExecutors.newDirectExecutorService()` which\n  uses the calling thread to execute the `Runnable`, mainting the existing behavior.\n\n- For testing, explicitly sets num threads to reduce to be > 1 to ensure functional\n  correctness is tested."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3e13eb46ffb1b57587d7116de043ef79d0a6afe9", "author": {"user": {"login": "mayankshriv", "name": "Mayank Shrivastava"}}, "url": "https://github.com/apache/pinot/commit/3e13eb46ffb1b57587d7116de043ef79d0a6afe9", "committedDate": "2020-09-22T16:22:19Z", "message": "Support for multi-threaded Group By reducer for SQL.\n\nThe existing implementation of Broker reduce phase is single-threaded.\nFor group-by queries where large response are being sent back from multiple servers,\nthis could become a bottlenect.\n\nGiven that brokers are generally light on CPU usage, making the reduce phase\nmulti-threaded would be a good idea to boost performance. This PR adds a multi-threaded\nimplementation for the Group-By reducer for SQL.\n\n- Added an executor service in BrokerReduceService that can be used by multi-threaded\n  execution of the broker reduce phase.\n\n- Added a broker side config to specify the number of threads to be used for reduce phase.\n  `pinot.broker.num.reduce.threads`\n\n- The default behavior in this PR is to use `MoreExecutors.newDirectExecutorService()` which\n  uses the calling thread to execute the `Runnable`, mainting the existing behavior.\n\n- For testing, explicitly sets num threads to reduce to be > 1 to ensure functional\n  correctness is tested."}, "afterCommit": {"oid": "105fe42f549b6a40313d051250cd2059c537041d", "author": {"user": {"login": "mayankshriv", "name": "Mayank Shrivastava"}}, "url": "https://github.com/apache/pinot/commit/105fe42f549b6a40313d051250cd2059c537041d", "committedDate": "2020-09-22T17:33:38Z", "message": "Support for multi-threaded Group By reducer for SQL.\n\nThe existing implementation of Broker reduce phase is single-threaded.\nFor group-by queries where large response are being sent back from multiple servers,\nthis could become a bottlenect.\n\nGiven that brokers are generally light on CPU usage, making the reduce phase\nmulti-threaded would be a good idea to boost performance. This PR adds a multi-threaded\nimplementation for the Group-By reducer for SQL.\n\n- Added an executor service in BrokerReduceService that can be used by multi-threaded\n  execution of the broker reduce phase.\n\n- Added a broker side config to specify the number of threads to be used for reduce phase.\n  `pinot.broker.num.reduce.threads`\n\n- The default behavior in this PR is to use `MoreExecutors.newDirectExecutorService()` which\n  uses the calling thread to execute the `Runnable`, mainting the existing behavior.\n\n- For testing, explicitly sets num threads to reduce to be > 1 to ensure functional\n  correctness is tested."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "105fe42f549b6a40313d051250cd2059c537041d", "author": {"user": {"login": "mayankshriv", "name": "Mayank Shrivastava"}}, "url": "https://github.com/apache/pinot/commit/105fe42f549b6a40313d051250cd2059c537041d", "committedDate": "2020-09-22T17:33:38Z", "message": "Support for multi-threaded Group By reducer for SQL.\n\nThe existing implementation of Broker reduce phase is single-threaded.\nFor group-by queries where large response are being sent back from multiple servers,\nthis could become a bottlenect.\n\nGiven that brokers are generally light on CPU usage, making the reduce phase\nmulti-threaded would be a good idea to boost performance. This PR adds a multi-threaded\nimplementation for the Group-By reducer for SQL.\n\n- Added an executor service in BrokerReduceService that can be used by multi-threaded\n  execution of the broker reduce phase.\n\n- Added a broker side config to specify the number of threads to be used for reduce phase.\n  `pinot.broker.num.reduce.threads`\n\n- The default behavior in this PR is to use `MoreExecutors.newDirectExecutorService()` which\n  uses the calling thread to execute the `Runnable`, mainting the existing behavior.\n\n- For testing, explicitly sets num threads to reduce to be > 1 to ensure functional\n  correctness is tested."}, "afterCommit": {"oid": "5174dceac6a0b13011a31febcd4b235a2f9bb96c", "author": {"user": {"login": "mayankshriv", "name": "Mayank Shrivastava"}}, "url": "https://github.com/apache/pinot/commit/5174dceac6a0b13011a31febcd4b235a2f9bb96c", "committedDate": "2020-09-24T16:11:26Z", "message": "Support for multi-threaded Group By reducer for SQL.\n\nThe existing implementation of Broker reduce phase is single-threaded.\nFor group-by queries where large response are being sent back from multiple servers,\nthis could become a bottlenect.\n\nGiven that brokers are generally light on CPU usage, making the reduce phase\nmulti-threaded would be a good idea to boost performance. This PR adds a multi-threaded\nimplementation for the Group-By reducer for SQL.\n\n- Added an executor service in BrokerReduceService that can be used by multi-threaded\n  execution of the broker reduce phase.\n\n- Added a broker side config to specify the number of threads to be used for reduce phase.\n  `pinot.broker.num.reduce.threads`\n\n- The default behavior in this PR is to use `MoreExecutors.newDirectExecutorService()` which\n  uses the calling thread to execute the `Runnable`, mainting the existing behavior.\n\n- For testing, explicitly sets num threads to reduce to be > 1 to ensure functional\n  correctness is tested."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5174dceac6a0b13011a31febcd4b235a2f9bb96c", "author": {"user": {"login": "mayankshriv", "name": "Mayank Shrivastava"}}, "url": "https://github.com/apache/pinot/commit/5174dceac6a0b13011a31febcd4b235a2f9bb96c", "committedDate": "2020-09-24T16:11:26Z", "message": "Support for multi-threaded Group By reducer for SQL.\n\nThe existing implementation of Broker reduce phase is single-threaded.\nFor group-by queries where large response are being sent back from multiple servers,\nthis could become a bottlenect.\n\nGiven that brokers are generally light on CPU usage, making the reduce phase\nmulti-threaded would be a good idea to boost performance. This PR adds a multi-threaded\nimplementation for the Group-By reducer for SQL.\n\n- Added an executor service in BrokerReduceService that can be used by multi-threaded\n  execution of the broker reduce phase.\n\n- Added a broker side config to specify the number of threads to be used for reduce phase.\n  `pinot.broker.num.reduce.threads`\n\n- The default behavior in this PR is to use `MoreExecutors.newDirectExecutorService()` which\n  uses the calling thread to execute the `Runnable`, mainting the existing behavior.\n\n- For testing, explicitly sets num threads to reduce to be > 1 to ensure functional\n  correctness is tested."}, "afterCommit": {"oid": "c73604641d5d84656b0e980b95329dc3152c66f7", "author": {"user": {"login": "mayankshriv", "name": "Mayank Shrivastava"}}, "url": "https://github.com/apache/pinot/commit/c73604641d5d84656b0e980b95329dc3152c66f7", "committedDate": "2020-10-08T04:45:01Z", "message": "Support for multi-threaded Group By reducer for SQL.\n\nThe existing implementation of Broker reduce phase is single-threaded.\nFor group-by queries where large response are being sent back from multiple servers,\nthis could become a bottlenect.\n\nGiven that brokers are generally light on CPU usage, making the reduce phase\nmulti-threaded would be a good idea to boost performance. This PR adds a multi-threaded\nimplementation for the Group-By reducer for SQL.\n\n- Added an executor service in BrokerReduceService that can be used by multi-threaded\n  execution of the broker reduce phase. This is initialized with number of threads as:\n  `Runtime.getRuntime().availableProcessors()`.\n\n- Added a broker side config to specify max number of threads per query to be used for reduce phase.\n  `pinot.broker.max.reduce.threads`. This has a same default value as server side combine phase:\n  `Math.max(1, Math.min(10, Runtime.getRuntime().availableProcessors() / 2))`\n\n   For reverting to single threaded reduce, set this config to 1.\n\n- The GroupByDataTableReducer uses the following algorithm for determining number of\n  threads to use in reduce phase (per query):\n  - If there are less than 2 data tables to reduce, it uses single threaded run.\n  - Else, it uses `Math.min(pinot.broker.max.reduce.threads, numDataTables).\n\n- For testing, explicitly sets num threads to reduce to be > 1 to ensure functional\n  correctness is tested."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA2MDUwNjE4", "url": "https://github.com/apache/pinot/pull/6044#pullrequestreview-506050618", "createdAt": "2020-10-10T01:11:40Z", "commit": {"oid": "c73604641d5d84656b0e980b95329dc3152c66f7"}, "state": "COMMENTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMFQwMToxMTo0MFrOHfb-Zg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMFQwMTozOTozNlrOHfcLLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyNjI0Ng==", "bodyText": "This can still be final?", "url": "https://github.com/apache/pinot/pull/6044#discussion_r502726246", "createdAt": "2020-10-10T01:11:40Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-broker/src/main/java/org/apache/pinot/broker/requesthandler/BaseBrokerRequestHandler.java", "diffHunk": "@@ -102,7 +102,7 @@\n \n   protected final AtomicLong _requestIdGenerator = new AtomicLong();\n   protected final BrokerRequestOptimizer _brokerRequestOptimizer = new BrokerRequestOptimizer();\n-  protected final BrokerReduceService _brokerReduceService = new BrokerReduceService();\n+  protected BrokerReduceService _brokerReduceService;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c73604641d5d84656b0e980b95329dc3152c66f7"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyNjkzOQ==", "bodyText": "pinot.broker.max.reduce.threads.per.query for clarity?", "url": "https://github.com/apache/pinot/pull/6044#discussion_r502726939", "createdAt": "2020-10-10T01:18:00Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-common/src/main/java/org/apache/pinot/common/utils/CommonConstants.java", "diffHunk": "@@ -166,6 +166,11 @@\n     public static final double DEFAULT_BROKER_MIN_RESOURCE_PERCENT_FOR_START = 100.0;\n     public static final String CONFIG_OF_ENABLE_QUERY_LIMIT_OVERRIDE = \"pinot.broker.enable.query.limit.override\";\n \n+    // Config for number of threads to use for Broker reduce-phase.\n+    public static final String CONFIG_OF_MAX_REDUCE_THREADS_PER_QUERY = \"pinot.broker.max.reduce.threads\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c73604641d5d84656b0e980b95329dc3152c66f7"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyNzE5Nw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public static final int MAX_REDUCE_THREADS_PER_QUERY =\n          \n          \n            \n                public static final int DEFAULT_MAX_REDUCE_THREADS_PER_QUERY =", "url": "https://github.com/apache/pinot/pull/6044#discussion_r502727197", "createdAt": "2020-10-10T01:20:04Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-common/src/main/java/org/apache/pinot/common/utils/CommonConstants.java", "diffHunk": "@@ -166,6 +166,11 @@\n     public static final double DEFAULT_BROKER_MIN_RESOURCE_PERCENT_FOR_START = 100.0;\n     public static final String CONFIG_OF_ENABLE_QUERY_LIMIT_OVERRIDE = \"pinot.broker.enable.query.limit.override\";\n \n+    // Config for number of threads to use for Broker reduce-phase.\n+    public static final String CONFIG_OF_MAX_REDUCE_THREADS_PER_QUERY = \"pinot.broker.max.reduce.threads\";\n+    public static final int MAX_REDUCE_THREADS_PER_QUERY =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c73604641d5d84656b0e980b95329dc3152c66f7"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyNzM0Ng==", "bodyText": "Log both number or worker threads and threads per query?\nAlso, if it is single-threaded, no need to launch the executor service.", "url": "https://github.com/apache/pinot/pull/6044#discussion_r502727346", "createdAt": "2020-10-10T01:21:28Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/query/reduce/BrokerReduceService.java", "diffHunk": "@@ -47,8 +57,32 @@\n @ThreadSafe\n public class BrokerReduceService {\n \n+  private static final Logger LOGGER = LoggerFactory.getLogger(BrokerReduceService.class);\n+\n+  // brw -> Shorthand for broker reduce worker threads.\n+  private static final String REDUCE_THREAD_NAME_FORMAT = \"brw-%d\";\n+  protected static final int QUERY_RUNNER_THREAD_PRIORITY = 7;\n+\n+  private final ListeningExecutorService _reduceExecutorService;\n+  private final int _maxReduceThreadsPerQuery;\n+\n+  public BrokerReduceService(PinotConfiguration config) {\n+    _maxReduceThreadsPerQuery = config.getProperty(CommonConstants.Broker.CONFIG_OF_MAX_REDUCE_THREADS_PER_QUERY,\n+        CommonConstants.Broker.MAX_REDUCE_THREADS_PER_QUERY);\n+    LOGGER.info(\"Initializing BrokerReduceService with {} reduce threads.\", _maxReduceThreadsPerQuery);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c73604641d5d84656b0e980b95329dc3152c66f7"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyNzQ0NQ==", "bodyText": "Any specific reason for this priority? Some comments will be appreciated", "url": "https://github.com/apache/pinot/pull/6044#discussion_r502727445", "createdAt": "2020-10-10T01:22:21Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/query/reduce/BrokerReduceService.java", "diffHunk": "@@ -47,8 +57,32 @@\n @ThreadSafe\n public class BrokerReduceService {\n \n+  private static final Logger LOGGER = LoggerFactory.getLogger(BrokerReduceService.class);\n+\n+  // brw -> Shorthand for broker reduce worker threads.\n+  private static final String REDUCE_THREAD_NAME_FORMAT = \"brw-%d\";\n+  protected static final int QUERY_RUNNER_THREAD_PRIORITY = 7;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c73604641d5d84656b0e980b95329dc3152c66f7"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyNzY1MA==", "bodyText": "I don't think we need to use the ListeningExecutorService here, ExecutorService should be enough with lower overhead", "url": "https://github.com/apache/pinot/pull/6044#discussion_r502727650", "createdAt": "2020-10-10T01:24:08Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/query/reduce/BrokerReduceService.java", "diffHunk": "@@ -47,8 +57,32 @@\n @ThreadSafe\n public class BrokerReduceService {\n \n+  private static final Logger LOGGER = LoggerFactory.getLogger(BrokerReduceService.class);\n+\n+  // brw -> Shorthand for broker reduce worker threads.\n+  private static final String REDUCE_THREAD_NAME_FORMAT = \"brw-%d\";\n+  protected static final int QUERY_RUNNER_THREAD_PRIORITY = 7;\n+\n+  private final ListeningExecutorService _reduceExecutorService;\n+  private final int _maxReduceThreadsPerQuery;\n+\n+  public BrokerReduceService(PinotConfiguration config) {\n+    _maxReduceThreadsPerQuery = config.getProperty(CommonConstants.Broker.CONFIG_OF_MAX_REDUCE_THREADS_PER_QUERY,\n+        CommonConstants.Broker.MAX_REDUCE_THREADS_PER_QUERY);\n+    LOGGER.info(\"Initializing BrokerReduceService with {} reduce threads.\", _maxReduceThreadsPerQuery);\n+\n+    ThreadFactory reduceThreadFactory =\n+        new ThreadFactoryBuilder().setDaemon(false).setPriority(QUERY_RUNNER_THREAD_PRIORITY)\n+            .setNameFormat(REDUCE_THREAD_NAME_FORMAT).build();\n+\n+    // ExecutorService is initialized with numThreads sames availableProcessors.\n+    ExecutorService delegate =\n+        Executors.newFixedThreadPool(Runtime.getRuntime().availableProcessors(), reduceThreadFactory);\n+    _reduceExecutorService = MoreExecutors.listeningDecorator(delegate);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c73604641d5d84656b0e980b95329dc3152c66f7"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyODQ1OA==", "bodyText": "Return 1? You need at least one thread", "url": "https://github.com/apache/pinot/pull/6044#discussion_r502728458", "createdAt": "2020-10-10T01:30:22Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/query/reduce/GroupByDataTableReducer.java", "diffHunk": "@@ -231,58 +239,130 @@ private DataSchema getPrePostAggregationDataSchema(DataSchema dataSchema) {\n     return new DataSchema(columnNames, columnDataTypes);\n   }\n \n-  private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTables) {\n+  private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTablesToReduce,\n+      DataTableReducerContext reducerContext) {\n+    long start = System.currentTimeMillis();\n+    int numDataTables = dataTablesToReduce.size();\n+\n+    // Get the number of threads to use for reducing.\n+    int numReduceThreadsToUse = getNumReduceThreadsToUse(numDataTables, reducerContext.getMaxReduceThreadsPerQuery());\n+\n+    // In case of single reduce thread, fall back to SimpleIndexedTable to avoid redundant locking/unlocking calls.\n     int capacity = GroupByUtils.getTableCapacity(_queryContext);\n-    IndexedTable indexedTable = new SimpleIndexedTable(dataSchema, _queryContext, capacity);\n+    IndexedTable indexedTable =\n+        (numReduceThreadsToUse > 1) ? new ConcurrentIndexedTable(dataSchema, _queryContext, capacity)\n+            : new SimpleIndexedTable(dataSchema, _queryContext, capacity);\n+\n+    Future[] futures = new Future[numDataTables];\n+    CountDownLatch countDownLatch = new CountDownLatch(numDataTables);\n+\n+    // Create groups of data tables that each thread can process concurrently.\n+    // Given that numReduceThreads is <= numDataTables, each group will have at least one data table.\n+    ArrayList<DataTable> dataTables = new ArrayList<>(dataTablesToReduce);\n+    List<List<DataTable>> reduceGroups = new ArrayList<>(numReduceThreadsToUse);\n+\n+    for (int i = 0; i < numReduceThreadsToUse; i++) {\n+      reduceGroups.add(new ArrayList<>());\n+    }\n+    for (int i = 0; i < numDataTables; i++) {\n+      reduceGroups.get(i % numReduceThreadsToUse).add(dataTables.get(i));\n+    }\n+\n+    int cnt = 0;\n     ColumnDataType[] columnDataTypes = dataSchema.getColumnDataTypes();\n-    for (DataTable dataTable : dataTables) {\n-      int numRows = dataTable.getNumberOfRows();\n-      for (int rowId = 0; rowId < numRows; rowId++) {\n-        Object[] values = new Object[_numColumns];\n-        for (int colId = 0; colId < _numColumns; colId++) {\n-          switch (columnDataTypes[colId]) {\n-            case INT:\n-              values[colId] = dataTable.getInt(rowId, colId);\n-              break;\n-            case LONG:\n-              values[colId] = dataTable.getLong(rowId, colId);\n-              break;\n-            case FLOAT:\n-              values[colId] = dataTable.getFloat(rowId, colId);\n-              break;\n-            case DOUBLE:\n-              values[colId] = dataTable.getDouble(rowId, colId);\n-              break;\n-            case STRING:\n-              values[colId] = dataTable.getString(rowId, colId);\n-              break;\n-            case BYTES:\n-              values[colId] = dataTable.getBytes(rowId, colId);\n-              break;\n-            case OBJECT:\n-              values[colId] = dataTable.getObject(rowId, colId);\n-              break;\n-            // Add other aggregation intermediate result / group-by column type supports here\n-            default:\n-              throw new IllegalStateException();\n+    for (List<DataTable> reduceGroup : reduceGroups) {\n+      futures[cnt++] = reducerContext.getExecutorService().submit(new TraceRunnable() {\n+        @Override\n+        public void runJob() {\n+          for (DataTable dataTable : reduceGroup) {\n+            int numRows = dataTable.getNumberOfRows();\n+\n+            try {\n+              for (int rowId = 0; rowId < numRows; rowId++) {\n+                Object[] values = new Object[_numColumns];\n+                for (int colId = 0; colId < _numColumns; colId++) {\n+                  switch (columnDataTypes[colId]) {\n+                    case INT:\n+                      values[colId] = dataTable.getInt(rowId, colId);\n+                      break;\n+                    case LONG:\n+                      values[colId] = dataTable.getLong(rowId, colId);\n+                      break;\n+                    case FLOAT:\n+                      values[colId] = dataTable.getFloat(rowId, colId);\n+                      break;\n+                    case DOUBLE:\n+                      values[colId] = dataTable.getDouble(rowId, colId);\n+                      break;\n+                    case STRING:\n+                      values[colId] = dataTable.getString(rowId, colId);\n+                      break;\n+                    case BYTES:\n+                      values[colId] = dataTable.getBytes(rowId, colId);\n+                      break;\n+                    case OBJECT:\n+                      values[colId] = dataTable.getObject(rowId, colId);\n+                      break;\n+                    // Add other aggregation intermediate result / group-by column type supports here\n+                    default:\n+                      throw new IllegalStateException();\n+                  }\n+                }\n+                indexedTable.upsert(new Record(values));\n+              }\n+            } finally {\n+              countDownLatch.countDown();\n+            }\n           }\n         }\n-        indexedTable.upsert(new Record(values));\n+      });\n+    }\n+\n+    try {\n+      long timeOutMs = reducerContext.getReduceTimeOutMs() - (System.currentTimeMillis() - start);\n+      countDownLatch.await(timeOutMs, TimeUnit.MILLISECONDS);\n+    } catch (InterruptedException e) {\n+      for (Future future : futures) {\n+        if (!future.isDone()) {\n+          future.cancel(true);\n+        }\n       }\n     }\n+\n     indexedTable.finish(true);\n     return indexedTable;\n   }\n \n+  /**\n+   * Computes the number of reduce threads to use per query.\n+   * <ul>\n+   *   <li> Use single thread if number of data tables to reduce is less than {@value #MIN_DATA_TABLES_FOR_CONCURRENT_REDUCE}.</li>\n+   *   <li> Else, use min of max allowed reduce threads per query, and number of data tables.</li>\n+   * </ul>\n+   *\n+   * @param numDataTables Number of data tables to reduce\n+   * @param maxReduceThreadsPerQuery Max allowed reduce threads per query\n+   * @return Number of reduce threads to use for the query\n+   */\n+  private int getNumReduceThreadsToUse(int numDataTables, int maxReduceThreadsPerQuery) {\n+    // Use single thread if number of data tables < MIN_DATA_TABLES_FOR_CONCURRENT_REDUCE.\n+    if (numDataTables < MIN_DATA_TABLES_FOR_CONCURRENT_REDUCE) {\n+      return Math.min(1, numDataTables); // Number of data tables can be zero.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c73604641d5d84656b0e980b95329dc3152c66f7"}, "originalPosition": 221}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyOTA0Mg==", "bodyText": "(Critical) You need to put the timeout exception into the query response, or the response will be wrong and there is no way to detect that", "url": "https://github.com/apache/pinot/pull/6044#discussion_r502729042", "createdAt": "2020-10-10T01:35:13Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/query/reduce/GroupByDataTableReducer.java", "diffHunk": "@@ -231,58 +239,130 @@ private DataSchema getPrePostAggregationDataSchema(DataSchema dataSchema) {\n     return new DataSchema(columnNames, columnDataTypes);\n   }\n \n-  private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTables) {\n+  private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTablesToReduce,\n+      DataTableReducerContext reducerContext) {\n+    long start = System.currentTimeMillis();\n+    int numDataTables = dataTablesToReduce.size();\n+\n+    // Get the number of threads to use for reducing.\n+    int numReduceThreadsToUse = getNumReduceThreadsToUse(numDataTables, reducerContext.getMaxReduceThreadsPerQuery());\n+\n+    // In case of single reduce thread, fall back to SimpleIndexedTable to avoid redundant locking/unlocking calls.\n     int capacity = GroupByUtils.getTableCapacity(_queryContext);\n-    IndexedTable indexedTable = new SimpleIndexedTable(dataSchema, _queryContext, capacity);\n+    IndexedTable indexedTable =\n+        (numReduceThreadsToUse > 1) ? new ConcurrentIndexedTable(dataSchema, _queryContext, capacity)\n+            : new SimpleIndexedTable(dataSchema, _queryContext, capacity);\n+\n+    Future[] futures = new Future[numDataTables];\n+    CountDownLatch countDownLatch = new CountDownLatch(numDataTables);\n+\n+    // Create groups of data tables that each thread can process concurrently.\n+    // Given that numReduceThreads is <= numDataTables, each group will have at least one data table.\n+    ArrayList<DataTable> dataTables = new ArrayList<>(dataTablesToReduce);\n+    List<List<DataTable>> reduceGroups = new ArrayList<>(numReduceThreadsToUse);\n+\n+    for (int i = 0; i < numReduceThreadsToUse; i++) {\n+      reduceGroups.add(new ArrayList<>());\n+    }\n+    for (int i = 0; i < numDataTables; i++) {\n+      reduceGroups.get(i % numReduceThreadsToUse).add(dataTables.get(i));\n+    }\n+\n+    int cnt = 0;\n     ColumnDataType[] columnDataTypes = dataSchema.getColumnDataTypes();\n-    for (DataTable dataTable : dataTables) {\n-      int numRows = dataTable.getNumberOfRows();\n-      for (int rowId = 0; rowId < numRows; rowId++) {\n-        Object[] values = new Object[_numColumns];\n-        for (int colId = 0; colId < _numColumns; colId++) {\n-          switch (columnDataTypes[colId]) {\n-            case INT:\n-              values[colId] = dataTable.getInt(rowId, colId);\n-              break;\n-            case LONG:\n-              values[colId] = dataTable.getLong(rowId, colId);\n-              break;\n-            case FLOAT:\n-              values[colId] = dataTable.getFloat(rowId, colId);\n-              break;\n-            case DOUBLE:\n-              values[colId] = dataTable.getDouble(rowId, colId);\n-              break;\n-            case STRING:\n-              values[colId] = dataTable.getString(rowId, colId);\n-              break;\n-            case BYTES:\n-              values[colId] = dataTable.getBytes(rowId, colId);\n-              break;\n-            case OBJECT:\n-              values[colId] = dataTable.getObject(rowId, colId);\n-              break;\n-            // Add other aggregation intermediate result / group-by column type supports here\n-            default:\n-              throw new IllegalStateException();\n+    for (List<DataTable> reduceGroup : reduceGroups) {\n+      futures[cnt++] = reducerContext.getExecutorService().submit(new TraceRunnable() {\n+        @Override\n+        public void runJob() {\n+          for (DataTable dataTable : reduceGroup) {\n+            int numRows = dataTable.getNumberOfRows();\n+\n+            try {\n+              for (int rowId = 0; rowId < numRows; rowId++) {\n+                Object[] values = new Object[_numColumns];\n+                for (int colId = 0; colId < _numColumns; colId++) {\n+                  switch (columnDataTypes[colId]) {\n+                    case INT:\n+                      values[colId] = dataTable.getInt(rowId, colId);\n+                      break;\n+                    case LONG:\n+                      values[colId] = dataTable.getLong(rowId, colId);\n+                      break;\n+                    case FLOAT:\n+                      values[colId] = dataTable.getFloat(rowId, colId);\n+                      break;\n+                    case DOUBLE:\n+                      values[colId] = dataTable.getDouble(rowId, colId);\n+                      break;\n+                    case STRING:\n+                      values[colId] = dataTable.getString(rowId, colId);\n+                      break;\n+                    case BYTES:\n+                      values[colId] = dataTable.getBytes(rowId, colId);\n+                      break;\n+                    case OBJECT:\n+                      values[colId] = dataTable.getObject(rowId, colId);\n+                      break;\n+                    // Add other aggregation intermediate result / group-by column type supports here\n+                    default:\n+                      throw new IllegalStateException();\n+                  }\n+                }\n+                indexedTable.upsert(new Record(values));\n+              }\n+            } finally {\n+              countDownLatch.countDown();\n+            }\n           }\n         }\n-        indexedTable.upsert(new Record(values));\n+      });\n+    }\n+\n+    try {\n+      long timeOutMs = reducerContext.getReduceTimeOutMs() - (System.currentTimeMillis() - start);\n+      countDownLatch.await(timeOutMs, TimeUnit.MILLISECONDS);\n+    } catch (InterruptedException e) {\n+      for (Future future : futures) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c73604641d5d84656b0e980b95329dc3152c66f7"}, "originalPosition": 196}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyOTEyNw==", "bodyText": "Don't use the executor service for single-threaded case. There is overhead of using that instead of the current thread, which might cause performance degradation.", "url": "https://github.com/apache/pinot/pull/6044#discussion_r502729127", "createdAt": "2020-10-10T01:36:14Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/query/reduce/GroupByDataTableReducer.java", "diffHunk": "@@ -231,58 +239,130 @@ private DataSchema getPrePostAggregationDataSchema(DataSchema dataSchema) {\n     return new DataSchema(columnNames, columnDataTypes);\n   }\n \n-  private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTables) {\n+  private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTablesToReduce,\n+      DataTableReducerContext reducerContext) {\n+    long start = System.currentTimeMillis();\n+    int numDataTables = dataTablesToReduce.size();\n+\n+    // Get the number of threads to use for reducing.\n+    int numReduceThreadsToUse = getNumReduceThreadsToUse(numDataTables, reducerContext.getMaxReduceThreadsPerQuery());\n+\n+    // In case of single reduce thread, fall back to SimpleIndexedTable to avoid redundant locking/unlocking calls.\n     int capacity = GroupByUtils.getTableCapacity(_queryContext);\n-    IndexedTable indexedTable = new SimpleIndexedTable(dataSchema, _queryContext, capacity);\n+    IndexedTable indexedTable =\n+        (numReduceThreadsToUse > 1) ? new ConcurrentIndexedTable(dataSchema, _queryContext, capacity)\n+            : new SimpleIndexedTable(dataSchema, _queryContext, capacity);\n+\n+    Future[] futures = new Future[numDataTables];", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c73604641d5d84656b0e980b95329dc3152c66f7"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyOTUxOQ==", "bodyText": "I feel the original formatting is better. You can skip the reformatting by adding //@formatter:off, see AggregationFunctionUtils.isFitForDictionaryBasedComputation() for details.", "url": "https://github.com/apache/pinot/pull/6044#discussion_r502729519", "createdAt": "2020-10-10T01:39:36Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/test/java/org/apache/pinot/queries/DistinctQueriesTest.java", "diffHunk": "@@ -319,13 +321,13 @@ private void testDistinctInnerSegmentHelper(String[] queries, boolean isPql)\n   @Test\n   public void testDistinctInnerSegment()\n       throws Exception {\n-    testDistinctInnerSegmentHelper(new String[]{\n-        \"SELECT DISTINCT(intColumn, longColumn, floatColumn, doubleColumn, stringColumn, bytesColumn) FROM testTable LIMIT 10000\",\n-        \"SELECT DISTINCT(stringColumn, bytesColumn, floatColumn) FROM testTable WHERE intColumn >= 60 LIMIT 10000\",\n-        \"SELECT DISTINCT(intColumn, bytesColumn) FROM testTable ORDER BY bytesColumn LIMIT 5\",\n-        \"SELECT DISTINCT(ADD ( intColumn,  floatColumn  ), stringColumn) FROM testTable WHERE longColumn < 60 ORDER BY stringColumn DESC, ADD(intColumn, floatColumn) ASC LIMIT 10\",\n-        \"SELECT DISTINCT(floatColumn, longColumn) FROM testTable WHERE stringColumn = 'a' ORDER BY longColumn LIMIT 10\"\n-    }, true);\n+    testDistinctInnerSegmentHelper(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c73604641d5d84656b0e980b95329dc3152c66f7"}, "originalPosition": 27}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c73604641d5d84656b0e980b95329dc3152c66f7", "author": {"user": {"login": "mayankshriv", "name": "Mayank Shrivastava"}}, "url": "https://github.com/apache/pinot/commit/c73604641d5d84656b0e980b95329dc3152c66f7", "committedDate": "2020-10-08T04:45:01Z", "message": "Support for multi-threaded Group By reducer for SQL.\n\nThe existing implementation of Broker reduce phase is single-threaded.\nFor group-by queries where large response are being sent back from multiple servers,\nthis could become a bottlenect.\n\nGiven that brokers are generally light on CPU usage, making the reduce phase\nmulti-threaded would be a good idea to boost performance. This PR adds a multi-threaded\nimplementation for the Group-By reducer for SQL.\n\n- Added an executor service in BrokerReduceService that can be used by multi-threaded\n  execution of the broker reduce phase. This is initialized with number of threads as:\n  `Runtime.getRuntime().availableProcessors()`.\n\n- Added a broker side config to specify max number of threads per query to be used for reduce phase.\n  `pinot.broker.max.reduce.threads`. This has a same default value as server side combine phase:\n  `Math.max(1, Math.min(10, Runtime.getRuntime().availableProcessors() / 2))`\n\n   For reverting to single threaded reduce, set this config to 1.\n\n- The GroupByDataTableReducer uses the following algorithm for determining number of\n  threads to use in reduce phase (per query):\n  - If there are less than 2 data tables to reduce, it uses single threaded run.\n  - Else, it uses `Math.min(pinot.broker.max.reduce.threads, numDataTables).\n\n- For testing, explicitly sets num threads to reduce to be > 1 to ensure functional\n  correctness is tested."}, "afterCommit": {"oid": "1ad015b3866ff92c1c0db6f139a4ab71035eace9", "author": {"user": {"login": "mayankshriv", "name": "Mayank Shrivastava"}}, "url": "https://github.com/apache/pinot/commit/1ad015b3866ff92c1c0db6f139a4ab71035eace9", "committedDate": "2020-10-13T03:35:20Z", "message": "Support for multi-threaded Group By reducer for SQL.\n\nThe existing implementation of Broker reduce phase is single-threaded.\nFor group-by queries where large response are being sent back from multiple servers,\nthis could become a bottlenect.\n\nGiven that brokers are generally light on CPU usage, making the reduce phase\nmulti-threaded would be a good idea to boost performance. This PR adds a multi-threaded\nimplementation for the Group-By reducer for SQL.\n\n- Added an executor service in BrokerReduceService that can be used by multi-threaded\n  execution of the broker reduce phase. This is initialized with number of threads as:\n  `Runtime.getRuntime().availableProcessors()`.\n\n- Added a broker side config to specify max number of threads per query to be used for reduce phase.\n  `pinot.broker.max.reduce.threads.per.query`. This has a same default value as server side combine phase:\n  `Math.max(1, Math.min(10, Runtime.getRuntime().availableProcessors() / 2))`\n\n   For reverting to single threaded reduce, set this config to 1.\n\n- The GroupByDataTableReducer uses the following algorithm for determining number of\n  threads to use in reduce phase (per query):\n  - If there are less than 2 data tables to reduce, it uses single threaded run.\n  - Else, it uses `Math.min(pinot.broker.max.reduce.threads.per.query, numDataTables).\n\n- For testing, explicitly sets num threads to reduce to be > 1 to ensure functional\n  correctness is tested."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "1ad015b3866ff92c1c0db6f139a4ab71035eace9", "author": {"user": {"login": "mayankshriv", "name": "Mayank Shrivastava"}}, "url": "https://github.com/apache/pinot/commit/1ad015b3866ff92c1c0db6f139a4ab71035eace9", "committedDate": "2020-10-13T03:35:20Z", "message": "Support for multi-threaded Group By reducer for SQL.\n\nThe existing implementation of Broker reduce phase is single-threaded.\nFor group-by queries where large response are being sent back from multiple servers,\nthis could become a bottlenect.\n\nGiven that brokers are generally light on CPU usage, making the reduce phase\nmulti-threaded would be a good idea to boost performance. This PR adds a multi-threaded\nimplementation for the Group-By reducer for SQL.\n\n- Added an executor service in BrokerReduceService that can be used by multi-threaded\n  execution of the broker reduce phase. This is initialized with number of threads as:\n  `Runtime.getRuntime().availableProcessors()`.\n\n- Added a broker side config to specify max number of threads per query to be used for reduce phase.\n  `pinot.broker.max.reduce.threads.per.query`. This has a same default value as server side combine phase:\n  `Math.max(1, Math.min(10, Runtime.getRuntime().availableProcessors() / 2))`\n\n   For reverting to single threaded reduce, set this config to 1.\n\n- The GroupByDataTableReducer uses the following algorithm for determining number of\n  threads to use in reduce phase (per query):\n  - If there are less than 2 data tables to reduce, it uses single threaded run.\n  - Else, it uses `Math.min(pinot.broker.max.reduce.threads.per.query, numDataTables).\n\n- For testing, explicitly sets num threads to reduce to be > 1 to ensure functional\n  correctness is tested."}, "afterCommit": {"oid": "39626d41c7c909b7f1e9697b69119f8eedc366b7", "author": {"user": {"login": "mayankshriv", "name": "Mayank Shrivastava"}}, "url": "https://github.com/apache/pinot/commit/39626d41c7c909b7f1e9697b69119f8eedc366b7", "committedDate": "2020-10-13T04:47:02Z", "message": "Support for multi-threaded Group By reducer for SQL.\n\nThe existing implementation of Broker reduce phase is single-threaded.\nFor group-by queries where large response are being sent back from multiple servers,\nthis could become a bottlenect.\n\nGiven that brokers are generally light on CPU usage, making the reduce phase\nmulti-threaded would be a good idea to boost performance. This PR adds a multi-threaded\nimplementation for the Group-By reducer for SQL.\n\n- Added an executor service in BrokerReduceService that can be used by multi-threaded\n  execution of the broker reduce phase. This is initialized with number of threads as:\n  `Runtime.getRuntime().availableProcessors()`.\n\n- Added a broker side config to specify max number of threads per query to be used for reduce phase.\n  `pinot.broker.max.reduce.threads.per.query`. This has a same default value as server side combine phase:\n  `Math.max(1, Math.min(10, Runtime.getRuntime().availableProcessors() / 2))`\n\n   For reverting to single threaded reduce, set this config to 1.\n\n- The GroupByDataTableReducer uses the following algorithm for determining number of\n  threads to use in reduce phase (per query):\n  - If there are less than 2 data tables to reduce, it uses single threaded run.\n  - Else, it uses `Math.min(pinot.broker.max.reduce.threads.per.query, numDataTables).\n\n- For testing, explicitly sets num threads to reduce to be > 1 to ensure functional\n  correctness is tested."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "39626d41c7c909b7f1e9697b69119f8eedc366b7", "author": {"user": {"login": "mayankshriv", "name": "Mayank Shrivastava"}}, "url": "https://github.com/apache/pinot/commit/39626d41c7c909b7f1e9697b69119f8eedc366b7", "committedDate": "2020-10-13T04:47:02Z", "message": "Support for multi-threaded Group By reducer for SQL.\n\nThe existing implementation of Broker reduce phase is single-threaded.\nFor group-by queries where large response are being sent back from multiple servers,\nthis could become a bottlenect.\n\nGiven that brokers are generally light on CPU usage, making the reduce phase\nmulti-threaded would be a good idea to boost performance. This PR adds a multi-threaded\nimplementation for the Group-By reducer for SQL.\n\n- Added an executor service in BrokerReduceService that can be used by multi-threaded\n  execution of the broker reduce phase. This is initialized with number of threads as:\n  `Runtime.getRuntime().availableProcessors()`.\n\n- Added a broker side config to specify max number of threads per query to be used for reduce phase.\n  `pinot.broker.max.reduce.threads.per.query`. This has a same default value as server side combine phase:\n  `Math.max(1, Math.min(10, Runtime.getRuntime().availableProcessors() / 2))`\n\n   For reverting to single threaded reduce, set this config to 1.\n\n- The GroupByDataTableReducer uses the following algorithm for determining number of\n  threads to use in reduce phase (per query):\n  - If there are less than 2 data tables to reduce, it uses single threaded run.\n  - Else, it uses `Math.min(pinot.broker.max.reduce.threads.per.query, numDataTables).\n\n- For testing, explicitly sets num threads to reduce to be > 1 to ensure functional\n  correctness is tested."}, "afterCommit": {"oid": "bb955b505c1d720158ec6642a2502b78b839e733", "author": {"user": {"login": "mayankshriv", "name": "Mayank Shrivastava"}}, "url": "https://github.com/apache/pinot/commit/bb955b505c1d720158ec6642a2502b78b839e733", "committedDate": "2020-10-13T21:39:05Z", "message": "Support for multi-threaded Group By reducer for SQL.\n\nThe existing implementation of Broker reduce phase is single-threaded.\nFor group-by queries where large response are being sent back from multiple servers,\nthis could become a bottlenect.\n\nGiven that brokers are generally light on CPU usage, making the reduce phase\nmulti-threaded would be a good idea to boost performance. This PR adds a multi-threaded\nimplementation for the Group-By reducer for SQL.\n\n- Added an executor service in BrokerReduceService that can be used by multi-threaded\n  execution of the broker reduce phase. This is initialized with number of threads as:\n  `Runtime.getRuntime().availableProcessors()`.\n\n- Added a broker side config to specify max number of threads per query to be used for reduce phase.\n  `pinot.broker.max.reduce.threads.per.query`. This has a same default value as server side combine phase:\n  `Math.max(1, Math.min(10, Runtime.getRuntime().availableProcessors() / 2))`\n\n   For reverting to single threaded reduce, set this config to 1.\n\n- The GroupByDataTableReducer uses the following algorithm for determining number of\n  threads to use in reduce phase (per query):\n  - If there are less than 2 data tables to reduce, it uses single threaded run.\n  - Else, it uses `Math.min(pinot.broker.max.reduce.threads.per.query, numDataTables).\n\n- For testing, explicitly sets num threads to reduce to be > 1 to ensure functional\n  correctness is tested."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "bb955b505c1d720158ec6642a2502b78b839e733", "author": {"user": {"login": "mayankshriv", "name": "Mayank Shrivastava"}}, "url": "https://github.com/apache/pinot/commit/bb955b505c1d720158ec6642a2502b78b839e733", "committedDate": "2020-10-13T21:39:05Z", "message": "Support for multi-threaded Group By reducer for SQL.\n\nThe existing implementation of Broker reduce phase is single-threaded.\nFor group-by queries where large response are being sent back from multiple servers,\nthis could become a bottlenect.\n\nGiven that brokers are generally light on CPU usage, making the reduce phase\nmulti-threaded would be a good idea to boost performance. This PR adds a multi-threaded\nimplementation for the Group-By reducer for SQL.\n\n- Added an executor service in BrokerReduceService that can be used by multi-threaded\n  execution of the broker reduce phase. This is initialized with number of threads as:\n  `Runtime.getRuntime().availableProcessors()`.\n\n- Added a broker side config to specify max number of threads per query to be used for reduce phase.\n  `pinot.broker.max.reduce.threads.per.query`. This has a same default value as server side combine phase:\n  `Math.max(1, Math.min(10, Runtime.getRuntime().availableProcessors() / 2))`\n\n   For reverting to single threaded reduce, set this config to 1.\n\n- The GroupByDataTableReducer uses the following algorithm for determining number of\n  threads to use in reduce phase (per query):\n  - If there are less than 2 data tables to reduce, it uses single threaded run.\n  - Else, it uses `Math.min(pinot.broker.max.reduce.threads.per.query, numDataTables).\n\n- For testing, explicitly sets num threads to reduce to be > 1 to ensure functional\n  correctness is tested."}, "afterCommit": {"oid": "e20f784d0300eb26ab31391156e75bd3915da646", "author": {"user": {"login": "mayankshriv", "name": "Mayank Shrivastava"}}, "url": "https://github.com/apache/pinot/commit/e20f784d0300eb26ab31391156e75bd3915da646", "committedDate": "2020-10-13T23:06:04Z", "message": "Support for multi-threaded Group By reducer for SQL.\n\nThe existing implementation of Broker reduce phase is single-threaded.\nFor group-by queries where large response are being sent back from multiple servers,\nthis could become a bottlenect.\n\nGiven that brokers are generally light on CPU usage, making the reduce phase\nmulti-threaded would be a good idea to boost performance. This PR adds a multi-threaded\nimplementation for the Group-By reducer for SQL.\n\n- Added an executor service in BrokerReduceService that can be used by multi-threaded\n  execution of the broker reduce phase. This is initialized with number of threads as:\n  `Runtime.getRuntime().availableProcessors()`.\n\n- Added a broker side config to specify max number of threads per query to be used for reduce phase.\n  `pinot.broker.max.reduce.threads.per.query`. This has a same default value as server side combine phase:\n  `Math.max(1, Math.min(10, Runtime.getRuntime().availableProcessors() / 2))`\n\n   For reverting to single threaded reduce, set this config to 1.\n\n- The GroupByDataTableReducer uses the following algorithm for determining number of\n  threads to use in reduce phase (per query):\n  - If there are less than 2 data tables to reduce, it uses single threaded run.\n  - Else, it uses `Math.min(pinot.broker.max.reduce.threads.per.query, numDataTables).\n\n- For testing, explicitly sets num threads to reduce to be > 1 to ensure functional\n  correctness is tested."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA3OTIzNTIy", "url": "https://github.com/apache/pinot/pull/6044#pullrequestreview-507923522", "createdAt": "2020-10-14T00:52:40Z", "commit": {"oid": "e20f784d0300eb26ab31391156e75bd3915da646"}, "state": "APPROVED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwMDo1Mjo0MFrOHg-Y6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwMDo1NToxNlrOHg-bdQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDMzODY2Ng==", "bodyText": "You need to have another comment //@formatter:on to turn the formatter on after the queries", "url": "https://github.com/apache/pinot/pull/6044#discussion_r504338666", "createdAt": "2020-10-14T00:52:40Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/test/java/org/apache/pinot/queries/DistinctQueriesTest.java", "diffHunk": "@@ -319,6 +321,7 @@ private void testDistinctInnerSegmentHelper(String[] queries, boolean isPql)\n   @Test\n   public void testDistinctInnerSegment()\n       throws Exception {\n+    //@formatter:off", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e20f784d0300eb26ab31391156e75bd3915da646"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDMzOTAwNw==", "bodyText": "Yes, numDataTables can be zero, but I think returning 0 or 1 should both work. Actually a better approach should be just short-circuit the zero data table case.", "url": "https://github.com/apache/pinot/pull/6044#discussion_r504339007", "createdAt": "2020-10-14T00:54:06Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/query/reduce/GroupByDataTableReducer.java", "diffHunk": "@@ -231,58 +239,130 @@ private DataSchema getPrePostAggregationDataSchema(DataSchema dataSchema) {\n     return new DataSchema(columnNames, columnDataTypes);\n   }\n \n-  private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTables) {\n+  private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTablesToReduce,\n+      DataTableReducerContext reducerContext) {\n+    long start = System.currentTimeMillis();\n+    int numDataTables = dataTablesToReduce.size();\n+\n+    // Get the number of threads to use for reducing.\n+    int numReduceThreadsToUse = getNumReduceThreadsToUse(numDataTables, reducerContext.getMaxReduceThreadsPerQuery());\n+\n+    // In case of single reduce thread, fall back to SimpleIndexedTable to avoid redundant locking/unlocking calls.\n     int capacity = GroupByUtils.getTableCapacity(_queryContext);\n-    IndexedTable indexedTable = new SimpleIndexedTable(dataSchema, _queryContext, capacity);\n+    IndexedTable indexedTable =\n+        (numReduceThreadsToUse > 1) ? new ConcurrentIndexedTable(dataSchema, _queryContext, capacity)\n+            : new SimpleIndexedTable(dataSchema, _queryContext, capacity);\n+\n+    Future[] futures = new Future[numDataTables];\n+    CountDownLatch countDownLatch = new CountDownLatch(numDataTables);\n+\n+    // Create groups of data tables that each thread can process concurrently.\n+    // Given that numReduceThreads is <= numDataTables, each group will have at least one data table.\n+    ArrayList<DataTable> dataTables = new ArrayList<>(dataTablesToReduce);\n+    List<List<DataTable>> reduceGroups = new ArrayList<>(numReduceThreadsToUse);\n+\n+    for (int i = 0; i < numReduceThreadsToUse; i++) {\n+      reduceGroups.add(new ArrayList<>());\n+    }\n+    for (int i = 0; i < numDataTables; i++) {\n+      reduceGroups.get(i % numReduceThreadsToUse).add(dataTables.get(i));\n+    }\n+\n+    int cnt = 0;\n     ColumnDataType[] columnDataTypes = dataSchema.getColumnDataTypes();\n-    for (DataTable dataTable : dataTables) {\n-      int numRows = dataTable.getNumberOfRows();\n-      for (int rowId = 0; rowId < numRows; rowId++) {\n-        Object[] values = new Object[_numColumns];\n-        for (int colId = 0; colId < _numColumns; colId++) {\n-          switch (columnDataTypes[colId]) {\n-            case INT:\n-              values[colId] = dataTable.getInt(rowId, colId);\n-              break;\n-            case LONG:\n-              values[colId] = dataTable.getLong(rowId, colId);\n-              break;\n-            case FLOAT:\n-              values[colId] = dataTable.getFloat(rowId, colId);\n-              break;\n-            case DOUBLE:\n-              values[colId] = dataTable.getDouble(rowId, colId);\n-              break;\n-            case STRING:\n-              values[colId] = dataTable.getString(rowId, colId);\n-              break;\n-            case BYTES:\n-              values[colId] = dataTable.getBytes(rowId, colId);\n-              break;\n-            case OBJECT:\n-              values[colId] = dataTable.getObject(rowId, colId);\n-              break;\n-            // Add other aggregation intermediate result / group-by column type supports here\n-            default:\n-              throw new IllegalStateException();\n+    for (List<DataTable> reduceGroup : reduceGroups) {\n+      futures[cnt++] = reducerContext.getExecutorService().submit(new TraceRunnable() {\n+        @Override\n+        public void runJob() {\n+          for (DataTable dataTable : reduceGroup) {\n+            int numRows = dataTable.getNumberOfRows();\n+\n+            try {\n+              for (int rowId = 0; rowId < numRows; rowId++) {\n+                Object[] values = new Object[_numColumns];\n+                for (int colId = 0; colId < _numColumns; colId++) {\n+                  switch (columnDataTypes[colId]) {\n+                    case INT:\n+                      values[colId] = dataTable.getInt(rowId, colId);\n+                      break;\n+                    case LONG:\n+                      values[colId] = dataTable.getLong(rowId, colId);\n+                      break;\n+                    case FLOAT:\n+                      values[colId] = dataTable.getFloat(rowId, colId);\n+                      break;\n+                    case DOUBLE:\n+                      values[colId] = dataTable.getDouble(rowId, colId);\n+                      break;\n+                    case STRING:\n+                      values[colId] = dataTable.getString(rowId, colId);\n+                      break;\n+                    case BYTES:\n+                      values[colId] = dataTable.getBytes(rowId, colId);\n+                      break;\n+                    case OBJECT:\n+                      values[colId] = dataTable.getObject(rowId, colId);\n+                      break;\n+                    // Add other aggregation intermediate result / group-by column type supports here\n+                    default:\n+                      throw new IllegalStateException();\n+                  }\n+                }\n+                indexedTable.upsert(new Record(values));\n+              }\n+            } finally {\n+              countDownLatch.countDown();\n+            }\n           }\n         }\n-        indexedTable.upsert(new Record(values));\n+      });\n+    }\n+\n+    try {\n+      long timeOutMs = reducerContext.getReduceTimeOutMs() - (System.currentTimeMillis() - start);\n+      countDownLatch.await(timeOutMs, TimeUnit.MILLISECONDS);\n+    } catch (InterruptedException e) {\n+      for (Future future : futures) {\n+        if (!future.isDone()) {\n+          future.cancel(true);\n+        }\n       }\n     }\n+\n     indexedTable.finish(true);\n     return indexedTable;\n   }\n \n+  /**\n+   * Computes the number of reduce threads to use per query.\n+   * <ul>\n+   *   <li> Use single thread if number of data tables to reduce is less than {@value #MIN_DATA_TABLES_FOR_CONCURRENT_REDUCE}.</li>\n+   *   <li> Else, use min of max allowed reduce threads per query, and number of data tables.</li>\n+   * </ul>\n+   *\n+   * @param numDataTables Number of data tables to reduce\n+   * @param maxReduceThreadsPerQuery Max allowed reduce threads per query\n+   * @return Number of reduce threads to use for the query\n+   */\n+  private int getNumReduceThreadsToUse(int numDataTables, int maxReduceThreadsPerQuery) {\n+    // Use single thread if number of data tables < MIN_DATA_TABLES_FOR_CONCURRENT_REDUCE.\n+    if (numDataTables < MIN_DATA_TABLES_FOR_CONCURRENT_REDUCE) {\n+      return Math.min(1, numDataTables); // Number of data tables can be zero.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyODQ1OA=="}, "originalCommit": {"oid": "c73604641d5d84656b0e980b95329dc3152c66f7"}, "originalPosition": 221}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDMzOTMxNw==", "bodyText": "We can short circuit the single data table case by directly return the new SimpleIndexedTable(dataSchema, _queryContext, capacity)", "url": "https://github.com/apache/pinot/pull/6044#discussion_r504339317", "createdAt": "2020-10-14T00:55:16Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/query/reduce/GroupByDataTableReducer.java", "diffHunk": "@@ -231,58 +255,134 @@ private DataSchema getPrePostAggregationDataSchema(DataSchema dataSchema) {\n     return new DataSchema(columnNames, columnDataTypes);\n   }\n \n-  private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTables) {\n+  private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTablesToReduce,\n+      DataTableReducerContext reducerContext)\n+      throws TimeoutException {\n+    long start = System.currentTimeMillis();\n+    int numDataTables = dataTablesToReduce.size();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e20f784d0300eb26ab31391156e75bd3915da646"}, "originalPosition": 104}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "283c8e35c959a2f969b3231c143f7bfbf22bd113", "author": {"user": {"login": "mayankshriv", "name": "Mayank Shrivastava"}}, "url": "https://github.com/apache/pinot/commit/283c8e35c959a2f969b3231c143f7bfbf22bd113", "committedDate": "2020-10-14T02:19:22Z", "message": "Support for multi-threaded Group By reducer for SQL.\n\nThe existing implementation of Broker reduce phase is single-threaded.\nFor group-by queries where large response are being sent back from multiple servers,\nthis could become a bottlenect.\n\nGiven that brokers are generally light on CPU usage, making the reduce phase\nmulti-threaded would be a good idea to boost performance. This PR adds a multi-threaded\nimplementation for the Group-By reducer for SQL.\n\n- Added an executor service in BrokerReduceService that can be used by multi-threaded\n  execution of the broker reduce phase. This is initialized with number of threads as:\n  `Runtime.getRuntime().availableProcessors()`.\n\n- Added a broker side config to specify max number of threads per query to be used for reduce phase.\n  `pinot.broker.max.reduce.threads.per.query`. This has a same default value as server side combine phase:\n  `Math.max(1, Math.min(10, Runtime.getRuntime().availableProcessors() / 2))`\n\n   For reverting to single threaded reduce, set this config to 1.\n\n- The GroupByDataTableReducer uses the following algorithm for determining number of\n  threads to use in reduce phase (per query):\n  - If there are less than 2 data tables to reduce, it uses single threaded run.\n  - Else, it uses `Math.min(pinot.broker.max.reduce.threads.per.query, numDataTables).\n\n- For testing, explicitly sets num threads to reduce to be > 1 to ensure functional\n  correctness is tested."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e20f784d0300eb26ab31391156e75bd3915da646", "author": {"user": {"login": "mayankshriv", "name": "Mayank Shrivastava"}}, "url": "https://github.com/apache/pinot/commit/e20f784d0300eb26ab31391156e75bd3915da646", "committedDate": "2020-10-13T23:06:04Z", "message": "Support for multi-threaded Group By reducer for SQL.\n\nThe existing implementation of Broker reduce phase is single-threaded.\nFor group-by queries where large response are being sent back from multiple servers,\nthis could become a bottlenect.\n\nGiven that brokers are generally light on CPU usage, making the reduce phase\nmulti-threaded would be a good idea to boost performance. This PR adds a multi-threaded\nimplementation for the Group-By reducer for SQL.\n\n- Added an executor service in BrokerReduceService that can be used by multi-threaded\n  execution of the broker reduce phase. This is initialized with number of threads as:\n  `Runtime.getRuntime().availableProcessors()`.\n\n- Added a broker side config to specify max number of threads per query to be used for reduce phase.\n  `pinot.broker.max.reduce.threads.per.query`. This has a same default value as server side combine phase:\n  `Math.max(1, Math.min(10, Runtime.getRuntime().availableProcessors() / 2))`\n\n   For reverting to single threaded reduce, set this config to 1.\n\n- The GroupByDataTableReducer uses the following algorithm for determining number of\n  threads to use in reduce phase (per query):\n  - If there are less than 2 data tables to reduce, it uses single threaded run.\n  - Else, it uses `Math.min(pinot.broker.max.reduce.threads.per.query, numDataTables).\n\n- For testing, explicitly sets num threads to reduce to be > 1 to ensure functional\n  correctness is tested."}, "afterCommit": {"oid": "283c8e35c959a2f969b3231c143f7bfbf22bd113", "author": {"user": {"login": "mayankshriv", "name": "Mayank Shrivastava"}}, "url": "https://github.com/apache/pinot/commit/283c8e35c959a2f969b3231c143f7bfbf22bd113", "committedDate": "2020-10-14T02:19:22Z", "message": "Support for multi-threaded Group By reducer for SQL.\n\nThe existing implementation of Broker reduce phase is single-threaded.\nFor group-by queries where large response are being sent back from multiple servers,\nthis could become a bottlenect.\n\nGiven that brokers are generally light on CPU usage, making the reduce phase\nmulti-threaded would be a good idea to boost performance. This PR adds a multi-threaded\nimplementation for the Group-By reducer for SQL.\n\n- Added an executor service in BrokerReduceService that can be used by multi-threaded\n  execution of the broker reduce phase. This is initialized with number of threads as:\n  `Runtime.getRuntime().availableProcessors()`.\n\n- Added a broker side config to specify max number of threads per query to be used for reduce phase.\n  `pinot.broker.max.reduce.threads.per.query`. This has a same default value as server side combine phase:\n  `Math.max(1, Math.min(10, Runtime.getRuntime().availableProcessors() / 2))`\n\n   For reverting to single threaded reduce, set this config to 1.\n\n- The GroupByDataTableReducer uses the following algorithm for determining number of\n  threads to use in reduce phase (per query):\n  - If there are less than 2 data tables to reduce, it uses single threaded run.\n  - Else, it uses `Math.min(pinot.broker.max.reduce.threads.per.query, numDataTables).\n\n- For testing, explicitly sets num threads to reduce to be > 1 to ensure functional\n  correctness is tested."}}]}}}, "rateLimit": {"limit": 5000, "remaining": 86, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}