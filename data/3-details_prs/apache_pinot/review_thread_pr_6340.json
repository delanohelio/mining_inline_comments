{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTM1MDA4MTA2", "number": 6340, "reviewThreads": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwMjoyNjo1M1rOFDx_pw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwMjo0Mjo1OVrOFGfqpw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NTA5MTU5OnYy", "diffSide": "RIGHT", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/helix/core/minion/generator/SegmentGenerationAndPushTaskGenerator.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwMjoyNjo1M1rOIDmPRw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwMjoyNjo1M1rOIDmPRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDY0MzE0Mw==", "bodyText": "May be just catch NumberFormatException?", "url": "https://github.com/apache/pinot/pull/6340#discussion_r540643143", "createdAt": "2020-12-11T02:26:53Z", "author": {"login": "mayankshriv"}, "path": "pinot-controller/src/main/java/org/apache/pinot/controller/helix/core/minion/generator/SegmentGenerationAndPushTaskGenerator.java", "diffHunk": "@@ -0,0 +1,255 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.helix.core.minion.generator;\n+\n+import com.google.common.base.Preconditions;\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.nio.file.FileSystems;\n+import java.nio.file.PathMatcher;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.pinot.common.metadata.segment.OfflineSegmentZKMetadata;\n+import org.apache.pinot.controller.helix.core.minion.ClusterInfoAccessor;\n+import org.apache.pinot.core.common.MinionConstants;\n+import org.apache.pinot.core.minion.PinotTaskConfig;\n+import org.apache.pinot.spi.config.table.TableConfig;\n+import org.apache.pinot.spi.config.table.TableTaskConfig;\n+import org.apache.pinot.spi.config.table.TableType;\n+import org.apache.pinot.spi.config.table.ingestion.BatchIngestionConfig;\n+import org.apache.pinot.spi.env.PinotConfiguration;\n+import org.apache.pinot.spi.filesystem.PinotFS;\n+import org.apache.pinot.spi.filesystem.PinotFSFactory;\n+import org.apache.pinot.spi.ingestion.batch.BatchConfigProperties;\n+import org.apache.pinot.spi.plugin.PluginManager;\n+import org.apache.pinot.spi.utils.IngestionConfigUtils;\n+import org.apache.pinot.spi.utils.JsonUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class SegmentGenerationAndPushTaskGenerator implements PinotTaskGenerator {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(SegmentGenerationAndPushTaskGenerator.class);\n+\n+  private final ClusterInfoAccessor _clusterInfoAccessor;\n+\n+  public SegmentGenerationAndPushTaskGenerator(ClusterInfoAccessor clusterInfoAccessor) {\n+    _clusterInfoAccessor = clusterInfoAccessor;\n+  }\n+\n+  @Override\n+  public String getTaskType() {\n+    return MinionConstants.SegmentGenerationAndPushTask.TASK_TYPE;\n+  }\n+\n+  @Override\n+  public List<PinotTaskConfig> generateTasks(List<TableConfig> tableConfigs) {\n+    List<PinotTaskConfig> pinotTaskConfigs = new ArrayList<>();\n+\n+    for (TableConfig tableConfig : tableConfigs) {\n+      // Only generate tasks for OFFLINE tables\n+      String offlineTableName = tableConfig.getTableName();\n+      if (tableConfig.getTableType() != TableType.OFFLINE) {\n+        LOGGER.warn(\"Skip generating SegmentGenerationAndPushTask for non-OFFLINE table: {}\", offlineTableName);\n+        continue;\n+      }\n+\n+      TableTaskConfig tableTaskConfig = tableConfig.getTaskConfig();\n+      Preconditions.checkNotNull(tableTaskConfig);\n+      Map<String, String> taskConfigs =\n+          tableTaskConfig.getConfigsForTaskType(MinionConstants.SegmentGenerationAndPushTask.TASK_TYPE);\n+      Preconditions.checkNotNull(taskConfigs, \"Task config shouldn't be null for Table: {}\", offlineTableName);\n+\n+      // Get max number of tasks for this table\n+      int tableMaxNumTasks;\n+      String tableMaxNumTasksConfig = taskConfigs.get(MinionConstants.TABLE_MAX_NUM_TASKS_KEY);\n+      if (tableMaxNumTasksConfig != null) {\n+        try {\n+          tableMaxNumTasks = Integer.parseInt(tableMaxNumTasksConfig);\n+        } catch (Exception e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4d7abbde616e9a82923b6ec39a01405ee6826b58"}, "originalPosition": 93}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NTEwMDExOnYy", "diffSide": "RIGHT", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/helix/core/minion/generator/SegmentGenerationAndPushTaskGenerator.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwMjozMDowMlrOIDmTow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwMjozMDowMlrOIDmTow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDY0NDI1OQ==", "bodyText": "In case of multiple table inputs, if one of them had this issue, the code will abandon the others? Perhaps it should just warn and move on to the next table config?", "url": "https://github.com/apache/pinot/pull/6340#discussion_r540644259", "createdAt": "2020-12-11T02:30:02Z", "author": {"login": "mayankshriv"}, "path": "pinot-controller/src/main/java/org/apache/pinot/controller/helix/core/minion/generator/SegmentGenerationAndPushTaskGenerator.java", "diffHunk": "@@ -0,0 +1,255 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.helix.core.minion.generator;\n+\n+import com.google.common.base.Preconditions;\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.nio.file.FileSystems;\n+import java.nio.file.PathMatcher;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.pinot.common.metadata.segment.OfflineSegmentZKMetadata;\n+import org.apache.pinot.controller.helix.core.minion.ClusterInfoAccessor;\n+import org.apache.pinot.core.common.MinionConstants;\n+import org.apache.pinot.core.minion.PinotTaskConfig;\n+import org.apache.pinot.spi.config.table.TableConfig;\n+import org.apache.pinot.spi.config.table.TableTaskConfig;\n+import org.apache.pinot.spi.config.table.TableType;\n+import org.apache.pinot.spi.config.table.ingestion.BatchIngestionConfig;\n+import org.apache.pinot.spi.env.PinotConfiguration;\n+import org.apache.pinot.spi.filesystem.PinotFS;\n+import org.apache.pinot.spi.filesystem.PinotFSFactory;\n+import org.apache.pinot.spi.ingestion.batch.BatchConfigProperties;\n+import org.apache.pinot.spi.plugin.PluginManager;\n+import org.apache.pinot.spi.utils.IngestionConfigUtils;\n+import org.apache.pinot.spi.utils.JsonUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class SegmentGenerationAndPushTaskGenerator implements PinotTaskGenerator {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(SegmentGenerationAndPushTaskGenerator.class);\n+\n+  private final ClusterInfoAccessor _clusterInfoAccessor;\n+\n+  public SegmentGenerationAndPushTaskGenerator(ClusterInfoAccessor clusterInfoAccessor) {\n+    _clusterInfoAccessor = clusterInfoAccessor;\n+  }\n+\n+  @Override\n+  public String getTaskType() {\n+    return MinionConstants.SegmentGenerationAndPushTask.TASK_TYPE;\n+  }\n+\n+  @Override\n+  public List<PinotTaskConfig> generateTasks(List<TableConfig> tableConfigs) {\n+    List<PinotTaskConfig> pinotTaskConfigs = new ArrayList<>();\n+\n+    for (TableConfig tableConfig : tableConfigs) {\n+      // Only generate tasks for OFFLINE tables\n+      String offlineTableName = tableConfig.getTableName();\n+      if (tableConfig.getTableType() != TableType.OFFLINE) {\n+        LOGGER.warn(\"Skip generating SegmentGenerationAndPushTask for non-OFFLINE table: {}\", offlineTableName);\n+        continue;\n+      }\n+\n+      TableTaskConfig tableTaskConfig = tableConfig.getTaskConfig();\n+      Preconditions.checkNotNull(tableTaskConfig);\n+      Map<String, String> taskConfigs =\n+          tableTaskConfig.getConfigsForTaskType(MinionConstants.SegmentGenerationAndPushTask.TASK_TYPE);\n+      Preconditions.checkNotNull(taskConfigs, \"Task config shouldn't be null for Table: {}\", offlineTableName);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4d7abbde616e9a82923b6ec39a01405ee6826b58"}, "originalPosition": 85}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NTEwMjY5OnYy", "diffSide": "RIGHT", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/helix/core/minion/generator/SegmentGenerationAndPushTaskGenerator.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwMjozMToxOFrOIDmVHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwMjozMToxOFrOIDmVHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDY0NDYzNw==", "bodyText": "This method has become too big, may be factor out into smaller helper routines for better readability?", "url": "https://github.com/apache/pinot/pull/6340#discussion_r540644637", "createdAt": "2020-12-11T02:31:18Z", "author": {"login": "mayankshriv"}, "path": "pinot-controller/src/main/java/org/apache/pinot/controller/helix/core/minion/generator/SegmentGenerationAndPushTaskGenerator.java", "diffHunk": "@@ -0,0 +1,255 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.helix.core.minion.generator;\n+\n+import com.google.common.base.Preconditions;\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.nio.file.FileSystems;\n+import java.nio.file.PathMatcher;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.pinot.common.metadata.segment.OfflineSegmentZKMetadata;\n+import org.apache.pinot.controller.helix.core.minion.ClusterInfoAccessor;\n+import org.apache.pinot.core.common.MinionConstants;\n+import org.apache.pinot.core.minion.PinotTaskConfig;\n+import org.apache.pinot.spi.config.table.TableConfig;\n+import org.apache.pinot.spi.config.table.TableTaskConfig;\n+import org.apache.pinot.spi.config.table.TableType;\n+import org.apache.pinot.spi.config.table.ingestion.BatchIngestionConfig;\n+import org.apache.pinot.spi.env.PinotConfiguration;\n+import org.apache.pinot.spi.filesystem.PinotFS;\n+import org.apache.pinot.spi.filesystem.PinotFSFactory;\n+import org.apache.pinot.spi.ingestion.batch.BatchConfigProperties;\n+import org.apache.pinot.spi.plugin.PluginManager;\n+import org.apache.pinot.spi.utils.IngestionConfigUtils;\n+import org.apache.pinot.spi.utils.JsonUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class SegmentGenerationAndPushTaskGenerator implements PinotTaskGenerator {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(SegmentGenerationAndPushTaskGenerator.class);\n+\n+  private final ClusterInfoAccessor _clusterInfoAccessor;\n+\n+  public SegmentGenerationAndPushTaskGenerator(ClusterInfoAccessor clusterInfoAccessor) {\n+    _clusterInfoAccessor = clusterInfoAccessor;\n+  }\n+\n+  @Override\n+  public String getTaskType() {\n+    return MinionConstants.SegmentGenerationAndPushTask.TASK_TYPE;\n+  }\n+\n+  @Override\n+  public List<PinotTaskConfig> generateTasks(List<TableConfig> tableConfigs) {\n+    List<PinotTaskConfig> pinotTaskConfigs = new ArrayList<>();\n+\n+    for (TableConfig tableConfig : tableConfigs) {\n+      // Only generate tasks for OFFLINE tables\n+      String offlineTableName = tableConfig.getTableName();\n+      if (tableConfig.getTableType() != TableType.OFFLINE) {\n+        LOGGER.warn(\"Skip generating SegmentGenerationAndPushTask for non-OFFLINE table: {}\", offlineTableName);\n+        continue;\n+      }\n+\n+      TableTaskConfig tableTaskConfig = tableConfig.getTaskConfig();\n+      Preconditions.checkNotNull(tableTaskConfig);\n+      Map<String, String> taskConfigs =\n+          tableTaskConfig.getConfigsForTaskType(MinionConstants.SegmentGenerationAndPushTask.TASK_TYPE);\n+      Preconditions.checkNotNull(taskConfigs, \"Task config shouldn't be null for Table: {}\", offlineTableName);\n+\n+      // Get max number of tasks for this table\n+      int tableMaxNumTasks;\n+      String tableMaxNumTasksConfig = taskConfigs.get(MinionConstants.TABLE_MAX_NUM_TASKS_KEY);\n+      if (tableMaxNumTasksConfig != null) {\n+        try {\n+          tableMaxNumTasks = Integer.parseInt(tableMaxNumTasksConfig);\n+        } catch (Exception e) {\n+          tableMaxNumTasks = Integer.MAX_VALUE;\n+        }\n+      } else {\n+        tableMaxNumTasks = Integer.MAX_VALUE;\n+      }\n+\n+      // Generate tasks\n+      int tableNumTasks = 0;\n+      // Generate up to tableMaxNumTasks tasks each time for each table\n+      if (tableNumTasks == tableMaxNumTasks) {\n+        break;\n+      }\n+      String batchSegmentIngestionType = IngestionConfigUtils.getBatchSegmentIngestionType(tableConfig);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4d7abbde616e9a82923b6ec39a01405ee6826b58"}, "originalPosition": 106}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NTEwNzI5OnYy", "diffSide": "RIGHT", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/helix/core/minion/generator/SegmentGenerationAndPushTaskGenerator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwMjozMzowNFrOIDmXfQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQyMzowMDowOVrOIHc8uw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDY0NTI0NQ==", "bodyText": "Use File.separator instead of /?", "url": "https://github.com/apache/pinot/pull/6340#discussion_r540645245", "createdAt": "2020-12-11T02:33:04Z", "author": {"login": "mayankshriv"}, "path": "pinot-controller/src/main/java/org/apache/pinot/controller/helix/core/minion/generator/SegmentGenerationAndPushTaskGenerator.java", "diffHunk": "@@ -0,0 +1,255 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.helix.core.minion.generator;\n+\n+import com.google.common.base.Preconditions;\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.nio.file.FileSystems;\n+import java.nio.file.PathMatcher;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.pinot.common.metadata.segment.OfflineSegmentZKMetadata;\n+import org.apache.pinot.controller.helix.core.minion.ClusterInfoAccessor;\n+import org.apache.pinot.core.common.MinionConstants;\n+import org.apache.pinot.core.minion.PinotTaskConfig;\n+import org.apache.pinot.spi.config.table.TableConfig;\n+import org.apache.pinot.spi.config.table.TableTaskConfig;\n+import org.apache.pinot.spi.config.table.TableType;\n+import org.apache.pinot.spi.config.table.ingestion.BatchIngestionConfig;\n+import org.apache.pinot.spi.env.PinotConfiguration;\n+import org.apache.pinot.spi.filesystem.PinotFS;\n+import org.apache.pinot.spi.filesystem.PinotFSFactory;\n+import org.apache.pinot.spi.ingestion.batch.BatchConfigProperties;\n+import org.apache.pinot.spi.plugin.PluginManager;\n+import org.apache.pinot.spi.utils.IngestionConfigUtils;\n+import org.apache.pinot.spi.utils.JsonUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class SegmentGenerationAndPushTaskGenerator implements PinotTaskGenerator {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(SegmentGenerationAndPushTaskGenerator.class);\n+\n+  private final ClusterInfoAccessor _clusterInfoAccessor;\n+\n+  public SegmentGenerationAndPushTaskGenerator(ClusterInfoAccessor clusterInfoAccessor) {\n+    _clusterInfoAccessor = clusterInfoAccessor;\n+  }\n+\n+  @Override\n+  public String getTaskType() {\n+    return MinionConstants.SegmentGenerationAndPushTask.TASK_TYPE;\n+  }\n+\n+  @Override\n+  public List<PinotTaskConfig> generateTasks(List<TableConfig> tableConfigs) {\n+    List<PinotTaskConfig> pinotTaskConfigs = new ArrayList<>();\n+\n+    for (TableConfig tableConfig : tableConfigs) {\n+      // Only generate tasks for OFFLINE tables\n+      String offlineTableName = tableConfig.getTableName();\n+      if (tableConfig.getTableType() != TableType.OFFLINE) {\n+        LOGGER.warn(\"Skip generating SegmentGenerationAndPushTask for non-OFFLINE table: {}\", offlineTableName);\n+        continue;\n+      }\n+\n+      TableTaskConfig tableTaskConfig = tableConfig.getTaskConfig();\n+      Preconditions.checkNotNull(tableTaskConfig);\n+      Map<String, String> taskConfigs =\n+          tableTaskConfig.getConfigsForTaskType(MinionConstants.SegmentGenerationAndPushTask.TASK_TYPE);\n+      Preconditions.checkNotNull(taskConfigs, \"Task config shouldn't be null for Table: {}\", offlineTableName);\n+\n+      // Get max number of tasks for this table\n+      int tableMaxNumTasks;\n+      String tableMaxNumTasksConfig = taskConfigs.get(MinionConstants.TABLE_MAX_NUM_TASKS_KEY);\n+      if (tableMaxNumTasksConfig != null) {\n+        try {\n+          tableMaxNumTasks = Integer.parseInt(tableMaxNumTasksConfig);\n+        } catch (Exception e) {\n+          tableMaxNumTasks = Integer.MAX_VALUE;\n+        }\n+      } else {\n+        tableMaxNumTasks = Integer.MAX_VALUE;\n+      }\n+\n+      // Generate tasks\n+      int tableNumTasks = 0;\n+      // Generate up to tableMaxNumTasks tasks each time for each table\n+      if (tableNumTasks == tableMaxNumTasks) {\n+        break;\n+      }\n+      String batchSegmentIngestionType = IngestionConfigUtils.getBatchSegmentIngestionType(tableConfig);\n+      String batchSegmentIngestionFrequency = IngestionConfigUtils.getBatchSegmentIngestionFrequency(tableConfig);\n+      BatchIngestionConfig batchIngestionConfig = tableConfig.getIngestionConfig().getBatchIngestionConfig();\n+      List<Map<String, String>> batchConfigMaps = batchIngestionConfig.getBatchConfigMaps();\n+      for (Map<String, String> batchConfigMap : batchConfigMaps) {\n+        try {\n+          URI inputDirURI = getDirectoryUri(batchConfigMap.get(BatchConfigProperties.INPUT_DIR_URI));\n+          URI outputDirURI = getDirectoryUri(batchConfigMap.get(BatchConfigProperties.OUTPUT_DIR_URI));\n+\n+          updateRecordReaderConfigs(batchConfigMap);\n+          List<OfflineSegmentZKMetadata> offlineSegmentsMetadata = Collections.emptyList();\n+          // For append mode, we don't create segments for input file URIs already created.\n+          if (BatchConfigProperties.SegmentIngestionType.APPEND.name().equalsIgnoreCase(batchSegmentIngestionType)) {\n+            offlineSegmentsMetadata = this._clusterInfoAccessor.getOfflineSegmentsMetadata(offlineTableName);\n+          }\n+          List<URI> inputFileURIs = getInputFilesFromDirectory(batchConfigMap, inputDirURI,\n+              getExistingSegmentInputFiles(offlineSegmentsMetadata));\n+\n+          String pushMode = IngestionConfigUtils.getPushMode(batchConfigMap);\n+          for (URI inputFileURI : inputFileURIs) {\n+            Map<String, String> singleFileGenerationTaskConfig = new HashMap<>(batchConfigMap);\n+            singleFileGenerationTaskConfig.put(BatchConfigProperties.INPUT_FILE_URI, inputFileURI.toString());\n+            URI outputSegmentDirURI = getRelativeOutputPath(inputDirURI, inputFileURI, outputDirURI);\n+            singleFileGenerationTaskConfig.put(BatchConfigProperties.OUTPUT_SEGMENT_DIR_URI, outputSegmentDirURI.toString());\n+            singleFileGenerationTaskConfig\n+                .put(BatchConfigProperties.SCHEMA, JsonUtils.objectToString(_clusterInfoAccessor.getTableSchema(offlineTableName)));\n+            singleFileGenerationTaskConfig\n+                .put(BatchConfigProperties.TABLE_CONFIGS, JsonUtils.objectToString(_clusterInfoAccessor.getTableConfig(offlineTableName)));\n+            singleFileGenerationTaskConfig.put(BatchConfigProperties.SEQUENCE_ID, String.valueOf(tableNumTasks));\n+            singleFileGenerationTaskConfig.put(BatchConfigProperties.SEGMENT_NAME_GENERATOR_TYPE, BatchConfigProperties.SegmentNameGeneratorType.SIMPLE);\n+            singleFileGenerationTaskConfig.put(BatchConfigProperties.PUSH_MODE, pushMode);\n+            singleFileGenerationTaskConfig.put(BatchConfigProperties.PUSH_CONTROLLER_URI, _clusterInfoAccessor.getVipUrl());\n+            // Only submit raw data files with timestamp larger than checkpoint\n+            pinotTaskConfigs.add(new PinotTaskConfig(MinionConstants.SegmentGenerationAndPushTask.TASK_TYPE,\n+                singleFileGenerationTaskConfig));\n+            tableNumTasks++;\n+\n+            // Generate up to tableMaxNumTasks tasks each time for each table\n+            if (tableNumTasks == tableMaxNumTasks) {\n+              break;\n+            }\n+          }\n+        } catch (Exception e) {\n+          LOGGER.error(\"Unable to generate the SegmentGenerationAndPush task. [ table configs: {}, task configs: {} ]\",\n+              tableConfig, taskConfigs, e);\n+        }\n+      }\n+    }\n+    return pinotTaskConfigs;\n+  }\n+\n+  private void updateRecordReaderConfigs(Map<String, String> batchConfigMap) {\n+    String inputFormat = batchConfigMap.get(BatchConfigProperties.INPUT_FORMAT);\n+    String recordReaderClassName = PluginManager.get().getRecordReaderClassName(inputFormat);\n+    if (recordReaderClassName != null) {\n+      batchConfigMap.putIfAbsent(BatchConfigProperties.RECORD_READER_CLASS, recordReaderClassName);\n+    }\n+    String recordReaderConfigClassName = PluginManager.get().getRecordReaderConfigClassName(inputFormat);\n+    if (recordReaderConfigClassName != null) {\n+      batchConfigMap.putIfAbsent(BatchConfigProperties.RECORD_READER_CONFIG_CLASS, recordReaderConfigClassName);\n+    }\n+  }\n+\n+  private List<URI> getInputFilesFromDirectory(Map<String, String> batchConfigMap, URI inputDirURI,\n+      Set<String> existingSegmentInputFileURIs) {\n+    String inputDirURIScheme = inputDirURI.getScheme();\n+    if (!PinotFSFactory.isSchemeSupported(inputDirURIScheme)) {\n+      String fsClass = batchConfigMap.get(BatchConfigProperties.INPUT_FS_CLASS);\n+      PinotConfiguration fsProps = IngestionConfigUtils.getFsProps(batchConfigMap);\n+      PinotFSFactory.register(inputDirURIScheme, fsClass, fsProps);\n+    }\n+    PinotFS inputDirFS = PinotFSFactory.create(inputDirURIScheme);\n+\n+    String includeFileNamePattern = batchConfigMap.get(BatchConfigProperties.INCLUDE_FILE_NAME_PATTERN);\n+    String excludeFileNamePattern = batchConfigMap.get(BatchConfigProperties.EXCLUDE_FILE_NAME_PATTERN);\n+\n+    //Get list of files to process\n+    String[] files;\n+    try {\n+      files = inputDirFS.listFiles(inputDirURI, true);\n+    } catch (IOException e) {\n+      LOGGER.error(\"Unable to list files under URI: \" + inputDirURI, e);\n+      return Collections.emptyList();\n+    }\n+    PathMatcher includeFilePathMatcher = null;\n+    if (includeFileNamePattern != null) {\n+      includeFilePathMatcher = FileSystems.getDefault().getPathMatcher(includeFileNamePattern);\n+    }\n+    PathMatcher excludeFilePathMatcher = null;\n+    if (excludeFileNamePattern != null) {\n+      excludeFilePathMatcher = FileSystems.getDefault().getPathMatcher(excludeFileNamePattern);\n+    }\n+    List<URI> inputFileURIs = new ArrayList<>();\n+    for (String file : files) {\n+      if (includeFilePathMatcher != null) {\n+        if (!includeFilePathMatcher.matches(Paths.get(file))) {\n+          continue;\n+        }\n+      }\n+      if (excludeFilePathMatcher != null) {\n+        if (excludeFilePathMatcher.matches(Paths.get(file))) {\n+          continue;\n+        }\n+      }\n+      try {\n+        URI inputFileURI = new URI(file);\n+        if (inputFileURI.getScheme() == null) {\n+          inputFileURI = new File(file).toURI();\n+        }\n+        if (inputDirFS.isDirectory(inputFileURI) || existingSegmentInputFileURIs.contains(inputFileURI.toString())) {\n+          continue;\n+        }\n+        inputFileURIs.add(inputFileURI);\n+      } catch (Exception e) {\n+        continue;\n+      }\n+    }\n+    return inputFileURIs;\n+  }\n+\n+  private Set<String> getExistingSegmentInputFiles(List<OfflineSegmentZKMetadata> offlineSegmentsMetadata) {\n+    Set<String> existingSegmentInputFiles = new HashSet<>();\n+    for (OfflineSegmentZKMetadata metadata : offlineSegmentsMetadata) {\n+      if ((metadata.getCustomMap() != null) && metadata.getCustomMap()\n+          .containsKey(BatchConfigProperties.INPUT_DATA_FILE_URI_KEY)) {\n+        existingSegmentInputFiles.add(metadata.getCustomMap().get(BatchConfigProperties.INPUT_DATA_FILE_URI_KEY));\n+      }\n+    }\n+    return existingSegmentInputFiles;\n+  }\n+\n+  private URI getDirectoryUri(String uriStr)\n+      throws URISyntaxException {\n+    URI uri = new URI(uriStr);\n+    if (uri.getScheme() == null) {\n+      uri = new File(uriStr).toURI();\n+    }\n+    return uri;\n+  }\n+\n+  public static URI getRelativeOutputPath(URI baseInputDir, URI inputFile, URI outputDir) {\n+    URI relativePath = baseInputDir.relativize(inputFile);\n+    Preconditions.checkState(relativePath.getPath().length() > 0 && !relativePath.equals(inputFile),\n+        \"Unable to extract out the relative path based on base input path: \" + baseInputDir);\n+    String outputDirStr = outputDir.toString();\n+    outputDir = !outputDirStr.endsWith(\"/\") ? URI.create(outputDirStr.concat(\"/\")) : outputDir;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4d7abbde616e9a82923b6ec39a01405ee6826b58"}, "originalPosition": 251}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDY4NTI0Mw==", "bodyText": "This is copied from pinot-ingestion-common module, I would keep it the same way and maybe later on merge them.", "url": "https://github.com/apache/pinot/pull/6340#discussion_r544685243", "createdAt": "2020-12-16T23:00:09Z", "author": {"login": "xiangfu0"}, "path": "pinot-controller/src/main/java/org/apache/pinot/controller/helix/core/minion/generator/SegmentGenerationAndPushTaskGenerator.java", "diffHunk": "@@ -0,0 +1,255 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.helix.core.minion.generator;\n+\n+import com.google.common.base.Preconditions;\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.nio.file.FileSystems;\n+import java.nio.file.PathMatcher;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.pinot.common.metadata.segment.OfflineSegmentZKMetadata;\n+import org.apache.pinot.controller.helix.core.minion.ClusterInfoAccessor;\n+import org.apache.pinot.core.common.MinionConstants;\n+import org.apache.pinot.core.minion.PinotTaskConfig;\n+import org.apache.pinot.spi.config.table.TableConfig;\n+import org.apache.pinot.spi.config.table.TableTaskConfig;\n+import org.apache.pinot.spi.config.table.TableType;\n+import org.apache.pinot.spi.config.table.ingestion.BatchIngestionConfig;\n+import org.apache.pinot.spi.env.PinotConfiguration;\n+import org.apache.pinot.spi.filesystem.PinotFS;\n+import org.apache.pinot.spi.filesystem.PinotFSFactory;\n+import org.apache.pinot.spi.ingestion.batch.BatchConfigProperties;\n+import org.apache.pinot.spi.plugin.PluginManager;\n+import org.apache.pinot.spi.utils.IngestionConfigUtils;\n+import org.apache.pinot.spi.utils.JsonUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class SegmentGenerationAndPushTaskGenerator implements PinotTaskGenerator {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(SegmentGenerationAndPushTaskGenerator.class);\n+\n+  private final ClusterInfoAccessor _clusterInfoAccessor;\n+\n+  public SegmentGenerationAndPushTaskGenerator(ClusterInfoAccessor clusterInfoAccessor) {\n+    _clusterInfoAccessor = clusterInfoAccessor;\n+  }\n+\n+  @Override\n+  public String getTaskType() {\n+    return MinionConstants.SegmentGenerationAndPushTask.TASK_TYPE;\n+  }\n+\n+  @Override\n+  public List<PinotTaskConfig> generateTasks(List<TableConfig> tableConfigs) {\n+    List<PinotTaskConfig> pinotTaskConfigs = new ArrayList<>();\n+\n+    for (TableConfig tableConfig : tableConfigs) {\n+      // Only generate tasks for OFFLINE tables\n+      String offlineTableName = tableConfig.getTableName();\n+      if (tableConfig.getTableType() != TableType.OFFLINE) {\n+        LOGGER.warn(\"Skip generating SegmentGenerationAndPushTask for non-OFFLINE table: {}\", offlineTableName);\n+        continue;\n+      }\n+\n+      TableTaskConfig tableTaskConfig = tableConfig.getTaskConfig();\n+      Preconditions.checkNotNull(tableTaskConfig);\n+      Map<String, String> taskConfigs =\n+          tableTaskConfig.getConfigsForTaskType(MinionConstants.SegmentGenerationAndPushTask.TASK_TYPE);\n+      Preconditions.checkNotNull(taskConfigs, \"Task config shouldn't be null for Table: {}\", offlineTableName);\n+\n+      // Get max number of tasks for this table\n+      int tableMaxNumTasks;\n+      String tableMaxNumTasksConfig = taskConfigs.get(MinionConstants.TABLE_MAX_NUM_TASKS_KEY);\n+      if (tableMaxNumTasksConfig != null) {\n+        try {\n+          tableMaxNumTasks = Integer.parseInt(tableMaxNumTasksConfig);\n+        } catch (Exception e) {\n+          tableMaxNumTasks = Integer.MAX_VALUE;\n+        }\n+      } else {\n+        tableMaxNumTasks = Integer.MAX_VALUE;\n+      }\n+\n+      // Generate tasks\n+      int tableNumTasks = 0;\n+      // Generate up to tableMaxNumTasks tasks each time for each table\n+      if (tableNumTasks == tableMaxNumTasks) {\n+        break;\n+      }\n+      String batchSegmentIngestionType = IngestionConfigUtils.getBatchSegmentIngestionType(tableConfig);\n+      String batchSegmentIngestionFrequency = IngestionConfigUtils.getBatchSegmentIngestionFrequency(tableConfig);\n+      BatchIngestionConfig batchIngestionConfig = tableConfig.getIngestionConfig().getBatchIngestionConfig();\n+      List<Map<String, String>> batchConfigMaps = batchIngestionConfig.getBatchConfigMaps();\n+      for (Map<String, String> batchConfigMap : batchConfigMaps) {\n+        try {\n+          URI inputDirURI = getDirectoryUri(batchConfigMap.get(BatchConfigProperties.INPUT_DIR_URI));\n+          URI outputDirURI = getDirectoryUri(batchConfigMap.get(BatchConfigProperties.OUTPUT_DIR_URI));\n+\n+          updateRecordReaderConfigs(batchConfigMap);\n+          List<OfflineSegmentZKMetadata> offlineSegmentsMetadata = Collections.emptyList();\n+          // For append mode, we don't create segments for input file URIs already created.\n+          if (BatchConfigProperties.SegmentIngestionType.APPEND.name().equalsIgnoreCase(batchSegmentIngestionType)) {\n+            offlineSegmentsMetadata = this._clusterInfoAccessor.getOfflineSegmentsMetadata(offlineTableName);\n+          }\n+          List<URI> inputFileURIs = getInputFilesFromDirectory(batchConfigMap, inputDirURI,\n+              getExistingSegmentInputFiles(offlineSegmentsMetadata));\n+\n+          String pushMode = IngestionConfigUtils.getPushMode(batchConfigMap);\n+          for (URI inputFileURI : inputFileURIs) {\n+            Map<String, String> singleFileGenerationTaskConfig = new HashMap<>(batchConfigMap);\n+            singleFileGenerationTaskConfig.put(BatchConfigProperties.INPUT_FILE_URI, inputFileURI.toString());\n+            URI outputSegmentDirURI = getRelativeOutputPath(inputDirURI, inputFileURI, outputDirURI);\n+            singleFileGenerationTaskConfig.put(BatchConfigProperties.OUTPUT_SEGMENT_DIR_URI, outputSegmentDirURI.toString());\n+            singleFileGenerationTaskConfig\n+                .put(BatchConfigProperties.SCHEMA, JsonUtils.objectToString(_clusterInfoAccessor.getTableSchema(offlineTableName)));\n+            singleFileGenerationTaskConfig\n+                .put(BatchConfigProperties.TABLE_CONFIGS, JsonUtils.objectToString(_clusterInfoAccessor.getTableConfig(offlineTableName)));\n+            singleFileGenerationTaskConfig.put(BatchConfigProperties.SEQUENCE_ID, String.valueOf(tableNumTasks));\n+            singleFileGenerationTaskConfig.put(BatchConfigProperties.SEGMENT_NAME_GENERATOR_TYPE, BatchConfigProperties.SegmentNameGeneratorType.SIMPLE);\n+            singleFileGenerationTaskConfig.put(BatchConfigProperties.PUSH_MODE, pushMode);\n+            singleFileGenerationTaskConfig.put(BatchConfigProperties.PUSH_CONTROLLER_URI, _clusterInfoAccessor.getVipUrl());\n+            // Only submit raw data files with timestamp larger than checkpoint\n+            pinotTaskConfigs.add(new PinotTaskConfig(MinionConstants.SegmentGenerationAndPushTask.TASK_TYPE,\n+                singleFileGenerationTaskConfig));\n+            tableNumTasks++;\n+\n+            // Generate up to tableMaxNumTasks tasks each time for each table\n+            if (tableNumTasks == tableMaxNumTasks) {\n+              break;\n+            }\n+          }\n+        } catch (Exception e) {\n+          LOGGER.error(\"Unable to generate the SegmentGenerationAndPush task. [ table configs: {}, task configs: {} ]\",\n+              tableConfig, taskConfigs, e);\n+        }\n+      }\n+    }\n+    return pinotTaskConfigs;\n+  }\n+\n+  private void updateRecordReaderConfigs(Map<String, String> batchConfigMap) {\n+    String inputFormat = batchConfigMap.get(BatchConfigProperties.INPUT_FORMAT);\n+    String recordReaderClassName = PluginManager.get().getRecordReaderClassName(inputFormat);\n+    if (recordReaderClassName != null) {\n+      batchConfigMap.putIfAbsent(BatchConfigProperties.RECORD_READER_CLASS, recordReaderClassName);\n+    }\n+    String recordReaderConfigClassName = PluginManager.get().getRecordReaderConfigClassName(inputFormat);\n+    if (recordReaderConfigClassName != null) {\n+      batchConfigMap.putIfAbsent(BatchConfigProperties.RECORD_READER_CONFIG_CLASS, recordReaderConfigClassName);\n+    }\n+  }\n+\n+  private List<URI> getInputFilesFromDirectory(Map<String, String> batchConfigMap, URI inputDirURI,\n+      Set<String> existingSegmentInputFileURIs) {\n+    String inputDirURIScheme = inputDirURI.getScheme();\n+    if (!PinotFSFactory.isSchemeSupported(inputDirURIScheme)) {\n+      String fsClass = batchConfigMap.get(BatchConfigProperties.INPUT_FS_CLASS);\n+      PinotConfiguration fsProps = IngestionConfigUtils.getFsProps(batchConfigMap);\n+      PinotFSFactory.register(inputDirURIScheme, fsClass, fsProps);\n+    }\n+    PinotFS inputDirFS = PinotFSFactory.create(inputDirURIScheme);\n+\n+    String includeFileNamePattern = batchConfigMap.get(BatchConfigProperties.INCLUDE_FILE_NAME_PATTERN);\n+    String excludeFileNamePattern = batchConfigMap.get(BatchConfigProperties.EXCLUDE_FILE_NAME_PATTERN);\n+\n+    //Get list of files to process\n+    String[] files;\n+    try {\n+      files = inputDirFS.listFiles(inputDirURI, true);\n+    } catch (IOException e) {\n+      LOGGER.error(\"Unable to list files under URI: \" + inputDirURI, e);\n+      return Collections.emptyList();\n+    }\n+    PathMatcher includeFilePathMatcher = null;\n+    if (includeFileNamePattern != null) {\n+      includeFilePathMatcher = FileSystems.getDefault().getPathMatcher(includeFileNamePattern);\n+    }\n+    PathMatcher excludeFilePathMatcher = null;\n+    if (excludeFileNamePattern != null) {\n+      excludeFilePathMatcher = FileSystems.getDefault().getPathMatcher(excludeFileNamePattern);\n+    }\n+    List<URI> inputFileURIs = new ArrayList<>();\n+    for (String file : files) {\n+      if (includeFilePathMatcher != null) {\n+        if (!includeFilePathMatcher.matches(Paths.get(file))) {\n+          continue;\n+        }\n+      }\n+      if (excludeFilePathMatcher != null) {\n+        if (excludeFilePathMatcher.matches(Paths.get(file))) {\n+          continue;\n+        }\n+      }\n+      try {\n+        URI inputFileURI = new URI(file);\n+        if (inputFileURI.getScheme() == null) {\n+          inputFileURI = new File(file).toURI();\n+        }\n+        if (inputDirFS.isDirectory(inputFileURI) || existingSegmentInputFileURIs.contains(inputFileURI.toString())) {\n+          continue;\n+        }\n+        inputFileURIs.add(inputFileURI);\n+      } catch (Exception e) {\n+        continue;\n+      }\n+    }\n+    return inputFileURIs;\n+  }\n+\n+  private Set<String> getExistingSegmentInputFiles(List<OfflineSegmentZKMetadata> offlineSegmentsMetadata) {\n+    Set<String> existingSegmentInputFiles = new HashSet<>();\n+    for (OfflineSegmentZKMetadata metadata : offlineSegmentsMetadata) {\n+      if ((metadata.getCustomMap() != null) && metadata.getCustomMap()\n+          .containsKey(BatchConfigProperties.INPUT_DATA_FILE_URI_KEY)) {\n+        existingSegmentInputFiles.add(metadata.getCustomMap().get(BatchConfigProperties.INPUT_DATA_FILE_URI_KEY));\n+      }\n+    }\n+    return existingSegmentInputFiles;\n+  }\n+\n+  private URI getDirectoryUri(String uriStr)\n+      throws URISyntaxException {\n+    URI uri = new URI(uriStr);\n+    if (uri.getScheme() == null) {\n+      uri = new File(uriStr).toURI();\n+    }\n+    return uri;\n+  }\n+\n+  public static URI getRelativeOutputPath(URI baseInputDir, URI inputFile, URI outputDir) {\n+    URI relativePath = baseInputDir.relativize(inputFile);\n+    Preconditions.checkState(relativePath.getPath().length() > 0 && !relativePath.equals(inputFile),\n+        \"Unable to extract out the relative path based on base input path: \" + baseInputDir);\n+    String outputDirStr = outputDir.toString();\n+    outputDir = !outputDirStr.endsWith(\"/\") ? URI.create(outputDirStr.concat(\"/\")) : outputDir;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDY0NTI0NQ=="}, "originalCommit": {"oid": "4d7abbde616e9a82923b6ec39a01405ee6826b58"}, "originalPosition": 251}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NTExNDQ2OnYy", "diffSide": "RIGHT", "path": "pinot-minion/src/main/java/org/apache/pinot/minion/executor/SegmentGenerationAndPushResult.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwMjozNjoyN1rOIDmbcg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQyMjo0NToyN1rOIHcfwA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDY0NjI1OA==", "bodyText": "Does this class need to be JSON deserialized? If so, would be good to add annotations to gracefully handle name changes, new or missing member variables etc.", "url": "https://github.com/apache/pinot/pull/6340#discussion_r540646258", "createdAt": "2020-12-11T02:36:27Z", "author": {"login": "mayankshriv"}, "path": "pinot-minion/src/main/java/org/apache/pinot/minion/executor/SegmentGenerationAndPushResult.java", "diffHunk": "@@ -0,0 +1,91 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.minion.executor;\n+\n+import java.io.File;\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.pinot.core.minion.PinotTaskConfig;\n+\n+\n+/**\n+ * The class <code>SegmentGenerationAndPushResult</code> wraps the segment generation and push\n+ * results.\n+ */\n+public class SegmentGenerationAndPushResult {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4d7abbde616e9a82923b6ec39a01405ee6826b58"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDY3NzgyNA==", "bodyText": "it won't.  This will be set in our code then serialize it to json", "url": "https://github.com/apache/pinot/pull/6340#discussion_r544677824", "createdAt": "2020-12-16T22:45:27Z", "author": {"login": "xiangfu0"}, "path": "pinot-minion/src/main/java/org/apache/pinot/minion/executor/SegmentGenerationAndPushResult.java", "diffHunk": "@@ -0,0 +1,91 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.minion.executor;\n+\n+import java.io.File;\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.pinot.core.minion.PinotTaskConfig;\n+\n+\n+/**\n+ * The class <code>SegmentGenerationAndPushResult</code> wraps the segment generation and push\n+ * results.\n+ */\n+public class SegmentGenerationAndPushResult {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDY0NjI1OA=="}, "originalCommit": {"oid": "4d7abbde616e9a82923b6ec39a01405ee6826b58"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NTExNzgxOnYy", "diffSide": "RIGHT", "path": "pinot-minion/src/main/java/org/apache/pinot/minion/executor/SegmentGenerationAndPushTaskExecutor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwMjozNzozNlrOIDmdLw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwMjozNzozNlrOIDmdLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDY0NjcwMw==", "bodyText": "Consider refactor into smaller utility routines.\nAlso, Return type Object seems a bit unsettling for an api. But that's outside the scope of this PR.", "url": "https://github.com/apache/pinot/pull/6340#discussion_r540646703", "createdAt": "2020-12-11T02:37:36Z", "author": {"login": "mayankshriv"}, "path": "pinot-minion/src/main/java/org/apache/pinot/minion/executor/SegmentGenerationAndPushTaskExecutor.java", "diffHunk": "@@ -0,0 +1,205 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.minion.executor;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import java.io.File;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.common.utils.TarGzCompressionUtils;\n+import org.apache.pinot.core.minion.PinotTaskConfig;\n+import org.apache.pinot.plugin.ingestion.batch.common.SegmentGenerationTaskRunner;\n+import org.apache.pinot.plugin.ingestion.batch.common.SegmentGenerationUtils;\n+import org.apache.pinot.plugin.ingestion.batch.common.SegmentPushUtils;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.spi.env.PinotConfiguration;\n+import org.apache.pinot.spi.filesystem.LocalPinotFS;\n+import org.apache.pinot.spi.filesystem.PinotFS;\n+import org.apache.pinot.spi.filesystem.PinotFSFactory;\n+import org.apache.pinot.spi.ingestion.batch.BatchConfigProperties;\n+import org.apache.pinot.spi.ingestion.batch.spec.Constants;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotClusterSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PushJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.RecordReaderSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentGenerationJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentGenerationTaskSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentNameGeneratorSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.TableSpec;\n+import org.apache.pinot.spi.utils.DataSizeUtils;\n+import org.apache.pinot.spi.utils.IngestionConfigUtils;\n+import org.apache.pinot.spi.utils.JsonUtils;\n+import org.apache.pinot.spi.utils.retry.AttemptsExceededException;\n+import org.apache.pinot.spi.utils.retry.RetriableOperationException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class SegmentGenerationAndPushTaskExecutor extends BaseTaskExecutor {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(SegmentGenerationAndPushTaskExecutor.class);\n+\n+  private static final PinotFS LOCAL_PINOT_FS = new LocalPinotFS();\n+\n+  @Override\n+  public Object executeTask(PinotTaskConfig pinotTaskConfig)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4d7abbde616e9a82923b6ec39a01405ee6826b58"}, "originalPosition": 64}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NTEyMTM0OnYy", "diffSide": "RIGHT", "path": "pinot-minion/src/main/java/org/apache/pinot/minion/executor/SegmentGenerationAndPushTaskExecutor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwMjozOToyN1rOIDmfRQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQyMjo0NjozMFrOIHchnw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDY0NzIzNw==", "bodyText": "Should generation and push be decoupled into separate sub-tasks?", "url": "https://github.com/apache/pinot/pull/6340#discussion_r540647237", "createdAt": "2020-12-11T02:39:27Z", "author": {"login": "mayankshriv"}, "path": "pinot-minion/src/main/java/org/apache/pinot/minion/executor/SegmentGenerationAndPushTaskExecutor.java", "diffHunk": "@@ -0,0 +1,205 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.minion.executor;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import java.io.File;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.common.utils.TarGzCompressionUtils;\n+import org.apache.pinot.core.minion.PinotTaskConfig;\n+import org.apache.pinot.plugin.ingestion.batch.common.SegmentGenerationTaskRunner;\n+import org.apache.pinot.plugin.ingestion.batch.common.SegmentGenerationUtils;\n+import org.apache.pinot.plugin.ingestion.batch.common.SegmentPushUtils;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.spi.env.PinotConfiguration;\n+import org.apache.pinot.spi.filesystem.LocalPinotFS;\n+import org.apache.pinot.spi.filesystem.PinotFS;\n+import org.apache.pinot.spi.filesystem.PinotFSFactory;\n+import org.apache.pinot.spi.ingestion.batch.BatchConfigProperties;\n+import org.apache.pinot.spi.ingestion.batch.spec.Constants;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotClusterSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PushJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.RecordReaderSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentGenerationJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentGenerationTaskSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentNameGeneratorSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.TableSpec;\n+import org.apache.pinot.spi.utils.DataSizeUtils;\n+import org.apache.pinot.spi.utils.IngestionConfigUtils;\n+import org.apache.pinot.spi.utils.JsonUtils;\n+import org.apache.pinot.spi.utils.retry.AttemptsExceededException;\n+import org.apache.pinot.spi.utils.retry.RetriableOperationException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class SegmentGenerationAndPushTaskExecutor extends BaseTaskExecutor {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4d7abbde616e9a82923b6ec39a01405ee6826b58"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDY3ODMwMw==", "bodyText": "i feel it should be combined as build and push. otherwise we need to let minion segment generation job submit a push job?", "url": "https://github.com/apache/pinot/pull/6340#discussion_r544678303", "createdAt": "2020-12-16T22:46:30Z", "author": {"login": "xiangfu0"}, "path": "pinot-minion/src/main/java/org/apache/pinot/minion/executor/SegmentGenerationAndPushTaskExecutor.java", "diffHunk": "@@ -0,0 +1,205 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.minion.executor;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import java.io.File;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.common.utils.TarGzCompressionUtils;\n+import org.apache.pinot.core.minion.PinotTaskConfig;\n+import org.apache.pinot.plugin.ingestion.batch.common.SegmentGenerationTaskRunner;\n+import org.apache.pinot.plugin.ingestion.batch.common.SegmentGenerationUtils;\n+import org.apache.pinot.plugin.ingestion.batch.common.SegmentPushUtils;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.spi.env.PinotConfiguration;\n+import org.apache.pinot.spi.filesystem.LocalPinotFS;\n+import org.apache.pinot.spi.filesystem.PinotFS;\n+import org.apache.pinot.spi.filesystem.PinotFSFactory;\n+import org.apache.pinot.spi.ingestion.batch.BatchConfigProperties;\n+import org.apache.pinot.spi.ingestion.batch.spec.Constants;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotClusterSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PushJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.RecordReaderSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentGenerationJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentGenerationTaskSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentNameGeneratorSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.TableSpec;\n+import org.apache.pinot.spi.utils.DataSizeUtils;\n+import org.apache.pinot.spi.utils.IngestionConfigUtils;\n+import org.apache.pinot.spi.utils.JsonUtils;\n+import org.apache.pinot.spi.utils.retry.AttemptsExceededException;\n+import org.apache.pinot.spi.utils.retry.RetriableOperationException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class SegmentGenerationAndPushTaskExecutor extends BaseTaskExecutor {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDY0NzIzNw=="}, "originalCommit": {"oid": "4d7abbde616e9a82923b6ec39a01405ee6826b58"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyMzQyODI2OnYy", "diffSide": "RIGHT", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/helix/core/minion/generator/SegmentGenerationAndPushTaskGenerator.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwMTo1NjozOFrOIHhGvg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwMTo1NjozOFrOIHhGvg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDc1MzM0Mg==", "bodyText": "please add some java docs on the Generator and the Executor and ExecutorFactory for the new task, explaining which configs are looked at and how they are used", "url": "https://github.com/apache/pinot/pull/6340#discussion_r544753342", "createdAt": "2020-12-17T01:56:38Z", "author": {"login": "npawar"}, "path": "pinot-controller/src/main/java/org/apache/pinot/controller/helix/core/minion/generator/SegmentGenerationAndPushTaskGenerator.java", "diffHunk": "@@ -0,0 +1,267 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.helix.core.minion.generator;\n+\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Preconditions;\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.nio.file.FileSystems;\n+import java.nio.file.PathMatcher;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.pinot.common.metadata.segment.OfflineSegmentZKMetadata;\n+import org.apache.pinot.controller.helix.core.minion.ClusterInfoAccessor;\n+import org.apache.pinot.core.common.MinionConstants;\n+import org.apache.pinot.core.minion.PinotTaskConfig;\n+import org.apache.pinot.spi.config.table.TableConfig;\n+import org.apache.pinot.spi.config.table.TableTaskConfig;\n+import org.apache.pinot.spi.config.table.TableType;\n+import org.apache.pinot.spi.config.table.ingestion.BatchIngestionConfig;\n+import org.apache.pinot.spi.env.PinotConfiguration;\n+import org.apache.pinot.spi.filesystem.PinotFS;\n+import org.apache.pinot.spi.filesystem.PinotFSFactory;\n+import org.apache.pinot.spi.ingestion.batch.BatchConfigProperties;\n+import org.apache.pinot.spi.plugin.PluginManager;\n+import org.apache.pinot.spi.utils.IngestionConfigUtils;\n+import org.apache.pinot.spi.utils.JsonUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class SegmentGenerationAndPushTaskGenerator implements PinotTaskGenerator {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a254c01b685ad6fcc08c665fb92329f53098f209"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyMzQ5NjY1OnYy", "diffSide": "RIGHT", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/helix/core/minion/generator/SegmentGenerationAndPushTaskGenerator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwMjoyMzozMlrOIHhriw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwNTo1NzozM1rOIIUJRQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDc2Mjc2Mw==", "bodyText": "this method can be private?", "url": "https://github.com/apache/pinot/pull/6340#discussion_r544762763", "createdAt": "2020-12-17T02:23:32Z", "author": {"login": "npawar"}, "path": "pinot-controller/src/main/java/org/apache/pinot/controller/helix/core/minion/generator/SegmentGenerationAndPushTaskGenerator.java", "diffHunk": "@@ -0,0 +1,267 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.helix.core.minion.generator;\n+\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Preconditions;\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.nio.file.FileSystems;\n+import java.nio.file.PathMatcher;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.pinot.common.metadata.segment.OfflineSegmentZKMetadata;\n+import org.apache.pinot.controller.helix.core.minion.ClusterInfoAccessor;\n+import org.apache.pinot.core.common.MinionConstants;\n+import org.apache.pinot.core.minion.PinotTaskConfig;\n+import org.apache.pinot.spi.config.table.TableConfig;\n+import org.apache.pinot.spi.config.table.TableTaskConfig;\n+import org.apache.pinot.spi.config.table.TableType;\n+import org.apache.pinot.spi.config.table.ingestion.BatchIngestionConfig;\n+import org.apache.pinot.spi.env.PinotConfiguration;\n+import org.apache.pinot.spi.filesystem.PinotFS;\n+import org.apache.pinot.spi.filesystem.PinotFSFactory;\n+import org.apache.pinot.spi.ingestion.batch.BatchConfigProperties;\n+import org.apache.pinot.spi.plugin.PluginManager;\n+import org.apache.pinot.spi.utils.IngestionConfigUtils;\n+import org.apache.pinot.spi.utils.JsonUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class SegmentGenerationAndPushTaskGenerator implements PinotTaskGenerator {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(SegmentGenerationAndPushTaskGenerator.class);\n+\n+  private final ClusterInfoAccessor _clusterInfoAccessor;\n+\n+  public SegmentGenerationAndPushTaskGenerator(ClusterInfoAccessor clusterInfoAccessor) {\n+    _clusterInfoAccessor = clusterInfoAccessor;\n+  }\n+\n+  @Override\n+  public String getTaskType() {\n+    return MinionConstants.SegmentGenerationAndPushTask.TASK_TYPE;\n+  }\n+\n+  @Override\n+  public List<PinotTaskConfig> generateTasks(List<TableConfig> tableConfigs) {\n+    List<PinotTaskConfig> pinotTaskConfigs = new ArrayList<>();\n+\n+    for (TableConfig tableConfig : tableConfigs) {\n+      // Only generate tasks for OFFLINE tables\n+      String offlineTableName = tableConfig.getTableName();\n+      if (tableConfig.getTableType() != TableType.OFFLINE) {\n+        LOGGER.warn(\"Skip generating SegmentGenerationAndPushTask for non-OFFLINE table: {}\", offlineTableName);\n+        continue;\n+      }\n+\n+      TableTaskConfig tableTaskConfig = tableConfig.getTaskConfig();\n+      Preconditions.checkNotNull(tableTaskConfig);\n+      Map<String, String> taskConfigs =\n+          tableTaskConfig.getConfigsForTaskType(MinionConstants.SegmentGenerationAndPushTask.TASK_TYPE);\n+      if (tableConfigs == null) {\n+        LOGGER.warn(\"Skip null task config for table: {}\", offlineTableName);\n+      }\n+\n+      // Get max number of tasks for this table\n+      int tableMaxNumTasks;\n+      String tableMaxNumTasksConfig = taskConfigs.get(MinionConstants.TABLE_MAX_NUM_TASKS_KEY);\n+      if (tableMaxNumTasksConfig != null) {\n+        try {\n+          tableMaxNumTasks = Integer.parseInt(tableMaxNumTasksConfig);\n+        } catch (NumberFormatException e) {\n+          tableMaxNumTasks = Integer.MAX_VALUE;\n+        }\n+      } else {\n+        tableMaxNumTasks = Integer.MAX_VALUE;\n+      }\n+\n+      // Generate tasks\n+      int tableNumTasks = 0;\n+      // Generate up to tableMaxNumTasks tasks each time for each table\n+      if (tableNumTasks == tableMaxNumTasks) {\n+        break;\n+      }\n+      String batchSegmentIngestionType = IngestionConfigUtils.getBatchSegmentIngestionType(tableConfig);\n+      String batchSegmentIngestionFrequency = IngestionConfigUtils.getBatchSegmentIngestionFrequency(tableConfig);\n+      BatchIngestionConfig batchIngestionConfig = tableConfig.getIngestionConfig().getBatchIngestionConfig();\n+      List<Map<String, String>> batchConfigMaps = batchIngestionConfig.getBatchConfigMaps();\n+      for (Map<String, String> batchConfigMap : batchConfigMaps) {\n+        try {\n+          URI inputDirURI = getDirectoryUri(batchConfigMap.get(BatchConfigProperties.INPUT_DIR_URI));\n+          updateRecordReaderConfigs(batchConfigMap);\n+          List<OfflineSegmentZKMetadata> offlineSegmentsMetadata = Collections.emptyList();\n+          // For append mode, we don't create segments for input file URIs already created.\n+          if (BatchConfigProperties.SegmentIngestionType.APPEND.name().equalsIgnoreCase(batchSegmentIngestionType)) {\n+            offlineSegmentsMetadata = this._clusterInfoAccessor.getOfflineSegmentsMetadata(offlineTableName);\n+          }\n+          List<URI> inputFileURIs = getInputFilesFromDirectory(batchConfigMap, inputDirURI,\n+              getExistingSegmentInputFiles(offlineSegmentsMetadata));\n+\n+          for (URI inputFileURI : inputFileURIs) {\n+            Map<String, String> singleFileGenerationTaskConfig =\n+                getSingleFileGenerationTaskConfig(offlineTableName, tableNumTasks, batchConfigMap, inputFileURI);\n+            pinotTaskConfigs.add(new PinotTaskConfig(MinionConstants.SegmentGenerationAndPushTask.TASK_TYPE,\n+                singleFileGenerationTaskConfig));\n+            tableNumTasks++;\n+\n+            // Generate up to tableMaxNumTasks tasks each time for each table\n+            if (tableNumTasks == tableMaxNumTasks) {\n+              break;\n+            }\n+          }\n+        } catch (Exception e) {\n+          LOGGER.error(\"Unable to generate the SegmentGenerationAndPush task. [ table configs: {}, task configs: {} ]\",\n+              tableConfig, taskConfigs, e);\n+        }\n+      }\n+    }\n+    return pinotTaskConfigs;\n+  }\n+\n+  private Map<String, String> getSingleFileGenerationTaskConfig(String offlineTableName, int sequenceID,\n+      Map<String, String> batchConfigMap, URI inputFileURI)\n+      throws JsonProcessingException, URISyntaxException {\n+\n+    URI inputDirURI = getDirectoryUri(batchConfigMap.get(BatchConfigProperties.INPUT_DIR_URI));\n+    URI outputDirURI = getDirectoryUri(batchConfigMap.get(BatchConfigProperties.OUTPUT_DIR_URI));\n+    String pushMode = IngestionConfigUtils.getPushMode(batchConfigMap);\n+\n+    Map<String, String> singleFileGenerationTaskConfig = new HashMap<>(batchConfigMap);\n+    singleFileGenerationTaskConfig.put(BatchConfigProperties.INPUT_FILE_URI, inputFileURI.toString());\n+    URI outputSegmentDirURI = getRelativeOutputPath(inputDirURI, inputFileURI, outputDirURI);\n+    singleFileGenerationTaskConfig.put(BatchConfigProperties.OUTPUT_SEGMENT_DIR_URI, outputSegmentDirURI.toString());\n+    singleFileGenerationTaskConfig\n+        .put(BatchConfigProperties.SCHEMA, JsonUtils.objectToString(_clusterInfoAccessor.getTableSchema(offlineTableName)));\n+    singleFileGenerationTaskConfig\n+        .put(BatchConfigProperties.TABLE_CONFIGS, JsonUtils.objectToString(_clusterInfoAccessor.getTableConfig(offlineTableName)));\n+    singleFileGenerationTaskConfig.put(BatchConfigProperties.SEQUENCE_ID, String.valueOf(sequenceID));\n+    singleFileGenerationTaskConfig.put(BatchConfigProperties.SEGMENT_NAME_GENERATOR_TYPE, BatchConfigProperties.SegmentNameGeneratorType.SIMPLE);\n+    singleFileGenerationTaskConfig.put(BatchConfigProperties.PUSH_MODE, pushMode);\n+    singleFileGenerationTaskConfig.put(BatchConfigProperties.PUSH_CONTROLLER_URI, _clusterInfoAccessor.getVipUrl());\n+    return singleFileGenerationTaskConfig;\n+  }\n+\n+  private void updateRecordReaderConfigs(Map<String, String> batchConfigMap) {\n+    String inputFormat = batchConfigMap.get(BatchConfigProperties.INPUT_FORMAT);\n+    String recordReaderClassName = PluginManager.get().getRecordReaderClassName(inputFormat);\n+    if (recordReaderClassName != null) {\n+      batchConfigMap.putIfAbsent(BatchConfigProperties.RECORD_READER_CLASS, recordReaderClassName);\n+    }\n+    String recordReaderConfigClassName = PluginManager.get().getRecordReaderConfigClassName(inputFormat);\n+    if (recordReaderConfigClassName != null) {\n+      batchConfigMap.putIfAbsent(BatchConfigProperties.RECORD_READER_CONFIG_CLASS, recordReaderConfigClassName);\n+    }\n+  }\n+\n+  private List<URI> getInputFilesFromDirectory(Map<String, String> batchConfigMap, URI inputDirURI,\n+      Set<String> existingSegmentInputFileURIs) {\n+    String inputDirURIScheme = inputDirURI.getScheme();\n+    if (!PinotFSFactory.isSchemeSupported(inputDirURIScheme)) {\n+      String fsClass = batchConfigMap.get(BatchConfigProperties.INPUT_FS_CLASS);\n+      PinotConfiguration fsProps = IngestionConfigUtils.getFsProps(batchConfigMap);\n+      PinotFSFactory.register(inputDirURIScheme, fsClass, fsProps);\n+    }\n+    PinotFS inputDirFS = PinotFSFactory.create(inputDirURIScheme);\n+\n+    String includeFileNamePattern = batchConfigMap.get(BatchConfigProperties.INCLUDE_FILE_NAME_PATTERN);\n+    String excludeFileNamePattern = batchConfigMap.get(BatchConfigProperties.EXCLUDE_FILE_NAME_PATTERN);\n+\n+    //Get list of files to process\n+    String[] files;\n+    try {\n+      files = inputDirFS.listFiles(inputDirURI, true);\n+    } catch (IOException e) {\n+      LOGGER.error(\"Unable to list files under URI: \" + inputDirURI, e);\n+      return Collections.emptyList();\n+    }\n+    PathMatcher includeFilePathMatcher = null;\n+    if (includeFileNamePattern != null) {\n+      includeFilePathMatcher = FileSystems.getDefault().getPathMatcher(includeFileNamePattern);\n+    }\n+    PathMatcher excludeFilePathMatcher = null;\n+    if (excludeFileNamePattern != null) {\n+      excludeFilePathMatcher = FileSystems.getDefault().getPathMatcher(excludeFileNamePattern);\n+    }\n+    List<URI> inputFileURIs = new ArrayList<>();\n+    for (String file : files) {\n+      if (includeFilePathMatcher != null) {\n+        if (!includeFilePathMatcher.matches(Paths.get(file))) {\n+          continue;\n+        }\n+      }\n+      if (excludeFilePathMatcher != null) {\n+        if (excludeFilePathMatcher.matches(Paths.get(file))) {\n+          continue;\n+        }\n+      }\n+      try {\n+        URI inputFileURI = new URI(file);\n+        if (inputFileURI.getScheme() == null) {\n+          inputFileURI = new File(file).toURI();\n+        }\n+        if (inputDirFS.isDirectory(inputFileURI) || existingSegmentInputFileURIs.contains(inputFileURI.toString())) {\n+          continue;\n+        }\n+        inputFileURIs.add(inputFileURI);\n+      } catch (Exception e) {\n+        continue;\n+      }\n+    }\n+    return inputFileURIs;\n+  }\n+\n+  private Set<String> getExistingSegmentInputFiles(List<OfflineSegmentZKMetadata> offlineSegmentsMetadata) {\n+    Set<String> existingSegmentInputFiles = new HashSet<>();\n+    for (OfflineSegmentZKMetadata metadata : offlineSegmentsMetadata) {\n+      if ((metadata.getCustomMap() != null) && metadata.getCustomMap()\n+          .containsKey(BatchConfigProperties.INPUT_DATA_FILE_URI_KEY)) {\n+        existingSegmentInputFiles.add(metadata.getCustomMap().get(BatchConfigProperties.INPUT_DATA_FILE_URI_KEY));\n+      }\n+    }\n+    return existingSegmentInputFiles;\n+  }\n+\n+  private URI getDirectoryUri(String uriStr)\n+      throws URISyntaxException {\n+    URI uri = new URI(uriStr);\n+    if (uri.getScheme() == null) {\n+      uri = new File(uriStr).toURI();\n+    }\n+    return uri;\n+  }\n+\n+  public static URI getRelativeOutputPath(URI baseInputDir, URI inputFile, URI outputDir) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a254c01b685ad6fcc08c665fb92329f53098f209"}, "originalPosition": 258}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU4OTU3Mw==", "bodyText": "updated.", "url": "https://github.com/apache/pinot/pull/6340#discussion_r545589573", "createdAt": "2020-12-18T05:57:33Z", "author": {"login": "xiangfu0"}, "path": "pinot-controller/src/main/java/org/apache/pinot/controller/helix/core/minion/generator/SegmentGenerationAndPushTaskGenerator.java", "diffHunk": "@@ -0,0 +1,267 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.helix.core.minion.generator;\n+\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Preconditions;\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.nio.file.FileSystems;\n+import java.nio.file.PathMatcher;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.pinot.common.metadata.segment.OfflineSegmentZKMetadata;\n+import org.apache.pinot.controller.helix.core.minion.ClusterInfoAccessor;\n+import org.apache.pinot.core.common.MinionConstants;\n+import org.apache.pinot.core.minion.PinotTaskConfig;\n+import org.apache.pinot.spi.config.table.TableConfig;\n+import org.apache.pinot.spi.config.table.TableTaskConfig;\n+import org.apache.pinot.spi.config.table.TableType;\n+import org.apache.pinot.spi.config.table.ingestion.BatchIngestionConfig;\n+import org.apache.pinot.spi.env.PinotConfiguration;\n+import org.apache.pinot.spi.filesystem.PinotFS;\n+import org.apache.pinot.spi.filesystem.PinotFSFactory;\n+import org.apache.pinot.spi.ingestion.batch.BatchConfigProperties;\n+import org.apache.pinot.spi.plugin.PluginManager;\n+import org.apache.pinot.spi.utils.IngestionConfigUtils;\n+import org.apache.pinot.spi.utils.JsonUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class SegmentGenerationAndPushTaskGenerator implements PinotTaskGenerator {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(SegmentGenerationAndPushTaskGenerator.class);\n+\n+  private final ClusterInfoAccessor _clusterInfoAccessor;\n+\n+  public SegmentGenerationAndPushTaskGenerator(ClusterInfoAccessor clusterInfoAccessor) {\n+    _clusterInfoAccessor = clusterInfoAccessor;\n+  }\n+\n+  @Override\n+  public String getTaskType() {\n+    return MinionConstants.SegmentGenerationAndPushTask.TASK_TYPE;\n+  }\n+\n+  @Override\n+  public List<PinotTaskConfig> generateTasks(List<TableConfig> tableConfigs) {\n+    List<PinotTaskConfig> pinotTaskConfigs = new ArrayList<>();\n+\n+    for (TableConfig tableConfig : tableConfigs) {\n+      // Only generate tasks for OFFLINE tables\n+      String offlineTableName = tableConfig.getTableName();\n+      if (tableConfig.getTableType() != TableType.OFFLINE) {\n+        LOGGER.warn(\"Skip generating SegmentGenerationAndPushTask for non-OFFLINE table: {}\", offlineTableName);\n+        continue;\n+      }\n+\n+      TableTaskConfig tableTaskConfig = tableConfig.getTaskConfig();\n+      Preconditions.checkNotNull(tableTaskConfig);\n+      Map<String, String> taskConfigs =\n+          tableTaskConfig.getConfigsForTaskType(MinionConstants.SegmentGenerationAndPushTask.TASK_TYPE);\n+      if (tableConfigs == null) {\n+        LOGGER.warn(\"Skip null task config for table: {}\", offlineTableName);\n+      }\n+\n+      // Get max number of tasks for this table\n+      int tableMaxNumTasks;\n+      String tableMaxNumTasksConfig = taskConfigs.get(MinionConstants.TABLE_MAX_NUM_TASKS_KEY);\n+      if (tableMaxNumTasksConfig != null) {\n+        try {\n+          tableMaxNumTasks = Integer.parseInt(tableMaxNumTasksConfig);\n+        } catch (NumberFormatException e) {\n+          tableMaxNumTasks = Integer.MAX_VALUE;\n+        }\n+      } else {\n+        tableMaxNumTasks = Integer.MAX_VALUE;\n+      }\n+\n+      // Generate tasks\n+      int tableNumTasks = 0;\n+      // Generate up to tableMaxNumTasks tasks each time for each table\n+      if (tableNumTasks == tableMaxNumTasks) {\n+        break;\n+      }\n+      String batchSegmentIngestionType = IngestionConfigUtils.getBatchSegmentIngestionType(tableConfig);\n+      String batchSegmentIngestionFrequency = IngestionConfigUtils.getBatchSegmentIngestionFrequency(tableConfig);\n+      BatchIngestionConfig batchIngestionConfig = tableConfig.getIngestionConfig().getBatchIngestionConfig();\n+      List<Map<String, String>> batchConfigMaps = batchIngestionConfig.getBatchConfigMaps();\n+      for (Map<String, String> batchConfigMap : batchConfigMaps) {\n+        try {\n+          URI inputDirURI = getDirectoryUri(batchConfigMap.get(BatchConfigProperties.INPUT_DIR_URI));\n+          updateRecordReaderConfigs(batchConfigMap);\n+          List<OfflineSegmentZKMetadata> offlineSegmentsMetadata = Collections.emptyList();\n+          // For append mode, we don't create segments for input file URIs already created.\n+          if (BatchConfigProperties.SegmentIngestionType.APPEND.name().equalsIgnoreCase(batchSegmentIngestionType)) {\n+            offlineSegmentsMetadata = this._clusterInfoAccessor.getOfflineSegmentsMetadata(offlineTableName);\n+          }\n+          List<URI> inputFileURIs = getInputFilesFromDirectory(batchConfigMap, inputDirURI,\n+              getExistingSegmentInputFiles(offlineSegmentsMetadata));\n+\n+          for (URI inputFileURI : inputFileURIs) {\n+            Map<String, String> singleFileGenerationTaskConfig =\n+                getSingleFileGenerationTaskConfig(offlineTableName, tableNumTasks, batchConfigMap, inputFileURI);\n+            pinotTaskConfigs.add(new PinotTaskConfig(MinionConstants.SegmentGenerationAndPushTask.TASK_TYPE,\n+                singleFileGenerationTaskConfig));\n+            tableNumTasks++;\n+\n+            // Generate up to tableMaxNumTasks tasks each time for each table\n+            if (tableNumTasks == tableMaxNumTasks) {\n+              break;\n+            }\n+          }\n+        } catch (Exception e) {\n+          LOGGER.error(\"Unable to generate the SegmentGenerationAndPush task. [ table configs: {}, task configs: {} ]\",\n+              tableConfig, taskConfigs, e);\n+        }\n+      }\n+    }\n+    return pinotTaskConfigs;\n+  }\n+\n+  private Map<String, String> getSingleFileGenerationTaskConfig(String offlineTableName, int sequenceID,\n+      Map<String, String> batchConfigMap, URI inputFileURI)\n+      throws JsonProcessingException, URISyntaxException {\n+\n+    URI inputDirURI = getDirectoryUri(batchConfigMap.get(BatchConfigProperties.INPUT_DIR_URI));\n+    URI outputDirURI = getDirectoryUri(batchConfigMap.get(BatchConfigProperties.OUTPUT_DIR_URI));\n+    String pushMode = IngestionConfigUtils.getPushMode(batchConfigMap);\n+\n+    Map<String, String> singleFileGenerationTaskConfig = new HashMap<>(batchConfigMap);\n+    singleFileGenerationTaskConfig.put(BatchConfigProperties.INPUT_FILE_URI, inputFileURI.toString());\n+    URI outputSegmentDirURI = getRelativeOutputPath(inputDirURI, inputFileURI, outputDirURI);\n+    singleFileGenerationTaskConfig.put(BatchConfigProperties.OUTPUT_SEGMENT_DIR_URI, outputSegmentDirURI.toString());\n+    singleFileGenerationTaskConfig\n+        .put(BatchConfigProperties.SCHEMA, JsonUtils.objectToString(_clusterInfoAccessor.getTableSchema(offlineTableName)));\n+    singleFileGenerationTaskConfig\n+        .put(BatchConfigProperties.TABLE_CONFIGS, JsonUtils.objectToString(_clusterInfoAccessor.getTableConfig(offlineTableName)));\n+    singleFileGenerationTaskConfig.put(BatchConfigProperties.SEQUENCE_ID, String.valueOf(sequenceID));\n+    singleFileGenerationTaskConfig.put(BatchConfigProperties.SEGMENT_NAME_GENERATOR_TYPE, BatchConfigProperties.SegmentNameGeneratorType.SIMPLE);\n+    singleFileGenerationTaskConfig.put(BatchConfigProperties.PUSH_MODE, pushMode);\n+    singleFileGenerationTaskConfig.put(BatchConfigProperties.PUSH_CONTROLLER_URI, _clusterInfoAccessor.getVipUrl());\n+    return singleFileGenerationTaskConfig;\n+  }\n+\n+  private void updateRecordReaderConfigs(Map<String, String> batchConfigMap) {\n+    String inputFormat = batchConfigMap.get(BatchConfigProperties.INPUT_FORMAT);\n+    String recordReaderClassName = PluginManager.get().getRecordReaderClassName(inputFormat);\n+    if (recordReaderClassName != null) {\n+      batchConfigMap.putIfAbsent(BatchConfigProperties.RECORD_READER_CLASS, recordReaderClassName);\n+    }\n+    String recordReaderConfigClassName = PluginManager.get().getRecordReaderConfigClassName(inputFormat);\n+    if (recordReaderConfigClassName != null) {\n+      batchConfigMap.putIfAbsent(BatchConfigProperties.RECORD_READER_CONFIG_CLASS, recordReaderConfigClassName);\n+    }\n+  }\n+\n+  private List<URI> getInputFilesFromDirectory(Map<String, String> batchConfigMap, URI inputDirURI,\n+      Set<String> existingSegmentInputFileURIs) {\n+    String inputDirURIScheme = inputDirURI.getScheme();\n+    if (!PinotFSFactory.isSchemeSupported(inputDirURIScheme)) {\n+      String fsClass = batchConfigMap.get(BatchConfigProperties.INPUT_FS_CLASS);\n+      PinotConfiguration fsProps = IngestionConfigUtils.getFsProps(batchConfigMap);\n+      PinotFSFactory.register(inputDirURIScheme, fsClass, fsProps);\n+    }\n+    PinotFS inputDirFS = PinotFSFactory.create(inputDirURIScheme);\n+\n+    String includeFileNamePattern = batchConfigMap.get(BatchConfigProperties.INCLUDE_FILE_NAME_PATTERN);\n+    String excludeFileNamePattern = batchConfigMap.get(BatchConfigProperties.EXCLUDE_FILE_NAME_PATTERN);\n+\n+    //Get list of files to process\n+    String[] files;\n+    try {\n+      files = inputDirFS.listFiles(inputDirURI, true);\n+    } catch (IOException e) {\n+      LOGGER.error(\"Unable to list files under URI: \" + inputDirURI, e);\n+      return Collections.emptyList();\n+    }\n+    PathMatcher includeFilePathMatcher = null;\n+    if (includeFileNamePattern != null) {\n+      includeFilePathMatcher = FileSystems.getDefault().getPathMatcher(includeFileNamePattern);\n+    }\n+    PathMatcher excludeFilePathMatcher = null;\n+    if (excludeFileNamePattern != null) {\n+      excludeFilePathMatcher = FileSystems.getDefault().getPathMatcher(excludeFileNamePattern);\n+    }\n+    List<URI> inputFileURIs = new ArrayList<>();\n+    for (String file : files) {\n+      if (includeFilePathMatcher != null) {\n+        if (!includeFilePathMatcher.matches(Paths.get(file))) {\n+          continue;\n+        }\n+      }\n+      if (excludeFilePathMatcher != null) {\n+        if (excludeFilePathMatcher.matches(Paths.get(file))) {\n+          continue;\n+        }\n+      }\n+      try {\n+        URI inputFileURI = new URI(file);\n+        if (inputFileURI.getScheme() == null) {\n+          inputFileURI = new File(file).toURI();\n+        }\n+        if (inputDirFS.isDirectory(inputFileURI) || existingSegmentInputFileURIs.contains(inputFileURI.toString())) {\n+          continue;\n+        }\n+        inputFileURIs.add(inputFileURI);\n+      } catch (Exception e) {\n+        continue;\n+      }\n+    }\n+    return inputFileURIs;\n+  }\n+\n+  private Set<String> getExistingSegmentInputFiles(List<OfflineSegmentZKMetadata> offlineSegmentsMetadata) {\n+    Set<String> existingSegmentInputFiles = new HashSet<>();\n+    for (OfflineSegmentZKMetadata metadata : offlineSegmentsMetadata) {\n+      if ((metadata.getCustomMap() != null) && metadata.getCustomMap()\n+          .containsKey(BatchConfigProperties.INPUT_DATA_FILE_URI_KEY)) {\n+        existingSegmentInputFiles.add(metadata.getCustomMap().get(BatchConfigProperties.INPUT_DATA_FILE_URI_KEY));\n+      }\n+    }\n+    return existingSegmentInputFiles;\n+  }\n+\n+  private URI getDirectoryUri(String uriStr)\n+      throws URISyntaxException {\n+    URI uri = new URI(uriStr);\n+    if (uri.getScheme() == null) {\n+      uri = new File(uriStr).toURI();\n+    }\n+    return uri;\n+  }\n+\n+  public static URI getRelativeOutputPath(URI baseInputDir, URI inputFile, URI outputDir) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDc2Mjc2Mw=="}, "originalCommit": {"oid": "a254c01b685ad6fcc08c665fb92329f53098f209"}, "originalPosition": 258}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyMzUxODY2OnYy", "diffSide": "RIGHT", "path": "pinot-tools/src/main/resources/examples/minions/batch/airlineStats/airlineStats_offline_table_config.json", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwMjozMjoxMlrOIHh3xQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwNTo1NzoyNlrOIIUJDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDc2NTg5Mw==", "bodyText": "aren't these expected to be \"input.fs.className\" and \"input.fs.prop.region\" ? Will this work without input/output prefix?", "url": "https://github.com/apache/pinot/pull/6340#discussion_r544765893", "createdAt": "2020-12-17T02:32:12Z", "author": {"login": "npawar"}, "path": "pinot-tools/src/main/resources/examples/minions/batch/airlineStats/airlineStats_offline_table_config.json", "diffHunk": "@@ -0,0 +1,45 @@\n+{\n+  \"tableName\": \"airlineStats\",\n+  \"tableType\": \"OFFLINE\",\n+  \"segmentsConfig\": {\n+    \"timeColumnName\": \"DaysSinceEpoch\",\n+    \"timeType\": \"DAYS\",\n+    \"segmentPushType\": \"APPEND\",\n+    \"segmentAssignmentStrategy\": \"BalanceNumSegmentAssignmentStrategy\",\n+    \"replication\": \"1\"\n+  },\n+  \"tenants\": {},\n+  \"tableIndexConfig\": {\n+    \"loadMode\": \"MMAP\"\n+  },\n+  \"metadata\": {\n+    \"customConfigs\": {}\n+  },\n+  \"ingestionConfig\": {\n+    \"batchIngestionConfig\": {\n+      \"segmentIngestionType\": \"APPEND\",\n+      \"segmentIngestionFrequency\": \"DAILY\",\n+      \"batchConfigMaps\": [\n+        {\n+          \"inputDirURI\": \"s3://my.s3.bucket/batch/airlineStats/rawdata/\",\n+          \"fs.className\": \"org.apache.pinot.plugin.filesystem.S3PinotFS\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a254c01b685ad6fcc08c665fb92329f53098f209"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU4OTUxNw==", "bodyText": "it won't, updated the config.", "url": "https://github.com/apache/pinot/pull/6340#discussion_r545589517", "createdAt": "2020-12-18T05:57:26Z", "author": {"login": "xiangfu0"}, "path": "pinot-tools/src/main/resources/examples/minions/batch/airlineStats/airlineStats_offline_table_config.json", "diffHunk": "@@ -0,0 +1,45 @@\n+{\n+  \"tableName\": \"airlineStats\",\n+  \"tableType\": \"OFFLINE\",\n+  \"segmentsConfig\": {\n+    \"timeColumnName\": \"DaysSinceEpoch\",\n+    \"timeType\": \"DAYS\",\n+    \"segmentPushType\": \"APPEND\",\n+    \"segmentAssignmentStrategy\": \"BalanceNumSegmentAssignmentStrategy\",\n+    \"replication\": \"1\"\n+  },\n+  \"tenants\": {},\n+  \"tableIndexConfig\": {\n+    \"loadMode\": \"MMAP\"\n+  },\n+  \"metadata\": {\n+    \"customConfigs\": {}\n+  },\n+  \"ingestionConfig\": {\n+    \"batchIngestionConfig\": {\n+      \"segmentIngestionType\": \"APPEND\",\n+      \"segmentIngestionFrequency\": \"DAILY\",\n+      \"batchConfigMaps\": [\n+        {\n+          \"inputDirURI\": \"s3://my.s3.bucket/batch/airlineStats/rawdata/\",\n+          \"fs.className\": \"org.apache.pinot.plugin.filesystem.S3PinotFS\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDc2NTg5Mw=="}, "originalCommit": {"oid": "a254c01b685ad6fcc08c665fb92329f53098f209"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyMzU0NTk5OnYy", "diffSide": "RIGHT", "path": "pinot-minion/src/main/java/org/apache/pinot/minion/executor/SegmentGenerationAndPushTaskExecutor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwMjo0Mjo1OVrOIHiGXw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwNTo1NzoxMVrOIIUIyQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDc2OTYzMQ==", "bodyText": "Doesn't the output FS also need to be registered if not present? Or is the assumption that it will be same as input fs?", "url": "https://github.com/apache/pinot/pull/6340#discussion_r544769631", "createdAt": "2020-12-17T02:42:59Z", "author": {"login": "npawar"}, "path": "pinot-minion/src/main/java/org/apache/pinot/minion/executor/SegmentGenerationAndPushTaskExecutor.java", "diffHunk": "@@ -0,0 +1,256 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.minion.executor;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.common.utils.TarGzCompressionUtils;\n+import org.apache.pinot.core.minion.PinotTaskConfig;\n+import org.apache.pinot.plugin.ingestion.batch.common.SegmentGenerationTaskRunner;\n+import org.apache.pinot.plugin.ingestion.batch.common.SegmentGenerationUtils;\n+import org.apache.pinot.plugin.ingestion.batch.common.SegmentPushUtils;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.spi.env.PinotConfiguration;\n+import org.apache.pinot.spi.filesystem.LocalPinotFS;\n+import org.apache.pinot.spi.filesystem.PinotFS;\n+import org.apache.pinot.spi.filesystem.PinotFSFactory;\n+import org.apache.pinot.spi.ingestion.batch.BatchConfigProperties;\n+import org.apache.pinot.spi.ingestion.batch.spec.Constants;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotClusterSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PushJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.RecordReaderSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentGenerationJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentGenerationTaskSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentNameGeneratorSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.TableSpec;\n+import org.apache.pinot.spi.utils.DataSizeUtils;\n+import org.apache.pinot.spi.utils.IngestionConfigUtils;\n+import org.apache.pinot.spi.utils.JsonUtils;\n+import org.apache.pinot.spi.utils.retry.AttemptsExceededException;\n+import org.apache.pinot.spi.utils.retry.RetriableOperationException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class SegmentGenerationAndPushTaskExecutor extends BaseTaskExecutor {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(SegmentGenerationAndPushTaskExecutor.class);\n+\n+  private static final PinotFS LOCAL_PINOT_FS = new LocalPinotFS();\n+  private static final int DEFUALT_PUSH_ATTEMPTS = 5;\n+  private static final int DEFAULT_PUSH_PARALLELISM = 1;\n+  private static final long DEFAULT_PUSH_RETRY_INTERVAL_MILLIS = 1000L;\n+\n+  @Override\n+  public Object executeTask(PinotTaskConfig pinotTaskConfig) {\n+    Map<String, String> taskConfigs = pinotTaskConfig.getConfigs();\n+    SegmentGenerationAndPushResult.Builder resultBuilder = new SegmentGenerationAndPushResult.Builder();\n+    File localTempDir = new File(FileUtils.getTempDirectory(), \"pinot-\" + UUID.randomUUID());\n+\n+    try {\n+      // Generate Pinot Segment\n+      SegmentGenerationTaskSpec taskSpec = generateTaskSpec(taskConfigs, localTempDir);\n+      SegmentGenerationTaskRunner taskRunner = new SegmentGenerationTaskRunner(taskSpec);\n+      String segmentName = taskRunner.run();\n+\n+      // Tar segment directory to compress file\n+      File localSegmentTarFile = tarSegmentDir(taskSpec, segmentName);\n+\n+      //move segment to output PinotFS\n+      URI outputSegmentTarURI = moveSegmentToOutputPinotFS(taskConfigs, localSegmentTarFile);\n+\n+      resultBuilder.setSegmentName(segmentName);\n+      // Segment push task\n+      pushSegment(taskSpec.getTableConfig().get(BatchConfigProperties.TABLE_NAME).asText(), taskConfigs,\n+          outputSegmentTarURI);\n+      resultBuilder.setSucceed(true);\n+    } catch (Exception e) {\n+      resultBuilder.setException(e);\n+    } finally {\n+      // Cleanup output dir\n+      FileUtils.deleteQuietly(localTempDir);\n+    }\n+    return resultBuilder.build();\n+  }\n+\n+  private void pushSegment(String tableName, Map<String, String> taskConfigs, URI outputSegmentTarURI)\n+      throws Exception {\n+    String pushMode = taskConfigs.get(BatchConfigProperties.PUSH_MODE);\n+\n+    PushJobSpec pushJobSpec = new PushJobSpec();\n+    pushJobSpec.setPushAttempts(DEFUALT_PUSH_ATTEMPTS);\n+    pushJobSpec.setPushParallelism(DEFAULT_PUSH_PARALLELISM);\n+    pushJobSpec.setPushRetryIntervalMillis(DEFAULT_PUSH_RETRY_INTERVAL_MILLIS);\n+    pushJobSpec.setSegmentUriPrefix(taskConfigs.get(BatchConfigProperties.PUSH_SEGMENT_URI_PREFIX));\n+    pushJobSpec.setSegmentUriSuffix(taskConfigs.get(BatchConfigProperties.PUSH_SEGMENT_URI_SUFFIX));\n+\n+    SegmentGenerationJobSpec spec = generatePushJobSpec(tableName, taskConfigs, pushJobSpec);\n+\n+    URI outputSegmentDirURI = URI.create(taskConfigs.get(BatchConfigProperties.OUTPUT_SEGMENT_DIR_URI));\n+    PinotFS outputFileFS = getPinotFS(outputSegmentDirURI);\n+    switch (BatchConfigProperties.SegmentPushType.valueOf(pushMode.toUpperCase())) {\n+      case TAR:\n+        try {\n+          SegmentPushUtils.pushSegments(spec, LOCAL_PINOT_FS, Arrays.asList(outputSegmentTarURI.toString()));\n+        } catch (RetriableOperationException | AttemptsExceededException e) {\n+          throw new RuntimeException(e);\n+        }\n+        break;\n+      case URI:\n+        try {\n+          List<String> segmentUris = new ArrayList<>();\n+          URI updatedURI = SegmentPushUtils\n+              .generateSegmentTarURI(outputSegmentDirURI, outputSegmentTarURI, pushJobSpec.getSegmentUriPrefix(),\n+                  pushJobSpec.getSegmentUriSuffix());\n+          segmentUris.add(updatedURI.toString());\n+          SegmentPushUtils.sendSegmentUris(spec, segmentUris);\n+        } catch (RetriableOperationException | AttemptsExceededException e) {\n+          throw new RuntimeException(e);\n+        }\n+        break;\n+      case METADATA:\n+        try {\n+          Map<String, String> segmentUriToTarPathMap = SegmentPushUtils\n+              .getSegmentUriToTarPathMap(outputSegmentDirURI, pushJobSpec.getSegmentUriPrefix(),\n+                  pushJobSpec.getSegmentUriSuffix(), new String[]{outputSegmentTarURI.toString()});\n+          SegmentPushUtils.sendSegmentUriAndMetadata(spec, outputFileFS, segmentUriToTarPathMap);\n+        } catch (RetriableOperationException | AttemptsExceededException e) {\n+          throw new RuntimeException(e);\n+        }\n+        break;\n+      default:\n+        throw new UnsupportedOperationException(\"Unrecognized push mode - \" + pushMode);\n+    }\n+  }\n+\n+  private SegmentGenerationJobSpec generatePushJobSpec(String tableName, Map<String, String> taskConfigs,\n+      PushJobSpec pushJobSpec) {\n+\n+    TableSpec tableSpec = new TableSpec();\n+    tableSpec.setTableName(tableName);\n+\n+    PinotClusterSpec pinotClusterSpec = new PinotClusterSpec();\n+    pinotClusterSpec.setControllerURI(taskConfigs.get(BatchConfigProperties.PUSH_CONTROLLER_URI));\n+    PinotClusterSpec[] pinotClusterSpecs = new PinotClusterSpec[]{pinotClusterSpec};\n+\n+    SegmentGenerationJobSpec spec = new SegmentGenerationJobSpec();\n+    spec.setPushJobSpec(pushJobSpec);\n+    spec.setTableSpec(tableSpec);\n+    spec.setPinotClusterSpecs(pinotClusterSpecs);\n+    return spec;\n+  }\n+\n+  private URI moveSegmentToOutputPinotFS(Map<String, String> taskConfigs, File localSegmentTarFile)\n+      throws Exception {\n+    URI outputSegmentDirURI = URI.create(taskConfigs.get(BatchConfigProperties.OUTPUT_SEGMENT_DIR_URI));\n+    PinotFS outputFileFS = getPinotFS(outputSegmentDirURI);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a254c01b685ad6fcc08c665fb92329f53098f209"}, "originalPosition": 169}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU4OTQ0OQ==", "bodyText": "fixed", "url": "https://github.com/apache/pinot/pull/6340#discussion_r545589449", "createdAt": "2020-12-18T05:57:11Z", "author": {"login": "xiangfu0"}, "path": "pinot-minion/src/main/java/org/apache/pinot/minion/executor/SegmentGenerationAndPushTaskExecutor.java", "diffHunk": "@@ -0,0 +1,256 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.minion.executor;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.common.utils.TarGzCompressionUtils;\n+import org.apache.pinot.core.minion.PinotTaskConfig;\n+import org.apache.pinot.plugin.ingestion.batch.common.SegmentGenerationTaskRunner;\n+import org.apache.pinot.plugin.ingestion.batch.common.SegmentGenerationUtils;\n+import org.apache.pinot.plugin.ingestion.batch.common.SegmentPushUtils;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.spi.env.PinotConfiguration;\n+import org.apache.pinot.spi.filesystem.LocalPinotFS;\n+import org.apache.pinot.spi.filesystem.PinotFS;\n+import org.apache.pinot.spi.filesystem.PinotFSFactory;\n+import org.apache.pinot.spi.ingestion.batch.BatchConfigProperties;\n+import org.apache.pinot.spi.ingestion.batch.spec.Constants;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotClusterSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PushJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.RecordReaderSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentGenerationJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentGenerationTaskSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentNameGeneratorSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.TableSpec;\n+import org.apache.pinot.spi.utils.DataSizeUtils;\n+import org.apache.pinot.spi.utils.IngestionConfigUtils;\n+import org.apache.pinot.spi.utils.JsonUtils;\n+import org.apache.pinot.spi.utils.retry.AttemptsExceededException;\n+import org.apache.pinot.spi.utils.retry.RetriableOperationException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class SegmentGenerationAndPushTaskExecutor extends BaseTaskExecutor {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(SegmentGenerationAndPushTaskExecutor.class);\n+\n+  private static final PinotFS LOCAL_PINOT_FS = new LocalPinotFS();\n+  private static final int DEFUALT_PUSH_ATTEMPTS = 5;\n+  private static final int DEFAULT_PUSH_PARALLELISM = 1;\n+  private static final long DEFAULT_PUSH_RETRY_INTERVAL_MILLIS = 1000L;\n+\n+  @Override\n+  public Object executeTask(PinotTaskConfig pinotTaskConfig) {\n+    Map<String, String> taskConfigs = pinotTaskConfig.getConfigs();\n+    SegmentGenerationAndPushResult.Builder resultBuilder = new SegmentGenerationAndPushResult.Builder();\n+    File localTempDir = new File(FileUtils.getTempDirectory(), \"pinot-\" + UUID.randomUUID());\n+\n+    try {\n+      // Generate Pinot Segment\n+      SegmentGenerationTaskSpec taskSpec = generateTaskSpec(taskConfigs, localTempDir);\n+      SegmentGenerationTaskRunner taskRunner = new SegmentGenerationTaskRunner(taskSpec);\n+      String segmentName = taskRunner.run();\n+\n+      // Tar segment directory to compress file\n+      File localSegmentTarFile = tarSegmentDir(taskSpec, segmentName);\n+\n+      //move segment to output PinotFS\n+      URI outputSegmentTarURI = moveSegmentToOutputPinotFS(taskConfigs, localSegmentTarFile);\n+\n+      resultBuilder.setSegmentName(segmentName);\n+      // Segment push task\n+      pushSegment(taskSpec.getTableConfig().get(BatchConfigProperties.TABLE_NAME).asText(), taskConfigs,\n+          outputSegmentTarURI);\n+      resultBuilder.setSucceed(true);\n+    } catch (Exception e) {\n+      resultBuilder.setException(e);\n+    } finally {\n+      // Cleanup output dir\n+      FileUtils.deleteQuietly(localTempDir);\n+    }\n+    return resultBuilder.build();\n+  }\n+\n+  private void pushSegment(String tableName, Map<String, String> taskConfigs, URI outputSegmentTarURI)\n+      throws Exception {\n+    String pushMode = taskConfigs.get(BatchConfigProperties.PUSH_MODE);\n+\n+    PushJobSpec pushJobSpec = new PushJobSpec();\n+    pushJobSpec.setPushAttempts(DEFUALT_PUSH_ATTEMPTS);\n+    pushJobSpec.setPushParallelism(DEFAULT_PUSH_PARALLELISM);\n+    pushJobSpec.setPushRetryIntervalMillis(DEFAULT_PUSH_RETRY_INTERVAL_MILLIS);\n+    pushJobSpec.setSegmentUriPrefix(taskConfigs.get(BatchConfigProperties.PUSH_SEGMENT_URI_PREFIX));\n+    pushJobSpec.setSegmentUriSuffix(taskConfigs.get(BatchConfigProperties.PUSH_SEGMENT_URI_SUFFIX));\n+\n+    SegmentGenerationJobSpec spec = generatePushJobSpec(tableName, taskConfigs, pushJobSpec);\n+\n+    URI outputSegmentDirURI = URI.create(taskConfigs.get(BatchConfigProperties.OUTPUT_SEGMENT_DIR_URI));\n+    PinotFS outputFileFS = getPinotFS(outputSegmentDirURI);\n+    switch (BatchConfigProperties.SegmentPushType.valueOf(pushMode.toUpperCase())) {\n+      case TAR:\n+        try {\n+          SegmentPushUtils.pushSegments(spec, LOCAL_PINOT_FS, Arrays.asList(outputSegmentTarURI.toString()));\n+        } catch (RetriableOperationException | AttemptsExceededException e) {\n+          throw new RuntimeException(e);\n+        }\n+        break;\n+      case URI:\n+        try {\n+          List<String> segmentUris = new ArrayList<>();\n+          URI updatedURI = SegmentPushUtils\n+              .generateSegmentTarURI(outputSegmentDirURI, outputSegmentTarURI, pushJobSpec.getSegmentUriPrefix(),\n+                  pushJobSpec.getSegmentUriSuffix());\n+          segmentUris.add(updatedURI.toString());\n+          SegmentPushUtils.sendSegmentUris(spec, segmentUris);\n+        } catch (RetriableOperationException | AttemptsExceededException e) {\n+          throw new RuntimeException(e);\n+        }\n+        break;\n+      case METADATA:\n+        try {\n+          Map<String, String> segmentUriToTarPathMap = SegmentPushUtils\n+              .getSegmentUriToTarPathMap(outputSegmentDirURI, pushJobSpec.getSegmentUriPrefix(),\n+                  pushJobSpec.getSegmentUriSuffix(), new String[]{outputSegmentTarURI.toString()});\n+          SegmentPushUtils.sendSegmentUriAndMetadata(spec, outputFileFS, segmentUriToTarPathMap);\n+        } catch (RetriableOperationException | AttemptsExceededException e) {\n+          throw new RuntimeException(e);\n+        }\n+        break;\n+      default:\n+        throw new UnsupportedOperationException(\"Unrecognized push mode - \" + pushMode);\n+    }\n+  }\n+\n+  private SegmentGenerationJobSpec generatePushJobSpec(String tableName, Map<String, String> taskConfigs,\n+      PushJobSpec pushJobSpec) {\n+\n+    TableSpec tableSpec = new TableSpec();\n+    tableSpec.setTableName(tableName);\n+\n+    PinotClusterSpec pinotClusterSpec = new PinotClusterSpec();\n+    pinotClusterSpec.setControllerURI(taskConfigs.get(BatchConfigProperties.PUSH_CONTROLLER_URI));\n+    PinotClusterSpec[] pinotClusterSpecs = new PinotClusterSpec[]{pinotClusterSpec};\n+\n+    SegmentGenerationJobSpec spec = new SegmentGenerationJobSpec();\n+    spec.setPushJobSpec(pushJobSpec);\n+    spec.setTableSpec(tableSpec);\n+    spec.setPinotClusterSpecs(pinotClusterSpecs);\n+    return spec;\n+  }\n+\n+  private URI moveSegmentToOutputPinotFS(Map<String, String> taskConfigs, File localSegmentTarFile)\n+      throws Exception {\n+    URI outputSegmentDirURI = URI.create(taskConfigs.get(BatchConfigProperties.OUTPUT_SEGMENT_DIR_URI));\n+    PinotFS outputFileFS = getPinotFS(outputSegmentDirURI);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDc2OTYzMQ=="}, "originalCommit": {"oid": "a254c01b685ad6fcc08c665fb92329f53098f209"}, "originalPosition": 169}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3022, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}