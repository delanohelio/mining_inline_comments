{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ3MzA1ODM5", "number": 5681, "title": "TransformConfig in IngestionConfig for ingestion transformations", "bodyText": "Description\nAdd TransformConfigs list to IngestionConfig. This is based on discussion in Extensions in the Filtering design doc: https://docs.google.com/document/d/1Cahnas3nh0XErETH0KHLaecN6xCnRVYWNKO3rDn7qcI/edit#heading=h.gr3yby6x7mlv\nRelease Notes\nTransform functions can be set in the table config instead of schema. In a future release, support for transform functions in schema will go away.\nDocumentation\nTBD for gitbooks\nExample config\n\"ingestionConfig\": {\n      \"filterConfig\": {\n        \"filterFunction\": \"Groovy({foo == \\\"VALUE1\\\"}, foo)\"\n      },\n      \"transformConfigs\": [{\n        \"columnName\": \"bar\",\n        \"transformFunction\": \"lower(moo)\"\n      },\n      {\n        \"columnName\": \"hoursSinceEpoch\",\n        \"transformFunction\": \"toEpochHours(millis)\"\n      }]\n    }", "createdAt": "2020-07-10T08:34:57Z", "url": "https://github.com/apache/pinot/pull/5681", "merged": true, "mergeCommit": {"oid": "51765c07ca2b0b71887b53c3e85e0bc5c698d60e"}, "closed": true, "closedAt": "2020-07-14T00:15:12Z", "author": {"login": "npawar"}, "timelineItems": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc0mPvoAFqTQ0NzQ4Nzc0MA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABc0p4tyAH2gAyNDQ3MzA1ODM5Ojc4YzIxOWRiYWFkNzZkYmQyM2FmOThkNTc1ZGVmMjY1ZGFiYWJhZDE=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ3NDg3NzQw", "url": "https://github.com/apache/pinot/pull/5681#pullrequestreview-447487740", "createdAt": "2020-07-13T18:15:08Z", "commit": {"oid": "0fc85a2dcea04a6d5448bb92cff68b7e00c36548"}, "state": "COMMENTED", "comments": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxODoxOToyMlrOGw0Swg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxOTowMDozN1rOGw1w-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg0MTYwMg==", "bodyText": "(nit) Let's put tableConfig in front of schema", "url": "https://github.com/apache/pinot/pull/5681#discussion_r453841602", "createdAt": "2020-07-13T18:19:22Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/data/recordtransformer/ExpressionTransformer.java", "diffHunk": "@@ -36,12 +38,19 @@\n \n   private final Map<String, FunctionEvaluator> _expressionEvaluators = new HashMap<>();\n \n-  public ExpressionTransformer(Schema schema) {\n+  public ExpressionTransformer(Schema schema, TableConfig tableConfig) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0fc85a2dcea04a6d5448bb92cff68b7e00c36548"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg0NDY2OQ==", "bodyText": "This will have conflict with #5667. Let's figure out the sequence of merging these 2 PRs", "url": "https://github.com/apache/pinot/pull/5681#discussion_r453844669", "createdAt": "2020-07-13T18:24:46Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/util/TableConfigUtils.java", "diffHunk": "@@ -0,0 +1,145 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.util;\n+\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import javax.annotation.Nullable;\n+import org.apache.pinot.common.utils.CommonConstants;\n+import org.apache.pinot.core.data.function.FunctionEvaluator;\n+import org.apache.pinot.core.data.function.FunctionEvaluatorFactory;\n+import org.apache.pinot.spi.config.table.FieldConfig;\n+import org.apache.pinot.spi.config.table.IngestionConfig;\n+import org.apache.pinot.spi.config.table.SegmentsValidationAndRetentionConfig;\n+import org.apache.pinot.spi.config.table.TableConfig;\n+import org.apache.pinot.spi.config.table.TableType;\n+import org.apache.pinot.spi.config.table.ingestion.FilterConfig;\n+import org.apache.pinot.spi.config.table.ingestion.TransformConfig;\n+\n+\n+/**\n+ * Utils related to table config operations\n+ * FIXME: Merge this TableConfigUtils with the TableConfigUtils from pinot-common when merging of modules is done\n+ */\n+public final class TableConfigUtils {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0fc85a2dcea04a6d5448bb92cff68b7e00c36548"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg0NTI3Mw==", "bodyText": "Please comment on why we extract both input and output column", "url": "https://github.com/apache/pinot/pull/5681#discussion_r453845273", "createdAt": "2020-07-13T18:25:44Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/util/IngestionUtils.java", "diffHunk": "@@ -67,12 +70,24 @@ private static void extractFieldsFromSchema(Schema schema, Set<String> fields) {\n    * Extracts the fields needed by a RecordExtractor from given {@link IngestionConfig}\n    */\n   private static void extractFieldsFromIngestionConfig(@Nullable IngestionConfig ingestionConfig, Set<String> fields) {\n-    if (ingestionConfig != null && ingestionConfig.getFilterConfig() != null) {\n-      String filterFunction = ingestionConfig.getFilterConfig().getFilterFunction();\n-      if (filterFunction != null) {\n-        FunctionEvaluator functionEvaluator = FunctionEvaluatorFactory.getExpressionEvaluator(filterFunction);\n-        if (functionEvaluator != null) {\n-          fields.addAll(functionEvaluator.getArguments());\n+    if (ingestionConfig != null) {\n+      FilterConfig filterConfig = ingestionConfig.getFilterConfig();\n+      if (filterConfig != null) {\n+        String filterFunction = filterConfig.getFilterFunction();\n+        if (filterFunction != null) {\n+          FunctionEvaluator functionEvaluator = FunctionEvaluatorFactory.getExpressionEvaluator(filterFunction);\n+          if (functionEvaluator != null) {\n+            fields.addAll(functionEvaluator.getArguments());\n+          }\n+        }\n+      }\n+      List<TransformConfig> transformConfigs = ingestionConfig.getTransformConfigs();\n+      if (transformConfigs != null) {\n+        for (TransformConfig transformConfig : transformConfigs) {\n+          FunctionEvaluator expressionEvaluator =\n+              FunctionEvaluatorFactory.getExpressionEvaluator(transformConfig.getTransformFunction());\n+          fields.addAll(expressionEvaluator.getArguments());\n+          fields.add(transformConfig.getColumnName());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0fc85a2dcea04a6d5448bb92cff68b7e00c36548"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg0NjgwMw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                      if (transformColumns.contains(columnName)) {\n          \n          \n            \n                        throw new IllegalStateException(\"Duplicate transform config found for column '\" + columnName + \"'\");\n          \n          \n            \n                      }\n          \n          \n            \n                      transformColumns.add(columnName);\n          \n          \n            \n                      if (!transformColumns.add(columnName)) {\n          \n          \n            \n                        throw new IllegalStateException(\"Duplicate transform config found for column '\" + columnName + \"'\");\n          \n          \n            \n                      }", "url": "https://github.com/apache/pinot/pull/5681#discussion_r453846803", "createdAt": "2020-07-13T18:28:13Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/util/TableConfigUtils.java", "diffHunk": "@@ -0,0 +1,145 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.util;\n+\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import javax.annotation.Nullable;\n+import org.apache.pinot.common.utils.CommonConstants;\n+import org.apache.pinot.core.data.function.FunctionEvaluator;\n+import org.apache.pinot.core.data.function.FunctionEvaluatorFactory;\n+import org.apache.pinot.spi.config.table.FieldConfig;\n+import org.apache.pinot.spi.config.table.IngestionConfig;\n+import org.apache.pinot.spi.config.table.SegmentsValidationAndRetentionConfig;\n+import org.apache.pinot.spi.config.table.TableConfig;\n+import org.apache.pinot.spi.config.table.TableType;\n+import org.apache.pinot.spi.config.table.ingestion.FilterConfig;\n+import org.apache.pinot.spi.config.table.ingestion.TransformConfig;\n+\n+\n+/**\n+ * Utils related to table config operations\n+ * FIXME: Merge this TableConfigUtils with the TableConfigUtils from pinot-common when merging of modules is done\n+ */\n+public final class TableConfigUtils {\n+\n+  private TableConfigUtils() {\n+\n+  }\n+\n+  /**\n+   * Validates the table config with the following rules:\n+   * <ul>\n+   *   <li>Text index column must be raw</li>\n+   *   <li>peerSegmentDownloadScheme in ValidationConfig must be http or https</li>\n+   * </ul>\n+   */\n+  public static void validate(TableConfig tableConfig) {\n+    validateFieldConfigList(tableConfig);\n+    validateValidationConfig(tableConfig);\n+    validateIngestionConfig(tableConfig.getIngestionConfig());\n+  }\n+\n+  private static void validateFieldConfigList(TableConfig tableConfig) {\n+    List<FieldConfig> fieldConfigList = tableConfig.getFieldConfigList();\n+    if (fieldConfigList != null) {\n+      List<String> noDictionaryColumns = tableConfig.getIndexingConfig().getNoDictionaryColumns();\n+      for (FieldConfig fieldConfig : fieldConfigList) {\n+        if (fieldConfig.getIndexType() == FieldConfig.IndexType.TEXT) {\n+          // For Text index column, it must be raw (no-dictionary)\n+          // NOTE: Check both encodingType and noDictionaryColumns before migrating indexing configs into field configs\n+          String column = fieldConfig.getName();\n+          if (fieldConfig.getEncodingType() != FieldConfig.EncodingType.RAW || noDictionaryColumns == null\n+              || !noDictionaryColumns.contains(column)) {\n+            throw new IllegalStateException(\n+                \"Text index column: \" + column + \" must be raw (no-dictionary) in both FieldConfig and IndexingConfig\");\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  private static void validateValidationConfig(TableConfig tableConfig) {\n+    SegmentsValidationAndRetentionConfig validationConfig = tableConfig.getValidationConfig();\n+    if (validationConfig != null) {\n+      if (tableConfig.getTableType() == TableType.REALTIME && validationConfig.getTimeColumnName() == null) {\n+        throw new IllegalStateException(\"Must provide time column in real-time table config\");\n+      }\n+      String peerSegmentDownloadScheme = validationConfig.getPeerSegmentDownloadScheme();\n+      if (peerSegmentDownloadScheme != null) {\n+        if (!CommonConstants.HTTP_PROTOCOL.equalsIgnoreCase(peerSegmentDownloadScheme) && !CommonConstants.HTTPS_PROTOCOL.equalsIgnoreCase(peerSegmentDownloadScheme)) {\n+          throw new IllegalStateException(\"Invalid value '\" + peerSegmentDownloadScheme + \"' for peerSegmentDownloadScheme. Must be one of http nor https\" );\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Validates the following:\n+   * 1. validity of filter function\n+   * 2. checks for duplicate transform configs\n+   * 3. checks for null column name or transform function in transform config\n+   * 4. validity of transform function string\n+   * 5. checks for source fields used in destination columns\n+   */\n+  private static void validateIngestionConfig(@Nullable IngestionConfig ingestionConfig) {\n+    if (ingestionConfig != null) {\n+      FilterConfig filterConfig = ingestionConfig.getFilterConfig();\n+      if (filterConfig != null) {\n+        String filterFunction = filterConfig.getFilterFunction();\n+        if (filterFunction != null) {\n+          try {\n+            FunctionEvaluatorFactory.getExpressionEvaluator(filterFunction);\n+          } catch (Exception e) {\n+            throw new IllegalStateException(\"Invalid filter function \" + filterFunction, e);\n+          }\n+        }\n+      }\n+      List<TransformConfig> transformConfigs = ingestionConfig.getTransformConfigs();\n+      if (transformConfigs != null) {\n+        Set<String> transformColumns = new HashSet<>();\n+        for (TransformConfig transformConfig : transformConfigs) {\n+          String columnName = transformConfig.getColumnName();\n+          if (transformColumns.contains(columnName)) {\n+            throw new IllegalStateException(\"Duplicate transform config found for column '\" + columnName + \"'\");\n+          }\n+          transformColumns.add(columnName);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0fc85a2dcea04a6d5448bb92cff68b7e00c36548"}, "originalPosition": 123}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg0NzU5Mw==", "bodyText": "Perform null check before the set check", "url": "https://github.com/apache/pinot/pull/5681#discussion_r453847593", "createdAt": "2020-07-13T18:29:22Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/util/TableConfigUtils.java", "diffHunk": "@@ -0,0 +1,145 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.util;\n+\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import javax.annotation.Nullable;\n+import org.apache.pinot.common.utils.CommonConstants;\n+import org.apache.pinot.core.data.function.FunctionEvaluator;\n+import org.apache.pinot.core.data.function.FunctionEvaluatorFactory;\n+import org.apache.pinot.spi.config.table.FieldConfig;\n+import org.apache.pinot.spi.config.table.IngestionConfig;\n+import org.apache.pinot.spi.config.table.SegmentsValidationAndRetentionConfig;\n+import org.apache.pinot.spi.config.table.TableConfig;\n+import org.apache.pinot.spi.config.table.TableType;\n+import org.apache.pinot.spi.config.table.ingestion.FilterConfig;\n+import org.apache.pinot.spi.config.table.ingestion.TransformConfig;\n+\n+\n+/**\n+ * Utils related to table config operations\n+ * FIXME: Merge this TableConfigUtils with the TableConfigUtils from pinot-common when merging of modules is done\n+ */\n+public final class TableConfigUtils {\n+\n+  private TableConfigUtils() {\n+\n+  }\n+\n+  /**\n+   * Validates the table config with the following rules:\n+   * <ul>\n+   *   <li>Text index column must be raw</li>\n+   *   <li>peerSegmentDownloadScheme in ValidationConfig must be http or https</li>\n+   * </ul>\n+   */\n+  public static void validate(TableConfig tableConfig) {\n+    validateFieldConfigList(tableConfig);\n+    validateValidationConfig(tableConfig);\n+    validateIngestionConfig(tableConfig.getIngestionConfig());\n+  }\n+\n+  private static void validateFieldConfigList(TableConfig tableConfig) {\n+    List<FieldConfig> fieldConfigList = tableConfig.getFieldConfigList();\n+    if (fieldConfigList != null) {\n+      List<String> noDictionaryColumns = tableConfig.getIndexingConfig().getNoDictionaryColumns();\n+      for (FieldConfig fieldConfig : fieldConfigList) {\n+        if (fieldConfig.getIndexType() == FieldConfig.IndexType.TEXT) {\n+          // For Text index column, it must be raw (no-dictionary)\n+          // NOTE: Check both encodingType and noDictionaryColumns before migrating indexing configs into field configs\n+          String column = fieldConfig.getName();\n+          if (fieldConfig.getEncodingType() != FieldConfig.EncodingType.RAW || noDictionaryColumns == null\n+              || !noDictionaryColumns.contains(column)) {\n+            throw new IllegalStateException(\n+                \"Text index column: \" + column + \" must be raw (no-dictionary) in both FieldConfig and IndexingConfig\");\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  private static void validateValidationConfig(TableConfig tableConfig) {\n+    SegmentsValidationAndRetentionConfig validationConfig = tableConfig.getValidationConfig();\n+    if (validationConfig != null) {\n+      if (tableConfig.getTableType() == TableType.REALTIME && validationConfig.getTimeColumnName() == null) {\n+        throw new IllegalStateException(\"Must provide time column in real-time table config\");\n+      }\n+      String peerSegmentDownloadScheme = validationConfig.getPeerSegmentDownloadScheme();\n+      if (peerSegmentDownloadScheme != null) {\n+        if (!CommonConstants.HTTP_PROTOCOL.equalsIgnoreCase(peerSegmentDownloadScheme) && !CommonConstants.HTTPS_PROTOCOL.equalsIgnoreCase(peerSegmentDownloadScheme)) {\n+          throw new IllegalStateException(\"Invalid value '\" + peerSegmentDownloadScheme + \"' for peerSegmentDownloadScheme. Must be one of http nor https\" );\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Validates the following:\n+   * 1. validity of filter function\n+   * 2. checks for duplicate transform configs\n+   * 3. checks for null column name or transform function in transform config\n+   * 4. validity of transform function string\n+   * 5. checks for source fields used in destination columns\n+   */\n+  private static void validateIngestionConfig(@Nullable IngestionConfig ingestionConfig) {\n+    if (ingestionConfig != null) {\n+      FilterConfig filterConfig = ingestionConfig.getFilterConfig();\n+      if (filterConfig != null) {\n+        String filterFunction = filterConfig.getFilterFunction();\n+        if (filterFunction != null) {\n+          try {\n+            FunctionEvaluatorFactory.getExpressionEvaluator(filterFunction);\n+          } catch (Exception e) {\n+            throw new IllegalStateException(\"Invalid filter function \" + filterFunction, e);\n+          }\n+        }\n+      }\n+      List<TransformConfig> transformConfigs = ingestionConfig.getTransformConfigs();\n+      if (transformConfigs != null) {\n+        Set<String> transformColumns = new HashSet<>();\n+        for (TransformConfig transformConfig : transformConfigs) {\n+          String columnName = transformConfig.getColumnName();\n+          if (transformColumns.contains(columnName)) {\n+            throw new IllegalStateException(\"Duplicate transform config found for column '\" + columnName + \"'\");\n+          }\n+          transformColumns.add(columnName);\n+          String transformFunction = transformConfig.getTransformFunction();\n+          if (columnName == null || transformFunction == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0fc85a2dcea04a6d5448bb92cff68b7e00c36548"}, "originalPosition": 125}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg0ODkzMQ==", "bodyText": "Should we check that arguments are not contained in the transformColumns? We do not support chained transforms currently", "url": "https://github.com/apache/pinot/pull/5681#discussion_r453848931", "createdAt": "2020-07-13T18:31:32Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/util/TableConfigUtils.java", "diffHunk": "@@ -0,0 +1,145 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.util;\n+\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import javax.annotation.Nullable;\n+import org.apache.pinot.common.utils.CommonConstants;\n+import org.apache.pinot.core.data.function.FunctionEvaluator;\n+import org.apache.pinot.core.data.function.FunctionEvaluatorFactory;\n+import org.apache.pinot.spi.config.table.FieldConfig;\n+import org.apache.pinot.spi.config.table.IngestionConfig;\n+import org.apache.pinot.spi.config.table.SegmentsValidationAndRetentionConfig;\n+import org.apache.pinot.spi.config.table.TableConfig;\n+import org.apache.pinot.spi.config.table.TableType;\n+import org.apache.pinot.spi.config.table.ingestion.FilterConfig;\n+import org.apache.pinot.spi.config.table.ingestion.TransformConfig;\n+\n+\n+/**\n+ * Utils related to table config operations\n+ * FIXME: Merge this TableConfigUtils with the TableConfigUtils from pinot-common when merging of modules is done\n+ */\n+public final class TableConfigUtils {\n+\n+  private TableConfigUtils() {\n+\n+  }\n+\n+  /**\n+   * Validates the table config with the following rules:\n+   * <ul>\n+   *   <li>Text index column must be raw</li>\n+   *   <li>peerSegmentDownloadScheme in ValidationConfig must be http or https</li>\n+   * </ul>\n+   */\n+  public static void validate(TableConfig tableConfig) {\n+    validateFieldConfigList(tableConfig);\n+    validateValidationConfig(tableConfig);\n+    validateIngestionConfig(tableConfig.getIngestionConfig());\n+  }\n+\n+  private static void validateFieldConfigList(TableConfig tableConfig) {\n+    List<FieldConfig> fieldConfigList = tableConfig.getFieldConfigList();\n+    if (fieldConfigList != null) {\n+      List<String> noDictionaryColumns = tableConfig.getIndexingConfig().getNoDictionaryColumns();\n+      for (FieldConfig fieldConfig : fieldConfigList) {\n+        if (fieldConfig.getIndexType() == FieldConfig.IndexType.TEXT) {\n+          // For Text index column, it must be raw (no-dictionary)\n+          // NOTE: Check both encodingType and noDictionaryColumns before migrating indexing configs into field configs\n+          String column = fieldConfig.getName();\n+          if (fieldConfig.getEncodingType() != FieldConfig.EncodingType.RAW || noDictionaryColumns == null\n+              || !noDictionaryColumns.contains(column)) {\n+            throw new IllegalStateException(\n+                \"Text index column: \" + column + \" must be raw (no-dictionary) in both FieldConfig and IndexingConfig\");\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  private static void validateValidationConfig(TableConfig tableConfig) {\n+    SegmentsValidationAndRetentionConfig validationConfig = tableConfig.getValidationConfig();\n+    if (validationConfig != null) {\n+      if (tableConfig.getTableType() == TableType.REALTIME && validationConfig.getTimeColumnName() == null) {\n+        throw new IllegalStateException(\"Must provide time column in real-time table config\");\n+      }\n+      String peerSegmentDownloadScheme = validationConfig.getPeerSegmentDownloadScheme();\n+      if (peerSegmentDownloadScheme != null) {\n+        if (!CommonConstants.HTTP_PROTOCOL.equalsIgnoreCase(peerSegmentDownloadScheme) && !CommonConstants.HTTPS_PROTOCOL.equalsIgnoreCase(peerSegmentDownloadScheme)) {\n+          throw new IllegalStateException(\"Invalid value '\" + peerSegmentDownloadScheme + \"' for peerSegmentDownloadScheme. Must be one of http nor https\" );\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Validates the following:\n+   * 1. validity of filter function\n+   * 2. checks for duplicate transform configs\n+   * 3. checks for null column name or transform function in transform config\n+   * 4. validity of transform function string\n+   * 5. checks for source fields used in destination columns\n+   */\n+  private static void validateIngestionConfig(@Nullable IngestionConfig ingestionConfig) {\n+    if (ingestionConfig != null) {\n+      FilterConfig filterConfig = ingestionConfig.getFilterConfig();\n+      if (filterConfig != null) {\n+        String filterFunction = filterConfig.getFilterFunction();\n+        if (filterFunction != null) {\n+          try {\n+            FunctionEvaluatorFactory.getExpressionEvaluator(filterFunction);\n+          } catch (Exception e) {\n+            throw new IllegalStateException(\"Invalid filter function \" + filterFunction, e);\n+          }\n+        }\n+      }\n+      List<TransformConfig> transformConfigs = ingestionConfig.getTransformConfigs();\n+      if (transformConfigs != null) {\n+        Set<String> transformColumns = new HashSet<>();\n+        for (TransformConfig transformConfig : transformConfigs) {\n+          String columnName = transformConfig.getColumnName();\n+          if (transformColumns.contains(columnName)) {\n+            throw new IllegalStateException(\"Duplicate transform config found for column '\" + columnName + \"'\");\n+          }\n+          transformColumns.add(columnName);\n+          String transformFunction = transformConfig.getTransformFunction();\n+          if (columnName == null || transformFunction == null) {\n+            throw new IllegalStateException(\"columnName/transformFunction cannot be null in TransformConfig \" + transformConfig);\n+          }\n+          FunctionEvaluator expressionEvaluator;\n+          try {\n+            expressionEvaluator = FunctionEvaluatorFactory.getExpressionEvaluator(transformFunction);\n+          } catch (Exception e) {\n+            throw new IllegalStateException(\n+                \"Invalid transform function '\" + transformFunction + \"' for column '\" + columnName + \"'\");\n+          }\n+          List<String> arguments = expressionEvaluator.getArguments();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0fc85a2dcea04a6d5448bb92cff68b7e00c36548"}, "originalPosition": 135}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg1MjQxOA==", "bodyText": "Remove this class", "url": "https://github.com/apache/pinot/pull/5681#discussion_r453852418", "createdAt": "2020-07-13T18:37:24Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/Temp.java", "diffHunk": "@@ -0,0 +1,224 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.integration.tests;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.node.ArrayNode;\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.controller.ControllerConf;\n+import org.apache.pinot.spi.config.table.IngestionConfig;\n+import org.apache.pinot.spi.config.table.TableConfig;\n+import org.apache.pinot.spi.config.table.ingestion.FilterConfig;\n+import org.apache.pinot.spi.config.table.ingestion.TransformConfig;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.util.TestUtils;\n+import org.testng.Assert;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Test;\n+\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertTrue;\n+\n+\n+/**\n+ * Hybrid cluster integration test that uses one of the DateTimeFieldSpec as primary time column\n+ */\n+public class Temp extends BaseClusterIntegrationTest {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0fc85a2dcea04a6d5448bb92cff68b7e00c36548"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg1MzEzNg==", "bodyText": "Why do we need this new schema?", "url": "https://github.com/apache/pinot/pull/5681#discussion_r453853136", "createdAt": "2020-07-13T18:38:40Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-integration-tests/src/test/resources/On_Time_On_Time_Performance_2014_100k_subset_nonulls_ingestion_config.schema", "diffHunk": "@@ -0,0 +1,342 @@\n+{", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0fc85a2dcea04a6d5448bb92cff68b7e00c36548"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg1NDU2OQ==", "bodyText": "Should we keep a test for transform in schema to ensure this change is backward-compatible? We can remove the test when we remove the schema transform support.", "url": "https://github.com/apache/pinot/pull/5681#discussion_r453854569", "createdAt": "2020-07-13T18:41:09Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/test/java/org/apache/pinot/core/data/recordtransformer/ExpressionTransformerTest.java", "diffHunk": "@@ -43,14 +43,28 @@\n public class ExpressionTransformerTest {\n \n   @Test\n-  public void testGroovyExpressionTransformer()\n-      throws IOException {\n-    URL resource = AbstractRecordExtractorTest.class.getClassLoader()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0fc85a2dcea04a6d5448bb92cff68b7e00c36548"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg1NjMxMQ==", "bodyText": "(nit) remove empty line", "url": "https://github.com/apache/pinot/pull/5681#discussion_r453856311", "createdAt": "2020-07-13T18:44:20Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/IngestionConfigIntegrationTest.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.integration.tests;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.google.common.collect.Lists;\n+import java.io.File;\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.spi.config.table.IngestionConfig;\n+import org.apache.pinot.spi.config.table.TableConfig;\n+import org.apache.pinot.spi.config.table.ingestion.FilterConfig;\n+import org.apache.pinot.spi.config.table.ingestion.TransformConfig;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.util.TestUtils;\n+import org.testng.Assert;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Test;\n+\n+import static org.testng.Assert.assertEquals;\n+\n+\n+/**\n+ * Integration test that converts Avro data for 12 segments and runs queries against it.\n+ */\n+public class IngestionConfigIntegrationTest extends BaseClusterIntegrationTestSet {\n+\n+  private static final String TIME_COLUMN_NAME = \"millisSinceEpoch\";\n+  private static final String SCHEMA_FILE_NAME = \"On_Time_On_Time_Performance_2014_100k_subset_nonulls_ingestion_config.schema\";\n+\n+  @Override\n+  protected String getSchemaFileName() {\n+    return SCHEMA_FILE_NAME;\n+  }\n+\n+  @Override\n+  protected String getTimeColumnName() {\n+    return TIME_COLUMN_NAME;\n+  }\n+\n+  @Override\n+  protected long getCountStarResult() {\n+    return 22300;\n+  }\n+\n+  @Override\n+  protected boolean useLlc() {\n+    return true;\n+  }\n+\n+  @Override\n+  protected IngestionConfig getIngestionConfig() {\n+    FilterConfig filterConfig = new FilterConfig(\"Groovy({AirlineID == 19393 || ArrDelayMinutes <= 5 }, AirlineID, ArrDelayMinutes)\");\n+    List<TransformConfig> transformConfigs = new ArrayList<>();\n+    transformConfigs.add(new TransformConfig(\"AmPm\", \"Groovy({DepTime < 1200 ? \\\"AM\\\": \\\"PM\\\"}, DepTime)\"));\n+    transformConfigs.add(new TransformConfig(\"millisSinceEpoch\", \"fromEpochDays(DaysSinceEpoch)\"));\n+    transformConfigs.add(new TransformConfig(\"lowerCaseDestCityName\", \"lower(DestCityName)\"));\n+    return new IngestionConfig(filterConfig, transformConfigs);\n+  }\n+\n+  @BeforeClass\n+  public void setUp()\n+      throws Exception {\n+    TestUtils.ensureDirectoriesExistAndEmpty(_tempDir, _segmentDir, _tarDir);\n+\n+    // Start the Pinot cluster\n+    startZk();\n+    startController();\n+    startBroker();\n+    startServer();\n+    startKafka();\n+\n+    // Create and upload the schema and table config\n+    Schema schema = createSchema();\n+    addSchema(schema);\n+    TableConfig tableConfig = createOfflineTableConfig();\n+    addTableConfig(tableConfig);\n+\n+    // Unpack the Avro files\n+    List<File> avroFiles = unpackAvroData(_tempDir);\n+\n+    // Create and upload segments\n+    ClusterIntegrationTestUtils.buildSegmentsFromAvro(avroFiles.subList(0, avroFiles.size() -1), tableConfig, schema, 0, _segmentDir, _tarDir);\n+    uploadSegments(getTableName(), _tarDir);\n+\n+    List<File> realtimeAvroFile = Lists.newArrayList(avroFiles.get(avroFiles.size() - 1));\n+    addTableConfig(createRealtimeTableConfig(realtimeAvroFile.get(0)));\n+    pushAvroIntoKafka(realtimeAvroFile);\n+\n+    // Wait for all documents loaded\n+    waitForAllDocsLoaded(600_000L);\n+  }\n+\n+  @Test\n+  public void testQueries()\n+      throws Exception {\n+    // Select column created with transform function\n+    String sqlQuery = \"Select millisSinceEpoch from \" + DEFAULT_TABLE_NAME;\n+    JsonNode response = postSqlQuery(sqlQuery);\n+    assertEquals(response.get(\"resultTable\").get(\"dataSchema\").get(\"columnNames\").get(0).asText(), \"millisSinceEpoch\");\n+    assertEquals(response.get(\"resultTable\").get(\"dataSchema\").get(\"columnDataTypes\").get(0).asText(), \"LONG\");\n+\n+    // Select column created with transform function\n+    sqlQuery = \"Select AmPm, DepTime from \" + DEFAULT_TABLE_NAME;\n+    response = postSqlQuery(sqlQuery);\n+    assertEquals(response.get(\"resultTable\").get(\"dataSchema\").get(\"columnNames\").get(0).asText(), \"AmPm\");\n+    assertEquals(response.get(\"resultTable\").get(\"dataSchema\").get(\"columnNames\").get(1).asText(), \"DepTime\");\n+    assertEquals(response.get(\"resultTable\").get(\"dataSchema\").get(\"columnDataTypes\").get(0).asText(), \"STRING\");\n+    assertEquals(response.get(\"resultTable\").get(\"dataSchema\").get(\"columnDataTypes\").get(1).asText(), \"INT\");\n+    for (int i = 0; i < response.get(\"resultTable\").get(\"rows\").size(); i++) {\n+      String amPm = response.get(\"resultTable\").get(\"rows\").get(i).get(0).asText();\n+      int depTime = response.get(\"resultTable\").get(\"rows\").get(i).get(1).asInt();\n+      Assert.assertEquals(amPm, (depTime < 1200) ? \"AM\" : \"PM\");\n+    }\n+\n+    // Select column created with transform function - offline table\n+    sqlQuery = \"Select AmPm, DepTime from \" + DEFAULT_TABLE_NAME + \"_OFFLINE\";\n+    response = postSqlQuery(sqlQuery);\n+    assertEquals(response.get(\"resultTable\").get(\"dataSchema\").get(\"columnNames\").get(0).asText(), \"AmPm\");\n+    assertEquals(response.get(\"resultTable\").get(\"dataSchema\").get(\"columnNames\").get(1).asText(), \"DepTime\");\n+    assertEquals(response.get(\"resultTable\").get(\"dataSchema\").get(\"columnDataTypes\").get(0).asText(), \"STRING\");\n+    assertEquals(response.get(\"resultTable\").get(\"dataSchema\").get(\"columnDataTypes\").get(1).asText(), \"INT\");\n+    for (int i = 0; i < response.get(\"resultTable\").get(\"rows\").size(); i++) {\n+      String amPm = response.get(\"resultTable\").get(\"rows\").get(i).get(0).asText();\n+      int depTime = response.get(\"resultTable\").get(\"rows\").get(i).get(1).asInt();\n+      Assert.assertEquals(amPm, (depTime < 1200) ? \"AM\" : \"PM\");\n+    }\n+\n+    // Select column created with transform - realtime table\n+    sqlQuery = \"Select AmPm, DepTime from \" + DEFAULT_TABLE_NAME + \"_REALTIME\";\n+    response = postSqlQuery(sqlQuery);\n+    assertEquals(response.get(\"resultTable\").get(\"dataSchema\").get(\"columnNames\").get(0).asText(), \"AmPm\");\n+    assertEquals(response.get(\"resultTable\").get(\"dataSchema\").get(\"columnNames\").get(1).asText(), \"DepTime\");\n+    assertEquals(response.get(\"resultTable\").get(\"dataSchema\").get(\"columnDataTypes\").get(0).asText(), \"STRING\");\n+    assertEquals(response.get(\"resultTable\").get(\"dataSchema\").get(\"columnDataTypes\").get(1).asText(), \"INT\");\n+    for (int i = 0; i < response.get(\"resultTable\").get(\"rows\").size(); i++) {\n+      String amPm = response.get(\"resultTable\").get(\"rows\").get(i).get(0).asText();\n+      int depTime = response.get(\"resultTable\").get(\"rows\").get(i).get(1).asInt();\n+      Assert.assertEquals(amPm, (depTime < 1200) ? \"AM\" : \"PM\");\n+    }\n+\n+    // Check there's no values that should've been filtered\n+    sqlQuery = \"Select * from \" + DEFAULT_TABLE_NAME\n+        + \"  where AirlineID = 19393 or ArrDelayMinutes <= 5\";\n+    response = postSqlQuery(sqlQuery);\n+    Assert.assertEquals(response.get(\"resultTable\").get(\"rows\").size(), 0);\n+\n+    // Check there's no values that should've been filtered - realtime table\n+    sqlQuery = \"Select * from \" + DEFAULT_TABLE_NAME + \"_REALTIME\"\n+        + \"  where AirlineID = 19393 or ArrDelayMinutes <= 5\";\n+    response = postSqlQuery(sqlQuery);\n+    Assert.assertEquals(response.get(\"resultTable\").get(\"rows\").size(), 0);\n+\n+    // Check there's no values that should've been filtered - offline table\n+    sqlQuery = \"Select * from \" + DEFAULT_TABLE_NAME + \"_OFFLINE\"\n+        + \"  where AirlineID = 19393 or ArrDelayMinutes <= 5\";\n+    response = postSqlQuery(sqlQuery);\n+    Assert.assertEquals(response.get(\"resultTable\").get(\"rows\").size(), 0);\n+  }\n+\n+  @AfterClass\n+  public void tearDown()\n+      throws Exception {\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0fc85a2dcea04a6d5448bb92cff68b7e00c36548"}, "originalPosition": 182}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg1OTQ2MA==", "bodyText": "extend BaseClusterIntegrationTest instead of BaseClusterIntegrationTestSet", "url": "https://github.com/apache/pinot/pull/5681#discussion_r453859460", "createdAt": "2020-07-13T18:49:37Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/IngestionConfigIntegrationTest.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.integration.tests;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.google.common.collect.Lists;\n+import java.io.File;\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.spi.config.table.IngestionConfig;\n+import org.apache.pinot.spi.config.table.TableConfig;\n+import org.apache.pinot.spi.config.table.ingestion.FilterConfig;\n+import org.apache.pinot.spi.config.table.ingestion.TransformConfig;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.util.TestUtils;\n+import org.testng.Assert;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Test;\n+\n+import static org.testng.Assert.assertEquals;\n+\n+\n+/**\n+ * Integration test that converts Avro data for 12 segments and runs queries against it.\n+ */\n+public class IngestionConfigIntegrationTest extends BaseClusterIntegrationTestSet {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0fc85a2dcea04a6d5448bb92cff68b7e00c36548"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg2MDMwOQ==", "bodyText": "Make a simplified schema (only contains the columns needed for the test).\nYou can directly override createSchema()", "url": "https://github.com/apache/pinot/pull/5681#discussion_r453860309", "createdAt": "2020-07-13T18:51:08Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/IngestionConfigIntegrationTest.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.integration.tests;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.google.common.collect.Lists;\n+import java.io.File;\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.spi.config.table.IngestionConfig;\n+import org.apache.pinot.spi.config.table.TableConfig;\n+import org.apache.pinot.spi.config.table.ingestion.FilterConfig;\n+import org.apache.pinot.spi.config.table.ingestion.TransformConfig;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.util.TestUtils;\n+import org.testng.Assert;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Test;\n+\n+import static org.testng.Assert.assertEquals;\n+\n+\n+/**\n+ * Integration test that converts Avro data for 12 segments and runs queries against it.\n+ */\n+public class IngestionConfigIntegrationTest extends BaseClusterIntegrationTestSet {\n+\n+  private static final String TIME_COLUMN_NAME = \"millisSinceEpoch\";\n+  private static final String SCHEMA_FILE_NAME = \"On_Time_On_Time_Performance_2014_100k_subset_nonulls_ingestion_config.schema\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0fc85a2dcea04a6d5448bb92cff68b7e00c36548"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg2MTI2OQ==", "bodyText": "No overlapping segments? The result won't be correct. Please use the set up  logic as in the HybridClusterIntegrationTest", "url": "https://github.com/apache/pinot/pull/5681#discussion_r453861269", "createdAt": "2020-07-13T18:52:48Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/IngestionConfigIntegrationTest.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.integration.tests;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.google.common.collect.Lists;\n+import java.io.File;\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.spi.config.table.IngestionConfig;\n+import org.apache.pinot.spi.config.table.TableConfig;\n+import org.apache.pinot.spi.config.table.ingestion.FilterConfig;\n+import org.apache.pinot.spi.config.table.ingestion.TransformConfig;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.util.TestUtils;\n+import org.testng.Assert;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Test;\n+\n+import static org.testng.Assert.assertEquals;\n+\n+\n+/**\n+ * Integration test that converts Avro data for 12 segments and runs queries against it.\n+ */\n+public class IngestionConfigIntegrationTest extends BaseClusterIntegrationTestSet {\n+\n+  private static final String TIME_COLUMN_NAME = \"millisSinceEpoch\";\n+  private static final String SCHEMA_FILE_NAME = \"On_Time_On_Time_Performance_2014_100k_subset_nonulls_ingestion_config.schema\";\n+\n+  @Override\n+  protected String getSchemaFileName() {\n+    return SCHEMA_FILE_NAME;\n+  }\n+\n+  @Override\n+  protected String getTimeColumnName() {\n+    return TIME_COLUMN_NAME;\n+  }\n+\n+  @Override\n+  protected long getCountStarResult() {\n+    return 22300;\n+  }\n+\n+  @Override\n+  protected boolean useLlc() {\n+    return true;\n+  }\n+\n+  @Override\n+  protected IngestionConfig getIngestionConfig() {\n+    FilterConfig filterConfig = new FilterConfig(\"Groovy({AirlineID == 19393 || ArrDelayMinutes <= 5 }, AirlineID, ArrDelayMinutes)\");\n+    List<TransformConfig> transformConfigs = new ArrayList<>();\n+    transformConfigs.add(new TransformConfig(\"AmPm\", \"Groovy({DepTime < 1200 ? \\\"AM\\\": \\\"PM\\\"}, DepTime)\"));\n+    transformConfigs.add(new TransformConfig(\"millisSinceEpoch\", \"fromEpochDays(DaysSinceEpoch)\"));\n+    transformConfigs.add(new TransformConfig(\"lowerCaseDestCityName\", \"lower(DestCityName)\"));\n+    return new IngestionConfig(filterConfig, transformConfigs);\n+  }\n+\n+  @BeforeClass\n+  public void setUp()\n+      throws Exception {\n+    TestUtils.ensureDirectoriesExistAndEmpty(_tempDir, _segmentDir, _tarDir);\n+\n+    // Start the Pinot cluster\n+    startZk();\n+    startController();\n+    startBroker();\n+    startServer();\n+    startKafka();\n+\n+    // Create and upload the schema and table config\n+    Schema schema = createSchema();\n+    addSchema(schema);\n+    TableConfig tableConfig = createOfflineTableConfig();\n+    addTableConfig(tableConfig);\n+\n+    // Unpack the Avro files\n+    List<File> avroFiles = unpackAvroData(_tempDir);\n+\n+    // Create and upload segments\n+    ClusterIntegrationTestUtils.buildSegmentsFromAvro(avroFiles.subList(0, avroFiles.size() -1), tableConfig, schema, 0, _segmentDir, _tarDir);\n+    uploadSegments(getTableName(), _tarDir);\n+\n+    List<File> realtimeAvroFile = Lists.newArrayList(avroFiles.get(avroFiles.size() - 1));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0fc85a2dcea04a6d5448bb92cff68b7e00c36548"}, "originalPosition": 104}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg2NTcyMw==", "bodyText": "This result should be the same as select count(*) from mytable where AirlineID != 19393 AND ArrDelayMinutes > 5 within other integration test, where I got 24047.\nPlease document how this number is calculated. When we add a test, we should not run the test and directly put the result as the expected value because that won't catch the bug of the code or the test logic", "url": "https://github.com/apache/pinot/pull/5681#discussion_r453865723", "createdAt": "2020-07-13T19:00:37Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/IngestionConfigIntegrationTest.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.integration.tests;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.google.common.collect.Lists;\n+import java.io.File;\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.spi.config.table.IngestionConfig;\n+import org.apache.pinot.spi.config.table.TableConfig;\n+import org.apache.pinot.spi.config.table.ingestion.FilterConfig;\n+import org.apache.pinot.spi.config.table.ingestion.TransformConfig;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.util.TestUtils;\n+import org.testng.Assert;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Test;\n+\n+import static org.testng.Assert.assertEquals;\n+\n+\n+/**\n+ * Integration test that converts Avro data for 12 segments and runs queries against it.\n+ */\n+public class IngestionConfigIntegrationTest extends BaseClusterIntegrationTestSet {\n+\n+  private static final String TIME_COLUMN_NAME = \"millisSinceEpoch\";\n+  private static final String SCHEMA_FILE_NAME = \"On_Time_On_Time_Performance_2014_100k_subset_nonulls_ingestion_config.schema\";\n+\n+  @Override\n+  protected String getSchemaFileName() {\n+    return SCHEMA_FILE_NAME;\n+  }\n+\n+  @Override\n+  protected String getTimeColumnName() {\n+    return TIME_COLUMN_NAME;\n+  }\n+\n+  @Override\n+  protected long getCountStarResult() {\n+    return 22300;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0fc85a2dcea04a6d5448bb92cff68b7e00c36548"}, "originalPosition": 61}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9b8fb534c8a02bf23c4ff6c3aad05fd4862b0f91", "author": {"user": {"login": "npawar", "name": "Neha Pawar"}}, "url": "https://github.com/apache/pinot/commit/9b8fb534c8a02bf23c4ff6c3aad05fd4862b0f91", "committedDate": "2020-07-13T22:36:52Z", "message": "TransformConfig in IngestionConfig for ingestion transformations"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "951fc7065cd3486a93bcd8f8f28f53de065cf1b7", "author": {"user": {"login": "npawar", "name": "Neha Pawar"}}, "url": "https://github.com/apache/pinot/commit/951fc7065cd3486a93bcd8f8f28f53de065cf1b7", "committedDate": "2020-07-13T22:36:52Z", "message": "Review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f3ef701d78488ae3c9363ec4d134a0446bf15766", "author": {"user": {"login": "npawar", "name": "Neha Pawar"}}, "url": "https://github.com/apache/pinot/commit/f3ef701d78488ae3c9363ec4d134a0446bf15766", "committedDate": "2020-07-13T22:37:22Z", "message": "Comment"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "1342333055d1c6082d946406c0483ebd8c6a200c", "author": {"user": {"login": "npawar", "name": "Neha Pawar"}}, "url": "https://github.com/apache/pinot/commit/1342333055d1c6082d946406c0483ebd8c6a200c", "committedDate": "2020-07-13T22:25:04Z", "message": "Review comments"}, "afterCommit": {"oid": "f3ef701d78488ae3c9363ec4d134a0446bf15766", "author": {"user": {"login": "npawar", "name": "Neha Pawar"}}, "url": "https://github.com/apache/pinot/commit/f3ef701d78488ae3c9363ec4d134a0446bf15766", "committedDate": "2020-07-13T22:37:22Z", "message": "Comment"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ3NjY1MDYw", "url": "https://github.com/apache/pinot/pull/5681#pullrequestreview-447665060", "createdAt": "2020-07-13T22:52:42Z", "commit": {"oid": "f3ef701d78488ae3c9363ec4d134a0446bf15766"}, "state": "APPROVED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QyMjo1Mjo0MlrOGw9evA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QyMjo1ODo1MlrOGw9nAg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5MjEyNA==", "bodyText": "Remove this line", "url": "https://github.com/apache/pinot/pull/5681#discussion_r453992124", "createdAt": "2020-07-13T22:52:42Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/IngestionConfigHybridIntegrationTest.java", "diffHunk": "@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.integration.tests;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import java.io.File;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import javax.annotation.Nullable;\n+import org.apache.pinot.controller.ControllerConf;\n+import org.apache.pinot.spi.config.table.IngestionConfig;\n+import org.apache.pinot.spi.config.table.TableConfig;\n+import org.apache.pinot.spi.config.table.ingestion.FilterConfig;\n+import org.apache.pinot.spi.config.table.ingestion.TransformConfig;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.util.TestUtils;\n+import org.testng.Assert;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Test;\n+\n+import static org.testng.Assert.assertEquals;\n+\n+\n+/**\n+ * Tests ingestion configs on a hybrid table\n+ */\n+public class IngestionConfigHybridIntegrationTest extends BaseClusterIntegrationTest {\n+  private static final int NUM_OFFLINE_SEGMENTS = 8;\n+  private static final int NUM_REALTIME_SEGMENTS = 6;\n+  private static final String TIME_COLUMN_NAME = \"millisSinceEpoch\";\n+  private static final String SCHEMA_FILE_NAME = \"On_Time_On_Time_Performance_2014_100k_subset_nonulls_ingestion_config.schema\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f3ef701d78488ae3c9363ec4d134a0446bf15766"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5MjE3OA==", "bodyText": "Remove", "url": "https://github.com/apache/pinot/pull/5681#discussion_r453992178", "createdAt": "2020-07-13T22:52:52Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/IngestionConfigHybridIntegrationTest.java", "diffHunk": "@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.integration.tests;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import java.io.File;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import javax.annotation.Nullable;\n+import org.apache.pinot.controller.ControllerConf;\n+import org.apache.pinot.spi.config.table.IngestionConfig;\n+import org.apache.pinot.spi.config.table.TableConfig;\n+import org.apache.pinot.spi.config.table.ingestion.FilterConfig;\n+import org.apache.pinot.spi.config.table.ingestion.TransformConfig;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.util.TestUtils;\n+import org.testng.Assert;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Test;\n+\n+import static org.testng.Assert.assertEquals;\n+\n+\n+/**\n+ * Tests ingestion configs on a hybrid table\n+ */\n+public class IngestionConfigHybridIntegrationTest extends BaseClusterIntegrationTest {\n+  private static final int NUM_OFFLINE_SEGMENTS = 8;\n+  private static final int NUM_REALTIME_SEGMENTS = 6;\n+  private static final String TIME_COLUMN_NAME = \"millisSinceEpoch\";\n+  private static final String SCHEMA_FILE_NAME = \"On_Time_On_Time_Performance_2014_100k_subset_nonulls_ingestion_config.schema\";\n+  private static final long FILTERED_COUNT_STAR_RESULT = 24047L;\n+\n+  @Override\n+  protected String getSchemaFileName() {\n+    return SCHEMA_FILE_NAME;\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f3ef701d78488ae3c9363ec4d134a0446bf15766"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5Mjc4Ng==", "bodyText": "Let's document how this value is calculated (query result of SELECT COUNT(*) FROM mytable WHERE AirlineID != 19393 AND ArrDelayMinutes > 5 on unfiltered data)", "url": "https://github.com/apache/pinot/pull/5681#discussion_r453992786", "createdAt": "2020-07-13T22:54:37Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/IngestionConfigHybridIntegrationTest.java", "diffHunk": "@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.integration.tests;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import java.io.File;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import javax.annotation.Nullable;\n+import org.apache.pinot.controller.ControllerConf;\n+import org.apache.pinot.spi.config.table.IngestionConfig;\n+import org.apache.pinot.spi.config.table.TableConfig;\n+import org.apache.pinot.spi.config.table.ingestion.FilterConfig;\n+import org.apache.pinot.spi.config.table.ingestion.TransformConfig;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.util.TestUtils;\n+import org.testng.Assert;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Test;\n+\n+import static org.testng.Assert.assertEquals;\n+\n+\n+/**\n+ * Tests ingestion configs on a hybrid table\n+ */\n+public class IngestionConfigHybridIntegrationTest extends BaseClusterIntegrationTest {\n+  private static final int NUM_OFFLINE_SEGMENTS = 8;\n+  private static final int NUM_REALTIME_SEGMENTS = 6;\n+  private static final String TIME_COLUMN_NAME = \"millisSinceEpoch\";\n+  private static final String SCHEMA_FILE_NAME = \"On_Time_On_Time_Performance_2014_100k_subset_nonulls_ingestion_config.schema\";\n+  private static final long FILTERED_COUNT_STAR_RESULT = 24047L;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f3ef701d78488ae3c9363ec4d134a0446bf15766"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5NDI0Mg==", "bodyText": "(nit)\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              public void testTransformConfigsFromTable() {\n          \n          \n            \n              public void testTransformConfigsFromTableConfig() {", "url": "https://github.com/apache/pinot/pull/5681#discussion_r453994242", "createdAt": "2020-07-13T22:58:52Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/test/java/org/apache/pinot/core/data/recordtransformer/ExpressionTransformerTest.java", "diffHunk": "@@ -43,14 +43,28 @@\n public class ExpressionTransformerTest {\n \n   @Test\n-  public void testGroovyExpressionTransformer()\n-      throws IOException {\n-    URL resource = AbstractRecordExtractorTest.class.getClassLoader()\n-        .getResource(\"data/expression_transformer/groovy_expression_transformer.json\");\n-    File schemaFile = new File(resource.getFile());\n-    Schema pinotSchema = Schema.fromFile(schemaFile);\n-\n-    ExpressionTransformer expressionTransformer = new ExpressionTransformer(pinotSchema);\n+  public void testTransformConfigsFromTable() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f3ef701d78488ae3c9363ec4d134a0446bf15766"}, "originalPosition": 39}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "78c219dbaad76dbd23af98d575def265dababad1", "author": {"user": {"login": "npawar", "name": "Neha Pawar"}}, "url": "https://github.com/apache/pinot/commit/78c219dbaad76dbd23af98d575def265dababad1", "committedDate": "2020-07-13T23:18:12Z", "message": "Review comments"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 318, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}