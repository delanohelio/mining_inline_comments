{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU1NTg2NTkz", "number": 5741, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxMzo1NTowNlrOERdesw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxMzo1NTo0MFrOERdgXQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NzQ0MjQzOnYy", "diffSide": "RIGHT", "path": "pinot-plugins/pinot-batch-ingestion/pinot-batch-ingestion-spark/src/main/java/org/apache/pinot/plugin/ingestion/batch/spark/SparkSegmentGenerationJobRunner.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxMzo1NTowNlrOG2LqhQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxOToxNToyMlrOG2YIQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTQ2NzM5Nw==", "bodyText": "I can't tell if the original or the new formatting adheres to the Pinot Style, please ensure the new one does.", "url": "https://github.com/apache/pinot/pull/5741#discussion_r459467397", "createdAt": "2020-07-23T13:55:06Z", "author": {"login": "mayankshriv"}, "path": "pinot-plugins/pinot-batch-ingestion/pinot-batch-ingestion-spark/src/main/java/org/apache/pinot/plugin/ingestion/batch/spark/SparkSegmentGenerationJobRunner.java", "diffHunk": "@@ -209,93 +210,99 @@ public void run()\n               .get(PLUGINS_INCLUDE_PROPERTY_NAME) : null;\n       final URI finalInputDirURI = inputDirURI;\n       final URI finalOutputDirURI = (stagingDirURI == null) ? outputDirURI : stagingDirURI;\n-      pathRDD.foreach(pathAndIdx -> {\n-        for (PinotFSSpec pinotFSSpec : _spec.getPinotFSSpecs()) {\n-          PinotFSFactory.register(pinotFSSpec.getScheme(), pinotFSSpec.getClassName(), new PinotConfiguration(pinotFSSpec));\n-        }\n-        PinotFS finalOutputDirFS = PinotFSFactory.create(finalOutputDirURI.getScheme());\n-        String[] splits = pathAndIdx.split(\" \");\n-        String path = splits[0];\n-        int idx = Integer.valueOf(splits[1]);\n-        // Load Pinot Plugins copied from Distributed cache.\n-        File localPluginsTarFile = new File(PINOT_PLUGINS_TAR_GZ);\n-        if (localPluginsTarFile.exists()) {\n-          File pluginsDirFile = new File(PINOT_PLUGINS_DIR + \"-\" + idx);\n-          try {\n-            TarGzCompressionUtils.untar(localPluginsTarFile, pluginsDirFile);\n-          } catch (Exception e) {\n-            LOGGER.error(\"Failed to untar local Pinot plugins tarball file [{}]\", localPluginsTarFile, e);\n-            throw new RuntimeException(e);\n+      // Prevent using lambda expression in Spark to avoid potential serialization exceptions, use inner function instead.\n+      pathRDD.foreach(new VoidFunction<String>() {\n+        @Override\n+        public void call(String pathAndIdx)\n+            throws Exception {\n+          PluginManager.get().init();\n+          for (PinotFSSpec pinotFSSpec : _spec.getPinotFSSpecs()) {\n+            PinotFSFactory.register(pinotFSSpec.getScheme(), pinotFSSpec.getClassName(), new PinotConfiguration(pinotFSSpec));\n           }\n-          LOGGER.info(\"Trying to set System Property: [{}={}]\", PLUGINS_DIR_PROPERTY_NAME,\n-              pluginsDirFile.getAbsolutePath());\n-          System.setProperty(PLUGINS_DIR_PROPERTY_NAME, pluginsDirFile.getAbsolutePath());\n-          if (pluginsInclude != null) {\n-            LOGGER.info(\"Trying to set System Property: [{}={}]\", PLUGINS_INCLUDE_PROPERTY_NAME, pluginsInclude);\n-            System.setProperty(PLUGINS_INCLUDE_PROPERTY_NAME, pluginsInclude);\n+          PinotFS finalOutputDirFS = PinotFSFactory.create(finalOutputDirURI.getScheme());\n+          String[] splits = pathAndIdx.split(\" \");\n+          String path = splits[0];\n+          int idx = Integer.valueOf(splits[1]);\n+          // Load Pinot Plugins copied from Distributed cache.\n+          File localPluginsTarFile = new File(PINOT_PLUGINS_TAR_GZ);\n+          if (localPluginsTarFile.exists()) {\n+            File pluginsDirFile = new File(PINOT_PLUGINS_DIR + \"-\" + idx);\n+            try {\n+              TarGzCompressionUtils.untar(localPluginsTarFile, pluginsDirFile);\n+            } catch (Exception e) {\n+              LOGGER.error(\"Failed to untar local Pinot plugins tarball file [{}]\", localPluginsTarFile, e);\n+              throw new RuntimeException(e);\n+            }\n+            LOGGER.info(\"Trying to set System Property: [{}={}]\", PLUGINS_DIR_PROPERTY_NAME,\n+                pluginsDirFile.getAbsolutePath());\n+            System.setProperty(PLUGINS_DIR_PROPERTY_NAME, pluginsDirFile.getAbsolutePath());\n+            if (pluginsInclude != null) {\n+              LOGGER.info(\"Trying to set System Property: [{}={}]\", PLUGINS_INCLUDE_PROPERTY_NAME, pluginsInclude);\n+              System.setProperty(PLUGINS_INCLUDE_PROPERTY_NAME, pluginsInclude);\n+            }\n+            LOGGER.info(\"Pinot plugins System Properties are set at [{}], plugins includes [{}]\",\n+                System.getProperty(PLUGINS_DIR_PROPERTY_NAME), System.getProperty(PLUGINS_INCLUDE_PROPERTY_NAME));\n+          } else {\n+            LOGGER.warn(\"Cannot find local Pinot plugins tar file at [{}]\", localPluginsTarFile.getAbsolutePath());\n+          }\n+          URI inputFileURI = URI.create(path);\n+          if (inputFileURI.getScheme() == null) {\n+            inputFileURI =\n+                new URI(finalInputDirURI.getScheme(), inputFileURI.getSchemeSpecificPart(), inputFileURI.getFragment());\n           }\n-          LOGGER.info(\"Pinot plugins System Properties are set at [{}], plugins includes [{}]\",\n-              System.getProperty(PLUGINS_DIR_PROPERTY_NAME), System.getProperty(PLUGINS_INCLUDE_PROPERTY_NAME));\n-        } else {\n-          LOGGER.warn(\"Cannot find local Pinot plugins tar file at [{}]\", localPluginsTarFile.getAbsolutePath());\n-        }\n-        URI inputFileURI = URI.create(path);\n-        if (inputFileURI.getScheme() == null) {\n-          inputFileURI =\n-              new URI(finalInputDirURI.getScheme(), inputFileURI.getSchemeSpecificPart(), inputFileURI.getFragment());\n-        }\n \n-        //create localTempDir for input and output\n-        File localTempDir = new File(FileUtils.getTempDirectory(), \"pinot-\" + UUID.randomUUID());\n-        File localInputTempDir = new File(localTempDir, \"input\");\n-        FileUtils.forceMkdir(localInputTempDir);\n-        File localOutputTempDir = new File(localTempDir, \"output\");\n-        FileUtils.forceMkdir(localOutputTempDir);\n+          //create localTempDir for input and output", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aaf69d8d26f1cb6ecebcad8660c95bd139eb24e6"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY3MTYxOQ==", "bodyText": "updated!", "url": "https://github.com/apache/pinot/pull/5741#discussion_r459671619", "createdAt": "2020-07-23T19:15:22Z", "author": {"login": "xiangfu0"}, "path": "pinot-plugins/pinot-batch-ingestion/pinot-batch-ingestion-spark/src/main/java/org/apache/pinot/plugin/ingestion/batch/spark/SparkSegmentGenerationJobRunner.java", "diffHunk": "@@ -209,93 +210,99 @@ public void run()\n               .get(PLUGINS_INCLUDE_PROPERTY_NAME) : null;\n       final URI finalInputDirURI = inputDirURI;\n       final URI finalOutputDirURI = (stagingDirURI == null) ? outputDirURI : stagingDirURI;\n-      pathRDD.foreach(pathAndIdx -> {\n-        for (PinotFSSpec pinotFSSpec : _spec.getPinotFSSpecs()) {\n-          PinotFSFactory.register(pinotFSSpec.getScheme(), pinotFSSpec.getClassName(), new PinotConfiguration(pinotFSSpec));\n-        }\n-        PinotFS finalOutputDirFS = PinotFSFactory.create(finalOutputDirURI.getScheme());\n-        String[] splits = pathAndIdx.split(\" \");\n-        String path = splits[0];\n-        int idx = Integer.valueOf(splits[1]);\n-        // Load Pinot Plugins copied from Distributed cache.\n-        File localPluginsTarFile = new File(PINOT_PLUGINS_TAR_GZ);\n-        if (localPluginsTarFile.exists()) {\n-          File pluginsDirFile = new File(PINOT_PLUGINS_DIR + \"-\" + idx);\n-          try {\n-            TarGzCompressionUtils.untar(localPluginsTarFile, pluginsDirFile);\n-          } catch (Exception e) {\n-            LOGGER.error(\"Failed to untar local Pinot plugins tarball file [{}]\", localPluginsTarFile, e);\n-            throw new RuntimeException(e);\n+      // Prevent using lambda expression in Spark to avoid potential serialization exceptions, use inner function instead.\n+      pathRDD.foreach(new VoidFunction<String>() {\n+        @Override\n+        public void call(String pathAndIdx)\n+            throws Exception {\n+          PluginManager.get().init();\n+          for (PinotFSSpec pinotFSSpec : _spec.getPinotFSSpecs()) {\n+            PinotFSFactory.register(pinotFSSpec.getScheme(), pinotFSSpec.getClassName(), new PinotConfiguration(pinotFSSpec));\n           }\n-          LOGGER.info(\"Trying to set System Property: [{}={}]\", PLUGINS_DIR_PROPERTY_NAME,\n-              pluginsDirFile.getAbsolutePath());\n-          System.setProperty(PLUGINS_DIR_PROPERTY_NAME, pluginsDirFile.getAbsolutePath());\n-          if (pluginsInclude != null) {\n-            LOGGER.info(\"Trying to set System Property: [{}={}]\", PLUGINS_INCLUDE_PROPERTY_NAME, pluginsInclude);\n-            System.setProperty(PLUGINS_INCLUDE_PROPERTY_NAME, pluginsInclude);\n+          PinotFS finalOutputDirFS = PinotFSFactory.create(finalOutputDirURI.getScheme());\n+          String[] splits = pathAndIdx.split(\" \");\n+          String path = splits[0];\n+          int idx = Integer.valueOf(splits[1]);\n+          // Load Pinot Plugins copied from Distributed cache.\n+          File localPluginsTarFile = new File(PINOT_PLUGINS_TAR_GZ);\n+          if (localPluginsTarFile.exists()) {\n+            File pluginsDirFile = new File(PINOT_PLUGINS_DIR + \"-\" + idx);\n+            try {\n+              TarGzCompressionUtils.untar(localPluginsTarFile, pluginsDirFile);\n+            } catch (Exception e) {\n+              LOGGER.error(\"Failed to untar local Pinot plugins tarball file [{}]\", localPluginsTarFile, e);\n+              throw new RuntimeException(e);\n+            }\n+            LOGGER.info(\"Trying to set System Property: [{}={}]\", PLUGINS_DIR_PROPERTY_NAME,\n+                pluginsDirFile.getAbsolutePath());\n+            System.setProperty(PLUGINS_DIR_PROPERTY_NAME, pluginsDirFile.getAbsolutePath());\n+            if (pluginsInclude != null) {\n+              LOGGER.info(\"Trying to set System Property: [{}={}]\", PLUGINS_INCLUDE_PROPERTY_NAME, pluginsInclude);\n+              System.setProperty(PLUGINS_INCLUDE_PROPERTY_NAME, pluginsInclude);\n+            }\n+            LOGGER.info(\"Pinot plugins System Properties are set at [{}], plugins includes [{}]\",\n+                System.getProperty(PLUGINS_DIR_PROPERTY_NAME), System.getProperty(PLUGINS_INCLUDE_PROPERTY_NAME));\n+          } else {\n+            LOGGER.warn(\"Cannot find local Pinot plugins tar file at [{}]\", localPluginsTarFile.getAbsolutePath());\n+          }\n+          URI inputFileURI = URI.create(path);\n+          if (inputFileURI.getScheme() == null) {\n+            inputFileURI =\n+                new URI(finalInputDirURI.getScheme(), inputFileURI.getSchemeSpecificPart(), inputFileURI.getFragment());\n           }\n-          LOGGER.info(\"Pinot plugins System Properties are set at [{}], plugins includes [{}]\",\n-              System.getProperty(PLUGINS_DIR_PROPERTY_NAME), System.getProperty(PLUGINS_INCLUDE_PROPERTY_NAME));\n-        } else {\n-          LOGGER.warn(\"Cannot find local Pinot plugins tar file at [{}]\", localPluginsTarFile.getAbsolutePath());\n-        }\n-        URI inputFileURI = URI.create(path);\n-        if (inputFileURI.getScheme() == null) {\n-          inputFileURI =\n-              new URI(finalInputDirURI.getScheme(), inputFileURI.getSchemeSpecificPart(), inputFileURI.getFragment());\n-        }\n \n-        //create localTempDir for input and output\n-        File localTempDir = new File(FileUtils.getTempDirectory(), \"pinot-\" + UUID.randomUUID());\n-        File localInputTempDir = new File(localTempDir, \"input\");\n-        FileUtils.forceMkdir(localInputTempDir);\n-        File localOutputTempDir = new File(localTempDir, \"output\");\n-        FileUtils.forceMkdir(localOutputTempDir);\n+          //create localTempDir for input and output", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTQ2NzM5Nw=="}, "originalCommit": {"oid": "aaf69d8d26f1cb6ecebcad8660c95bd139eb24e6"}, "originalPosition": 92}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NzQ0NjY5OnYy", "diffSide": "RIGHT", "path": "pinot-plugins/pinot-batch-ingestion/pinot-batch-ingestion-spark/src/main/java/org/apache/pinot/plugin/ingestion/batch/spark/SparkSegmentGenerationJobRunner.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxMzo1NTo0MFrOG2Ls-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxODowMDozNVrOG2Vnjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTQ2ODAyNg==", "bodyText": "Thanks for fixing this, how to prevent this from happening in future? Is this unit testable?", "url": "https://github.com/apache/pinot/pull/5741#discussion_r459468026", "createdAt": "2020-07-23T13:55:40Z", "author": {"login": "mayankshriv"}, "path": "pinot-plugins/pinot-batch-ingestion/pinot-batch-ingestion-spark/src/main/java/org/apache/pinot/plugin/ingestion/batch/spark/SparkSegmentGenerationJobRunner.java", "diffHunk": "@@ -209,93 +210,99 @@ public void run()\n               .get(PLUGINS_INCLUDE_PROPERTY_NAME) : null;\n       final URI finalInputDirURI = inputDirURI;\n       final URI finalOutputDirURI = (stagingDirURI == null) ? outputDirURI : stagingDirURI;\n-      pathRDD.foreach(pathAndIdx -> {\n-        for (PinotFSSpec pinotFSSpec : _spec.getPinotFSSpecs()) {\n-          PinotFSFactory.register(pinotFSSpec.getScheme(), pinotFSSpec.getClassName(), new PinotConfiguration(pinotFSSpec));\n-        }\n-        PinotFS finalOutputDirFS = PinotFSFactory.create(finalOutputDirURI.getScheme());\n-        String[] splits = pathAndIdx.split(\" \");\n-        String path = splits[0];\n-        int idx = Integer.valueOf(splits[1]);\n-        // Load Pinot Plugins copied from Distributed cache.\n-        File localPluginsTarFile = new File(PINOT_PLUGINS_TAR_GZ);\n-        if (localPluginsTarFile.exists()) {\n-          File pluginsDirFile = new File(PINOT_PLUGINS_DIR + \"-\" + idx);\n-          try {\n-            TarGzCompressionUtils.untar(localPluginsTarFile, pluginsDirFile);\n-          } catch (Exception e) {\n-            LOGGER.error(\"Failed to untar local Pinot plugins tarball file [{}]\", localPluginsTarFile, e);\n-            throw new RuntimeException(e);\n+      // Prevent using lambda expression in Spark to avoid potential serialization exceptions, use inner function instead.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aaf69d8d26f1cb6ecebcad8660c95bd139eb24e6"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTYzMDQ3OA==", "bodyText": "This is not happening in every spark version/cluster, likely to be a bug in Spark, but we need to accommodate it .\nWe hit this issue until some users reported it.\nI don't have a good way to prevent it apart from code review :(", "url": "https://github.com/apache/pinot/pull/5741#discussion_r459630478", "createdAt": "2020-07-23T18:00:35Z", "author": {"login": "xiangfu0"}, "path": "pinot-plugins/pinot-batch-ingestion/pinot-batch-ingestion-spark/src/main/java/org/apache/pinot/plugin/ingestion/batch/spark/SparkSegmentGenerationJobRunner.java", "diffHunk": "@@ -209,93 +210,99 @@ public void run()\n               .get(PLUGINS_INCLUDE_PROPERTY_NAME) : null;\n       final URI finalInputDirURI = inputDirURI;\n       final URI finalOutputDirURI = (stagingDirURI == null) ? outputDirURI : stagingDirURI;\n-      pathRDD.foreach(pathAndIdx -> {\n-        for (PinotFSSpec pinotFSSpec : _spec.getPinotFSSpecs()) {\n-          PinotFSFactory.register(pinotFSSpec.getScheme(), pinotFSSpec.getClassName(), new PinotConfiguration(pinotFSSpec));\n-        }\n-        PinotFS finalOutputDirFS = PinotFSFactory.create(finalOutputDirURI.getScheme());\n-        String[] splits = pathAndIdx.split(\" \");\n-        String path = splits[0];\n-        int idx = Integer.valueOf(splits[1]);\n-        // Load Pinot Plugins copied from Distributed cache.\n-        File localPluginsTarFile = new File(PINOT_PLUGINS_TAR_GZ);\n-        if (localPluginsTarFile.exists()) {\n-          File pluginsDirFile = new File(PINOT_PLUGINS_DIR + \"-\" + idx);\n-          try {\n-            TarGzCompressionUtils.untar(localPluginsTarFile, pluginsDirFile);\n-          } catch (Exception e) {\n-            LOGGER.error(\"Failed to untar local Pinot plugins tarball file [{}]\", localPluginsTarFile, e);\n-            throw new RuntimeException(e);\n+      // Prevent using lambda expression in Spark to avoid potential serialization exceptions, use inner function instead.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTQ2ODAyNg=="}, "originalCommit": {"oid": "aaf69d8d26f1cb6ecebcad8660c95bd139eb24e6"}, "originalPosition": 29}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4197, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}