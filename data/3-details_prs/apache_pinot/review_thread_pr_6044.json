{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDkwNjkwMzYx", "number": 6044, "reviewThreads": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQxNTo1OTowOFrOEmKBWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwMDo1NToxNVrOEtRn3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA4NDQ1NTI5OnYy", "diffSide": "RIGHT", "path": "pinot-common/src/main/java/org/apache/pinot/common/utils/CommonConstants.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQxNTo1OTowOFrOHWBZ6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQyMDo0NDo1NFrOHWLssA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mjg1MzczNg==", "bodyText": "Actually, on second thought, the default value of 1 is not good, as it will make reduce across concurrent queries as sequential. Moreover, if we add more threads, then it may cause contention in case of high qps use cases.\nWhile we tune this, perhaps the behavior should be:\n\nIf config not explicitly specified, then preserve current behavior without executor service, or perhaps using MoreExecutors.newDirectExecutorService() that uses the calling thread to execute the Runnable.\nIf config specified, use executor service with num threads specified in the config.\n\nThoughts @kishoreg  @Jackie-Jiang ?\n(I have updated the PR with the approach above).", "url": "https://github.com/apache/pinot/pull/6044#discussion_r492853736", "createdAt": "2020-09-22T15:59:08Z", "author": {"login": "mayankshriv"}, "path": "pinot-common/src/main/java/org/apache/pinot/common/utils/CommonConstants.java", "diffHunk": "@@ -161,6 +161,10 @@\n     public static final double DEFAULT_BROKER_MIN_RESOURCE_PERCENT_FOR_START = 100.0;\n     public static final String CONFIG_OF_ENABLE_QUERY_LIMIT_OVERRIDE = \"pinot.broker.enable.query.limit.override\";\n \n+    // Config for number of threads to use for Broker reduce-phase.\n+    public static final String CONFIG_OF_NUM_REDUCE_THREADS = \"pinot.broker.num.reduce.threads\";\n+    public static final int DEFAULT_NUM_REDUCE_THREADS = 1; // TBD: Change to a more appropriate default (eg numCores).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b72494adf2992c66fd30bafada7acd4b308d6d4"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzAyMjM4NA==", "bodyText": "This config is right but the implementation can be changed. This should be something similar to what we have in combine operator - Executor pool is cached or capped at a high number based on the number of cores. But the number of callables we create be based on this config.", "url": "https://github.com/apache/pinot/pull/6044#discussion_r493022384", "createdAt": "2020-09-22T20:44:54Z", "author": {"login": "kishoreg"}, "path": "pinot-common/src/main/java/org/apache/pinot/common/utils/CommonConstants.java", "diffHunk": "@@ -161,6 +161,10 @@\n     public static final double DEFAULT_BROKER_MIN_RESOURCE_PERCENT_FOR_START = 100.0;\n     public static final String CONFIG_OF_ENABLE_QUERY_LIMIT_OVERRIDE = \"pinot.broker.enable.query.limit.override\";\n \n+    // Config for number of threads to use for Broker reduce-phase.\n+    public static final String CONFIG_OF_NUM_REDUCE_THREADS = \"pinot.broker.num.reduce.threads\";\n+    public static final int DEFAULT_NUM_REDUCE_THREADS = 1; // TBD: Change to a more appropriate default (eg numCores).", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mjg1MzczNg=="}, "originalCommit": {"oid": "8b72494adf2992c66fd30bafada7acd4b308d6d4"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0ODE3MTU1OnYy", "diffSide": "RIGHT", "path": "pinot-broker/src/main/java/org/apache/pinot/broker/requesthandler/BaseBrokerRequestHandler.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMFQwMToxMTo0MFrOHfb-Zg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwMjo0NjozMFrOHgTeRQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyNjI0Ng==", "bodyText": "This can still be final?", "url": "https://github.com/apache/pinot/pull/6044#discussion_r502726246", "createdAt": "2020-10-10T01:11:40Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-broker/src/main/java/org/apache/pinot/broker/requesthandler/BaseBrokerRequestHandler.java", "diffHunk": "@@ -102,7 +102,7 @@\n \n   protected final AtomicLong _requestIdGenerator = new AtomicLong();\n   protected final BrokerRequestOptimizer _brokerRequestOptimizer = new BrokerRequestOptimizer();\n-  protected final BrokerReduceService _brokerReduceService = new BrokerReduceService();\n+  protected BrokerReduceService _brokerReduceService;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c73604641d5d84656b0e980b95329dc3152c66f7"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzYzNTUyNQ==", "bodyText": "Yeah, not sure what happened there. Probably a side effect of trying out some intermediate code.", "url": "https://github.com/apache/pinot/pull/6044#discussion_r503635525", "createdAt": "2020-10-13T02:46:30Z", "author": {"login": "mayankshriv"}, "path": "pinot-broker/src/main/java/org/apache/pinot/broker/requesthandler/BaseBrokerRequestHandler.java", "diffHunk": "@@ -102,7 +102,7 @@\n \n   protected final AtomicLong _requestIdGenerator = new AtomicLong();\n   protected final BrokerRequestOptimizer _brokerRequestOptimizer = new BrokerRequestOptimizer();\n-  protected final BrokerReduceService _brokerReduceService = new BrokerReduceService();\n+  protected BrokerReduceService _brokerReduceService;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyNjI0Ng=="}, "originalCommit": {"oid": "c73604641d5d84656b0e980b95329dc3152c66f7"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0ODE3Njg4OnYy", "diffSide": "RIGHT", "path": "pinot-common/src/main/java/org/apache/pinot/common/utils/CommonConstants.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMFQwMToxODowMFrOHfcBGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwMjo0ODozNlrOHgTgeg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyNjkzOQ==", "bodyText": "pinot.broker.max.reduce.threads.per.query for clarity?", "url": "https://github.com/apache/pinot/pull/6044#discussion_r502726939", "createdAt": "2020-10-10T01:18:00Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-common/src/main/java/org/apache/pinot/common/utils/CommonConstants.java", "diffHunk": "@@ -166,6 +166,11 @@\n     public static final double DEFAULT_BROKER_MIN_RESOURCE_PERCENT_FOR_START = 100.0;\n     public static final String CONFIG_OF_ENABLE_QUERY_LIMIT_OVERRIDE = \"pinot.broker.enable.query.limit.override\";\n \n+    // Config for number of threads to use for Broker reduce-phase.\n+    public static final String CONFIG_OF_MAX_REDUCE_THREADS_PER_QUERY = \"pinot.broker.max.reduce.threads\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c73604641d5d84656b0e980b95329dc3152c66f7"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzYzNjA5MA==", "bodyText": "I debated about it, and felt that sub-setting becomes interesting (e.g. what does 'per' mean). But seems like there's other configs that also follow this, so will change.", "url": "https://github.com/apache/pinot/pull/6044#discussion_r503636090", "createdAt": "2020-10-13T02:48:36Z", "author": {"login": "mayankshriv"}, "path": "pinot-common/src/main/java/org/apache/pinot/common/utils/CommonConstants.java", "diffHunk": "@@ -166,6 +166,11 @@\n     public static final double DEFAULT_BROKER_MIN_RESOURCE_PERCENT_FOR_START = 100.0;\n     public static final String CONFIG_OF_ENABLE_QUERY_LIMIT_OVERRIDE = \"pinot.broker.enable.query.limit.override\";\n \n+    // Config for number of threads to use for Broker reduce-phase.\n+    public static final String CONFIG_OF_MAX_REDUCE_THREADS_PER_QUERY = \"pinot.broker.max.reduce.threads\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyNjkzOQ=="}, "originalCommit": {"oid": "c73604641d5d84656b0e980b95329dc3152c66f7"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0ODE3ODg5OnYy", "diffSide": "RIGHT", "path": "pinot-common/src/main/java/org/apache/pinot/common/utils/CommonConstants.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMFQwMToyMDowNFrOHfcCHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMFQwMToyMDowNFrOHfcCHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyNzE5Nw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public static final int MAX_REDUCE_THREADS_PER_QUERY =\n          \n          \n            \n                public static final int DEFAULT_MAX_REDUCE_THREADS_PER_QUERY =", "url": "https://github.com/apache/pinot/pull/6044#discussion_r502727197", "createdAt": "2020-10-10T01:20:04Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-common/src/main/java/org/apache/pinot/common/utils/CommonConstants.java", "diffHunk": "@@ -166,6 +166,11 @@\n     public static final double DEFAULT_BROKER_MIN_RESOURCE_PERCENT_FOR_START = 100.0;\n     public static final String CONFIG_OF_ENABLE_QUERY_LIMIT_OVERRIDE = \"pinot.broker.enable.query.limit.override\";\n \n+    // Config for number of threads to use for Broker reduce-phase.\n+    public static final String CONFIG_OF_MAX_REDUCE_THREADS_PER_QUERY = \"pinot.broker.max.reduce.threads\";\n+    public static final int MAX_REDUCE_THREADS_PER_QUERY =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c73604641d5d84656b0e980b95329dc3152c66f7"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0ODE4MDExOnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/query/reduce/BrokerReduceService.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMFQwMToyMToyOFrOHfcCsg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwMjo1NTowMFrOHgTmsA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyNzM0Ng==", "bodyText": "Log both number or worker threads and threads per query?\nAlso, if it is single-threaded, no need to launch the executor service.", "url": "https://github.com/apache/pinot/pull/6044#discussion_r502727346", "createdAt": "2020-10-10T01:21:28Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/query/reduce/BrokerReduceService.java", "diffHunk": "@@ -47,8 +57,32 @@\n @ThreadSafe\n public class BrokerReduceService {\n \n+  private static final Logger LOGGER = LoggerFactory.getLogger(BrokerReduceService.class);\n+\n+  // brw -> Shorthand for broker reduce worker threads.\n+  private static final String REDUCE_THREAD_NAME_FORMAT = \"brw-%d\";\n+  protected static final int QUERY_RUNNER_THREAD_PRIORITY = 7;\n+\n+  private final ListeningExecutorService _reduceExecutorService;\n+  private final int _maxReduceThreadsPerQuery;\n+\n+  public BrokerReduceService(PinotConfiguration config) {\n+    _maxReduceThreadsPerQuery = config.getProperty(CommonConstants.Broker.CONFIG_OF_MAX_REDUCE_THREADS_PER_QUERY,\n+        CommonConstants.Broker.MAX_REDUCE_THREADS_PER_QUERY);\n+    LOGGER.info(\"Initializing BrokerReduceService with {} reduce threads.\", _maxReduceThreadsPerQuery);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c73604641d5d84656b0e980b95329dc3152c66f7"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzYzNzY4MA==", "bodyText": "Initially, I had Guava's MoreExecutor.directorExecutor() that uses the current thread to run the task, in case of single thread. I decided to just keep it simple and have the exact same code in case of single vs multi-thread (with exception of index table). We can revisit that if needed.", "url": "https://github.com/apache/pinot/pull/6044#discussion_r503637680", "createdAt": "2020-10-13T02:55:00Z", "author": {"login": "mayankshriv"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/query/reduce/BrokerReduceService.java", "diffHunk": "@@ -47,8 +57,32 @@\n @ThreadSafe\n public class BrokerReduceService {\n \n+  private static final Logger LOGGER = LoggerFactory.getLogger(BrokerReduceService.class);\n+\n+  // brw -> Shorthand for broker reduce worker threads.\n+  private static final String REDUCE_THREAD_NAME_FORMAT = \"brw-%d\";\n+  protected static final int QUERY_RUNNER_THREAD_PRIORITY = 7;\n+\n+  private final ListeningExecutorService _reduceExecutorService;\n+  private final int _maxReduceThreadsPerQuery;\n+\n+  public BrokerReduceService(PinotConfiguration config) {\n+    _maxReduceThreadsPerQuery = config.getProperty(CommonConstants.Broker.CONFIG_OF_MAX_REDUCE_THREADS_PER_QUERY,\n+        CommonConstants.Broker.MAX_REDUCE_THREADS_PER_QUERY);\n+    LOGGER.info(\"Initializing BrokerReduceService with {} reduce threads.\", _maxReduceThreadsPerQuery);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyNzM0Ng=="}, "originalCommit": {"oid": "c73604641d5d84656b0e980b95329dc3152c66f7"}, "originalPosition": 50}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0ODE4MDg2OnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/query/reduce/BrokerReduceService.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMFQwMToyMjoyMVrOHfcDFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwMjo1NToxN1rOHgTnFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyNzQ0NQ==", "bodyText": "Any specific reason for this priority? Some comments will be appreciated", "url": "https://github.com/apache/pinot/pull/6044#discussion_r502727445", "createdAt": "2020-10-10T01:22:21Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/query/reduce/BrokerReduceService.java", "diffHunk": "@@ -47,8 +57,32 @@\n @ThreadSafe\n public class BrokerReduceService {\n \n+  private static final Logger LOGGER = LoggerFactory.getLogger(BrokerReduceService.class);\n+\n+  // brw -> Shorthand for broker reduce worker threads.\n+  private static final String REDUCE_THREAD_NAME_FORMAT = \"brw-%d\";\n+  protected static final int QUERY_RUNNER_THREAD_PRIORITY = 7;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c73604641d5d84656b0e980b95329dc3152c66f7"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzYzNzc4Mg==", "bodyText": "This is the same as the server side code, will add comments.", "url": "https://github.com/apache/pinot/pull/6044#discussion_r503637782", "createdAt": "2020-10-13T02:55:17Z", "author": {"login": "mayankshriv"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/query/reduce/BrokerReduceService.java", "diffHunk": "@@ -47,8 +57,32 @@\n @ThreadSafe\n public class BrokerReduceService {\n \n+  private static final Logger LOGGER = LoggerFactory.getLogger(BrokerReduceService.class);\n+\n+  // brw -> Shorthand for broker reduce worker threads.\n+  private static final String REDUCE_THREAD_NAME_FORMAT = \"brw-%d\";\n+  protected static final int QUERY_RUNNER_THREAD_PRIORITY = 7;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyNzQ0NQ=="}, "originalCommit": {"oid": "c73604641d5d84656b0e980b95329dc3152c66f7"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0ODE4MjQ1OnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/query/reduce/BrokerReduceService.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMFQwMToyNDowOFrOHfcD4g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMFQwMToyNDowOFrOHfcD4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyNzY1MA==", "bodyText": "I don't think we need to use the ListeningExecutorService here, ExecutorService should be enough with lower overhead", "url": "https://github.com/apache/pinot/pull/6044#discussion_r502727650", "createdAt": "2020-10-10T01:24:08Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/query/reduce/BrokerReduceService.java", "diffHunk": "@@ -47,8 +57,32 @@\n @ThreadSafe\n public class BrokerReduceService {\n \n+  private static final Logger LOGGER = LoggerFactory.getLogger(BrokerReduceService.class);\n+\n+  // brw -> Shorthand for broker reduce worker threads.\n+  private static final String REDUCE_THREAD_NAME_FORMAT = \"brw-%d\";\n+  protected static final int QUERY_RUNNER_THREAD_PRIORITY = 7;\n+\n+  private final ListeningExecutorService _reduceExecutorService;\n+  private final int _maxReduceThreadsPerQuery;\n+\n+  public BrokerReduceService(PinotConfiguration config) {\n+    _maxReduceThreadsPerQuery = config.getProperty(CommonConstants.Broker.CONFIG_OF_MAX_REDUCE_THREADS_PER_QUERY,\n+        CommonConstants.Broker.MAX_REDUCE_THREADS_PER_QUERY);\n+    LOGGER.info(\"Initializing BrokerReduceService with {} reduce threads.\", _maxReduceThreadsPerQuery);\n+\n+    ThreadFactory reduceThreadFactory =\n+        new ThreadFactoryBuilder().setDaemon(false).setPriority(QUERY_RUNNER_THREAD_PRIORITY)\n+            .setNameFormat(REDUCE_THREAD_NAME_FORMAT).build();\n+\n+    // ExecutorService is initialized with numThreads sames availableProcessors.\n+    ExecutorService delegate =\n+        Executors.newFixedThreadPool(Runtime.getRuntime().availableProcessors(), reduceThreadFactory);\n+    _reduceExecutorService = MoreExecutors.listeningDecorator(delegate);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c73604641d5d84656b0e980b95329dc3152c66f7"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0ODE4ODYzOnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/query/reduce/GroupByDataTableReducer.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMFQwMTozMDoyMlrOHfcHCg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwMDo1NDowNlrOHg-aPw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyODQ1OA==", "bodyText": "Return 1? You need at least one thread", "url": "https://github.com/apache/pinot/pull/6044#discussion_r502728458", "createdAt": "2020-10-10T01:30:22Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/query/reduce/GroupByDataTableReducer.java", "diffHunk": "@@ -231,58 +239,130 @@ private DataSchema getPrePostAggregationDataSchema(DataSchema dataSchema) {\n     return new DataSchema(columnNames, columnDataTypes);\n   }\n \n-  private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTables) {\n+  private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTablesToReduce,\n+      DataTableReducerContext reducerContext) {\n+    long start = System.currentTimeMillis();\n+    int numDataTables = dataTablesToReduce.size();\n+\n+    // Get the number of threads to use for reducing.\n+    int numReduceThreadsToUse = getNumReduceThreadsToUse(numDataTables, reducerContext.getMaxReduceThreadsPerQuery());\n+\n+    // In case of single reduce thread, fall back to SimpleIndexedTable to avoid redundant locking/unlocking calls.\n     int capacity = GroupByUtils.getTableCapacity(_queryContext);\n-    IndexedTable indexedTable = new SimpleIndexedTable(dataSchema, _queryContext, capacity);\n+    IndexedTable indexedTable =\n+        (numReduceThreadsToUse > 1) ? new ConcurrentIndexedTable(dataSchema, _queryContext, capacity)\n+            : new SimpleIndexedTable(dataSchema, _queryContext, capacity);\n+\n+    Future[] futures = new Future[numDataTables];\n+    CountDownLatch countDownLatch = new CountDownLatch(numDataTables);\n+\n+    // Create groups of data tables that each thread can process concurrently.\n+    // Given that numReduceThreads is <= numDataTables, each group will have at least one data table.\n+    ArrayList<DataTable> dataTables = new ArrayList<>(dataTablesToReduce);\n+    List<List<DataTable>> reduceGroups = new ArrayList<>(numReduceThreadsToUse);\n+\n+    for (int i = 0; i < numReduceThreadsToUse; i++) {\n+      reduceGroups.add(new ArrayList<>());\n+    }\n+    for (int i = 0; i < numDataTables; i++) {\n+      reduceGroups.get(i % numReduceThreadsToUse).add(dataTables.get(i));\n+    }\n+\n+    int cnt = 0;\n     ColumnDataType[] columnDataTypes = dataSchema.getColumnDataTypes();\n-    for (DataTable dataTable : dataTables) {\n-      int numRows = dataTable.getNumberOfRows();\n-      for (int rowId = 0; rowId < numRows; rowId++) {\n-        Object[] values = new Object[_numColumns];\n-        for (int colId = 0; colId < _numColumns; colId++) {\n-          switch (columnDataTypes[colId]) {\n-            case INT:\n-              values[colId] = dataTable.getInt(rowId, colId);\n-              break;\n-            case LONG:\n-              values[colId] = dataTable.getLong(rowId, colId);\n-              break;\n-            case FLOAT:\n-              values[colId] = dataTable.getFloat(rowId, colId);\n-              break;\n-            case DOUBLE:\n-              values[colId] = dataTable.getDouble(rowId, colId);\n-              break;\n-            case STRING:\n-              values[colId] = dataTable.getString(rowId, colId);\n-              break;\n-            case BYTES:\n-              values[colId] = dataTable.getBytes(rowId, colId);\n-              break;\n-            case OBJECT:\n-              values[colId] = dataTable.getObject(rowId, colId);\n-              break;\n-            // Add other aggregation intermediate result / group-by column type supports here\n-            default:\n-              throw new IllegalStateException();\n+    for (List<DataTable> reduceGroup : reduceGroups) {\n+      futures[cnt++] = reducerContext.getExecutorService().submit(new TraceRunnable() {\n+        @Override\n+        public void runJob() {\n+          for (DataTable dataTable : reduceGroup) {\n+            int numRows = dataTable.getNumberOfRows();\n+\n+            try {\n+              for (int rowId = 0; rowId < numRows; rowId++) {\n+                Object[] values = new Object[_numColumns];\n+                for (int colId = 0; colId < _numColumns; colId++) {\n+                  switch (columnDataTypes[colId]) {\n+                    case INT:\n+                      values[colId] = dataTable.getInt(rowId, colId);\n+                      break;\n+                    case LONG:\n+                      values[colId] = dataTable.getLong(rowId, colId);\n+                      break;\n+                    case FLOAT:\n+                      values[colId] = dataTable.getFloat(rowId, colId);\n+                      break;\n+                    case DOUBLE:\n+                      values[colId] = dataTable.getDouble(rowId, colId);\n+                      break;\n+                    case STRING:\n+                      values[colId] = dataTable.getString(rowId, colId);\n+                      break;\n+                    case BYTES:\n+                      values[colId] = dataTable.getBytes(rowId, colId);\n+                      break;\n+                    case OBJECT:\n+                      values[colId] = dataTable.getObject(rowId, colId);\n+                      break;\n+                    // Add other aggregation intermediate result / group-by column type supports here\n+                    default:\n+                      throw new IllegalStateException();\n+                  }\n+                }\n+                indexedTable.upsert(new Record(values));\n+              }\n+            } finally {\n+              countDownLatch.countDown();\n+            }\n           }\n         }\n-        indexedTable.upsert(new Record(values));\n+      });\n+    }\n+\n+    try {\n+      long timeOutMs = reducerContext.getReduceTimeOutMs() - (System.currentTimeMillis() - start);\n+      countDownLatch.await(timeOutMs, TimeUnit.MILLISECONDS);\n+    } catch (InterruptedException e) {\n+      for (Future future : futures) {\n+        if (!future.isDone()) {\n+          future.cancel(true);\n+        }\n       }\n     }\n+\n     indexedTable.finish(true);\n     return indexedTable;\n   }\n \n+  /**\n+   * Computes the number of reduce threads to use per query.\n+   * <ul>\n+   *   <li> Use single thread if number of data tables to reduce is less than {@value #MIN_DATA_TABLES_FOR_CONCURRENT_REDUCE}.</li>\n+   *   <li> Else, use min of max allowed reduce threads per query, and number of data tables.</li>\n+   * </ul>\n+   *\n+   * @param numDataTables Number of data tables to reduce\n+   * @param maxReduceThreadsPerQuery Max allowed reduce threads per query\n+   * @return Number of reduce threads to use for the query\n+   */\n+  private int getNumReduceThreadsToUse(int numDataTables, int maxReduceThreadsPerQuery) {\n+    // Use single thread if number of data tables < MIN_DATA_TABLES_FOR_CONCURRENT_REDUCE.\n+    if (numDataTables < MIN_DATA_TABLES_FOR_CONCURRENT_REDUCE) {\n+      return Math.min(1, numDataTables); // Number of data tables can be zero.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c73604641d5d84656b0e980b95329dc3152c66f7"}, "originalPosition": 221}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzY0MDIzNg==", "bodyText": "Unit test fails, seems numDataTables can be zero.", "url": "https://github.com/apache/pinot/pull/6044#discussion_r503640236", "createdAt": "2020-10-13T03:04:33Z", "author": {"login": "mayankshriv"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/query/reduce/GroupByDataTableReducer.java", "diffHunk": "@@ -231,58 +239,130 @@ private DataSchema getPrePostAggregationDataSchema(DataSchema dataSchema) {\n     return new DataSchema(columnNames, columnDataTypes);\n   }\n \n-  private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTables) {\n+  private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTablesToReduce,\n+      DataTableReducerContext reducerContext) {\n+    long start = System.currentTimeMillis();\n+    int numDataTables = dataTablesToReduce.size();\n+\n+    // Get the number of threads to use for reducing.\n+    int numReduceThreadsToUse = getNumReduceThreadsToUse(numDataTables, reducerContext.getMaxReduceThreadsPerQuery());\n+\n+    // In case of single reduce thread, fall back to SimpleIndexedTable to avoid redundant locking/unlocking calls.\n     int capacity = GroupByUtils.getTableCapacity(_queryContext);\n-    IndexedTable indexedTable = new SimpleIndexedTable(dataSchema, _queryContext, capacity);\n+    IndexedTable indexedTable =\n+        (numReduceThreadsToUse > 1) ? new ConcurrentIndexedTable(dataSchema, _queryContext, capacity)\n+            : new SimpleIndexedTable(dataSchema, _queryContext, capacity);\n+\n+    Future[] futures = new Future[numDataTables];\n+    CountDownLatch countDownLatch = new CountDownLatch(numDataTables);\n+\n+    // Create groups of data tables that each thread can process concurrently.\n+    // Given that numReduceThreads is <= numDataTables, each group will have at least one data table.\n+    ArrayList<DataTable> dataTables = new ArrayList<>(dataTablesToReduce);\n+    List<List<DataTable>> reduceGroups = new ArrayList<>(numReduceThreadsToUse);\n+\n+    for (int i = 0; i < numReduceThreadsToUse; i++) {\n+      reduceGroups.add(new ArrayList<>());\n+    }\n+    for (int i = 0; i < numDataTables; i++) {\n+      reduceGroups.get(i % numReduceThreadsToUse).add(dataTables.get(i));\n+    }\n+\n+    int cnt = 0;\n     ColumnDataType[] columnDataTypes = dataSchema.getColumnDataTypes();\n-    for (DataTable dataTable : dataTables) {\n-      int numRows = dataTable.getNumberOfRows();\n-      for (int rowId = 0; rowId < numRows; rowId++) {\n-        Object[] values = new Object[_numColumns];\n-        for (int colId = 0; colId < _numColumns; colId++) {\n-          switch (columnDataTypes[colId]) {\n-            case INT:\n-              values[colId] = dataTable.getInt(rowId, colId);\n-              break;\n-            case LONG:\n-              values[colId] = dataTable.getLong(rowId, colId);\n-              break;\n-            case FLOAT:\n-              values[colId] = dataTable.getFloat(rowId, colId);\n-              break;\n-            case DOUBLE:\n-              values[colId] = dataTable.getDouble(rowId, colId);\n-              break;\n-            case STRING:\n-              values[colId] = dataTable.getString(rowId, colId);\n-              break;\n-            case BYTES:\n-              values[colId] = dataTable.getBytes(rowId, colId);\n-              break;\n-            case OBJECT:\n-              values[colId] = dataTable.getObject(rowId, colId);\n-              break;\n-            // Add other aggregation intermediate result / group-by column type supports here\n-            default:\n-              throw new IllegalStateException();\n+    for (List<DataTable> reduceGroup : reduceGroups) {\n+      futures[cnt++] = reducerContext.getExecutorService().submit(new TraceRunnable() {\n+        @Override\n+        public void runJob() {\n+          for (DataTable dataTable : reduceGroup) {\n+            int numRows = dataTable.getNumberOfRows();\n+\n+            try {\n+              for (int rowId = 0; rowId < numRows; rowId++) {\n+                Object[] values = new Object[_numColumns];\n+                for (int colId = 0; colId < _numColumns; colId++) {\n+                  switch (columnDataTypes[colId]) {\n+                    case INT:\n+                      values[colId] = dataTable.getInt(rowId, colId);\n+                      break;\n+                    case LONG:\n+                      values[colId] = dataTable.getLong(rowId, colId);\n+                      break;\n+                    case FLOAT:\n+                      values[colId] = dataTable.getFloat(rowId, colId);\n+                      break;\n+                    case DOUBLE:\n+                      values[colId] = dataTable.getDouble(rowId, colId);\n+                      break;\n+                    case STRING:\n+                      values[colId] = dataTable.getString(rowId, colId);\n+                      break;\n+                    case BYTES:\n+                      values[colId] = dataTable.getBytes(rowId, colId);\n+                      break;\n+                    case OBJECT:\n+                      values[colId] = dataTable.getObject(rowId, colId);\n+                      break;\n+                    // Add other aggregation intermediate result / group-by column type supports here\n+                    default:\n+                      throw new IllegalStateException();\n+                  }\n+                }\n+                indexedTable.upsert(new Record(values));\n+              }\n+            } finally {\n+              countDownLatch.countDown();\n+            }\n           }\n         }\n-        indexedTable.upsert(new Record(values));\n+      });\n+    }\n+\n+    try {\n+      long timeOutMs = reducerContext.getReduceTimeOutMs() - (System.currentTimeMillis() - start);\n+      countDownLatch.await(timeOutMs, TimeUnit.MILLISECONDS);\n+    } catch (InterruptedException e) {\n+      for (Future future : futures) {\n+        if (!future.isDone()) {\n+          future.cancel(true);\n+        }\n       }\n     }\n+\n     indexedTable.finish(true);\n     return indexedTable;\n   }\n \n+  /**\n+   * Computes the number of reduce threads to use per query.\n+   * <ul>\n+   *   <li> Use single thread if number of data tables to reduce is less than {@value #MIN_DATA_TABLES_FOR_CONCURRENT_REDUCE}.</li>\n+   *   <li> Else, use min of max allowed reduce threads per query, and number of data tables.</li>\n+   * </ul>\n+   *\n+   * @param numDataTables Number of data tables to reduce\n+   * @param maxReduceThreadsPerQuery Max allowed reduce threads per query\n+   * @return Number of reduce threads to use for the query\n+   */\n+  private int getNumReduceThreadsToUse(int numDataTables, int maxReduceThreadsPerQuery) {\n+    // Use single thread if number of data tables < MIN_DATA_TABLES_FOR_CONCURRENT_REDUCE.\n+    if (numDataTables < MIN_DATA_TABLES_FOR_CONCURRENT_REDUCE) {\n+      return Math.min(1, numDataTables); // Number of data tables can be zero.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyODQ1OA=="}, "originalCommit": {"oid": "c73604641d5d84656b0e980b95329dc3152c66f7"}, "originalPosition": 221}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDMzOTAwNw==", "bodyText": "Yes, numDataTables can be zero, but I think returning 0 or 1 should both work. Actually a better approach should be just short-circuit the zero data table case.", "url": "https://github.com/apache/pinot/pull/6044#discussion_r504339007", "createdAt": "2020-10-14T00:54:06Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/query/reduce/GroupByDataTableReducer.java", "diffHunk": "@@ -231,58 +239,130 @@ private DataSchema getPrePostAggregationDataSchema(DataSchema dataSchema) {\n     return new DataSchema(columnNames, columnDataTypes);\n   }\n \n-  private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTables) {\n+  private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTablesToReduce,\n+      DataTableReducerContext reducerContext) {\n+    long start = System.currentTimeMillis();\n+    int numDataTables = dataTablesToReduce.size();\n+\n+    // Get the number of threads to use for reducing.\n+    int numReduceThreadsToUse = getNumReduceThreadsToUse(numDataTables, reducerContext.getMaxReduceThreadsPerQuery());\n+\n+    // In case of single reduce thread, fall back to SimpleIndexedTable to avoid redundant locking/unlocking calls.\n     int capacity = GroupByUtils.getTableCapacity(_queryContext);\n-    IndexedTable indexedTable = new SimpleIndexedTable(dataSchema, _queryContext, capacity);\n+    IndexedTable indexedTable =\n+        (numReduceThreadsToUse > 1) ? new ConcurrentIndexedTable(dataSchema, _queryContext, capacity)\n+            : new SimpleIndexedTable(dataSchema, _queryContext, capacity);\n+\n+    Future[] futures = new Future[numDataTables];\n+    CountDownLatch countDownLatch = new CountDownLatch(numDataTables);\n+\n+    // Create groups of data tables that each thread can process concurrently.\n+    // Given that numReduceThreads is <= numDataTables, each group will have at least one data table.\n+    ArrayList<DataTable> dataTables = new ArrayList<>(dataTablesToReduce);\n+    List<List<DataTable>> reduceGroups = new ArrayList<>(numReduceThreadsToUse);\n+\n+    for (int i = 0; i < numReduceThreadsToUse; i++) {\n+      reduceGroups.add(new ArrayList<>());\n+    }\n+    for (int i = 0; i < numDataTables; i++) {\n+      reduceGroups.get(i % numReduceThreadsToUse).add(dataTables.get(i));\n+    }\n+\n+    int cnt = 0;\n     ColumnDataType[] columnDataTypes = dataSchema.getColumnDataTypes();\n-    for (DataTable dataTable : dataTables) {\n-      int numRows = dataTable.getNumberOfRows();\n-      for (int rowId = 0; rowId < numRows; rowId++) {\n-        Object[] values = new Object[_numColumns];\n-        for (int colId = 0; colId < _numColumns; colId++) {\n-          switch (columnDataTypes[colId]) {\n-            case INT:\n-              values[colId] = dataTable.getInt(rowId, colId);\n-              break;\n-            case LONG:\n-              values[colId] = dataTable.getLong(rowId, colId);\n-              break;\n-            case FLOAT:\n-              values[colId] = dataTable.getFloat(rowId, colId);\n-              break;\n-            case DOUBLE:\n-              values[colId] = dataTable.getDouble(rowId, colId);\n-              break;\n-            case STRING:\n-              values[colId] = dataTable.getString(rowId, colId);\n-              break;\n-            case BYTES:\n-              values[colId] = dataTable.getBytes(rowId, colId);\n-              break;\n-            case OBJECT:\n-              values[colId] = dataTable.getObject(rowId, colId);\n-              break;\n-            // Add other aggregation intermediate result / group-by column type supports here\n-            default:\n-              throw new IllegalStateException();\n+    for (List<DataTable> reduceGroup : reduceGroups) {\n+      futures[cnt++] = reducerContext.getExecutorService().submit(new TraceRunnable() {\n+        @Override\n+        public void runJob() {\n+          for (DataTable dataTable : reduceGroup) {\n+            int numRows = dataTable.getNumberOfRows();\n+\n+            try {\n+              for (int rowId = 0; rowId < numRows; rowId++) {\n+                Object[] values = new Object[_numColumns];\n+                for (int colId = 0; colId < _numColumns; colId++) {\n+                  switch (columnDataTypes[colId]) {\n+                    case INT:\n+                      values[colId] = dataTable.getInt(rowId, colId);\n+                      break;\n+                    case LONG:\n+                      values[colId] = dataTable.getLong(rowId, colId);\n+                      break;\n+                    case FLOAT:\n+                      values[colId] = dataTable.getFloat(rowId, colId);\n+                      break;\n+                    case DOUBLE:\n+                      values[colId] = dataTable.getDouble(rowId, colId);\n+                      break;\n+                    case STRING:\n+                      values[colId] = dataTable.getString(rowId, colId);\n+                      break;\n+                    case BYTES:\n+                      values[colId] = dataTable.getBytes(rowId, colId);\n+                      break;\n+                    case OBJECT:\n+                      values[colId] = dataTable.getObject(rowId, colId);\n+                      break;\n+                    // Add other aggregation intermediate result / group-by column type supports here\n+                    default:\n+                      throw new IllegalStateException();\n+                  }\n+                }\n+                indexedTable.upsert(new Record(values));\n+              }\n+            } finally {\n+              countDownLatch.countDown();\n+            }\n           }\n         }\n-        indexedTable.upsert(new Record(values));\n+      });\n+    }\n+\n+    try {\n+      long timeOutMs = reducerContext.getReduceTimeOutMs() - (System.currentTimeMillis() - start);\n+      countDownLatch.await(timeOutMs, TimeUnit.MILLISECONDS);\n+    } catch (InterruptedException e) {\n+      for (Future future : futures) {\n+        if (!future.isDone()) {\n+          future.cancel(true);\n+        }\n       }\n     }\n+\n     indexedTable.finish(true);\n     return indexedTable;\n   }\n \n+  /**\n+   * Computes the number of reduce threads to use per query.\n+   * <ul>\n+   *   <li> Use single thread if number of data tables to reduce is less than {@value #MIN_DATA_TABLES_FOR_CONCURRENT_REDUCE}.</li>\n+   *   <li> Else, use min of max allowed reduce threads per query, and number of data tables.</li>\n+   * </ul>\n+   *\n+   * @param numDataTables Number of data tables to reduce\n+   * @param maxReduceThreadsPerQuery Max allowed reduce threads per query\n+   * @return Number of reduce threads to use for the query\n+   */\n+  private int getNumReduceThreadsToUse(int numDataTables, int maxReduceThreadsPerQuery) {\n+    // Use single thread if number of data tables < MIN_DATA_TABLES_FOR_CONCURRENT_REDUCE.\n+    if (numDataTables < MIN_DATA_TABLES_FOR_CONCURRENT_REDUCE) {\n+      return Math.min(1, numDataTables); // Number of data tables can be zero.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyODQ1OA=="}, "originalCommit": {"oid": "c73604641d5d84656b0e980b95329dc3152c66f7"}, "originalPosition": 221}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0ODE5MjkxOnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/query/reduce/GroupByDataTableReducer.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMFQwMTozNToxM1rOHfcJUg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMFQwMTozNToxM1rOHfcJUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyOTA0Mg==", "bodyText": "(Critical) You need to put the timeout exception into the query response, or the response will be wrong and there is no way to detect that", "url": "https://github.com/apache/pinot/pull/6044#discussion_r502729042", "createdAt": "2020-10-10T01:35:13Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/query/reduce/GroupByDataTableReducer.java", "diffHunk": "@@ -231,58 +239,130 @@ private DataSchema getPrePostAggregationDataSchema(DataSchema dataSchema) {\n     return new DataSchema(columnNames, columnDataTypes);\n   }\n \n-  private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTables) {\n+  private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTablesToReduce,\n+      DataTableReducerContext reducerContext) {\n+    long start = System.currentTimeMillis();\n+    int numDataTables = dataTablesToReduce.size();\n+\n+    // Get the number of threads to use for reducing.\n+    int numReduceThreadsToUse = getNumReduceThreadsToUse(numDataTables, reducerContext.getMaxReduceThreadsPerQuery());\n+\n+    // In case of single reduce thread, fall back to SimpleIndexedTable to avoid redundant locking/unlocking calls.\n     int capacity = GroupByUtils.getTableCapacity(_queryContext);\n-    IndexedTable indexedTable = new SimpleIndexedTable(dataSchema, _queryContext, capacity);\n+    IndexedTable indexedTable =\n+        (numReduceThreadsToUse > 1) ? new ConcurrentIndexedTable(dataSchema, _queryContext, capacity)\n+            : new SimpleIndexedTable(dataSchema, _queryContext, capacity);\n+\n+    Future[] futures = new Future[numDataTables];\n+    CountDownLatch countDownLatch = new CountDownLatch(numDataTables);\n+\n+    // Create groups of data tables that each thread can process concurrently.\n+    // Given that numReduceThreads is <= numDataTables, each group will have at least one data table.\n+    ArrayList<DataTable> dataTables = new ArrayList<>(dataTablesToReduce);\n+    List<List<DataTable>> reduceGroups = new ArrayList<>(numReduceThreadsToUse);\n+\n+    for (int i = 0; i < numReduceThreadsToUse; i++) {\n+      reduceGroups.add(new ArrayList<>());\n+    }\n+    for (int i = 0; i < numDataTables; i++) {\n+      reduceGroups.get(i % numReduceThreadsToUse).add(dataTables.get(i));\n+    }\n+\n+    int cnt = 0;\n     ColumnDataType[] columnDataTypes = dataSchema.getColumnDataTypes();\n-    for (DataTable dataTable : dataTables) {\n-      int numRows = dataTable.getNumberOfRows();\n-      for (int rowId = 0; rowId < numRows; rowId++) {\n-        Object[] values = new Object[_numColumns];\n-        for (int colId = 0; colId < _numColumns; colId++) {\n-          switch (columnDataTypes[colId]) {\n-            case INT:\n-              values[colId] = dataTable.getInt(rowId, colId);\n-              break;\n-            case LONG:\n-              values[colId] = dataTable.getLong(rowId, colId);\n-              break;\n-            case FLOAT:\n-              values[colId] = dataTable.getFloat(rowId, colId);\n-              break;\n-            case DOUBLE:\n-              values[colId] = dataTable.getDouble(rowId, colId);\n-              break;\n-            case STRING:\n-              values[colId] = dataTable.getString(rowId, colId);\n-              break;\n-            case BYTES:\n-              values[colId] = dataTable.getBytes(rowId, colId);\n-              break;\n-            case OBJECT:\n-              values[colId] = dataTable.getObject(rowId, colId);\n-              break;\n-            // Add other aggregation intermediate result / group-by column type supports here\n-            default:\n-              throw new IllegalStateException();\n+    for (List<DataTable> reduceGroup : reduceGroups) {\n+      futures[cnt++] = reducerContext.getExecutorService().submit(new TraceRunnable() {\n+        @Override\n+        public void runJob() {\n+          for (DataTable dataTable : reduceGroup) {\n+            int numRows = dataTable.getNumberOfRows();\n+\n+            try {\n+              for (int rowId = 0; rowId < numRows; rowId++) {\n+                Object[] values = new Object[_numColumns];\n+                for (int colId = 0; colId < _numColumns; colId++) {\n+                  switch (columnDataTypes[colId]) {\n+                    case INT:\n+                      values[colId] = dataTable.getInt(rowId, colId);\n+                      break;\n+                    case LONG:\n+                      values[colId] = dataTable.getLong(rowId, colId);\n+                      break;\n+                    case FLOAT:\n+                      values[colId] = dataTable.getFloat(rowId, colId);\n+                      break;\n+                    case DOUBLE:\n+                      values[colId] = dataTable.getDouble(rowId, colId);\n+                      break;\n+                    case STRING:\n+                      values[colId] = dataTable.getString(rowId, colId);\n+                      break;\n+                    case BYTES:\n+                      values[colId] = dataTable.getBytes(rowId, colId);\n+                      break;\n+                    case OBJECT:\n+                      values[colId] = dataTable.getObject(rowId, colId);\n+                      break;\n+                    // Add other aggregation intermediate result / group-by column type supports here\n+                    default:\n+                      throw new IllegalStateException();\n+                  }\n+                }\n+                indexedTable.upsert(new Record(values));\n+              }\n+            } finally {\n+              countDownLatch.countDown();\n+            }\n           }\n         }\n-        indexedTable.upsert(new Record(values));\n+      });\n+    }\n+\n+    try {\n+      long timeOutMs = reducerContext.getReduceTimeOutMs() - (System.currentTimeMillis() - start);\n+      countDownLatch.await(timeOutMs, TimeUnit.MILLISECONDS);\n+    } catch (InterruptedException e) {\n+      for (Future future : futures) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c73604641d5d84656b0e980b95329dc3152c66f7"}, "originalPosition": 196}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0ODE5MzU0OnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/query/reduce/GroupByDataTableReducer.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMFQwMTozNjoxNFrOHfcJpw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QyMzowNzowOVrOHg8izA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyOTEyNw==", "bodyText": "Don't use the executor service for single-threaded case. There is overhead of using that instead of the current thread, which might cause performance degradation.", "url": "https://github.com/apache/pinot/pull/6044#discussion_r502729127", "createdAt": "2020-10-10T01:36:14Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/query/reduce/GroupByDataTableReducer.java", "diffHunk": "@@ -231,58 +239,130 @@ private DataSchema getPrePostAggregationDataSchema(DataSchema dataSchema) {\n     return new DataSchema(columnNames, columnDataTypes);\n   }\n \n-  private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTables) {\n+  private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTablesToReduce,\n+      DataTableReducerContext reducerContext) {\n+    long start = System.currentTimeMillis();\n+    int numDataTables = dataTablesToReduce.size();\n+\n+    // Get the number of threads to use for reducing.\n+    int numReduceThreadsToUse = getNumReduceThreadsToUse(numDataTables, reducerContext.getMaxReduceThreadsPerQuery());\n+\n+    // In case of single reduce thread, fall back to SimpleIndexedTable to avoid redundant locking/unlocking calls.\n     int capacity = GroupByUtils.getTableCapacity(_queryContext);\n-    IndexedTable indexedTable = new SimpleIndexedTable(dataSchema, _queryContext, capacity);\n+    IndexedTable indexedTable =\n+        (numReduceThreadsToUse > 1) ? new ConcurrentIndexedTable(dataSchema, _queryContext, capacity)\n+            : new SimpleIndexedTable(dataSchema, _queryContext, capacity);\n+\n+    Future[] futures = new Future[numDataTables];", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c73604641d5d84656b0e980b95329dc3152c66f7"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzY0MDYxNA==", "bodyText": "I really don't want to have two separate implementations (one for single thread and one for multi-thread). I did some benchmark with high qps use case, and the overhead is not measurable. Will keep it like this for now, until we find evidence that it hurts performance.", "url": "https://github.com/apache/pinot/pull/6044#discussion_r503640614", "createdAt": "2020-10-13T03:06:03Z", "author": {"login": "mayankshriv"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/query/reduce/GroupByDataTableReducer.java", "diffHunk": "@@ -231,58 +239,130 @@ private DataSchema getPrePostAggregationDataSchema(DataSchema dataSchema) {\n     return new DataSchema(columnNames, columnDataTypes);\n   }\n \n-  private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTables) {\n+  private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTablesToReduce,\n+      DataTableReducerContext reducerContext) {\n+    long start = System.currentTimeMillis();\n+    int numDataTables = dataTablesToReduce.size();\n+\n+    // Get the number of threads to use for reducing.\n+    int numReduceThreadsToUse = getNumReduceThreadsToUse(numDataTables, reducerContext.getMaxReduceThreadsPerQuery());\n+\n+    // In case of single reduce thread, fall back to SimpleIndexedTable to avoid redundant locking/unlocking calls.\n     int capacity = GroupByUtils.getTableCapacity(_queryContext);\n-    IndexedTable indexedTable = new SimpleIndexedTable(dataSchema, _queryContext, capacity);\n+    IndexedTable indexedTable =\n+        (numReduceThreadsToUse > 1) ? new ConcurrentIndexedTable(dataSchema, _queryContext, capacity)\n+            : new SimpleIndexedTable(dataSchema, _queryContext, capacity);\n+\n+    Future[] futures = new Future[numDataTables];", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyOTEyNw=="}, "originalCommit": {"oid": "c73604641d5d84656b0e980b95329dc3152c66f7"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDMwODQyOA==", "bodyText": "Did some perf benchmarking on high throughput use case. The overhead does not seem to register. Will leave it as-is for now.", "url": "https://github.com/apache/pinot/pull/6044#discussion_r504308428", "createdAt": "2020-10-13T23:07:09Z", "author": {"login": "mayankshriv"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/query/reduce/GroupByDataTableReducer.java", "diffHunk": "@@ -231,58 +239,130 @@ private DataSchema getPrePostAggregationDataSchema(DataSchema dataSchema) {\n     return new DataSchema(columnNames, columnDataTypes);\n   }\n \n-  private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTables) {\n+  private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTablesToReduce,\n+      DataTableReducerContext reducerContext) {\n+    long start = System.currentTimeMillis();\n+    int numDataTables = dataTablesToReduce.size();\n+\n+    // Get the number of threads to use for reducing.\n+    int numReduceThreadsToUse = getNumReduceThreadsToUse(numDataTables, reducerContext.getMaxReduceThreadsPerQuery());\n+\n+    // In case of single reduce thread, fall back to SimpleIndexedTable to avoid redundant locking/unlocking calls.\n     int capacity = GroupByUtils.getTableCapacity(_queryContext);\n-    IndexedTable indexedTable = new SimpleIndexedTable(dataSchema, _queryContext, capacity);\n+    IndexedTable indexedTable =\n+        (numReduceThreadsToUse > 1) ? new ConcurrentIndexedTable(dataSchema, _queryContext, capacity)\n+            : new SimpleIndexedTable(dataSchema, _queryContext, capacity);\n+\n+    Future[] futures = new Future[numDataTables];", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyOTEyNw=="}, "originalCommit": {"oid": "c73604641d5d84656b0e980b95329dc3152c66f7"}, "originalPosition": 96}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0ODE5NjU4OnYy", "diffSide": "RIGHT", "path": "pinot-core/src/test/java/org/apache/pinot/queries/DistinctQueriesTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMFQwMTozOTozNlrOHfcLLw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMFQwMTozOTozNlrOHfcLLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyOTUxOQ==", "bodyText": "I feel the original formatting is better. You can skip the reformatting by adding //@formatter:off, see AggregationFunctionUtils.isFitForDictionaryBasedComputation() for details.", "url": "https://github.com/apache/pinot/pull/6044#discussion_r502729519", "createdAt": "2020-10-10T01:39:36Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/test/java/org/apache/pinot/queries/DistinctQueriesTest.java", "diffHunk": "@@ -319,13 +321,13 @@ private void testDistinctInnerSegmentHelper(String[] queries, boolean isPql)\n   @Test\n   public void testDistinctInnerSegment()\n       throws Exception {\n-    testDistinctInnerSegmentHelper(new String[]{\n-        \"SELECT DISTINCT(intColumn, longColumn, floatColumn, doubleColumn, stringColumn, bytesColumn) FROM testTable LIMIT 10000\",\n-        \"SELECT DISTINCT(stringColumn, bytesColumn, floatColumn) FROM testTable WHERE intColumn >= 60 LIMIT 10000\",\n-        \"SELECT DISTINCT(intColumn, bytesColumn) FROM testTable ORDER BY bytesColumn LIMIT 5\",\n-        \"SELECT DISTINCT(ADD ( intColumn,  floatColumn  ), stringColumn) FROM testTable WHERE longColumn < 60 ORDER BY stringColumn DESC, ADD(intColumn, floatColumn) ASC LIMIT 10\",\n-        \"SELECT DISTINCT(floatColumn, longColumn) FROM testTable WHERE stringColumn = 'a' ORDER BY longColumn LIMIT 10\"\n-    }, true);\n+    testDistinctInnerSegmentHelper(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c73604641d5d84656b0e980b95329dc3152c66f7"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1OTA5NjU1OnYy", "diffSide": "RIGHT", "path": "pinot-core/src/test/java/org/apache/pinot/queries/DistinctQueriesTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwMDo1Mjo0MFrOHg-Y6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwMDo1Mjo0MFrOHg-Y6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDMzODY2Ng==", "bodyText": "You need to have another comment //@formatter:on to turn the formatter on after the queries", "url": "https://github.com/apache/pinot/pull/6044#discussion_r504338666", "createdAt": "2020-10-14T00:52:40Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/test/java/org/apache/pinot/queries/DistinctQueriesTest.java", "diffHunk": "@@ -319,6 +321,7 @@ private void testDistinctInnerSegmentHelper(String[] queries, boolean isPql)\n   @Test\n   public void testDistinctInnerSegment()\n       throws Exception {\n+    //@formatter:off", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e20f784d0300eb26ab31391156e75bd3915da646"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1OTEwMTA4OnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/query/reduce/GroupByDataTableReducer.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwMDo1NToxNlrOHg-bdQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwMjoyMzo0MlrOHg_ybw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDMzOTMxNw==", "bodyText": "We can short circuit the single data table case by directly return the new SimpleIndexedTable(dataSchema, _queryContext, capacity)", "url": "https://github.com/apache/pinot/pull/6044#discussion_r504339317", "createdAt": "2020-10-14T00:55:16Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/query/reduce/GroupByDataTableReducer.java", "diffHunk": "@@ -231,58 +255,134 @@ private DataSchema getPrePostAggregationDataSchema(DataSchema dataSchema) {\n     return new DataSchema(columnNames, columnDataTypes);\n   }\n \n-  private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTables) {\n+  private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTablesToReduce,\n+      DataTableReducerContext reducerContext)\n+      throws TimeoutException {\n+    long start = System.currentTimeMillis();\n+    int numDataTables = dataTablesToReduce.size();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e20f784d0300eb26ab31391156e75bd3915da646"}, "originalPosition": 104}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM2MTU4Mw==", "bodyText": "I did that initially. But then it requires indexTable.finish(). And in future anytime we have other such tasks that need to be done before returning, they will need to be done at two places. So I chose to avoid that.", "url": "https://github.com/apache/pinot/pull/6044#discussion_r504361583", "createdAt": "2020-10-14T02:23:42Z", "author": {"login": "mayankshriv"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/query/reduce/GroupByDataTableReducer.java", "diffHunk": "@@ -231,58 +255,134 @@ private DataSchema getPrePostAggregationDataSchema(DataSchema dataSchema) {\n     return new DataSchema(columnNames, columnDataTypes);\n   }\n \n-  private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTables) {\n+  private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTablesToReduce,\n+      DataTableReducerContext reducerContext)\n+      throws TimeoutException {\n+    long start = System.currentTimeMillis();\n+    int numDataTables = dataTablesToReduce.size();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDMzOTMxNw=="}, "originalCommit": {"oid": "e20f784d0300eb26ab31391156e75bd3915da646"}, "originalPosition": 104}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3914, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}