{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzk2NTAzNDYw", "number": 5200, "title": "[TE] Runner to generate SLA based metric data missing alerts", "bodyText": "Implements the runner that generates Data SLA anomalies. DATA_MISSING anomalies are created if the data is not available for the sla detection window within the configured SLA.\nAlso inclues:\n\nLogic to merge SLA anomaly with existing and mark them as parent-child.\nUnit tests covering various DATA MISSING use-cases", "createdAt": "2020-03-31T18:06:46Z", "url": "https://github.com/apache/pinot/pull/5200", "merged": true, "mergeCommit": {"oid": "7f04bdf9955ffa1c63ada90e40a899c27bd3eac3"}, "closed": true, "closedAt": "2020-04-15T00:12:17Z", "author": {"login": "akshayrai"}, "timelineItems": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcTGCgugH2gAyMzk2NTAzNDYwOjA3YjY4ZjBiNjZlZjVjZjc1ZmE1M2E4OTc2ZDg0NWUyZjI3MTQ5ZTM=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcWysibgFqTM5MTgyMDg2OA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "07b68f0b66ef5cf75fa53a8976d845e2f27149e3", "author": {"user": {"login": "akshayrai", "name": "Akshay Rai"}}, "url": "https://github.com/apache/pinot/commit/07b68f0b66ef5cf75fa53a8976d845e2f27149e3", "committedDate": "2020-03-31T16:52:49Z", "message": "[TE] SLA based metric data missing alerts"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c9fbaca08d2ffaadf26c3c25bdf2ff4aa088ef96", "author": {"user": {"login": "akshayrai", "name": "Akshay Rai"}}, "url": "https://github.com/apache/pinot/commit/c9fbaca08d2ffaadf26c3c25bdf2ff4aa088ef96", "committedDate": "2020-03-31T18:23:29Z", "message": "remove comment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "722322b51c1f62f0dd9dde7aadec9fb34574fb40", "author": {"user": {"login": "akshayrai", "name": "Akshay Rai"}}, "url": "https://github.com/apache/pinot/commit/722322b51c1f62f0dd9dde7aadec9fb34574fb40", "committedDate": "2020-03-31T22:08:05Z", "message": "injected sla settings into the anomaly properties"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg2ODk4NjA3", "url": "https://github.com/apache/pinot/pull/5200#pullrequestreview-386898607", "createdAt": "2020-04-03T02:26:33Z", "commit": {"oid": "722322b51c1f62f0dd9dde7aadec9fb34574fb40"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QwMjoyNjozM1rOGAC8dQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QwMjo1MjoyM1rOGADV2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjcwMTQyOQ==", "bodyText": "Why we care about partial data? In our current setting the endTime of detection window is current time, and it is very common that we have partial data.", "url": "https://github.com/apache/pinot/pull/5200#discussion_r402701429", "createdAt": "2020-04-03T02:26:33Z", "author": {"login": "xiaohui-sun"}, "path": "thirdeye/thirdeye-pinot/src/main/java/org/apache/pinot/thirdeye/detection/datasla/DatasetSlaTaskRunner.java", "diffHunk": "@@ -0,0 +1,320 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.pinot.thirdeye.detection.datasla;\n+\n+import com.google.common.collect.ArrayListMultimap;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.pinot.thirdeye.anomaly.AnomalyType;\n+import org.apache.pinot.thirdeye.anomaly.task.TaskContext;\n+import org.apache.pinot.thirdeye.anomaly.task.TaskInfo;\n+import org.apache.pinot.thirdeye.anomaly.task.TaskResult;\n+import org.apache.pinot.thirdeye.anomaly.task.TaskRunner;\n+import org.apache.pinot.thirdeye.common.time.TimeGranularity;\n+import org.apache.pinot.thirdeye.dataframe.DataFrame;\n+import org.apache.pinot.thirdeye.dataframe.util.MetricSlice;\n+import org.apache.pinot.thirdeye.datalayer.bao.DatasetConfigManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.DetectionConfigManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.EvaluationManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.EventManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.MergedAnomalyResultManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.MetricConfigManager;\n+import org.apache.pinot.thirdeye.datalayer.dto.DatasetConfigDTO;\n+import org.apache.pinot.thirdeye.datalayer.dto.DetectionConfigDTO;\n+import org.apache.pinot.thirdeye.datalayer.dto.MergedAnomalyResultDTO;\n+import org.apache.pinot.thirdeye.datalayer.dto.MetricConfigDTO;\n+import org.apache.pinot.thirdeye.datalayer.pojo.MergedAnomalyResultBean;\n+import org.apache.pinot.thirdeye.datasource.DAORegistry;\n+import org.apache.pinot.thirdeye.datasource.ThirdEyeCacheRegistry;\n+import org.apache.pinot.thirdeye.datasource.loader.AggregationLoader;\n+import org.apache.pinot.thirdeye.datasource.loader.DefaultAggregationLoader;\n+import org.apache.pinot.thirdeye.datasource.loader.DefaultTimeSeriesLoader;\n+import org.apache.pinot.thirdeye.datasource.loader.TimeSeriesLoader;\n+import org.apache.pinot.thirdeye.detection.ConfigUtils;\n+import org.apache.pinot.thirdeye.detection.DataProvider;\n+import org.apache.pinot.thirdeye.detection.DefaultDataProvider;\n+import org.apache.pinot.thirdeye.detection.DetectionPipelineLoader;\n+import org.apache.pinot.thirdeye.detection.DetectionPipelineTaskInfo;\n+import org.apache.pinot.thirdeye.detection.DetectionUtils;\n+import org.apache.pinot.thirdeye.detection.cache.builder.AnomaliesCacheBuilder;\n+import org.apache.pinot.thirdeye.detection.cache.builder.TimeSeriesCacheBuilder;\n+import org.apache.pinot.thirdeye.rootcause.impl.MetricEntity;\n+import org.joda.time.DateTime;\n+import org.joda.time.DateTimeZone;\n+import org.joda.time.Period;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.pinot.thirdeye.anomaly.utils.ThirdeyeMetricsUtil.*;\n+\n+\n+/**\n+ * Runner that generates Data SLA anomalies. DATA_MISSING anomalies are created if\n+ * the data is not available for the sla detection window within the configured SLA.\n+ */\n+public class DatasetSlaTaskRunner implements TaskRunner {\n+  private static final Logger LOG = LoggerFactory.getLogger(DatasetSlaTaskRunner.class);\n+\n+  private final DetectionConfigManager detectionDAO;\n+  private final MergedAnomalyResultManager anomalyDAO;\n+  private final EvaluationManager evaluationDAO;\n+  private DataProvider provider;\n+\n+  private final String DEFAULT_DATA_SLA = \"3_DAYS\";\n+\n+  public DatasetSlaTaskRunner() {\n+    this.detectionDAO = DAORegistry.getInstance().getDetectionConfigManager();\n+    this.anomalyDAO = DAORegistry.getInstance().getMergedAnomalyResultDAO();\n+    this.evaluationDAO = DAORegistry.getInstance().getEvaluationManager();\n+\n+    MetricConfigManager metricDAO = DAORegistry.getInstance().getMetricConfigDAO();\n+    DatasetConfigManager datasetDAO = DAORegistry.getInstance().getDatasetConfigDAO();\n+    EventManager eventDAO = DAORegistry.getInstance().getEventDAO();\n+\n+    TimeSeriesLoader timeseriesLoader =\n+        new DefaultTimeSeriesLoader(metricDAO, datasetDAO,\n+            ThirdEyeCacheRegistry.getInstance().getQueryCache(), ThirdEyeCacheRegistry.getInstance().getTimeSeriesCache());\n+\n+    AggregationLoader aggregationLoader =\n+        new DefaultAggregationLoader(metricDAO, datasetDAO,\n+            ThirdEyeCacheRegistry.getInstance().getQueryCache(),\n+            ThirdEyeCacheRegistry.getInstance().getDatasetMaxDataTimeCache());\n+\n+    this.provider = new DefaultDataProvider(metricDAO, datasetDAO, eventDAO, this.anomalyDAO, this.evaluationDAO,\n+        timeseriesLoader, aggregationLoader, new DetectionPipelineLoader(), TimeSeriesCacheBuilder.getInstance(),\n+        AnomaliesCacheBuilder.getInstance());\n+  }\n+\n+  public DatasetSlaTaskRunner(DetectionConfigManager detectionDAO, MergedAnomalyResultManager anomalyDAO,\n+      EvaluationManager evaluationDAO, DataProvider provider) {\n+    this.detectionDAO = detectionDAO;\n+    this.anomalyDAO = anomalyDAO;\n+    this.evaluationDAO = evaluationDAO;\n+    this.provider = provider;\n+  }\n+\n+  @Override\n+  public List<TaskResult> execute(TaskInfo taskInfo, TaskContext taskContext) throws Exception {\n+    dataAvailabilityTaskCounter.inc();\n+\n+    try {\n+      DetectionPipelineTaskInfo info = (DetectionPipelineTaskInfo) taskInfo;\n+\n+      DetectionConfigDTO config = this.detectionDAO.findById(info.getConfigId());\n+      if (config == null) {\n+        throw new IllegalArgumentException(String.format(\"Could not resolve config id %d\", info.getConfigId()));\n+      }\n+\n+      LOG.info(\"Check data sla for config {} between {} and {}\", config.getId(), info.getStart(), info.getEnd());\n+      Map<String, Object> metricUrnToSlaMap = config.getDataSLAProperties();\n+      if (MapUtils.isNotEmpty(metricUrnToSlaMap)) {\n+        for (Map.Entry<String, Object> metricUrnToSlaMapEntry : metricUrnToSlaMap.entrySet()) {\n+          MetricEntity me = MetricEntity.fromURN(metricUrnToSlaMapEntry.getKey());\n+          MetricConfigDTO\n+              metricConfigDTO = this.provider.fetchMetrics(Collections.singletonList(me.getId())).get(me.getId());\n+          Map<String, DatasetConfigDTO> datasetToConfigMap = provider.fetchDatasets(Collections.singletonList(metricConfigDTO.getDataset()));\n+          for (Map.Entry<String, DatasetConfigDTO> datasetSLA : datasetToConfigMap.entrySet()) {\n+            try {\n+              runSLACheck(me, datasetSLA.getValue(), info, ConfigUtils.getMap(metricUrnToSlaMapEntry.getValue()));\n+            } catch (Exception e) {\n+              LOG.error(\"Failed to run sla check on metric URN %s\", datasetSLA.getKey(), e);\n+            }\n+          }\n+        }\n+      }\n+\n+      //LOG.info(\"End data availability for config {} between {} and {}. Detected {} anomalies.\", config.getId(),\n+      //    info.getStart(), info.getEnd(), anomaliesList.size());\n+      return Collections.emptyList();\n+    } catch(Exception e) {\n+      throw e;\n+    }\n+  }\n+\n+  /**\n+   * Runs the data sla check for the window (info) on the given metric (me) using the configured sla properties\n+   */\n+  private void runSLACheck(MetricEntity me, DatasetConfigDTO datasetConfig, DetectionPipelineTaskInfo info,\n+      Map<String, Object> slaProps) {\n+    if (me == null || datasetConfig == null || info == null) {\n+      //nothing to check\n+      return;\n+    }\n+\n+    try {\n+      long datasetLastRefreshTime = datasetConfig.getLastRefreshTime();\n+      if (datasetLastRefreshTime <= 0) {\n+        // assume we have processed data till the current detection start\n+        datasetLastRefreshTime = info.getStart() - 1;\n+      }\n+\n+      if (isMissingData(datasetLastRefreshTime, info)) {\n+        // Double check with data source as 2 things are possible.\n+        // 1. This dataset/source may not support availability events\n+        // 2. The data availability event pipeline has some issue.\n+        // We want to measure the overall dataset availability (filters are not taken into account)\n+        MetricSlice metricSlice = MetricSlice.from(me.getId(), info.getStart(), info.getEnd(), ArrayListMultimap.<String, String>create());\n+        DataFrame dataFrame = this.provider.fetchTimeseries(Collections.singleton(metricSlice)).get(metricSlice);\n+        if (dataFrame == null || dataFrame.isEmpty()) {\n+          // no data\n+          if (hasMissedSLA(datasetLastRefreshTime, slaProps, info.getEnd())) {\n+            createDataSLAAnomaly(datasetLastRefreshTime + 1, info.getEnd(), info.getConfigId(), datasetConfig, slaProps);\n+          }\n+        } else {\n+          datasetLastRefreshTime = dataFrame.getDoubles(\"timestamp\").max().longValue();\n+          if (isPartialData(datasetLastRefreshTime, info, datasetConfig)) {\n+            if (hasMissedSLA(datasetLastRefreshTime, slaProps, info.getEnd())) {\n+              createDataSLAAnomaly(datasetLastRefreshTime + 1, info.getEnd(), info.getConfigId(), datasetConfig, slaProps);\n+            }\n+          }\n+        }\n+      } else if (isPartialData(datasetLastRefreshTime, info, datasetConfig)) {\n+        // Optimize for the common case - the common case is that the data availability events are arriving\n+        // correctly and we need not re-fetch the data to double check.\n+        if (hasMissedSLA(datasetLastRefreshTime, slaProps, info.getEnd())) {\n+          createDataSLAAnomaly(datasetLastRefreshTime + 1, info.getEnd(), info.getConfigId(), datasetConfig, slaProps);\n+        }\n+      }\n+    } catch (Exception e) {\n+      LOG.error(String.format(\"Failed to run sla check on metric URN %s\", me.getUrn()), e);\n+    }\n+  }\n+\n+  /**\n+   * We say the data is missing we do not have data in the sla detection window.\n+   * Or more specifically if the dataset watermark is below the sla detection window.\n+   */\n+  private boolean isMissingData(long datasetLastRefreshTime, DetectionPipelineTaskInfo info) {\n+    return datasetLastRefreshTime < info.getStart();\n+  }\n+\n+  /**\n+   * We say the data is partial if we do not have all the data points in the sla detection window.\n+   * Or more specifically if we have at least 1 data-point missing in the sla detection window.\n+   *\n+   * For example:\n+   * Assume that the data is delayed and our current sla detection window is [1st Feb to 3rd Feb). During this scan,\n+   * let's say data for 1st Feb arrives. Now, we have a situation where partial data is present. In other words, in the\n+   * current sla detection window [1st to 3rd Feb) we have data for 1st Feb but data for 2nd Feb is missing. This is the\n+   * partial data scenario.\n+   */\n+  private boolean isPartialData(long datasetLastRefreshTime, DetectionPipelineTaskInfo info, DatasetConfigDTO datasetConfig) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "722322b51c1f62f0dd9dde7aadec9fb34574fb40"}, "originalPosition": 221}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjcwMTk4OQ==", "bodyText": "We need to consider the expectedDelay in datasetconfig.\nThere are some datasets that have several days delay by default.", "url": "https://github.com/apache/pinot/pull/5200#discussion_r402701989", "createdAt": "2020-04-03T02:28:49Z", "author": {"login": "xiaohui-sun"}, "path": "thirdeye/thirdeye-pinot/src/main/java/org/apache/pinot/thirdeye/detection/datasla/DatasetSlaTaskRunner.java", "diffHunk": "@@ -0,0 +1,320 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.pinot.thirdeye.detection.datasla;\n+\n+import com.google.common.collect.ArrayListMultimap;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.pinot.thirdeye.anomaly.AnomalyType;\n+import org.apache.pinot.thirdeye.anomaly.task.TaskContext;\n+import org.apache.pinot.thirdeye.anomaly.task.TaskInfo;\n+import org.apache.pinot.thirdeye.anomaly.task.TaskResult;\n+import org.apache.pinot.thirdeye.anomaly.task.TaskRunner;\n+import org.apache.pinot.thirdeye.common.time.TimeGranularity;\n+import org.apache.pinot.thirdeye.dataframe.DataFrame;\n+import org.apache.pinot.thirdeye.dataframe.util.MetricSlice;\n+import org.apache.pinot.thirdeye.datalayer.bao.DatasetConfigManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.DetectionConfigManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.EvaluationManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.EventManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.MergedAnomalyResultManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.MetricConfigManager;\n+import org.apache.pinot.thirdeye.datalayer.dto.DatasetConfigDTO;\n+import org.apache.pinot.thirdeye.datalayer.dto.DetectionConfigDTO;\n+import org.apache.pinot.thirdeye.datalayer.dto.MergedAnomalyResultDTO;\n+import org.apache.pinot.thirdeye.datalayer.dto.MetricConfigDTO;\n+import org.apache.pinot.thirdeye.datalayer.pojo.MergedAnomalyResultBean;\n+import org.apache.pinot.thirdeye.datasource.DAORegistry;\n+import org.apache.pinot.thirdeye.datasource.ThirdEyeCacheRegistry;\n+import org.apache.pinot.thirdeye.datasource.loader.AggregationLoader;\n+import org.apache.pinot.thirdeye.datasource.loader.DefaultAggregationLoader;\n+import org.apache.pinot.thirdeye.datasource.loader.DefaultTimeSeriesLoader;\n+import org.apache.pinot.thirdeye.datasource.loader.TimeSeriesLoader;\n+import org.apache.pinot.thirdeye.detection.ConfigUtils;\n+import org.apache.pinot.thirdeye.detection.DataProvider;\n+import org.apache.pinot.thirdeye.detection.DefaultDataProvider;\n+import org.apache.pinot.thirdeye.detection.DetectionPipelineLoader;\n+import org.apache.pinot.thirdeye.detection.DetectionPipelineTaskInfo;\n+import org.apache.pinot.thirdeye.detection.DetectionUtils;\n+import org.apache.pinot.thirdeye.detection.cache.builder.AnomaliesCacheBuilder;\n+import org.apache.pinot.thirdeye.detection.cache.builder.TimeSeriesCacheBuilder;\n+import org.apache.pinot.thirdeye.rootcause.impl.MetricEntity;\n+import org.joda.time.DateTime;\n+import org.joda.time.DateTimeZone;\n+import org.joda.time.Period;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.pinot.thirdeye.anomaly.utils.ThirdeyeMetricsUtil.*;\n+\n+\n+/**\n+ * Runner that generates Data SLA anomalies. DATA_MISSING anomalies are created if\n+ * the data is not available for the sla detection window within the configured SLA.\n+ */\n+public class DatasetSlaTaskRunner implements TaskRunner {\n+  private static final Logger LOG = LoggerFactory.getLogger(DatasetSlaTaskRunner.class);\n+\n+  private final DetectionConfigManager detectionDAO;\n+  private final MergedAnomalyResultManager anomalyDAO;\n+  private final EvaluationManager evaluationDAO;\n+  private DataProvider provider;\n+\n+  private final String DEFAULT_DATA_SLA = \"3_DAYS\";\n+\n+  public DatasetSlaTaskRunner() {\n+    this.detectionDAO = DAORegistry.getInstance().getDetectionConfigManager();\n+    this.anomalyDAO = DAORegistry.getInstance().getMergedAnomalyResultDAO();\n+    this.evaluationDAO = DAORegistry.getInstance().getEvaluationManager();\n+\n+    MetricConfigManager metricDAO = DAORegistry.getInstance().getMetricConfigDAO();\n+    DatasetConfigManager datasetDAO = DAORegistry.getInstance().getDatasetConfigDAO();\n+    EventManager eventDAO = DAORegistry.getInstance().getEventDAO();\n+\n+    TimeSeriesLoader timeseriesLoader =\n+        new DefaultTimeSeriesLoader(metricDAO, datasetDAO,\n+            ThirdEyeCacheRegistry.getInstance().getQueryCache(), ThirdEyeCacheRegistry.getInstance().getTimeSeriesCache());\n+\n+    AggregationLoader aggregationLoader =\n+        new DefaultAggregationLoader(metricDAO, datasetDAO,\n+            ThirdEyeCacheRegistry.getInstance().getQueryCache(),\n+            ThirdEyeCacheRegistry.getInstance().getDatasetMaxDataTimeCache());\n+\n+    this.provider = new DefaultDataProvider(metricDAO, datasetDAO, eventDAO, this.anomalyDAO, this.evaluationDAO,\n+        timeseriesLoader, aggregationLoader, new DetectionPipelineLoader(), TimeSeriesCacheBuilder.getInstance(),\n+        AnomaliesCacheBuilder.getInstance());\n+  }\n+\n+  public DatasetSlaTaskRunner(DetectionConfigManager detectionDAO, MergedAnomalyResultManager anomalyDAO,\n+      EvaluationManager evaluationDAO, DataProvider provider) {\n+    this.detectionDAO = detectionDAO;\n+    this.anomalyDAO = anomalyDAO;\n+    this.evaluationDAO = evaluationDAO;\n+    this.provider = provider;\n+  }\n+\n+  @Override\n+  public List<TaskResult> execute(TaskInfo taskInfo, TaskContext taskContext) throws Exception {\n+    dataAvailabilityTaskCounter.inc();\n+\n+    try {\n+      DetectionPipelineTaskInfo info = (DetectionPipelineTaskInfo) taskInfo;\n+\n+      DetectionConfigDTO config = this.detectionDAO.findById(info.getConfigId());\n+      if (config == null) {\n+        throw new IllegalArgumentException(String.format(\"Could not resolve config id %d\", info.getConfigId()));\n+      }\n+\n+      LOG.info(\"Check data sla for config {} between {} and {}\", config.getId(), info.getStart(), info.getEnd());\n+      Map<String, Object> metricUrnToSlaMap = config.getDataSLAProperties();\n+      if (MapUtils.isNotEmpty(metricUrnToSlaMap)) {\n+        for (Map.Entry<String, Object> metricUrnToSlaMapEntry : metricUrnToSlaMap.entrySet()) {\n+          MetricEntity me = MetricEntity.fromURN(metricUrnToSlaMapEntry.getKey());\n+          MetricConfigDTO\n+              metricConfigDTO = this.provider.fetchMetrics(Collections.singletonList(me.getId())).get(me.getId());\n+          Map<String, DatasetConfigDTO> datasetToConfigMap = provider.fetchDatasets(Collections.singletonList(metricConfigDTO.getDataset()));\n+          for (Map.Entry<String, DatasetConfigDTO> datasetSLA : datasetToConfigMap.entrySet()) {\n+            try {\n+              runSLACheck(me, datasetSLA.getValue(), info, ConfigUtils.getMap(metricUrnToSlaMapEntry.getValue()));\n+            } catch (Exception e) {\n+              LOG.error(\"Failed to run sla check on metric URN %s\", datasetSLA.getKey(), e);\n+            }\n+          }\n+        }\n+      }\n+\n+      //LOG.info(\"End data availability for config {} between {} and {}. Detected {} anomalies.\", config.getId(),\n+      //    info.getStart(), info.getEnd(), anomaliesList.size());\n+      return Collections.emptyList();\n+    } catch(Exception e) {\n+      throw e;\n+    }\n+  }\n+\n+  /**\n+   * Runs the data sla check for the window (info) on the given metric (me) using the configured sla properties\n+   */\n+  private void runSLACheck(MetricEntity me, DatasetConfigDTO datasetConfig, DetectionPipelineTaskInfo info,\n+      Map<String, Object> slaProps) {\n+    if (me == null || datasetConfig == null || info == null) {\n+      //nothing to check\n+      return;\n+    }\n+\n+    try {\n+      long datasetLastRefreshTime = datasetConfig.getLastRefreshTime();\n+      if (datasetLastRefreshTime <= 0) {\n+        // assume we have processed data till the current detection start\n+        datasetLastRefreshTime = info.getStart() - 1;\n+      }\n+\n+      if (isMissingData(datasetLastRefreshTime, info)) {\n+        // Double check with data source as 2 things are possible.\n+        // 1. This dataset/source may not support availability events\n+        // 2. The data availability event pipeline has some issue.\n+        // We want to measure the overall dataset availability (filters are not taken into account)\n+        MetricSlice metricSlice = MetricSlice.from(me.getId(), info.getStart(), info.getEnd(), ArrayListMultimap.<String, String>create());\n+        DataFrame dataFrame = this.provider.fetchTimeseries(Collections.singleton(metricSlice)).get(metricSlice);\n+        if (dataFrame == null || dataFrame.isEmpty()) {\n+          // no data\n+          if (hasMissedSLA(datasetLastRefreshTime, slaProps, info.getEnd())) {\n+            createDataSLAAnomaly(datasetLastRefreshTime + 1, info.getEnd(), info.getConfigId(), datasetConfig, slaProps);\n+          }\n+        } else {\n+          datasetLastRefreshTime = dataFrame.getDoubles(\"timestamp\").max().longValue();\n+          if (isPartialData(datasetLastRefreshTime, info, datasetConfig)) {\n+            if (hasMissedSLA(datasetLastRefreshTime, slaProps, info.getEnd())) {\n+              createDataSLAAnomaly(datasetLastRefreshTime + 1, info.getEnd(), info.getConfigId(), datasetConfig, slaProps);\n+            }\n+          }\n+        }\n+      } else if (isPartialData(datasetLastRefreshTime, info, datasetConfig)) {\n+        // Optimize for the common case - the common case is that the data availability events are arriving\n+        // correctly and we need not re-fetch the data to double check.\n+        if (hasMissedSLA(datasetLastRefreshTime, slaProps, info.getEnd())) {\n+          createDataSLAAnomaly(datasetLastRefreshTime + 1, info.getEnd(), info.getConfigId(), datasetConfig, slaProps);\n+        }\n+      }\n+    } catch (Exception e) {\n+      LOG.error(String.format(\"Failed to run sla check on metric URN %s\", me.getUrn()), e);\n+    }\n+  }\n+\n+  /**\n+   * We say the data is missing we do not have data in the sla detection window.\n+   * Or more specifically if the dataset watermark is below the sla detection window.\n+   */\n+  private boolean isMissingData(long datasetLastRefreshTime, DetectionPipelineTaskInfo info) {\n+    return datasetLastRefreshTime < info.getStart();\n+  }\n+\n+  /**\n+   * We say the data is partial if we do not have all the data points in the sla detection window.\n+   * Or more specifically if we have at least 1 data-point missing in the sla detection window.\n+   *\n+   * For example:\n+   * Assume that the data is delayed and our current sla detection window is [1st Feb to 3rd Feb). During this scan,\n+   * let's say data for 1st Feb arrives. Now, we have a situation where partial data is present. In other words, in the\n+   * current sla detection window [1st to 3rd Feb) we have data for 1st Feb but data for 2nd Feb is missing. This is the\n+   * partial data scenario.\n+   */\n+  private boolean isPartialData(long datasetLastRefreshTime, DetectionPipelineTaskInfo info, DatasetConfigDTO datasetConfig) {\n+    long granularity = datasetConfig.bucketTimeGranularity().toMillis();\n+    return (info.getEnd() - datasetLastRefreshTime) * 1.0 / granularity > 1;\n+  }\n+\n+  /**\n+   * Validates if the data is delayed or not based on the user specified SLA configuration\n+   */\n+  private boolean hasMissedSLA(long datasetLastRefreshTime, Map<String, Object> slaProps, long slaDetectionEndTime) {\n+    // fetch the user configured SLA, otherwise default 3_DAYS.\n+    long delay = TimeGranularity.fromString(MapUtils.getString(slaProps,  \"sla\", DEFAULT_DATA_SLA))\n+        .toPeriod().toStandardDuration().getMillis();\n+\n+    return (slaDetectionEndTime - datasetLastRefreshTime) >= delay;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "722322b51c1f62f0dd9dde7aadec9fb34574fb40"}, "originalPosition": 234}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjcwNjg3Mg==", "bodyText": "What we have so complicated logic here? Let's find some time to discuss offline.", "url": "https://github.com/apache/pinot/pull/5200#discussion_r402706872", "createdAt": "2020-04-03T02:48:23Z", "author": {"login": "xiaohui-sun"}, "path": "thirdeye/thirdeye-pinot/src/main/java/org/apache/pinot/thirdeye/detection/datasla/DatasetSlaTaskRunner.java", "diffHunk": "@@ -0,0 +1,320 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.pinot.thirdeye.detection.datasla;\n+\n+import com.google.common.collect.ArrayListMultimap;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.pinot.thirdeye.anomaly.AnomalyType;\n+import org.apache.pinot.thirdeye.anomaly.task.TaskContext;\n+import org.apache.pinot.thirdeye.anomaly.task.TaskInfo;\n+import org.apache.pinot.thirdeye.anomaly.task.TaskResult;\n+import org.apache.pinot.thirdeye.anomaly.task.TaskRunner;\n+import org.apache.pinot.thirdeye.common.time.TimeGranularity;\n+import org.apache.pinot.thirdeye.dataframe.DataFrame;\n+import org.apache.pinot.thirdeye.dataframe.util.MetricSlice;\n+import org.apache.pinot.thirdeye.datalayer.bao.DatasetConfigManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.DetectionConfigManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.EvaluationManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.EventManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.MergedAnomalyResultManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.MetricConfigManager;\n+import org.apache.pinot.thirdeye.datalayer.dto.DatasetConfigDTO;\n+import org.apache.pinot.thirdeye.datalayer.dto.DetectionConfigDTO;\n+import org.apache.pinot.thirdeye.datalayer.dto.MergedAnomalyResultDTO;\n+import org.apache.pinot.thirdeye.datalayer.dto.MetricConfigDTO;\n+import org.apache.pinot.thirdeye.datalayer.pojo.MergedAnomalyResultBean;\n+import org.apache.pinot.thirdeye.datasource.DAORegistry;\n+import org.apache.pinot.thirdeye.datasource.ThirdEyeCacheRegistry;\n+import org.apache.pinot.thirdeye.datasource.loader.AggregationLoader;\n+import org.apache.pinot.thirdeye.datasource.loader.DefaultAggregationLoader;\n+import org.apache.pinot.thirdeye.datasource.loader.DefaultTimeSeriesLoader;\n+import org.apache.pinot.thirdeye.datasource.loader.TimeSeriesLoader;\n+import org.apache.pinot.thirdeye.detection.ConfigUtils;\n+import org.apache.pinot.thirdeye.detection.DataProvider;\n+import org.apache.pinot.thirdeye.detection.DefaultDataProvider;\n+import org.apache.pinot.thirdeye.detection.DetectionPipelineLoader;\n+import org.apache.pinot.thirdeye.detection.DetectionPipelineTaskInfo;\n+import org.apache.pinot.thirdeye.detection.DetectionUtils;\n+import org.apache.pinot.thirdeye.detection.cache.builder.AnomaliesCacheBuilder;\n+import org.apache.pinot.thirdeye.detection.cache.builder.TimeSeriesCacheBuilder;\n+import org.apache.pinot.thirdeye.rootcause.impl.MetricEntity;\n+import org.joda.time.DateTime;\n+import org.joda.time.DateTimeZone;\n+import org.joda.time.Period;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.pinot.thirdeye.anomaly.utils.ThirdeyeMetricsUtil.*;\n+\n+\n+/**\n+ * Runner that generates Data SLA anomalies. DATA_MISSING anomalies are created if\n+ * the data is not available for the sla detection window within the configured SLA.\n+ */\n+public class DatasetSlaTaskRunner implements TaskRunner {\n+  private static final Logger LOG = LoggerFactory.getLogger(DatasetSlaTaskRunner.class);\n+\n+  private final DetectionConfigManager detectionDAO;\n+  private final MergedAnomalyResultManager anomalyDAO;\n+  private final EvaluationManager evaluationDAO;\n+  private DataProvider provider;\n+\n+  private final String DEFAULT_DATA_SLA = \"3_DAYS\";\n+\n+  public DatasetSlaTaskRunner() {\n+    this.detectionDAO = DAORegistry.getInstance().getDetectionConfigManager();\n+    this.anomalyDAO = DAORegistry.getInstance().getMergedAnomalyResultDAO();\n+    this.evaluationDAO = DAORegistry.getInstance().getEvaluationManager();\n+\n+    MetricConfigManager metricDAO = DAORegistry.getInstance().getMetricConfigDAO();\n+    DatasetConfigManager datasetDAO = DAORegistry.getInstance().getDatasetConfigDAO();\n+    EventManager eventDAO = DAORegistry.getInstance().getEventDAO();\n+\n+    TimeSeriesLoader timeseriesLoader =\n+        new DefaultTimeSeriesLoader(metricDAO, datasetDAO,\n+            ThirdEyeCacheRegistry.getInstance().getQueryCache(), ThirdEyeCacheRegistry.getInstance().getTimeSeriesCache());\n+\n+    AggregationLoader aggregationLoader =\n+        new DefaultAggregationLoader(metricDAO, datasetDAO,\n+            ThirdEyeCacheRegistry.getInstance().getQueryCache(),\n+            ThirdEyeCacheRegistry.getInstance().getDatasetMaxDataTimeCache());\n+\n+    this.provider = new DefaultDataProvider(metricDAO, datasetDAO, eventDAO, this.anomalyDAO, this.evaluationDAO,\n+        timeseriesLoader, aggregationLoader, new DetectionPipelineLoader(), TimeSeriesCacheBuilder.getInstance(),\n+        AnomaliesCacheBuilder.getInstance());\n+  }\n+\n+  public DatasetSlaTaskRunner(DetectionConfigManager detectionDAO, MergedAnomalyResultManager anomalyDAO,\n+      EvaluationManager evaluationDAO, DataProvider provider) {\n+    this.detectionDAO = detectionDAO;\n+    this.anomalyDAO = anomalyDAO;\n+    this.evaluationDAO = evaluationDAO;\n+    this.provider = provider;\n+  }\n+\n+  @Override\n+  public List<TaskResult> execute(TaskInfo taskInfo, TaskContext taskContext) throws Exception {\n+    dataAvailabilityTaskCounter.inc();\n+\n+    try {\n+      DetectionPipelineTaskInfo info = (DetectionPipelineTaskInfo) taskInfo;\n+\n+      DetectionConfigDTO config = this.detectionDAO.findById(info.getConfigId());\n+      if (config == null) {\n+        throw new IllegalArgumentException(String.format(\"Could not resolve config id %d\", info.getConfigId()));\n+      }\n+\n+      LOG.info(\"Check data sla for config {} between {} and {}\", config.getId(), info.getStart(), info.getEnd());\n+      Map<String, Object> metricUrnToSlaMap = config.getDataSLAProperties();\n+      if (MapUtils.isNotEmpty(metricUrnToSlaMap)) {\n+        for (Map.Entry<String, Object> metricUrnToSlaMapEntry : metricUrnToSlaMap.entrySet()) {\n+          MetricEntity me = MetricEntity.fromURN(metricUrnToSlaMapEntry.getKey());\n+          MetricConfigDTO\n+              metricConfigDTO = this.provider.fetchMetrics(Collections.singletonList(me.getId())).get(me.getId());\n+          Map<String, DatasetConfigDTO> datasetToConfigMap = provider.fetchDatasets(Collections.singletonList(metricConfigDTO.getDataset()));\n+          for (Map.Entry<String, DatasetConfigDTO> datasetSLA : datasetToConfigMap.entrySet()) {\n+            try {\n+              runSLACheck(me, datasetSLA.getValue(), info, ConfigUtils.getMap(metricUrnToSlaMapEntry.getValue()));\n+            } catch (Exception e) {\n+              LOG.error(\"Failed to run sla check on metric URN %s\", datasetSLA.getKey(), e);\n+            }\n+          }\n+        }\n+      }\n+\n+      //LOG.info(\"End data availability for config {} between {} and {}. Detected {} anomalies.\", config.getId(),\n+      //    info.getStart(), info.getEnd(), anomaliesList.size());\n+      return Collections.emptyList();\n+    } catch(Exception e) {\n+      throw e;\n+    }\n+  }\n+\n+  /**\n+   * Runs the data sla check for the window (info) on the given metric (me) using the configured sla properties\n+   */\n+  private void runSLACheck(MetricEntity me, DatasetConfigDTO datasetConfig, DetectionPipelineTaskInfo info,\n+      Map<String, Object> slaProps) {\n+    if (me == null || datasetConfig == null || info == null) {\n+      //nothing to check\n+      return;\n+    }\n+\n+    try {\n+      long datasetLastRefreshTime = datasetConfig.getLastRefreshTime();\n+      if (datasetLastRefreshTime <= 0) {\n+        // assume we have processed data till the current detection start\n+        datasetLastRefreshTime = info.getStart() - 1;\n+      }\n+\n+      if (isMissingData(datasetLastRefreshTime, info)) {\n+        // Double check with data source as 2 things are possible.\n+        // 1. This dataset/source may not support availability events\n+        // 2. The data availability event pipeline has some issue.\n+        // We want to measure the overall dataset availability (filters are not taken into account)\n+        MetricSlice metricSlice = MetricSlice.from(me.getId(), info.getStart(), info.getEnd(), ArrayListMultimap.<String, String>create());\n+        DataFrame dataFrame = this.provider.fetchTimeseries(Collections.singleton(metricSlice)).get(metricSlice);\n+        if (dataFrame == null || dataFrame.isEmpty()) {\n+          // no data\n+          if (hasMissedSLA(datasetLastRefreshTime, slaProps, info.getEnd())) {\n+            createDataSLAAnomaly(datasetLastRefreshTime + 1, info.getEnd(), info.getConfigId(), datasetConfig, slaProps);\n+          }\n+        } else {\n+          datasetLastRefreshTime = dataFrame.getDoubles(\"timestamp\").max().longValue();\n+          if (isPartialData(datasetLastRefreshTime, info, datasetConfig)) {\n+            if (hasMissedSLA(datasetLastRefreshTime, slaProps, info.getEnd())) {\n+              createDataSLAAnomaly(datasetLastRefreshTime + 1, info.getEnd(), info.getConfigId(), datasetConfig, slaProps);\n+            }\n+          }\n+        }\n+      } else if (isPartialData(datasetLastRefreshTime, info, datasetConfig)) {\n+        // Optimize for the common case - the common case is that the data availability events are arriving\n+        // correctly and we need not re-fetch the data to double check.\n+        if (hasMissedSLA(datasetLastRefreshTime, slaProps, info.getEnd())) {\n+          createDataSLAAnomaly(datasetLastRefreshTime + 1, info.getEnd(), info.getConfigId(), datasetConfig, slaProps);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "722322b51c1f62f0dd9dde7aadec9fb34574fb40"}, "originalPosition": 195}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjcwNzkzMA==", "bodyText": "Why this is called \"merge\"? It is only dedup?", "url": "https://github.com/apache/pinot/pull/5200#discussion_r402707930", "createdAt": "2020-04-03T02:52:23Z", "author": {"login": "xiaohui-sun"}, "path": "thirdeye/thirdeye-pinot/src/main/java/org/apache/pinot/thirdeye/detection/datasla/DatasetSlaTaskRunner.java", "diffHunk": "@@ -0,0 +1,320 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.pinot.thirdeye.detection.datasla;\n+\n+import com.google.common.collect.ArrayListMultimap;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.pinot.thirdeye.anomaly.AnomalyType;\n+import org.apache.pinot.thirdeye.anomaly.task.TaskContext;\n+import org.apache.pinot.thirdeye.anomaly.task.TaskInfo;\n+import org.apache.pinot.thirdeye.anomaly.task.TaskResult;\n+import org.apache.pinot.thirdeye.anomaly.task.TaskRunner;\n+import org.apache.pinot.thirdeye.common.time.TimeGranularity;\n+import org.apache.pinot.thirdeye.dataframe.DataFrame;\n+import org.apache.pinot.thirdeye.dataframe.util.MetricSlice;\n+import org.apache.pinot.thirdeye.datalayer.bao.DatasetConfigManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.DetectionConfigManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.EvaluationManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.EventManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.MergedAnomalyResultManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.MetricConfigManager;\n+import org.apache.pinot.thirdeye.datalayer.dto.DatasetConfigDTO;\n+import org.apache.pinot.thirdeye.datalayer.dto.DetectionConfigDTO;\n+import org.apache.pinot.thirdeye.datalayer.dto.MergedAnomalyResultDTO;\n+import org.apache.pinot.thirdeye.datalayer.dto.MetricConfigDTO;\n+import org.apache.pinot.thirdeye.datalayer.pojo.MergedAnomalyResultBean;\n+import org.apache.pinot.thirdeye.datasource.DAORegistry;\n+import org.apache.pinot.thirdeye.datasource.ThirdEyeCacheRegistry;\n+import org.apache.pinot.thirdeye.datasource.loader.AggregationLoader;\n+import org.apache.pinot.thirdeye.datasource.loader.DefaultAggregationLoader;\n+import org.apache.pinot.thirdeye.datasource.loader.DefaultTimeSeriesLoader;\n+import org.apache.pinot.thirdeye.datasource.loader.TimeSeriesLoader;\n+import org.apache.pinot.thirdeye.detection.ConfigUtils;\n+import org.apache.pinot.thirdeye.detection.DataProvider;\n+import org.apache.pinot.thirdeye.detection.DefaultDataProvider;\n+import org.apache.pinot.thirdeye.detection.DetectionPipelineLoader;\n+import org.apache.pinot.thirdeye.detection.DetectionPipelineTaskInfo;\n+import org.apache.pinot.thirdeye.detection.DetectionUtils;\n+import org.apache.pinot.thirdeye.detection.cache.builder.AnomaliesCacheBuilder;\n+import org.apache.pinot.thirdeye.detection.cache.builder.TimeSeriesCacheBuilder;\n+import org.apache.pinot.thirdeye.rootcause.impl.MetricEntity;\n+import org.joda.time.DateTime;\n+import org.joda.time.DateTimeZone;\n+import org.joda.time.Period;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.pinot.thirdeye.anomaly.utils.ThirdeyeMetricsUtil.*;\n+\n+\n+/**\n+ * Runner that generates Data SLA anomalies. DATA_MISSING anomalies are created if\n+ * the data is not available for the sla detection window within the configured SLA.\n+ */\n+public class DatasetSlaTaskRunner implements TaskRunner {\n+  private static final Logger LOG = LoggerFactory.getLogger(DatasetSlaTaskRunner.class);\n+\n+  private final DetectionConfigManager detectionDAO;\n+  private final MergedAnomalyResultManager anomalyDAO;\n+  private final EvaluationManager evaluationDAO;\n+  private DataProvider provider;\n+\n+  private final String DEFAULT_DATA_SLA = \"3_DAYS\";\n+\n+  public DatasetSlaTaskRunner() {\n+    this.detectionDAO = DAORegistry.getInstance().getDetectionConfigManager();\n+    this.anomalyDAO = DAORegistry.getInstance().getMergedAnomalyResultDAO();\n+    this.evaluationDAO = DAORegistry.getInstance().getEvaluationManager();\n+\n+    MetricConfigManager metricDAO = DAORegistry.getInstance().getMetricConfigDAO();\n+    DatasetConfigManager datasetDAO = DAORegistry.getInstance().getDatasetConfigDAO();\n+    EventManager eventDAO = DAORegistry.getInstance().getEventDAO();\n+\n+    TimeSeriesLoader timeseriesLoader =\n+        new DefaultTimeSeriesLoader(metricDAO, datasetDAO,\n+            ThirdEyeCacheRegistry.getInstance().getQueryCache(), ThirdEyeCacheRegistry.getInstance().getTimeSeriesCache());\n+\n+    AggregationLoader aggregationLoader =\n+        new DefaultAggregationLoader(metricDAO, datasetDAO,\n+            ThirdEyeCacheRegistry.getInstance().getQueryCache(),\n+            ThirdEyeCacheRegistry.getInstance().getDatasetMaxDataTimeCache());\n+\n+    this.provider = new DefaultDataProvider(metricDAO, datasetDAO, eventDAO, this.anomalyDAO, this.evaluationDAO,\n+        timeseriesLoader, aggregationLoader, new DetectionPipelineLoader(), TimeSeriesCacheBuilder.getInstance(),\n+        AnomaliesCacheBuilder.getInstance());\n+  }\n+\n+  public DatasetSlaTaskRunner(DetectionConfigManager detectionDAO, MergedAnomalyResultManager anomalyDAO,\n+      EvaluationManager evaluationDAO, DataProvider provider) {\n+    this.detectionDAO = detectionDAO;\n+    this.anomalyDAO = anomalyDAO;\n+    this.evaluationDAO = evaluationDAO;\n+    this.provider = provider;\n+  }\n+\n+  @Override\n+  public List<TaskResult> execute(TaskInfo taskInfo, TaskContext taskContext) throws Exception {\n+    dataAvailabilityTaskCounter.inc();\n+\n+    try {\n+      DetectionPipelineTaskInfo info = (DetectionPipelineTaskInfo) taskInfo;\n+\n+      DetectionConfigDTO config = this.detectionDAO.findById(info.getConfigId());\n+      if (config == null) {\n+        throw new IllegalArgumentException(String.format(\"Could not resolve config id %d\", info.getConfigId()));\n+      }\n+\n+      LOG.info(\"Check data sla for config {} between {} and {}\", config.getId(), info.getStart(), info.getEnd());\n+      Map<String, Object> metricUrnToSlaMap = config.getDataSLAProperties();\n+      if (MapUtils.isNotEmpty(metricUrnToSlaMap)) {\n+        for (Map.Entry<String, Object> metricUrnToSlaMapEntry : metricUrnToSlaMap.entrySet()) {\n+          MetricEntity me = MetricEntity.fromURN(metricUrnToSlaMapEntry.getKey());\n+          MetricConfigDTO\n+              metricConfigDTO = this.provider.fetchMetrics(Collections.singletonList(me.getId())).get(me.getId());\n+          Map<String, DatasetConfigDTO> datasetToConfigMap = provider.fetchDatasets(Collections.singletonList(metricConfigDTO.getDataset()));\n+          for (Map.Entry<String, DatasetConfigDTO> datasetSLA : datasetToConfigMap.entrySet()) {\n+            try {\n+              runSLACheck(me, datasetSLA.getValue(), info, ConfigUtils.getMap(metricUrnToSlaMapEntry.getValue()));\n+            } catch (Exception e) {\n+              LOG.error(\"Failed to run sla check on metric URN %s\", datasetSLA.getKey(), e);\n+            }\n+          }\n+        }\n+      }\n+\n+      //LOG.info(\"End data availability for config {} between {} and {}. Detected {} anomalies.\", config.getId(),\n+      //    info.getStart(), info.getEnd(), anomaliesList.size());\n+      return Collections.emptyList();\n+    } catch(Exception e) {\n+      throw e;\n+    }\n+  }\n+\n+  /**\n+   * Runs the data sla check for the window (info) on the given metric (me) using the configured sla properties\n+   */\n+  private void runSLACheck(MetricEntity me, DatasetConfigDTO datasetConfig, DetectionPipelineTaskInfo info,\n+      Map<String, Object> slaProps) {\n+    if (me == null || datasetConfig == null || info == null) {\n+      //nothing to check\n+      return;\n+    }\n+\n+    try {\n+      long datasetLastRefreshTime = datasetConfig.getLastRefreshTime();\n+      if (datasetLastRefreshTime <= 0) {\n+        // assume we have processed data till the current detection start\n+        datasetLastRefreshTime = info.getStart() - 1;\n+      }\n+\n+      if (isMissingData(datasetLastRefreshTime, info)) {\n+        // Double check with data source as 2 things are possible.\n+        // 1. This dataset/source may not support availability events\n+        // 2. The data availability event pipeline has some issue.\n+        // We want to measure the overall dataset availability (filters are not taken into account)\n+        MetricSlice metricSlice = MetricSlice.from(me.getId(), info.getStart(), info.getEnd(), ArrayListMultimap.<String, String>create());\n+        DataFrame dataFrame = this.provider.fetchTimeseries(Collections.singleton(metricSlice)).get(metricSlice);\n+        if (dataFrame == null || dataFrame.isEmpty()) {\n+          // no data\n+          if (hasMissedSLA(datasetLastRefreshTime, slaProps, info.getEnd())) {\n+            createDataSLAAnomaly(datasetLastRefreshTime + 1, info.getEnd(), info.getConfigId(), datasetConfig, slaProps);\n+          }\n+        } else {\n+          datasetLastRefreshTime = dataFrame.getDoubles(\"timestamp\").max().longValue();\n+          if (isPartialData(datasetLastRefreshTime, info, datasetConfig)) {\n+            if (hasMissedSLA(datasetLastRefreshTime, slaProps, info.getEnd())) {\n+              createDataSLAAnomaly(datasetLastRefreshTime + 1, info.getEnd(), info.getConfigId(), datasetConfig, slaProps);\n+            }\n+          }\n+        }\n+      } else if (isPartialData(datasetLastRefreshTime, info, datasetConfig)) {\n+        // Optimize for the common case - the common case is that the data availability events are arriving\n+        // correctly and we need not re-fetch the data to double check.\n+        if (hasMissedSLA(datasetLastRefreshTime, slaProps, info.getEnd())) {\n+          createDataSLAAnomaly(datasetLastRefreshTime + 1, info.getEnd(), info.getConfigId(), datasetConfig, slaProps);\n+        }\n+      }\n+    } catch (Exception e) {\n+      LOG.error(String.format(\"Failed to run sla check on metric URN %s\", me.getUrn()), e);\n+    }\n+  }\n+\n+  /**\n+   * We say the data is missing we do not have data in the sla detection window.\n+   * Or more specifically if the dataset watermark is below the sla detection window.\n+   */\n+  private boolean isMissingData(long datasetLastRefreshTime, DetectionPipelineTaskInfo info) {\n+    return datasetLastRefreshTime < info.getStart();\n+  }\n+\n+  /**\n+   * We say the data is partial if we do not have all the data points in the sla detection window.\n+   * Or more specifically if we have at least 1 data-point missing in the sla detection window.\n+   *\n+   * For example:\n+   * Assume that the data is delayed and our current sla detection window is [1st Feb to 3rd Feb). During this scan,\n+   * let's say data for 1st Feb arrives. Now, we have a situation where partial data is present. In other words, in the\n+   * current sla detection window [1st to 3rd Feb) we have data for 1st Feb but data for 2nd Feb is missing. This is the\n+   * partial data scenario.\n+   */\n+  private boolean isPartialData(long datasetLastRefreshTime, DetectionPipelineTaskInfo info, DatasetConfigDTO datasetConfig) {\n+    long granularity = datasetConfig.bucketTimeGranularity().toMillis();\n+    return (info.getEnd() - datasetLastRefreshTime) * 1.0 / granularity > 1;\n+  }\n+\n+  /**\n+   * Validates if the data is delayed or not based on the user specified SLA configuration\n+   */\n+  private boolean hasMissedSLA(long datasetLastRefreshTime, Map<String, Object> slaProps, long slaDetectionEndTime) {\n+    // fetch the user configured SLA, otherwise default 3_DAYS.\n+    long delay = TimeGranularity.fromString(MapUtils.getString(slaProps,  \"sla\", DEFAULT_DATA_SLA))\n+        .toPeriod().toStandardDuration().getMillis();\n+\n+    return (slaDetectionEndTime - datasetLastRefreshTime) >= delay;\n+  }\n+\n+  /**\n+   * Align and round off start time to the upper boundary of the granularity\n+   */\n+  private static long alignToUpperBoundary(long start, DatasetConfigDTO datasetConfig) {\n+    Period granularityPeriod = datasetConfig.bucketTimeGranularity().toPeriod();\n+    DateTimeZone timezone = DateTimeZone.forID(datasetConfig.getTimezone());\n+    DateTime startTime = new DateTime(start - 1, timezone).plus(granularityPeriod);\n+    return startTime.getMillis() / granularityPeriod.toStandardDuration().getMillis() * granularityPeriod.toStandardDuration().getMillis();\n+  }\n+\n+  /**\n+   * Merges one DATA_MISSING anomaly with remaining existing anomalies.\n+   */\n+  private void mergeSLAAnomalies(MergedAnomalyResultDTO anomaly, List<MergedAnomalyResultDTO> existingAnomalies) {\n+    // Extract the parent SLA anomaly. We can have only 1 parent DATA_MISSING anomaly in a window.\n+    existingAnomalies.removeIf(MergedAnomalyResultBean::isChild);\n+    MergedAnomalyResultDTO existingParentSLAAnomaly = existingAnomalies.get(0);\n+\n+    if (isDuplicateSLAAnomaly(existingParentSLAAnomaly, anomaly)) {\n+      // Ensure anomalies are not duplicated. Ignore and return.\n+      // Example: daily data with hourly cron should generate only 1 sla alert if data is missing\n+      return;\n+    }\n+    existingParentSLAAnomaly.setChild(true);\n+    anomaly.setChild(false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "722322b51c1f62f0dd9dde7aadec9fb34574fb40"}, "originalPosition": 261}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkxODIwODY4", "url": "https://github.com/apache/pinot/pull/5200#pullrequestreview-391820868", "createdAt": "2020-04-12T04:36:19Z", "commit": {"oid": "722322b51c1f62f0dd9dde7aadec9fb34574fb40"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1144, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}