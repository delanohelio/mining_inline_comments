{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDc0OTg0Nzgy", "number": 5934, "reviewThreads": {"totalCount": 40, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QyMzoyMzo0M1rOEda8mA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQwMTozMTozMVrOEh6iJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk5Mjg1NjU2OnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentMapper.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QyMzoyMzo0M1rOHIkfDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQxNjo0NToyMFrOHSKuWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc0ODQyOQ==", "bodyText": "Can we interface out the underlying data file format? We may need to plug in other data format for performance boost (for example, @Jackie-Jiang used the mmaped file with custom format for star-tree generator and it's also doing the similar work - sort, aggregation)", "url": "https://github.com/apache/pinot/pull/5934#discussion_r478748429", "createdAt": "2020-08-27T23:23:43Z", "author": {"login": "snleee"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentMapper.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.framework;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.avro.Schema;\n+import org.apache.avro.file.DataFileWriter;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.pinot.core.data.readers.PinotSegmentRecordReader;\n+import org.apache.pinot.core.segment.processing.partitioner.PartitionFilter;\n+import org.apache.pinot.core.segment.processing.partitioner.Partitioner;\n+import org.apache.pinot.core.segment.processing.partitioner.PartitionerFactory;\n+import org.apache.pinot.core.segment.processing.transformer.RecordTransformer;\n+import org.apache.pinot.core.segment.processing.transformer.RecordTransformerFactory;\n+import org.apache.pinot.core.segment.processing.utils.SegmentProcessorUtils;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Mapper phase of the SegmentProcessorFramework.\n+ * Reads the input segment and creates partitioned avro data files\n+ * Performs:\n+ * - record transformations\n+ * - partitioning\n+ * - partition filtering\n+ */\n+public class SegmentMapper {\n+\n+  private static final Logger LOGGER = LoggerFactory.getLogger(SegmentMapper.class);\n+  private final File _inputSegment;\n+  private final File _mapperOutputDir;\n+\n+  private final String _mapperId;\n+  private final Schema _avroSchema;\n+  private final RecordTransformer _recordTransformer;\n+  private final Partitioner _partitioner;\n+  private final PartitionFilter _partitionFilter;\n+  private final Map<String, DataFileWriter<GenericData.Record>> _partitionToDataFileWriterMap = new HashMap<>();\n+\n+  public SegmentMapper(String mapperId, File inputSegment, SegmentMapperConfig mapperConfig, File mapperOutputDir) {\n+    _inputSegment = inputSegment;\n+    _mapperOutputDir = mapperOutputDir;\n+\n+    _mapperId = mapperId;\n+    _avroSchema = SegmentProcessorUtils.convertPinotSchemaToAvroSchema(mapperConfig.getPinotSchema());\n+    _recordTransformer = RecordTransformerFactory.getRecordTransformer(mapperConfig.getRecordTransformerConfig());\n+    _partitioner = PartitionerFactory.getPartitioner(mapperConfig.getPartitioningConfig());\n+    _partitionFilter = PartitionerFactory.getPartitionFilter(mapperConfig.getPartitioningConfig());\n+    LOGGER.info(\n+        \"Initialized mapper with id: {}, input segment: {}, output dir: {}, recordTransformer: {}, partitioner: {}, partitionFilter: {}\",\n+        _mapperId, _inputSegment, _mapperOutputDir, _recordTransformer.getClass(), _partitioner.getClass(),\n+        _partitionFilter.getClass());\n+  }\n+\n+  /**\n+   * Reads the input segment and generates partitioned avro data files into the mapper output directory\n+   * Records for each partition are put into a directory of its own withing the mapper output directory, identified by the partition name\n+   */\n+  public void map()\n+      throws Exception {\n+\n+    PinotSegmentRecordReader segmentRecordReader = new PinotSegmentRecordReader(_inputSegment);\n+    GenericRow reusableRow = new GenericRow();\n+    GenericData.Record reusableRecord = new GenericData.Record(_avroSchema);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODgxMjEyMg==", "bodyText": "Added TODO", "url": "https://github.com/apache/pinot/pull/5934#discussion_r488812122", "createdAt": "2020-09-15T16:45:20Z", "author": {"login": "npawar"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentMapper.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.framework;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.avro.Schema;\n+import org.apache.avro.file.DataFileWriter;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.pinot.core.data.readers.PinotSegmentRecordReader;\n+import org.apache.pinot.core.segment.processing.partitioner.PartitionFilter;\n+import org.apache.pinot.core.segment.processing.partitioner.Partitioner;\n+import org.apache.pinot.core.segment.processing.partitioner.PartitionerFactory;\n+import org.apache.pinot.core.segment.processing.transformer.RecordTransformer;\n+import org.apache.pinot.core.segment.processing.transformer.RecordTransformerFactory;\n+import org.apache.pinot.core.segment.processing.utils.SegmentProcessorUtils;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Mapper phase of the SegmentProcessorFramework.\n+ * Reads the input segment and creates partitioned avro data files\n+ * Performs:\n+ * - record transformations\n+ * - partitioning\n+ * - partition filtering\n+ */\n+public class SegmentMapper {\n+\n+  private static final Logger LOGGER = LoggerFactory.getLogger(SegmentMapper.class);\n+  private final File _inputSegment;\n+  private final File _mapperOutputDir;\n+\n+  private final String _mapperId;\n+  private final Schema _avroSchema;\n+  private final RecordTransformer _recordTransformer;\n+  private final Partitioner _partitioner;\n+  private final PartitionFilter _partitionFilter;\n+  private final Map<String, DataFileWriter<GenericData.Record>> _partitionToDataFileWriterMap = new HashMap<>();\n+\n+  public SegmentMapper(String mapperId, File inputSegment, SegmentMapperConfig mapperConfig, File mapperOutputDir) {\n+    _inputSegment = inputSegment;\n+    _mapperOutputDir = mapperOutputDir;\n+\n+    _mapperId = mapperId;\n+    _avroSchema = SegmentProcessorUtils.convertPinotSchemaToAvroSchema(mapperConfig.getPinotSchema());\n+    _recordTransformer = RecordTransformerFactory.getRecordTransformer(mapperConfig.getRecordTransformerConfig());\n+    _partitioner = PartitionerFactory.getPartitioner(mapperConfig.getPartitioningConfig());\n+    _partitionFilter = PartitionerFactory.getPartitionFilter(mapperConfig.getPartitioningConfig());\n+    LOGGER.info(\n+        \"Initialized mapper with id: {}, input segment: {}, output dir: {}, recordTransformer: {}, partitioner: {}, partitionFilter: {}\",\n+        _mapperId, _inputSegment, _mapperOutputDir, _recordTransformer.getClass(), _partitioner.getClass(),\n+        _partitionFilter.getClass());\n+  }\n+\n+  /**\n+   * Reads the input segment and generates partitioned avro data files into the mapper output directory\n+   * Records for each partition are put into a directory of its own withing the mapper output directory, identified by the partition name\n+   */\n+  public void map()\n+      throws Exception {\n+\n+    PinotSegmentRecordReader segmentRecordReader = new PinotSegmentRecordReader(_inputSegment);\n+    GenericRow reusableRow = new GenericRow();\n+    GenericData.Record reusableRecord = new GenericData.Record(_avroSchema);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc0ODQyOQ=="}, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 90}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk5MjkxMDc4OnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/transformer/TransformFunctionRecordTransformer.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QyMzo1MjoxN1rOHIk_JQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxNzoyMDoxN1rOHJOihg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc1NjY0NQ==", "bodyText": "What if I want to run the custom transformation function?\nThe spec doesn't have a way to specify the custom transformer?\n  \"recordTransformerConfig\": {\n    \"transformFunctionsMap\": {\n      \"epochMillis\": \"round(epochMillis, 86400000)\" // round to nearest day\n    }\n  },", "url": "https://github.com/apache/pinot/pull/5934#discussion_r478756645", "createdAt": "2020-08-27T23:52:17Z", "author": {"login": "snleee"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/transformer/TransformFunctionRecordTransformer.java", "diffHunk": "@@ -0,0 +1,49 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.transformer;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.pinot.core.data.function.FunctionEvaluator;\n+import org.apache.pinot.core.data.function.FunctionEvaluatorFactory;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * RecordTransformer which executes transform functions to transform columns of record\n+ * Does not follow any particular order, and hence cannot support transformations where strict order of execution is needed\n+ */\n+public class TransformFunctionRecordTransformer implements RecordTransformer {\n+\n+  private final Map<String, FunctionEvaluator> _functionEvaluatorMap = new HashMap<>();\n+\n+  public TransformFunctionRecordTransformer(Map<String, String> transformFunctionMap) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQzNzQ0Ng==", "bodyText": "Several options:\n\nUse Groovy function to write any logic\nAdd the function with @ScalarFunction annotation and put the jar in plugins dir\nIntroduce new RecordTransformer implementation, introduce RecordTransformerType and change factory logic\n\nI did not do the option 3 right now, because I think 1 and 2 should suffice most times. If we see the demand for 3, we can add it.\nWill put a TODO", "url": "https://github.com/apache/pinot/pull/5934#discussion_r479437446", "createdAt": "2020-08-28T17:20:17Z", "author": {"login": "npawar"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/transformer/TransformFunctionRecordTransformer.java", "diffHunk": "@@ -0,0 +1,49 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.transformer;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.pinot.core.data.function.FunctionEvaluator;\n+import org.apache.pinot.core.data.function.FunctionEvaluatorFactory;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * RecordTransformer which executes transform functions to transform columns of record\n+ * Does not follow any particular order, and hence cannot support transformations where strict order of execution is needed\n+ */\n+public class TransformFunctionRecordTransformer implements RecordTransformer {\n+\n+  private final Map<String, FunctionEvaluator> _functionEvaluatorMap = new HashMap<>();\n+\n+  public TransformFunctionRecordTransformer(Map<String, String> transformFunctionMap) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc1NjY0NQ=="}, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk5MjkxNzkyOnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentMapper.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QyMzo1NjoxM1rOHIlDVg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQxNzo1MTozMFrOHLB0Cw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc1NzcxOA==", "bodyText": "How do we handle record filtering that happens in the transformRecord?\nOr, are we model filtering as a separate step? We should have some convention on record filtering.", "url": "https://github.com/apache/pinot/pull/5934#discussion_r478757718", "createdAt": "2020-08-27T23:56:13Z", "author": {"login": "snleee"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentMapper.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.framework;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.avro.Schema;\n+import org.apache.avro.file.DataFileWriter;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.pinot.core.data.readers.PinotSegmentRecordReader;\n+import org.apache.pinot.core.segment.processing.partitioner.PartitionFilter;\n+import org.apache.pinot.core.segment.processing.partitioner.Partitioner;\n+import org.apache.pinot.core.segment.processing.partitioner.PartitionerFactory;\n+import org.apache.pinot.core.segment.processing.transformer.RecordTransformer;\n+import org.apache.pinot.core.segment.processing.transformer.RecordTransformerFactory;\n+import org.apache.pinot.core.segment.processing.utils.SegmentProcessorUtils;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Mapper phase of the SegmentProcessorFramework.\n+ * Reads the input segment and creates partitioned avro data files\n+ * Performs:\n+ * - record transformations\n+ * - partitioning\n+ * - partition filtering\n+ */\n+public class SegmentMapper {\n+\n+  private static final Logger LOGGER = LoggerFactory.getLogger(SegmentMapper.class);\n+  private final File _inputSegment;\n+  private final File _mapperOutputDir;\n+\n+  private final String _mapperId;\n+  private final Schema _avroSchema;\n+  private final RecordTransformer _recordTransformer;\n+  private final Partitioner _partitioner;\n+  private final PartitionFilter _partitionFilter;\n+  private final Map<String, DataFileWriter<GenericData.Record>> _partitionToDataFileWriterMap = new HashMap<>();\n+\n+  public SegmentMapper(String mapperId, File inputSegment, SegmentMapperConfig mapperConfig, File mapperOutputDir) {\n+    _inputSegment = inputSegment;\n+    _mapperOutputDir = mapperOutputDir;\n+\n+    _mapperId = mapperId;\n+    _avroSchema = SegmentProcessorUtils.convertPinotSchemaToAvroSchema(mapperConfig.getPinotSchema());\n+    _recordTransformer = RecordTransformerFactory.getRecordTransformer(mapperConfig.getRecordTransformerConfig());\n+    _partitioner = PartitionerFactory.getPartitioner(mapperConfig.getPartitioningConfig());\n+    _partitionFilter = PartitionerFactory.getPartitionFilter(mapperConfig.getPartitioningConfig());\n+    LOGGER.info(\n+        \"Initialized mapper with id: {}, input segment: {}, output dir: {}, recordTransformer: {}, partitioner: {}, partitionFilter: {}\",\n+        _mapperId, _inputSegment, _mapperOutputDir, _recordTransformer.getClass(), _partitioner.getClass(),\n+        _partitionFilter.getClass());\n+  }\n+\n+  /**\n+   * Reads the input segment and generates partitioned avro data files into the mapper output directory\n+   * Records for each partition are put into a directory of its own withing the mapper output directory, identified by the partition name\n+   */\n+  public void map()\n+      throws Exception {\n+\n+    PinotSegmentRecordReader segmentRecordReader = new PinotSegmentRecordReader(_inputSegment);\n+    GenericRow reusableRow = new GenericRow();\n+    GenericData.Record reusableRecord = new GenericData.Record(_avroSchema);\n+\n+    Set<String> selectedPartitions = new HashSet<>();\n+    Set<String> rejectedPartitions = new HashSet<>();\n+\n+    while (segmentRecordReader.hasNext()) {\n+      reusableRow = segmentRecordReader.next(reusableRow);\n+\n+      // Record transformation\n+      reusableRow = _recordTransformer.transformRecord(reusableRow);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQzOTExMA==", "bodyText": "Currently no record filtering happens in transformRecord. If we want to do record filtering in Mapper, I think we can introduce a new phase, then it would become  transform -> filter -> partition -> partitionFilter -> flush\nMy goal in this phase was to mainly include everything that the current code (of segment merge/rollup) does, and also include everything that the realtime to offline project needs. Filtering wasn't required in either, so skipped it", "url": "https://github.com/apache/pinot/pull/5934#discussion_r479439110", "createdAt": "2020-08-28T17:23:39Z", "author": {"login": "npawar"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentMapper.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.framework;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.avro.Schema;\n+import org.apache.avro.file.DataFileWriter;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.pinot.core.data.readers.PinotSegmentRecordReader;\n+import org.apache.pinot.core.segment.processing.partitioner.PartitionFilter;\n+import org.apache.pinot.core.segment.processing.partitioner.Partitioner;\n+import org.apache.pinot.core.segment.processing.partitioner.PartitionerFactory;\n+import org.apache.pinot.core.segment.processing.transformer.RecordTransformer;\n+import org.apache.pinot.core.segment.processing.transformer.RecordTransformerFactory;\n+import org.apache.pinot.core.segment.processing.utils.SegmentProcessorUtils;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Mapper phase of the SegmentProcessorFramework.\n+ * Reads the input segment and creates partitioned avro data files\n+ * Performs:\n+ * - record transformations\n+ * - partitioning\n+ * - partition filtering\n+ */\n+public class SegmentMapper {\n+\n+  private static final Logger LOGGER = LoggerFactory.getLogger(SegmentMapper.class);\n+  private final File _inputSegment;\n+  private final File _mapperOutputDir;\n+\n+  private final String _mapperId;\n+  private final Schema _avroSchema;\n+  private final RecordTransformer _recordTransformer;\n+  private final Partitioner _partitioner;\n+  private final PartitionFilter _partitionFilter;\n+  private final Map<String, DataFileWriter<GenericData.Record>> _partitionToDataFileWriterMap = new HashMap<>();\n+\n+  public SegmentMapper(String mapperId, File inputSegment, SegmentMapperConfig mapperConfig, File mapperOutputDir) {\n+    _inputSegment = inputSegment;\n+    _mapperOutputDir = mapperOutputDir;\n+\n+    _mapperId = mapperId;\n+    _avroSchema = SegmentProcessorUtils.convertPinotSchemaToAvroSchema(mapperConfig.getPinotSchema());\n+    _recordTransformer = RecordTransformerFactory.getRecordTransformer(mapperConfig.getRecordTransformerConfig());\n+    _partitioner = PartitionerFactory.getPartitioner(mapperConfig.getPartitioningConfig());\n+    _partitionFilter = PartitionerFactory.getPartitionFilter(mapperConfig.getPartitioningConfig());\n+    LOGGER.info(\n+        \"Initialized mapper with id: {}, input segment: {}, output dir: {}, recordTransformer: {}, partitioner: {}, partitionFilter: {}\",\n+        _mapperId, _inputSegment, _mapperOutputDir, _recordTransformer.getClass(), _partitioner.getClass(),\n+        _partitionFilter.getClass());\n+  }\n+\n+  /**\n+   * Reads the input segment and generates partitioned avro data files into the mapper output directory\n+   * Records for each partition are put into a directory of its own withing the mapper output directory, identified by the partition name\n+   */\n+  public void map()\n+      throws Exception {\n+\n+    PinotSegmentRecordReader segmentRecordReader = new PinotSegmentRecordReader(_inputSegment);\n+    GenericRow reusableRow = new GenericRow();\n+    GenericData.Record reusableRecord = new GenericData.Record(_avroSchema);\n+\n+    Set<String> selectedPartitions = new HashSet<>();\n+    Set<String> rejectedPartitions = new HashSet<>();\n+\n+    while (segmentRecordReader.hasNext()) {\n+      reusableRow = segmentRecordReader.next(reusableRow);\n+\n+      // Record transformation\n+      reusableRow = _recordTransformer.transformRecord(reusableRow);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc1NzcxOA=="}, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTMyNjA5MQ==", "bodyText": "I have removed PartitionFilter step and introduced record filtering.", "url": "https://github.com/apache/pinot/pull/5934#discussion_r481326091", "createdAt": "2020-09-01T17:51:30Z", "author": {"login": "npawar"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentMapper.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.framework;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.avro.Schema;\n+import org.apache.avro.file.DataFileWriter;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.pinot.core.data.readers.PinotSegmentRecordReader;\n+import org.apache.pinot.core.segment.processing.partitioner.PartitionFilter;\n+import org.apache.pinot.core.segment.processing.partitioner.Partitioner;\n+import org.apache.pinot.core.segment.processing.partitioner.PartitionerFactory;\n+import org.apache.pinot.core.segment.processing.transformer.RecordTransformer;\n+import org.apache.pinot.core.segment.processing.transformer.RecordTransformerFactory;\n+import org.apache.pinot.core.segment.processing.utils.SegmentProcessorUtils;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Mapper phase of the SegmentProcessorFramework.\n+ * Reads the input segment and creates partitioned avro data files\n+ * Performs:\n+ * - record transformations\n+ * - partitioning\n+ * - partition filtering\n+ */\n+public class SegmentMapper {\n+\n+  private static final Logger LOGGER = LoggerFactory.getLogger(SegmentMapper.class);\n+  private final File _inputSegment;\n+  private final File _mapperOutputDir;\n+\n+  private final String _mapperId;\n+  private final Schema _avroSchema;\n+  private final RecordTransformer _recordTransformer;\n+  private final Partitioner _partitioner;\n+  private final PartitionFilter _partitionFilter;\n+  private final Map<String, DataFileWriter<GenericData.Record>> _partitionToDataFileWriterMap = new HashMap<>();\n+\n+  public SegmentMapper(String mapperId, File inputSegment, SegmentMapperConfig mapperConfig, File mapperOutputDir) {\n+    _inputSegment = inputSegment;\n+    _mapperOutputDir = mapperOutputDir;\n+\n+    _mapperId = mapperId;\n+    _avroSchema = SegmentProcessorUtils.convertPinotSchemaToAvroSchema(mapperConfig.getPinotSchema());\n+    _recordTransformer = RecordTransformerFactory.getRecordTransformer(mapperConfig.getRecordTransformerConfig());\n+    _partitioner = PartitionerFactory.getPartitioner(mapperConfig.getPartitioningConfig());\n+    _partitionFilter = PartitionerFactory.getPartitionFilter(mapperConfig.getPartitioningConfig());\n+    LOGGER.info(\n+        \"Initialized mapper with id: {}, input segment: {}, output dir: {}, recordTransformer: {}, partitioner: {}, partitionFilter: {}\",\n+        _mapperId, _inputSegment, _mapperOutputDir, _recordTransformer.getClass(), _partitioner.getClass(),\n+        _partitionFilter.getClass());\n+  }\n+\n+  /**\n+   * Reads the input segment and generates partitioned avro data files into the mapper output directory\n+   * Records for each partition are put into a directory of its own withing the mapper output directory, identified by the partition name\n+   */\n+  public void map()\n+      throws Exception {\n+\n+    PinotSegmentRecordReader segmentRecordReader = new PinotSegmentRecordReader(_inputSegment);\n+    GenericRow reusableRow = new GenericRow();\n+    GenericData.Record reusableRecord = new GenericData.Record(_avroSchema);\n+\n+    Set<String> selectedPartitions = new HashSet<>();\n+    Set<String> rejectedPartitions = new HashSet<>();\n+\n+    while (segmentRecordReader.hasNext()) {\n+      reusableRow = segmentRecordReader.next(reusableRow);\n+\n+      // Record transformation\n+      reusableRow = _recordTransformer.transformRecord(reusableRow);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc1NzcxOA=="}, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 99}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAwNDM4ODg1OnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/data/function/FunctionEvaluator.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMVQyMzozMzoxM1rOHKNvOw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMVQyMzozMzoxM1rOHKNvOw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ3Mjg5MQ==", "bodyText": "The arguments should be a key-value pair instead of an array. How are you going to map the values to the function parameters?", "url": "https://github.com/apache/pinot/pull/5934#discussion_r480472891", "createdAt": "2020-08-31T23:33:13Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/data/function/FunctionEvaluator.java", "diffHunk": "@@ -36,4 +36,9 @@\n    * Evaluate the function on the generic row and return the result\n    */\n   Object evaluate(GenericRow genericRow);\n+\n+  /**\n+   * Evaluate the function on the given arguments\n+   */\n+  Object evaluate(Object[] arguments);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAwNDM5NDQ0OnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/data/function/InbuiltFunctionEvaluator.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMVQyMzozNjowMFrOHKNykw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMVQyMzozNjowMFrOHKNykw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ3Mzc0Nw==", "bodyText": "This API won't work for in-build functions because it needs to read column values in order to evaluate the function", "url": "https://github.com/apache/pinot/pull/5934#discussion_r480473747", "createdAt": "2020-08-31T23:36:00Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/data/function/InbuiltFunctionEvaluator.java", "diffHunk": "@@ -155,5 +177,10 @@ public String execute(GenericRow row) {\n     public Object execute(GenericRow row) {\n       return row.getValue(_column);\n     }\n+\n+    @Override\n+    public Object execute(Object[] arguments) {\n+      throw new UnsupportedOperationException(\"Operation not supported for ColumnExecutionNode\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAwNDQxNDY1OnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/partitioner/PartitionFilter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMVQyMzo0NjoyMFrOHKN-Rw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQwMDozNDoxM1rOHLM0hw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ3Njc0Mw==", "bodyText": "Why associating the filter with the partition? We can apply the filter to the records, then you can directly use the current FunctionEvaluator\nIMO, record filtering is easier to config and more intuitive. I cannot think of a use case where we have to apply the filter to the partition", "url": "https://github.com/apache/pinot/pull/5934#discussion_r480476743", "createdAt": "2020-08-31T23:46:20Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/partitioner/PartitionFilter.java", "diffHunk": "@@ -0,0 +1,30 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.partitioner;\n+\n+/**\n+ * Used for filtering partitions in the mapper\n+ */\n+public interface PartitionFilter {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTUwNjQzOQ==", "bodyText": "true. Remove partition filtering. Added Record filtering. All the corresponding changes in FunctionEvaluator have been reverted.", "url": "https://github.com/apache/pinot/pull/5934#discussion_r481506439", "createdAt": "2020-09-02T00:34:13Z", "author": {"login": "npawar"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/partitioner/PartitionFilter.java", "diffHunk": "@@ -0,0 +1,30 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.partitioner;\n+\n+/**\n+ * Used for filtering partitions in the mapper\n+ */\n+public interface PartitionFilter {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ3Njc0Mw=="}, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAwNDQ3OTY5OnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/partitioner/PartitionerFactory.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwMDoxMzoyOFrOHKOlpA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNFQyMzowMDo0NFrOHNdXsg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ4NjgyMA==", "bodyText": "I feel ROW_HASH is not really useful. Maybe ROUND_ROBIN is enough?", "url": "https://github.com/apache/pinot/pull/5934#discussion_r480486820", "createdAt": "2020-09-01T00:13:28Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/partitioner/PartitionerFactory.java", "diffHunk": "@@ -0,0 +1,82 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.partitioner;\n+\n+import com.google.common.base.Preconditions;\n+\n+\n+/**\n+ * Factory for Partitioner and PartitionFilter\n+ */\n+public final class PartitionerFactory {\n+\n+  private PartitionerFactory() {\n+\n+  }\n+\n+  public enum PartitionerType {\n+    NO_OP, ROW_HASH, COLUMN_VALUE, TRANSFORM_FUNCTION, TABLE_PARTITION_CONFIG\n+  }\n+\n+  /**\n+   * Construct a Partitioner using the PartitioningConfig\n+   */\n+  public static Partitioner getPartitioner(PartitioningConfig config) {\n+\n+    Partitioner partitioner = null;\n+    switch (config.getPartitionerType()) {\n+      case NO_OP:\n+        partitioner = new NoOpPartitioner();\n+        break;\n+      case ROW_HASH:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTUxNzYwMg==", "bodyText": "The intention of ROW_HASH is to have a partitioner which lets you set number of partitions. The existing SegmentConverter supports only this type of partitioning. Agree that ROW_HASH is not very indicative of the intent.\nHow about NUM_PARTITIONS or FIXED_NUM_PARTITIONS", "url": "https://github.com/apache/pinot/pull/5934#discussion_r481517602", "createdAt": "2020-09-02T01:00:01Z", "author": {"login": "npawar"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/partitioner/PartitionerFactory.java", "diffHunk": "@@ -0,0 +1,82 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.partitioner;\n+\n+import com.google.common.base.Preconditions;\n+\n+\n+/**\n+ * Factory for Partitioner and PartitionFilter\n+ */\n+public final class PartitionerFactory {\n+\n+  private PartitionerFactory() {\n+\n+  }\n+\n+  public enum PartitionerType {\n+    NO_OP, ROW_HASH, COLUMN_VALUE, TRANSFORM_FUNCTION, TABLE_PARTITION_CONFIG\n+  }\n+\n+  /**\n+   * Construct a Partitioner using the PartitioningConfig\n+   */\n+  public static Partitioner getPartitioner(PartitioningConfig config) {\n+\n+    Partitioner partitioner = null;\n+    switch (config.getPartitionerType()) {\n+      case NO_OP:\n+        partitioner = new NoOpPartitioner();\n+        break;\n+      case ROW_HASH:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ4NjgyMA=="}, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMyNTI1MA==", "bodyText": "I understand the intention for this partitioner. My suggestion is to not use the hashcode of the record, but simply do round robin on the configured _numPartitions which guarantees the records for each partition are balanced. This is also much cheaper comparing to the hashcode based partitioner.", "url": "https://github.com/apache/pinot/pull/5934#discussion_r483325250", "createdAt": "2020-09-04T00:44:11Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/partitioner/PartitionerFactory.java", "diffHunk": "@@ -0,0 +1,82 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.partitioner;\n+\n+import com.google.common.base.Preconditions;\n+\n+\n+/**\n+ * Factory for Partitioner and PartitionFilter\n+ */\n+public final class PartitionerFactory {\n+\n+  private PartitionerFactory() {\n+\n+  }\n+\n+  public enum PartitionerType {\n+    NO_OP, ROW_HASH, COLUMN_VALUE, TRANSFORM_FUNCTION, TABLE_PARTITION_CONFIG\n+  }\n+\n+  /**\n+   * Construct a Partitioner using the PartitioningConfig\n+   */\n+  public static Partitioner getPartitioner(PartitioningConfig config) {\n+\n+    Partitioner partitioner = null;\n+    switch (config.getPartitionerType()) {\n+      case NO_OP:\n+        partitioner = new NoOpPartitioner();\n+        break;\n+      case ROW_HASH:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ4NjgyMA=="}, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mzg3NDczOA==", "bodyText": "got it. Done", "url": "https://github.com/apache/pinot/pull/5934#discussion_r483874738", "createdAt": "2020-09-04T23:00:44Z", "author": {"login": "npawar"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/partitioner/PartitionerFactory.java", "diffHunk": "@@ -0,0 +1,82 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.partitioner;\n+\n+import com.google.common.base.Preconditions;\n+\n+\n+/**\n+ * Factory for Partitioner and PartitionFilter\n+ */\n+public final class PartitionerFactory {\n+\n+  private PartitionerFactory() {\n+\n+  }\n+\n+  public enum PartitionerType {\n+    NO_OP, ROW_HASH, COLUMN_VALUE, TRANSFORM_FUNCTION, TABLE_PARTITION_CONFIG\n+  }\n+\n+  /**\n+   * Construct a Partitioner using the PartitioningConfig\n+   */\n+  public static Partitioner getPartitioner(PartitioningConfig config) {\n+\n+    Partitioner partitioner = null;\n+    switch (config.getPartitionerType()) {\n+      case NO_OP:\n+        partitioner = new NoOpPartitioner();\n+        break;\n+      case ROW_HASH:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ4NjgyMA=="}, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAwNDQ5ODk3OnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/CollectorConfig.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwMDoxODoxNFrOHKOxtg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQwMDo0MDoxOFrOHLM6jw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ4OTkxMA==", "bodyText": "It should also include sorted columns (list of columns, where records are sorted on the first column firstly, then second column etc.)", "url": "https://github.com/apache/pinot/pull/5934#discussion_r480489910", "createdAt": "2020-09-01T00:18:14Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/CollectorConfig.java", "diffHunk": "@@ -0,0 +1,85 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import com.fasterxml.jackson.databind.annotation.JsonDeserialize;\n+import com.fasterxml.jackson.databind.annotation.JsonPOJOBuilder;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+\n+\n+/**\n+ * Config for Collector\n+ */\n+@JsonDeserialize(builder = CollectorConfig.Builder.class)\n+public class CollectorConfig {\n+  private static final CollectorFactory.CollectorType DEFAULT_COLLECTOR_TYPE = CollectorFactory.CollectorType.CONCAT;\n+\n+  private final CollectorFactory.CollectorType _collectorType;\n+  private final Map<String, ValueAggregatorFactory.ValueAggregatorType> _aggregatorTypeMap;\n+\n+  private CollectorConfig(CollectorFactory.CollectorType collectorType,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTUwNzk4Mw==", "bodyText": "Added the config and support for sorting in the collection.\nKept the implementation very simple right now. Since we already have all records in memory due to aggregation step, simply called list.sort.\nLmk if you think any optimizations are needed there", "url": "https://github.com/apache/pinot/pull/5934#discussion_r481507983", "createdAt": "2020-09-02T00:40:18Z", "author": {"login": "npawar"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/CollectorConfig.java", "diffHunk": "@@ -0,0 +1,85 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import com.fasterxml.jackson.databind.annotation.JsonDeserialize;\n+import com.fasterxml.jackson.databind.annotation.JsonPOJOBuilder;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+\n+\n+/**\n+ * Config for Collector\n+ */\n+@JsonDeserialize(builder = CollectorConfig.Builder.class)\n+public class CollectorConfig {\n+  private static final CollectorFactory.CollectorType DEFAULT_COLLECTOR_TYPE = CollectorFactory.CollectorType.CONCAT;\n+\n+  private final CollectorFactory.CollectorType _collectorType;\n+  private final Map<String, ValueAggregatorFactory.ValueAggregatorType> _aggregatorTypeMap;\n+\n+  private CollectorConfig(CollectorFactory.CollectorType collectorType,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ4OTkxMA=="}, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAwNDUyOTk2OnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentReducer.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwMDoyNToyOVrOHKPEqA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNFQyMzowNzozNVrOHNdcsw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ5NDc2MA==", "bodyText": "Should we use _numRecordsPerPart here so that once the collector collects enough records, we flush them?", "url": "https://github.com/apache/pinot/pull/5934#discussion_r480494760", "createdAt": "2020-09-01T00:25:29Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentReducer.java", "diffHunk": "@@ -0,0 +1,151 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.framework;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import org.apache.avro.file.DataFileWriter;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.pinot.core.segment.processing.collector.Collector;\n+import org.apache.pinot.core.segment.processing.collector.CollectorFactory;\n+import org.apache.pinot.core.segment.processing.utils.SegmentProcessorUtils;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+import org.apache.pinot.spi.data.readers.RecordReader;\n+import org.apache.pinot.spi.data.readers.RecordReaderFactory;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Reducer phase of the SegmentProcessorFramework\n+ * Reads the avro files in the input directory and creates output avro files in the reducer output directory.\n+ * The avro files in the input directory are expected to contain data for only 1 partition\n+ * Performs operations on that partition data as follows:\n+ * - concatenation/rollup of records\n+ * - split\n+ * - TODO: dedup\n+ */\n+public class SegmentReducer {\n+\n+  private static final Logger LOGGER = LoggerFactory.getLogger(SegmentReducer.class);\n+  private static final int MAX_RECORDS_TO_COLLECT = 5_000_000;\n+\n+  private final File _reducerInputDir;\n+  private final File _reducerOutputDir;\n+\n+  private final String _reducerId;\n+  private final Schema _pinotSchema;\n+  private final org.apache.avro.Schema _avroSchema;\n+  private final Collector _collector;\n+  private final int _numRecordsPerPart;\n+\n+  public SegmentReducer(String reducerId, File reducerInputDir, SegmentReducerConfig reducerConfig,\n+      File reducerOutputDir) {\n+    _reducerInputDir = reducerInputDir;\n+    _reducerOutputDir = reducerOutputDir;\n+\n+    _reducerId = reducerId;\n+    _pinotSchema = reducerConfig.getPinotSchema();\n+    _avroSchema = SegmentProcessorUtils.convertPinotSchemaToAvroSchema(_pinotSchema);\n+    _collector = CollectorFactory.getCollector(reducerConfig.getCollectorConfig(), _pinotSchema);\n+    _numRecordsPerPart = reducerConfig.getNumRecordsPerPart();\n+    LOGGER.info(\"Initialized reducer with id: {}, input dir: {}, output dir: {}, collector: {}, numRecordsPerPart: {}\",\n+        _reducerId, _reducerInputDir, _reducerOutputDir, _collector.getClass(), _numRecordsPerPart);\n+  }\n+\n+  /**\n+   * Reads the avro files in the input directory.\n+   * Performs configured operations and outputs to other avro file(s) in the reducer output directory.\n+   */\n+  public void reduce()\n+      throws Exception {\n+\n+    int part = 0;\n+    for (File inputFile : _reducerInputDir.listFiles()) {\n+\n+      RecordReader avroRecordReader = RecordReaderFactory\n+          .getRecordReaderByClass(\"org.apache.pinot.plugin.inputformat.avro.AvroRecordReader\", inputFile,\n+              _pinotSchema.getColumnNames(), null);\n+\n+      while (avroRecordReader.hasNext()) {\n+        GenericRow next = avroRecordReader.next();\n+\n+        // Aggregations\n+        _collector.collect(next);\n+\n+        // Exceeded max records allowed to collect. Flush\n+        if (_collector.size() == MAX_RECORDS_TO_COLLECT) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTMwNzA3NQ==", "bodyText": "Instead of checking the number of records, can we have a method in the collector that says it is full? The criteria could then be something else depending on the collector.", "url": "https://github.com/apache/pinot/pull/5934#discussion_r481307075", "createdAt": "2020-09-01T17:18:02Z", "author": {"login": "mcvsubbu"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentReducer.java", "diffHunk": "@@ -0,0 +1,151 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.framework;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import org.apache.avro.file.DataFileWriter;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.pinot.core.segment.processing.collector.Collector;\n+import org.apache.pinot.core.segment.processing.collector.CollectorFactory;\n+import org.apache.pinot.core.segment.processing.utils.SegmentProcessorUtils;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+import org.apache.pinot.spi.data.readers.RecordReader;\n+import org.apache.pinot.spi.data.readers.RecordReaderFactory;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Reducer phase of the SegmentProcessorFramework\n+ * Reads the avro files in the input directory and creates output avro files in the reducer output directory.\n+ * The avro files in the input directory are expected to contain data for only 1 partition\n+ * Performs operations on that partition data as follows:\n+ * - concatenation/rollup of records\n+ * - split\n+ * - TODO: dedup\n+ */\n+public class SegmentReducer {\n+\n+  private static final Logger LOGGER = LoggerFactory.getLogger(SegmentReducer.class);\n+  private static final int MAX_RECORDS_TO_COLLECT = 5_000_000;\n+\n+  private final File _reducerInputDir;\n+  private final File _reducerOutputDir;\n+\n+  private final String _reducerId;\n+  private final Schema _pinotSchema;\n+  private final org.apache.avro.Schema _avroSchema;\n+  private final Collector _collector;\n+  private final int _numRecordsPerPart;\n+\n+  public SegmentReducer(String reducerId, File reducerInputDir, SegmentReducerConfig reducerConfig,\n+      File reducerOutputDir) {\n+    _reducerInputDir = reducerInputDir;\n+    _reducerOutputDir = reducerOutputDir;\n+\n+    _reducerId = reducerId;\n+    _pinotSchema = reducerConfig.getPinotSchema();\n+    _avroSchema = SegmentProcessorUtils.convertPinotSchemaToAvroSchema(_pinotSchema);\n+    _collector = CollectorFactory.getCollector(reducerConfig.getCollectorConfig(), _pinotSchema);\n+    _numRecordsPerPart = reducerConfig.getNumRecordsPerPart();\n+    LOGGER.info(\"Initialized reducer with id: {}, input dir: {}, output dir: {}, collector: {}, numRecordsPerPart: {}\",\n+        _reducerId, _reducerInputDir, _reducerOutputDir, _collector.getClass(), _numRecordsPerPart);\n+  }\n+\n+  /**\n+   * Reads the avro files in the input directory.\n+   * Performs configured operations and outputs to other avro file(s) in the reducer output directory.\n+   */\n+  public void reduce()\n+      throws Exception {\n+\n+    int part = 0;\n+    for (File inputFile : _reducerInputDir.listFiles()) {\n+\n+      RecordReader avroRecordReader = RecordReaderFactory\n+          .getRecordReaderByClass(\"org.apache.pinot.plugin.inputformat.avro.AvroRecordReader\", inputFile,\n+              _pinotSchema.getColumnNames(), null);\n+\n+      while (avroRecordReader.hasNext()) {\n+        GenericRow next = avroRecordReader.next();\n+\n+        // Aggregations\n+        _collector.collect(next);\n+\n+        // Exceeded max records allowed to collect. Flush\n+        if (_collector.size() == MAX_RECORDS_TO_COLLECT) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ5NDc2MA=="}, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTUxOTk3Nw==", "bodyText": "Should we use _numRecordsPerPart here so that once the collector collects enough records, we flush them - I had initially done it this way. But then I noticed while testing, that if user sets a very low numRecordsPerPart, we will aggregate less and flush very frequently. Having a bigger MAX_RECORDS_TO_COLLECT allows for more aggregation", "url": "https://github.com/apache/pinot/pull/5934#discussion_r481519977", "createdAt": "2020-09-02T01:09:23Z", "author": {"login": "npawar"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentReducer.java", "diffHunk": "@@ -0,0 +1,151 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.framework;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import org.apache.avro.file.DataFileWriter;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.pinot.core.segment.processing.collector.Collector;\n+import org.apache.pinot.core.segment.processing.collector.CollectorFactory;\n+import org.apache.pinot.core.segment.processing.utils.SegmentProcessorUtils;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+import org.apache.pinot.spi.data.readers.RecordReader;\n+import org.apache.pinot.spi.data.readers.RecordReaderFactory;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Reducer phase of the SegmentProcessorFramework\n+ * Reads the avro files in the input directory and creates output avro files in the reducer output directory.\n+ * The avro files in the input directory are expected to contain data for only 1 partition\n+ * Performs operations on that partition data as follows:\n+ * - concatenation/rollup of records\n+ * - split\n+ * - TODO: dedup\n+ */\n+public class SegmentReducer {\n+\n+  private static final Logger LOGGER = LoggerFactory.getLogger(SegmentReducer.class);\n+  private static final int MAX_RECORDS_TO_COLLECT = 5_000_000;\n+\n+  private final File _reducerInputDir;\n+  private final File _reducerOutputDir;\n+\n+  private final String _reducerId;\n+  private final Schema _pinotSchema;\n+  private final org.apache.avro.Schema _avroSchema;\n+  private final Collector _collector;\n+  private final int _numRecordsPerPart;\n+\n+  public SegmentReducer(String reducerId, File reducerInputDir, SegmentReducerConfig reducerConfig,\n+      File reducerOutputDir) {\n+    _reducerInputDir = reducerInputDir;\n+    _reducerOutputDir = reducerOutputDir;\n+\n+    _reducerId = reducerId;\n+    _pinotSchema = reducerConfig.getPinotSchema();\n+    _avroSchema = SegmentProcessorUtils.convertPinotSchemaToAvroSchema(_pinotSchema);\n+    _collector = CollectorFactory.getCollector(reducerConfig.getCollectorConfig(), _pinotSchema);\n+    _numRecordsPerPart = reducerConfig.getNumRecordsPerPart();\n+    LOGGER.info(\"Initialized reducer with id: {}, input dir: {}, output dir: {}, collector: {}, numRecordsPerPart: {}\",\n+        _reducerId, _reducerInputDir, _reducerOutputDir, _collector.getClass(), _numRecordsPerPart);\n+  }\n+\n+  /**\n+   * Reads the avro files in the input directory.\n+   * Performs configured operations and outputs to other avro file(s) in the reducer output directory.\n+   */\n+  public void reduce()\n+      throws Exception {\n+\n+    int part = 0;\n+    for (File inputFile : _reducerInputDir.listFiles()) {\n+\n+      RecordReader avroRecordReader = RecordReaderFactory\n+          .getRecordReaderByClass(\"org.apache.pinot.plugin.inputformat.avro.AvroRecordReader\", inputFile,\n+              _pinotSchema.getColumnNames(), null);\n+\n+      while (avroRecordReader.hasNext()) {\n+        GenericRow next = avroRecordReader.next();\n+\n+        // Aggregations\n+        _collector.collect(next);\n+\n+        // Exceeded max records allowed to collect. Flush\n+        if (_collector.size() == MAX_RECORDS_TO_COLLECT) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ5NDc2MA=="}, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMyODA4Ng==", "bodyText": "I think it is fine if user configures a very low _numRecordsPerPart and not get much aggregation. The problem of using the fixed large threshold is that for use cases with lots of columns, we might run out of memory and there is no knob to tune it.\nIn order to get better aggregation, we can potentially do an external sort on the files before collecting the records. Not requires in the first version.", "url": "https://github.com/apache/pinot/pull/5934#discussion_r483328086", "createdAt": "2020-09-04T00:55:52Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentReducer.java", "diffHunk": "@@ -0,0 +1,151 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.framework;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import org.apache.avro.file.DataFileWriter;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.pinot.core.segment.processing.collector.Collector;\n+import org.apache.pinot.core.segment.processing.collector.CollectorFactory;\n+import org.apache.pinot.core.segment.processing.utils.SegmentProcessorUtils;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+import org.apache.pinot.spi.data.readers.RecordReader;\n+import org.apache.pinot.spi.data.readers.RecordReaderFactory;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Reducer phase of the SegmentProcessorFramework\n+ * Reads the avro files in the input directory and creates output avro files in the reducer output directory.\n+ * The avro files in the input directory are expected to contain data for only 1 partition\n+ * Performs operations on that partition data as follows:\n+ * - concatenation/rollup of records\n+ * - split\n+ * - TODO: dedup\n+ */\n+public class SegmentReducer {\n+\n+  private static final Logger LOGGER = LoggerFactory.getLogger(SegmentReducer.class);\n+  private static final int MAX_RECORDS_TO_COLLECT = 5_000_000;\n+\n+  private final File _reducerInputDir;\n+  private final File _reducerOutputDir;\n+\n+  private final String _reducerId;\n+  private final Schema _pinotSchema;\n+  private final org.apache.avro.Schema _avroSchema;\n+  private final Collector _collector;\n+  private final int _numRecordsPerPart;\n+\n+  public SegmentReducer(String reducerId, File reducerInputDir, SegmentReducerConfig reducerConfig,\n+      File reducerOutputDir) {\n+    _reducerInputDir = reducerInputDir;\n+    _reducerOutputDir = reducerOutputDir;\n+\n+    _reducerId = reducerId;\n+    _pinotSchema = reducerConfig.getPinotSchema();\n+    _avroSchema = SegmentProcessorUtils.convertPinotSchemaToAvroSchema(_pinotSchema);\n+    _collector = CollectorFactory.getCollector(reducerConfig.getCollectorConfig(), _pinotSchema);\n+    _numRecordsPerPart = reducerConfig.getNumRecordsPerPart();\n+    LOGGER.info(\"Initialized reducer with id: {}, input dir: {}, output dir: {}, collector: {}, numRecordsPerPart: {}\",\n+        _reducerId, _reducerInputDir, _reducerOutputDir, _collector.getClass(), _numRecordsPerPart);\n+  }\n+\n+  /**\n+   * Reads the avro files in the input directory.\n+   * Performs configured operations and outputs to other avro file(s) in the reducer output directory.\n+   */\n+  public void reduce()\n+      throws Exception {\n+\n+    int part = 0;\n+    for (File inputFile : _reducerInputDir.listFiles()) {\n+\n+      RecordReader avroRecordReader = RecordReaderFactory\n+          .getRecordReaderByClass(\"org.apache.pinot.plugin.inputformat.avro.AvroRecordReader\", inputFile,\n+              _pinotSchema.getColumnNames(), null);\n+\n+      while (avroRecordReader.hasNext()) {\n+        GenericRow next = avroRecordReader.next();\n+\n+        // Aggregations\n+        _collector.collect(next);\n+\n+        // Exceeded max records allowed to collect. Flush\n+        if (_collector.size() == MAX_RECORDS_TO_COLLECT) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ5NDc2MA=="}, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mzg3NjAxOQ==", "bodyText": "sounds good. Changed it to flush when numRecordsPerPart is reached.\n@mcvsubbu we can certainly do that if we start seeing need for more advanced controls and tuning on collection size and flushing. For starters, I wanted to keep it simple", "url": "https://github.com/apache/pinot/pull/5934#discussion_r483876019", "createdAt": "2020-09-04T23:07:35Z", "author": {"login": "npawar"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentReducer.java", "diffHunk": "@@ -0,0 +1,151 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.framework;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import org.apache.avro.file.DataFileWriter;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.pinot.core.segment.processing.collector.Collector;\n+import org.apache.pinot.core.segment.processing.collector.CollectorFactory;\n+import org.apache.pinot.core.segment.processing.utils.SegmentProcessorUtils;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+import org.apache.pinot.spi.data.readers.RecordReader;\n+import org.apache.pinot.spi.data.readers.RecordReaderFactory;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Reducer phase of the SegmentProcessorFramework\n+ * Reads the avro files in the input directory and creates output avro files in the reducer output directory.\n+ * The avro files in the input directory are expected to contain data for only 1 partition\n+ * Performs operations on that partition data as follows:\n+ * - concatenation/rollup of records\n+ * - split\n+ * - TODO: dedup\n+ */\n+public class SegmentReducer {\n+\n+  private static final Logger LOGGER = LoggerFactory.getLogger(SegmentReducer.class);\n+  private static final int MAX_RECORDS_TO_COLLECT = 5_000_000;\n+\n+  private final File _reducerInputDir;\n+  private final File _reducerOutputDir;\n+\n+  private final String _reducerId;\n+  private final Schema _pinotSchema;\n+  private final org.apache.avro.Schema _avroSchema;\n+  private final Collector _collector;\n+  private final int _numRecordsPerPart;\n+\n+  public SegmentReducer(String reducerId, File reducerInputDir, SegmentReducerConfig reducerConfig,\n+      File reducerOutputDir) {\n+    _reducerInputDir = reducerInputDir;\n+    _reducerOutputDir = reducerOutputDir;\n+\n+    _reducerId = reducerId;\n+    _pinotSchema = reducerConfig.getPinotSchema();\n+    _avroSchema = SegmentProcessorUtils.convertPinotSchemaToAvroSchema(_pinotSchema);\n+    _collector = CollectorFactory.getCollector(reducerConfig.getCollectorConfig(), _pinotSchema);\n+    _numRecordsPerPart = reducerConfig.getNumRecordsPerPart();\n+    LOGGER.info(\"Initialized reducer with id: {}, input dir: {}, output dir: {}, collector: {}, numRecordsPerPart: {}\",\n+        _reducerId, _reducerInputDir, _reducerOutputDir, _collector.getClass(), _numRecordsPerPart);\n+  }\n+\n+  /**\n+   * Reads the avro files in the input directory.\n+   * Performs configured operations and outputs to other avro file(s) in the reducer output directory.\n+   */\n+  public void reduce()\n+      throws Exception {\n+\n+    int part = 0;\n+    for (File inputFile : _reducerInputDir.listFiles()) {\n+\n+      RecordReader avroRecordReader = RecordReaderFactory\n+          .getRecordReaderByClass(\"org.apache.pinot.plugin.inputformat.avro.AvroRecordReader\", inputFile,\n+              _pinotSchema.getColumnNames(), null);\n+\n+      while (avroRecordReader.hasNext()) {\n+        GenericRow next = avroRecordReader.next();\n+\n+        // Aggregations\n+        _collector.collect(next);\n+\n+        // Exceeded max records allowed to collect. Flush\n+        if (_collector.size() == MAX_RECORDS_TO_COLLECT) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ5NDc2MA=="}, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 96}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAwOTQwMDE0OnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/Collector.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQxNjo1OTozMFrOHLAALw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQwMTozNzoyN1rOHLOdEQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTI5NjQzMQ==", "bodyText": "What does size mean? Number of rows? Memory size?", "url": "https://github.com/apache/pinot/pull/5934#discussion_r481296431", "createdAt": "2020-09-01T16:59:30Z", "author": {"login": "mcvsubbu"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/Collector.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import java.util.Iterator;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * Collects and stores GenericRows\n+ */\n+public interface Collector {\n+\n+  /**\n+   * Collects the given GenericRow and stores it\n+   * @param genericRow the generic row to add to the collection\n+   */\n+  void collect(GenericRow genericRow);\n+\n+  /**\n+   * Provides an iterator for the GenericRows in the collection\n+   */\n+  Iterator<GenericRow> iterator();\n+\n+  /**\n+   * The size of the collection", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTUzMzIwMQ==", "bodyText": "clarified in javadoc. it is num records", "url": "https://github.com/apache/pinot/pull/5934#discussion_r481533201", "createdAt": "2020-09-02T01:37:27Z", "author": {"login": "npawar"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/Collector.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import java.util.Iterator;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * Collects and stores GenericRows\n+ */\n+public interface Collector {\n+\n+  /**\n+   * Collects the given GenericRow and stores it\n+   * @param genericRow the generic row to add to the collection\n+   */\n+  void collect(GenericRow genericRow);\n+\n+  /**\n+   * Provides an iterator for the GenericRows in the collection\n+   */\n+  Iterator<GenericRow> iterator();\n+\n+  /**\n+   * The size of the collection", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTI5NjQzMQ=="}, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAwOTQwODI3OnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/CollectorFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQxNzowMTo0NlrOHLAFTg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQxNzowMTo0NlrOHLAFTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTI5Nzc0Mg==", "bodyText": "It is useful to add some comments on each of these, explaining what the collector does (or, what it does differently than others)", "url": "https://github.com/apache/pinot/pull/5934#discussion_r481297742", "createdAt": "2020-09-01T17:01:46Z", "author": {"login": "mcvsubbu"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/CollectorFactory.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import org.apache.pinot.spi.data.Schema;\n+\n+\n+/**\n+ * Factory for constructing a Collector from CollectorConfig\n+ */\n+public final class CollectorFactory {\n+\n+  private CollectorFactory() {\n+\n+  }\n+\n+  public enum CollectorType {\n+    ROLLUP, CONCAT", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAwOTQyNDUxOnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/RollupCollector.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQxNzowNjoyOFrOHLAPkg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQwMTozNjoxOFrOHLOaLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTMwMDM3MA==", "bodyText": "So, we include virtual columns here?", "url": "https://github.com/apache/pinot/pull/5934#discussion_r481300370", "createdAt": "2020-09-01T17:06:28Z", "author": {"login": "mcvsubbu"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/RollupCollector.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.Map;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.MetricFieldSpec;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * A Collector that rolls up the incoming records on unique dimensions + time columns, based on provided aggregation types for metrics.\n+ * By default will use the SUM aggregation on metrics.\n+ */\n+public class RollupCollector implements Collector {\n+\n+  private final Map<Record, GenericRow> _collection = new HashMap<>();\n+\n+  private final int _keySize;\n+  private final int _valueSize;\n+  private final String[] _keyColumns;\n+  private final String[] _valueColumns;\n+  private final ValueAggregator[] _valueAggregators;\n+  private final MetricFieldSpec[] _metricFieldSpecs;\n+\n+  public RollupCollector(CollectorConfig collectorConfig, Schema schema) {\n+    _keySize = schema.getColumnNames().size() - schema.getMetricNames().size();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTUzMjQ2MA==", "bodyText": "nope, changed it.", "url": "https://github.com/apache/pinot/pull/5934#discussion_r481532460", "createdAt": "2020-09-02T01:36:18Z", "author": {"login": "npawar"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/RollupCollector.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.Map;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.MetricFieldSpec;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * A Collector that rolls up the incoming records on unique dimensions + time columns, based on provided aggregation types for metrics.\n+ * By default will use the SUM aggregation on metrics.\n+ */\n+public class RollupCollector implements Collector {\n+\n+  private final Map<Record, GenericRow> _collection = new HashMap<>();\n+\n+  private final int _keySize;\n+  private final int _valueSize;\n+  private final String[] _keyColumns;\n+  private final String[] _valueColumns;\n+  private final ValueAggregator[] _valueAggregators;\n+  private final MetricFieldSpec[] _metricFieldSpecs;\n+\n+  public RollupCollector(CollectorConfig collectorConfig, Schema schema) {\n+    _keySize = schema.getColumnNames().size() - schema.getMetricNames().size();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTMwMDM3MA=="}, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAwOTQzMTE2OnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/RollupCollector.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQxNzowODoyOFrOHLAT6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQxNzowODoyOFrOHLAT6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTMwMTQ4Mg==", "bodyText": "Are these the key parts? If so, can we name the member as _keyParts? _values gets confusing.", "url": "https://github.com/apache/pinot/pull/5934#discussion_r481301482", "createdAt": "2020-09-01T17:08:28Z", "author": {"login": "mcvsubbu"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/RollupCollector.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.Map;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.MetricFieldSpec;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * A Collector that rolls up the incoming records on unique dimensions + time columns, based on provided aggregation types for metrics.\n+ * By default will use the SUM aggregation on metrics.\n+ */\n+public class RollupCollector implements Collector {\n+\n+  private final Map<Record, GenericRow> _collection = new HashMap<>();\n+\n+  private final int _keySize;\n+  private final int _valueSize;\n+  private final String[] _keyColumns;\n+  private final String[] _valueColumns;\n+  private final ValueAggregator[] _valueAggregators;\n+  private final MetricFieldSpec[] _metricFieldSpecs;\n+\n+  public RollupCollector(CollectorConfig collectorConfig, Schema schema) {\n+    _keySize = schema.getColumnNames().size() - schema.getMetricNames().size();\n+    _valueSize = schema.getMetricNames().size();\n+    _keyColumns = new String[_keySize];\n+    _valueColumns = new String[_valueSize];\n+    _valueAggregators = new ValueAggregator[_valueSize];\n+    _metricFieldSpecs = new MetricFieldSpec[_valueSize];\n+\n+    Map<String, ValueAggregatorFactory.ValueAggregatorType> aggregatorTypeMap = collectorConfig.getAggregatorTypeMap();\n+    if (aggregatorTypeMap == null) {\n+      aggregatorTypeMap = Collections.emptyMap();\n+    }\n+    int valIdx = 0;\n+    int keyIdx = 0;\n+    for (FieldSpec fieldSpec : schema.getAllFieldSpecs()) {\n+      if (!fieldSpec.isVirtualColumn()) {\n+        String name = fieldSpec.getName();\n+        if (fieldSpec.getFieldType().equals(FieldSpec.FieldType.METRIC)) {\n+          _metricFieldSpecs[valIdx] = (MetricFieldSpec) fieldSpec;\n+          _valueColumns[valIdx] = name;\n+          _valueAggregators[valIdx] = ValueAggregatorFactory.getValueAggregator(\n+              aggregatorTypeMap.getOrDefault(name, ValueAggregatorFactory.ValueAggregatorType.SUM).toString());\n+          valIdx++;\n+        } else {\n+          _keyColumns[keyIdx++] = name;\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * If a row already exists in the collection (based on dimension + time columns), rollup the metric values, else add the row\n+   */\n+  @Override\n+  public void collect(GenericRow genericRow) {\n+    Object[] key = new Object[_keySize];\n+    for (int i = 0; i < _keySize; i++) {\n+      key[i] = genericRow.getValue(_keyColumns[i]);\n+    }\n+    Record keyRecord = new Record(key);\n+    GenericRow prev = _collection.get(keyRecord);\n+    if (prev == null) {\n+      _collection.put(keyRecord, genericRow);\n+    } else {\n+      for (int i = 0; i < _valueSize; i++) {\n+        String valueColumn = _valueColumns[i];\n+        Object aggregate = _valueAggregators[i]\n+            .aggregate(prev.getValue(valueColumn), genericRow.getValue(valueColumn), _metricFieldSpecs[i]);\n+        prev.putValue(valueColumn, aggregate);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public Iterator<GenericRow> iterator() {\n+    return _collection.values().iterator();\n+  }\n+\n+  @Override\n+  public int size() {\n+    return _collection.size();\n+  }\n+\n+  @Override\n+  public void reset() {\n+    _collection.clear();\n+  }\n+\n+  /**\n+   * A record representation for the keys of the record\n+   * Note that the dimensions can have multi-value columns, and hence the equals and hashCode need deep array operations\n+   */\n+  private static class Record {\n+    private final Object[] _values;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 120}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAwOTQ3Njc5OnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/partitioner/PartitionerFactory.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQxNzoyMDo1OVrOHLAwOw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQwMToyMjo1NlrOHRtDvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTMwODczMQ==", "bodyText": "Since these are all derived segments (merging m segments into n) should we not be using the table's partitioner all the time?", "url": "https://github.com/apache/pinot/pull/5934#discussion_r481308731", "createdAt": "2020-09-01T17:20:59Z", "author": {"login": "mcvsubbu"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/partitioner/PartitionerFactory.java", "diffHunk": "@@ -0,0 +1,82 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.partitioner;\n+\n+import com.google.common.base.Preconditions;\n+\n+\n+/**\n+ * Factory for Partitioner and PartitionFilter\n+ */\n+public final class PartitionerFactory {\n+\n+  private PartitionerFactory() {\n+\n+  }\n+\n+  public enum PartitionerType {\n+    NO_OP, ROW_HASH, COLUMN_VALUE, TRANSFORM_FUNCTION, TABLE_PARTITION_CONFIG\n+  }\n+\n+  /**\n+   * Construct a Partitioner using the PartitioningConfig\n+   */\n+  public static Partitioner getPartitioner(PartitioningConfig config) {\n+\n+    Partitioner partitioner = null;\n+    switch (config.getPartitionerType()) {\n+      case NO_OP:\n+        partitioner = new NoOpPartitioner();\n+        break;\n+      case ROW_HASH:\n+        Preconditions\n+            .checkState(config.getNumPartitions() > 0, \"Must provide numPartitions > 0 for ROW_HASH partitioner\");\n+        partitioner = new RowHashPartitioner(config.getNumPartitions());\n+        break;\n+      case COLUMN_VALUE:\n+        Preconditions.checkState(config.getColumnName() != null, \"Must provide columnName for COLUMN_VALUE partitioner\");\n+        partitioner = new ColumnValuePartitioner(config.getColumnName());\n+        break;\n+      case TRANSFORM_FUNCTION:\n+        Preconditions.checkState(config.getTransformFunction() != null,\n+            \"Must provide transformFunction for TRANSFORM_FUNCTION partitioner\");\n+        partitioner = new TransformFunctionPartitioner(config.getTransformFunction());\n+        break;\n+      case TABLE_PARTITION_CONFIG:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMyNjA3OQ==", "bodyText": "Yes. Seunghyun pointed out similar thing. Discussion below.\nI'll be changing the partitioner to a 2 step partitioner in the next PR (have put TODO in code and descritpion)", "url": "https://github.com/apache/pinot/pull/5934#discussion_r488326079", "createdAt": "2020-09-15T01:22:56Z", "author": {"login": "npawar"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/partitioner/PartitionerFactory.java", "diffHunk": "@@ -0,0 +1,82 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.partitioner;\n+\n+import com.google.common.base.Preconditions;\n+\n+\n+/**\n+ * Factory for Partitioner and PartitionFilter\n+ */\n+public final class PartitionerFactory {\n+\n+  private PartitionerFactory() {\n+\n+  }\n+\n+  public enum PartitionerType {\n+    NO_OP, ROW_HASH, COLUMN_VALUE, TRANSFORM_FUNCTION, TABLE_PARTITION_CONFIG\n+  }\n+\n+  /**\n+   * Construct a Partitioner using the PartitioningConfig\n+   */\n+  public static Partitioner getPartitioner(PartitioningConfig config) {\n+\n+    Partitioner partitioner = null;\n+    switch (config.getPartitionerType()) {\n+      case NO_OP:\n+        partitioner = new NoOpPartitioner();\n+        break;\n+      case ROW_HASH:\n+        Preconditions\n+            .checkState(config.getNumPartitions() > 0, \"Must provide numPartitions > 0 for ROW_HASH partitioner\");\n+        partitioner = new RowHashPartitioner(config.getNumPartitions());\n+        break;\n+      case COLUMN_VALUE:\n+        Preconditions.checkState(config.getColumnName() != null, \"Must provide columnName for COLUMN_VALUE partitioner\");\n+        partitioner = new ColumnValuePartitioner(config.getColumnName());\n+        break;\n+      case TRANSFORM_FUNCTION:\n+        Preconditions.checkState(config.getTransformFunction() != null,\n+            \"Must provide transformFunction for TRANSFORM_FUNCTION partitioner\");\n+        partitioner = new TransformFunctionPartitioner(config.getTransformFunction());\n+        break;\n+      case TABLE_PARTITION_CONFIG:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTMwODczMQ=="}, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAwOTQ4NDA0OnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/utils/SegmentProcessorUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQxNzoyMzoxM1rOHLA1DQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQwMToyMjowN1rOHRtC_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTMwOTk2NQ==", "bodyText": "These should go into avro plugins? Why introduce dependency on avro here?", "url": "https://github.com/apache/pinot/pull/5934#discussion_r481309965", "createdAt": "2020-09-01T17:23:13Z", "author": {"login": "mcvsubbu"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/utils/SegmentProcessorUtils.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.utils;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * Helper util methods for SegmentProcessorFramework\n+ */\n+public final class SegmentProcessorUtils {\n+\n+  private SegmentProcessorUtils() {\n+  }\n+\n+  /**\n+   * Convert a GenericRow to an avro GenericRecord\n+   */\n+  public static GenericData.Record convertGenericRowToAvroRecord(GenericRow genericRow,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMyNTg4Nw==", "bodyText": "avro plugins is more for the extractors and decoders right. This is a util method. And pinot-core cannot be made to depend on pinot-avro for this", "url": "https://github.com/apache/pinot/pull/5934#discussion_r488325887", "createdAt": "2020-09-15T01:22:07Z", "author": {"login": "npawar"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/utils/SegmentProcessorUtils.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.utils;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * Helper util methods for SegmentProcessorFramework\n+ */\n+public final class SegmentProcessorUtils {\n+\n+  private SegmentProcessorUtils() {\n+  }\n+\n+  /**\n+   * Convert a GenericRow to an avro GenericRecord\n+   */\n+  public static GenericData.Record convertGenericRowToAvroRecord(GenericRow genericRow,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTMwOTk2NQ=="}, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNTE0OTIxOnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/Collector.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQyMzo1Njo1NFrOHOx0Qg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQyMzo1Njo1NFrOHOx0Qg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI1ODMwNg==", "bodyText": "Recommend combining iterator() and finish() into one method because we always need to call finish() then iterator() (maybe remove finish() and move the sorting logic into iterator())", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485258306", "createdAt": "2020-09-08T23:56:54Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/Collector.java", "diffHunk": "@@ -0,0 +1,55 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import java.util.Iterator;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * Collects and stores GenericRows\n+ */\n+public interface Collector {\n+\n+  /**\n+   * Collects the given GenericRow and stores it\n+   * @param genericRow the generic row to add to the collection\n+   */\n+  void collect(GenericRow genericRow);\n+\n+  /**\n+   * Provides an iterator for the GenericRows in the collection\n+   */\n+  Iterator<GenericRow> iterator();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNTE2NTE1OnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/GenericRowSorter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMDowNDo0MVrOHOx9ag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMDowNDo0MVrOHOx9ag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2MDY1MA==", "bodyText": "From the past experience, storing dataType and do per-value switch is faster than storing Comparator", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485260650", "createdAt": "2020-09-09T00:04:41Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/GenericRowSorter.java", "diffHunk": "@@ -0,0 +1,81 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import com.google.common.base.Preconditions;\n+import java.util.Comparator;\n+import java.util.List;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * A sorter for GenericRows\n+ */\n+public class GenericRowSorter {\n+\n+  private final Comparator<GenericRow> _genericRowComparator;\n+\n+  public GenericRowSorter(List<String> sortOrder, Schema schema) {\n+    int sortOrderSize = sortOrder.size();\n+    Comparator[] comparators = new Comparator[sortOrderSize];\n+    for (int i = 0; i < sortOrderSize; i++) {\n+      String column = sortOrder.get(i);\n+      FieldSpec fieldSpec = schema.getFieldSpecFor(column);\n+      Preconditions.checkState(fieldSpec.isSingleValueField(), \"Cannot use multi value column: %s for sorting\", column);\n+      comparators[i] = getComparator(fieldSpec.getDataType());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNTE2NzA0OnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/GenericRowSorter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMDowNToyNlrOHOx-Xw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMDowNToyNlrOHOx-Xw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2MDg5NQ==", "bodyText": "In favor of this flavor for performance concern (avoid using function)", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485260895", "createdAt": "2020-09-09T00:05:26Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/GenericRowSorter.java", "diffHunk": "@@ -0,0 +1,81 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import com.google.common.base.Preconditions;\n+import java.util.Comparator;\n+import java.util.List;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * A sorter for GenericRows\n+ */\n+public class GenericRowSorter {\n+\n+  private final Comparator<GenericRow> _genericRowComparator;\n+\n+  public GenericRowSorter(List<String> sortOrder, Schema schema) {\n+    int sortOrderSize = sortOrder.size();\n+    Comparator[] comparators = new Comparator[sortOrderSize];\n+    for (int i = 0; i < sortOrderSize; i++) {\n+      String column = sortOrder.get(i);\n+      FieldSpec fieldSpec = schema.getFieldSpecFor(column);\n+      Preconditions.checkState(fieldSpec.isSingleValueField(), \"Cannot use multi value column: %s for sorting\", column);\n+      comparators[i] = getComparator(fieldSpec.getDataType());\n+    }\n+    _genericRowComparator = (o1, o2) -> {\n+      for (int i = 0; i < comparators.length; i++) {\n+        String column = sortOrder.get(i);\n+        int result = comparators[i].compare(o1.getValue(column), o2.getValue(column));\n+        if (result != 0) {\n+          return result;\n+        }\n+      }\n+      return 0;\n+    };\n+  }\n+\n+  private Comparator getComparator(FieldSpec.DataType dataType) {\n+    switch (dataType) {\n+\n+      case INT:\n+        return Comparator.comparingInt(o -> (int) o);\n+      case LONG:\n+        return Comparator.comparingLong(o -> (long) o);\n+      case FLOAT:\n+        return (o1, o2) -> Float.compare((float) o1, (float) o2);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNTE2ODIwOnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/GenericRowSorter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMDowNjowNFrOHOx_GQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMDowNjowNFrOHOx_GQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2MTA4MQ==", "bodyText": "Add BYTES support ByteArray.compare()", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485261081", "createdAt": "2020-09-09T00:06:04Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/GenericRowSorter.java", "diffHunk": "@@ -0,0 +1,81 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import com.google.common.base.Preconditions;\n+import java.util.Comparator;\n+import java.util.List;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * A sorter for GenericRows\n+ */\n+public class GenericRowSorter {\n+\n+  private final Comparator<GenericRow> _genericRowComparator;\n+\n+  public GenericRowSorter(List<String> sortOrder, Schema schema) {\n+    int sortOrderSize = sortOrder.size();\n+    Comparator[] comparators = new Comparator[sortOrderSize];\n+    for (int i = 0; i < sortOrderSize; i++) {\n+      String column = sortOrder.get(i);\n+      FieldSpec fieldSpec = schema.getFieldSpecFor(column);\n+      Preconditions.checkState(fieldSpec.isSingleValueField(), \"Cannot use multi value column: %s for sorting\", column);\n+      comparators[i] = getComparator(fieldSpec.getDataType());\n+    }\n+    _genericRowComparator = (o1, o2) -> {\n+      for (int i = 0; i < comparators.length; i++) {\n+        String column = sortOrder.get(i);\n+        int result = comparators[i].compare(o1.getValue(column), o2.getValue(column));\n+        if (result != 0) {\n+          return result;\n+        }\n+      }\n+      return 0;\n+    };\n+  }\n+\n+  private Comparator getComparator(FieldSpec.DataType dataType) {\n+    switch (dataType) {\n+\n+      case INT:\n+        return Comparator.comparingInt(o -> (int) o);\n+      case LONG:\n+        return Comparator.comparingLong(o -> (long) o);\n+      case FLOAT:\n+        return (o1, o2) -> Float.compare((float) o1, (float) o2);\n+      case DOUBLE:\n+        return Comparator.comparingDouble(o -> (double) o);\n+      case STRING:\n+        return Comparator.comparing(o -> ((String) o));\n+      default:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNTE3MDU4OnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/CollectorConfig.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMDowNzoxOVrOHOyAag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMDowNzoxOVrOHOyAag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2MTQxOA==", "bodyText": "(Code style)\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                private CollectorFactory.CollectorType collectorType = DEFAULT_COLLECTOR_TYPE;\n          \n          \n            \n                private CollectorFactory.CollectorType _collectorType = DEFAULT_COLLECTOR_TYPE;", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485261418", "createdAt": "2020-09-09T00:07:19Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/CollectorConfig.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import com.fasterxml.jackson.databind.annotation.JsonDeserialize;\n+import com.fasterxml.jackson.databind.annotation.JsonPOJOBuilder;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+\n+\n+/**\n+ * Config for Collector\n+ */\n+@JsonDeserialize(builder = CollectorConfig.Builder.class)\n+public class CollectorConfig {\n+  private static final CollectorFactory.CollectorType DEFAULT_COLLECTOR_TYPE = CollectorFactory.CollectorType.CONCAT;\n+\n+  private final CollectorFactory.CollectorType _collectorType;\n+  private final Map<String, ValueAggregatorFactory.ValueAggregatorType> _aggregatorTypeMap;\n+  private final List<String> _sortOrder;\n+\n+  private CollectorConfig(CollectorFactory.CollectorType collectorType,\n+      Map<String, ValueAggregatorFactory.ValueAggregatorType> aggregatorTypeMap, List<String> sortOrder) {\n+    _collectorType = collectorType;\n+    _aggregatorTypeMap = aggregatorTypeMap;\n+    _sortOrder = sortOrder;\n+  }\n+\n+  /**\n+   * The type of the Collector\n+   */\n+  public CollectorFactory.CollectorType getCollectorType() {\n+    return _collectorType;\n+  }\n+\n+  /**\n+   * Map containing aggregation types for the metrics\n+   */\n+  @Nullable\n+  public Map<String, ValueAggregatorFactory.ValueAggregatorType> getAggregatorTypeMap() {\n+    return _aggregatorTypeMap;\n+  }\n+\n+  /**\n+   * The columns on which to sort\n+   */\n+  public List<String> getSortOrder() {\n+    return _sortOrder;\n+  }\n+\n+  /**\n+   * Builder for CollectorConfig\n+   */\n+  @JsonPOJOBuilder(withPrefix = \"set\")\n+  public static class Builder {\n+    private CollectorFactory.CollectorType collectorType = DEFAULT_COLLECTOR_TYPE;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 74}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNTE3MDgxOnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/CollectorConfig.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMDowNzozM1rOHOyAkw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMDowNzozM1rOHOyAkw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2MTQ1OQ==", "bodyText": "Avoid using this", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485261459", "createdAt": "2020-09-09T00:07:33Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/CollectorConfig.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import com.fasterxml.jackson.databind.annotation.JsonDeserialize;\n+import com.fasterxml.jackson.databind.annotation.JsonPOJOBuilder;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+\n+\n+/**\n+ * Config for Collector\n+ */\n+@JsonDeserialize(builder = CollectorConfig.Builder.class)\n+public class CollectorConfig {\n+  private static final CollectorFactory.CollectorType DEFAULT_COLLECTOR_TYPE = CollectorFactory.CollectorType.CONCAT;\n+\n+  private final CollectorFactory.CollectorType _collectorType;\n+  private final Map<String, ValueAggregatorFactory.ValueAggregatorType> _aggregatorTypeMap;\n+  private final List<String> _sortOrder;\n+\n+  private CollectorConfig(CollectorFactory.CollectorType collectorType,\n+      Map<String, ValueAggregatorFactory.ValueAggregatorType> aggregatorTypeMap, List<String> sortOrder) {\n+    _collectorType = collectorType;\n+    _aggregatorTypeMap = aggregatorTypeMap;\n+    _sortOrder = sortOrder;\n+  }\n+\n+  /**\n+   * The type of the Collector\n+   */\n+  public CollectorFactory.CollectorType getCollectorType() {\n+    return _collectorType;\n+  }\n+\n+  /**\n+   * Map containing aggregation types for the metrics\n+   */\n+  @Nullable\n+  public Map<String, ValueAggregatorFactory.ValueAggregatorType> getAggregatorTypeMap() {\n+    return _aggregatorTypeMap;\n+  }\n+\n+  /**\n+   * The columns on which to sort\n+   */\n+  public List<String> getSortOrder() {\n+    return _sortOrder;\n+  }\n+\n+  /**\n+   * Builder for CollectorConfig\n+   */\n+  @JsonPOJOBuilder(withPrefix = \"set\")\n+  public static class Builder {\n+    private CollectorFactory.CollectorType collectorType = DEFAULT_COLLECTOR_TYPE;\n+    private Map<String, ValueAggregatorFactory.ValueAggregatorType> aggregatorTypeMap;\n+    private List<String> sortOrder = new ArrayList<>();\n+\n+    public Builder setCollectorType(CollectorFactory.CollectorType collectorType) {\n+      this.collectorType = collectorType;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 79}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNTE3NzI0OnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentProcessorConfig.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMDoxMTowMlrOHOyEUw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMDoxMTowMlrOHOyEUw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2MjQxOQ==", "bodyText": "Same for other places\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                private TableConfig tableConfig;\n          \n          \n            \n                private TableConfig _tableConfig;", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485262419", "createdAt": "2020-09-09T00:11:02Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentProcessorConfig.java", "diffHunk": "@@ -0,0 +1,181 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.framework;\n+\n+import com.google.common.base.Preconditions;\n+import org.apache.pinot.core.segment.processing.collector.CollectorConfig;\n+import org.apache.pinot.core.segment.processing.filter.RecordFilterConfig;\n+import org.apache.pinot.core.segment.processing.partitioner.PartitioningConfig;\n+import org.apache.pinot.core.segment.processing.transformer.RecordTransformerConfig;\n+import org.apache.pinot.spi.config.table.TableConfig;\n+import org.apache.pinot.spi.data.Schema;\n+\n+\n+/**\n+ * Config for configuring the phases of {@link SegmentProcessorFramework}\n+ */\n+public class SegmentProcessorConfig {\n+\n+  private final TableConfig _tableConfig;\n+  private final Schema _schema;\n+  private final RecordTransformerConfig _recordTransformerConfig;\n+  private final RecordFilterConfig _recordFilterConfig;\n+  private final PartitioningConfig _partitioningConfig;\n+  private final CollectorConfig _collectorConfig;\n+  private final SegmentConfig _segmentConfig;\n+\n+  private SegmentProcessorConfig(TableConfig tableConfig, Schema schema,\n+      RecordTransformerConfig recordTransformerConfig, RecordFilterConfig recordFilterConfig,\n+      PartitioningConfig partitioningConfig, CollectorConfig collectorConfig, SegmentConfig segmentConfig) {\n+    _tableConfig = tableConfig;\n+    _schema = schema;\n+    _recordTransformerConfig = recordTransformerConfig;\n+    _recordFilterConfig = recordFilterConfig;\n+    _partitioningConfig = partitioningConfig;\n+    _collectorConfig = collectorConfig;\n+    _segmentConfig = segmentConfig;\n+  }\n+\n+  /**\n+   * The Pinot table config\n+   */\n+  public TableConfig getTableConfig() {\n+    return _tableConfig;\n+  }\n+\n+  /**\n+   * The Pinot schema\n+   */\n+  public Schema getSchema() {\n+    return _schema;\n+  }\n+\n+  /**\n+   * The RecordTransformerConfig for the SegmentProcessorFramework's map phase\n+   */\n+  public RecordTransformerConfig getRecordTransformerConfig() {\n+    return _recordTransformerConfig;\n+  }\n+\n+  /**\n+   * The RecordFilterConfig to filter records\n+   */\n+  public RecordFilterConfig getRecordFilterConfig() {\n+    return _recordFilterConfig;\n+  }\n+\n+  /**\n+   * The PartitioningConfig for the SegmentProcessorFramework's map phase\n+   */\n+  public PartitioningConfig getPartitioningConfig() {\n+    return _partitioningConfig;\n+  }\n+\n+  /**\n+   * The CollectorConfig for the SegmentProcessorFramework's reduce phase\n+   */\n+  public CollectorConfig getCollectorConfig() {\n+    return _collectorConfig;\n+  }\n+\n+  /**\n+   * The SegmentConfig for the SegmentProcessorFramework's segment generation phase\n+   */\n+  public SegmentConfig getSegmentConfig() {\n+    return _segmentConfig;\n+  }\n+\n+  /**\n+   * Builder for SegmentProcessorConfig\n+   */\n+  public static class Builder {\n+    private TableConfig tableConfig;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 108}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNTE3ODg0OnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentProcessorConfig.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMDoxMTo0N1rOHOyFNg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMDoxMTo0N1rOHOyFNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2MjY0Ng==", "bodyText": "Same for other places\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  this.tableConfig = tableConfig;\n          \n          \n            \n                  _tableConfig = tableConfig;", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485262646", "createdAt": "2020-09-09T00:11:47Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentProcessorConfig.java", "diffHunk": "@@ -0,0 +1,181 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.framework;\n+\n+import com.google.common.base.Preconditions;\n+import org.apache.pinot.core.segment.processing.collector.CollectorConfig;\n+import org.apache.pinot.core.segment.processing.filter.RecordFilterConfig;\n+import org.apache.pinot.core.segment.processing.partitioner.PartitioningConfig;\n+import org.apache.pinot.core.segment.processing.transformer.RecordTransformerConfig;\n+import org.apache.pinot.spi.config.table.TableConfig;\n+import org.apache.pinot.spi.data.Schema;\n+\n+\n+/**\n+ * Config for configuring the phases of {@link SegmentProcessorFramework}\n+ */\n+public class SegmentProcessorConfig {\n+\n+  private final TableConfig _tableConfig;\n+  private final Schema _schema;\n+  private final RecordTransformerConfig _recordTransformerConfig;\n+  private final RecordFilterConfig _recordFilterConfig;\n+  private final PartitioningConfig _partitioningConfig;\n+  private final CollectorConfig _collectorConfig;\n+  private final SegmentConfig _segmentConfig;\n+\n+  private SegmentProcessorConfig(TableConfig tableConfig, Schema schema,\n+      RecordTransformerConfig recordTransformerConfig, RecordFilterConfig recordFilterConfig,\n+      PartitioningConfig partitioningConfig, CollectorConfig collectorConfig, SegmentConfig segmentConfig) {\n+    _tableConfig = tableConfig;\n+    _schema = schema;\n+    _recordTransformerConfig = recordTransformerConfig;\n+    _recordFilterConfig = recordFilterConfig;\n+    _partitioningConfig = partitioningConfig;\n+    _collectorConfig = collectorConfig;\n+    _segmentConfig = segmentConfig;\n+  }\n+\n+  /**\n+   * The Pinot table config\n+   */\n+  public TableConfig getTableConfig() {\n+    return _tableConfig;\n+  }\n+\n+  /**\n+   * The Pinot schema\n+   */\n+  public Schema getSchema() {\n+    return _schema;\n+  }\n+\n+  /**\n+   * The RecordTransformerConfig for the SegmentProcessorFramework's map phase\n+   */\n+  public RecordTransformerConfig getRecordTransformerConfig() {\n+    return _recordTransformerConfig;\n+  }\n+\n+  /**\n+   * The RecordFilterConfig to filter records\n+   */\n+  public RecordFilterConfig getRecordFilterConfig() {\n+    return _recordFilterConfig;\n+  }\n+\n+  /**\n+   * The PartitioningConfig for the SegmentProcessorFramework's map phase\n+   */\n+  public PartitioningConfig getPartitioningConfig() {\n+    return _partitioningConfig;\n+  }\n+\n+  /**\n+   * The CollectorConfig for the SegmentProcessorFramework's reduce phase\n+   */\n+  public CollectorConfig getCollectorConfig() {\n+    return _collectorConfig;\n+  }\n+\n+  /**\n+   * The SegmentConfig for the SegmentProcessorFramework's segment generation phase\n+   */\n+  public SegmentConfig getSegmentConfig() {\n+    return _segmentConfig;\n+  }\n+\n+  /**\n+   * Builder for SegmentProcessorConfig\n+   */\n+  public static class Builder {\n+    private TableConfig tableConfig;\n+    private Schema schema;\n+    private RecordTransformerConfig recordTransformerConfig;\n+    private RecordFilterConfig recordFilterConfig;\n+    private PartitioningConfig partitioningConfig;\n+    private CollectorConfig collectorConfig;\n+    private SegmentConfig _segmentConfig;\n+\n+    public Builder setTableConfig(TableConfig tableConfig) {\n+      this.tableConfig = tableConfig;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 117}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNTIwMzcxOnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/CollectorConfig.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMDoyNDo0N1rOHOyTLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQyMjozOTozMlrOHQIIgQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2NjIyMA==", "bodyText": "I feel this is not as readable as the JsonCreator annotation on the constructor. IMO We don't really need a builder for very simple config", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485266220", "createdAt": "2020-09-09T00:24:47Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/CollectorConfig.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import com.fasterxml.jackson.databind.annotation.JsonDeserialize;\n+import com.fasterxml.jackson.databind.annotation.JsonPOJOBuilder;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+\n+\n+/**\n+ * Config for Collector\n+ */\n+@JsonDeserialize(builder = CollectorConfig.Builder.class)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjY3MjUxMw==", "bodyText": "Agreed about JsonCreator. Changed it.", "url": "https://github.com/apache/pinot/pull/5934#discussion_r486672513", "createdAt": "2020-09-10T22:39:32Z", "author": {"login": "npawar"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/CollectorConfig.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import com.fasterxml.jackson.databind.annotation.JsonDeserialize;\n+import com.fasterxml.jackson.databind.annotation.JsonPOJOBuilder;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+\n+\n+/**\n+ * Config for Collector\n+ */\n+@JsonDeserialize(builder = CollectorConfig.Builder.class)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2NjIyMA=="}, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNTIwODE2OnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/ConcatCollector.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMDoyNzoxOFrOHOyV1A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMDoyNzoxOFrOHOyV1A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2NjkwMA==", "bodyText": "(nit) Make it final?", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485266900", "createdAt": "2020-09-09T00:27:18Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/ConcatCollector.java", "diffHunk": "@@ -0,0 +1,71 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * A Collector implementation for collecting and concatenating all incoming rows\n+ */\n+public class ConcatCollector implements Collector {\n+  private final List<GenericRow> _collection = new ArrayList<>();\n+  private Iterator<GenericRow> _iterator;\n+  private GenericRowSorter _sorter;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNTIwODM3OnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/RollupCollector.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMDoyNzoyNlrOHOyV8Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMDoyNzoyNlrOHOyV8Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2NjkyOQ==", "bodyText": "(nit) Make it final?", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485266929", "createdAt": "2020-09-09T00:27:26Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/RollupCollector.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.MetricFieldSpec;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * A Collector that rolls up the incoming records on unique dimensions + time columns, based on provided aggregation types for metrics.\n+ * By default will use the SUM aggregation on metrics.\n+ */\n+public class RollupCollector implements Collector {\n+\n+  private final Map<Record, GenericRow> _collection = new HashMap<>();\n+  private Iterator<GenericRow> _iterator;\n+  private GenericRowSorter _sorter;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNTIxNTIyOnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/RollupCollector.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMDozMTowMVrOHOyZ1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMDozMTowMVrOHOyZ1Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2NzkyNQ==", "bodyText": "You might want to extract number of columns from field specs as metric column can also be virtual", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485267925", "createdAt": "2020-09-09T00:31:01Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/RollupCollector.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.MetricFieldSpec;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * A Collector that rolls up the incoming records on unique dimensions + time columns, based on provided aggregation types for metrics.\n+ * By default will use the SUM aggregation on metrics.\n+ */\n+public class RollupCollector implements Collector {\n+\n+  private final Map<Record, GenericRow> _collection = new HashMap<>();\n+  private Iterator<GenericRow> _iterator;\n+  private GenericRowSorter _sorter;\n+\n+  private final int _keySize;\n+  private final int _valueSize;\n+  private final String[] _keyColumns;\n+  private final String[] _valueColumns;\n+  private final ValueAggregator[] _valueAggregators;\n+  private final MetricFieldSpec[] _metricFieldSpecs;\n+\n+  public RollupCollector(CollectorConfig collectorConfig, Schema schema) {\n+    _keySize = schema.getPhysicalColumnNames().size() - schema.getMetricNames().size();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNTIxNzY5OnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/RollupCollector.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMDozMjoyMVrOHOybOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMDozMjoyMVrOHOybOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2ODI4Mg==", "bodyText": "(nit)\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    if (fieldSpec.getFieldType().equals(FieldSpec.FieldType.METRIC)) {\n          \n          \n            \n                    if (fieldSpec.getFieldType() == FieldSpec.FieldType.METRIC) {", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485268282", "createdAt": "2020-09-09T00:32:21Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/RollupCollector.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.MetricFieldSpec;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * A Collector that rolls up the incoming records on unique dimensions + time columns, based on provided aggregation types for metrics.\n+ * By default will use the SUM aggregation on metrics.\n+ */\n+public class RollupCollector implements Collector {\n+\n+  private final Map<Record, GenericRow> _collection = new HashMap<>();\n+  private Iterator<GenericRow> _iterator;\n+  private GenericRowSorter _sorter;\n+\n+  private final int _keySize;\n+  private final int _valueSize;\n+  private final String[] _keyColumns;\n+  private final String[] _valueColumns;\n+  private final ValueAggregator[] _valueAggregators;\n+  private final MetricFieldSpec[] _metricFieldSpecs;\n+\n+  public RollupCollector(CollectorConfig collectorConfig, Schema schema) {\n+    _keySize = schema.getPhysicalColumnNames().size() - schema.getMetricNames().size();\n+    _valueSize = schema.getMetricNames().size();\n+    _keyColumns = new String[_keySize];\n+    _valueColumns = new String[_valueSize];\n+    _valueAggregators = new ValueAggregator[_valueSize];\n+    _metricFieldSpecs = new MetricFieldSpec[_valueSize];\n+\n+    Map<String, ValueAggregatorFactory.ValueAggregatorType> aggregatorTypeMap = collectorConfig.getAggregatorTypeMap();\n+    if (aggregatorTypeMap == null) {\n+      aggregatorTypeMap = Collections.emptyMap();\n+    }\n+    int valIdx = 0;\n+    int keyIdx = 0;\n+    for (FieldSpec fieldSpec : schema.getAllFieldSpecs()) {\n+      if (!fieldSpec.isVirtualColumn()) {\n+        String name = fieldSpec.getName();\n+        if (fieldSpec.getFieldType().equals(FieldSpec.FieldType.METRIC)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 68}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNTIyNTUwOnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/ValueAggregator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMDozNjoyNVrOHOyfdw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQyMTo1OToyNFrOHQHMpQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2OTM2Nw==", "bodyText": "Not introduced in this PR, but we should not pass in MetricFieldSpec for every aggregate() call. Instead, we should set it in constructor or add an init() method", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485269367", "createdAt": "2020-09-09T00:36:25Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/ValueAggregator.java", "diffHunk": "@@ -16,7 +16,7 @@\n  * specific language governing permissions and limitations\n  * under the License.\n  */\n-package org.apache.pinot.core.minion.rollup.aggregate;\n+package org.apache.pinot.core.segment.processing.collector;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjY1NzE4OQ==", "bodyText": "Changed.", "url": "https://github.com/apache/pinot/pull/5934#discussion_r486657189", "createdAt": "2020-09-10T21:59:24Z", "author": {"login": "npawar"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/ValueAggregator.java", "diffHunk": "@@ -16,7 +16,7 @@\n  * specific language governing permissions and limitations\n  * under the License.\n  */\n-package org.apache.pinot.core.minion.rollup.aggregate;\n+package org.apache.pinot.core.segment.processing.collector;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2OTM2Nw=="}, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNTIyODMxOnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/CollectorConfig.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMDozNzo1OFrOHOyhFw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMDozNzo1OFrOHOyhFw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2OTc4Mw==", "bodyText": "Should this be nullable as well?", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485269783", "createdAt": "2020-09-09T00:37:58Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/CollectorConfig.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import com.fasterxml.jackson.databind.annotation.JsonDeserialize;\n+import com.fasterxml.jackson.databind.annotation.JsonPOJOBuilder;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+\n+\n+/**\n+ * Config for Collector\n+ */\n+@JsonDeserialize(builder = CollectorConfig.Builder.class)\n+public class CollectorConfig {\n+  private static final CollectorFactory.CollectorType DEFAULT_COLLECTOR_TYPE = CollectorFactory.CollectorType.CONCAT;\n+\n+  private final CollectorFactory.CollectorType _collectorType;\n+  private final Map<String, ValueAggregatorFactory.ValueAggregatorType> _aggregatorTypeMap;\n+  private final List<String> _sortOrder;\n+\n+  private CollectorConfig(CollectorFactory.CollectorType collectorType,\n+      Map<String, ValueAggregatorFactory.ValueAggregatorType> aggregatorTypeMap, List<String> sortOrder) {\n+    _collectorType = collectorType;\n+    _aggregatorTypeMap = aggregatorTypeMap;\n+    _sortOrder = sortOrder;\n+  }\n+\n+  /**\n+   * The type of the Collector\n+   */\n+  public CollectorFactory.CollectorType getCollectorType() {\n+    return _collectorType;\n+  }\n+\n+  /**\n+   * Map containing aggregation types for the metrics\n+   */\n+  @Nullable\n+  public Map<String, ValueAggregatorFactory.ValueAggregatorType> getAggregatorTypeMap() {\n+    return _aggregatorTypeMap;\n+  }\n+\n+  /**\n+   * The columns on which to sort\n+   */\n+  public List<String> getSortOrder() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNTI0NjUwOnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/Collector.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMDo0ODoxN1rOHOyrtQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQyMjo1NToyN1rOHQIdUw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI3MjUwMQ==", "bodyText": "Should we work on GenericRecord (Avro object) instead of GenericRow (Pinot object)? We are converting them back and forth right now", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485272501", "createdAt": "2020-09-09T00:48:17Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/Collector.java", "diffHunk": "@@ -0,0 +1,55 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import java.util.Iterator;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * Collects and stores GenericRows\n+ */\n+public interface Collector {\n+\n+  /**\n+   * Collects the given GenericRow and stores it\n+   * @param genericRow the generic row to add to the collection\n+   */\n+  void collect(GenericRow genericRow);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTk2NTA2NA==", "bodyText": "I personally prefer GenericRow to allow the extension of using file formats other than Avro.", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485965064", "createdAt": "2020-09-09T22:53:09Z", "author": {"login": "snleee"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/Collector.java", "diffHunk": "@@ -0,0 +1,55 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import java.util.Iterator;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * Collects and stores GenericRows\n+ */\n+public interface Collector {\n+\n+  /**\n+   * Collects the given GenericRow and stores it\n+   * @param genericRow the generic row to add to the collection\n+   */\n+  void collect(GenericRow genericRow);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI3MjUwMQ=="}, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjY3Nzg0Mw==", "bodyText": "I also felt that GenericRow is better. I was aiming for consistency between Mapper and Reducer. Seunghyun's point also makes sense.", "url": "https://github.com/apache/pinot/pull/5934#discussion_r486677843", "createdAt": "2020-09-10T22:55:27Z", "author": {"login": "npawar"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/Collector.java", "diffHunk": "@@ -0,0 +1,55 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import java.util.Iterator;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * Collects and stores GenericRows\n+ */\n+public interface Collector {\n+\n+  /**\n+   * Collects the given GenericRow and stores it\n+   * @param genericRow the generic row to add to the collection\n+   */\n+  void collect(GenericRow genericRow);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI3MjUwMQ=="}, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNTMzMzM3OnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentMapper.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMTozNToxNFrOHOzdWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMTozNToxNFrOHOzdWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI4NTIwOA==", "bodyText": "Use _partitionToDataFileWriterMap.get(partition) and check if the value is null to save one map lookup", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485285208", "createdAt": "2020-09-09T01:35:14Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentMapper.java", "diffHunk": "@@ -0,0 +1,135 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.framework;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.avro.Schema;\n+import org.apache.avro.file.DataFileWriter;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.pinot.core.data.readers.PinotSegmentRecordReader;\n+import org.apache.pinot.core.segment.processing.filter.RecordFilter;\n+import org.apache.pinot.core.segment.processing.filter.RecordFilterFactory;\n+import org.apache.pinot.core.segment.processing.partitioner.Partitioner;\n+import org.apache.pinot.core.segment.processing.partitioner.PartitionerFactory;\n+import org.apache.pinot.core.segment.processing.transformer.RecordTransformer;\n+import org.apache.pinot.core.segment.processing.transformer.RecordTransformerFactory;\n+import org.apache.pinot.core.segment.processing.utils.SegmentProcessorUtils;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Mapper phase of the SegmentProcessorFramework.\n+ * Reads the input segment and creates partitioned avro data files\n+ * Performs:\n+ * - record transformations\n+ * - partitioning\n+ * - partition filtering\n+ */\n+public class SegmentMapper {\n+\n+  private static final Logger LOGGER = LoggerFactory.getLogger(SegmentMapper.class);\n+  private final File _inputSegment;\n+  private final File _mapperOutputDir;\n+\n+  private final String _mapperId;\n+  private final Schema _avroSchema;\n+  private final RecordTransformer _recordTransformer;\n+  private final RecordFilter _recordFilter;\n+  private final Partitioner _partitioner;\n+  private final Map<String, DataFileWriter<GenericData.Record>> _partitionToDataFileWriterMap = new HashMap<>();\n+\n+  public SegmentMapper(String mapperId, File inputSegment, SegmentMapperConfig mapperConfig, File mapperOutputDir) {\n+    _inputSegment = inputSegment;\n+    _mapperOutputDir = mapperOutputDir;\n+\n+    _mapperId = mapperId;\n+    _avroSchema = SegmentProcessorUtils.convertPinotSchemaToAvroSchema(mapperConfig.getPinotSchema());\n+    _recordTransformer = RecordTransformerFactory.getRecordTransformer(mapperConfig.getRecordTransformerConfig());\n+    _recordFilter = RecordFilterFactory.getRecordFilter(mapperConfig.getRecordFilterConfig());\n+    _partitioner = PartitionerFactory.getPartitioner(mapperConfig.getPartitioningConfig());\n+    LOGGER.info(\n+        \"Initialized mapper with id: {}, input segment: {}, output dir: {}, recordTransformer: {}, recordFilter: {}, partitioner: {}\",\n+        _mapperId, _inputSegment, _mapperOutputDir, _recordTransformer.getClass(), _recordFilter.getClass(),\n+        _partitioner.getClass());\n+  }\n+\n+  /**\n+   * Reads the input segment and generates partitioned avro data files into the mapper output directory\n+   * Records for each partition are put into a directory of its own withing the mapper output directory, identified by the partition name\n+   */\n+  public void map()\n+      throws Exception {\n+\n+    PinotSegmentRecordReader segmentRecordReader = new PinotSegmentRecordReader(_inputSegment);\n+    GenericRow reusableRow = new GenericRow();\n+    GenericData.Record reusableRecord = new GenericData.Record(_avroSchema);\n+\n+    while (segmentRecordReader.hasNext()) {\n+      reusableRow = segmentRecordReader.next(reusableRow);\n+\n+      // Record transformation\n+      reusableRow = _recordTransformer.transformRecord(reusableRow);\n+\n+      // Record filtering\n+      if (_recordFilter.filter(reusableRow)) {\n+        continue;\n+      }\n+\n+      // Partitioning\n+      String partition = _partitioner.getPartition(reusableRow);\n+\n+      // Create writer for the partition, if not exists\n+      if (!_partitionToDataFileWriterMap.containsKey(partition)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 106}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNTM0MzQzOnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentProcessorConfig.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMTozODo0NlrOHOzjVg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQyMjo0MToxOVrOHQILQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI4Njc0Mg==", "bodyText": "Suggest leaving them as null if not set", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485286742", "createdAt": "2020-09-09T01:38:46Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentProcessorConfig.java", "diffHunk": "@@ -0,0 +1,181 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.framework;\n+\n+import com.google.common.base.Preconditions;\n+import org.apache.pinot.core.segment.processing.collector.CollectorConfig;\n+import org.apache.pinot.core.segment.processing.filter.RecordFilterConfig;\n+import org.apache.pinot.core.segment.processing.partitioner.PartitioningConfig;\n+import org.apache.pinot.core.segment.processing.transformer.RecordTransformerConfig;\n+import org.apache.pinot.spi.config.table.TableConfig;\n+import org.apache.pinot.spi.data.Schema;\n+\n+\n+/**\n+ * Config for configuring the phases of {@link SegmentProcessorFramework}\n+ */\n+public class SegmentProcessorConfig {\n+\n+  private final TableConfig _tableConfig;\n+  private final Schema _schema;\n+  private final RecordTransformerConfig _recordTransformerConfig;\n+  private final RecordFilterConfig _recordFilterConfig;\n+  private final PartitioningConfig _partitioningConfig;\n+  private final CollectorConfig _collectorConfig;\n+  private final SegmentConfig _segmentConfig;\n+\n+  private SegmentProcessorConfig(TableConfig tableConfig, Schema schema,\n+      RecordTransformerConfig recordTransformerConfig, RecordFilterConfig recordFilterConfig,\n+      PartitioningConfig partitioningConfig, CollectorConfig collectorConfig, SegmentConfig segmentConfig) {\n+    _tableConfig = tableConfig;\n+    _schema = schema;\n+    _recordTransformerConfig = recordTransformerConfig;\n+    _recordFilterConfig = recordFilterConfig;\n+    _partitioningConfig = partitioningConfig;\n+    _collectorConfig = collectorConfig;\n+    _segmentConfig = segmentConfig;\n+  }\n+\n+  /**\n+   * The Pinot table config\n+   */\n+  public TableConfig getTableConfig() {\n+    return _tableConfig;\n+  }\n+\n+  /**\n+   * The Pinot schema\n+   */\n+  public Schema getSchema() {\n+    return _schema;\n+  }\n+\n+  /**\n+   * The RecordTransformerConfig for the SegmentProcessorFramework's map phase\n+   */\n+  public RecordTransformerConfig getRecordTransformerConfig() {\n+    return _recordTransformerConfig;\n+  }\n+\n+  /**\n+   * The RecordFilterConfig to filter records\n+   */\n+  public RecordFilterConfig getRecordFilterConfig() {\n+    return _recordFilterConfig;\n+  }\n+\n+  /**\n+   * The PartitioningConfig for the SegmentProcessorFramework's map phase\n+   */\n+  public PartitioningConfig getPartitioningConfig() {\n+    return _partitioningConfig;\n+  }\n+\n+  /**\n+   * The CollectorConfig for the SegmentProcessorFramework's reduce phase\n+   */\n+  public CollectorConfig getCollectorConfig() {\n+    return _collectorConfig;\n+  }\n+\n+  /**\n+   * The SegmentConfig for the SegmentProcessorFramework's segment generation phase\n+   */\n+  public SegmentConfig getSegmentConfig() {\n+    return _segmentConfig;\n+  }\n+\n+  /**\n+   * Builder for SegmentProcessorConfig\n+   */\n+  public static class Builder {\n+    private TableConfig tableConfig;\n+    private Schema schema;\n+    private RecordTransformerConfig recordTransformerConfig;\n+    private RecordFilterConfig recordFilterConfig;\n+    private PartitioningConfig partitioningConfig;\n+    private CollectorConfig collectorConfig;\n+    private SegmentConfig _segmentConfig;\n+\n+    public Builder setTableConfig(TableConfig tableConfig) {\n+      this.tableConfig = tableConfig;\n+      return this;\n+    }\n+\n+    public Builder setSchema(Schema schema) {\n+      this.schema = schema;\n+      return this;\n+    }\n+\n+    public Builder setRecordTransformerConfig(RecordTransformerConfig recordTransformerConfig) {\n+      this.recordTransformerConfig = recordTransformerConfig;\n+      return this;\n+    }\n+\n+    public Builder setRecordFilterConfig(RecordFilterConfig recordFilterConfig) {\n+      this.recordFilterConfig = recordFilterConfig;\n+      return this;\n+    }\n+\n+    public Builder setPartitioningConfig(PartitioningConfig partitioningConfig) {\n+      this.partitioningConfig = partitioningConfig;\n+      return this;\n+    }\n+\n+    public Builder setCollectorConfig(CollectorConfig collectorConfig) {\n+      this.collectorConfig = collectorConfig;\n+      return this;\n+    }\n+\n+    public Builder setSegmentConfig(SegmentConfig segmentConfig) {\n+      this._segmentConfig = segmentConfig;\n+      return this;\n+    }\n+\n+    public SegmentProcessorConfig build() {\n+      Preconditions.checkNotNull(tableConfig, \"Must provide table config in SegmentProcessorConfig\");\n+      Preconditions.checkNotNull(schema, \"Must provide schema in SegmentProcessorConfig\");\n+      if (recordTransformerConfig == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 154}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjY3MzIxNw==", "bodyText": "i just wanted to avoid checking null before using them in the mapper and reducer", "url": "https://github.com/apache/pinot/pull/5934#discussion_r486673217", "createdAt": "2020-09-10T22:41:19Z", "author": {"login": "npawar"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentProcessorConfig.java", "diffHunk": "@@ -0,0 +1,181 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.framework;\n+\n+import com.google.common.base.Preconditions;\n+import org.apache.pinot.core.segment.processing.collector.CollectorConfig;\n+import org.apache.pinot.core.segment.processing.filter.RecordFilterConfig;\n+import org.apache.pinot.core.segment.processing.partitioner.PartitioningConfig;\n+import org.apache.pinot.core.segment.processing.transformer.RecordTransformerConfig;\n+import org.apache.pinot.spi.config.table.TableConfig;\n+import org.apache.pinot.spi.data.Schema;\n+\n+\n+/**\n+ * Config for configuring the phases of {@link SegmentProcessorFramework}\n+ */\n+public class SegmentProcessorConfig {\n+\n+  private final TableConfig _tableConfig;\n+  private final Schema _schema;\n+  private final RecordTransformerConfig _recordTransformerConfig;\n+  private final RecordFilterConfig _recordFilterConfig;\n+  private final PartitioningConfig _partitioningConfig;\n+  private final CollectorConfig _collectorConfig;\n+  private final SegmentConfig _segmentConfig;\n+\n+  private SegmentProcessorConfig(TableConfig tableConfig, Schema schema,\n+      RecordTransformerConfig recordTransformerConfig, RecordFilterConfig recordFilterConfig,\n+      PartitioningConfig partitioningConfig, CollectorConfig collectorConfig, SegmentConfig segmentConfig) {\n+    _tableConfig = tableConfig;\n+    _schema = schema;\n+    _recordTransformerConfig = recordTransformerConfig;\n+    _recordFilterConfig = recordFilterConfig;\n+    _partitioningConfig = partitioningConfig;\n+    _collectorConfig = collectorConfig;\n+    _segmentConfig = segmentConfig;\n+  }\n+\n+  /**\n+   * The Pinot table config\n+   */\n+  public TableConfig getTableConfig() {\n+    return _tableConfig;\n+  }\n+\n+  /**\n+   * The Pinot schema\n+   */\n+  public Schema getSchema() {\n+    return _schema;\n+  }\n+\n+  /**\n+   * The RecordTransformerConfig for the SegmentProcessorFramework's map phase\n+   */\n+  public RecordTransformerConfig getRecordTransformerConfig() {\n+    return _recordTransformerConfig;\n+  }\n+\n+  /**\n+   * The RecordFilterConfig to filter records\n+   */\n+  public RecordFilterConfig getRecordFilterConfig() {\n+    return _recordFilterConfig;\n+  }\n+\n+  /**\n+   * The PartitioningConfig for the SegmentProcessorFramework's map phase\n+   */\n+  public PartitioningConfig getPartitioningConfig() {\n+    return _partitioningConfig;\n+  }\n+\n+  /**\n+   * The CollectorConfig for the SegmentProcessorFramework's reduce phase\n+   */\n+  public CollectorConfig getCollectorConfig() {\n+    return _collectorConfig;\n+  }\n+\n+  /**\n+   * The SegmentConfig for the SegmentProcessorFramework's segment generation phase\n+   */\n+  public SegmentConfig getSegmentConfig() {\n+    return _segmentConfig;\n+  }\n+\n+  /**\n+   * Builder for SegmentProcessorConfig\n+   */\n+  public static class Builder {\n+    private TableConfig tableConfig;\n+    private Schema schema;\n+    private RecordTransformerConfig recordTransformerConfig;\n+    private RecordFilterConfig recordFilterConfig;\n+    private PartitioningConfig partitioningConfig;\n+    private CollectorConfig collectorConfig;\n+    private SegmentConfig _segmentConfig;\n+\n+    public Builder setTableConfig(TableConfig tableConfig) {\n+      this.tableConfig = tableConfig;\n+      return this;\n+    }\n+\n+    public Builder setSchema(Schema schema) {\n+      this.schema = schema;\n+      return this;\n+    }\n+\n+    public Builder setRecordTransformerConfig(RecordTransformerConfig recordTransformerConfig) {\n+      this.recordTransformerConfig = recordTransformerConfig;\n+      return this;\n+    }\n+\n+    public Builder setRecordFilterConfig(RecordFilterConfig recordFilterConfig) {\n+      this.recordFilterConfig = recordFilterConfig;\n+      return this;\n+    }\n+\n+    public Builder setPartitioningConfig(PartitioningConfig partitioningConfig) {\n+      this.partitioningConfig = partitioningConfig;\n+      return this;\n+    }\n+\n+    public Builder setCollectorConfig(CollectorConfig collectorConfig) {\n+      this.collectorConfig = collectorConfig;\n+      return this;\n+    }\n+\n+    public Builder setSegmentConfig(SegmentConfig segmentConfig) {\n+      this._segmentConfig = segmentConfig;\n+      return this;\n+    }\n+\n+    public SegmentProcessorConfig build() {\n+      Preconditions.checkNotNull(tableConfig, \"Must provide table config in SegmentProcessorConfig\");\n+      Preconditions.checkNotNull(schema, \"Must provide schema in SegmentProcessorConfig\");\n+      if (recordTransformerConfig == null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI4Njc0Mg=="}, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 154}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNTM1MTQ0OnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/utils/SegmentProcessorUtils.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMTo0MzowNlrOHOzoBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQxNjo0NjoyMlrOHSKwzQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI4Nzk0MQ==", "bodyText": "Put null values from GenericRow.getNullValueFields()?", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485287941", "createdAt": "2020-09-09T01:43:06Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/utils/SegmentProcessorUtils.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.utils;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * Helper util methods for SegmentProcessorFramework\n+ */\n+public final class SegmentProcessorUtils {\n+\n+  private SegmentProcessorUtils() {\n+  }\n+\n+  /**\n+   * Convert a GenericRow to an avro GenericRecord\n+   */\n+  public static GenericData.Record convertGenericRowToAvroRecord(GenericRow genericRow,\n+      GenericData.Record reusableRecord) {\n+    for (String field : genericRow.getFieldToValueMap().keySet()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjY3Njg3OA==", "bodyText": "I didn't follow. Aren't all nullValueFields already expected to be in fieldToValueMap ?", "url": "https://github.com/apache/pinot/pull/5934#discussion_r486676878", "createdAt": "2020-09-10T22:52:28Z", "author": {"login": "npawar"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/utils/SegmentProcessorUtils.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.utils;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * Helper util methods for SegmentProcessorFramework\n+ */\n+public final class SegmentProcessorUtils {\n+\n+  private SegmentProcessorUtils() {\n+  }\n+\n+  /**\n+   * Convert a GenericRow to an avro GenericRecord\n+   */\n+  public static GenericData.Record convertGenericRowToAvroRecord(GenericRow genericRow,\n+      GenericData.Record reusableRecord) {\n+    for (String field : genericRow.getFieldToValueMap().keySet()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI4Nzk0MQ=="}, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMzNzQyMQ==", "bodyText": "The value in fieldToValueMap should be the default value if the original value is null. IMO we should put null instead of default value in the GenericData.Record so that the null values can be populated to the new segment", "url": "https://github.com/apache/pinot/pull/5934#discussion_r488337421", "createdAt": "2020-09-15T02:02:09Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/utils/SegmentProcessorUtils.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.utils;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * Helper util methods for SegmentProcessorFramework\n+ */\n+public final class SegmentProcessorUtils {\n+\n+  private SegmentProcessorUtils() {\n+  }\n+\n+  /**\n+   * Convert a GenericRow to an avro GenericRecord\n+   */\n+  public static GenericData.Record convertGenericRowToAvroRecord(GenericRow genericRow,\n+      GenericData.Record reusableRecord) {\n+    for (String field : genericRow.getFieldToValueMap().keySet()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI4Nzk0MQ=="}, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODgxMjc0OQ==", "bodyText": "I see, got it. In the interest of getting this long standing and big PR merged, I will think about this and take it up immediately in a following PR.", "url": "https://github.com/apache/pinot/pull/5934#discussion_r488812749", "createdAt": "2020-09-15T16:46:22Z", "author": {"login": "npawar"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/utils/SegmentProcessorUtils.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.utils;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * Helper util methods for SegmentProcessorFramework\n+ */\n+public final class SegmentProcessorUtils {\n+\n+  private SegmentProcessorUtils() {\n+  }\n+\n+  /**\n+   * Convert a GenericRow to an avro GenericRecord\n+   */\n+  public static GenericData.Record convertGenericRowToAvroRecord(GenericRow genericRow,\n+      GenericData.Record reusableRecord) {\n+    for (String field : genericRow.getFieldToValueMap().keySet()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI4Nzk0MQ=="}, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzOTY5NjU3OnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/RollupCollector.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQyMzowNToxMFrOHPdMlg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQyMzoxMzoyN1rOHQIzcQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTk2OTA0Ng==", "bodyText": "I guess that we basically keep the entire data for a segment on JVM heap?\nIn the future, we may need to add the off-heap or file-based collector to avoid OOM error when reading large segments. (e.g. 1-2gb Pinot segment can be extremely large in row format)\nAnother way to save memory is to sort the data on all dimensions and scan at once for aggregation (but this paying a large cost for cases when the data doesn't need to be sorted)", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485969046", "createdAt": "2020-09-09T23:05:10Z", "author": {"login": "snleee"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/RollupCollector.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.MetricFieldSpec;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * A Collector that rolls up the incoming records on unique dimensions + time columns, based on provided aggregation types for metrics.\n+ * By default will use the SUM aggregation on metrics.\n+ */\n+public class RollupCollector implements Collector {\n+\n+  private final Map<Record, GenericRow> _collection = new HashMap<>();\n+  private Iterator<GenericRow> _iterator;\n+  private GenericRowSorter _sorter;\n+\n+  private final int _keySize;\n+  private final int _valueSize;\n+  private final String[] _keyColumns;\n+  private final String[] _valueColumns;\n+  private final ValueAggregator[] _valueAggregators;\n+  private final MetricFieldSpec[] _metricFieldSpecs;\n+\n+  public RollupCollector(CollectorConfig collectorConfig, Schema schema) {\n+    _keySize = schema.getPhysicalColumnNames().size() - schema.getMetricNames().size();\n+    _valueSize = schema.getMetricNames().size();\n+    _keyColumns = new String[_keySize];\n+    _valueColumns = new String[_valueSize];\n+    _valueAggregators = new ValueAggregator[_valueSize];\n+    _metricFieldSpecs = new MetricFieldSpec[_valueSize];\n+\n+    Map<String, ValueAggregatorFactory.ValueAggregatorType> aggregatorTypeMap = collectorConfig.getAggregatorTypeMap();\n+    if (aggregatorTypeMap == null) {\n+      aggregatorTypeMap = Collections.emptyMap();\n+    }\n+    int valIdx = 0;\n+    int keyIdx = 0;\n+    for (FieldSpec fieldSpec : schema.getAllFieldSpecs()) {\n+      if (!fieldSpec.isVirtualColumn()) {\n+        String name = fieldSpec.getName();\n+        if (fieldSpec.getFieldType().equals(FieldSpec.FieldType.METRIC)) {\n+          _metricFieldSpecs[valIdx] = (MetricFieldSpec) fieldSpec;\n+          _valueColumns[valIdx] = name;\n+          _valueAggregators[valIdx] = ValueAggregatorFactory.getValueAggregator(\n+              aggregatorTypeMap.getOrDefault(name, ValueAggregatorFactory.ValueAggregatorType.SUM).toString());\n+          valIdx++;\n+        } else {\n+          _keyColumns[keyIdx++] = name;\n+        }\n+      }\n+    }\n+\n+    List<String> sortOrder = collectorConfig.getSortOrder();\n+    if (sortOrder.size() > 0) {\n+      _sorter = new GenericRowSorter(sortOrder, schema);\n+    }\n+  }\n+\n+  /**\n+   * If a row already exists in the collection (based on dimension + time columns), rollup the metric values, else add the row\n+   */\n+  @Override\n+  public void collect(GenericRow genericRow) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjY4MzUwNQ==", "bodyText": "Yes, we keep the entire data of a partition on heap.\nYes, we can certainly add off-heap or file-based in future. For now, we have a knob maxRecordsPerSegment that can help control number of records collected in memory.\nIn order to sort the entire data on all dimensions+time, we'll still have to get all the data into memory right? And then we'll have to rewrite the files onto disk. The input is raw avro file. We do not have any dictionaries or docIds.", "url": "https://github.com/apache/pinot/pull/5934#discussion_r486683505", "createdAt": "2020-09-10T23:13:27Z", "author": {"login": "npawar"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/RollupCollector.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.MetricFieldSpec;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * A Collector that rolls up the incoming records on unique dimensions + time columns, based on provided aggregation types for metrics.\n+ * By default will use the SUM aggregation on metrics.\n+ */\n+public class RollupCollector implements Collector {\n+\n+  private final Map<Record, GenericRow> _collection = new HashMap<>();\n+  private Iterator<GenericRow> _iterator;\n+  private GenericRowSorter _sorter;\n+\n+  private final int _keySize;\n+  private final int _valueSize;\n+  private final String[] _keyColumns;\n+  private final String[] _valueColumns;\n+  private final ValueAggregator[] _valueAggregators;\n+  private final MetricFieldSpec[] _metricFieldSpecs;\n+\n+  public RollupCollector(CollectorConfig collectorConfig, Schema schema) {\n+    _keySize = schema.getPhysicalColumnNames().size() - schema.getMetricNames().size();\n+    _valueSize = schema.getMetricNames().size();\n+    _keyColumns = new String[_keySize];\n+    _valueColumns = new String[_valueSize];\n+    _valueAggregators = new ValueAggregator[_valueSize];\n+    _metricFieldSpecs = new MetricFieldSpec[_valueSize];\n+\n+    Map<String, ValueAggregatorFactory.ValueAggregatorType> aggregatorTypeMap = collectorConfig.getAggregatorTypeMap();\n+    if (aggregatorTypeMap == null) {\n+      aggregatorTypeMap = Collections.emptyMap();\n+    }\n+    int valIdx = 0;\n+    int keyIdx = 0;\n+    for (FieldSpec fieldSpec : schema.getAllFieldSpecs()) {\n+      if (!fieldSpec.isVirtualColumn()) {\n+        String name = fieldSpec.getName();\n+        if (fieldSpec.getFieldType().equals(FieldSpec.FieldType.METRIC)) {\n+          _metricFieldSpecs[valIdx] = (MetricFieldSpec) fieldSpec;\n+          _valueColumns[valIdx] = name;\n+          _valueAggregators[valIdx] = ValueAggregatorFactory.getValueAggregator(\n+              aggregatorTypeMap.getOrDefault(name, ValueAggregatorFactory.ValueAggregatorType.SUM).toString());\n+          valIdx++;\n+        } else {\n+          _keyColumns[keyIdx++] = name;\n+        }\n+      }\n+    }\n+\n+    List<String> sortOrder = collectorConfig.getSortOrder();\n+    if (sortOrder.size() > 0) {\n+      _sorter = new GenericRowSorter(sortOrder, schema);\n+    }\n+  }\n+\n+  /**\n+   * If a row already exists in the collection (based on dimension + time columns), rollup the metric values, else add the row\n+   */\n+  @Override\n+  public void collect(GenericRow genericRow) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTk2OTA0Ng=="}, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 90}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzOTk0Mzk4OnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/partitioner/NoOpPartitioner.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQwMToxNDozN1rOHPfbAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQyMzoxNDozM1rOHQI0wQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjAwNTUwNQ==", "bodyText": "No-op partitioner means that we always create a single output file?", "url": "https://github.com/apache/pinot/pull/5934#discussion_r486005505", "createdAt": "2020-09-10T01:14:37Z", "author": {"login": "snleee"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/partitioner/NoOpPartitioner.java", "diffHunk": "@@ -0,0 +1,32 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.partitioner;\n+\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * Partitioner implementation which always returns constant partition value \"0\"\n+ */\n+public class NoOpPartitioner implements Partitioner {\n+  @Override\n+  public String getPartition(GenericRow genericRow) {\n+    return \"0\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjY4Mzg0MQ==", "bodyText": "That is correct. A single output from each Mapper. The reducer will break that data as per maxRecordsPerSegment", "url": "https://github.com/apache/pinot/pull/5934#discussion_r486683841", "createdAt": "2020-09-10T23:14:33Z", "author": {"login": "npawar"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/partitioner/NoOpPartitioner.java", "diffHunk": "@@ -0,0 +1,32 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.partitioner;\n+\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * Partitioner implementation which always returns constant partition value \"0\"\n+ */\n+public class NoOpPartitioner implements Partitioner {\n+  @Override\n+  public String getPartition(GenericRow genericRow) {\n+    return \"0\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjAwNTUwNQ=="}, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzOTk0OTEzOnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/partitioner/ColumnValuePartitioner.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQwMToxNzozMlrOHPfeAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQyMzozNzo0NlrOHQJP2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjAwNjI3Mw==", "bodyText": "Is this intended for supporting time alignment?\nWhat if the time column granularity is in seconds/hours while push frequency is DAY?\nIn that case, we may need to use TransformationPartitioner?", "url": "https://github.com/apache/pinot/pull/5934#discussion_r486006273", "createdAt": "2020-09-10T01:17:32Z", "author": {"login": "snleee"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/partitioner/ColumnValuePartitioner.java", "diffHunk": "@@ -0,0 +1,39 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.partitioner;\n+\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * Partitioner which extracts a column value as the partition\n+ */\n+public class ColumnValuePartitioner implements Partitioner {\n+\n+  private final String _columnName;\n+\n+  public ColumnValuePartitioner(String columnName) {\n+    _columnName = columnName;\n+  }\n+\n+  @Override\n+  public String getPartition(GenericRow genericRow) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjY5MDc3OA==", "bodyText": "Not particularly written for supporting time alignment, but can be used for that if values are to be used as is for partitioning.\nIf time column is seconds/hours, but we want to align by day, yes we can use TransformFunctionPartitioner.", "url": "https://github.com/apache/pinot/pull/5934#discussion_r486690778", "createdAt": "2020-09-10T23:37:46Z", "author": {"login": "npawar"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/partitioner/ColumnValuePartitioner.java", "diffHunk": "@@ -0,0 +1,39 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.partitioner;\n+\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * Partitioner which extracts a column value as the partition\n+ */\n+public class ColumnValuePartitioner implements Partitioner {\n+\n+  private final String _columnName;\n+\n+  public ColumnValuePartitioner(String columnName) {\n+    _columnName = columnName;\n+  }\n+\n+  @Override\n+  public String getPartition(GenericRow genericRow) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjAwNjI3Mw=="}, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzOTk1MjEyOnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/partitioner/TableConfigPartitioner.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQwMToxOToyOFrOHPff4g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQyMzo0NToxMFrOHQJYKg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjAwNjc1NA==", "bodyText": "What if I need to align data on time while the table is custom partitioned? (just trying to brainstorm how we will extend the current partitioner to support this)\nThen, we can probably add the new partitioner that combines the value from TableConfigPartitioner and TransformationPartitioner?\ne.g.  partition on memberId using murmur, need to enable segment merge so the data needs to be time aligned.\n\nUse table config partitioner to get the partition id based on murmur on memberId -> Let's day 2\nUse time align partitioner -> Let's say 2020/12/12\n\nCombine 1&2 -> 2020/12/12-2 <- example of partitionId\nWe can do something like the above?", "url": "https://github.com/apache/pinot/pull/5934#discussion_r486006754", "createdAt": "2020-09-10T01:19:28Z", "author": {"login": "snleee"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/partitioner/TableConfigPartitioner.java", "diffHunk": "@@ -0,0 +1,45 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.partitioner;\n+\n+import org.apache.pinot.core.data.partition.PartitionFunction;\n+import org.apache.pinot.core.data.partition.PartitionFunctionFactory;\n+import org.apache.pinot.spi.config.table.ColumnPartitionConfig;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * Partitioner which computes partition values based on the ColumnPartitionConfig from the table config\n+ */\n+public class TableConfigPartitioner implements Partitioner {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjY5MjkwNg==", "bodyText": "this is a good point. I did not think of this. Yes we can add new partitioner in that case, which combines the partitioners as needed.\nBut this might be a common case, so I'm thinking if we should have that in the design itself. How about we always do partitioning in 2 steps inside the mapper.\n\nApply any partitioning from Segment Processor Config\nApply any partitioning from Table Config.\nSo in your example, first TransformFunctionPartitioner gets applied and generated date partition. Then, we check if table config partitioner exists, and if yes, further apply the PartitionFunction. Then concat the 2 partitions like you suggested.\n\nAnother option is we make PartitionerConfig a list, and apply all partitioners one by one, and concat all to get final partition.", "url": "https://github.com/apache/pinot/pull/5934#discussion_r486692906", "createdAt": "2020-09-10T23:45:10Z", "author": {"login": "npawar"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/partitioner/TableConfigPartitioner.java", "diffHunk": "@@ -0,0 +1,45 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.partitioner;\n+\n+import org.apache.pinot.core.data.partition.PartitionFunction;\n+import org.apache.pinot.core.data.partition.PartitionFunctionFactory;\n+import org.apache.pinot.spi.config.table.ColumnPartitionConfig;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * Partitioner which computes partition values based on the ColumnPartitionConfig from the table config\n+ */\n+public class TableConfigPartitioner implements Partitioner {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjAwNjc1NA=="}, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzOTk2ODM1OnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentProcessorConfig.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQwMToyODowOFrOHPfpEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMVQwMDoxMDoxNlrOHQJzkg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjAwOTEwNg==", "bodyText": "One requirement for SegmentMergeRollup is to be able to put the custom name for the segment name (or at least need to put the prefix and the sequenced merged_XXX_0...M Where do you think it's the best place to configure those?\nYour segment framework also faces the same issue with the sequence id. So, the sequence id should be handled implicitly by the framework at least. And, we can probably add the config for the prefix.", "url": "https://github.com/apache/pinot/pull/5934#discussion_r486009106", "createdAt": "2020-09-10T01:28:08Z", "author": {"login": "snleee"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentProcessorConfig.java", "diffHunk": "@@ -0,0 +1,181 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.framework;\n+\n+import com.google.common.base.Preconditions;\n+import org.apache.pinot.core.segment.processing.collector.CollectorConfig;\n+import org.apache.pinot.core.segment.processing.filter.RecordFilterConfig;\n+import org.apache.pinot.core.segment.processing.partitioner.PartitioningConfig;\n+import org.apache.pinot.core.segment.processing.transformer.RecordTransformerConfig;\n+import org.apache.pinot.spi.config.table.TableConfig;\n+import org.apache.pinot.spi.data.Schema;\n+\n+\n+/**\n+ * Config for configuring the phases of {@link SegmentProcessorFramework}\n+ */\n+public class SegmentProcessorConfig {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjY5OTkyMg==", "bodyText": "These config can go into the SegmentConfig class.\nI will add seqId to the SegmentGenerationConfig in the driver. seqId will also help with the other problem you caught (same segment name if start/end is same).\nConfig for prefix can be added in a future change.", "url": "https://github.com/apache/pinot/pull/5934#discussion_r486699922", "createdAt": "2020-09-11T00:10:16Z", "author": {"login": "npawar"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentProcessorConfig.java", "diffHunk": "@@ -0,0 +1,181 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.framework;\n+\n+import com.google.common.base.Preconditions;\n+import org.apache.pinot.core.segment.processing.collector.CollectorConfig;\n+import org.apache.pinot.core.segment.processing.filter.RecordFilterConfig;\n+import org.apache.pinot.core.segment.processing.partitioner.PartitioningConfig;\n+import org.apache.pinot.core.segment.processing.transformer.RecordTransformerConfig;\n+import org.apache.pinot.spi.config.table.TableConfig;\n+import org.apache.pinot.spi.data.Schema;\n+\n+\n+/**\n+ * Config for configuring the phases of {@link SegmentProcessorFramework}\n+ */\n+public class SegmentProcessorConfig {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjAwOTEwNg=="}, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzOTk3NDc4OnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentProcessorFramework.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQwMTozMTozMVrOHPfsmQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMVQwMDoxMTowNFrOHQJ0gQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjAxMDAwOQ==", "bodyText": "Did you check the output segment names when the output is more than 1 files?\nIt's possible that the final segments may have the same segment name. (e.g. <tablename>_<start>_<end>)", "url": "https://github.com/apache/pinot/pull/5934#discussion_r486010009", "createdAt": "2020-09-10T01:31:31Z", "author": {"login": "snleee"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentProcessorFramework.java", "diffHunk": "@@ -0,0 +1,194 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.framework;\n+\n+import com.google.common.base.Preconditions;\n+import java.io.File;\n+import java.util.Arrays;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.common.utils.TarGzCompressionUtils;\n+import org.apache.pinot.core.indexsegment.generator.SegmentGeneratorConfig;\n+import org.apache.pinot.core.segment.creator.impl.SegmentIndexCreationDriverImpl;\n+import org.apache.pinot.spi.config.table.TableConfig;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.spi.data.readers.FileFormat;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * A framework to process \"m\" given segments and convert them into \"n\" segments\n+ * The phases of the Segment Processor are\n+ * 1. Map - record transformation, partitioning, partition filtering\n+ * 2. Reduce - rollup, concat, split etc\n+ * 3. Segment generation\n+ *\n+ * This will typically be used by minion tasks, which want to perform some processing on segments\n+ * (eg task which merges segments, tasks which aligns segments per time boundaries etc)\n+ */\n+public class SegmentProcessorFramework {\n+\n+  private static final Logger LOGGER = LoggerFactory.getLogger(SegmentProcessorFramework.class);\n+\n+  private final File _inputSegmentsDir;\n+  private final File _outputSegmentsDir;\n+  private final SegmentProcessorConfig _segmentProcessorConfig;\n+\n+  private final Schema _pinotSchema;\n+  private final TableConfig _tableConfig;\n+\n+  private final File _baseDir;\n+  private final File _mapperInputDir;\n+  private final File _mapperOutputDir;\n+  private final File _reducerOutputDir;\n+\n+  /**\n+   * Initializes the Segment Processor framework with input segments, output path and processing config\n+   * @param inputSegmentsDir directory containing the input segments. These can be tarred or untarred.\n+   * @param segmentProcessorConfig config for segment processing\n+   * @param outputSegmentsDir directory for placing the resulting segments. This should already exist.\n+   */\n+  public SegmentProcessorFramework(File inputSegmentsDir, SegmentProcessorConfig segmentProcessorConfig,\n+      File outputSegmentsDir) {\n+\n+    LOGGER.info(\n+        \"Initializing SegmentProcessorFramework with input segments dir: {}, output segments dir: {} and segment processor config: {}\",\n+        inputSegmentsDir.getAbsolutePath(), outputSegmentsDir.getAbsolutePath(), segmentProcessorConfig.toString());\n+\n+    _inputSegmentsDir = inputSegmentsDir;\n+    Preconditions.checkState(_inputSegmentsDir.exists() && _inputSegmentsDir.isDirectory(),\n+        \"Input path: %s must be a directory with Pinot segments\", _inputSegmentsDir.getAbsolutePath());\n+    _outputSegmentsDir = outputSegmentsDir;\n+    Preconditions.checkState(\n+        _outputSegmentsDir.exists() && _outputSegmentsDir.isDirectory() && (_outputSegmentsDir.list().length == 0),\n+        \"Must provide existing empty output directory: %s\", _outputSegmentsDir.getAbsolutePath());\n+\n+    _segmentProcessorConfig = segmentProcessorConfig;\n+    _pinotSchema = segmentProcessorConfig.getSchema();\n+    _tableConfig = segmentProcessorConfig.getTableConfig();\n+\n+    _baseDir = new File(FileUtils.getTempDirectory(), \"segment_processor_\" + System.currentTimeMillis());\n+    FileUtils.deleteQuietly(_baseDir);\n+    Preconditions.checkState(_baseDir.mkdirs(), \"Failed to create base directory: %s for SegmentProcessor\", _baseDir);\n+    _mapperInputDir = new File(_baseDir, \"mapper_input\");\n+    Preconditions\n+        .checkState(_mapperInputDir.mkdirs(), \"Failed to create mapper input directory: %s for SegmentProcessor\",\n+            _mapperInputDir);\n+    _mapperOutputDir = new File(_baseDir, \"mapper_output\");\n+    Preconditions\n+        .checkState(_mapperOutputDir.mkdirs(), \"Failed to create mapper output directory: %s for SegmentProcessor\",\n+            _mapperOutputDir);\n+    _reducerOutputDir = new File(_baseDir, \"reducer_output\");\n+    Preconditions\n+        .checkState(_reducerOutputDir.mkdirs(), \"Failed to create reducer output directory: %s for SegmentProcessor\",\n+            _reducerOutputDir);\n+  }\n+\n+  /**\n+   * Processes segments from the input directory as per the provided configs, then puts resulting segments into the output directory\n+   */\n+  public void processSegments()\n+      throws Exception {\n+\n+    // Check for input segments\n+    File[] segmentFiles = _inputSegmentsDir.listFiles();\n+    if (segmentFiles.length == 0) {\n+      throw new IllegalStateException(\"No segments found in input dir: \" + _inputSegmentsDir.getAbsolutePath()\n+          + \". Exiting SegmentProcessorFramework.\");\n+    }\n+\n+    // Mapper phase.\n+    LOGGER.info(\"Beginning mapper phase. Processing segments: {}\", Arrays.toString(_inputSegmentsDir.list()));\n+    for (File segment : segmentFiles) {\n+\n+      String fileName = segment.getName();\n+      File mapperInput = segment;\n+\n+      // Untar the segments if needed\n+      if (!segment.isDirectory()) {\n+        if (fileName.endsWith(\".tar.gz\") || fileName.endsWith(\".tgz\")) {\n+          mapperInput = TarGzCompressionUtils.untar(segment, _mapperInputDir).get(0);\n+        } else {\n+          throw new IllegalStateException(\"Unsupported segment format: \" + segment.getAbsolutePath());\n+        }\n+      }\n+\n+      // Set mapperId as the name of the segment\n+      SegmentMapperConfig mapperConfig =\n+          new SegmentMapperConfig(_pinotSchema, _segmentProcessorConfig.getRecordTransformerConfig(),\n+              _segmentProcessorConfig.getRecordFilterConfig(), _segmentProcessorConfig.getPartitioningConfig());\n+      SegmentMapper mapper = new SegmentMapper(mapperInput.getName(), mapperInput, mapperConfig, _mapperOutputDir);\n+      mapper.map();\n+      mapper.cleanup();\n+    }\n+\n+    // Check for mapper output files\n+    File[] mapperOutputFiles = _mapperOutputDir.listFiles();\n+    if (mapperOutputFiles.length == 0) {\n+      throw new IllegalStateException(\"No files found in mapper output directory: \" + _mapperOutputDir.getAbsolutePath()\n+          + \". Exiting SegmentProcessorFramework.\");\n+    }\n+\n+    // Reducer phase.\n+    LOGGER.info(\"Beginning reducer phase. Processing files: {}\", Arrays.toString(_mapperOutputDir.list()));\n+    // Mapper output directory has 1 directory per partition, named after the partition. Each directory contains 1 or more avro files.\n+    for (File partDir : mapperOutputFiles) {\n+\n+      // Set partition as reducerId\n+      SegmentReducerConfig reducerConfig =\n+          new SegmentReducerConfig(_pinotSchema, _segmentProcessorConfig.getCollectorConfig(),\n+              _segmentProcessorConfig.getSegmentConfig().getMaxNumRecordsPerSegment());\n+      SegmentReducer reducer = new SegmentReducer(partDir.getName(), partDir, reducerConfig, _reducerOutputDir);\n+      reducer.reduce();\n+      reducer.cleanup();\n+    }\n+\n+    // Check for reducer output files\n+    File[] reducerOutputFiles = _reducerOutputDir.listFiles();\n+    if (reducerOutputFiles.length == 0) {\n+      throw new IllegalStateException(\n+          \"No files found in reducer output directory: \" + _reducerOutputDir.getAbsolutePath()\n+              + \". Exiting SegmentProcessorFramework.\");\n+    }\n+\n+    // Segment generation phase.\n+    LOGGER.info(\"Beginning segment generation phase. Processing files: {}\", Arrays.toString(_reducerOutputDir.list()));\n+    // Reducer output directory will have 1 or more avro files\n+    for (File resultFile : reducerOutputFiles) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 173}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjcwMDE2MQ==", "bodyText": "Good catch. Will add seqId. Also working on adding end-to-end tests for the framework, and will include this case", "url": "https://github.com/apache/pinot/pull/5934#discussion_r486700161", "createdAt": "2020-09-11T00:11:04Z", "author": {"login": "npawar"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentProcessorFramework.java", "diffHunk": "@@ -0,0 +1,194 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.framework;\n+\n+import com.google.common.base.Preconditions;\n+import java.io.File;\n+import java.util.Arrays;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.common.utils.TarGzCompressionUtils;\n+import org.apache.pinot.core.indexsegment.generator.SegmentGeneratorConfig;\n+import org.apache.pinot.core.segment.creator.impl.SegmentIndexCreationDriverImpl;\n+import org.apache.pinot.spi.config.table.TableConfig;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.spi.data.readers.FileFormat;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * A framework to process \"m\" given segments and convert them into \"n\" segments\n+ * The phases of the Segment Processor are\n+ * 1. Map - record transformation, partitioning, partition filtering\n+ * 2. Reduce - rollup, concat, split etc\n+ * 3. Segment generation\n+ *\n+ * This will typically be used by minion tasks, which want to perform some processing on segments\n+ * (eg task which merges segments, tasks which aligns segments per time boundaries etc)\n+ */\n+public class SegmentProcessorFramework {\n+\n+  private static final Logger LOGGER = LoggerFactory.getLogger(SegmentProcessorFramework.class);\n+\n+  private final File _inputSegmentsDir;\n+  private final File _outputSegmentsDir;\n+  private final SegmentProcessorConfig _segmentProcessorConfig;\n+\n+  private final Schema _pinotSchema;\n+  private final TableConfig _tableConfig;\n+\n+  private final File _baseDir;\n+  private final File _mapperInputDir;\n+  private final File _mapperOutputDir;\n+  private final File _reducerOutputDir;\n+\n+  /**\n+   * Initializes the Segment Processor framework with input segments, output path and processing config\n+   * @param inputSegmentsDir directory containing the input segments. These can be tarred or untarred.\n+   * @param segmentProcessorConfig config for segment processing\n+   * @param outputSegmentsDir directory for placing the resulting segments. This should already exist.\n+   */\n+  public SegmentProcessorFramework(File inputSegmentsDir, SegmentProcessorConfig segmentProcessorConfig,\n+      File outputSegmentsDir) {\n+\n+    LOGGER.info(\n+        \"Initializing SegmentProcessorFramework with input segments dir: {}, output segments dir: {} and segment processor config: {}\",\n+        inputSegmentsDir.getAbsolutePath(), outputSegmentsDir.getAbsolutePath(), segmentProcessorConfig.toString());\n+\n+    _inputSegmentsDir = inputSegmentsDir;\n+    Preconditions.checkState(_inputSegmentsDir.exists() && _inputSegmentsDir.isDirectory(),\n+        \"Input path: %s must be a directory with Pinot segments\", _inputSegmentsDir.getAbsolutePath());\n+    _outputSegmentsDir = outputSegmentsDir;\n+    Preconditions.checkState(\n+        _outputSegmentsDir.exists() && _outputSegmentsDir.isDirectory() && (_outputSegmentsDir.list().length == 0),\n+        \"Must provide existing empty output directory: %s\", _outputSegmentsDir.getAbsolutePath());\n+\n+    _segmentProcessorConfig = segmentProcessorConfig;\n+    _pinotSchema = segmentProcessorConfig.getSchema();\n+    _tableConfig = segmentProcessorConfig.getTableConfig();\n+\n+    _baseDir = new File(FileUtils.getTempDirectory(), \"segment_processor_\" + System.currentTimeMillis());\n+    FileUtils.deleteQuietly(_baseDir);\n+    Preconditions.checkState(_baseDir.mkdirs(), \"Failed to create base directory: %s for SegmentProcessor\", _baseDir);\n+    _mapperInputDir = new File(_baseDir, \"mapper_input\");\n+    Preconditions\n+        .checkState(_mapperInputDir.mkdirs(), \"Failed to create mapper input directory: %s for SegmentProcessor\",\n+            _mapperInputDir);\n+    _mapperOutputDir = new File(_baseDir, \"mapper_output\");\n+    Preconditions\n+        .checkState(_mapperOutputDir.mkdirs(), \"Failed to create mapper output directory: %s for SegmentProcessor\",\n+            _mapperOutputDir);\n+    _reducerOutputDir = new File(_baseDir, \"reducer_output\");\n+    Preconditions\n+        .checkState(_reducerOutputDir.mkdirs(), \"Failed to create reducer output directory: %s for SegmentProcessor\",\n+            _reducerOutputDir);\n+  }\n+\n+  /**\n+   * Processes segments from the input directory as per the provided configs, then puts resulting segments into the output directory\n+   */\n+  public void processSegments()\n+      throws Exception {\n+\n+    // Check for input segments\n+    File[] segmentFiles = _inputSegmentsDir.listFiles();\n+    if (segmentFiles.length == 0) {\n+      throw new IllegalStateException(\"No segments found in input dir: \" + _inputSegmentsDir.getAbsolutePath()\n+          + \". Exiting SegmentProcessorFramework.\");\n+    }\n+\n+    // Mapper phase.\n+    LOGGER.info(\"Beginning mapper phase. Processing segments: {}\", Arrays.toString(_inputSegmentsDir.list()));\n+    for (File segment : segmentFiles) {\n+\n+      String fileName = segment.getName();\n+      File mapperInput = segment;\n+\n+      // Untar the segments if needed\n+      if (!segment.isDirectory()) {\n+        if (fileName.endsWith(\".tar.gz\") || fileName.endsWith(\".tgz\")) {\n+          mapperInput = TarGzCompressionUtils.untar(segment, _mapperInputDir).get(0);\n+        } else {\n+          throw new IllegalStateException(\"Unsupported segment format: \" + segment.getAbsolutePath());\n+        }\n+      }\n+\n+      // Set mapperId as the name of the segment\n+      SegmentMapperConfig mapperConfig =\n+          new SegmentMapperConfig(_pinotSchema, _segmentProcessorConfig.getRecordTransformerConfig(),\n+              _segmentProcessorConfig.getRecordFilterConfig(), _segmentProcessorConfig.getPartitioningConfig());\n+      SegmentMapper mapper = new SegmentMapper(mapperInput.getName(), mapperInput, mapperConfig, _mapperOutputDir);\n+      mapper.map();\n+      mapper.cleanup();\n+    }\n+\n+    // Check for mapper output files\n+    File[] mapperOutputFiles = _mapperOutputDir.listFiles();\n+    if (mapperOutputFiles.length == 0) {\n+      throw new IllegalStateException(\"No files found in mapper output directory: \" + _mapperOutputDir.getAbsolutePath()\n+          + \". Exiting SegmentProcessorFramework.\");\n+    }\n+\n+    // Reducer phase.\n+    LOGGER.info(\"Beginning reducer phase. Processing files: {}\", Arrays.toString(_mapperOutputDir.list()));\n+    // Mapper output directory has 1 directory per partition, named after the partition. Each directory contains 1 or more avro files.\n+    for (File partDir : mapperOutputFiles) {\n+\n+      // Set partition as reducerId\n+      SegmentReducerConfig reducerConfig =\n+          new SegmentReducerConfig(_pinotSchema, _segmentProcessorConfig.getCollectorConfig(),\n+              _segmentProcessorConfig.getSegmentConfig().getMaxNumRecordsPerSegment());\n+      SegmentReducer reducer = new SegmentReducer(partDir.getName(), partDir, reducerConfig, _reducerOutputDir);\n+      reducer.reduce();\n+      reducer.cleanup();\n+    }\n+\n+    // Check for reducer output files\n+    File[] reducerOutputFiles = _reducerOutputDir.listFiles();\n+    if (reducerOutputFiles.length == 0) {\n+      throw new IllegalStateException(\n+          \"No files found in reducer output directory: \" + _reducerOutputDir.getAbsolutePath()\n+              + \". Exiting SegmentProcessorFramework.\");\n+    }\n+\n+    // Segment generation phase.\n+    LOGGER.info(\"Beginning segment generation phase. Processing files: {}\", Arrays.toString(_reducerOutputDir.list()));\n+    // Reducer output directory will have 1 or more avro files\n+    for (File resultFile : reducerOutputFiles) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjAxMDAwOQ=="}, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 173}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4076, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}