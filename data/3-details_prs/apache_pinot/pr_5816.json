{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDYzNjkzNjQz", "number": 5816, "title": "Enhance VarByteChunkSVForwardIndexReader to directly read from data buffer for uncompressed data", "bodyText": "Description\nCurrently for var-byte raw index, we always pre-allocate the chunk buffer even for uncompressed data (the buffer could be huge if the index contains large size value). When reading values, we first copy the data into the chunk buffer, then read from the buffer. This could cause unnecessary overhead on copying the data as well as allocating the direct memory, and can even cause OOM if the buffer size is too big. The chunk buffer is needed for compressed data in order to decompress it, but not necessary for uncompressed data as we can directly read from the data buffer.\nThis PR enhances the VarByteChunkSVForwardIndexReader to directly read from the data buffer for uncompressed data, and avoid the overhead of the chunk buffer.", "createdAt": "2020-08-05T23:21:20Z", "url": "https://github.com/apache/pinot/pull/5816", "merged": true, "mergeCommit": {"oid": "f68b82e511a0776f6f9c449637efcd332ab434ea"}, "closed": true, "closedAt": "2020-08-06T04:25:02Z", "author": {"login": "Jackie-Jiang"}, "timelineItems": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc8EQWYgFqTQ2MjEwMDQ4Nw==", "endCursor": "Y3Vyc29yOnYyOpPPAAABc8FHX3gBqjM2MjY5NTExNTE=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYyMTAwNDg3", "url": "https://github.com/apache/pinot/pull/5816#pullrequestreview-462100487", "createdAt": "2020-08-05T23:59:01Z", "commit": {"oid": "d6deb16d447bdef5356709e7583fa5e6f0f7d9d0"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYyMTE0ODY4", "url": "https://github.com/apache/pinot/pull/5816#pullrequestreview-462114868", "createdAt": "2020-08-06T00:47:17Z", "commit": {"oid": "d6deb16d447bdef5356709e7583fa5e6f0f7d9d0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwMDo0NzoxN1rOG8fXHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwMDo0NzoxN1rOG8fXHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA4MTU2NQ==", "bodyText": "typo; trunk -> chunk", "url": "https://github.com/apache/pinot/pull/5816#discussion_r466081565", "createdAt": "2020-08-06T00:47:17Z", "author": {"login": "siddharthteotia"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/index/readers/forward/VarByteChunkSVForwardIndexReader.java", "diffHunk": "@@ -19,92 +19,172 @@\n package org.apache.pinot.core.segment.index.readers.forward;\n \n import java.nio.ByteBuffer;\n+import javax.annotation.Nullable;\n import org.apache.pinot.common.utils.StringUtil;\n import org.apache.pinot.core.io.writer.impl.VarByteChunkSVForwardIndexWriter;\n import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n import org.apache.pinot.spi.data.FieldSpec.DataType;\n \n \n /**\n- * Chunk-based single-value raw (non-dictionary-encoded) forward index reader for values of  of variable length data\n- * type (STRING, BYTES).\n+ * Chunk-based single-value raw (non-dictionary-encoded) forward index reader for values of variable length data type\n+ * (STRING, BYTES).\n  * <p>For data layout, please refer to the documentation for {@link VarByteChunkSVForwardIndexWriter}\n  */\n public final class VarByteChunkSVForwardIndexReader extends BaseChunkSVForwardIndexReader {\n+  private static final int ROW_OFFSET_SIZE = VarByteChunkSVForwardIndexWriter.CHUNK_HEADER_ENTRY_ROW_OFFSET_SIZE;\n+\n   private final int _maxChunkSize;\n \n   // Thread local (reusable) byte[] to read bytes from data file.\n   private final ThreadLocal<byte[]> _reusableBytes = ThreadLocal.withInitial(() -> new byte[_lengthOfLongestEntry]);\n \n   public VarByteChunkSVForwardIndexReader(PinotDataBuffer dataBuffer, DataType valueType) {\n     super(dataBuffer, valueType);\n-    _maxChunkSize = _numDocsPerChunk * (VarByteChunkSVForwardIndexWriter.CHUNK_HEADER_ENTRY_ROW_OFFSET_SIZE\n-        + _lengthOfLongestEntry);\n+    _maxChunkSize = _numDocsPerChunk * (ROW_OFFSET_SIZE + _lengthOfLongestEntry);\n   }\n \n+  @Nullable\n   @Override\n   public ChunkReaderContext createContext() {\n-    return new ChunkReaderContext(_maxChunkSize);\n+    if (_isCompressed) {\n+      return new ChunkReaderContext(_maxChunkSize);\n+    } else {\n+      return null;\n+    }\n   }\n \n   @Override\n   public String getString(int docId, ChunkReaderContext context) {\n+    if (_isCompressed) {\n+      return getStringCompressed(docId, context);\n+    } else {\n+      return getStringUncompressed(docId);\n+    }\n+  }\n+\n+  /**\n+   * Helper method to read STRING value from the compressed index.\n+   */\n+  private String getStringCompressed(int docId, ChunkReaderContext context) {\n     int chunkRowId = docId % _numDocsPerChunk;\n     ByteBuffer chunkBuffer = getChunkBuffer(docId, context);\n \n-    int rowOffset =\n-        chunkBuffer.getInt(chunkRowId * VarByteChunkSVForwardIndexWriter.CHUNK_HEADER_ENTRY_ROW_OFFSET_SIZE);\n-    int nextRowOffset = getNextRowOffset(chunkRowId, chunkBuffer);\n+    // These offsets are offset in the chunk buffer\n+    int valueStartOffset = chunkBuffer.getInt(chunkRowId * ROW_OFFSET_SIZE);\n+    int valueEndOffset = getValueEndOffset(chunkRowId, chunkBuffer);\n \n-    int length = nextRowOffset - rowOffset;\n+    int length = valueEndOffset - valueStartOffset;\n     byte[] bytes = _reusableBytes.get();\n-\n-    chunkBuffer.position(rowOffset);\n+    chunkBuffer.position(valueStartOffset);\n     chunkBuffer.get(bytes, 0, length);\n+    return StringUtil.decodeUtf8(bytes, 0, length);\n+  }\n+\n+  /**\n+   * Helper method to read STRING value from the uncompressed index.\n+   */\n+  private String getStringUncompressed(int docId) {\n+    int chunkId = docId / _numDocsPerChunk;\n+    int chunkRowId = docId % _numDocsPerChunk;\n+\n+    // These offsets are offset in the data buffer\n+    long chunkStartOffset = getChunkPosition(chunkId);\n+    long valueStartOffset = chunkStartOffset + _dataBuffer.getInt(chunkStartOffset + chunkRowId * ROW_OFFSET_SIZE);\n+    long valueEndOffset = getValueEndOffset(chunkId, chunkRowId, chunkStartOffset);\n \n+    int length = (int) (valueEndOffset - valueStartOffset);\n+    byte[] bytes = _reusableBytes.get();\n+    _dataBuffer.copyTo(valueStartOffset, bytes, 0, length);\n     return StringUtil.decodeUtf8(bytes, 0, length);\n   }\n \n   @Override\n   public byte[] getBytes(int docId, ChunkReaderContext context) {\n+    if (_isCompressed) {\n+      return getBytesCompressed(docId, context);\n+    } else {\n+      return getBytesUncompressed(docId);\n+    }\n+  }\n+\n+  /**\n+   * Helper method to read BYTES value from the compressed index.\n+   */\n+  private byte[] getBytesCompressed(int docId, ChunkReaderContext context) {\n     int chunkRowId = docId % _numDocsPerChunk;\n     ByteBuffer chunkBuffer = getChunkBuffer(docId, context);\n \n-    int rowOffset =\n-        chunkBuffer.getInt(chunkRowId * VarByteChunkSVForwardIndexWriter.CHUNK_HEADER_ENTRY_ROW_OFFSET_SIZE);\n-    int nextRowOffset = getNextRowOffset(chunkRowId, chunkBuffer);\n+    // These offsets are offset in the chunk buffer\n+    int valueStartOffset = chunkBuffer.getInt(chunkRowId * ROW_OFFSET_SIZE);\n+    int valueEndOffset = getValueEndOffset(chunkRowId, chunkBuffer);\n \n-    int length = nextRowOffset - rowOffset;\n-    byte[] bytes = new byte[length];\n+    byte[] bytes = new byte[valueEndOffset - valueStartOffset];\n+    chunkBuffer.position(valueStartOffset);\n+    chunkBuffer.get(bytes);\n+    return bytes;\n+  }\n \n-    chunkBuffer.position(rowOffset);\n-    chunkBuffer.get(bytes, 0, length);\n+  /**\n+   * Helper method to read BYTES value from the uncompressed index.\n+   */\n+  private byte[] getBytesUncompressed(int docId) {\n+    int chunkId = docId / _numDocsPerChunk;\n+    int chunkRowId = docId % _numDocsPerChunk;\n+\n+    // These offsets are offset in the data buffer\n+    long chunkStartOffset = getChunkPosition(chunkId);\n+    long valueStartOffset = chunkStartOffset + _dataBuffer.getInt(chunkStartOffset + chunkRowId * ROW_OFFSET_SIZE);\n+    long valueEndOffset = getValueEndOffset(chunkId, chunkRowId, chunkStartOffset);\n+\n+    byte[] bytes = new byte[(int) (valueEndOffset - valueStartOffset)];\n+    _dataBuffer.copyTo(valueStartOffset, bytes);\n     return bytes;\n   }\n \n   /**\n-   * Helper method to compute the offset of next row in the chunk buffer.\n-   *\n-   * @param currentRowId Current row id within the chunk buffer.\n-   * @param chunkBuffer Chunk buffer containing the rows.\n-   *\n-   * @return Offset of next row within the chunk buffer. If current row is the last one,\n-   * chunkBuffer.limit() is returned.\n+   * Helper method to compute the end offset of the value in the chunk buffer.\n    */\n-  private int getNextRowOffset(int currentRowId, ByteBuffer chunkBuffer) {\n-    int nextRowOffset;\n+  private int getValueEndOffset(int rowId, ByteBuffer chunkBuffer) {\n+    if (rowId == _numDocsPerChunk - 1) {\n+      // Last row in the trunk", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6deb16d447bdef5356709e7583fa5e6f0f7d9d0"}, "originalPosition": 159}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYyMTE2OTk3", "url": "https://github.com/apache/pinot/pull/5816#pullrequestreview-462116997", "createdAt": "2020-08-06T00:54:50Z", "commit": {"oid": "d6deb16d447bdef5356709e7583fa5e6f0f7d9d0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwMDo1NDo1MFrOG8fezw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwMDo1NDo1MFrOG8fezw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA4MzUzNQ==", "bodyText": "Why the algorithm for getting endoffset or startOffset for next row is different for uncompressed?", "url": "https://github.com/apache/pinot/pull/5816#discussion_r466083535", "createdAt": "2020-08-06T00:54:50Z", "author": {"login": "siddharthteotia"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/index/readers/forward/VarByteChunkSVForwardIndexReader.java", "diffHunk": "@@ -19,92 +19,172 @@\n package org.apache.pinot.core.segment.index.readers.forward;\n \n import java.nio.ByteBuffer;\n+import javax.annotation.Nullable;\n import org.apache.pinot.common.utils.StringUtil;\n import org.apache.pinot.core.io.writer.impl.VarByteChunkSVForwardIndexWriter;\n import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n import org.apache.pinot.spi.data.FieldSpec.DataType;\n \n \n /**\n- * Chunk-based single-value raw (non-dictionary-encoded) forward index reader for values of  of variable length data\n- * type (STRING, BYTES).\n+ * Chunk-based single-value raw (non-dictionary-encoded) forward index reader for values of variable length data type\n+ * (STRING, BYTES).\n  * <p>For data layout, please refer to the documentation for {@link VarByteChunkSVForwardIndexWriter}\n  */\n public final class VarByteChunkSVForwardIndexReader extends BaseChunkSVForwardIndexReader {\n+  private static final int ROW_OFFSET_SIZE = VarByteChunkSVForwardIndexWriter.CHUNK_HEADER_ENTRY_ROW_OFFSET_SIZE;\n+\n   private final int _maxChunkSize;\n \n   // Thread local (reusable) byte[] to read bytes from data file.\n   private final ThreadLocal<byte[]> _reusableBytes = ThreadLocal.withInitial(() -> new byte[_lengthOfLongestEntry]);\n \n   public VarByteChunkSVForwardIndexReader(PinotDataBuffer dataBuffer, DataType valueType) {\n     super(dataBuffer, valueType);\n-    _maxChunkSize = _numDocsPerChunk * (VarByteChunkSVForwardIndexWriter.CHUNK_HEADER_ENTRY_ROW_OFFSET_SIZE\n-        + _lengthOfLongestEntry);\n+    _maxChunkSize = _numDocsPerChunk * (ROW_OFFSET_SIZE + _lengthOfLongestEntry);\n   }\n \n+  @Nullable\n   @Override\n   public ChunkReaderContext createContext() {\n-    return new ChunkReaderContext(_maxChunkSize);\n+    if (_isCompressed) {\n+      return new ChunkReaderContext(_maxChunkSize);\n+    } else {\n+      return null;\n+    }\n   }\n \n   @Override\n   public String getString(int docId, ChunkReaderContext context) {\n+    if (_isCompressed) {\n+      return getStringCompressed(docId, context);\n+    } else {\n+      return getStringUncompressed(docId);\n+    }\n+  }\n+\n+  /**\n+   * Helper method to read STRING value from the compressed index.\n+   */\n+  private String getStringCompressed(int docId, ChunkReaderContext context) {\n     int chunkRowId = docId % _numDocsPerChunk;\n     ByteBuffer chunkBuffer = getChunkBuffer(docId, context);\n \n-    int rowOffset =\n-        chunkBuffer.getInt(chunkRowId * VarByteChunkSVForwardIndexWriter.CHUNK_HEADER_ENTRY_ROW_OFFSET_SIZE);\n-    int nextRowOffset = getNextRowOffset(chunkRowId, chunkBuffer);\n+    // These offsets are offset in the chunk buffer\n+    int valueStartOffset = chunkBuffer.getInt(chunkRowId * ROW_OFFSET_SIZE);\n+    int valueEndOffset = getValueEndOffset(chunkRowId, chunkBuffer);\n \n-    int length = nextRowOffset - rowOffset;\n+    int length = valueEndOffset - valueStartOffset;\n     byte[] bytes = _reusableBytes.get();\n-\n-    chunkBuffer.position(rowOffset);\n+    chunkBuffer.position(valueStartOffset);\n     chunkBuffer.get(bytes, 0, length);\n+    return StringUtil.decodeUtf8(bytes, 0, length);\n+  }\n+\n+  /**\n+   * Helper method to read STRING value from the uncompressed index.\n+   */\n+  private String getStringUncompressed(int docId) {\n+    int chunkId = docId / _numDocsPerChunk;\n+    int chunkRowId = docId % _numDocsPerChunk;\n+\n+    // These offsets are offset in the data buffer\n+    long chunkStartOffset = getChunkPosition(chunkId);\n+    long valueStartOffset = chunkStartOffset + _dataBuffer.getInt(chunkStartOffset + chunkRowId * ROW_OFFSET_SIZE);\n+    long valueEndOffset = getValueEndOffset(chunkId, chunkRowId, chunkStartOffset);\n \n+    int length = (int) (valueEndOffset - valueStartOffset);\n+    byte[] bytes = _reusableBytes.get();\n+    _dataBuffer.copyTo(valueStartOffset, bytes, 0, length);\n     return StringUtil.decodeUtf8(bytes, 0, length);\n   }\n \n   @Override\n   public byte[] getBytes(int docId, ChunkReaderContext context) {\n+    if (_isCompressed) {\n+      return getBytesCompressed(docId, context);\n+    } else {\n+      return getBytesUncompressed(docId);\n+    }\n+  }\n+\n+  /**\n+   * Helper method to read BYTES value from the compressed index.\n+   */\n+  private byte[] getBytesCompressed(int docId, ChunkReaderContext context) {\n     int chunkRowId = docId % _numDocsPerChunk;\n     ByteBuffer chunkBuffer = getChunkBuffer(docId, context);\n \n-    int rowOffset =\n-        chunkBuffer.getInt(chunkRowId * VarByteChunkSVForwardIndexWriter.CHUNK_HEADER_ENTRY_ROW_OFFSET_SIZE);\n-    int nextRowOffset = getNextRowOffset(chunkRowId, chunkBuffer);\n+    // These offsets are offset in the chunk buffer\n+    int valueStartOffset = chunkBuffer.getInt(chunkRowId * ROW_OFFSET_SIZE);\n+    int valueEndOffset = getValueEndOffset(chunkRowId, chunkBuffer);\n \n-    int length = nextRowOffset - rowOffset;\n-    byte[] bytes = new byte[length];\n+    byte[] bytes = new byte[valueEndOffset - valueStartOffset];\n+    chunkBuffer.position(valueStartOffset);\n+    chunkBuffer.get(bytes);\n+    return bytes;\n+  }\n \n-    chunkBuffer.position(rowOffset);\n-    chunkBuffer.get(bytes, 0, length);\n+  /**\n+   * Helper method to read BYTES value from the uncompressed index.\n+   */\n+  private byte[] getBytesUncompressed(int docId) {\n+    int chunkId = docId / _numDocsPerChunk;\n+    int chunkRowId = docId % _numDocsPerChunk;\n+\n+    // These offsets are offset in the data buffer\n+    long chunkStartOffset = getChunkPosition(chunkId);\n+    long valueStartOffset = chunkStartOffset + _dataBuffer.getInt(chunkStartOffset + chunkRowId * ROW_OFFSET_SIZE);\n+    long valueEndOffset = getValueEndOffset(chunkId, chunkRowId, chunkStartOffset);\n+\n+    byte[] bytes = new byte[(int) (valueEndOffset - valueStartOffset)];\n+    _dataBuffer.copyTo(valueStartOffset, bytes);\n     return bytes;\n   }\n \n   /**\n-   * Helper method to compute the offset of next row in the chunk buffer.\n-   *\n-   * @param currentRowId Current row id within the chunk buffer.\n-   * @param chunkBuffer Chunk buffer containing the rows.\n-   *\n-   * @return Offset of next row within the chunk buffer. If current row is the last one,\n-   * chunkBuffer.limit() is returned.\n+   * Helper method to compute the end offset of the value in the chunk buffer.\n    */\n-  private int getNextRowOffset(int currentRowId, ByteBuffer chunkBuffer) {\n-    int nextRowOffset;\n+  private int getValueEndOffset(int rowId, ByteBuffer chunkBuffer) {\n+    if (rowId == _numDocsPerChunk - 1) {\n+      // Last row in the trunk\n+      return chunkBuffer.limit();\n+    } else {\n+      int valueEndOffset = chunkBuffer.getInt((rowId + 1) * ROW_OFFSET_SIZE);\n+      if (valueEndOffset == 0) {\n+        // Last row in the last chunk (chunk is incomplete, which stores 0 as the offset for the absent rows)\n+        return chunkBuffer.limit();\n+      } else {\n+        return valueEndOffset;\n+      }\n+    }\n+  }\n \n-    if (currentRowId == _numDocsPerChunk - 1) {\n-      // Last row in this trunk.\n-      nextRowOffset = chunkBuffer.limit();\n+  /**\n+   * Helper method to compute the end offset of the value in the data buffer.\n+   */\n+  private long getValueEndOffset(int chunkId, int chunkRowId, long chunkStartOffset) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6deb16d447bdef5356709e7583fa5e6f0f7d9d0"}, "originalPosition": 178}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2d12ff65d4dd547fdd85e92530811b07f69f13ba", "author": {"user": {"login": "Jackie-Jiang", "name": "Xiaotian (Jackie) Jiang"}}, "url": "https://github.com/apache/pinot/commit/2d12ff65d4dd547fdd85e92530811b07f69f13ba", "committedDate": "2020-08-06T00:58:56Z", "message": "Enhance VarByteChunkSVForwardIndexReader to directly read from data buffer for uncompressed data"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d6deb16d447bdef5356709e7583fa5e6f0f7d9d0", "author": {"user": {"login": "Jackie-Jiang", "name": "Xiaotian (Jackie) Jiang"}}, "url": "https://github.com/apache/pinot/commit/d6deb16d447bdef5356709e7583fa5e6f0f7d9d0", "committedDate": "2020-08-05T23:09:53Z", "message": "Enhance VarByteChunkSVForwardIndexReader to directly read from data buffer for uncompressed data"}, "afterCommit": {"oid": "2d12ff65d4dd547fdd85e92530811b07f69f13ba", "author": {"user": {"login": "Jackie-Jiang", "name": "Xiaotian (Jackie) Jiang"}}, "url": "https://github.com/apache/pinot/commit/2d12ff65d4dd547fdd85e92530811b07f69f13ba", "committedDate": "2020-08-06T00:58:56Z", "message": "Enhance VarByteChunkSVForwardIndexReader to directly read from data buffer for uncompressed data"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 132, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}