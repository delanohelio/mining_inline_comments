{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQ2ODI0NjY4", "number": 6396, "title": "Adding ImportData sub command in pinot admin", "bodyText": "Description\nAdding a Pinot-Admin subcommand: ImportData to help users import data into Pinot.\nUsage:\npinot-admin.sh ImportData -dataFilePath <data-file-path> -format <file-format> -table <table-name> [-controllerURI <controller-uri>] [-tempDir <temp-working-dir>] [-extraConfigs <extra-configs-for-jobs>]\n\nExamples:\nbin/pinot-admin.sh ImportData -dataFilePath /path/to/my-local-data.gz.parquet -format parquet -table my-table\n\nbin/pinot-admin.sh ImportData -dataFilePath s3://<my-bucket>/path/to/my-s3-data.gz.parquet  -format parquet -table my-table", "createdAt": "2020-12-30T07:05:32Z", "url": "https://github.com/apache/pinot/pull/6396", "merged": true, "mergeCommit": {"oid": "d83e3710dae5c8c294578d21a589cfb78396b8c7"}, "closed": true, "closedAt": "2021-01-18T07:10:01Z", "author": {"login": "xiangfu0"}, "timelineItems": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdrJ4HWgBqjQxNTY0NTk3NDk=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdxPSCSABqjQyMTYxNTAzODM=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5985dc5c0eeaec0516cf8cd33b251c9617cd6540", "author": {"user": {"login": "xiangfu0", "name": "Xiang Fu"}}, "url": "https://github.com/apache/pinot/commit/5985dc5c0eeaec0516cf8cd33b251c9617cd6540", "committedDate": "2020-12-30T07:01:30Z", "message": "Adding InsertData sub command for pinot-admin"}, "afterCommit": {"oid": "c3d16d1b88323bce14a2879f8d1111cf6d6f404b", "author": {"user": {"login": "xiangfu0", "name": "Xiang Fu"}}, "url": "https://github.com/apache/pinot/commit/c3d16d1b88323bce14a2879f8d1111cf6d6f404b", "committedDate": "2020-12-30T07:06:04Z", "message": "Adding ImportData sub command for pinot-admin"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c3d16d1b88323bce14a2879f8d1111cf6d6f404b", "author": {"user": {"login": "xiangfu0", "name": "Xiang Fu"}}, "url": "https://github.com/apache/pinot/commit/c3d16d1b88323bce14a2879f8d1111cf6d6f404b", "committedDate": "2020-12-30T07:06:04Z", "message": "Adding ImportData sub command for pinot-admin"}, "afterCommit": {"oid": "6905653c66fa8d2ac8e4bd23d7281badbfa7c6ee", "author": {"user": {"login": "xiangfu0", "name": "Xiang Fu"}}, "url": "https://github.com/apache/pinot/commit/6905653c66fa8d2ac8e4bd23d7281badbfa7c6ee", "committedDate": "2020-12-30T07:08:53Z", "message": "Adding ImportData sub command for pinot-admin"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6905653c66fa8d2ac8e4bd23d7281badbfa7c6ee", "author": {"user": {"login": "xiangfu0", "name": "Xiang Fu"}}, "url": "https://github.com/apache/pinot/commit/6905653c66fa8d2ac8e4bd23d7281badbfa7c6ee", "committedDate": "2020-12-30T07:08:53Z", "message": "Adding ImportData sub command for pinot-admin"}, "afterCommit": {"oid": "f924f2fa0a4e3c0460c6ae6d91b7b359357e57ba", "author": {"user": {"login": "xiangfu0", "name": "Xiang Fu"}}, "url": "https://github.com/apache/pinot/commit/f924f2fa0a4e3c0460c6ae6d91b7b359357e57ba", "committedDate": "2020-12-31T09:15:53Z", "message": "Adding ImportData sub command for pinot-admin"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYxMTUyNDE1", "url": "https://github.com/apache/pinot/pull/6396#pullrequestreview-561152415", "createdAt": "2021-01-04T16:12:59Z", "commit": {"oid": "f924f2fa0a4e3c0460c6ae6d91b7b359357e57ba"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQxNjoxMjo1OVrOIN3nvw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQxNjoyMzozN1rOIN4CGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQxMzY5NQ==", "bodyText": "Better to use File.separator than /?", "url": "https://github.com/apache/pinot/pull/6396#discussion_r551413695", "createdAt": "2021-01-04T16:12:59Z", "author": {"login": "mayankshriv"}, "path": "pinot-controller/src/main/java/org/apache/pinot/controller/helix/ControllerRequestURLBuilder.java", "diffHunk": "@@ -211,6 +211,9 @@ public String forTableView(String tableName, String view, @Nullable String table\n     }\n     return url;\n   }\n+  public String forTableSchemaGet(String tableName) {\n+    return StringUtil.join(\"/\", _baseUrl, \"tables\", tableName, \"schema\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f924f2fa0a4e3c0460c6ae6d91b7b359357e57ba"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQxODIzMQ==", "bodyText": "File system to use should come from input argument?", "url": "https://github.com/apache/pinot/pull/6396#discussion_r551418231", "createdAt": "2021-01-04T16:20:01Z", "author": {"login": "mayankshriv"}, "path": "pinot-tools/src/main/java/org/apache/pinot/tools/admin/command/ImportDataCommand.java", "diffHunk": "@@ -0,0 +1,340 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.tools.admin.command;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableMap;\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.commons.codec.digest.DigestUtils;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.controller.helix.ControllerRequestURLBuilder;\n+import org.apache.pinot.spi.data.readers.FileFormat;\n+import org.apache.pinot.spi.ingestion.batch.BatchConfigProperties;\n+import org.apache.pinot.spi.ingestion.batch.IngestionJobLauncher;\n+import org.apache.pinot.spi.ingestion.batch.spec.ExecutionFrameworkSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotClusterSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotFSSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PushJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.RecordReaderSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentGenerationJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentNameGeneratorSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.TableSpec;\n+import org.apache.pinot.spi.utils.IngestionConfigUtils;\n+import org.apache.pinot.tools.Command;\n+import org.kohsuke.args4j.Option;\n+import org.kohsuke.args4j.spi.StringArrayOptionHandler;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Class to implement ImportData command.\n+ */\n+@SuppressWarnings(\"unused\")\n+public class ImportDataCommand extends AbstractBaseAdminCommand implements Command {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(ImportDataCommand.class);\n+  private static final String SEGMENT_NAME = \"segment.name\";\n+\n+  @Option(name = \"-dataFilePath\", required = true, metaVar = \"<string>\", usage = \"data file path.\")\n+  private String _dataFilePath;\n+\n+  @Option(name = \"-format\", required = true, metaVar = \"<AVRO/CSV/JSON/THRIFT/PARQUET/ORC>\", usage = \"Input data format.\")\n+  private FileFormat _format;\n+\n+  @Option(name = \"-table\", required = true, metaVar = \"<string>\", usage = \"Table name.\")\n+  private String _table;\n+\n+  @Option(name = \"-controllerURI\", metaVar = \"<string>\", usage = \"Pinot Controller URI.\")\n+  private String _controllerURI = \"http://localhost:9000\";\n+\n+  @Option(name = \"-tempDir\", metaVar = \"<string>\", usage = \"Temporary directory used to hold data during segment creation.\")\n+  private String _tempDir = new File(FileUtils.getTempDirectory(), getClass().getSimpleName()).getAbsolutePath();\n+\n+  @Option(name = \"-extraConfigs\", metaVar = \"<extra configs>\", handler = StringArrayOptionHandler.class, usage = \"Extra configs to be set.\")\n+  private List<String> _extraConfigs;\n+\n+  @SuppressWarnings(\"FieldCanBeLocal\")\n+  @Option(name = \"-help\", help = true, aliases = {\"-h\", \"--h\", \"--help\"}, usage = \"Print this message.\")\n+  private boolean _help = false;\n+\n+  public ImportDataCommand setDataFilePath(String dataFilePath) {\n+    _dataFilePath = dataFilePath;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setFormat(FileFormat format) {\n+    _format = format;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setTable(String table) {\n+    _table = table;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setControllerURI(String controllerURI) {\n+    _controllerURI = controllerURI;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setTempDir(String tempDir) {\n+    _tempDir = tempDir;\n+    return this;\n+  }\n+\n+  public List<String> getExtraConfigs() {\n+    return _extraConfigs;\n+  }\n+\n+  public ImportDataCommand setExtraConfigs(List<String> extraConfigs) {\n+    _extraConfigs = extraConfigs;\n+    return this;\n+  }\n+\n+  public String getDataFilePath() {\n+    return _dataFilePath;\n+  }\n+\n+  public FileFormat getFormat() {\n+    return _format;\n+  }\n+\n+  public String getTable() {\n+    return _table;\n+  }\n+\n+  public String getControllerURI() {\n+    return _controllerURI;\n+  }\n+\n+  public String getTempDir() {\n+    return _tempDir;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    String results = String\n+        .format(\"InsertData -dataFilePath %s -format %s -table %s -controllerURI %s -tempDir %s\", _dataFilePath,\n+            _format, _table, _controllerURI, _tempDir);\n+    if (_extraConfigs != null) {\n+      results += \" -extraConfigs \" + Arrays.toString(_extraConfigs.toArray());\n+    }\n+    return results;\n+  }\n+\n+  @Override\n+  public final String getName() {\n+    return \"InsertData\";\n+  }\n+\n+  @Override\n+  public String description() {\n+    return \"Insert data into Pinot cluster.\";\n+  }\n+\n+  @Override\n+  public boolean getHelp() {\n+    return _help;\n+  }\n+\n+  @Override\n+  public boolean execute()\n+      throws IOException {\n+    LOGGER.info(\"Executing command: {}\", toString());\n+    Preconditions.checkArgument(_table != null, \"'table' must be specified\");\n+    Preconditions.checkArgument(_format != null, \"'format' must be specified\");\n+    Preconditions.checkArgument(_dataFilePath != null, \"'dataFilePath' must be specified\");\n+\n+    try {\n+\n+      URI dataFileURI = URI.create(_dataFilePath);\n+      if ((dataFileURI.getScheme() == null)) {\n+        File dataFile = new File(_dataFilePath);\n+        Preconditions.checkArgument(dataFile.exists(), \"'dataFile': '%s' doesn't exist\", dataFile);\n+        LOGGER.info(\"Found data files: {} of format: {}\", dataFile, _format);\n+      }\n+\n+      initTempDir();\n+      IngestionJobLauncher.runIngestionJob(generateSegmentGenerationJobSpec());\n+      LOGGER.info(\"Successfully load data from {} to Pinot.\", _dataFilePath);\n+      return true;\n+    } catch (Exception e) {\n+      throw e;\n+    } finally {\n+      FileUtils.deleteQuietly(new File(_tempDir));\n+    }\n+  }\n+\n+  private void initTempDir()\n+      throws IOException {\n+    File tempDir = new File(_tempDir);\n+    if (tempDir.exists()) {\n+      LOGGER.info(\"Deleting the existing 'tempDir': {}\", tempDir);\n+      FileUtils.forceDelete(tempDir);\n+    }\n+    FileUtils.forceMkdir(tempDir);\n+  }\n+\n+  private SegmentGenerationJobSpec generateSegmentGenerationJobSpec() {\n+    final Map<String, String> extraConfigs = getExtraConfigs(_extraConfigs);\n+\n+    SegmentGenerationJobSpec spec = new SegmentGenerationJobSpec();\n+    URI dataFileURI = URI.create(_dataFilePath);\n+    URI parent = dataFileURI.getPath().endsWith(\"/\") ? dataFileURI.resolve(\"..\") : dataFileURI.resolve(\".\");\n+    spec.setInputDirURI(parent.toString());\n+    spec.setIncludeFileNamePattern(\"glob:**\" + dataFileURI.getPath());\n+    spec.setOutputDirURI(_tempDir);\n+    spec.setCleanUpOutputDir(true);\n+    spec.setOverwriteOutput(true);\n+    spec.setJobType(\"SegmentCreationAndTarPush\");\n+\n+    // set ExecutionFrameworkSpec\n+    ExecutionFrameworkSpec executionFrameworkSpec = new ExecutionFrameworkSpec();\n+    executionFrameworkSpec.setName(\"standalone\");\n+    executionFrameworkSpec.setSegmentGenerationJobRunnerClassName(\n+        \"org.apache.pinot.plugin.ingestion.batch.standalone.SegmentGenerationJobRunner\");\n+    executionFrameworkSpec.setSegmentTarPushJobRunnerClassName(\n+        \"org.apache.pinot.plugin.ingestion.batch.standalone.SegmentTarPushJobRunner\");\n+    spec.setExecutionFrameworkSpec(executionFrameworkSpec);\n+\n+    // set PinotFSSpecs\n+    List<PinotFSSpec> pinotFSSpecs = new ArrayList<>();\n+    pinotFSSpecs.add(getPinotFSSpec(\"file\", \"org.apache.pinot.spi.filesystem.LocalPinotFS\", Collections.emptyMap()));\n+    pinotFSSpecs\n+        .add(getPinotFSSpec(\"s3\", \"org.apache.pinot.plugin.filesystem.S3PinotFS\", getS3PinotFSConfigs(extraConfigs)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f924f2fa0a4e3c0460c6ae6d91b7b359357e57ba"}, "originalPosition": 228}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQxODYwOA==", "bodyText": "SegmentNameGenerator should also come from input arg?", "url": "https://github.com/apache/pinot/pull/6396#discussion_r551418608", "createdAt": "2021-01-04T16:20:40Z", "author": {"login": "mayankshriv"}, "path": "pinot-tools/src/main/java/org/apache/pinot/tools/admin/command/ImportDataCommand.java", "diffHunk": "@@ -0,0 +1,340 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.tools.admin.command;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableMap;\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.commons.codec.digest.DigestUtils;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.controller.helix.ControllerRequestURLBuilder;\n+import org.apache.pinot.spi.data.readers.FileFormat;\n+import org.apache.pinot.spi.ingestion.batch.BatchConfigProperties;\n+import org.apache.pinot.spi.ingestion.batch.IngestionJobLauncher;\n+import org.apache.pinot.spi.ingestion.batch.spec.ExecutionFrameworkSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotClusterSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotFSSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PushJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.RecordReaderSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentGenerationJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentNameGeneratorSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.TableSpec;\n+import org.apache.pinot.spi.utils.IngestionConfigUtils;\n+import org.apache.pinot.tools.Command;\n+import org.kohsuke.args4j.Option;\n+import org.kohsuke.args4j.spi.StringArrayOptionHandler;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Class to implement ImportData command.\n+ */\n+@SuppressWarnings(\"unused\")\n+public class ImportDataCommand extends AbstractBaseAdminCommand implements Command {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(ImportDataCommand.class);\n+  private static final String SEGMENT_NAME = \"segment.name\";\n+\n+  @Option(name = \"-dataFilePath\", required = true, metaVar = \"<string>\", usage = \"data file path.\")\n+  private String _dataFilePath;\n+\n+  @Option(name = \"-format\", required = true, metaVar = \"<AVRO/CSV/JSON/THRIFT/PARQUET/ORC>\", usage = \"Input data format.\")\n+  private FileFormat _format;\n+\n+  @Option(name = \"-table\", required = true, metaVar = \"<string>\", usage = \"Table name.\")\n+  private String _table;\n+\n+  @Option(name = \"-controllerURI\", metaVar = \"<string>\", usage = \"Pinot Controller URI.\")\n+  private String _controllerURI = \"http://localhost:9000\";\n+\n+  @Option(name = \"-tempDir\", metaVar = \"<string>\", usage = \"Temporary directory used to hold data during segment creation.\")\n+  private String _tempDir = new File(FileUtils.getTempDirectory(), getClass().getSimpleName()).getAbsolutePath();\n+\n+  @Option(name = \"-extraConfigs\", metaVar = \"<extra configs>\", handler = StringArrayOptionHandler.class, usage = \"Extra configs to be set.\")\n+  private List<String> _extraConfigs;\n+\n+  @SuppressWarnings(\"FieldCanBeLocal\")\n+  @Option(name = \"-help\", help = true, aliases = {\"-h\", \"--h\", \"--help\"}, usage = \"Print this message.\")\n+  private boolean _help = false;\n+\n+  public ImportDataCommand setDataFilePath(String dataFilePath) {\n+    _dataFilePath = dataFilePath;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setFormat(FileFormat format) {\n+    _format = format;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setTable(String table) {\n+    _table = table;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setControllerURI(String controllerURI) {\n+    _controllerURI = controllerURI;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setTempDir(String tempDir) {\n+    _tempDir = tempDir;\n+    return this;\n+  }\n+\n+  public List<String> getExtraConfigs() {\n+    return _extraConfigs;\n+  }\n+\n+  public ImportDataCommand setExtraConfigs(List<String> extraConfigs) {\n+    _extraConfigs = extraConfigs;\n+    return this;\n+  }\n+\n+  public String getDataFilePath() {\n+    return _dataFilePath;\n+  }\n+\n+  public FileFormat getFormat() {\n+    return _format;\n+  }\n+\n+  public String getTable() {\n+    return _table;\n+  }\n+\n+  public String getControllerURI() {\n+    return _controllerURI;\n+  }\n+\n+  public String getTempDir() {\n+    return _tempDir;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    String results = String\n+        .format(\"InsertData -dataFilePath %s -format %s -table %s -controllerURI %s -tempDir %s\", _dataFilePath,\n+            _format, _table, _controllerURI, _tempDir);\n+    if (_extraConfigs != null) {\n+      results += \" -extraConfigs \" + Arrays.toString(_extraConfigs.toArray());\n+    }\n+    return results;\n+  }\n+\n+  @Override\n+  public final String getName() {\n+    return \"InsertData\";\n+  }\n+\n+  @Override\n+  public String description() {\n+    return \"Insert data into Pinot cluster.\";\n+  }\n+\n+  @Override\n+  public boolean getHelp() {\n+    return _help;\n+  }\n+\n+  @Override\n+  public boolean execute()\n+      throws IOException {\n+    LOGGER.info(\"Executing command: {}\", toString());\n+    Preconditions.checkArgument(_table != null, \"'table' must be specified\");\n+    Preconditions.checkArgument(_format != null, \"'format' must be specified\");\n+    Preconditions.checkArgument(_dataFilePath != null, \"'dataFilePath' must be specified\");\n+\n+    try {\n+\n+      URI dataFileURI = URI.create(_dataFilePath);\n+      if ((dataFileURI.getScheme() == null)) {\n+        File dataFile = new File(_dataFilePath);\n+        Preconditions.checkArgument(dataFile.exists(), \"'dataFile': '%s' doesn't exist\", dataFile);\n+        LOGGER.info(\"Found data files: {} of format: {}\", dataFile, _format);\n+      }\n+\n+      initTempDir();\n+      IngestionJobLauncher.runIngestionJob(generateSegmentGenerationJobSpec());\n+      LOGGER.info(\"Successfully load data from {} to Pinot.\", _dataFilePath);\n+      return true;\n+    } catch (Exception e) {\n+      throw e;\n+    } finally {\n+      FileUtils.deleteQuietly(new File(_tempDir));\n+    }\n+  }\n+\n+  private void initTempDir()\n+      throws IOException {\n+    File tempDir = new File(_tempDir);\n+    if (tempDir.exists()) {\n+      LOGGER.info(\"Deleting the existing 'tempDir': {}\", tempDir);\n+      FileUtils.forceDelete(tempDir);\n+    }\n+    FileUtils.forceMkdir(tempDir);\n+  }\n+\n+  private SegmentGenerationJobSpec generateSegmentGenerationJobSpec() {\n+    final Map<String, String> extraConfigs = getExtraConfigs(_extraConfigs);\n+\n+    SegmentGenerationJobSpec spec = new SegmentGenerationJobSpec();\n+    URI dataFileURI = URI.create(_dataFilePath);\n+    URI parent = dataFileURI.getPath().endsWith(\"/\") ? dataFileURI.resolve(\"..\") : dataFileURI.resolve(\".\");\n+    spec.setInputDirURI(parent.toString());\n+    spec.setIncludeFileNamePattern(\"glob:**\" + dataFileURI.getPath());\n+    spec.setOutputDirURI(_tempDir);\n+    spec.setCleanUpOutputDir(true);\n+    spec.setOverwriteOutput(true);\n+    spec.setJobType(\"SegmentCreationAndTarPush\");\n+\n+    // set ExecutionFrameworkSpec\n+    ExecutionFrameworkSpec executionFrameworkSpec = new ExecutionFrameworkSpec();\n+    executionFrameworkSpec.setName(\"standalone\");\n+    executionFrameworkSpec.setSegmentGenerationJobRunnerClassName(\n+        \"org.apache.pinot.plugin.ingestion.batch.standalone.SegmentGenerationJobRunner\");\n+    executionFrameworkSpec.setSegmentTarPushJobRunnerClassName(\n+        \"org.apache.pinot.plugin.ingestion.batch.standalone.SegmentTarPushJobRunner\");\n+    spec.setExecutionFrameworkSpec(executionFrameworkSpec);\n+\n+    // set PinotFSSpecs\n+    List<PinotFSSpec> pinotFSSpecs = new ArrayList<>();\n+    pinotFSSpecs.add(getPinotFSSpec(\"file\", \"org.apache.pinot.spi.filesystem.LocalPinotFS\", Collections.emptyMap()));\n+    pinotFSSpecs\n+        .add(getPinotFSSpec(\"s3\", \"org.apache.pinot.plugin.filesystem.S3PinotFS\", getS3PinotFSConfigs(extraConfigs)));\n+    spec.setPinotFSSpecs(pinotFSSpecs);\n+\n+    // set RecordReaderSpec\n+    RecordReaderSpec recordReaderSpec = new RecordReaderSpec();\n+    recordReaderSpec.setDataFormat(_format.name());\n+    recordReaderSpec.setClassName(getRecordReaderClass(_format));\n+    recordReaderSpec.setConfigClassName(getRecordReaderConfigClass(_format));\n+    recordReaderSpec.setConfigs(IngestionConfigUtils.getRecordReaderProps(extraConfigs));\n+    spec.setRecordReaderSpec(recordReaderSpec);\n+\n+    // set TableSpec\n+    TableSpec tableSpec = new TableSpec();\n+    tableSpec.setTableName(_table);\n+    tableSpec.setSchemaURI(ControllerRequestURLBuilder.baseUrl(_controllerURI).forTableSchemaGet(_table));\n+    tableSpec.setTableConfigURI(ControllerRequestURLBuilder.baseUrl(_controllerURI).forTableGet(_table));\n+    spec.setTableSpec(tableSpec);\n+\n+    // set SegmentNameGeneratorSpec\n+    SegmentNameGeneratorSpec segmentNameGeneratorSpec = new SegmentNameGeneratorSpec();\n+    segmentNameGeneratorSpec\n+        .setType(org.apache.pinot.spi.ingestion.batch.BatchConfigProperties.SegmentNameGeneratorType.FIXED);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f924f2fa0a4e3c0460c6ae6d91b7b359357e57ba"}, "originalPosition": 249}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQxOTI4Ng==", "bodyText": "Hmm, should these be hardcoded?", "url": "https://github.com/apache/pinot/pull/6396#discussion_r551419286", "createdAt": "2021-01-04T16:21:47Z", "author": {"login": "mayankshriv"}, "path": "pinot-tools/src/main/java/org/apache/pinot/tools/admin/command/ImportDataCommand.java", "diffHunk": "@@ -0,0 +1,340 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.tools.admin.command;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableMap;\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.commons.codec.digest.DigestUtils;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.controller.helix.ControllerRequestURLBuilder;\n+import org.apache.pinot.spi.data.readers.FileFormat;\n+import org.apache.pinot.spi.ingestion.batch.BatchConfigProperties;\n+import org.apache.pinot.spi.ingestion.batch.IngestionJobLauncher;\n+import org.apache.pinot.spi.ingestion.batch.spec.ExecutionFrameworkSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotClusterSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotFSSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PushJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.RecordReaderSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentGenerationJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentNameGeneratorSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.TableSpec;\n+import org.apache.pinot.spi.utils.IngestionConfigUtils;\n+import org.apache.pinot.tools.Command;\n+import org.kohsuke.args4j.Option;\n+import org.kohsuke.args4j.spi.StringArrayOptionHandler;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Class to implement ImportData command.\n+ */\n+@SuppressWarnings(\"unused\")\n+public class ImportDataCommand extends AbstractBaseAdminCommand implements Command {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(ImportDataCommand.class);\n+  private static final String SEGMENT_NAME = \"segment.name\";\n+\n+  @Option(name = \"-dataFilePath\", required = true, metaVar = \"<string>\", usage = \"data file path.\")\n+  private String _dataFilePath;\n+\n+  @Option(name = \"-format\", required = true, metaVar = \"<AVRO/CSV/JSON/THRIFT/PARQUET/ORC>\", usage = \"Input data format.\")\n+  private FileFormat _format;\n+\n+  @Option(name = \"-table\", required = true, metaVar = \"<string>\", usage = \"Table name.\")\n+  private String _table;\n+\n+  @Option(name = \"-controllerURI\", metaVar = \"<string>\", usage = \"Pinot Controller URI.\")\n+  private String _controllerURI = \"http://localhost:9000\";\n+\n+  @Option(name = \"-tempDir\", metaVar = \"<string>\", usage = \"Temporary directory used to hold data during segment creation.\")\n+  private String _tempDir = new File(FileUtils.getTempDirectory(), getClass().getSimpleName()).getAbsolutePath();\n+\n+  @Option(name = \"-extraConfigs\", metaVar = \"<extra configs>\", handler = StringArrayOptionHandler.class, usage = \"Extra configs to be set.\")\n+  private List<String> _extraConfigs;\n+\n+  @SuppressWarnings(\"FieldCanBeLocal\")\n+  @Option(name = \"-help\", help = true, aliases = {\"-h\", \"--h\", \"--help\"}, usage = \"Print this message.\")\n+  private boolean _help = false;\n+\n+  public ImportDataCommand setDataFilePath(String dataFilePath) {\n+    _dataFilePath = dataFilePath;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setFormat(FileFormat format) {\n+    _format = format;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setTable(String table) {\n+    _table = table;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setControllerURI(String controllerURI) {\n+    _controllerURI = controllerURI;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setTempDir(String tempDir) {\n+    _tempDir = tempDir;\n+    return this;\n+  }\n+\n+  public List<String> getExtraConfigs() {\n+    return _extraConfigs;\n+  }\n+\n+  public ImportDataCommand setExtraConfigs(List<String> extraConfigs) {\n+    _extraConfigs = extraConfigs;\n+    return this;\n+  }\n+\n+  public String getDataFilePath() {\n+    return _dataFilePath;\n+  }\n+\n+  public FileFormat getFormat() {\n+    return _format;\n+  }\n+\n+  public String getTable() {\n+    return _table;\n+  }\n+\n+  public String getControllerURI() {\n+    return _controllerURI;\n+  }\n+\n+  public String getTempDir() {\n+    return _tempDir;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    String results = String\n+        .format(\"InsertData -dataFilePath %s -format %s -table %s -controllerURI %s -tempDir %s\", _dataFilePath,\n+            _format, _table, _controllerURI, _tempDir);\n+    if (_extraConfigs != null) {\n+      results += \" -extraConfigs \" + Arrays.toString(_extraConfigs.toArray());\n+    }\n+    return results;\n+  }\n+\n+  @Override\n+  public final String getName() {\n+    return \"InsertData\";\n+  }\n+\n+  @Override\n+  public String description() {\n+    return \"Insert data into Pinot cluster.\";\n+  }\n+\n+  @Override\n+  public boolean getHelp() {\n+    return _help;\n+  }\n+\n+  @Override\n+  public boolean execute()\n+      throws IOException {\n+    LOGGER.info(\"Executing command: {}\", toString());\n+    Preconditions.checkArgument(_table != null, \"'table' must be specified\");\n+    Preconditions.checkArgument(_format != null, \"'format' must be specified\");\n+    Preconditions.checkArgument(_dataFilePath != null, \"'dataFilePath' must be specified\");\n+\n+    try {\n+\n+      URI dataFileURI = URI.create(_dataFilePath);\n+      if ((dataFileURI.getScheme() == null)) {\n+        File dataFile = new File(_dataFilePath);\n+        Preconditions.checkArgument(dataFile.exists(), \"'dataFile': '%s' doesn't exist\", dataFile);\n+        LOGGER.info(\"Found data files: {} of format: {}\", dataFile, _format);\n+      }\n+\n+      initTempDir();\n+      IngestionJobLauncher.runIngestionJob(generateSegmentGenerationJobSpec());\n+      LOGGER.info(\"Successfully load data from {} to Pinot.\", _dataFilePath);\n+      return true;\n+    } catch (Exception e) {\n+      throw e;\n+    } finally {\n+      FileUtils.deleteQuietly(new File(_tempDir));\n+    }\n+  }\n+\n+  private void initTempDir()\n+      throws IOException {\n+    File tempDir = new File(_tempDir);\n+    if (tempDir.exists()) {\n+      LOGGER.info(\"Deleting the existing 'tempDir': {}\", tempDir);\n+      FileUtils.forceDelete(tempDir);\n+    }\n+    FileUtils.forceMkdir(tempDir);\n+  }\n+\n+  private SegmentGenerationJobSpec generateSegmentGenerationJobSpec() {\n+    final Map<String, String> extraConfigs = getExtraConfigs(_extraConfigs);\n+\n+    SegmentGenerationJobSpec spec = new SegmentGenerationJobSpec();\n+    URI dataFileURI = URI.create(_dataFilePath);\n+    URI parent = dataFileURI.getPath().endsWith(\"/\") ? dataFileURI.resolve(\"..\") : dataFileURI.resolve(\".\");\n+    spec.setInputDirURI(parent.toString());\n+    spec.setIncludeFileNamePattern(\"glob:**\" + dataFileURI.getPath());\n+    spec.setOutputDirURI(_tempDir);\n+    spec.setCleanUpOutputDir(true);\n+    spec.setOverwriteOutput(true);\n+    spec.setJobType(\"SegmentCreationAndTarPush\");\n+\n+    // set ExecutionFrameworkSpec\n+    ExecutionFrameworkSpec executionFrameworkSpec = new ExecutionFrameworkSpec();\n+    executionFrameworkSpec.setName(\"standalone\");\n+    executionFrameworkSpec.setSegmentGenerationJobRunnerClassName(\n+        \"org.apache.pinot.plugin.ingestion.batch.standalone.SegmentGenerationJobRunner\");\n+    executionFrameworkSpec.setSegmentTarPushJobRunnerClassName(\n+        \"org.apache.pinot.plugin.ingestion.batch.standalone.SegmentTarPushJobRunner\");\n+    spec.setExecutionFrameworkSpec(executionFrameworkSpec);\n+\n+    // set PinotFSSpecs\n+    List<PinotFSSpec> pinotFSSpecs = new ArrayList<>();\n+    pinotFSSpecs.add(getPinotFSSpec(\"file\", \"org.apache.pinot.spi.filesystem.LocalPinotFS\", Collections.emptyMap()));\n+    pinotFSSpecs\n+        .add(getPinotFSSpec(\"s3\", \"org.apache.pinot.plugin.filesystem.S3PinotFS\", getS3PinotFSConfigs(extraConfigs)));\n+    spec.setPinotFSSpecs(pinotFSSpecs);\n+\n+    // set RecordReaderSpec\n+    RecordReaderSpec recordReaderSpec = new RecordReaderSpec();\n+    recordReaderSpec.setDataFormat(_format.name());\n+    recordReaderSpec.setClassName(getRecordReaderClass(_format));\n+    recordReaderSpec.setConfigClassName(getRecordReaderConfigClass(_format));\n+    recordReaderSpec.setConfigs(IngestionConfigUtils.getRecordReaderProps(extraConfigs));\n+    spec.setRecordReaderSpec(recordReaderSpec);\n+\n+    // set TableSpec\n+    TableSpec tableSpec = new TableSpec();\n+    tableSpec.setTableName(_table);\n+    tableSpec.setSchemaURI(ControllerRequestURLBuilder.baseUrl(_controllerURI).forTableSchemaGet(_table));\n+    tableSpec.setTableConfigURI(ControllerRequestURLBuilder.baseUrl(_controllerURI).forTableGet(_table));\n+    spec.setTableSpec(tableSpec);\n+\n+    // set SegmentNameGeneratorSpec\n+    SegmentNameGeneratorSpec segmentNameGeneratorSpec = new SegmentNameGeneratorSpec();\n+    segmentNameGeneratorSpec\n+        .setType(org.apache.pinot.spi.ingestion.batch.BatchConfigProperties.SegmentNameGeneratorType.FIXED);\n+    String segmentName = (extraConfigs.containsKey(SEGMENT_NAME)) ? extraConfigs.get(SEGMENT_NAME)\n+        : String.format(\"%s_%s\", _table, DigestUtils.sha256Hex(_dataFilePath));\n+    segmentNameGeneratorSpec.setConfigs(ImmutableMap.of(SEGMENT_NAME, segmentName));\n+    spec.setSegmentNameGeneratorSpec(segmentNameGeneratorSpec);\n+\n+    // set PinotClusterSpecs\n+    PinotClusterSpec pinotClusterSpec = new PinotClusterSpec();\n+    pinotClusterSpec.setControllerURI(_controllerURI);\n+    PinotClusterSpec[] pinotClusterSpecs = new PinotClusterSpec[]{pinotClusterSpec};\n+    spec.setPinotClusterSpecs(pinotClusterSpecs);\n+\n+    // set PushJobSpec\n+    PushJobSpec pushJobSpec = new PushJobSpec();\n+    pushJobSpec.setPushAttempts(3);\n+    pushJobSpec.setPushRetryIntervalMillis(10000);\n+    spec.setPushJobSpec(pushJobSpec);\n+\n+    return spec;\n+  }\n+\n+  private Map<String, String> getS3PinotFSConfigs(Map<String, String> extraConfigs) {\n+    Map<String, String> s3PinotFSConfigs = new HashMap<>();\n+    s3PinotFSConfigs.put(\"region\", System.getProperty(\"AWS_REGION\", \"us-west-2\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f924f2fa0a4e3c0460c6ae6d91b7b359357e57ba"}, "originalPosition": 272}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQxOTg2Mw==", "bodyText": "Do we not have a RecordReaderFactory?", "url": "https://github.com/apache/pinot/pull/6396#discussion_r551419863", "createdAt": "2021-01-04T16:22:44Z", "author": {"login": "mayankshriv"}, "path": "pinot-tools/src/main/java/org/apache/pinot/tools/admin/command/ImportDataCommand.java", "diffHunk": "@@ -0,0 +1,340 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.tools.admin.command;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableMap;\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.commons.codec.digest.DigestUtils;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.controller.helix.ControllerRequestURLBuilder;\n+import org.apache.pinot.spi.data.readers.FileFormat;\n+import org.apache.pinot.spi.ingestion.batch.BatchConfigProperties;\n+import org.apache.pinot.spi.ingestion.batch.IngestionJobLauncher;\n+import org.apache.pinot.spi.ingestion.batch.spec.ExecutionFrameworkSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotClusterSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotFSSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PushJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.RecordReaderSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentGenerationJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentNameGeneratorSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.TableSpec;\n+import org.apache.pinot.spi.utils.IngestionConfigUtils;\n+import org.apache.pinot.tools.Command;\n+import org.kohsuke.args4j.Option;\n+import org.kohsuke.args4j.spi.StringArrayOptionHandler;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Class to implement ImportData command.\n+ */\n+@SuppressWarnings(\"unused\")\n+public class ImportDataCommand extends AbstractBaseAdminCommand implements Command {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(ImportDataCommand.class);\n+  private static final String SEGMENT_NAME = \"segment.name\";\n+\n+  @Option(name = \"-dataFilePath\", required = true, metaVar = \"<string>\", usage = \"data file path.\")\n+  private String _dataFilePath;\n+\n+  @Option(name = \"-format\", required = true, metaVar = \"<AVRO/CSV/JSON/THRIFT/PARQUET/ORC>\", usage = \"Input data format.\")\n+  private FileFormat _format;\n+\n+  @Option(name = \"-table\", required = true, metaVar = \"<string>\", usage = \"Table name.\")\n+  private String _table;\n+\n+  @Option(name = \"-controllerURI\", metaVar = \"<string>\", usage = \"Pinot Controller URI.\")\n+  private String _controllerURI = \"http://localhost:9000\";\n+\n+  @Option(name = \"-tempDir\", metaVar = \"<string>\", usage = \"Temporary directory used to hold data during segment creation.\")\n+  private String _tempDir = new File(FileUtils.getTempDirectory(), getClass().getSimpleName()).getAbsolutePath();\n+\n+  @Option(name = \"-extraConfigs\", metaVar = \"<extra configs>\", handler = StringArrayOptionHandler.class, usage = \"Extra configs to be set.\")\n+  private List<String> _extraConfigs;\n+\n+  @SuppressWarnings(\"FieldCanBeLocal\")\n+  @Option(name = \"-help\", help = true, aliases = {\"-h\", \"--h\", \"--help\"}, usage = \"Print this message.\")\n+  private boolean _help = false;\n+\n+  public ImportDataCommand setDataFilePath(String dataFilePath) {\n+    _dataFilePath = dataFilePath;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setFormat(FileFormat format) {\n+    _format = format;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setTable(String table) {\n+    _table = table;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setControllerURI(String controllerURI) {\n+    _controllerURI = controllerURI;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setTempDir(String tempDir) {\n+    _tempDir = tempDir;\n+    return this;\n+  }\n+\n+  public List<String> getExtraConfigs() {\n+    return _extraConfigs;\n+  }\n+\n+  public ImportDataCommand setExtraConfigs(List<String> extraConfigs) {\n+    _extraConfigs = extraConfigs;\n+    return this;\n+  }\n+\n+  public String getDataFilePath() {\n+    return _dataFilePath;\n+  }\n+\n+  public FileFormat getFormat() {\n+    return _format;\n+  }\n+\n+  public String getTable() {\n+    return _table;\n+  }\n+\n+  public String getControllerURI() {\n+    return _controllerURI;\n+  }\n+\n+  public String getTempDir() {\n+    return _tempDir;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    String results = String\n+        .format(\"InsertData -dataFilePath %s -format %s -table %s -controllerURI %s -tempDir %s\", _dataFilePath,\n+            _format, _table, _controllerURI, _tempDir);\n+    if (_extraConfigs != null) {\n+      results += \" -extraConfigs \" + Arrays.toString(_extraConfigs.toArray());\n+    }\n+    return results;\n+  }\n+\n+  @Override\n+  public final String getName() {\n+    return \"InsertData\";\n+  }\n+\n+  @Override\n+  public String description() {\n+    return \"Insert data into Pinot cluster.\";\n+  }\n+\n+  @Override\n+  public boolean getHelp() {\n+    return _help;\n+  }\n+\n+  @Override\n+  public boolean execute()\n+      throws IOException {\n+    LOGGER.info(\"Executing command: {}\", toString());\n+    Preconditions.checkArgument(_table != null, \"'table' must be specified\");\n+    Preconditions.checkArgument(_format != null, \"'format' must be specified\");\n+    Preconditions.checkArgument(_dataFilePath != null, \"'dataFilePath' must be specified\");\n+\n+    try {\n+\n+      URI dataFileURI = URI.create(_dataFilePath);\n+      if ((dataFileURI.getScheme() == null)) {\n+        File dataFile = new File(_dataFilePath);\n+        Preconditions.checkArgument(dataFile.exists(), \"'dataFile': '%s' doesn't exist\", dataFile);\n+        LOGGER.info(\"Found data files: {} of format: {}\", dataFile, _format);\n+      }\n+\n+      initTempDir();\n+      IngestionJobLauncher.runIngestionJob(generateSegmentGenerationJobSpec());\n+      LOGGER.info(\"Successfully load data from {} to Pinot.\", _dataFilePath);\n+      return true;\n+    } catch (Exception e) {\n+      throw e;\n+    } finally {\n+      FileUtils.deleteQuietly(new File(_tempDir));\n+    }\n+  }\n+\n+  private void initTempDir()\n+      throws IOException {\n+    File tempDir = new File(_tempDir);\n+    if (tempDir.exists()) {\n+      LOGGER.info(\"Deleting the existing 'tempDir': {}\", tempDir);\n+      FileUtils.forceDelete(tempDir);\n+    }\n+    FileUtils.forceMkdir(tempDir);\n+  }\n+\n+  private SegmentGenerationJobSpec generateSegmentGenerationJobSpec() {\n+    final Map<String, String> extraConfigs = getExtraConfigs(_extraConfigs);\n+\n+    SegmentGenerationJobSpec spec = new SegmentGenerationJobSpec();\n+    URI dataFileURI = URI.create(_dataFilePath);\n+    URI parent = dataFileURI.getPath().endsWith(\"/\") ? dataFileURI.resolve(\"..\") : dataFileURI.resolve(\".\");\n+    spec.setInputDirURI(parent.toString());\n+    spec.setIncludeFileNamePattern(\"glob:**\" + dataFileURI.getPath());\n+    spec.setOutputDirURI(_tempDir);\n+    spec.setCleanUpOutputDir(true);\n+    spec.setOverwriteOutput(true);\n+    spec.setJobType(\"SegmentCreationAndTarPush\");\n+\n+    // set ExecutionFrameworkSpec\n+    ExecutionFrameworkSpec executionFrameworkSpec = new ExecutionFrameworkSpec();\n+    executionFrameworkSpec.setName(\"standalone\");\n+    executionFrameworkSpec.setSegmentGenerationJobRunnerClassName(\n+        \"org.apache.pinot.plugin.ingestion.batch.standalone.SegmentGenerationJobRunner\");\n+    executionFrameworkSpec.setSegmentTarPushJobRunnerClassName(\n+        \"org.apache.pinot.plugin.ingestion.batch.standalone.SegmentTarPushJobRunner\");\n+    spec.setExecutionFrameworkSpec(executionFrameworkSpec);\n+\n+    // set PinotFSSpecs\n+    List<PinotFSSpec> pinotFSSpecs = new ArrayList<>();\n+    pinotFSSpecs.add(getPinotFSSpec(\"file\", \"org.apache.pinot.spi.filesystem.LocalPinotFS\", Collections.emptyMap()));\n+    pinotFSSpecs\n+        .add(getPinotFSSpec(\"s3\", \"org.apache.pinot.plugin.filesystem.S3PinotFS\", getS3PinotFSConfigs(extraConfigs)));\n+    spec.setPinotFSSpecs(pinotFSSpecs);\n+\n+    // set RecordReaderSpec\n+    RecordReaderSpec recordReaderSpec = new RecordReaderSpec();\n+    recordReaderSpec.setDataFormat(_format.name());\n+    recordReaderSpec.setClassName(getRecordReaderClass(_format));\n+    recordReaderSpec.setConfigClassName(getRecordReaderConfigClass(_format));\n+    recordReaderSpec.setConfigs(IngestionConfigUtils.getRecordReaderProps(extraConfigs));\n+    spec.setRecordReaderSpec(recordReaderSpec);\n+\n+    // set TableSpec\n+    TableSpec tableSpec = new TableSpec();\n+    tableSpec.setTableName(_table);\n+    tableSpec.setSchemaURI(ControllerRequestURLBuilder.baseUrl(_controllerURI).forTableSchemaGet(_table));\n+    tableSpec.setTableConfigURI(ControllerRequestURLBuilder.baseUrl(_controllerURI).forTableGet(_table));\n+    spec.setTableSpec(tableSpec);\n+\n+    // set SegmentNameGeneratorSpec\n+    SegmentNameGeneratorSpec segmentNameGeneratorSpec = new SegmentNameGeneratorSpec();\n+    segmentNameGeneratorSpec\n+        .setType(org.apache.pinot.spi.ingestion.batch.BatchConfigProperties.SegmentNameGeneratorType.FIXED);\n+    String segmentName = (extraConfigs.containsKey(SEGMENT_NAME)) ? extraConfigs.get(SEGMENT_NAME)\n+        : String.format(\"%s_%s\", _table, DigestUtils.sha256Hex(_dataFilePath));\n+    segmentNameGeneratorSpec.setConfigs(ImmutableMap.of(SEGMENT_NAME, segmentName));\n+    spec.setSegmentNameGeneratorSpec(segmentNameGeneratorSpec);\n+\n+    // set PinotClusterSpecs\n+    PinotClusterSpec pinotClusterSpec = new PinotClusterSpec();\n+    pinotClusterSpec.setControllerURI(_controllerURI);\n+    PinotClusterSpec[] pinotClusterSpecs = new PinotClusterSpec[]{pinotClusterSpec};\n+    spec.setPinotClusterSpecs(pinotClusterSpecs);\n+\n+    // set PushJobSpec\n+    PushJobSpec pushJobSpec = new PushJobSpec();\n+    pushJobSpec.setPushAttempts(3);\n+    pushJobSpec.setPushRetryIntervalMillis(10000);\n+    spec.setPushJobSpec(pushJobSpec);\n+\n+    return spec;\n+  }\n+\n+  private Map<String, String> getS3PinotFSConfigs(Map<String, String> extraConfigs) {\n+    Map<String, String> s3PinotFSConfigs = new HashMap<>();\n+    s3PinotFSConfigs.put(\"region\", System.getProperty(\"AWS_REGION\", \"us-west-2\"));\n+    s3PinotFSConfigs.putAll(IngestionConfigUtils.getConfigMapWithPrefix(extraConfigs,\n+        BatchConfigProperties.INPUT_FS_PROP_PREFIX + IngestionConfigUtils.DOT_SEPARATOR));\n+    return s3PinotFSConfigs;\n+  }\n+\n+  private PinotFSSpec getPinotFSSpec(String scheme, String className, Map<String, String> configs) {\n+    PinotFSSpec pinotFSSpec = new PinotFSSpec();\n+    pinotFSSpec.setScheme(scheme);\n+    pinotFSSpec.setClassName(className);\n+    pinotFSSpec.setConfigs(configs);\n+    return pinotFSSpec;\n+  }\n+\n+  private Map<String, String> getExtraConfigs(List<String> extraConfigs) {\n+    if (extraConfigs == null) {\n+      return Collections.emptyMap();\n+    }\n+    Map<String, String> recordReaderConfigs = new HashMap<>();\n+    for (String kvPair : extraConfigs) {\n+      String[] splits = kvPair.split(\"=\", 2);\n+      if ((splits.length == 2) && (splits[0] != null) && (splits[1] != null)) {\n+        recordReaderConfigs.put(splits[0], splits[1]);\n+      }\n+    }\n+    return recordReaderConfigs;\n+  }\n+\n+  private String getRecordReaderConfigClass(FileFormat format) {\n+    switch (format) {\n+      case CSV:\n+        return \"org.apache.pinot.plugin.inputformat.csv.CSVRecordReaderConfig\";\n+      case PROTO:\n+        return \"org.apache.pinot.plugin.inputformat.protobuf.ProtoBufRecordReaderConfig\";\n+      case THRIFT:\n+        return \"org.apache.pinot.plugin.inputformat.thrift.ThriftRecordReaderConfig\";\n+      case ORC:\n+      case JSON:\n+      case AVRO:\n+      case GZIPPED_AVRO:\n+      case PARQUET:\n+        return null;\n+      default:\n+        throw new IllegalArgumentException(\"Unsupported file format - \" + format);\n+    }\n+  }\n+\n+  private String getRecordReaderClass(FileFormat format) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f924f2fa0a4e3c0460c6ae6d91b7b359357e57ba"}, "originalPosition": 319}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQyMDQ0MQ==", "bodyText": "Does this also support dataDir that contains multiple data files to be imported?", "url": "https://github.com/apache/pinot/pull/6396#discussion_r551420441", "createdAt": "2021-01-04T16:23:37Z", "author": {"login": "mayankshriv"}, "path": "pinot-tools/src/main/java/org/apache/pinot/tools/admin/command/ImportDataCommand.java", "diffHunk": "@@ -0,0 +1,340 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.tools.admin.command;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableMap;\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.commons.codec.digest.DigestUtils;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.controller.helix.ControllerRequestURLBuilder;\n+import org.apache.pinot.spi.data.readers.FileFormat;\n+import org.apache.pinot.spi.ingestion.batch.BatchConfigProperties;\n+import org.apache.pinot.spi.ingestion.batch.IngestionJobLauncher;\n+import org.apache.pinot.spi.ingestion.batch.spec.ExecutionFrameworkSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotClusterSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotFSSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PushJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.RecordReaderSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentGenerationJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentNameGeneratorSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.TableSpec;\n+import org.apache.pinot.spi.utils.IngestionConfigUtils;\n+import org.apache.pinot.tools.Command;\n+import org.kohsuke.args4j.Option;\n+import org.kohsuke.args4j.spi.StringArrayOptionHandler;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Class to implement ImportData command.\n+ */\n+@SuppressWarnings(\"unused\")\n+public class ImportDataCommand extends AbstractBaseAdminCommand implements Command {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(ImportDataCommand.class);\n+  private static final String SEGMENT_NAME = \"segment.name\";\n+\n+  @Option(name = \"-dataFilePath\", required = true, metaVar = \"<string>\", usage = \"data file path.\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f924f2fa0a4e3c0460c6ae6d91b7b359357e57ba"}, "originalPosition": 62}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f924f2fa0a4e3c0460c6ae6d91b7b359357e57ba", "author": {"user": {"login": "xiangfu0", "name": "Xiang Fu"}}, "url": "https://github.com/apache/pinot/commit/f924f2fa0a4e3c0460c6ae6d91b7b359357e57ba", "committedDate": "2020-12-31T09:15:53Z", "message": "Adding ImportData sub command for pinot-admin"}, "afterCommit": {"oid": "79feffc18ec44b980d0f6b9115c50659411dd653", "author": {"user": {"login": "xiangfu0", "name": "Xiang Fu"}}, "url": "https://github.com/apache/pinot/commit/79feffc18ec44b980d0f6b9115c50659411dd653", "committedDate": "2021-01-06T23:56:09Z", "message": "Address comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "79feffc18ec44b980d0f6b9115c50659411dd653", "author": {"user": {"login": "xiangfu0", "name": "Xiang Fu"}}, "url": "https://github.com/apache/pinot/commit/79feffc18ec44b980d0f6b9115c50659411dd653", "committedDate": "2021-01-06T23:56:09Z", "message": "Address comments"}, "afterCommit": {"oid": "ff29061a741f1d43e4ff7dbc4dbe8036dbf3aea9", "author": {"user": {"login": "xiangfu0", "name": "Xiang Fu"}}, "url": "https://github.com/apache/pinot/commit/ff29061a741f1d43e4ff7dbc4dbe8036dbf3aea9", "committedDate": "2021-01-07T01:54:19Z", "message": "Address comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ff29061a741f1d43e4ff7dbc4dbe8036dbf3aea9", "author": {"user": {"login": "xiangfu0", "name": "Xiang Fu"}}, "url": "https://github.com/apache/pinot/commit/ff29061a741f1d43e4ff7dbc4dbe8036dbf3aea9", "committedDate": "2021-01-07T01:54:19Z", "message": "Address comments"}, "afterCommit": {"oid": "3c037f54788c2c02dad1c131912748fad8b7e151", "author": {"user": {"login": "xiangfu0", "name": "Xiang Fu"}}, "url": "https://github.com/apache/pinot/commit/3c037f54788c2c02dad1c131912748fad8b7e151", "committedDate": "2021-01-08T01:29:10Z", "message": "Address comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3c037f54788c2c02dad1c131912748fad8b7e151", "author": {"user": {"login": "xiangfu0", "name": "Xiang Fu"}}, "url": "https://github.com/apache/pinot/commit/3c037f54788c2c02dad1c131912748fad8b7e151", "committedDate": "2021-01-08T01:29:10Z", "message": "Address comments"}, "afterCommit": {"oid": "f966c1ecc4086b3399d191ea882c4ac7ba89d338", "author": {"user": {"login": "xiangfu0", "name": "Xiang Fu"}}, "url": "https://github.com/apache/pinot/commit/f966c1ecc4086b3399d191ea882c4ac7ba89d338", "committedDate": "2021-01-08T01:32:10Z", "message": "Address comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTcwMTY1NjAw", "url": "https://github.com/apache/pinot/pull/6396#pullrequestreview-570165600", "createdAt": "2021-01-18T04:36:04Z", "commit": {"oid": "f966c1ecc4086b3399d191ea882c4ac7ba89d338"}, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQwNDozNjowNFrOIVZuIg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQwNDozNjo0MVrOIVZulQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTMxMjQxOA==", "bodyText": "Yeah, perhaps can be done outside of this PR.", "url": "https://github.com/apache/pinot/pull/6396#discussion_r559312418", "createdAt": "2021-01-18T04:36:04Z", "author": {"login": "mayankshriv"}, "path": "pinot-tools/src/main/java/org/apache/pinot/tools/admin/command/ImportDataCommand.java", "diffHunk": "@@ -0,0 +1,340 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.tools.admin.command;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableMap;\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.commons.codec.digest.DigestUtils;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.controller.helix.ControllerRequestURLBuilder;\n+import org.apache.pinot.spi.data.readers.FileFormat;\n+import org.apache.pinot.spi.ingestion.batch.BatchConfigProperties;\n+import org.apache.pinot.spi.ingestion.batch.IngestionJobLauncher;\n+import org.apache.pinot.spi.ingestion.batch.spec.ExecutionFrameworkSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotClusterSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotFSSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PushJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.RecordReaderSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentGenerationJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentNameGeneratorSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.TableSpec;\n+import org.apache.pinot.spi.utils.IngestionConfigUtils;\n+import org.apache.pinot.tools.Command;\n+import org.kohsuke.args4j.Option;\n+import org.kohsuke.args4j.spi.StringArrayOptionHandler;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Class to implement ImportData command.\n+ */\n+@SuppressWarnings(\"unused\")\n+public class ImportDataCommand extends AbstractBaseAdminCommand implements Command {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(ImportDataCommand.class);\n+  private static final String SEGMENT_NAME = \"segment.name\";\n+\n+  @Option(name = \"-dataFilePath\", required = true, metaVar = \"<string>\", usage = \"data file path.\")\n+  private String _dataFilePath;\n+\n+  @Option(name = \"-format\", required = true, metaVar = \"<AVRO/CSV/JSON/THRIFT/PARQUET/ORC>\", usage = \"Input data format.\")\n+  private FileFormat _format;\n+\n+  @Option(name = \"-table\", required = true, metaVar = \"<string>\", usage = \"Table name.\")\n+  private String _table;\n+\n+  @Option(name = \"-controllerURI\", metaVar = \"<string>\", usage = \"Pinot Controller URI.\")\n+  private String _controllerURI = \"http://localhost:9000\";\n+\n+  @Option(name = \"-tempDir\", metaVar = \"<string>\", usage = \"Temporary directory used to hold data during segment creation.\")\n+  private String _tempDir = new File(FileUtils.getTempDirectory(), getClass().getSimpleName()).getAbsolutePath();\n+\n+  @Option(name = \"-extraConfigs\", metaVar = \"<extra configs>\", handler = StringArrayOptionHandler.class, usage = \"Extra configs to be set.\")\n+  private List<String> _extraConfigs;\n+\n+  @SuppressWarnings(\"FieldCanBeLocal\")\n+  @Option(name = \"-help\", help = true, aliases = {\"-h\", \"--h\", \"--help\"}, usage = \"Print this message.\")\n+  private boolean _help = false;\n+\n+  public ImportDataCommand setDataFilePath(String dataFilePath) {\n+    _dataFilePath = dataFilePath;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setFormat(FileFormat format) {\n+    _format = format;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setTable(String table) {\n+    _table = table;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setControllerURI(String controllerURI) {\n+    _controllerURI = controllerURI;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setTempDir(String tempDir) {\n+    _tempDir = tempDir;\n+    return this;\n+  }\n+\n+  public List<String> getExtraConfigs() {\n+    return _extraConfigs;\n+  }\n+\n+  public ImportDataCommand setExtraConfigs(List<String> extraConfigs) {\n+    _extraConfigs = extraConfigs;\n+    return this;\n+  }\n+\n+  public String getDataFilePath() {\n+    return _dataFilePath;\n+  }\n+\n+  public FileFormat getFormat() {\n+    return _format;\n+  }\n+\n+  public String getTable() {\n+    return _table;\n+  }\n+\n+  public String getControllerURI() {\n+    return _controllerURI;\n+  }\n+\n+  public String getTempDir() {\n+    return _tempDir;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    String results = String\n+        .format(\"InsertData -dataFilePath %s -format %s -table %s -controllerURI %s -tempDir %s\", _dataFilePath,\n+            _format, _table, _controllerURI, _tempDir);\n+    if (_extraConfigs != null) {\n+      results += \" -extraConfigs \" + Arrays.toString(_extraConfigs.toArray());\n+    }\n+    return results;\n+  }\n+\n+  @Override\n+  public final String getName() {\n+    return \"InsertData\";\n+  }\n+\n+  @Override\n+  public String description() {\n+    return \"Insert data into Pinot cluster.\";\n+  }\n+\n+  @Override\n+  public boolean getHelp() {\n+    return _help;\n+  }\n+\n+  @Override\n+  public boolean execute()\n+      throws IOException {\n+    LOGGER.info(\"Executing command: {}\", toString());\n+    Preconditions.checkArgument(_table != null, \"'table' must be specified\");\n+    Preconditions.checkArgument(_format != null, \"'format' must be specified\");\n+    Preconditions.checkArgument(_dataFilePath != null, \"'dataFilePath' must be specified\");\n+\n+    try {\n+\n+      URI dataFileURI = URI.create(_dataFilePath);\n+      if ((dataFileURI.getScheme() == null)) {\n+        File dataFile = new File(_dataFilePath);\n+        Preconditions.checkArgument(dataFile.exists(), \"'dataFile': '%s' doesn't exist\", dataFile);\n+        LOGGER.info(\"Found data files: {} of format: {}\", dataFile, _format);\n+      }\n+\n+      initTempDir();\n+      IngestionJobLauncher.runIngestionJob(generateSegmentGenerationJobSpec());\n+      LOGGER.info(\"Successfully load data from {} to Pinot.\", _dataFilePath);\n+      return true;\n+    } catch (Exception e) {\n+      throw e;\n+    } finally {\n+      FileUtils.deleteQuietly(new File(_tempDir));\n+    }\n+  }\n+\n+  private void initTempDir()\n+      throws IOException {\n+    File tempDir = new File(_tempDir);\n+    if (tempDir.exists()) {\n+      LOGGER.info(\"Deleting the existing 'tempDir': {}\", tempDir);\n+      FileUtils.forceDelete(tempDir);\n+    }\n+    FileUtils.forceMkdir(tempDir);\n+  }\n+\n+  private SegmentGenerationJobSpec generateSegmentGenerationJobSpec() {\n+    final Map<String, String> extraConfigs = getExtraConfigs(_extraConfigs);\n+\n+    SegmentGenerationJobSpec spec = new SegmentGenerationJobSpec();\n+    URI dataFileURI = URI.create(_dataFilePath);\n+    URI parent = dataFileURI.getPath().endsWith(\"/\") ? dataFileURI.resolve(\"..\") : dataFileURI.resolve(\".\");\n+    spec.setInputDirURI(parent.toString());\n+    spec.setIncludeFileNamePattern(\"glob:**\" + dataFileURI.getPath());\n+    spec.setOutputDirURI(_tempDir);\n+    spec.setCleanUpOutputDir(true);\n+    spec.setOverwriteOutput(true);\n+    spec.setJobType(\"SegmentCreationAndTarPush\");\n+\n+    // set ExecutionFrameworkSpec\n+    ExecutionFrameworkSpec executionFrameworkSpec = new ExecutionFrameworkSpec();\n+    executionFrameworkSpec.setName(\"standalone\");\n+    executionFrameworkSpec.setSegmentGenerationJobRunnerClassName(\n+        \"org.apache.pinot.plugin.ingestion.batch.standalone.SegmentGenerationJobRunner\");\n+    executionFrameworkSpec.setSegmentTarPushJobRunnerClassName(\n+        \"org.apache.pinot.plugin.ingestion.batch.standalone.SegmentTarPushJobRunner\");\n+    spec.setExecutionFrameworkSpec(executionFrameworkSpec);\n+\n+    // set PinotFSSpecs\n+    List<PinotFSSpec> pinotFSSpecs = new ArrayList<>();\n+    pinotFSSpecs.add(getPinotFSSpec(\"file\", \"org.apache.pinot.spi.filesystem.LocalPinotFS\", Collections.emptyMap()));\n+    pinotFSSpecs\n+        .add(getPinotFSSpec(\"s3\", \"org.apache.pinot.plugin.filesystem.S3PinotFS\", getS3PinotFSConfigs(extraConfigs)));\n+    spec.setPinotFSSpecs(pinotFSSpecs);\n+\n+    // set RecordReaderSpec\n+    RecordReaderSpec recordReaderSpec = new RecordReaderSpec();\n+    recordReaderSpec.setDataFormat(_format.name());\n+    recordReaderSpec.setClassName(getRecordReaderClass(_format));\n+    recordReaderSpec.setConfigClassName(getRecordReaderConfigClass(_format));\n+    recordReaderSpec.setConfigs(IngestionConfigUtils.getRecordReaderProps(extraConfigs));\n+    spec.setRecordReaderSpec(recordReaderSpec);\n+\n+    // set TableSpec\n+    TableSpec tableSpec = new TableSpec();\n+    tableSpec.setTableName(_table);\n+    tableSpec.setSchemaURI(ControllerRequestURLBuilder.baseUrl(_controllerURI).forTableSchemaGet(_table));\n+    tableSpec.setTableConfigURI(ControllerRequestURLBuilder.baseUrl(_controllerURI).forTableGet(_table));\n+    spec.setTableSpec(tableSpec);\n+\n+    // set SegmentNameGeneratorSpec\n+    SegmentNameGeneratorSpec segmentNameGeneratorSpec = new SegmentNameGeneratorSpec();\n+    segmentNameGeneratorSpec\n+        .setType(org.apache.pinot.spi.ingestion.batch.BatchConfigProperties.SegmentNameGeneratorType.FIXED);\n+    String segmentName = (extraConfigs.containsKey(SEGMENT_NAME)) ? extraConfigs.get(SEGMENT_NAME)\n+        : String.format(\"%s_%s\", _table, DigestUtils.sha256Hex(_dataFilePath));\n+    segmentNameGeneratorSpec.setConfigs(ImmutableMap.of(SEGMENT_NAME, segmentName));\n+    spec.setSegmentNameGeneratorSpec(segmentNameGeneratorSpec);\n+\n+    // set PinotClusterSpecs\n+    PinotClusterSpec pinotClusterSpec = new PinotClusterSpec();\n+    pinotClusterSpec.setControllerURI(_controllerURI);\n+    PinotClusterSpec[] pinotClusterSpecs = new PinotClusterSpec[]{pinotClusterSpec};\n+    spec.setPinotClusterSpecs(pinotClusterSpecs);\n+\n+    // set PushJobSpec\n+    PushJobSpec pushJobSpec = new PushJobSpec();\n+    pushJobSpec.setPushAttempts(3);\n+    pushJobSpec.setPushRetryIntervalMillis(10000);\n+    spec.setPushJobSpec(pushJobSpec);\n+\n+    return spec;\n+  }\n+\n+  private Map<String, String> getS3PinotFSConfigs(Map<String, String> extraConfigs) {\n+    Map<String, String> s3PinotFSConfigs = new HashMap<>();\n+    s3PinotFSConfigs.put(\"region\", System.getProperty(\"AWS_REGION\", \"us-west-2\"));\n+    s3PinotFSConfigs.putAll(IngestionConfigUtils.getConfigMapWithPrefix(extraConfigs,\n+        BatchConfigProperties.INPUT_FS_PROP_PREFIX + IngestionConfigUtils.DOT_SEPARATOR));\n+    return s3PinotFSConfigs;\n+  }\n+\n+  private PinotFSSpec getPinotFSSpec(String scheme, String className, Map<String, String> configs) {\n+    PinotFSSpec pinotFSSpec = new PinotFSSpec();\n+    pinotFSSpec.setScheme(scheme);\n+    pinotFSSpec.setClassName(className);\n+    pinotFSSpec.setConfigs(configs);\n+    return pinotFSSpec;\n+  }\n+\n+  private Map<String, String> getExtraConfigs(List<String> extraConfigs) {\n+    if (extraConfigs == null) {\n+      return Collections.emptyMap();\n+    }\n+    Map<String, String> recordReaderConfigs = new HashMap<>();\n+    for (String kvPair : extraConfigs) {\n+      String[] splits = kvPair.split(\"=\", 2);\n+      if ((splits.length == 2) && (splits[0] != null) && (splits[1] != null)) {\n+        recordReaderConfigs.put(splits[0], splits[1]);\n+      }\n+    }\n+    return recordReaderConfigs;\n+  }\n+\n+  private String getRecordReaderConfigClass(FileFormat format) {\n+    switch (format) {\n+      case CSV:\n+        return \"org.apache.pinot.plugin.inputformat.csv.CSVRecordReaderConfig\";\n+      case PROTO:\n+        return \"org.apache.pinot.plugin.inputformat.protobuf.ProtoBufRecordReaderConfig\";\n+      case THRIFT:\n+        return \"org.apache.pinot.plugin.inputformat.thrift.ThriftRecordReaderConfig\";\n+      case ORC:\n+      case JSON:\n+      case AVRO:\n+      case GZIPPED_AVRO:\n+      case PARQUET:\n+        return null;\n+      default:\n+        throw new IllegalArgumentException(\"Unsupported file format - \" + format);\n+    }\n+  }\n+\n+  private String getRecordReaderClass(FileFormat format) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQxOTg2Mw=="}, "originalCommit": {"oid": "f924f2fa0a4e3c0460c6ae6d91b7b359357e57ba"}, "originalPosition": 319}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTMxMjUzMw==", "bodyText": "-additionalConfigs?", "url": "https://github.com/apache/pinot/pull/6396#discussion_r559312533", "createdAt": "2021-01-18T04:36:41Z", "author": {"login": "mayankshriv"}, "path": "pinot-tools/src/main/java/org/apache/pinot/tools/admin/command/ImportDataCommand.java", "diffHunk": "@@ -0,0 +1,390 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.tools.admin.command;\n+\n+import com.google.common.base.Preconditions;\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.commons.codec.digest.DigestUtils;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.controller.helix.ControllerRequestURLBuilder;\n+import org.apache.pinot.spi.data.readers.FileFormat;\n+import org.apache.pinot.spi.filesystem.PinotFSFactory;\n+import org.apache.pinot.spi.ingestion.batch.BatchConfigProperties;\n+import org.apache.pinot.spi.ingestion.batch.IngestionJobLauncher;\n+import org.apache.pinot.spi.ingestion.batch.spec.ExecutionFrameworkSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotClusterSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotFSSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PushJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.RecordReaderSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentGenerationJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentNameGeneratorSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.TableSpec;\n+import org.apache.pinot.spi.utils.IngestionConfigUtils;\n+import org.apache.pinot.tools.Command;\n+import org.kohsuke.args4j.Option;\n+import org.kohsuke.args4j.spi.StringArrayOptionHandler;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Class to implement ImportData command.\n+ */\n+@SuppressWarnings(\"unused\")\n+public class ImportDataCommand extends AbstractBaseAdminCommand implements Command {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(ImportDataCommand.class);\n+  private static final String SEGMENT_NAME = \"segment.name\";\n+\n+  @Option(name = \"-dataFilePath\", required = true, metaVar = \"<string>\", usage = \"data file path.\")\n+  private String _dataFilePath;\n+\n+  @Option(name = \"-format\", required = true, metaVar = \"<AVRO/CSV/JSON/THRIFT/PARQUET/ORC>\", usage = \"Input data format.\")\n+  private FileFormat _format;\n+\n+  @Option(name = \"-segmentNameGeneratorType\", metaVar = \"<AVRO/CSV/JSON/THRIFT/PARQUET/ORC>\", usage = \"Segment name generator type, default to FIXED type.\")\n+  private String _segmentNameGeneratorType = BatchConfigProperties.SegmentNameGeneratorType.FIXED;\n+\n+  @Option(name = \"-table\", required = true, metaVar = \"<string>\", usage = \"Table name.\")\n+  private String _table;\n+\n+  @Option(name = \"-controllerURI\", metaVar = \"<string>\", usage = \"Pinot Controller URI.\")\n+  private String _controllerURI = \"http://localhost:9000\";\n+\n+  @Option(name = \"-tempDir\", metaVar = \"<string>\", usage = \"Temporary directory used to hold data during segment creation.\")\n+  private String _tempDir = new File(FileUtils.getTempDirectory(), getClass().getSimpleName()).getAbsolutePath();\n+\n+  @Option(name = \"-extraConfigs\", metaVar = \"<extra configs>\", handler = StringArrayOptionHandler.class, usage = \"Extra configs to be set.\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f966c1ecc4086b3399d191ea882c4ac7ba89d338"}, "originalPosition": 80}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "15692af5f602f579671ed90ef7b9c4327542431e", "author": {"user": {"login": "xiangfu0", "name": "Xiang Fu"}}, "url": "https://github.com/apache/pinot/commit/15692af5f602f579671ed90ef7b9c4327542431e", "committedDate": "2021-01-18T04:47:34Z", "message": "Adding ImportData sub command for pinot-admin"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6a0b9ddf964f8f5d75ef60c6f057a84acbc310c0", "author": {"user": {"login": "xiangfu0", "name": "Xiang Fu"}}, "url": "https://github.com/apache/pinot/commit/6a0b9ddf964f8f5d75ef60c6f057a84acbc310c0", "committedDate": "2021-01-18T04:47:34Z", "message": "Address comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e2a0fc9de417f32fd91ecd4ff134c5d89567ea50", "author": {"user": {"login": "xiangfu0", "name": "Xiang Fu"}}, "url": "https://github.com/apache/pinot/commit/e2a0fc9de417f32fd91ecd4ff134c5d89567ea50", "committedDate": "2021-01-18T04:47:34Z", "message": "Change extraConfigs to additionalConfigs"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f966c1ecc4086b3399d191ea882c4ac7ba89d338", "author": {"user": {"login": "xiangfu0", "name": "Xiang Fu"}}, "url": "https://github.com/apache/pinot/commit/f966c1ecc4086b3399d191ea882c4ac7ba89d338", "committedDate": "2021-01-08T01:32:10Z", "message": "Address comments"}, "afterCommit": {"oid": "e2a0fc9de417f32fd91ecd4ff134c5d89567ea50", "author": {"user": {"login": "xiangfu0", "name": "Xiang Fu"}}, "url": "https://github.com/apache/pinot/commit/e2a0fc9de417f32fd91ecd4ff134c5d89567ea50", "committedDate": "2021-01-18T04:47:34Z", "message": "Change extraConfigs to additionalConfigs"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1710, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}