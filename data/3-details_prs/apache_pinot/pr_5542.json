{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDMzMjc3NjE4", "number": 5542, "title": "Changed the stream and metadata interface", "bodyText": "Changed the interface exposed by streams to return StreamPartitionMsgOffset\ninstead of long.\nAlso changed the LLCRealtimeSegmentZKMetadat class to return String instead\nof long offsets, since it is stored as a String anyway. Luckily zk metadata does\nnot generate java objects from json automatically (instead, parses the\nfields one by one via custom code).\nThe segment endOffset can now be null in LLCRealtimeSegmentZKMetadata. For\nbackward compatibility, we store Long.toSring(Long.MAX_VALUE) in zookeeper\nso that readers of the metadata do not get NPE if they are still running old\nversion. We will remove this workaround after we release 0.5.0\nThe segment completion protocol for realtime LLC segment completion will\ncontinue to use both 'offset' and 'streamPartitionOffset' elements for\none more release, and will move to use the latter completely in 0.6.0\nInterfaces PartitionLevelConsumer and StreamMetadataProvider (marked STABLE)\nare now deprecating the calls that use long offsets. Instead, new calls have\nbeen added that use StreamPartitionMsgOffset. Kafka Plugins have been updated\nto use the new calls, but the old methods are preserved to make sure that any\nother Kafka implementations have time to move over. These calls will be removed\nin 0.6.0\nThe metric to show the highest offset consumed has disappeared.\nTesting done:\nManually verified that LLCRealtimeClusterIntegrationTest completes segments correctly with\nold server and new controller as well as with new server and old controller, using\ncommand line starter for controller and server to start a component at a specific\nrevision after 0.4.0 release.\nIssue #5359\nDescription\nAdd a description of your PR here.\nA good description should include pointers to an issue or design document, etc.\nUpgrade Notes\nDoes this PR prevent a zero down-time upgrade? (Assume upgrade order: Controller, Broker, Server, Minion)\n\n Yes (Please label as backward-incompat, and complete the section below on Release Notes)\n\nDoes this PR fix a zero-downtime upgrade introduced earlier?\n\n Yes (Please label this as backward-incompat, and complete the section below on Release Notes)\n\nDoes this PR otherwise need attention when creating release notes? Things to consider:\n\nNew configuration options\nDeprecation of configurations\nSignature changes to public methods/interfaces\nNew plugins added or old plugins removed\n\n\n Yes (Please label this PR as release-notes and complete the section on Release Notes)\n\nRelease Notes\nIf you have tagged this as either backward-incompat or release-notes,\nyou MUST add text here that you would like to see appear in release notes of the\nnext release.\nIf you have a series of commits adding or enabling a feature, then\nadd this section only in final commit that marks the feature completed.\nRefer to earlier release notes to see examples of text\nDocumentation\nIf you have introduced a new feature or configuration, please add it to the documentation as well.\nSee https://docs.pinot.apache.org/developers/developers-and-contributors/update-document", "createdAt": "2020-06-11T19:09:52Z", "url": "https://github.com/apache/pinot/pull/5542", "merged": true, "mergeCommit": {"oid": "f1f1c41d9f661d3ec1eb37c863d0049cbe5a434a"}, "closed": true, "closedAt": "2020-06-11T21:48:40Z", "author": {"login": "mcvsubbu"}, "timelineItems": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcqTHBXAH2gAyNDMzMjc3NjE4OjIwMGJiNzVjM2M4NjJjNGQxMDZjYTExNjk3YjJlMDFjYWIzZmRiMjY=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcqUn_agFqTQyOTI3NzY5NA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "200bb75c3c862c4d106ca11697b2e01cab3fdb26", "author": {"user": {"login": "mcvsubbu", "name": "Subbu Subramaniam"}}, "url": "https://github.com/apache/pinot/commit/200bb75c3c862c4d106ca11697b2e01cab3fdb26", "committedDate": "2020-06-11T19:06:46Z", "message": "Changed the stream and metadata interface\n\nChanged the interface exposed by streams to return StreamPartitionMsgOffset\ninstead of long.\n\nAlso changed the LLCRealtimeSegmentZKMetadat class to return String instead\nof long offsets, since it is stored as a String anyway. Luckily zk metadata does\nnot generate java objects from json automatically (instead, parses the\nfields one by one via custom code).\n\nThe segment endOffset can now be null in LLCRealtimeSegmentZKMetadata. For\nbackward compatibility, we store Long.toSring(Long.MAX_VALUE) in zookeeper\nso that readers of the metadata do not get NPE if they are still running old\nversion. We will remove this workaround after we release 0.5.0\n\nThe segment completion protocol for realtime LLC segment completion will\ncontinue to use both 'offset' and 'streamPartitionOffset' elements for\none more release, and will move to use the latter completely in 0.6.0\n\nInterfaces PartitionLevelConsumer and StreamMetadataProvider (marked STABLE)\nare now deprecating the calls that use long offsets. Instead, new calls have\nbeen added that use StreamPartitionMsgOffset. Kafka Plugins have been updated\nto use the new calls, but the old methods are preserved to make sure that any\nother Kafka implementations have time to move over. These calls will be removed\nin 0.6.0\n\nThe metric to show the highest offset consumed has disappeared.\n\nTesting done:\nManually verified that LLCRealtimeClusterIntegrationTest completes segments correctly with\nold server and new controller as well as with new server and old controller, using\ncommand line starter for controller and server to start a component at a specific\nrevision after 0.4.0 release.\n\nThis PR concludes the work for this Issue\n\nIssue #5359"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI5Mjc3Njk0", "url": "https://github.com/apache/pinot/pull/5542#pullrequestreview-429277694", "createdAt": "2020-06-11T20:47:37Z", "commit": {"oid": "200bb75c3c862c4d106ca11697b2e01cab3fdb26"}, "state": "APPROVED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQyMDo0NzozN1rOGiuPFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQyMDo1MDo0NFrOGiuUeA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTA2MjI5Mg==", "bodyText": "We can drop Msg, StreamPartitionOffset", "url": "https://github.com/apache/pinot/pull/5542#discussion_r439062292", "createdAt": "2020-06-11T20:47:37Z", "author": {"login": "kishoreg"}, "path": "pinot-controller/src/main/java/org/apache/pinot/controller/helix/core/realtime/PinotLLCRealtimeSegmentManager.java", "diffHunk": "@@ -893,20 +891,19 @@ IdealState ensureAllPartitionsConsuming(TableConfig tableConfig, PartitionLevelS\n \n             // Create a new segment to re-consume from the previous start offset\n             LLCSegmentName newLLCSegmentName = getNextLLCSegmentName(latestLLCSegmentName, currentTimeMs);\n-            long startOffset = latestSegmentZKMetadata.getStartOffset();\n+            StreamPartitionMsgOffset startOffset = offsetFactory.create(latestSegmentZKMetadata.getStartOffset());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "200bb75c3c862c4d106ca11697b2e01cab3fdb26"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTA2MjQwNQ==", "bodyText": "LongOffset", "url": "https://github.com/apache/pinot/pull/5542#discussion_r439062405", "createdAt": "2020-06-11T20:47:50Z", "author": {"login": "kishoreg"}, "path": "pinot-controller/src/test/java/org/apache/pinot/controller/helix/core/realtime/PinotLLCRealtimeSegmentManagerTest.java", "diffHunk": "@@ -77,7 +79,7 @@\n \n   private static final long RANDOM_SEED = System.currentTimeMillis();\n   private static final Random RANDOM = new Random(RANDOM_SEED);\n-  static final long PARTITION_OFFSET = RANDOM.nextInt(Integer.MAX_VALUE);\n+  static final LongMsgOffset PARTITION_OFFSET = new LongMsgOffset(RANDOM.nextInt(Integer.MAX_VALUE));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "200bb75c3c862c4d106ca11697b2e01cab3fdb26"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTA2MzY3Mg==", "bodyText": "Nicely done!", "url": "https://github.com/apache/pinot/pull/5542#discussion_r439063672", "createdAt": "2020-06-11T20:50:44Z", "author": {"login": "kishoreg"}, "path": "pinot-spi/src/main/java/org/apache/pinot/spi/stream/PartitionLevelConsumer.java", "diffHunk": "@@ -31,19 +31,35 @@\n public interface PartitionLevelConsumer extends Closeable {\n \n   /**\n+   * Is here for backward compatibility for a short time.\n+   * TODO Issue 5359 remove this API once external kafka consumers implements return of StreamPartitionMsgOffset\n    * Fetch messages from the stream between the specified offsets\n    * @param startOffset\n    * @param endOffset\n    * @param timeoutMillis\n    * @return\n    * @throws java.util.concurrent.TimeoutException\n    */\n+  @Deprecated\n   MessageBatch fetchMessages(long startOffset, long endOffset, int timeoutMillis)\n       throws java.util.concurrent.TimeoutException;\n \n+  /**\n+   * Fetch messages and the per-partition high watermark from Kafka between the specified offsets.\n+   *\n+   * @param startOffset The offset of the first message desired, inclusive\n+   * @param endOffset The offset of the last message desired, exclusive, or null\n+   * @param timeoutMillis Timeout in milliseconds\n+   * @throws java.util.concurrent.TimeoutException If the operation could not be completed within {@code timeoutMillis}\n+   * milliseconds\n+   * @return An iterable containing messages fetched from the stream partition and their offsets, as well as the\n+   * high watermark for this partition.\n+   */\n   default MessageBatch fetchMessages(StreamPartitionMsgOffset startOffset, StreamPartitionMsgOffset endOffset, int timeoutMillis)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "200bb75c3c862c4d106ca11697b2e01cab3fdb26"}, "originalPosition": 28}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 581, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}