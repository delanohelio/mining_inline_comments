{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDYzNjkzNjQz", "number": 5816, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwMDo0NzoxN1rOEVoETQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwMDo1NDo1MFrOEVoJcQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMTEyMDEzOnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/index/readers/forward/VarByteChunkSVForwardIndexReader.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwMDo0NzoxN1rOG8fXHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwMDo1NTo1NVrOG8fgEA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA4MTU2NQ==", "bodyText": "typo; trunk -> chunk", "url": "https://github.com/apache/pinot/pull/5816#discussion_r466081565", "createdAt": "2020-08-06T00:47:17Z", "author": {"login": "siddharthteotia"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/index/readers/forward/VarByteChunkSVForwardIndexReader.java", "diffHunk": "@@ -19,92 +19,172 @@\n package org.apache.pinot.core.segment.index.readers.forward;\n \n import java.nio.ByteBuffer;\n+import javax.annotation.Nullable;\n import org.apache.pinot.common.utils.StringUtil;\n import org.apache.pinot.core.io.writer.impl.VarByteChunkSVForwardIndexWriter;\n import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n import org.apache.pinot.spi.data.FieldSpec.DataType;\n \n \n /**\n- * Chunk-based single-value raw (non-dictionary-encoded) forward index reader for values of  of variable length data\n- * type (STRING, BYTES).\n+ * Chunk-based single-value raw (non-dictionary-encoded) forward index reader for values of variable length data type\n+ * (STRING, BYTES).\n  * <p>For data layout, please refer to the documentation for {@link VarByteChunkSVForwardIndexWriter}\n  */\n public final class VarByteChunkSVForwardIndexReader extends BaseChunkSVForwardIndexReader {\n+  private static final int ROW_OFFSET_SIZE = VarByteChunkSVForwardIndexWriter.CHUNK_HEADER_ENTRY_ROW_OFFSET_SIZE;\n+\n   private final int _maxChunkSize;\n \n   // Thread local (reusable) byte[] to read bytes from data file.\n   private final ThreadLocal<byte[]> _reusableBytes = ThreadLocal.withInitial(() -> new byte[_lengthOfLongestEntry]);\n \n   public VarByteChunkSVForwardIndexReader(PinotDataBuffer dataBuffer, DataType valueType) {\n     super(dataBuffer, valueType);\n-    _maxChunkSize = _numDocsPerChunk * (VarByteChunkSVForwardIndexWriter.CHUNK_HEADER_ENTRY_ROW_OFFSET_SIZE\n-        + _lengthOfLongestEntry);\n+    _maxChunkSize = _numDocsPerChunk * (ROW_OFFSET_SIZE + _lengthOfLongestEntry);\n   }\n \n+  @Nullable\n   @Override\n   public ChunkReaderContext createContext() {\n-    return new ChunkReaderContext(_maxChunkSize);\n+    if (_isCompressed) {\n+      return new ChunkReaderContext(_maxChunkSize);\n+    } else {\n+      return null;\n+    }\n   }\n \n   @Override\n   public String getString(int docId, ChunkReaderContext context) {\n+    if (_isCompressed) {\n+      return getStringCompressed(docId, context);\n+    } else {\n+      return getStringUncompressed(docId);\n+    }\n+  }\n+\n+  /**\n+   * Helper method to read STRING value from the compressed index.\n+   */\n+  private String getStringCompressed(int docId, ChunkReaderContext context) {\n     int chunkRowId = docId % _numDocsPerChunk;\n     ByteBuffer chunkBuffer = getChunkBuffer(docId, context);\n \n-    int rowOffset =\n-        chunkBuffer.getInt(chunkRowId * VarByteChunkSVForwardIndexWriter.CHUNK_HEADER_ENTRY_ROW_OFFSET_SIZE);\n-    int nextRowOffset = getNextRowOffset(chunkRowId, chunkBuffer);\n+    // These offsets are offset in the chunk buffer\n+    int valueStartOffset = chunkBuffer.getInt(chunkRowId * ROW_OFFSET_SIZE);\n+    int valueEndOffset = getValueEndOffset(chunkRowId, chunkBuffer);\n \n-    int length = nextRowOffset - rowOffset;\n+    int length = valueEndOffset - valueStartOffset;\n     byte[] bytes = _reusableBytes.get();\n-\n-    chunkBuffer.position(rowOffset);\n+    chunkBuffer.position(valueStartOffset);\n     chunkBuffer.get(bytes, 0, length);\n+    return StringUtil.decodeUtf8(bytes, 0, length);\n+  }\n+\n+  /**\n+   * Helper method to read STRING value from the uncompressed index.\n+   */\n+  private String getStringUncompressed(int docId) {\n+    int chunkId = docId / _numDocsPerChunk;\n+    int chunkRowId = docId % _numDocsPerChunk;\n+\n+    // These offsets are offset in the data buffer\n+    long chunkStartOffset = getChunkPosition(chunkId);\n+    long valueStartOffset = chunkStartOffset + _dataBuffer.getInt(chunkStartOffset + chunkRowId * ROW_OFFSET_SIZE);\n+    long valueEndOffset = getValueEndOffset(chunkId, chunkRowId, chunkStartOffset);\n \n+    int length = (int) (valueEndOffset - valueStartOffset);\n+    byte[] bytes = _reusableBytes.get();\n+    _dataBuffer.copyTo(valueStartOffset, bytes, 0, length);\n     return StringUtil.decodeUtf8(bytes, 0, length);\n   }\n \n   @Override\n   public byte[] getBytes(int docId, ChunkReaderContext context) {\n+    if (_isCompressed) {\n+      return getBytesCompressed(docId, context);\n+    } else {\n+      return getBytesUncompressed(docId);\n+    }\n+  }\n+\n+  /**\n+   * Helper method to read BYTES value from the compressed index.\n+   */\n+  private byte[] getBytesCompressed(int docId, ChunkReaderContext context) {\n     int chunkRowId = docId % _numDocsPerChunk;\n     ByteBuffer chunkBuffer = getChunkBuffer(docId, context);\n \n-    int rowOffset =\n-        chunkBuffer.getInt(chunkRowId * VarByteChunkSVForwardIndexWriter.CHUNK_HEADER_ENTRY_ROW_OFFSET_SIZE);\n-    int nextRowOffset = getNextRowOffset(chunkRowId, chunkBuffer);\n+    // These offsets are offset in the chunk buffer\n+    int valueStartOffset = chunkBuffer.getInt(chunkRowId * ROW_OFFSET_SIZE);\n+    int valueEndOffset = getValueEndOffset(chunkRowId, chunkBuffer);\n \n-    int length = nextRowOffset - rowOffset;\n-    byte[] bytes = new byte[length];\n+    byte[] bytes = new byte[valueEndOffset - valueStartOffset];\n+    chunkBuffer.position(valueStartOffset);\n+    chunkBuffer.get(bytes);\n+    return bytes;\n+  }\n \n-    chunkBuffer.position(rowOffset);\n-    chunkBuffer.get(bytes, 0, length);\n+  /**\n+   * Helper method to read BYTES value from the uncompressed index.\n+   */\n+  private byte[] getBytesUncompressed(int docId) {\n+    int chunkId = docId / _numDocsPerChunk;\n+    int chunkRowId = docId % _numDocsPerChunk;\n+\n+    // These offsets are offset in the data buffer\n+    long chunkStartOffset = getChunkPosition(chunkId);\n+    long valueStartOffset = chunkStartOffset + _dataBuffer.getInt(chunkStartOffset + chunkRowId * ROW_OFFSET_SIZE);\n+    long valueEndOffset = getValueEndOffset(chunkId, chunkRowId, chunkStartOffset);\n+\n+    byte[] bytes = new byte[(int) (valueEndOffset - valueStartOffset)];\n+    _dataBuffer.copyTo(valueStartOffset, bytes);\n     return bytes;\n   }\n \n   /**\n-   * Helper method to compute the offset of next row in the chunk buffer.\n-   *\n-   * @param currentRowId Current row id within the chunk buffer.\n-   * @param chunkBuffer Chunk buffer containing the rows.\n-   *\n-   * @return Offset of next row within the chunk buffer. If current row is the last one,\n-   * chunkBuffer.limit() is returned.\n+   * Helper method to compute the end offset of the value in the chunk buffer.\n    */\n-  private int getNextRowOffset(int currentRowId, ByteBuffer chunkBuffer) {\n-    int nextRowOffset;\n+  private int getValueEndOffset(int rowId, ByteBuffer chunkBuffer) {\n+    if (rowId == _numDocsPerChunk - 1) {\n+      // Last row in the trunk", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6deb16d447bdef5356709e7583fa5e6f0f7d9d0"}, "originalPosition": 159}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA4Mzg1Ng==", "bodyText": "Good catch", "url": "https://github.com/apache/pinot/pull/5816#discussion_r466083856", "createdAt": "2020-08-06T00:55:55Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/index/readers/forward/VarByteChunkSVForwardIndexReader.java", "diffHunk": "@@ -19,92 +19,172 @@\n package org.apache.pinot.core.segment.index.readers.forward;\n \n import java.nio.ByteBuffer;\n+import javax.annotation.Nullable;\n import org.apache.pinot.common.utils.StringUtil;\n import org.apache.pinot.core.io.writer.impl.VarByteChunkSVForwardIndexWriter;\n import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n import org.apache.pinot.spi.data.FieldSpec.DataType;\n \n \n /**\n- * Chunk-based single-value raw (non-dictionary-encoded) forward index reader for values of  of variable length data\n- * type (STRING, BYTES).\n+ * Chunk-based single-value raw (non-dictionary-encoded) forward index reader for values of variable length data type\n+ * (STRING, BYTES).\n  * <p>For data layout, please refer to the documentation for {@link VarByteChunkSVForwardIndexWriter}\n  */\n public final class VarByteChunkSVForwardIndexReader extends BaseChunkSVForwardIndexReader {\n+  private static final int ROW_OFFSET_SIZE = VarByteChunkSVForwardIndexWriter.CHUNK_HEADER_ENTRY_ROW_OFFSET_SIZE;\n+\n   private final int _maxChunkSize;\n \n   // Thread local (reusable) byte[] to read bytes from data file.\n   private final ThreadLocal<byte[]> _reusableBytes = ThreadLocal.withInitial(() -> new byte[_lengthOfLongestEntry]);\n \n   public VarByteChunkSVForwardIndexReader(PinotDataBuffer dataBuffer, DataType valueType) {\n     super(dataBuffer, valueType);\n-    _maxChunkSize = _numDocsPerChunk * (VarByteChunkSVForwardIndexWriter.CHUNK_HEADER_ENTRY_ROW_OFFSET_SIZE\n-        + _lengthOfLongestEntry);\n+    _maxChunkSize = _numDocsPerChunk * (ROW_OFFSET_SIZE + _lengthOfLongestEntry);\n   }\n \n+  @Nullable\n   @Override\n   public ChunkReaderContext createContext() {\n-    return new ChunkReaderContext(_maxChunkSize);\n+    if (_isCompressed) {\n+      return new ChunkReaderContext(_maxChunkSize);\n+    } else {\n+      return null;\n+    }\n   }\n \n   @Override\n   public String getString(int docId, ChunkReaderContext context) {\n+    if (_isCompressed) {\n+      return getStringCompressed(docId, context);\n+    } else {\n+      return getStringUncompressed(docId);\n+    }\n+  }\n+\n+  /**\n+   * Helper method to read STRING value from the compressed index.\n+   */\n+  private String getStringCompressed(int docId, ChunkReaderContext context) {\n     int chunkRowId = docId % _numDocsPerChunk;\n     ByteBuffer chunkBuffer = getChunkBuffer(docId, context);\n \n-    int rowOffset =\n-        chunkBuffer.getInt(chunkRowId * VarByteChunkSVForwardIndexWriter.CHUNK_HEADER_ENTRY_ROW_OFFSET_SIZE);\n-    int nextRowOffset = getNextRowOffset(chunkRowId, chunkBuffer);\n+    // These offsets are offset in the chunk buffer\n+    int valueStartOffset = chunkBuffer.getInt(chunkRowId * ROW_OFFSET_SIZE);\n+    int valueEndOffset = getValueEndOffset(chunkRowId, chunkBuffer);\n \n-    int length = nextRowOffset - rowOffset;\n+    int length = valueEndOffset - valueStartOffset;\n     byte[] bytes = _reusableBytes.get();\n-\n-    chunkBuffer.position(rowOffset);\n+    chunkBuffer.position(valueStartOffset);\n     chunkBuffer.get(bytes, 0, length);\n+    return StringUtil.decodeUtf8(bytes, 0, length);\n+  }\n+\n+  /**\n+   * Helper method to read STRING value from the uncompressed index.\n+   */\n+  private String getStringUncompressed(int docId) {\n+    int chunkId = docId / _numDocsPerChunk;\n+    int chunkRowId = docId % _numDocsPerChunk;\n+\n+    // These offsets are offset in the data buffer\n+    long chunkStartOffset = getChunkPosition(chunkId);\n+    long valueStartOffset = chunkStartOffset + _dataBuffer.getInt(chunkStartOffset + chunkRowId * ROW_OFFSET_SIZE);\n+    long valueEndOffset = getValueEndOffset(chunkId, chunkRowId, chunkStartOffset);\n \n+    int length = (int) (valueEndOffset - valueStartOffset);\n+    byte[] bytes = _reusableBytes.get();\n+    _dataBuffer.copyTo(valueStartOffset, bytes, 0, length);\n     return StringUtil.decodeUtf8(bytes, 0, length);\n   }\n \n   @Override\n   public byte[] getBytes(int docId, ChunkReaderContext context) {\n+    if (_isCompressed) {\n+      return getBytesCompressed(docId, context);\n+    } else {\n+      return getBytesUncompressed(docId);\n+    }\n+  }\n+\n+  /**\n+   * Helper method to read BYTES value from the compressed index.\n+   */\n+  private byte[] getBytesCompressed(int docId, ChunkReaderContext context) {\n     int chunkRowId = docId % _numDocsPerChunk;\n     ByteBuffer chunkBuffer = getChunkBuffer(docId, context);\n \n-    int rowOffset =\n-        chunkBuffer.getInt(chunkRowId * VarByteChunkSVForwardIndexWriter.CHUNK_HEADER_ENTRY_ROW_OFFSET_SIZE);\n-    int nextRowOffset = getNextRowOffset(chunkRowId, chunkBuffer);\n+    // These offsets are offset in the chunk buffer\n+    int valueStartOffset = chunkBuffer.getInt(chunkRowId * ROW_OFFSET_SIZE);\n+    int valueEndOffset = getValueEndOffset(chunkRowId, chunkBuffer);\n \n-    int length = nextRowOffset - rowOffset;\n-    byte[] bytes = new byte[length];\n+    byte[] bytes = new byte[valueEndOffset - valueStartOffset];\n+    chunkBuffer.position(valueStartOffset);\n+    chunkBuffer.get(bytes);\n+    return bytes;\n+  }\n \n-    chunkBuffer.position(rowOffset);\n-    chunkBuffer.get(bytes, 0, length);\n+  /**\n+   * Helper method to read BYTES value from the uncompressed index.\n+   */\n+  private byte[] getBytesUncompressed(int docId) {\n+    int chunkId = docId / _numDocsPerChunk;\n+    int chunkRowId = docId % _numDocsPerChunk;\n+\n+    // These offsets are offset in the data buffer\n+    long chunkStartOffset = getChunkPosition(chunkId);\n+    long valueStartOffset = chunkStartOffset + _dataBuffer.getInt(chunkStartOffset + chunkRowId * ROW_OFFSET_SIZE);\n+    long valueEndOffset = getValueEndOffset(chunkId, chunkRowId, chunkStartOffset);\n+\n+    byte[] bytes = new byte[(int) (valueEndOffset - valueStartOffset)];\n+    _dataBuffer.copyTo(valueStartOffset, bytes);\n     return bytes;\n   }\n \n   /**\n-   * Helper method to compute the offset of next row in the chunk buffer.\n-   *\n-   * @param currentRowId Current row id within the chunk buffer.\n-   * @param chunkBuffer Chunk buffer containing the rows.\n-   *\n-   * @return Offset of next row within the chunk buffer. If current row is the last one,\n-   * chunkBuffer.limit() is returned.\n+   * Helper method to compute the end offset of the value in the chunk buffer.\n    */\n-  private int getNextRowOffset(int currentRowId, ByteBuffer chunkBuffer) {\n-    int nextRowOffset;\n+  private int getValueEndOffset(int rowId, ByteBuffer chunkBuffer) {\n+    if (rowId == _numDocsPerChunk - 1) {\n+      // Last row in the trunk", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA4MTU2NQ=="}, "originalCommit": {"oid": "d6deb16d447bdef5356709e7583fa5e6f0f7d9d0"}, "originalPosition": 159}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMTEzMzI5OnYy", "diffSide": "RIGHT", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/index/readers/forward/VarByteChunkSVForwardIndexReader.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwMDo1NDo1MFrOG8fezw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwMTowMDozN1rOG8flAg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA4MzUzNQ==", "bodyText": "Why the algorithm for getting endoffset or startOffset for next row is different for uncompressed?", "url": "https://github.com/apache/pinot/pull/5816#discussion_r466083535", "createdAt": "2020-08-06T00:54:50Z", "author": {"login": "siddharthteotia"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/index/readers/forward/VarByteChunkSVForwardIndexReader.java", "diffHunk": "@@ -19,92 +19,172 @@\n package org.apache.pinot.core.segment.index.readers.forward;\n \n import java.nio.ByteBuffer;\n+import javax.annotation.Nullable;\n import org.apache.pinot.common.utils.StringUtil;\n import org.apache.pinot.core.io.writer.impl.VarByteChunkSVForwardIndexWriter;\n import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n import org.apache.pinot.spi.data.FieldSpec.DataType;\n \n \n /**\n- * Chunk-based single-value raw (non-dictionary-encoded) forward index reader for values of  of variable length data\n- * type (STRING, BYTES).\n+ * Chunk-based single-value raw (non-dictionary-encoded) forward index reader for values of variable length data type\n+ * (STRING, BYTES).\n  * <p>For data layout, please refer to the documentation for {@link VarByteChunkSVForwardIndexWriter}\n  */\n public final class VarByteChunkSVForwardIndexReader extends BaseChunkSVForwardIndexReader {\n+  private static final int ROW_OFFSET_SIZE = VarByteChunkSVForwardIndexWriter.CHUNK_HEADER_ENTRY_ROW_OFFSET_SIZE;\n+\n   private final int _maxChunkSize;\n \n   // Thread local (reusable) byte[] to read bytes from data file.\n   private final ThreadLocal<byte[]> _reusableBytes = ThreadLocal.withInitial(() -> new byte[_lengthOfLongestEntry]);\n \n   public VarByteChunkSVForwardIndexReader(PinotDataBuffer dataBuffer, DataType valueType) {\n     super(dataBuffer, valueType);\n-    _maxChunkSize = _numDocsPerChunk * (VarByteChunkSVForwardIndexWriter.CHUNK_HEADER_ENTRY_ROW_OFFSET_SIZE\n-        + _lengthOfLongestEntry);\n+    _maxChunkSize = _numDocsPerChunk * (ROW_OFFSET_SIZE + _lengthOfLongestEntry);\n   }\n \n+  @Nullable\n   @Override\n   public ChunkReaderContext createContext() {\n-    return new ChunkReaderContext(_maxChunkSize);\n+    if (_isCompressed) {\n+      return new ChunkReaderContext(_maxChunkSize);\n+    } else {\n+      return null;\n+    }\n   }\n \n   @Override\n   public String getString(int docId, ChunkReaderContext context) {\n+    if (_isCompressed) {\n+      return getStringCompressed(docId, context);\n+    } else {\n+      return getStringUncompressed(docId);\n+    }\n+  }\n+\n+  /**\n+   * Helper method to read STRING value from the compressed index.\n+   */\n+  private String getStringCompressed(int docId, ChunkReaderContext context) {\n     int chunkRowId = docId % _numDocsPerChunk;\n     ByteBuffer chunkBuffer = getChunkBuffer(docId, context);\n \n-    int rowOffset =\n-        chunkBuffer.getInt(chunkRowId * VarByteChunkSVForwardIndexWriter.CHUNK_HEADER_ENTRY_ROW_OFFSET_SIZE);\n-    int nextRowOffset = getNextRowOffset(chunkRowId, chunkBuffer);\n+    // These offsets are offset in the chunk buffer\n+    int valueStartOffset = chunkBuffer.getInt(chunkRowId * ROW_OFFSET_SIZE);\n+    int valueEndOffset = getValueEndOffset(chunkRowId, chunkBuffer);\n \n-    int length = nextRowOffset - rowOffset;\n+    int length = valueEndOffset - valueStartOffset;\n     byte[] bytes = _reusableBytes.get();\n-\n-    chunkBuffer.position(rowOffset);\n+    chunkBuffer.position(valueStartOffset);\n     chunkBuffer.get(bytes, 0, length);\n+    return StringUtil.decodeUtf8(bytes, 0, length);\n+  }\n+\n+  /**\n+   * Helper method to read STRING value from the uncompressed index.\n+   */\n+  private String getStringUncompressed(int docId) {\n+    int chunkId = docId / _numDocsPerChunk;\n+    int chunkRowId = docId % _numDocsPerChunk;\n+\n+    // These offsets are offset in the data buffer\n+    long chunkStartOffset = getChunkPosition(chunkId);\n+    long valueStartOffset = chunkStartOffset + _dataBuffer.getInt(chunkStartOffset + chunkRowId * ROW_OFFSET_SIZE);\n+    long valueEndOffset = getValueEndOffset(chunkId, chunkRowId, chunkStartOffset);\n \n+    int length = (int) (valueEndOffset - valueStartOffset);\n+    byte[] bytes = _reusableBytes.get();\n+    _dataBuffer.copyTo(valueStartOffset, bytes, 0, length);\n     return StringUtil.decodeUtf8(bytes, 0, length);\n   }\n \n   @Override\n   public byte[] getBytes(int docId, ChunkReaderContext context) {\n+    if (_isCompressed) {\n+      return getBytesCompressed(docId, context);\n+    } else {\n+      return getBytesUncompressed(docId);\n+    }\n+  }\n+\n+  /**\n+   * Helper method to read BYTES value from the compressed index.\n+   */\n+  private byte[] getBytesCompressed(int docId, ChunkReaderContext context) {\n     int chunkRowId = docId % _numDocsPerChunk;\n     ByteBuffer chunkBuffer = getChunkBuffer(docId, context);\n \n-    int rowOffset =\n-        chunkBuffer.getInt(chunkRowId * VarByteChunkSVForwardIndexWriter.CHUNK_HEADER_ENTRY_ROW_OFFSET_SIZE);\n-    int nextRowOffset = getNextRowOffset(chunkRowId, chunkBuffer);\n+    // These offsets are offset in the chunk buffer\n+    int valueStartOffset = chunkBuffer.getInt(chunkRowId * ROW_OFFSET_SIZE);\n+    int valueEndOffset = getValueEndOffset(chunkRowId, chunkBuffer);\n \n-    int length = nextRowOffset - rowOffset;\n-    byte[] bytes = new byte[length];\n+    byte[] bytes = new byte[valueEndOffset - valueStartOffset];\n+    chunkBuffer.position(valueStartOffset);\n+    chunkBuffer.get(bytes);\n+    return bytes;\n+  }\n \n-    chunkBuffer.position(rowOffset);\n-    chunkBuffer.get(bytes, 0, length);\n+  /**\n+   * Helper method to read BYTES value from the uncompressed index.\n+   */\n+  private byte[] getBytesUncompressed(int docId) {\n+    int chunkId = docId / _numDocsPerChunk;\n+    int chunkRowId = docId % _numDocsPerChunk;\n+\n+    // These offsets are offset in the data buffer\n+    long chunkStartOffset = getChunkPosition(chunkId);\n+    long valueStartOffset = chunkStartOffset + _dataBuffer.getInt(chunkStartOffset + chunkRowId * ROW_OFFSET_SIZE);\n+    long valueEndOffset = getValueEndOffset(chunkId, chunkRowId, chunkStartOffset);\n+\n+    byte[] bytes = new byte[(int) (valueEndOffset - valueStartOffset)];\n+    _dataBuffer.copyTo(valueStartOffset, bytes);\n     return bytes;\n   }\n \n   /**\n-   * Helper method to compute the offset of next row in the chunk buffer.\n-   *\n-   * @param currentRowId Current row id within the chunk buffer.\n-   * @param chunkBuffer Chunk buffer containing the rows.\n-   *\n-   * @return Offset of next row within the chunk buffer. If current row is the last one,\n-   * chunkBuffer.limit() is returned.\n+   * Helper method to compute the end offset of the value in the chunk buffer.\n    */\n-  private int getNextRowOffset(int currentRowId, ByteBuffer chunkBuffer) {\n-    int nextRowOffset;\n+  private int getValueEndOffset(int rowId, ByteBuffer chunkBuffer) {\n+    if (rowId == _numDocsPerChunk - 1) {\n+      // Last row in the trunk\n+      return chunkBuffer.limit();\n+    } else {\n+      int valueEndOffset = chunkBuffer.getInt((rowId + 1) * ROW_OFFSET_SIZE);\n+      if (valueEndOffset == 0) {\n+        // Last row in the last chunk (chunk is incomplete, which stores 0 as the offset for the absent rows)\n+        return chunkBuffer.limit();\n+      } else {\n+        return valueEndOffset;\n+      }\n+    }\n+  }\n \n-    if (currentRowId == _numDocsPerChunk - 1) {\n-      // Last row in this trunk.\n-      nextRowOffset = chunkBuffer.limit();\n+  /**\n+   * Helper method to compute the end offset of the value in the data buffer.\n+   */\n+  private long getValueEndOffset(int chunkId, int chunkRowId, long chunkStartOffset) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6deb16d447bdef5356709e7583fa5e6f0f7d9d0"}, "originalPosition": 178}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA4NTEyMg==", "bodyText": "Algorithm is slightly different because with the chunkBuffer we can directly get the chunkEndOffset via chunkBuffer.limit(), which is not the case for the uncompressed one. That is why we have a branch on the last chunk.", "url": "https://github.com/apache/pinot/pull/5816#discussion_r466085122", "createdAt": "2020-08-06T01:00:37Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/index/readers/forward/VarByteChunkSVForwardIndexReader.java", "diffHunk": "@@ -19,92 +19,172 @@\n package org.apache.pinot.core.segment.index.readers.forward;\n \n import java.nio.ByteBuffer;\n+import javax.annotation.Nullable;\n import org.apache.pinot.common.utils.StringUtil;\n import org.apache.pinot.core.io.writer.impl.VarByteChunkSVForwardIndexWriter;\n import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n import org.apache.pinot.spi.data.FieldSpec.DataType;\n \n \n /**\n- * Chunk-based single-value raw (non-dictionary-encoded) forward index reader for values of  of variable length data\n- * type (STRING, BYTES).\n+ * Chunk-based single-value raw (non-dictionary-encoded) forward index reader for values of variable length data type\n+ * (STRING, BYTES).\n  * <p>For data layout, please refer to the documentation for {@link VarByteChunkSVForwardIndexWriter}\n  */\n public final class VarByteChunkSVForwardIndexReader extends BaseChunkSVForwardIndexReader {\n+  private static final int ROW_OFFSET_SIZE = VarByteChunkSVForwardIndexWriter.CHUNK_HEADER_ENTRY_ROW_OFFSET_SIZE;\n+\n   private final int _maxChunkSize;\n \n   // Thread local (reusable) byte[] to read bytes from data file.\n   private final ThreadLocal<byte[]> _reusableBytes = ThreadLocal.withInitial(() -> new byte[_lengthOfLongestEntry]);\n \n   public VarByteChunkSVForwardIndexReader(PinotDataBuffer dataBuffer, DataType valueType) {\n     super(dataBuffer, valueType);\n-    _maxChunkSize = _numDocsPerChunk * (VarByteChunkSVForwardIndexWriter.CHUNK_HEADER_ENTRY_ROW_OFFSET_SIZE\n-        + _lengthOfLongestEntry);\n+    _maxChunkSize = _numDocsPerChunk * (ROW_OFFSET_SIZE + _lengthOfLongestEntry);\n   }\n \n+  @Nullable\n   @Override\n   public ChunkReaderContext createContext() {\n-    return new ChunkReaderContext(_maxChunkSize);\n+    if (_isCompressed) {\n+      return new ChunkReaderContext(_maxChunkSize);\n+    } else {\n+      return null;\n+    }\n   }\n \n   @Override\n   public String getString(int docId, ChunkReaderContext context) {\n+    if (_isCompressed) {\n+      return getStringCompressed(docId, context);\n+    } else {\n+      return getStringUncompressed(docId);\n+    }\n+  }\n+\n+  /**\n+   * Helper method to read STRING value from the compressed index.\n+   */\n+  private String getStringCompressed(int docId, ChunkReaderContext context) {\n     int chunkRowId = docId % _numDocsPerChunk;\n     ByteBuffer chunkBuffer = getChunkBuffer(docId, context);\n \n-    int rowOffset =\n-        chunkBuffer.getInt(chunkRowId * VarByteChunkSVForwardIndexWriter.CHUNK_HEADER_ENTRY_ROW_OFFSET_SIZE);\n-    int nextRowOffset = getNextRowOffset(chunkRowId, chunkBuffer);\n+    // These offsets are offset in the chunk buffer\n+    int valueStartOffset = chunkBuffer.getInt(chunkRowId * ROW_OFFSET_SIZE);\n+    int valueEndOffset = getValueEndOffset(chunkRowId, chunkBuffer);\n \n-    int length = nextRowOffset - rowOffset;\n+    int length = valueEndOffset - valueStartOffset;\n     byte[] bytes = _reusableBytes.get();\n-\n-    chunkBuffer.position(rowOffset);\n+    chunkBuffer.position(valueStartOffset);\n     chunkBuffer.get(bytes, 0, length);\n+    return StringUtil.decodeUtf8(bytes, 0, length);\n+  }\n+\n+  /**\n+   * Helper method to read STRING value from the uncompressed index.\n+   */\n+  private String getStringUncompressed(int docId) {\n+    int chunkId = docId / _numDocsPerChunk;\n+    int chunkRowId = docId % _numDocsPerChunk;\n+\n+    // These offsets are offset in the data buffer\n+    long chunkStartOffset = getChunkPosition(chunkId);\n+    long valueStartOffset = chunkStartOffset + _dataBuffer.getInt(chunkStartOffset + chunkRowId * ROW_OFFSET_SIZE);\n+    long valueEndOffset = getValueEndOffset(chunkId, chunkRowId, chunkStartOffset);\n \n+    int length = (int) (valueEndOffset - valueStartOffset);\n+    byte[] bytes = _reusableBytes.get();\n+    _dataBuffer.copyTo(valueStartOffset, bytes, 0, length);\n     return StringUtil.decodeUtf8(bytes, 0, length);\n   }\n \n   @Override\n   public byte[] getBytes(int docId, ChunkReaderContext context) {\n+    if (_isCompressed) {\n+      return getBytesCompressed(docId, context);\n+    } else {\n+      return getBytesUncompressed(docId);\n+    }\n+  }\n+\n+  /**\n+   * Helper method to read BYTES value from the compressed index.\n+   */\n+  private byte[] getBytesCompressed(int docId, ChunkReaderContext context) {\n     int chunkRowId = docId % _numDocsPerChunk;\n     ByteBuffer chunkBuffer = getChunkBuffer(docId, context);\n \n-    int rowOffset =\n-        chunkBuffer.getInt(chunkRowId * VarByteChunkSVForwardIndexWriter.CHUNK_HEADER_ENTRY_ROW_OFFSET_SIZE);\n-    int nextRowOffset = getNextRowOffset(chunkRowId, chunkBuffer);\n+    // These offsets are offset in the chunk buffer\n+    int valueStartOffset = chunkBuffer.getInt(chunkRowId * ROW_OFFSET_SIZE);\n+    int valueEndOffset = getValueEndOffset(chunkRowId, chunkBuffer);\n \n-    int length = nextRowOffset - rowOffset;\n-    byte[] bytes = new byte[length];\n+    byte[] bytes = new byte[valueEndOffset - valueStartOffset];\n+    chunkBuffer.position(valueStartOffset);\n+    chunkBuffer.get(bytes);\n+    return bytes;\n+  }\n \n-    chunkBuffer.position(rowOffset);\n-    chunkBuffer.get(bytes, 0, length);\n+  /**\n+   * Helper method to read BYTES value from the uncompressed index.\n+   */\n+  private byte[] getBytesUncompressed(int docId) {\n+    int chunkId = docId / _numDocsPerChunk;\n+    int chunkRowId = docId % _numDocsPerChunk;\n+\n+    // These offsets are offset in the data buffer\n+    long chunkStartOffset = getChunkPosition(chunkId);\n+    long valueStartOffset = chunkStartOffset + _dataBuffer.getInt(chunkStartOffset + chunkRowId * ROW_OFFSET_SIZE);\n+    long valueEndOffset = getValueEndOffset(chunkId, chunkRowId, chunkStartOffset);\n+\n+    byte[] bytes = new byte[(int) (valueEndOffset - valueStartOffset)];\n+    _dataBuffer.copyTo(valueStartOffset, bytes);\n     return bytes;\n   }\n \n   /**\n-   * Helper method to compute the offset of next row in the chunk buffer.\n-   *\n-   * @param currentRowId Current row id within the chunk buffer.\n-   * @param chunkBuffer Chunk buffer containing the rows.\n-   *\n-   * @return Offset of next row within the chunk buffer. If current row is the last one,\n-   * chunkBuffer.limit() is returned.\n+   * Helper method to compute the end offset of the value in the chunk buffer.\n    */\n-  private int getNextRowOffset(int currentRowId, ByteBuffer chunkBuffer) {\n-    int nextRowOffset;\n+  private int getValueEndOffset(int rowId, ByteBuffer chunkBuffer) {\n+    if (rowId == _numDocsPerChunk - 1) {\n+      // Last row in the trunk\n+      return chunkBuffer.limit();\n+    } else {\n+      int valueEndOffset = chunkBuffer.getInt((rowId + 1) * ROW_OFFSET_SIZE);\n+      if (valueEndOffset == 0) {\n+        // Last row in the last chunk (chunk is incomplete, which stores 0 as the offset for the absent rows)\n+        return chunkBuffer.limit();\n+      } else {\n+        return valueEndOffset;\n+      }\n+    }\n+  }\n \n-    if (currentRowId == _numDocsPerChunk - 1) {\n-      // Last row in this trunk.\n-      nextRowOffset = chunkBuffer.limit();\n+  /**\n+   * Helper method to compute the end offset of the value in the data buffer.\n+   */\n+  private long getValueEndOffset(int chunkId, int chunkRowId, long chunkStartOffset) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA4MzUzNQ=="}, "originalCommit": {"oid": "d6deb16d447bdef5356709e7583fa5e6f0f7d9d0"}, "originalPosition": 178}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3947, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}