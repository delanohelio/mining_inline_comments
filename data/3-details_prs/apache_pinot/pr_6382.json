{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQ1NDc5MTYx", "number": 6382, "title": "Compatibility test for segment operations upload and delete", "bodyText": "Description\nImplemented compatibility test for Segment UPLOAD and DELETE operations.\n\nUPLOAD: Operation requires the below input and performs segment generation, compress to tar.gz file and then upload to controller. Also, does the validation check for segment uploaded to the controller and whether it is in ONLINE state.\n\n\n\ninputDataFileName\nschemaFileName\ntableConfigFileName\nrecordReaderConfigFileName\nsegmentName\n\n\n\nDELETE: Operation requires the below input and deletes segment. Also, does the validation check for the segment deletion.\n\n\n\ntableConfigFileName\nsegmentName\n\n\nNote:\n\nSegment UPLOAD and DELETE operation required TableOp to complete creating Schema and Table before calling SegmentOp.\n\nTest Case:\n\nUPLOAD - Tested Segment Generation, Compression to tar.gz file, Upload segment, validate upload segment successful, and cleanup of temp directories.\nDELETE - Tested Segment deletion and validation to check segment deleted is successful.\n\nIssue #4854\nUpgrade Notes\nDoes this PR prevent a zero down-time upgrade? (Assume upgrade order: Controller, Broker, Server, Minion)\n\n Yes (Please label as backward-incompat, and complete the section below on Release Notes)\n\nDoes this PR fix a zero-downtime upgrade introduced earlier?\n\n Yes (Please label this as backward-incompat, and complete the section below on Release Notes)\n\nDoes this PR otherwise need attention when creating release notes? Things to consider:\n\nNew configuration options\nDeprecation of configurations\nSignature changes to public methods/interfaces\nNew plugins added or old plugins removed\n\n\n Yes (Please label this PR as release-notes and complete the section on Release Notes)\n\nRelease Notes\nIf you have tagged this as either backward-incompat or release-notes,\nyou MUST add text here that you would like to see appear in release notes of the\nnext release.\nIf you have a series of commits adding or enabling a feature, then\nadd this section only in final commit that marks the feature completed.\nRefer to earlier release notes to see examples of text\nDocumentation\nIf you have introduced a new feature or configuration, please add it to the documentation as well.\nSee https://docs.pinot.apache.org/developers/developers-and-contributors/update-document", "createdAt": "2020-12-24T23:09:25Z", "url": "https://github.com/apache/pinot/pull/6382", "merged": true, "mergeCommit": {"oid": "8fcb17d2f4a6f9f374081d8d651f6d046c4da14d"}, "closed": true, "closedAt": "2021-01-05T01:52:16Z", "author": {"login": "amarnathkarthik"}, "timelineItems": {"totalCount": 15, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdpbjzAAH2gAyNTQ1NDc5MTYxOjVlY2E5Y2MzZmRmNDMyMzJlZWYwNzA1NzVkMjM0NDRiYzVkYmQzNDA=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdtAVL1AFqTU2MTQ1MjIwMQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "5eca9cc3fdf43232eef070575d23444bc5dbd340", "author": {"user": {"login": "amarnathkarthik", "name": "Karthik Amarnath"}}, "url": "https://github.com/apache/pinot/commit/5eca9cc3fdf43232eef070575d23444bc5dbd340", "committedDate": "2020-12-24T22:34:40Z", "message": "Compatibility test for segment operations upload and delete"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e16354ab71abd522b8a1bee49aa1ee00ce153143", "author": {"user": {"login": "amarnathkarthik", "name": "Karthik Amarnath"}}, "url": "https://github.com/apache/pinot/commit/e16354ab71abd522b8a1bee49aa1ee00ce153143", "committedDate": "2020-12-24T22:47:46Z", "message": "Compatibility test for segment operations upload and delete"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6aa246fa7eafa2319c5f9a32d3495a7a8b75f72f", "author": {"user": {"login": "amarnathkarthik", "name": "Karthik Amarnath"}}, "url": "https://github.com/apache/pinot/commit/6aa246fa7eafa2319c5f9a32d3495a7a8b75f72f", "committedDate": "2020-12-24T23:14:53Z", "message": "Merge branch 'compat-test' of https://github.com/amarnathkarthik/incubator-pinot into compat-test"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU4NzI0NDIx", "url": "https://github.com/apache/pinot/pull/6382#pullrequestreview-558724421", "createdAt": "2020-12-24T23:39:40Z", "commit": {"oid": "5eca9cc3fdf43232eef070575d23444bc5dbd340"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNFQyMzozOTo0MVrOILV4cQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNFQyMzo0OToyNVrOILV7iQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc2Mzc2MQ==", "bodyText": "It may be better to also add a deleteOnExit to this file handle. Just in case someone starts the test suite and kills it mid-way?", "url": "https://github.com/apache/pinot/pull/6382#discussion_r548763761", "createdAt": "2020-12-24T23:39:41Z", "author": {"login": "mcvsubbu"}, "path": "pinot-integration-tests/src/test/java/org/apache/pinot/compat/tests/SegmentOp.java", "diffHunk": "@@ -82,14 +107,175 @@ public void setTableConfigFileName(String tableConfigFileName) {\n     _tableConfigFileName = tableConfigFileName;\n   }\n \n+  public void setSchemaFileName(String schemaFileName) {\n+    _schemaFileName = schemaFileName;\n+  }\n+\n+  public String getSchemaFileName() {\n+    return _schemaFileName;\n+  }\n+\n+  public void setRecordReaderConfigFileName(String recordReaderConfigFileName) {\n+    _recordReaderConfigFileName = recordReaderConfigFileName;\n+  }\n+\n+  public String getRecordReaderConfigFileName() {\n+    return _recordReaderConfigFileName;\n+  }\n+\n+  public void setSegmentName(String segmentName) {\n+    _segmentName = segmentName;\n+  }\n+\n+  public String getSegmentName() {\n+    return _segmentName;\n+  }\n+\n   @Override\n   boolean runOp() {\n-    switch(_op) {\n+    switch (_op) {\n       case UPLOAD:\n-        System.out.println(\"Generating segment \" + _segmentName + \" from \" + _inputDataFileName + \" and uploading to \" +\n-            _tableConfigFileName);\n+        return createAndUploadSegments();\n       case DELETE:\n+        return deleteSegment();\n     }\n     return true;\n   }\n+\n+  /**\n+   * Create Segment file, compress to TarGz, and upload the files to controller.\n+   * @return true if all successful, false in case of failure.\n+   */\n+  private boolean createAndUploadSegments() {\n+    File localTempDir = new File(FileUtils.getTempDirectory(), \"pinot-compat-test-\" + UUID.randomUUID());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5eca9cc3fdf43232eef070575d23444bc5dbd340"}, "originalPosition": 129}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc2Mzg2Nw==", "bodyText": "Why do we have this? Wy not just use _segmentName?", "url": "https://github.com/apache/pinot/pull/6382#discussion_r548763867", "createdAt": "2020-12-24T23:41:31Z", "author": {"login": "mcvsubbu"}, "path": "pinot-integration-tests/src/test/java/org/apache/pinot/compat/tests/SegmentOp.java", "diffHunk": "@@ -82,14 +107,175 @@ public void setTableConfigFileName(String tableConfigFileName) {\n     _tableConfigFileName = tableConfigFileName;\n   }\n \n+  public void setSchemaFileName(String schemaFileName) {\n+    _schemaFileName = schemaFileName;\n+  }\n+\n+  public String getSchemaFileName() {\n+    return _schemaFileName;\n+  }\n+\n+  public void setRecordReaderConfigFileName(String recordReaderConfigFileName) {\n+    _recordReaderConfigFileName = recordReaderConfigFileName;\n+  }\n+\n+  public String getRecordReaderConfigFileName() {\n+    return _recordReaderConfigFileName;\n+  }\n+\n+  public void setSegmentName(String segmentName) {\n+    _segmentName = segmentName;\n+  }\n+\n+  public String getSegmentName() {\n+    return _segmentName;\n+  }\n+\n   @Override\n   boolean runOp() {\n-    switch(_op) {\n+    switch (_op) {\n       case UPLOAD:\n-        System.out.println(\"Generating segment \" + _segmentName + \" from \" + _inputDataFileName + \" and uploading to \" +\n-            _tableConfigFileName);\n+        return createAndUploadSegments();\n       case DELETE:\n+        return deleteSegment();\n     }\n     return true;\n   }\n+\n+  /**\n+   * Create Segment file, compress to TarGz, and upload the files to controller.\n+   * @return true if all successful, false in case of failure.\n+   */\n+  private boolean createAndUploadSegments() {\n+    File localTempDir = new File(FileUtils.getTempDirectory(), \"pinot-compat-test-\" + UUID.randomUUID());\n+    File localOutputTempDir = new File(localTempDir, \"output\");\n+    try {\n+      FileUtils.forceMkdir(localOutputTempDir);\n+      File segmentTarFile = generateSegment(localOutputTempDir);\n+      uploadSegment(segmentTarFile);\n+\n+      Pair<Long, Long> onlineSegmentCount = getOnlineSegmentCount(getTableExternalView());\n+      if (onlineSegmentCount.getFirst() <= 0 && onlineSegmentCount.getSecond() <= 0) {\n+        LOGGER.error(\"Uploaded segment {} not found or not in {} state.\", _segmentName, STATE_ONLINE);\n+        return false;\n+      }\n+      LOGGER.info(\"Successfully verified segment {} and its current status is {}.\", _segmentName, STATE_ONLINE);\n+\n+      return true;\n+    } catch (Exception e) {\n+      LOGGER.error(\"Failed to create and upload segment for input data file {}.\", _inputDataFileName, e);\n+      return false;\n+    } finally {\n+      FileUtils.deleteQuietly(localTempDir);\n+    }\n+  }\n+\n+  /**\n+   * Generate the Segment(s) and then compress to TarGz file. Supports generation of segment files for one input data\n+   * file.\n+   * @param outputDir to generate the Segment file(s).\n+   * @return File object of the TarGz compressed segment file.\n+   * @throws Exception while generating segment files and/or compressing to TarGz.\n+   */\n+  private File generateSegment(File outputDir)\n+      throws Exception {\n+    TableConfig tableConfig = JsonUtils.fileToObject(new File(_tableConfigFileName), TableConfig.class);\n+    _tableName = tableConfig.getTableName();\n+\n+    Schema schema = JsonUtils.fileToObject(new File(_schemaFileName), Schema.class);\n+    RecordReaderConfig recordReaderConfig =\n+        RecordReaderFactory.getRecordReaderConfig(DEFAULT_FILE_FORMAT, _recordReaderConfigFileName);\n+\n+    SegmentGeneratorConfig segmentGeneratorConfig = new SegmentGeneratorConfig(tableConfig, schema);\n+    segmentGeneratorConfig.setInputFilePath(_inputDataFileName);\n+    segmentGeneratorConfig.setFormat(DEFAULT_FILE_FORMAT);\n+    segmentGeneratorConfig.setOutDir(outputDir.getAbsolutePath());\n+    segmentGeneratorConfig.setReaderConfig(recordReaderConfig);\n+    segmentGeneratorConfig.setTableName(_tableName);\n+    segmentGeneratorConfig.setSegmentName(_segmentName);\n+\n+    SegmentIndexCreationDriver driver = new SegmentIndexCreationDriverImpl();\n+    driver.init(segmentGeneratorConfig);\n+    driver.build();\n+    String segmentName = driver.getSegmentName();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5eca9cc3fdf43232eef070575d23444bc5dbd340"}, "originalPosition": 179}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc2NDM0OQ==", "bodyText": "Since it takes some time for the segment to  make it to the external view, it is best to put this in a while loop. I suggest:\nwhile (segmentNotOnline()) { sleep(100ms) if (it took more than max time) { return false } }\nMax time can be hardcoded as 30s.", "url": "https://github.com/apache/pinot/pull/6382#discussion_r548764349", "createdAt": "2020-12-24T23:46:53Z", "author": {"login": "mcvsubbu"}, "path": "pinot-integration-tests/src/test/java/org/apache/pinot/compat/tests/SegmentOp.java", "diffHunk": "@@ -82,14 +107,175 @@ public void setTableConfigFileName(String tableConfigFileName) {\n     _tableConfigFileName = tableConfigFileName;\n   }\n \n+  public void setSchemaFileName(String schemaFileName) {\n+    _schemaFileName = schemaFileName;\n+  }\n+\n+  public String getSchemaFileName() {\n+    return _schemaFileName;\n+  }\n+\n+  public void setRecordReaderConfigFileName(String recordReaderConfigFileName) {\n+    _recordReaderConfigFileName = recordReaderConfigFileName;\n+  }\n+\n+  public String getRecordReaderConfigFileName() {\n+    return _recordReaderConfigFileName;\n+  }\n+\n+  public void setSegmentName(String segmentName) {\n+    _segmentName = segmentName;\n+  }\n+\n+  public String getSegmentName() {\n+    return _segmentName;\n+  }\n+\n   @Override\n   boolean runOp() {\n-    switch(_op) {\n+    switch (_op) {\n       case UPLOAD:\n-        System.out.println(\"Generating segment \" + _segmentName + \" from \" + _inputDataFileName + \" and uploading to \" +\n-            _tableConfigFileName);\n+        return createAndUploadSegments();\n       case DELETE:\n+        return deleteSegment();\n     }\n     return true;\n   }\n+\n+  /**\n+   * Create Segment file, compress to TarGz, and upload the files to controller.\n+   * @return true if all successful, false in case of failure.\n+   */\n+  private boolean createAndUploadSegments() {\n+    File localTempDir = new File(FileUtils.getTempDirectory(), \"pinot-compat-test-\" + UUID.randomUUID());\n+    File localOutputTempDir = new File(localTempDir, \"output\");\n+    try {\n+      FileUtils.forceMkdir(localOutputTempDir);\n+      File segmentTarFile = generateSegment(localOutputTempDir);\n+      uploadSegment(segmentTarFile);\n+\n+      Pair<Long, Long> onlineSegmentCount = getOnlineSegmentCount(getTableExternalView());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5eca9cc3fdf43232eef070575d23444bc5dbd340"}, "originalPosition": 136}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc2NDUyMQ==", "bodyText": "Also, in general we may have more than one replica of the segment, so it may be better to parse the external view for all replicas. True, we will be having only one replica to start with, but I would like to be able to extend the tests  along that dimension if needed.", "url": "https://github.com/apache/pinot/pull/6382#discussion_r548764521", "createdAt": "2020-12-24T23:48:58Z", "author": {"login": "mcvsubbu"}, "path": "pinot-integration-tests/src/test/java/org/apache/pinot/compat/tests/SegmentOp.java", "diffHunk": "@@ -82,14 +107,175 @@ public void setTableConfigFileName(String tableConfigFileName) {\n     _tableConfigFileName = tableConfigFileName;\n   }\n \n+  public void setSchemaFileName(String schemaFileName) {\n+    _schemaFileName = schemaFileName;\n+  }\n+\n+  public String getSchemaFileName() {\n+    return _schemaFileName;\n+  }\n+\n+  public void setRecordReaderConfigFileName(String recordReaderConfigFileName) {\n+    _recordReaderConfigFileName = recordReaderConfigFileName;\n+  }\n+\n+  public String getRecordReaderConfigFileName() {\n+    return _recordReaderConfigFileName;\n+  }\n+\n+  public void setSegmentName(String segmentName) {\n+    _segmentName = segmentName;\n+  }\n+\n+  public String getSegmentName() {\n+    return _segmentName;\n+  }\n+\n   @Override\n   boolean runOp() {\n-    switch(_op) {\n+    switch (_op) {\n       case UPLOAD:\n-        System.out.println(\"Generating segment \" + _segmentName + \" from \" + _inputDataFileName + \" and uploading to \" +\n-            _tableConfigFileName);\n+        return createAndUploadSegments();\n       case DELETE:\n+        return deleteSegment();\n     }\n     return true;\n   }\n+\n+  /**\n+   * Create Segment file, compress to TarGz, and upload the files to controller.\n+   * @return true if all successful, false in case of failure.\n+   */\n+  private boolean createAndUploadSegments() {\n+    File localTempDir = new File(FileUtils.getTempDirectory(), \"pinot-compat-test-\" + UUID.randomUUID());\n+    File localOutputTempDir = new File(localTempDir, \"output\");\n+    try {\n+      FileUtils.forceMkdir(localOutputTempDir);\n+      File segmentTarFile = generateSegment(localOutputTempDir);\n+      uploadSegment(segmentTarFile);\n+\n+      Pair<Long, Long> onlineSegmentCount = getOnlineSegmentCount(getTableExternalView());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc2NDM0OQ=="}, "originalCommit": {"oid": "5eca9cc3fdf43232eef070575d23444bc5dbd340"}, "originalPosition": 136}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc2NDU1Mw==", "bodyText": "Same comment about looping with sleep and handling multiple replicas.", "url": "https://github.com/apache/pinot/pull/6382#discussion_r548764553", "createdAt": "2020-12-24T23:49:25Z", "author": {"login": "mcvsubbu"}, "path": "pinot-integration-tests/src/test/java/org/apache/pinot/compat/tests/SegmentOp.java", "diffHunk": "@@ -82,14 +107,175 @@ public void setTableConfigFileName(String tableConfigFileName) {\n     _tableConfigFileName = tableConfigFileName;\n   }\n \n+  public void setSchemaFileName(String schemaFileName) {\n+    _schemaFileName = schemaFileName;\n+  }\n+\n+  public String getSchemaFileName() {\n+    return _schemaFileName;\n+  }\n+\n+  public void setRecordReaderConfigFileName(String recordReaderConfigFileName) {\n+    _recordReaderConfigFileName = recordReaderConfigFileName;\n+  }\n+\n+  public String getRecordReaderConfigFileName() {\n+    return _recordReaderConfigFileName;\n+  }\n+\n+  public void setSegmentName(String segmentName) {\n+    _segmentName = segmentName;\n+  }\n+\n+  public String getSegmentName() {\n+    return _segmentName;\n+  }\n+\n   @Override\n   boolean runOp() {\n-    switch(_op) {\n+    switch (_op) {\n       case UPLOAD:\n-        System.out.println(\"Generating segment \" + _segmentName + \" from \" + _inputDataFileName + \" and uploading to \" +\n-            _tableConfigFileName);\n+        return createAndUploadSegments();\n       case DELETE:\n+        return deleteSegment();\n     }\n     return true;\n   }\n+\n+  /**\n+   * Create Segment file, compress to TarGz, and upload the files to controller.\n+   * @return true if all successful, false in case of failure.\n+   */\n+  private boolean createAndUploadSegments() {\n+    File localTempDir = new File(FileUtils.getTempDirectory(), \"pinot-compat-test-\" + UUID.randomUUID());\n+    File localOutputTempDir = new File(localTempDir, \"output\");\n+    try {\n+      FileUtils.forceMkdir(localOutputTempDir);\n+      File segmentTarFile = generateSegment(localOutputTempDir);\n+      uploadSegment(segmentTarFile);\n+\n+      Pair<Long, Long> onlineSegmentCount = getOnlineSegmentCount(getTableExternalView());\n+      if (onlineSegmentCount.getFirst() <= 0 && onlineSegmentCount.getSecond() <= 0) {\n+        LOGGER.error(\"Uploaded segment {} not found or not in {} state.\", _segmentName, STATE_ONLINE);\n+        return false;\n+      }\n+      LOGGER.info(\"Successfully verified segment {} and its current status is {}.\", _segmentName, STATE_ONLINE);\n+\n+      return true;\n+    } catch (Exception e) {\n+      LOGGER.error(\"Failed to create and upload segment for input data file {}.\", _inputDataFileName, e);\n+      return false;\n+    } finally {\n+      FileUtils.deleteQuietly(localTempDir);\n+    }\n+  }\n+\n+  /**\n+   * Generate the Segment(s) and then compress to TarGz file. Supports generation of segment files for one input data\n+   * file.\n+   * @param outputDir to generate the Segment file(s).\n+   * @return File object of the TarGz compressed segment file.\n+   * @throws Exception while generating segment files and/or compressing to TarGz.\n+   */\n+  private File generateSegment(File outputDir)\n+      throws Exception {\n+    TableConfig tableConfig = JsonUtils.fileToObject(new File(_tableConfigFileName), TableConfig.class);\n+    _tableName = tableConfig.getTableName();\n+\n+    Schema schema = JsonUtils.fileToObject(new File(_schemaFileName), Schema.class);\n+    RecordReaderConfig recordReaderConfig =\n+        RecordReaderFactory.getRecordReaderConfig(DEFAULT_FILE_FORMAT, _recordReaderConfigFileName);\n+\n+    SegmentGeneratorConfig segmentGeneratorConfig = new SegmentGeneratorConfig(tableConfig, schema);\n+    segmentGeneratorConfig.setInputFilePath(_inputDataFileName);\n+    segmentGeneratorConfig.setFormat(DEFAULT_FILE_FORMAT);\n+    segmentGeneratorConfig.setOutDir(outputDir.getAbsolutePath());\n+    segmentGeneratorConfig.setReaderConfig(recordReaderConfig);\n+    segmentGeneratorConfig.setTableName(_tableName);\n+    segmentGeneratorConfig.setSegmentName(_segmentName);\n+\n+    SegmentIndexCreationDriver driver = new SegmentIndexCreationDriverImpl();\n+    driver.init(segmentGeneratorConfig);\n+    driver.build();\n+    String segmentName = driver.getSegmentName();\n+    File indexDir = new File(outputDir, segmentName);\n+    LOGGER.info(\"Successfully created segment: {} at directory: {}\", segmentName, indexDir);\n+    File segmentTarFile = new File(outputDir, segmentName + TarGzCompressionUtils.TAR_GZ_FILE_EXTENSION);\n+    TarGzCompressionUtils.createTarGzFile(indexDir, segmentTarFile);\n+    LOGGER.info(\"Tarring segment from: {} to: {}\", indexDir, segmentTarFile);\n+\n+    return segmentTarFile;\n+  }\n+\n+  /**\n+   * Upload the TarGz Segment file to the controller.\n+   * @param segmentTarFile TarGz Segment file\n+   * @throws Exception when upload segment fails.\n+   */\n+  private void uploadSegment(File segmentTarFile)\n+      throws Exception {\n+    URI controllerURI = FileUploadDownloadClient.getUploadSegmentURI(new URI(ClusterDescriptor.CONTROLLER_URL));\n+    try (FileUploadDownloadClient fileUploadDownloadClient = new FileUploadDownloadClient()) {\n+      fileUploadDownloadClient.uploadSegment(controllerURI, segmentTarFile.getName(), segmentTarFile, _tableName);\n+    }\n+  }\n+\n+  /**\n+   * Deletes the segment for the given segment name and table name.\n+   * @return true if delete successful, else false.\n+   */\n+  private boolean deleteSegment() {\n+    try {\n+      TableConfig tableConfig = JsonUtils.fileToObject(new File(_tableConfigFileName), TableConfig.class);\n+      _tableName = tableConfig.getTableName();\n+\n+      ControllerTest.sendDeleteRequest(ControllerRequestURLBuilder.baseUrl(ClusterDescriptor.CONTROLLER_URL)\n+          .forSegmentDelete(_tableName, _segmentName));\n+\n+      Pair<Long, Long> onlineSegmentCount = getOnlineSegmentCount(getTableExternalView());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5eca9cc3fdf43232eef070575d23444bc5dbd340"}, "originalPosition": 214}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ab6751039723203cff10196c75d43bcc0f16f9ed", "author": {"user": {"login": "amarnathkarthik", "name": "Karthik Amarnath"}}, "url": "https://github.com/apache/pinot/commit/ab6751039723203cff10196c75d43bcc0f16f9ed", "committedDate": "2020-12-25T02:00:57Z", "message": "Compatibility test for segment operations upload and delete"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d61e79d2deeac63d658fc393fb0ce4fe22439301", "author": {"user": {"login": "amarnathkarthik", "name": "Karthik Amarnath"}}, "url": "https://github.com/apache/pinot/commit/d61e79d2deeac63d658fc393fb0ce4fe22439301", "committedDate": "2020-12-25T04:24:28Z", "message": "Trigger build"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU4ODk1ODEx", "url": "https://github.com/apache/pinot/pull/6382#pullrequestreview-558895811", "createdAt": "2020-12-26T23:31:06Z", "commit": {"oid": "5eca9cc3fdf43232eef070575d23444bc5dbd340"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNlQyMzozMTowNlrOILm41Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNlQyMzozNzoxN1rOILm6uA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA0MjM4OQ==", "bodyText": "Thanks", "url": "https://github.com/apache/pinot/pull/6382#discussion_r549042389", "createdAt": "2020-12-26T23:31:06Z", "author": {"login": "mcvsubbu"}, "path": "pinot-integration-tests/src/test/java/org/apache/pinot/compat/tests/SegmentOp.java", "diffHunk": "@@ -82,14 +107,175 @@ public void setTableConfigFileName(String tableConfigFileName) {\n     _tableConfigFileName = tableConfigFileName;\n   }\n \n+  public void setSchemaFileName(String schemaFileName) {\n+    _schemaFileName = schemaFileName;\n+  }\n+\n+  public String getSchemaFileName() {\n+    return _schemaFileName;\n+  }\n+\n+  public void setRecordReaderConfigFileName(String recordReaderConfigFileName) {\n+    _recordReaderConfigFileName = recordReaderConfigFileName;\n+  }\n+\n+  public String getRecordReaderConfigFileName() {\n+    return _recordReaderConfigFileName;\n+  }\n+\n+  public void setSegmentName(String segmentName) {\n+    _segmentName = segmentName;\n+  }\n+\n+  public String getSegmentName() {\n+    return _segmentName;\n+  }\n+\n   @Override\n   boolean runOp() {\n-    switch(_op) {\n+    switch (_op) {\n       case UPLOAD:\n-        System.out.println(\"Generating segment \" + _segmentName + \" from \" + _inputDataFileName + \" and uploading to \" +\n-            _tableConfigFileName);\n+        return createAndUploadSegments();\n       case DELETE:\n+        return deleteSegment();\n     }\n     return true;\n   }\n+\n+  /**\n+   * Create Segment file, compress to TarGz, and upload the files to controller.\n+   * @return true if all successful, false in case of failure.\n+   */\n+  private boolean createAndUploadSegments() {\n+    File localTempDir = new File(FileUtils.getTempDirectory(), \"pinot-compat-test-\" + UUID.randomUUID());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc2Mzc2MQ=="}, "originalCommit": {"oid": "5eca9cc3fdf43232eef070575d23444bc5dbd340"}, "originalPosition": 129}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA0MjYwMg==", "bodyText": "We cannot assume that this is the last segment to be deleted.\nBetter to call a method called verifySegmentdeleted(),  and have that method fetch the externalView and make sure the segment is not there in it.", "url": "https://github.com/apache/pinot/pull/6382#discussion_r549042602", "createdAt": "2020-12-26T23:34:15Z", "author": {"login": "mcvsubbu"}, "path": "pinot-integration-tests/src/test/java/org/apache/pinot/compat/tests/SegmentOp.java", "diffHunk": "@@ -82,14 +103,172 @@ public void setTableConfigFileName(String tableConfigFileName) {\n     _tableConfigFileName = tableConfigFileName;\n   }\n \n+  public void setSchemaFileName(String schemaFileName) {\n+    _schemaFileName = schemaFileName;\n+  }\n+\n+  public String getSchemaFileName() {\n+    return _schemaFileName;\n+  }\n+\n+  public void setRecordReaderConfigFileName(String recordReaderConfigFileName) {\n+    _recordReaderConfigFileName = recordReaderConfigFileName;\n+  }\n+\n+  public String getRecordReaderConfigFileName() {\n+    return _recordReaderConfigFileName;\n+  }\n+\n+  public void setSegmentName(String segmentName) {\n+    _segmentName = segmentName;\n+  }\n+\n+  public String getSegmentName() {\n+    return _segmentName;\n+  }\n+\n   @Override\n   boolean runOp() {\n-    switch(_op) {\n+    switch (_op) {\n       case UPLOAD:\n-        System.out.println(\"Generating segment \" + _segmentName + \" from \" + _inputDataFileName + \" and uploading to \" +\n-            _tableConfigFileName);\n+        return createAndUploadSegments();\n       case DELETE:\n+        return deleteSegment();\n     }\n     return true;\n   }\n+\n+  /**\n+   * Create Segment file, compress to TarGz, and upload the files to controller.\n+   * @return true if all successful, false in case of failure.\n+   */\n+  private boolean createAndUploadSegments() {\n+    File localTempDir = new File(FileUtils.getTempDirectory(), \"pinot-compat-test-\" + UUID.randomUUID());\n+    localTempDir.deleteOnExit();\n+    File localOutputTempDir = new File(localTempDir, \"output\");\n+    try {\n+      FileUtils.forceMkdir(localOutputTempDir);\n+      File segmentTarFile = generateSegment(localOutputTempDir);\n+      uploadSegment(segmentTarFile);\n+\n+      long startTime = System.currentTimeMillis();\n+      while (getOnlineSegmentCount() <= 0) {\n+        if ((System.currentTimeMillis() - startTime) > DEFAULT_MAX_SLEEP_TIME_MS) {\n+          LOGGER.error(\"Upload segment verification failed, count is zero after max wait time {} ms.\",\n+              DEFAULT_MAX_SLEEP_TIME_MS);\n+          return false;\n+        }\n+        LOGGER.warn(\"Upload segment verification count is zero, will retry after {} ms.\", DEFAULT_WAIT_TIME_MS);\n+        Thread.sleep(DEFAULT_WAIT_TIME_MS);\n+      }\n+      LOGGER.info(\"Successfully verified segment {} and its current status is {}.\", _segmentName, STATE_ONLINE);\n+\n+      return true;\n+    } catch (Exception e) {\n+      LOGGER.error(\"Failed to create and upload segment for input data file {}.\", _inputDataFileName, e);\n+      return false;\n+    } finally {\n+      FileUtils.deleteQuietly(localTempDir);\n+    }\n+  }\n+\n+  /**\n+   * Generate the Segment(s) and then compress to TarGz file. Supports generation of segment files for one input data\n+   * file.\n+   * @param outputDir to generate the Segment file(s).\n+   * @return File object of the TarGz compressed segment file.\n+   * @throws Exception while generating segment files and/or compressing to TarGz.\n+   */\n+  private File generateSegment(File outputDir)\n+      throws Exception {\n+    TableConfig tableConfig = JsonUtils.fileToObject(new File(_tableConfigFileName), TableConfig.class);\n+    _tableName = tableConfig.getTableName();\n+\n+    Schema schema = JsonUtils.fileToObject(new File(_schemaFileName), Schema.class);\n+    RecordReaderConfig recordReaderConfig =\n+        RecordReaderFactory.getRecordReaderConfig(DEFAULT_FILE_FORMAT, _recordReaderConfigFileName);\n+\n+    SegmentGeneratorConfig segmentGeneratorConfig = new SegmentGeneratorConfig(tableConfig, schema);\n+    segmentGeneratorConfig.setInputFilePath(_inputDataFileName);\n+    segmentGeneratorConfig.setFormat(DEFAULT_FILE_FORMAT);\n+    segmentGeneratorConfig.setOutDir(outputDir.getAbsolutePath());\n+    segmentGeneratorConfig.setReaderConfig(recordReaderConfig);\n+    segmentGeneratorConfig.setTableName(_tableName);\n+    segmentGeneratorConfig.setSegmentName(_segmentName);\n+\n+    SegmentIndexCreationDriver driver = new SegmentIndexCreationDriverImpl();\n+    driver.init(segmentGeneratorConfig);\n+    driver.build();\n+    File indexDir = new File(outputDir, _segmentName);\n+    LOGGER.info(\"Successfully created segment: {} at directory: {}\", _segmentName, indexDir);\n+    File segmentTarFile = new File(outputDir, _segmentName + TarGzCompressionUtils.TAR_GZ_FILE_EXTENSION);\n+    TarGzCompressionUtils.createTarGzFile(indexDir, segmentTarFile);\n+    LOGGER.info(\"Tarring segment from: {} to: {}\", indexDir, segmentTarFile);\n+\n+    return segmentTarFile;\n+  }\n+\n+  /**\n+   * Upload the TarGz Segment file to the controller.\n+   * @param segmentTarFile TarGz Segment file\n+   * @throws Exception when upload segment fails.\n+   */\n+  private void uploadSegment(File segmentTarFile)\n+      throws Exception {\n+    URI controllerURI = FileUploadDownloadClient.getUploadSegmentURI(new URI(ClusterDescriptor.CONTROLLER_URL));\n+    try (FileUploadDownloadClient fileUploadDownloadClient = new FileUploadDownloadClient()) {\n+      fileUploadDownloadClient.uploadSegment(controllerURI, segmentTarFile.getName(), segmentTarFile, _tableName);\n+    }\n+  }\n+\n+  /**\n+   * Deletes the segment for the given segment name and table name.\n+   * @return true if delete successful, else false.\n+   */\n+  private boolean deleteSegment() {\n+    try {\n+      TableConfig tableConfig = JsonUtils.fileToObject(new File(_tableConfigFileName), TableConfig.class);\n+      _tableName = tableConfig.getTableName();\n+\n+      ControllerTest.sendDeleteRequest(ControllerRequestURLBuilder.baseUrl(ClusterDescriptor.CONTROLLER_URL)\n+          .forSegmentDelete(_tableName, _segmentName));\n+\n+      long startTime = System.currentTimeMillis();\n+      while (getOnlineSegmentCount() > 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d61e79d2deeac63d658fc393fb0ce4fe22439301"}, "originalPosition": 216}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA0MjgzOQ==", "bodyText": "we can't assume that this is the first segment that we upload. The tests will upload segments to the same table across each upgrade/downgrade. Better to call a method called verifySegmentInState(\"ONLINE\") and have that method fetch the externalview and make sure that the segment is in the required state. We can then use the same method for realtime table and consuming segments also, if needed. (As in, `verifySegmentInState(\"CONSUMING\")). Thanks.", "url": "https://github.com/apache/pinot/pull/6382#discussion_r549042839", "createdAt": "2020-12-26T23:36:46Z", "author": {"login": "mcvsubbu"}, "path": "pinot-integration-tests/src/test/java/org/apache/pinot/compat/tests/SegmentOp.java", "diffHunk": "@@ -82,14 +103,172 @@ public void setTableConfigFileName(String tableConfigFileName) {\n     _tableConfigFileName = tableConfigFileName;\n   }\n \n+  public void setSchemaFileName(String schemaFileName) {\n+    _schemaFileName = schemaFileName;\n+  }\n+\n+  public String getSchemaFileName() {\n+    return _schemaFileName;\n+  }\n+\n+  public void setRecordReaderConfigFileName(String recordReaderConfigFileName) {\n+    _recordReaderConfigFileName = recordReaderConfigFileName;\n+  }\n+\n+  public String getRecordReaderConfigFileName() {\n+    return _recordReaderConfigFileName;\n+  }\n+\n+  public void setSegmentName(String segmentName) {\n+    _segmentName = segmentName;\n+  }\n+\n+  public String getSegmentName() {\n+    return _segmentName;\n+  }\n+\n   @Override\n   boolean runOp() {\n-    switch(_op) {\n+    switch (_op) {\n       case UPLOAD:\n-        System.out.println(\"Generating segment \" + _segmentName + \" from \" + _inputDataFileName + \" and uploading to \" +\n-            _tableConfigFileName);\n+        return createAndUploadSegments();\n       case DELETE:\n+        return deleteSegment();\n     }\n     return true;\n   }\n+\n+  /**\n+   * Create Segment file, compress to TarGz, and upload the files to controller.\n+   * @return true if all successful, false in case of failure.\n+   */\n+  private boolean createAndUploadSegments() {\n+    File localTempDir = new File(FileUtils.getTempDirectory(), \"pinot-compat-test-\" + UUID.randomUUID());\n+    localTempDir.deleteOnExit();\n+    File localOutputTempDir = new File(localTempDir, \"output\");\n+    try {\n+      FileUtils.forceMkdir(localOutputTempDir);\n+      File segmentTarFile = generateSegment(localOutputTempDir);\n+      uploadSegment(segmentTarFile);\n+\n+      long startTime = System.currentTimeMillis();\n+      while (getOnlineSegmentCount() <= 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d61e79d2deeac63d658fc393fb0ce4fe22439301"}, "originalPosition": 134}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA0Mjg3Mg==", "bodyText": "Thanks for the fix", "url": "https://github.com/apache/pinot/pull/6382#discussion_r549042872", "createdAt": "2020-12-26T23:37:17Z", "author": {"login": "mcvsubbu"}, "path": "pinot-integration-tests/src/test/java/org/apache/pinot/compat/tests/TableOp.java", "diffHunk": "@@ -104,9 +103,12 @@ boolean runOp() {\n \n   private boolean createSchema() {\n     try {\n-      ControllerTest.sendPostRequest(\n-          ControllerRequestURLBuilder.baseUrl(ClusterDescriptor.CONTROLLER_URL).forSchemaCreate(),\n-          FileUtils.readFileToString(new File(_schemaFileName)));\n+      Map<String, String> headers = new HashMap<String, String>() {{\n+        put(\"Content-type\", \"application/json\");\n+      }};\n+      ControllerTest\n+          .sendPostRequest(ControllerRequestURLBuilder.baseUrl(ClusterDescriptor.CONTROLLER_URL).forSchemaCreate(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d61e79d2deeac63d658fc393fb0ce4fe22439301"}, "originalPosition": 32}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU4OTU0NzUz", "url": "https://github.com/apache/pinot/pull/6382#pullrequestreview-558954753", "createdAt": "2020-12-27T19:39:34Z", "commit": {"oid": "d61e79d2deeac63d658fc393fb0ce4fe22439301"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yN1QxOTozOTozNVrOILt03Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yN1QxOTo0NTo1OFrOILt27w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTE1NjA2MQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              private static final int DEFAULT_WAIT_TIME_MS = 5000;\n          \n          \n            \n              private static final int DEFAULT_SLEEP_INTERVAL_MS = 200;", "url": "https://github.com/apache/pinot/pull/6382#discussion_r549156061", "createdAt": "2020-12-27T19:39:35Z", "author": {"login": "mcvsubbu"}, "path": "pinot-integration-tests/src/test/java/org/apache/pinot/compat/tests/SegmentOp.java", "diffHunk": "@@ -36,15 +57,23 @@\n  */\n @JsonIgnoreProperties(ignoreUnknown = true)\n public class SegmentOp extends BaseOp {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(SegmentOp.class);\n+  private static final FileFormat DEFAULT_FILE_FORMAT = FileFormat.CSV;\n+  private static final String STATE_ONLINE = \"ONLINE\";\n+  private static final int DEFAULT_MAX_SLEEP_TIME_MS = 30000;\n+  private static final int DEFAULT_WAIT_TIME_MS = 5000;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d61e79d2deeac63d658fc393fb0ce4fe22439301"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTE1NjU5MQ==", "bodyText": "Valid question.\nSo, assuming that we create a table and add a segment each between phases of upgrades (there are six) and delete some of them in some phases. So, we will start with 0 segments, and maybe end with 2 or 3, while going up to 6.\nIn this case, a count of online segments can be anythning.\nSo,  all we want to make sure is that the given segment (segment name) is in the state we want it to be. If deleted, then we want to make sure that it disappeared from externalview.\nA segment is deleted when it goes away from externalview. If it is present, it better not be in ERROR state (unless we intend that). For now, let us just implement plain old ADD and DELETE operations and check for ONLINE state in the case  of adding a segment, and for not being there in the case of a delete.\nSo, the best way seems to be to get the externalview, parse it into a json object, and look for specific fields.\nAnd while we are there, might as well account for all replicas being in the same state rather than just one replica. If we add test cases involving replicas this will be one less thing to take care of.", "url": "https://github.com/apache/pinot/pull/6382#discussion_r549156591", "createdAt": "2020-12-27T19:45:58Z", "author": {"login": "mcvsubbu"}, "path": "pinot-integration-tests/src/test/java/org/apache/pinot/compat/tests/SegmentOp.java", "diffHunk": "@@ -82,14 +103,172 @@ public void setTableConfigFileName(String tableConfigFileName) {\n     _tableConfigFileName = tableConfigFileName;\n   }\n \n+  public void setSchemaFileName(String schemaFileName) {\n+    _schemaFileName = schemaFileName;\n+  }\n+\n+  public String getSchemaFileName() {\n+    return _schemaFileName;\n+  }\n+\n+  public void setRecordReaderConfigFileName(String recordReaderConfigFileName) {\n+    _recordReaderConfigFileName = recordReaderConfigFileName;\n+  }\n+\n+  public String getRecordReaderConfigFileName() {\n+    return _recordReaderConfigFileName;\n+  }\n+\n+  public void setSegmentName(String segmentName) {\n+    _segmentName = segmentName;\n+  }\n+\n+  public String getSegmentName() {\n+    return _segmentName;\n+  }\n+\n   @Override\n   boolean runOp() {\n-    switch(_op) {\n+    switch (_op) {\n       case UPLOAD:\n-        System.out.println(\"Generating segment \" + _segmentName + \" from \" + _inputDataFileName + \" and uploading to \" +\n-            _tableConfigFileName);\n+        return createAndUploadSegments();\n       case DELETE:\n+        return deleteSegment();\n     }\n     return true;\n   }\n+\n+  /**\n+   * Create Segment file, compress to TarGz, and upload the files to controller.\n+   * @return true if all successful, false in case of failure.\n+   */\n+  private boolean createAndUploadSegments() {\n+    File localTempDir = new File(FileUtils.getTempDirectory(), \"pinot-compat-test-\" + UUID.randomUUID());\n+    localTempDir.deleteOnExit();\n+    File localOutputTempDir = new File(localTempDir, \"output\");\n+    try {\n+      FileUtils.forceMkdir(localOutputTempDir);\n+      File segmentTarFile = generateSegment(localOutputTempDir);\n+      uploadSegment(segmentTarFile);\n+\n+      long startTime = System.currentTimeMillis();\n+      while (getOnlineSegmentCount() <= 0) {\n+        if ((System.currentTimeMillis() - startTime) > DEFAULT_MAX_SLEEP_TIME_MS) {\n+          LOGGER.error(\"Upload segment verification failed, count is zero after max wait time {} ms.\",\n+              DEFAULT_MAX_SLEEP_TIME_MS);\n+          return false;\n+        }\n+        LOGGER.warn(\"Upload segment verification count is zero, will retry after {} ms.\", DEFAULT_WAIT_TIME_MS);\n+        Thread.sleep(DEFAULT_WAIT_TIME_MS);\n+      }\n+      LOGGER.info(\"Successfully verified segment {} and its current status is {}.\", _segmentName, STATE_ONLINE);\n+\n+      return true;\n+    } catch (Exception e) {\n+      LOGGER.error(\"Failed to create and upload segment for input data file {}.\", _inputDataFileName, e);\n+      return false;\n+    } finally {\n+      FileUtils.deleteQuietly(localTempDir);\n+    }\n+  }\n+\n+  /**\n+   * Generate the Segment(s) and then compress to TarGz file. Supports generation of segment files for one input data\n+   * file.\n+   * @param outputDir to generate the Segment file(s).\n+   * @return File object of the TarGz compressed segment file.\n+   * @throws Exception while generating segment files and/or compressing to TarGz.\n+   */\n+  private File generateSegment(File outputDir)\n+      throws Exception {\n+    TableConfig tableConfig = JsonUtils.fileToObject(new File(_tableConfigFileName), TableConfig.class);\n+    _tableName = tableConfig.getTableName();\n+\n+    Schema schema = JsonUtils.fileToObject(new File(_schemaFileName), Schema.class);\n+    RecordReaderConfig recordReaderConfig =\n+        RecordReaderFactory.getRecordReaderConfig(DEFAULT_FILE_FORMAT, _recordReaderConfigFileName);\n+\n+    SegmentGeneratorConfig segmentGeneratorConfig = new SegmentGeneratorConfig(tableConfig, schema);\n+    segmentGeneratorConfig.setInputFilePath(_inputDataFileName);\n+    segmentGeneratorConfig.setFormat(DEFAULT_FILE_FORMAT);\n+    segmentGeneratorConfig.setOutDir(outputDir.getAbsolutePath());\n+    segmentGeneratorConfig.setReaderConfig(recordReaderConfig);\n+    segmentGeneratorConfig.setTableName(_tableName);\n+    segmentGeneratorConfig.setSegmentName(_segmentName);\n+\n+    SegmentIndexCreationDriver driver = new SegmentIndexCreationDriverImpl();\n+    driver.init(segmentGeneratorConfig);\n+    driver.build();\n+    File indexDir = new File(outputDir, _segmentName);\n+    LOGGER.info(\"Successfully created segment: {} at directory: {}\", _segmentName, indexDir);\n+    File segmentTarFile = new File(outputDir, _segmentName + TarGzCompressionUtils.TAR_GZ_FILE_EXTENSION);\n+    TarGzCompressionUtils.createTarGzFile(indexDir, segmentTarFile);\n+    LOGGER.info(\"Tarring segment from: {} to: {}\", indexDir, segmentTarFile);\n+\n+    return segmentTarFile;\n+  }\n+\n+  /**\n+   * Upload the TarGz Segment file to the controller.\n+   * @param segmentTarFile TarGz Segment file\n+   * @throws Exception when upload segment fails.\n+   */\n+  private void uploadSegment(File segmentTarFile)\n+      throws Exception {\n+    URI controllerURI = FileUploadDownloadClient.getUploadSegmentURI(new URI(ClusterDescriptor.CONTROLLER_URL));\n+    try (FileUploadDownloadClient fileUploadDownloadClient = new FileUploadDownloadClient()) {\n+      fileUploadDownloadClient.uploadSegment(controllerURI, segmentTarFile.getName(), segmentTarFile, _tableName);\n+    }\n+  }\n+\n+  /**\n+   * Deletes the segment for the given segment name and table name.\n+   * @return true if delete successful, else false.\n+   */\n+  private boolean deleteSegment() {\n+    try {\n+      TableConfig tableConfig = JsonUtils.fileToObject(new File(_tableConfigFileName), TableConfig.class);\n+      _tableName = tableConfig.getTableName();\n+\n+      ControllerTest.sendDeleteRequest(ControllerRequestURLBuilder.baseUrl(ClusterDescriptor.CONTROLLER_URL)\n+          .forSegmentDelete(_tableName, _segmentName));\n+\n+      long startTime = System.currentTimeMillis();\n+      while (getOnlineSegmentCount() > 0) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA0MjYwMg=="}, "originalCommit": {"oid": "d61e79d2deeac63d658fc393fb0ce4fe22439301"}, "originalPosition": 216}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bb5c23fbcfa0f67afc495b829c4bb59503fe0038", "author": {"user": {"login": "amarnathkarthik", "name": "Karthik Amarnath"}}, "url": "https://github.com/apache/pinot/commit/bb5c23fbcfa0f67afc495b829c4bb59503fe0038", "committedDate": "2020-12-27T20:59:13Z", "message": "Compatibility test for segment operations upload and delete"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYxMjQ4MDIy", "url": "https://github.com/apache/pinot/pull/6382#pullrequestreview-561248022", "createdAt": "2021-01-04T18:34:14Z", "commit": {"oid": "bb5c23fbcfa0f67afc495b829c4bb59503fe0038"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQxODozNDoxNFrOIN8aOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQxODo0MzowNlrOIN8sDA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQ5MjE1Mw==", "bodyText": "Please pick this up from CommonConstants", "url": "https://github.com/apache/pinot/pull/6382#discussion_r551492153", "createdAt": "2021-01-04T18:34:14Z", "author": {"login": "mcvsubbu"}, "path": "pinot-integration-tests/src/test/java/org/apache/pinot/compat/tests/SegmentOp.java", "diffHunk": "@@ -36,15 +57,23 @@\n  */\n @JsonIgnoreProperties(ignoreUnknown = true)\n public class SegmentOp extends BaseOp {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(SegmentOp.class);\n+  private static final FileFormat DEFAULT_FILE_FORMAT = FileFormat.CSV;\n+  private static final String STATE_ONLINE = \"ONLINE\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bb5c23fbcfa0f67afc495b829c4bb59503fe0038"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQ5MjQ2MA==", "bodyText": "nit:  I prefer one per line for readability", "url": "https://github.com/apache/pinot/pull/6382#discussion_r551492460", "createdAt": "2021-01-04T18:34:53Z", "author": {"login": "mcvsubbu"}, "path": "pinot-integration-tests/src/test/java/org/apache/pinot/compat/tests/SegmentOp.java", "diffHunk": "@@ -36,15 +57,23 @@\n  */\n @JsonIgnoreProperties(ignoreUnknown = true)\n public class SegmentOp extends BaseOp {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(SegmentOp.class);\n+  private static final FileFormat DEFAULT_FILE_FORMAT = FileFormat.CSV;\n+  private static final String STATE_ONLINE = \"ONLINE\";\n+  private static final int DEFAULT_MAX_SLEEP_TIME_MS = 30000;\n+  private static final int DEFAULT_SLEEP_INTERVAL_MS = 200;\n+\n   public enum Op {\n-    UPLOAD,\n-    DELETE\n+    UPLOAD, DELETE", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bb5c23fbcfa0f67afc495b829c4bb59503fe0038"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQ5NDk1Nw==", "bodyText": "The code here will work, but seems to be implying that a segment can exist in both realtime and offline tables with the same name.\nYou can choose to just consider the offline side for now. Let us add the realtime side later.", "url": "https://github.com/apache/pinot/pull/6382#discussion_r551494957", "createdAt": "2021-01-04T18:39:38Z", "author": {"login": "mcvsubbu"}, "path": "pinot-integration-tests/src/test/java/org/apache/pinot/compat/tests/SegmentOp.java", "diffHunk": "@@ -82,14 +103,220 @@ public void setTableConfigFileName(String tableConfigFileName) {\n     _tableConfigFileName = tableConfigFileName;\n   }\n \n+  public void setSchemaFileName(String schemaFileName) {\n+    _schemaFileName = schemaFileName;\n+  }\n+\n+  public String getSchemaFileName() {\n+    return _schemaFileName;\n+  }\n+\n+  public void setRecordReaderConfigFileName(String recordReaderConfigFileName) {\n+    _recordReaderConfigFileName = recordReaderConfigFileName;\n+  }\n+\n+  public String getRecordReaderConfigFileName() {\n+    return _recordReaderConfigFileName;\n+  }\n+\n+  public void setSegmentName(String segmentName) {\n+    _segmentName = segmentName;\n+  }\n+\n+  public String getSegmentName() {\n+    return _segmentName;\n+  }\n+\n   @Override\n   boolean runOp() {\n-    switch(_op) {\n+    switch (_op) {\n       case UPLOAD:\n-        System.out.println(\"Generating segment \" + _segmentName + \" from \" + _inputDataFileName + \" and uploading to \" +\n-            _tableConfigFileName);\n+        return createAndUploadSegments();\n       case DELETE:\n+        return deleteSegment();\n     }\n     return true;\n   }\n+\n+  /**\n+   * Create Segment file, compress to TarGz, upload the files to controller and verify segment upload.\n+   * @return true if all successful, false in case of failure.\n+   */\n+  private boolean createAndUploadSegments() {\n+    File localTempDir = new File(FileUtils.getTempDirectory(), \"pinot-compat-test-\" + UUID.randomUUID());\n+    localTempDir.deleteOnExit();\n+    File localOutputTempDir = new File(localTempDir, \"output\");\n+    try {\n+      FileUtils.forceMkdir(localOutputTempDir);\n+      File segmentTarFile = generateSegment(localOutputTempDir);\n+      uploadSegment(segmentTarFile);\n+      return verifySegmentInState(STATE_ONLINE);\n+    } catch (Exception e) {\n+      LOGGER.error(\"Failed to create and upload segment for input data file {}.\", _inputDataFileName, e);\n+      return false;\n+    } finally {\n+      FileUtils.deleteQuietly(localTempDir);\n+    }\n+  }\n+\n+  /**\n+   * Generate the Segment(s) and then compress to TarGz file. Supports generation of segment files for one input data\n+   * file.\n+   * @param outputDir to generate the Segment file(s).\n+   * @return File object of the TarGz compressed segment file.\n+   * @throws Exception while generating segment files and/or compressing to TarGz.\n+   */\n+  private File generateSegment(File outputDir)\n+      throws Exception {\n+    TableConfig tableConfig = JsonUtils.fileToObject(new File(_tableConfigFileName), TableConfig.class);\n+    _tableName = tableConfig.getTableName();\n+\n+    Schema schema = JsonUtils.fileToObject(new File(_schemaFileName), Schema.class);\n+    RecordReaderConfig recordReaderConfig =\n+        RecordReaderFactory.getRecordReaderConfig(DEFAULT_FILE_FORMAT, _recordReaderConfigFileName);\n+\n+    SegmentGeneratorConfig segmentGeneratorConfig = new SegmentGeneratorConfig(tableConfig, schema);\n+    segmentGeneratorConfig.setInputFilePath(_inputDataFileName);\n+    segmentGeneratorConfig.setFormat(DEFAULT_FILE_FORMAT);\n+    segmentGeneratorConfig.setOutDir(outputDir.getAbsolutePath());\n+    segmentGeneratorConfig.setReaderConfig(recordReaderConfig);\n+    segmentGeneratorConfig.setTableName(_tableName);\n+    segmentGeneratorConfig.setSegmentName(_segmentName);\n+\n+    SegmentIndexCreationDriver driver = new SegmentIndexCreationDriverImpl();\n+    driver.init(segmentGeneratorConfig);\n+    driver.build();\n+    File indexDir = new File(outputDir, _segmentName);\n+    LOGGER.info(\"Successfully created segment: {} at directory: {}\", _segmentName, indexDir);\n+    File segmentTarFile = new File(outputDir, _segmentName + TarGzCompressionUtils.TAR_GZ_FILE_EXTENSION);\n+    TarGzCompressionUtils.createTarGzFile(indexDir, segmentTarFile);\n+    LOGGER.info(\"Tarring segment from: {} to: {}\", indexDir, segmentTarFile);\n+\n+    return segmentTarFile;\n+  }\n+\n+  /**\n+   * Upload the TarGz Segment file to the controller.\n+   * @param segmentTarFile TarGz Segment file\n+   * @throws Exception when upload segment fails.\n+   */\n+  private void uploadSegment(File segmentTarFile)\n+      throws Exception {\n+    URI controllerURI = FileUploadDownloadClient.getUploadSegmentURI(new URI(ClusterDescriptor.CONTROLLER_URL));\n+    try (FileUploadDownloadClient fileUploadDownloadClient = new FileUploadDownloadClient()) {\n+      fileUploadDownloadClient.uploadSegment(controllerURI, segmentTarFile.getName(), segmentTarFile, _tableName);\n+    }\n+  }\n+\n+  /**\n+   * Verify given table and segment name in the controller are in the state matching the parameter.\n+   * @param state of the segment to be verified in the controller.\n+   * @return true if segment is in the state provided in the parameter, else false.\n+   * @throws IOException\n+   * @throws InterruptedException\n+   */\n+  private boolean verifySegmentInState(String state)\n+      throws IOException, InterruptedException {\n+    long startTime = System.currentTimeMillis();\n+    while (getSegmentCountInState(state) <= 0) {\n+      if ((System.currentTimeMillis() - startTime) > DEFAULT_MAX_SLEEP_TIME_MS) {\n+        LOGGER.error(\"Upload segment verification failed, count is zero after max wait time {} ms.\",\n+            DEFAULT_MAX_SLEEP_TIME_MS);\n+        return false;\n+      }\n+      LOGGER.warn(\"Upload segment verification count is zero, will retry after {} ms.\", DEFAULT_SLEEP_INTERVAL_MS);\n+      Thread.sleep(DEFAULT_SLEEP_INTERVAL_MS);\n+    }\n+\n+    LOGGER.info(\"Successfully verified segment {} and its current status is {}.\", _segmentName, state);\n+    return true;\n+  }\n+\n+  /**\n+   * Deletes the segment for the given segment name and table name.\n+   * @return true if delete successful, else false.\n+   */\n+  private boolean deleteSegment() {\n+    try {\n+      TableConfig tableConfig = JsonUtils.fileToObject(new File(_tableConfigFileName), TableConfig.class);\n+      _tableName = tableConfig.getTableName();\n+\n+      ControllerTest.sendDeleteRequest(ControllerRequestURLBuilder.baseUrl(ClusterDescriptor.CONTROLLER_URL)\n+          .forSegmentDelete(_tableName, _segmentName));\n+      return verifySegmentDeleted();\n+    } catch (Exception e) {\n+      LOGGER.error(\"Request to delete the segment {} for the table {} failed.\", _segmentName, _tableName, e);\n+      return false;\n+    }\n+  }\n+\n+  /**\n+   * Verify given table name and segment name deleted from the controller.\n+   * @return true if no segment found, else false.\n+   * @throws IOException\n+   * @throws InterruptedException\n+   */\n+  private boolean verifySegmentDeleted()\n+      throws IOException, InterruptedException {\n+    long startTime = System.currentTimeMillis();\n+    while (getCountForSegmentName() > 0) {\n+      if ((System.currentTimeMillis() - startTime) > DEFAULT_MAX_SLEEP_TIME_MS) {\n+        LOGGER.error(\"Delete segment verification failed, count is greater than zero after max wait time {} ms.\",\n+            DEFAULT_MAX_SLEEP_TIME_MS);\n+        return false;\n+      }\n+      LOGGER.warn(\"Delete segment verification count greater than zero, will retry after {} ms.\",\n+          DEFAULT_SLEEP_INTERVAL_MS);\n+      Thread.sleep(DEFAULT_SLEEP_INTERVAL_MS);\n+    }\n+\n+    LOGGER.info(\"Successfully delete the segment {} for the table {}.\", _segmentName, _tableName);\n+    return true;\n+  }\n+\n+  /**\n+   * Retrieve external view for the given table name.\n+   * @return TableViews.TableView of OFFLINE and REALTIME segments.\n+   */\n+  private TableViews.TableView getExternalViewForTable()\n+      throws IOException {\n+    return JsonUtils.stringToObject(ControllerTest.sendGetRequest(\n+        ControllerRequestURLBuilder.baseUrl(ClusterDescriptor.CONTROLLER_URL).forTableExternalView(_tableName)),\n+        TableViews.TableView.class);\n+  }\n+\n+  /**\n+   * Retrieve the number of segments for both OFFLINE and REALTIME which are in state matching the parameter.\n+   * @param state of the segment to be verified in the controller.\n+   * @return count for OFFLINE and REALTIME segments.\n+   */\n+  private long getSegmentCountInState(String state)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bb5c23fbcfa0f67afc495b829c4bb59503fe0038"}, "originalPosition": 272}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQ5NjcxNg==", "bodyText": "segment names are case sensitive. Please do not ignore case.\nAlso, if there are two replicas, and one of them is ONLINE and the other is not, how will this work?\nInstead of counting the number, can we get the state of all replicas of the segment and return true ONLY if they are in the intended state?", "url": "https://github.com/apache/pinot/pull/6382#discussion_r551496716", "createdAt": "2021-01-04T18:43:06Z", "author": {"login": "mcvsubbu"}, "path": "pinot-integration-tests/src/test/java/org/apache/pinot/compat/tests/SegmentOp.java", "diffHunk": "@@ -82,14 +103,220 @@ public void setTableConfigFileName(String tableConfigFileName) {\n     _tableConfigFileName = tableConfigFileName;\n   }\n \n+  public void setSchemaFileName(String schemaFileName) {\n+    _schemaFileName = schemaFileName;\n+  }\n+\n+  public String getSchemaFileName() {\n+    return _schemaFileName;\n+  }\n+\n+  public void setRecordReaderConfigFileName(String recordReaderConfigFileName) {\n+    _recordReaderConfigFileName = recordReaderConfigFileName;\n+  }\n+\n+  public String getRecordReaderConfigFileName() {\n+    return _recordReaderConfigFileName;\n+  }\n+\n+  public void setSegmentName(String segmentName) {\n+    _segmentName = segmentName;\n+  }\n+\n+  public String getSegmentName() {\n+    return _segmentName;\n+  }\n+\n   @Override\n   boolean runOp() {\n-    switch(_op) {\n+    switch (_op) {\n       case UPLOAD:\n-        System.out.println(\"Generating segment \" + _segmentName + \" from \" + _inputDataFileName + \" and uploading to \" +\n-            _tableConfigFileName);\n+        return createAndUploadSegments();\n       case DELETE:\n+        return deleteSegment();\n     }\n     return true;\n   }\n+\n+  /**\n+   * Create Segment file, compress to TarGz, upload the files to controller and verify segment upload.\n+   * @return true if all successful, false in case of failure.\n+   */\n+  private boolean createAndUploadSegments() {\n+    File localTempDir = new File(FileUtils.getTempDirectory(), \"pinot-compat-test-\" + UUID.randomUUID());\n+    localTempDir.deleteOnExit();\n+    File localOutputTempDir = new File(localTempDir, \"output\");\n+    try {\n+      FileUtils.forceMkdir(localOutputTempDir);\n+      File segmentTarFile = generateSegment(localOutputTempDir);\n+      uploadSegment(segmentTarFile);\n+      return verifySegmentInState(STATE_ONLINE);\n+    } catch (Exception e) {\n+      LOGGER.error(\"Failed to create and upload segment for input data file {}.\", _inputDataFileName, e);\n+      return false;\n+    } finally {\n+      FileUtils.deleteQuietly(localTempDir);\n+    }\n+  }\n+\n+  /**\n+   * Generate the Segment(s) and then compress to TarGz file. Supports generation of segment files for one input data\n+   * file.\n+   * @param outputDir to generate the Segment file(s).\n+   * @return File object of the TarGz compressed segment file.\n+   * @throws Exception while generating segment files and/or compressing to TarGz.\n+   */\n+  private File generateSegment(File outputDir)\n+      throws Exception {\n+    TableConfig tableConfig = JsonUtils.fileToObject(new File(_tableConfigFileName), TableConfig.class);\n+    _tableName = tableConfig.getTableName();\n+\n+    Schema schema = JsonUtils.fileToObject(new File(_schemaFileName), Schema.class);\n+    RecordReaderConfig recordReaderConfig =\n+        RecordReaderFactory.getRecordReaderConfig(DEFAULT_FILE_FORMAT, _recordReaderConfigFileName);\n+\n+    SegmentGeneratorConfig segmentGeneratorConfig = new SegmentGeneratorConfig(tableConfig, schema);\n+    segmentGeneratorConfig.setInputFilePath(_inputDataFileName);\n+    segmentGeneratorConfig.setFormat(DEFAULT_FILE_FORMAT);\n+    segmentGeneratorConfig.setOutDir(outputDir.getAbsolutePath());\n+    segmentGeneratorConfig.setReaderConfig(recordReaderConfig);\n+    segmentGeneratorConfig.setTableName(_tableName);\n+    segmentGeneratorConfig.setSegmentName(_segmentName);\n+\n+    SegmentIndexCreationDriver driver = new SegmentIndexCreationDriverImpl();\n+    driver.init(segmentGeneratorConfig);\n+    driver.build();\n+    File indexDir = new File(outputDir, _segmentName);\n+    LOGGER.info(\"Successfully created segment: {} at directory: {}\", _segmentName, indexDir);\n+    File segmentTarFile = new File(outputDir, _segmentName + TarGzCompressionUtils.TAR_GZ_FILE_EXTENSION);\n+    TarGzCompressionUtils.createTarGzFile(indexDir, segmentTarFile);\n+    LOGGER.info(\"Tarring segment from: {} to: {}\", indexDir, segmentTarFile);\n+\n+    return segmentTarFile;\n+  }\n+\n+  /**\n+   * Upload the TarGz Segment file to the controller.\n+   * @param segmentTarFile TarGz Segment file\n+   * @throws Exception when upload segment fails.\n+   */\n+  private void uploadSegment(File segmentTarFile)\n+      throws Exception {\n+    URI controllerURI = FileUploadDownloadClient.getUploadSegmentURI(new URI(ClusterDescriptor.CONTROLLER_URL));\n+    try (FileUploadDownloadClient fileUploadDownloadClient = new FileUploadDownloadClient()) {\n+      fileUploadDownloadClient.uploadSegment(controllerURI, segmentTarFile.getName(), segmentTarFile, _tableName);\n+    }\n+  }\n+\n+  /**\n+   * Verify given table and segment name in the controller are in the state matching the parameter.\n+   * @param state of the segment to be verified in the controller.\n+   * @return true if segment is in the state provided in the parameter, else false.\n+   * @throws IOException\n+   * @throws InterruptedException\n+   */\n+  private boolean verifySegmentInState(String state)\n+      throws IOException, InterruptedException {\n+    long startTime = System.currentTimeMillis();\n+    while (getSegmentCountInState(state) <= 0) {\n+      if ((System.currentTimeMillis() - startTime) > DEFAULT_MAX_SLEEP_TIME_MS) {\n+        LOGGER.error(\"Upload segment verification failed, count is zero after max wait time {} ms.\",\n+            DEFAULT_MAX_SLEEP_TIME_MS);\n+        return false;\n+      }\n+      LOGGER.warn(\"Upload segment verification count is zero, will retry after {} ms.\", DEFAULT_SLEEP_INTERVAL_MS);\n+      Thread.sleep(DEFAULT_SLEEP_INTERVAL_MS);\n+    }\n+\n+    LOGGER.info(\"Successfully verified segment {} and its current status is {}.\", _segmentName, state);\n+    return true;\n+  }\n+\n+  /**\n+   * Deletes the segment for the given segment name and table name.\n+   * @return true if delete successful, else false.\n+   */\n+  private boolean deleteSegment() {\n+    try {\n+      TableConfig tableConfig = JsonUtils.fileToObject(new File(_tableConfigFileName), TableConfig.class);\n+      _tableName = tableConfig.getTableName();\n+\n+      ControllerTest.sendDeleteRequest(ControllerRequestURLBuilder.baseUrl(ClusterDescriptor.CONTROLLER_URL)\n+          .forSegmentDelete(_tableName, _segmentName));\n+      return verifySegmentDeleted();\n+    } catch (Exception e) {\n+      LOGGER.error(\"Request to delete the segment {} for the table {} failed.\", _segmentName, _tableName, e);\n+      return false;\n+    }\n+  }\n+\n+  /**\n+   * Verify given table name and segment name deleted from the controller.\n+   * @return true if no segment found, else false.\n+   * @throws IOException\n+   * @throws InterruptedException\n+   */\n+  private boolean verifySegmentDeleted()\n+      throws IOException, InterruptedException {\n+    long startTime = System.currentTimeMillis();\n+    while (getCountForSegmentName() > 0) {\n+      if ((System.currentTimeMillis() - startTime) > DEFAULT_MAX_SLEEP_TIME_MS) {\n+        LOGGER.error(\"Delete segment verification failed, count is greater than zero after max wait time {} ms.\",\n+            DEFAULT_MAX_SLEEP_TIME_MS);\n+        return false;\n+      }\n+      LOGGER.warn(\"Delete segment verification count greater than zero, will retry after {} ms.\",\n+          DEFAULT_SLEEP_INTERVAL_MS);\n+      Thread.sleep(DEFAULT_SLEEP_INTERVAL_MS);\n+    }\n+\n+    LOGGER.info(\"Successfully delete the segment {} for the table {}.\", _segmentName, _tableName);\n+    return true;\n+  }\n+\n+  /**\n+   * Retrieve external view for the given table name.\n+   * @return TableViews.TableView of OFFLINE and REALTIME segments.\n+   */\n+  private TableViews.TableView getExternalViewForTable()\n+      throws IOException {\n+    return JsonUtils.stringToObject(ControllerTest.sendGetRequest(\n+        ControllerRequestURLBuilder.baseUrl(ClusterDescriptor.CONTROLLER_URL).forTableExternalView(_tableName)),\n+        TableViews.TableView.class);\n+  }\n+\n+  /**\n+   * Retrieve the number of segments for both OFFLINE and REALTIME which are in state matching the parameter.\n+   * @param state of the segment to be verified in the controller.\n+   * @return count for OFFLINE and REALTIME segments.\n+   */\n+  private long getSegmentCountInState(String state)\n+      throws IOException {\n+    long offlineSegmentCount =\n+        getExternalViewForTable().offline != null ? getExternalViewForTable().offline.entrySet().stream()\n+            .filter(k -> k.getKey().equalsIgnoreCase(_segmentName)).filter(v -> v.getValue().values().contains(state))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bb5c23fbcfa0f67afc495b829c4bb59503fe0038"}, "originalPosition": 276}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d48a442b8c3951872987bdd3bd6e32f8b6c5c859", "author": {"user": {"login": "amarnathkarthik", "name": "Karthik Amarnath"}}, "url": "https://github.com/apache/pinot/commit/d48a442b8c3951872987bdd3bd6e32f8b6c5c859", "committedDate": "2021-01-04T23:07:46Z", "message": "Compatibility test for segment operations upload and delete"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYxNDE4Mzk0", "url": "https://github.com/apache/pinot/pull/6382#pullrequestreview-561418394", "createdAt": "2021-01-04T23:28:48Z", "commit": {"oid": "d48a442b8c3951872987bdd3bd6e32f8b6c5c859"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQyMzoyODo0OFrOIOEw0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQyMzoyODo0OFrOIOEw0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTYyOTAxMQ==", "bodyText": "Isnt it better to extract the specific segment instance state from the externalview (helix APIs) and explicitly check for specific state we want? Then we can apply this method to any state we like -- CONSUMING or ONLINE or OFFLINE etc.\nSo,\n\nread externalview into json\nExtract the key mapFields (a map)\nExtract the key _segmentName (another map)\nCheck all values to be the same as desired state\n\nThis way, we can re-use the method to check for any state we like (which we will in the future).\nthanks", "url": "https://github.com/apache/pinot/pull/6382#discussion_r551629011", "createdAt": "2021-01-04T23:28:48Z", "author": {"login": "mcvsubbu"}, "path": "pinot-integration-tests/src/test/java/org/apache/pinot/compat/tests/SegmentOp.java", "diffHunk": "@@ -82,14 +107,221 @@ public void setTableConfigFileName(String tableConfigFileName) {\n     _tableConfigFileName = tableConfigFileName;\n   }\n \n+  public void setSchemaFileName(String schemaFileName) {\n+    _schemaFileName = schemaFileName;\n+  }\n+\n+  public String getSchemaFileName() {\n+    return _schemaFileName;\n+  }\n+\n+  public void setRecordReaderConfigFileName(String recordReaderConfigFileName) {\n+    _recordReaderConfigFileName = recordReaderConfigFileName;\n+  }\n+\n+  public String getRecordReaderConfigFileName() {\n+    return _recordReaderConfigFileName;\n+  }\n+\n+  public void setSegmentName(String segmentName) {\n+    _segmentName = segmentName;\n+  }\n+\n+  public String getSegmentName() {\n+    return _segmentName;\n+  }\n+\n   @Override\n   boolean runOp() {\n-    switch(_op) {\n+    switch (_op) {\n       case UPLOAD:\n-        System.out.println(\"Generating segment \" + _segmentName + \" from \" + _inputDataFileName + \" and uploading to \" +\n-            _tableConfigFileName);\n+        return createAndUploadSegments();\n       case DELETE:\n+        return deleteSegment();\n+    }\n+    return true;\n+  }\n+\n+  /**\n+   * Create Segment file, compress to TarGz, upload the files to controller and verify segment upload.\n+   * @return true if all successful, false in case of failure.\n+   */\n+  private boolean createAndUploadSegments() {\n+    File localTempDir = new File(FileUtils.getTempDirectory(), \"pinot-compat-test-\" + UUID.randomUUID());\n+    localTempDir.deleteOnExit();\n+    File localOutputTempDir = new File(localTempDir, \"output\");\n+    try {\n+      FileUtils.forceMkdir(localOutputTempDir);\n+      File segmentTarFile = generateSegment(localOutputTempDir);\n+      uploadSegment(segmentTarFile);\n+      return verifySegmentInState(CommonConstants.Helix.StateModel.SegmentStateModel.ONLINE);\n+    } catch (Exception e) {\n+      LOGGER.error(\"Failed to create and upload segment for input data file {}.\", _inputDataFileName, e);\n+      return false;\n+    } finally {\n+      FileUtils.deleteQuietly(localTempDir);\n     }\n+  }\n+\n+  /**\n+   * Generate the Segment(s) and then compress to TarGz file. Supports generation of segment files for one input data\n+   * file.\n+   * @param outputDir to generate the Segment file(s).\n+   * @return File object of the TarGz compressed segment file.\n+   * @throws Exception while generating segment files and/or compressing to TarGz.\n+   */\n+  private File generateSegment(File outputDir)\n+      throws Exception {\n+    TableConfig tableConfig = JsonUtils.fileToObject(new File(_tableConfigFileName), TableConfig.class);\n+    _tableName = tableConfig.getTableName();\n+\n+    Schema schema = JsonUtils.fileToObject(new File(_schemaFileName), Schema.class);\n+    RecordReaderConfig recordReaderConfig =\n+        RecordReaderFactory.getRecordReaderConfig(DEFAULT_FILE_FORMAT, _recordReaderConfigFileName);\n+\n+    SegmentGeneratorConfig segmentGeneratorConfig = new SegmentGeneratorConfig(tableConfig, schema);\n+    segmentGeneratorConfig.setInputFilePath(_inputDataFileName);\n+    segmentGeneratorConfig.setFormat(DEFAULT_FILE_FORMAT);\n+    segmentGeneratorConfig.setOutDir(outputDir.getAbsolutePath());\n+    segmentGeneratorConfig.setReaderConfig(recordReaderConfig);\n+    segmentGeneratorConfig.setTableName(_tableName);\n+    segmentGeneratorConfig.setSegmentName(_segmentName);\n+\n+    SegmentIndexCreationDriver driver = new SegmentIndexCreationDriverImpl();\n+    driver.init(segmentGeneratorConfig);\n+    driver.build();\n+    File indexDir = new File(outputDir, _segmentName);\n+    LOGGER.info(\"Successfully created segment: {} at directory: {}\", _segmentName, indexDir);\n+    File segmentTarFile = new File(outputDir, _segmentName + TarGzCompressionUtils.TAR_GZ_FILE_EXTENSION);\n+    TarGzCompressionUtils.createTarGzFile(indexDir, segmentTarFile);\n+    LOGGER.info(\"Tarring segment from: {} to: {}\", indexDir, segmentTarFile);\n+\n+    return segmentTarFile;\n+  }\n+\n+  /**\n+   * Upload the TarGz Segment file to the controller.\n+   * @param segmentTarFile TarGz Segment file\n+   * @throws Exception when upload segment fails.\n+   */\n+  private void uploadSegment(File segmentTarFile)\n+      throws Exception {\n+    URI controllerURI = FileUploadDownloadClient.getUploadSegmentURI(new URI(ClusterDescriptor.CONTROLLER_URL));\n+    try (FileUploadDownloadClient fileUploadDownloadClient = new FileUploadDownloadClient()) {\n+      fileUploadDownloadClient.uploadSegment(controllerURI, segmentTarFile.getName(), segmentTarFile, _tableName);\n+    }\n+  }\n+\n+  /**\n+   * Verify given table and segment name in the controller are in the state matching the parameter.\n+   * @param state of the segment to be verified in the controller.\n+   * @return true if segment is in the state provided in the parameter, else false.\n+   * @throws IOException\n+   * @throws InterruptedException\n+   */\n+  private boolean verifySegmentInState(String state)\n+      throws IOException, InterruptedException {\n+    long startTime = System.currentTimeMillis();\n+    long segmentCount;\n+    while ((segmentCount = getSegmentCountInState(state)) <= 0) {\n+      if ((System.currentTimeMillis() - startTime) > DEFAULT_MAX_SLEEP_TIME_MS) {\n+        LOGGER.error(\"Upload segment verification failed, count is zero after max wait time {} ms.\",\n+            DEFAULT_MAX_SLEEP_TIME_MS);\n+        return false;\n+      } else if (segmentCount == -1) {\n+        LOGGER.error(\"Upload segment verification failed, one or more segment(s) is in {} state.\",\n+            CommonConstants.Helix.StateModel.SegmentStateModel.ERROR);\n+        return false;\n+      }\n+      LOGGER.warn(\"Upload segment verification count is zero, will retry after {} ms.\", DEFAULT_SLEEP_INTERVAL_MS);\n+      Thread.sleep(DEFAULT_SLEEP_INTERVAL_MS);\n+    }\n+\n+    LOGGER.info(\"Successfully verified segment {} and its current status is {}.\", _segmentName, state);\n     return true;\n   }\n+\n+  /**\n+   * Deletes the segment for the given segment name and table name.\n+   * @return true if delete successful, else false.\n+   */\n+  private boolean deleteSegment() {\n+    try {\n+      TableConfig tableConfig = JsonUtils.fileToObject(new File(_tableConfigFileName), TableConfig.class);\n+      _tableName = tableConfig.getTableName();\n+\n+      ControllerTest.sendDeleteRequest(ControllerRequestURLBuilder.baseUrl(ClusterDescriptor.CONTROLLER_URL)\n+          .forSegmentDelete(_tableName, _segmentName));\n+      return verifySegmentDeleted();\n+    } catch (Exception e) {\n+      LOGGER.error(\"Request to delete the segment {} for the table {} failed.\", _segmentName, _tableName, e);\n+      return false;\n+    }\n+  }\n+\n+  /**\n+   * Verify given table name and segment name deleted from the controller.\n+   * @return true if no segment found, else false.\n+   * @throws IOException\n+   * @throws InterruptedException\n+   */\n+  private boolean verifySegmentDeleted()\n+      throws IOException, InterruptedException {\n+    long startTime = System.currentTimeMillis();\n+    while (getCountForSegmentName() > 0) {\n+      if ((System.currentTimeMillis() - startTime) > DEFAULT_MAX_SLEEP_TIME_MS) {\n+        LOGGER.error(\"Delete segment verification failed, count is greater than zero after max wait time {} ms.\",\n+            DEFAULT_MAX_SLEEP_TIME_MS);\n+        return false;\n+      }\n+      LOGGER.warn(\"Delete segment verification count greater than zero, will retry after {} ms.\",\n+          DEFAULT_SLEEP_INTERVAL_MS);\n+      Thread.sleep(DEFAULT_SLEEP_INTERVAL_MS);\n+    }\n+\n+    LOGGER.info(\"Successfully delete the segment {} for the table {}.\", _segmentName, _tableName);\n+    return true;\n+  }\n+\n+  /**\n+   * Retrieve external view for the given table name.\n+   * @return TableViews.TableView of OFFLINE and REALTIME segments.\n+   */\n+  private TableViews.TableView getExternalViewForTable()\n+      throws IOException {\n+    return JsonUtils.stringToObject(ControllerTest.sendGetRequest(\n+        ControllerRequestURLBuilder.baseUrl(ClusterDescriptor.CONTROLLER_URL).forTableExternalView(_tableName)),\n+        TableViews.TableView.class);\n+  }\n+\n+  /**\n+   * Retrieve the number of segments for OFFLINE which are in state matching the parameter.\n+   * @param state of the segment to be verified in the controller.\n+   * @return -1 in case of ERROR, 0 if in OFFLINE state else return count of state matching parameter.\n+   */\n+  private long getSegmentCountInState(String state)\n+      throws IOException {\n+    final Set<String> segmentState =\n+        getExternalViewForTable().offline != null ? getExternalViewForTable().offline.entrySet().stream()\n+            .filter(k -> k.getKey().equals(_segmentName)).flatMap(x -> x.getValue().values().stream())\n+            .collect(Collectors.toSet()) : Collections.emptySet();\n+\n+    if (segmentState.contains(CommonConstants.Helix.StateModel.SegmentStateModel.ERROR)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d48a442b8c3951872987bdd3bd6e32f8b6c5c859"}, "originalPosition": 286}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d658cc9569e2bc50000850f695d29f247a0c63ec", "author": {"user": {"login": "amarnathkarthik", "name": "Karthik Amarnath"}}, "url": "https://github.com/apache/pinot/commit/d658cc9569e2bc50000850f695d29f247a0c63ec", "committedDate": "2021-01-05T00:31:42Z", "message": "Compatibility test for segment operations upload and delete"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYxNDUyMTY5", "url": "https://github.com/apache/pinot/pull/6382#pullrequestreview-561452169", "createdAt": "2021-01-05T01:06:53Z", "commit": {"oid": "d658cc9569e2bc50000850f695d29f247a0c63ec"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNVQwMTowNjo1M1rOIOGnTw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNVQwMTowNjo1M1rOIOGnTw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY1OTM0Mw==", "bodyText": "Better if this is a boolean return?", "url": "https://github.com/apache/pinot/pull/6382#discussion_r551659343", "createdAt": "2021-01-05T01:06:53Z", "author": {"login": "mcvsubbu"}, "path": "pinot-integration-tests/src/test/java/org/apache/pinot/compat/tests/SegmentOp.java", "diffHunk": "@@ -82,14 +107,219 @@ public void setTableConfigFileName(String tableConfigFileName) {\n     _tableConfigFileName = tableConfigFileName;\n   }\n \n+  public void setSchemaFileName(String schemaFileName) {\n+    _schemaFileName = schemaFileName;\n+  }\n+\n+  public String getSchemaFileName() {\n+    return _schemaFileName;\n+  }\n+\n+  public void setRecordReaderConfigFileName(String recordReaderConfigFileName) {\n+    _recordReaderConfigFileName = recordReaderConfigFileName;\n+  }\n+\n+  public String getRecordReaderConfigFileName() {\n+    return _recordReaderConfigFileName;\n+  }\n+\n+  public void setSegmentName(String segmentName) {\n+    _segmentName = segmentName;\n+  }\n+\n+  public String getSegmentName() {\n+    return _segmentName;\n+  }\n+\n   @Override\n   boolean runOp() {\n-    switch(_op) {\n+    switch (_op) {\n       case UPLOAD:\n-        System.out.println(\"Generating segment \" + _segmentName + \" from \" + _inputDataFileName + \" and uploading to \" +\n-            _tableConfigFileName);\n+        return createAndUploadSegments();\n       case DELETE:\n+        return deleteSegment();\n+    }\n+    return true;\n+  }\n+\n+  /**\n+   * Create Segment file, compress to TarGz, upload the files to controller and verify segment upload.\n+   * @return true if all successful, false in case of failure.\n+   */\n+  private boolean createAndUploadSegments() {\n+    File localTempDir = new File(FileUtils.getTempDirectory(), \"pinot-compat-test-\" + UUID.randomUUID());\n+    localTempDir.deleteOnExit();\n+    File localOutputTempDir = new File(localTempDir, \"output\");\n+    try {\n+      FileUtils.forceMkdir(localOutputTempDir);\n+      File segmentTarFile = generateSegment(localOutputTempDir);\n+      uploadSegment(segmentTarFile);\n+      return verifySegmentInState(CommonConstants.Helix.StateModel.SegmentStateModel.ONLINE);\n+    } catch (Exception e) {\n+      LOGGER.error(\"Failed to create and upload segment for input data file {}.\", _inputDataFileName, e);\n+      return false;\n+    } finally {\n+      FileUtils.deleteQuietly(localTempDir);\n     }\n+  }\n+\n+  /**\n+   * Generate the Segment(s) and then compress to TarGz file. Supports generation of segment files for one input data\n+   * file.\n+   * @param outputDir to generate the Segment file(s).\n+   * @return File object of the TarGz compressed segment file.\n+   * @throws Exception while generating segment files and/or compressing to TarGz.\n+   */\n+  private File generateSegment(File outputDir)\n+      throws Exception {\n+    TableConfig tableConfig = JsonUtils.fileToObject(new File(_tableConfigFileName), TableConfig.class);\n+    _tableName = tableConfig.getTableName();\n+\n+    Schema schema = JsonUtils.fileToObject(new File(_schemaFileName), Schema.class);\n+    RecordReaderConfig recordReaderConfig =\n+        RecordReaderFactory.getRecordReaderConfig(DEFAULT_FILE_FORMAT, _recordReaderConfigFileName);\n+\n+    SegmentGeneratorConfig segmentGeneratorConfig = new SegmentGeneratorConfig(tableConfig, schema);\n+    segmentGeneratorConfig.setInputFilePath(_inputDataFileName);\n+    segmentGeneratorConfig.setFormat(DEFAULT_FILE_FORMAT);\n+    segmentGeneratorConfig.setOutDir(outputDir.getAbsolutePath());\n+    segmentGeneratorConfig.setReaderConfig(recordReaderConfig);\n+    segmentGeneratorConfig.setTableName(_tableName);\n+    segmentGeneratorConfig.setSegmentName(_segmentName);\n+\n+    SegmentIndexCreationDriver driver = new SegmentIndexCreationDriverImpl();\n+    driver.init(segmentGeneratorConfig);\n+    driver.build();\n+    File indexDir = new File(outputDir, _segmentName);\n+    LOGGER.info(\"Successfully created segment: {} at directory: {}\", _segmentName, indexDir);\n+    File segmentTarFile = new File(outputDir, _segmentName + TarGzCompressionUtils.TAR_GZ_FILE_EXTENSION);\n+    TarGzCompressionUtils.createTarGzFile(indexDir, segmentTarFile);\n+    LOGGER.info(\"Tarring segment from: {} to: {}\", indexDir, segmentTarFile);\n+\n+    return segmentTarFile;\n+  }\n+\n+  /**\n+   * Upload the TarGz Segment file to the controller.\n+   * @param segmentTarFile TarGz Segment file\n+   * @throws Exception when upload segment fails.\n+   */\n+  private void uploadSegment(File segmentTarFile)\n+      throws Exception {\n+    URI controllerURI = FileUploadDownloadClient.getUploadSegmentURI(new URI(ClusterDescriptor.CONTROLLER_URL));\n+    try (FileUploadDownloadClient fileUploadDownloadClient = new FileUploadDownloadClient()) {\n+      fileUploadDownloadClient.uploadSegment(controllerURI, segmentTarFile.getName(), segmentTarFile, _tableName);\n+    }\n+  }\n+\n+  /**\n+   * Verify given table and segment name in the controller are in the state matching the parameter.\n+   * @param state of the segment to be verified in the controller.\n+   * @return true if segment is in the state provided in the parameter, else false.\n+   * @throws IOException\n+   * @throws InterruptedException\n+   */\n+  private boolean verifySegmentInState(String state)\n+      throws IOException, InterruptedException {\n+    long startTime = System.currentTimeMillis();\n+    long segmentCount;\n+    while ((segmentCount = getSegmentCountInState(state)) <= 0) {\n+      if ((System.currentTimeMillis() - startTime) > DEFAULT_MAX_SLEEP_TIME_MS) {\n+        LOGGER.error(\"Upload segment verification failed, count is zero after max wait time {} ms.\",\n+            DEFAULT_MAX_SLEEP_TIME_MS);\n+        return false;\n+      } else if (segmentCount == -1) {\n+        LOGGER.error(\"Upload segment verification failed, one or more segment(s) is in {} state.\",\n+            CommonConstants.Helix.StateModel.SegmentStateModel.ERROR);\n+        return false;\n+      }\n+      LOGGER.warn(\"Upload segment verification count is zero, will retry after {} ms.\", DEFAULT_SLEEP_INTERVAL_MS);\n+      Thread.sleep(DEFAULT_SLEEP_INTERVAL_MS);\n+    }\n+\n+    LOGGER.info(\"Successfully verified segment {} and its current status is {}.\", _segmentName, state);\n     return true;\n   }\n+\n+  /**\n+   * Deletes the segment for the given segment name and table name.\n+   * @return true if delete successful, else false.\n+   */\n+  private boolean deleteSegment() {\n+    try {\n+      TableConfig tableConfig = JsonUtils.fileToObject(new File(_tableConfigFileName), TableConfig.class);\n+      _tableName = tableConfig.getTableName();\n+\n+      ControllerTest.sendDeleteRequest(ControllerRequestURLBuilder.baseUrl(ClusterDescriptor.CONTROLLER_URL)\n+          .forSegmentDelete(_tableName, _segmentName));\n+      return verifySegmentDeleted();\n+    } catch (Exception e) {\n+      LOGGER.error(\"Request to delete the segment {} for the table {} failed.\", _segmentName, _tableName, e);\n+      return false;\n+    }\n+  }\n+\n+  /**\n+   * Verify given table name and segment name deleted from the controller.\n+   * @return true if no segment found, else false.\n+   * @throws IOException\n+   * @throws InterruptedException\n+   */\n+  private boolean verifySegmentDeleted()\n+      throws IOException, InterruptedException {\n+    long startTime = System.currentTimeMillis();\n+    while (getCountForSegmentName() > 0) {\n+      if ((System.currentTimeMillis() - startTime) > DEFAULT_MAX_SLEEP_TIME_MS) {\n+        LOGGER.error(\"Delete segment verification failed, count is greater than zero after max wait time {} ms.\",\n+            DEFAULT_MAX_SLEEP_TIME_MS);\n+        return false;\n+      }\n+      LOGGER.warn(\"Delete segment verification count greater than zero, will retry after {} ms.\",\n+          DEFAULT_SLEEP_INTERVAL_MS);\n+      Thread.sleep(DEFAULT_SLEEP_INTERVAL_MS);\n+    }\n+\n+    LOGGER.info(\"Successfully delete the segment {} for the table {}.\", _segmentName, _tableName);\n+    return true;\n+  }\n+\n+  /**\n+   * Retrieve external view for the given table name.\n+   * @return TableViews.TableView of OFFLINE and REALTIME segments.\n+   */\n+  private TableViews.TableView getExternalViewForTable()\n+      throws IOException {\n+    return JsonUtils.stringToObject(ControllerTest.sendGetRequest(\n+        ControllerRequestURLBuilder.baseUrl(ClusterDescriptor.CONTROLLER_URL).forTableExternalView(_tableName)),\n+        TableViews.TableView.class);\n+  }\n+\n+  /**\n+   * Retrieve the number of segments for OFFLINE which are in state matching the parameter.\n+   * @param state of the segment to be verified in the controller.\n+   * @return -1 in case of ERROR, 1 if all matches the state else 0.\n+   */\n+  private long getSegmentCountInState(String state)\n+      throws IOException {\n+    final Set<String> segmentState =\n+        getExternalViewForTable().offline != null ? getExternalViewForTable().offline.entrySet().stream()\n+            .filter(k -> k.getKey().equals(_segmentName)).flatMap(x -> x.getValue().values().stream())\n+            .collect(Collectors.toSet()) : Collections.emptySet();\n+\n+    if (segmentState.contains(CommonConstants.Helix.StateModel.SegmentStateModel.ERROR)) {\n+      return -1;\n+    }\n+\n+    return segmentState.stream().allMatch(x -> x.contains(state)) ? 1 : 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d658cc9569e2bc50000850f695d29f247a0c63ec"}, "originalPosition": 290}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYxNDUyMjAx", "url": "https://github.com/apache/pinot/pull/6382#pullrequestreview-561452201", "createdAt": "2021-01-05T01:06:58Z", "commit": {"oid": "d658cc9569e2bc50000850f695d29f247a0c63ec"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1689, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}