{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDc0OTg0Nzgy", "number": 5934, "title": "Segment processing framework", "bodyText": "Description\n#5753\nA Segment Processing Framework to convert \"m\" segments to \"n\" segments\nThe phases of the Segment Processor are\n\nMap\n\n\nRecordTransformation (using transform functions)\nRecord filtering (using filter functions)\nPartitioning (Column value based, transform function based, table config's partition config based)\n\n\nReduce\n\n\nRollup/Concat records\nSplit into parts\nSort\n\n\nSegment generation\n\nA SegmentProcessorFrameworkCommand is provided to run this on demand. Run using command\nbin/pinot-admin.sh SegmentProcessorFramework -segmentProcessorFrameworkSpec /<path>/spec.json\nwhere spec.json is\n{\n  \"inputSegmentsDir\": \"/<base_dir>/segmentsDir\",\n  \"outputSegmentsDir\": \"/<base_dir>/outputDir/\",\n  \"schemaFile\": \"/<base_dir>/schema.json\",\n  \"tableConfigFile\": \"/<base_dir>/table.json\",\n  \"recordTransformerConfig\": {\n    \"transformFunctionsMap\": {\n      \"epochMillis\": \"round(epochMillis, 86400000)\" // round to nearest day\n    }\n  },\n  \"recordFilterConfig\": {\n    \"recordFilterType\": \"FILTER_FUNCTION\",\n    \"filterFunction\": \"Groovy({epochMillis != \\\"1597795200000\\\"}, epochMillis)\"\n  },\n  \"partitioningConfig\": {\n    \"partitionerType\": \"COLUMN_VALUE\", // partition on epochMillis\n    \"columnName\": \"epochMillis\"\n  },\n  \"collectorConfig\": {\n    \"collectorType\": \"ROLLUP\", // rollup clicks by summing\n    \"aggregatorTypeMap\": {\n      \"clicks\": \"SUM\"\n    }\n  },\n  \"segmentConfig\": {\n    \"maxNumRecordsPerSegment\": 200_000\n  }\n}\n\nNote:\n\nCurrently this framework attempts to do no parallelism in the map/reduce/segment creation jobs. Each input file will be processed sequentially in map stage, each part will be executed sequentially in reduce, and each segment will be built one after another. We can change this in the future if the need arises to make this more advanced.\nThe framework makes the assumption that there's enough memory to hold all records of a partition in memory, during rollups in reducer. A limit of 5M records has been set on the Reducer as the number of records to collect before forcing a flush, as a safety measure. In future we could consider using off heap processing, if memory becomes a problem.\n\nThis framework will typically be used by minion tasks, which want to perform some processing on segments\n(eg task which merges segments, tasks which aligns segments per time boundaries etc). The existing Segment merge jobs can be changed to use this framework.\nPending\nEnhancements like (Added TODOs in code)\n\nPut null in GenericRecord if nullValueFields contains the field\nInterface out underlying file format (currently avro)\nDedup\nUsing off-heap based implementation for aggregation/sorting in the reduce\n2 step partitioner 1) Apply custom partitioner 2) Apply table config partitioner. Combine both to get final partition.\nConfigs for segment name (like prefix)", "createdAt": "2020-08-27T22:29:38Z", "url": "https://github.com/apache/pinot/pull/5934", "merged": true, "mergeCommit": {"oid": "41de9a62c6fab818d0e70a65701e00217cafd67d"}, "closed": true, "closedAt": "2020-09-15T16:49:15Z", "author": {"login": "npawar"}, "timelineItems": {"totalCount": 20, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdBPTLagH2gAyNDc0OTg0NzgyOmY1MTFmMWY2N2IxMjU1MGM1ZmViMTg4NTE3YTBiOWYzY2JjMjJiYjU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdJKSfWAFqTQ4ODg1NDcwNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "f511f1f67b12550c5feb188517a0b9f3cbc22bb5", "author": {"user": {"login": "npawar", "name": "Neha Pawar"}}, "url": "https://github.com/apache/pinot/commit/f511f1f67b12550c5feb188517a0b9f3cbc22bb5", "committedDate": "2020-08-22T01:40:41Z", "message": "Segment Processor Framework"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b6d941c28ca25c9772ca78a7e60b222339662235", "author": {"user": {"login": "npawar", "name": "Neha Pawar"}}, "url": "https://github.com/apache/pinot/commit/b6d941c28ca25c9772ca78a7e60b222339662235", "committedDate": "2020-08-27T02:06:44Z", "message": "Tests and pinot-admin command"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a4e0f72a47771e951659b0661de58dc2a0f2c068", "author": {"user": {"login": "npawar", "name": "Neha Pawar"}}, "url": "https://github.com/apache/pinot/commit/a4e0f72a47771e951659b0661de58dc2a0f2c068", "committedDate": "2020-08-27T20:05:34Z", "message": "MV tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1", "author": {"user": {"login": "npawar", "name": "Neha Pawar"}}, "url": "https://github.com/apache/pinot/commit/eece981149304287c751d3dd44be40f14bfcc7a1", "committedDate": "2020-08-27T22:39:20Z", "message": "Javadoc and imports"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "80e1c4d3a6a85f212d224b8d7c548f747d05a8ae", "author": {"user": {"login": "npawar", "name": "Neha Pawar"}}, "url": "https://github.com/apache/pinot/commit/80e1c4d3a6a85f212d224b8d7c548f747d05a8ae", "committedDate": "2020-08-27T22:30:33Z", "message": "Remove unused imports"}, "afterCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1", "author": {"user": {"login": "npawar", "name": "Neha Pawar"}}, "url": "https://github.com/apache/pinot/commit/eece981149304287c751d3dd44be40f14bfcc7a1", "committedDate": "2020-08-27T22:39:20Z", "message": "Javadoc and imports"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc3MTQ3NTk0", "url": "https://github.com/apache/pinot/pull/5934#pullrequestreview-477147594", "createdAt": "2020-08-27T23:23:43Z", "commit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QyMzoyMzo0M1rOHIkfDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QyMzo1NjoxM1rOHIlDVg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc0ODQyOQ==", "bodyText": "Can we interface out the underlying data file format? We may need to plug in other data format for performance boost (for example, @Jackie-Jiang used the mmaped file with custom format for star-tree generator and it's also doing the similar work - sort, aggregation)", "url": "https://github.com/apache/pinot/pull/5934#discussion_r478748429", "createdAt": "2020-08-27T23:23:43Z", "author": {"login": "snleee"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentMapper.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.framework;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.avro.Schema;\n+import org.apache.avro.file.DataFileWriter;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.pinot.core.data.readers.PinotSegmentRecordReader;\n+import org.apache.pinot.core.segment.processing.partitioner.PartitionFilter;\n+import org.apache.pinot.core.segment.processing.partitioner.Partitioner;\n+import org.apache.pinot.core.segment.processing.partitioner.PartitionerFactory;\n+import org.apache.pinot.core.segment.processing.transformer.RecordTransformer;\n+import org.apache.pinot.core.segment.processing.transformer.RecordTransformerFactory;\n+import org.apache.pinot.core.segment.processing.utils.SegmentProcessorUtils;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Mapper phase of the SegmentProcessorFramework.\n+ * Reads the input segment and creates partitioned avro data files\n+ * Performs:\n+ * - record transformations\n+ * - partitioning\n+ * - partition filtering\n+ */\n+public class SegmentMapper {\n+\n+  private static final Logger LOGGER = LoggerFactory.getLogger(SegmentMapper.class);\n+  private final File _inputSegment;\n+  private final File _mapperOutputDir;\n+\n+  private final String _mapperId;\n+  private final Schema _avroSchema;\n+  private final RecordTransformer _recordTransformer;\n+  private final Partitioner _partitioner;\n+  private final PartitionFilter _partitionFilter;\n+  private final Map<String, DataFileWriter<GenericData.Record>> _partitionToDataFileWriterMap = new HashMap<>();\n+\n+  public SegmentMapper(String mapperId, File inputSegment, SegmentMapperConfig mapperConfig, File mapperOutputDir) {\n+    _inputSegment = inputSegment;\n+    _mapperOutputDir = mapperOutputDir;\n+\n+    _mapperId = mapperId;\n+    _avroSchema = SegmentProcessorUtils.convertPinotSchemaToAvroSchema(mapperConfig.getPinotSchema());\n+    _recordTransformer = RecordTransformerFactory.getRecordTransformer(mapperConfig.getRecordTransformerConfig());\n+    _partitioner = PartitionerFactory.getPartitioner(mapperConfig.getPartitioningConfig());\n+    _partitionFilter = PartitionerFactory.getPartitionFilter(mapperConfig.getPartitioningConfig());\n+    LOGGER.info(\n+        \"Initialized mapper with id: {}, input segment: {}, output dir: {}, recordTransformer: {}, partitioner: {}, partitionFilter: {}\",\n+        _mapperId, _inputSegment, _mapperOutputDir, _recordTransformer.getClass(), _partitioner.getClass(),\n+        _partitionFilter.getClass());\n+  }\n+\n+  /**\n+   * Reads the input segment and generates partitioned avro data files into the mapper output directory\n+   * Records for each partition are put into a directory of its own withing the mapper output directory, identified by the partition name\n+   */\n+  public void map()\n+      throws Exception {\n+\n+    PinotSegmentRecordReader segmentRecordReader = new PinotSegmentRecordReader(_inputSegment);\n+    GenericRow reusableRow = new GenericRow();\n+    GenericData.Record reusableRecord = new GenericData.Record(_avroSchema);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc1NjY0NQ==", "bodyText": "What if I want to run the custom transformation function?\nThe spec doesn't have a way to specify the custom transformer?\n  \"recordTransformerConfig\": {\n    \"transformFunctionsMap\": {\n      \"epochMillis\": \"round(epochMillis, 86400000)\" // round to nearest day\n    }\n  },", "url": "https://github.com/apache/pinot/pull/5934#discussion_r478756645", "createdAt": "2020-08-27T23:52:17Z", "author": {"login": "snleee"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/transformer/TransformFunctionRecordTransformer.java", "diffHunk": "@@ -0,0 +1,49 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.transformer;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.pinot.core.data.function.FunctionEvaluator;\n+import org.apache.pinot.core.data.function.FunctionEvaluatorFactory;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * RecordTransformer which executes transform functions to transform columns of record\n+ * Does not follow any particular order, and hence cannot support transformations where strict order of execution is needed\n+ */\n+public class TransformFunctionRecordTransformer implements RecordTransformer {\n+\n+  private final Map<String, FunctionEvaluator> _functionEvaluatorMap = new HashMap<>();\n+\n+  public TransformFunctionRecordTransformer(Map<String, String> transformFunctionMap) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc1NzcxOA==", "bodyText": "How do we handle record filtering that happens in the transformRecord?\nOr, are we model filtering as a separate step? We should have some convention on record filtering.", "url": "https://github.com/apache/pinot/pull/5934#discussion_r478757718", "createdAt": "2020-08-27T23:56:13Z", "author": {"login": "snleee"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentMapper.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.framework;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.avro.Schema;\n+import org.apache.avro.file.DataFileWriter;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.pinot.core.data.readers.PinotSegmentRecordReader;\n+import org.apache.pinot.core.segment.processing.partitioner.PartitionFilter;\n+import org.apache.pinot.core.segment.processing.partitioner.Partitioner;\n+import org.apache.pinot.core.segment.processing.partitioner.PartitionerFactory;\n+import org.apache.pinot.core.segment.processing.transformer.RecordTransformer;\n+import org.apache.pinot.core.segment.processing.transformer.RecordTransformerFactory;\n+import org.apache.pinot.core.segment.processing.utils.SegmentProcessorUtils;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Mapper phase of the SegmentProcessorFramework.\n+ * Reads the input segment and creates partitioned avro data files\n+ * Performs:\n+ * - record transformations\n+ * - partitioning\n+ * - partition filtering\n+ */\n+public class SegmentMapper {\n+\n+  private static final Logger LOGGER = LoggerFactory.getLogger(SegmentMapper.class);\n+  private final File _inputSegment;\n+  private final File _mapperOutputDir;\n+\n+  private final String _mapperId;\n+  private final Schema _avroSchema;\n+  private final RecordTransformer _recordTransformer;\n+  private final Partitioner _partitioner;\n+  private final PartitionFilter _partitionFilter;\n+  private final Map<String, DataFileWriter<GenericData.Record>> _partitionToDataFileWriterMap = new HashMap<>();\n+\n+  public SegmentMapper(String mapperId, File inputSegment, SegmentMapperConfig mapperConfig, File mapperOutputDir) {\n+    _inputSegment = inputSegment;\n+    _mapperOutputDir = mapperOutputDir;\n+\n+    _mapperId = mapperId;\n+    _avroSchema = SegmentProcessorUtils.convertPinotSchemaToAvroSchema(mapperConfig.getPinotSchema());\n+    _recordTransformer = RecordTransformerFactory.getRecordTransformer(mapperConfig.getRecordTransformerConfig());\n+    _partitioner = PartitionerFactory.getPartitioner(mapperConfig.getPartitioningConfig());\n+    _partitionFilter = PartitionerFactory.getPartitionFilter(mapperConfig.getPartitioningConfig());\n+    LOGGER.info(\n+        \"Initialized mapper with id: {}, input segment: {}, output dir: {}, recordTransformer: {}, partitioner: {}, partitionFilter: {}\",\n+        _mapperId, _inputSegment, _mapperOutputDir, _recordTransformer.getClass(), _partitioner.getClass(),\n+        _partitionFilter.getClass());\n+  }\n+\n+  /**\n+   * Reads the input segment and generates partitioned avro data files into the mapper output directory\n+   * Records for each partition are put into a directory of its own withing the mapper output directory, identified by the partition name\n+   */\n+  public void map()\n+      throws Exception {\n+\n+    PinotSegmentRecordReader segmentRecordReader = new PinotSegmentRecordReader(_inputSegment);\n+    GenericRow reusableRow = new GenericRow();\n+    GenericData.Record reusableRecord = new GenericData.Record(_avroSchema);\n+\n+    Set<String> selectedPartitions = new HashSet<>();\n+    Set<String> rejectedPartitions = new HashSet<>();\n+\n+    while (segmentRecordReader.hasNext()) {\n+      reusableRow = segmentRecordReader.next(reusableRow);\n+\n+      // Record transformation\n+      reusableRow = _recordTransformer.transformRecord(reusableRow);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 99}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc5MDQ3MDk5", "url": "https://github.com/apache/pinot/pull/5934#pullrequestreview-479047099", "createdAt": "2020-08-31T23:33:13Z", "commit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMVQyMzozMzoxM1rOHKNvOw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwMDoyNToyOVrOHKPEqA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ3Mjg5MQ==", "bodyText": "The arguments should be a key-value pair instead of an array. How are you going to map the values to the function parameters?", "url": "https://github.com/apache/pinot/pull/5934#discussion_r480472891", "createdAt": "2020-08-31T23:33:13Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/data/function/FunctionEvaluator.java", "diffHunk": "@@ -36,4 +36,9 @@\n    * Evaluate the function on the generic row and return the result\n    */\n   Object evaluate(GenericRow genericRow);\n+\n+  /**\n+   * Evaluate the function on the given arguments\n+   */\n+  Object evaluate(Object[] arguments);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ3Mzc0Nw==", "bodyText": "This API won't work for in-build functions because it needs to read column values in order to evaluate the function", "url": "https://github.com/apache/pinot/pull/5934#discussion_r480473747", "createdAt": "2020-08-31T23:36:00Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/data/function/InbuiltFunctionEvaluator.java", "diffHunk": "@@ -155,5 +177,10 @@ public String execute(GenericRow row) {\n     public Object execute(GenericRow row) {\n       return row.getValue(_column);\n     }\n+\n+    @Override\n+    public Object execute(Object[] arguments) {\n+      throw new UnsupportedOperationException(\"Operation not supported for ColumnExecutionNode\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ3Njc0Mw==", "bodyText": "Why associating the filter with the partition? We can apply the filter to the records, then you can directly use the current FunctionEvaluator\nIMO, record filtering is easier to config and more intuitive. I cannot think of a use case where we have to apply the filter to the partition", "url": "https://github.com/apache/pinot/pull/5934#discussion_r480476743", "createdAt": "2020-08-31T23:46:20Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/partitioner/PartitionFilter.java", "diffHunk": "@@ -0,0 +1,30 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.partitioner;\n+\n+/**\n+ * Used for filtering partitions in the mapper\n+ */\n+public interface PartitionFilter {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ4NjgyMA==", "bodyText": "I feel ROW_HASH is not really useful. Maybe ROUND_ROBIN is enough?", "url": "https://github.com/apache/pinot/pull/5934#discussion_r480486820", "createdAt": "2020-09-01T00:13:28Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/partitioner/PartitionerFactory.java", "diffHunk": "@@ -0,0 +1,82 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.partitioner;\n+\n+import com.google.common.base.Preconditions;\n+\n+\n+/**\n+ * Factory for Partitioner and PartitionFilter\n+ */\n+public final class PartitionerFactory {\n+\n+  private PartitionerFactory() {\n+\n+  }\n+\n+  public enum PartitionerType {\n+    NO_OP, ROW_HASH, COLUMN_VALUE, TRANSFORM_FUNCTION, TABLE_PARTITION_CONFIG\n+  }\n+\n+  /**\n+   * Construct a Partitioner using the PartitioningConfig\n+   */\n+  public static Partitioner getPartitioner(PartitioningConfig config) {\n+\n+    Partitioner partitioner = null;\n+    switch (config.getPartitionerType()) {\n+      case NO_OP:\n+        partitioner = new NoOpPartitioner();\n+        break;\n+      case ROW_HASH:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ4OTkxMA==", "bodyText": "It should also include sorted columns (list of columns, where records are sorted on the first column firstly, then second column etc.)", "url": "https://github.com/apache/pinot/pull/5934#discussion_r480489910", "createdAt": "2020-09-01T00:18:14Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/CollectorConfig.java", "diffHunk": "@@ -0,0 +1,85 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import com.fasterxml.jackson.databind.annotation.JsonDeserialize;\n+import com.fasterxml.jackson.databind.annotation.JsonPOJOBuilder;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+\n+\n+/**\n+ * Config for Collector\n+ */\n+@JsonDeserialize(builder = CollectorConfig.Builder.class)\n+public class CollectorConfig {\n+  private static final CollectorFactory.CollectorType DEFAULT_COLLECTOR_TYPE = CollectorFactory.CollectorType.CONCAT;\n+\n+  private final CollectorFactory.CollectorType _collectorType;\n+  private final Map<String, ValueAggregatorFactory.ValueAggregatorType> _aggregatorTypeMap;\n+\n+  private CollectorConfig(CollectorFactory.CollectorType collectorType,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ5NDc2MA==", "bodyText": "Should we use _numRecordsPerPart here so that once the collector collects enough records, we flush them?", "url": "https://github.com/apache/pinot/pull/5934#discussion_r480494760", "createdAt": "2020-09-01T00:25:29Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentReducer.java", "diffHunk": "@@ -0,0 +1,151 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.framework;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import org.apache.avro.file.DataFileWriter;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.pinot.core.segment.processing.collector.Collector;\n+import org.apache.pinot.core.segment.processing.collector.CollectorFactory;\n+import org.apache.pinot.core.segment.processing.utils.SegmentProcessorUtils;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+import org.apache.pinot.spi.data.readers.RecordReader;\n+import org.apache.pinot.spi.data.readers.RecordReaderFactory;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Reducer phase of the SegmentProcessorFramework\n+ * Reads the avro files in the input directory and creates output avro files in the reducer output directory.\n+ * The avro files in the input directory are expected to contain data for only 1 partition\n+ * Performs operations on that partition data as follows:\n+ * - concatenation/rollup of records\n+ * - split\n+ * - TODO: dedup\n+ */\n+public class SegmentReducer {\n+\n+  private static final Logger LOGGER = LoggerFactory.getLogger(SegmentReducer.class);\n+  private static final int MAX_RECORDS_TO_COLLECT = 5_000_000;\n+\n+  private final File _reducerInputDir;\n+  private final File _reducerOutputDir;\n+\n+  private final String _reducerId;\n+  private final Schema _pinotSchema;\n+  private final org.apache.avro.Schema _avroSchema;\n+  private final Collector _collector;\n+  private final int _numRecordsPerPart;\n+\n+  public SegmentReducer(String reducerId, File reducerInputDir, SegmentReducerConfig reducerConfig,\n+      File reducerOutputDir) {\n+    _reducerInputDir = reducerInputDir;\n+    _reducerOutputDir = reducerOutputDir;\n+\n+    _reducerId = reducerId;\n+    _pinotSchema = reducerConfig.getPinotSchema();\n+    _avroSchema = SegmentProcessorUtils.convertPinotSchemaToAvroSchema(_pinotSchema);\n+    _collector = CollectorFactory.getCollector(reducerConfig.getCollectorConfig(), _pinotSchema);\n+    _numRecordsPerPart = reducerConfig.getNumRecordsPerPart();\n+    LOGGER.info(\"Initialized reducer with id: {}, input dir: {}, output dir: {}, collector: {}, numRecordsPerPart: {}\",\n+        _reducerId, _reducerInputDir, _reducerOutputDir, _collector.getClass(), _numRecordsPerPart);\n+  }\n+\n+  /**\n+   * Reads the avro files in the input directory.\n+   * Performs configured operations and outputs to other avro file(s) in the reducer output directory.\n+   */\n+  public void reduce()\n+      throws Exception {\n+\n+    int part = 0;\n+    for (File inputFile : _reducerInputDir.listFiles()) {\n+\n+      RecordReader avroRecordReader = RecordReaderFactory\n+          .getRecordReaderByClass(\"org.apache.pinot.plugin.inputformat.avro.AvroRecordReader\", inputFile,\n+              _pinotSchema.getColumnNames(), null);\n+\n+      while (avroRecordReader.hasNext()) {\n+        GenericRow next = avroRecordReader.next();\n+\n+        // Aggregations\n+        _collector.collect(next);\n+\n+        // Exceeded max records allowed to collect. Flush\n+        if (_collector.size() == MAX_RECORDS_TO_COLLECT) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 96}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc5OTQ1Nzcz", "url": "https://github.com/apache/pinot/pull/5934#pullrequestreview-479945773", "createdAt": "2020-09-01T16:59:30Z", "commit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQxNjo1OTozMFrOHLAALw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQxNzoyMzoxM1rOHLA1DQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTI5NjQzMQ==", "bodyText": "What does size mean? Number of rows? Memory size?", "url": "https://github.com/apache/pinot/pull/5934#discussion_r481296431", "createdAt": "2020-09-01T16:59:30Z", "author": {"login": "mcvsubbu"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/Collector.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import java.util.Iterator;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * Collects and stores GenericRows\n+ */\n+public interface Collector {\n+\n+  /**\n+   * Collects the given GenericRow and stores it\n+   * @param genericRow the generic row to add to the collection\n+   */\n+  void collect(GenericRow genericRow);\n+\n+  /**\n+   * Provides an iterator for the GenericRows in the collection\n+   */\n+  Iterator<GenericRow> iterator();\n+\n+  /**\n+   * The size of the collection", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTI5Nzc0Mg==", "bodyText": "It is useful to add some comments on each of these, explaining what the collector does (or, what it does differently than others)", "url": "https://github.com/apache/pinot/pull/5934#discussion_r481297742", "createdAt": "2020-09-01T17:01:46Z", "author": {"login": "mcvsubbu"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/CollectorFactory.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import org.apache.pinot.spi.data.Schema;\n+\n+\n+/**\n+ * Factory for constructing a Collector from CollectorConfig\n+ */\n+public final class CollectorFactory {\n+\n+  private CollectorFactory() {\n+\n+  }\n+\n+  public enum CollectorType {\n+    ROLLUP, CONCAT", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTMwMDM3MA==", "bodyText": "So, we include virtual columns here?", "url": "https://github.com/apache/pinot/pull/5934#discussion_r481300370", "createdAt": "2020-09-01T17:06:28Z", "author": {"login": "mcvsubbu"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/RollupCollector.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.Map;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.MetricFieldSpec;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * A Collector that rolls up the incoming records on unique dimensions + time columns, based on provided aggregation types for metrics.\n+ * By default will use the SUM aggregation on metrics.\n+ */\n+public class RollupCollector implements Collector {\n+\n+  private final Map<Record, GenericRow> _collection = new HashMap<>();\n+\n+  private final int _keySize;\n+  private final int _valueSize;\n+  private final String[] _keyColumns;\n+  private final String[] _valueColumns;\n+  private final ValueAggregator[] _valueAggregators;\n+  private final MetricFieldSpec[] _metricFieldSpecs;\n+\n+  public RollupCollector(CollectorConfig collectorConfig, Schema schema) {\n+    _keySize = schema.getColumnNames().size() - schema.getMetricNames().size();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTMwMTQ4Mg==", "bodyText": "Are these the key parts? If so, can we name the member as _keyParts? _values gets confusing.", "url": "https://github.com/apache/pinot/pull/5934#discussion_r481301482", "createdAt": "2020-09-01T17:08:28Z", "author": {"login": "mcvsubbu"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/RollupCollector.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.Map;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.MetricFieldSpec;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * A Collector that rolls up the incoming records on unique dimensions + time columns, based on provided aggregation types for metrics.\n+ * By default will use the SUM aggregation on metrics.\n+ */\n+public class RollupCollector implements Collector {\n+\n+  private final Map<Record, GenericRow> _collection = new HashMap<>();\n+\n+  private final int _keySize;\n+  private final int _valueSize;\n+  private final String[] _keyColumns;\n+  private final String[] _valueColumns;\n+  private final ValueAggregator[] _valueAggregators;\n+  private final MetricFieldSpec[] _metricFieldSpecs;\n+\n+  public RollupCollector(CollectorConfig collectorConfig, Schema schema) {\n+    _keySize = schema.getColumnNames().size() - schema.getMetricNames().size();\n+    _valueSize = schema.getMetricNames().size();\n+    _keyColumns = new String[_keySize];\n+    _valueColumns = new String[_valueSize];\n+    _valueAggregators = new ValueAggregator[_valueSize];\n+    _metricFieldSpecs = new MetricFieldSpec[_valueSize];\n+\n+    Map<String, ValueAggregatorFactory.ValueAggregatorType> aggregatorTypeMap = collectorConfig.getAggregatorTypeMap();\n+    if (aggregatorTypeMap == null) {\n+      aggregatorTypeMap = Collections.emptyMap();\n+    }\n+    int valIdx = 0;\n+    int keyIdx = 0;\n+    for (FieldSpec fieldSpec : schema.getAllFieldSpecs()) {\n+      if (!fieldSpec.isVirtualColumn()) {\n+        String name = fieldSpec.getName();\n+        if (fieldSpec.getFieldType().equals(FieldSpec.FieldType.METRIC)) {\n+          _metricFieldSpecs[valIdx] = (MetricFieldSpec) fieldSpec;\n+          _valueColumns[valIdx] = name;\n+          _valueAggregators[valIdx] = ValueAggregatorFactory.getValueAggregator(\n+              aggregatorTypeMap.getOrDefault(name, ValueAggregatorFactory.ValueAggregatorType.SUM).toString());\n+          valIdx++;\n+        } else {\n+          _keyColumns[keyIdx++] = name;\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * If a row already exists in the collection (based on dimension + time columns), rollup the metric values, else add the row\n+   */\n+  @Override\n+  public void collect(GenericRow genericRow) {\n+    Object[] key = new Object[_keySize];\n+    for (int i = 0; i < _keySize; i++) {\n+      key[i] = genericRow.getValue(_keyColumns[i]);\n+    }\n+    Record keyRecord = new Record(key);\n+    GenericRow prev = _collection.get(keyRecord);\n+    if (prev == null) {\n+      _collection.put(keyRecord, genericRow);\n+    } else {\n+      for (int i = 0; i < _valueSize; i++) {\n+        String valueColumn = _valueColumns[i];\n+        Object aggregate = _valueAggregators[i]\n+            .aggregate(prev.getValue(valueColumn), genericRow.getValue(valueColumn), _metricFieldSpecs[i]);\n+        prev.putValue(valueColumn, aggregate);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public Iterator<GenericRow> iterator() {\n+    return _collection.values().iterator();\n+  }\n+\n+  @Override\n+  public int size() {\n+    return _collection.size();\n+  }\n+\n+  @Override\n+  public void reset() {\n+    _collection.clear();\n+  }\n+\n+  /**\n+   * A record representation for the keys of the record\n+   * Note that the dimensions can have multi-value columns, and hence the equals and hashCode need deep array operations\n+   */\n+  private static class Record {\n+    private final Object[] _values;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 120}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTMwNzA3NQ==", "bodyText": "Instead of checking the number of records, can we have a method in the collector that says it is full? The criteria could then be something else depending on the collector.", "url": "https://github.com/apache/pinot/pull/5934#discussion_r481307075", "createdAt": "2020-09-01T17:18:02Z", "author": {"login": "mcvsubbu"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentReducer.java", "diffHunk": "@@ -0,0 +1,151 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.framework;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import org.apache.avro.file.DataFileWriter;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.pinot.core.segment.processing.collector.Collector;\n+import org.apache.pinot.core.segment.processing.collector.CollectorFactory;\n+import org.apache.pinot.core.segment.processing.utils.SegmentProcessorUtils;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+import org.apache.pinot.spi.data.readers.RecordReader;\n+import org.apache.pinot.spi.data.readers.RecordReaderFactory;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Reducer phase of the SegmentProcessorFramework\n+ * Reads the avro files in the input directory and creates output avro files in the reducer output directory.\n+ * The avro files in the input directory are expected to contain data for only 1 partition\n+ * Performs operations on that partition data as follows:\n+ * - concatenation/rollup of records\n+ * - split\n+ * - TODO: dedup\n+ */\n+public class SegmentReducer {\n+\n+  private static final Logger LOGGER = LoggerFactory.getLogger(SegmentReducer.class);\n+  private static final int MAX_RECORDS_TO_COLLECT = 5_000_000;\n+\n+  private final File _reducerInputDir;\n+  private final File _reducerOutputDir;\n+\n+  private final String _reducerId;\n+  private final Schema _pinotSchema;\n+  private final org.apache.avro.Schema _avroSchema;\n+  private final Collector _collector;\n+  private final int _numRecordsPerPart;\n+\n+  public SegmentReducer(String reducerId, File reducerInputDir, SegmentReducerConfig reducerConfig,\n+      File reducerOutputDir) {\n+    _reducerInputDir = reducerInputDir;\n+    _reducerOutputDir = reducerOutputDir;\n+\n+    _reducerId = reducerId;\n+    _pinotSchema = reducerConfig.getPinotSchema();\n+    _avroSchema = SegmentProcessorUtils.convertPinotSchemaToAvroSchema(_pinotSchema);\n+    _collector = CollectorFactory.getCollector(reducerConfig.getCollectorConfig(), _pinotSchema);\n+    _numRecordsPerPart = reducerConfig.getNumRecordsPerPart();\n+    LOGGER.info(\"Initialized reducer with id: {}, input dir: {}, output dir: {}, collector: {}, numRecordsPerPart: {}\",\n+        _reducerId, _reducerInputDir, _reducerOutputDir, _collector.getClass(), _numRecordsPerPart);\n+  }\n+\n+  /**\n+   * Reads the avro files in the input directory.\n+   * Performs configured operations and outputs to other avro file(s) in the reducer output directory.\n+   */\n+  public void reduce()\n+      throws Exception {\n+\n+    int part = 0;\n+    for (File inputFile : _reducerInputDir.listFiles()) {\n+\n+      RecordReader avroRecordReader = RecordReaderFactory\n+          .getRecordReaderByClass(\"org.apache.pinot.plugin.inputformat.avro.AvroRecordReader\", inputFile,\n+              _pinotSchema.getColumnNames(), null);\n+\n+      while (avroRecordReader.hasNext()) {\n+        GenericRow next = avroRecordReader.next();\n+\n+        // Aggregations\n+        _collector.collect(next);\n+\n+        // Exceeded max records allowed to collect. Flush\n+        if (_collector.size() == MAX_RECORDS_TO_COLLECT) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ5NDc2MA=="}, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTMwODczMQ==", "bodyText": "Since these are all derived segments (merging m segments into n) should we not be using the table's partitioner all the time?", "url": "https://github.com/apache/pinot/pull/5934#discussion_r481308731", "createdAt": "2020-09-01T17:20:59Z", "author": {"login": "mcvsubbu"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/partitioner/PartitionerFactory.java", "diffHunk": "@@ -0,0 +1,82 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.partitioner;\n+\n+import com.google.common.base.Preconditions;\n+\n+\n+/**\n+ * Factory for Partitioner and PartitionFilter\n+ */\n+public final class PartitionerFactory {\n+\n+  private PartitionerFactory() {\n+\n+  }\n+\n+  public enum PartitionerType {\n+    NO_OP, ROW_HASH, COLUMN_VALUE, TRANSFORM_FUNCTION, TABLE_PARTITION_CONFIG\n+  }\n+\n+  /**\n+   * Construct a Partitioner using the PartitioningConfig\n+   */\n+  public static Partitioner getPartitioner(PartitioningConfig config) {\n+\n+    Partitioner partitioner = null;\n+    switch (config.getPartitionerType()) {\n+      case NO_OP:\n+        partitioner = new NoOpPartitioner();\n+        break;\n+      case ROW_HASH:\n+        Preconditions\n+            .checkState(config.getNumPartitions() > 0, \"Must provide numPartitions > 0 for ROW_HASH partitioner\");\n+        partitioner = new RowHashPartitioner(config.getNumPartitions());\n+        break;\n+      case COLUMN_VALUE:\n+        Preconditions.checkState(config.getColumnName() != null, \"Must provide columnName for COLUMN_VALUE partitioner\");\n+        partitioner = new ColumnValuePartitioner(config.getColumnName());\n+        break;\n+      case TRANSFORM_FUNCTION:\n+        Preconditions.checkState(config.getTransformFunction() != null,\n+            \"Must provide transformFunction for TRANSFORM_FUNCTION partitioner\");\n+        partitioner = new TransformFunctionPartitioner(config.getTransformFunction());\n+        break;\n+      case TABLE_PARTITION_CONFIG:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTMwOTk2NQ==", "bodyText": "These should go into avro plugins? Why introduce dependency on avro here?", "url": "https://github.com/apache/pinot/pull/5934#discussion_r481309965", "createdAt": "2020-09-01T17:23:13Z", "author": {"login": "mcvsubbu"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/utils/SegmentProcessorUtils.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.utils;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * Helper util methods for SegmentProcessorFramework\n+ */\n+public final class SegmentProcessorUtils {\n+\n+  private SegmentProcessorUtils() {\n+  }\n+\n+  /**\n+   * Convert a GenericRow to an avro GenericRecord\n+   */\n+  public static GenericData.Record convertGenericRowToAvroRecord(GenericRow genericRow,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1"}, "originalPosition": 41}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "2c642d623556517f4d66d54674dc5da1e546f8a3", "author": {"user": {"login": "npawar", "name": "Neha Pawar"}}, "url": "https://github.com/apache/pinot/commit/2c642d623556517f4d66d54674dc5da1e546f8a3", "committedDate": "2020-09-01T02:22:15Z", "message": "RecordFilter and remove PartitionFilter"}, "afterCommit": {"oid": "eece981149304287c751d3dd44be40f14bfcc7a1", "author": {"user": {"login": "npawar", "name": "Neha Pawar"}}, "url": "https://github.com/apache/pinot/commit/eece981149304287c751d3dd44be40f14bfcc7a1", "committedDate": "2020-08-27T22:39:20Z", "message": "Javadoc and imports"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cd9e39e6f4b31a858dbac2d8666ac050ba90c8c3", "author": {"user": {"login": "npawar", "name": "Neha Pawar"}}, "url": "https://github.com/apache/pinot/commit/cd9e39e6f4b31a858dbac2d8666ac050ba90c8c3", "committedDate": "2020-09-01T17:59:35Z", "message": "RecordFilter and remove PartitionFilter"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "41ac0a9e6fff9fdbc2543380f3541238272c4a21", "author": {"user": {"login": "npawar", "name": "Neha Pawar"}}, "url": "https://github.com/apache/pinot/commit/41ac0a9e6fff9fdbc2543380f3541238272c4a21", "committedDate": "2020-09-02T00:29:20Z", "message": "Sort in Collector"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "aa42f94b8e76dc18a19a1da1c90fab03ca18bf27", "author": {"user": {"login": "npawar", "name": "Neha Pawar"}}, "url": "https://github.com/apache/pinot/commit/aa42f94b8e76dc18a19a1da1c90fab03ca18bf27", "committedDate": "2020-09-02T01:35:05Z", "message": "Javadocs, rename, skip virtual columns"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83", "author": {"user": {"login": "npawar", "name": "Neha Pawar"}}, "url": "https://github.com/apache/pinot/commit/38a8accc19144087fb2b568d097554f826e80c83", "committedDate": "2020-09-04T22:59:48Z", "message": "RoundRobin partitioner and flush on maxRecordsPerPart"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg0NTY5MTAy", "url": "https://github.com/apache/pinot/pull/5934#pullrequestreview-484569102", "createdAt": "2020-09-08T23:56:54Z", "commit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "state": "APPROVED", "comments": {"totalCount": 19, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQyMzo1Njo1NFrOHOx0Qg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMTo0MzowNlrOHOzoBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI1ODMwNg==", "bodyText": "Recommend combining iterator() and finish() into one method because we always need to call finish() then iterator() (maybe remove finish() and move the sorting logic into iterator())", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485258306", "createdAt": "2020-09-08T23:56:54Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/Collector.java", "diffHunk": "@@ -0,0 +1,55 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import java.util.Iterator;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * Collects and stores GenericRows\n+ */\n+public interface Collector {\n+\n+  /**\n+   * Collects the given GenericRow and stores it\n+   * @param genericRow the generic row to add to the collection\n+   */\n+  void collect(GenericRow genericRow);\n+\n+  /**\n+   * Provides an iterator for the GenericRows in the collection\n+   */\n+  Iterator<GenericRow> iterator();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2MDY1MA==", "bodyText": "From the past experience, storing dataType and do per-value switch is faster than storing Comparator", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485260650", "createdAt": "2020-09-09T00:04:41Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/GenericRowSorter.java", "diffHunk": "@@ -0,0 +1,81 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import com.google.common.base.Preconditions;\n+import java.util.Comparator;\n+import java.util.List;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * A sorter for GenericRows\n+ */\n+public class GenericRowSorter {\n+\n+  private final Comparator<GenericRow> _genericRowComparator;\n+\n+  public GenericRowSorter(List<String> sortOrder, Schema schema) {\n+    int sortOrderSize = sortOrder.size();\n+    Comparator[] comparators = new Comparator[sortOrderSize];\n+    for (int i = 0; i < sortOrderSize; i++) {\n+      String column = sortOrder.get(i);\n+      FieldSpec fieldSpec = schema.getFieldSpecFor(column);\n+      Preconditions.checkState(fieldSpec.isSingleValueField(), \"Cannot use multi value column: %s for sorting\", column);\n+      comparators[i] = getComparator(fieldSpec.getDataType());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2MDg5NQ==", "bodyText": "In favor of this flavor for performance concern (avoid using function)", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485260895", "createdAt": "2020-09-09T00:05:26Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/GenericRowSorter.java", "diffHunk": "@@ -0,0 +1,81 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import com.google.common.base.Preconditions;\n+import java.util.Comparator;\n+import java.util.List;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * A sorter for GenericRows\n+ */\n+public class GenericRowSorter {\n+\n+  private final Comparator<GenericRow> _genericRowComparator;\n+\n+  public GenericRowSorter(List<String> sortOrder, Schema schema) {\n+    int sortOrderSize = sortOrder.size();\n+    Comparator[] comparators = new Comparator[sortOrderSize];\n+    for (int i = 0; i < sortOrderSize; i++) {\n+      String column = sortOrder.get(i);\n+      FieldSpec fieldSpec = schema.getFieldSpecFor(column);\n+      Preconditions.checkState(fieldSpec.isSingleValueField(), \"Cannot use multi value column: %s for sorting\", column);\n+      comparators[i] = getComparator(fieldSpec.getDataType());\n+    }\n+    _genericRowComparator = (o1, o2) -> {\n+      for (int i = 0; i < comparators.length; i++) {\n+        String column = sortOrder.get(i);\n+        int result = comparators[i].compare(o1.getValue(column), o2.getValue(column));\n+        if (result != 0) {\n+          return result;\n+        }\n+      }\n+      return 0;\n+    };\n+  }\n+\n+  private Comparator getComparator(FieldSpec.DataType dataType) {\n+    switch (dataType) {\n+\n+      case INT:\n+        return Comparator.comparingInt(o -> (int) o);\n+      case LONG:\n+        return Comparator.comparingLong(o -> (long) o);\n+      case FLOAT:\n+        return (o1, o2) -> Float.compare((float) o1, (float) o2);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2MTA4MQ==", "bodyText": "Add BYTES support ByteArray.compare()", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485261081", "createdAt": "2020-09-09T00:06:04Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/GenericRowSorter.java", "diffHunk": "@@ -0,0 +1,81 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import com.google.common.base.Preconditions;\n+import java.util.Comparator;\n+import java.util.List;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * A sorter for GenericRows\n+ */\n+public class GenericRowSorter {\n+\n+  private final Comparator<GenericRow> _genericRowComparator;\n+\n+  public GenericRowSorter(List<String> sortOrder, Schema schema) {\n+    int sortOrderSize = sortOrder.size();\n+    Comparator[] comparators = new Comparator[sortOrderSize];\n+    for (int i = 0; i < sortOrderSize; i++) {\n+      String column = sortOrder.get(i);\n+      FieldSpec fieldSpec = schema.getFieldSpecFor(column);\n+      Preconditions.checkState(fieldSpec.isSingleValueField(), \"Cannot use multi value column: %s for sorting\", column);\n+      comparators[i] = getComparator(fieldSpec.getDataType());\n+    }\n+    _genericRowComparator = (o1, o2) -> {\n+      for (int i = 0; i < comparators.length; i++) {\n+        String column = sortOrder.get(i);\n+        int result = comparators[i].compare(o1.getValue(column), o2.getValue(column));\n+        if (result != 0) {\n+          return result;\n+        }\n+      }\n+      return 0;\n+    };\n+  }\n+\n+  private Comparator getComparator(FieldSpec.DataType dataType) {\n+    switch (dataType) {\n+\n+      case INT:\n+        return Comparator.comparingInt(o -> (int) o);\n+      case LONG:\n+        return Comparator.comparingLong(o -> (long) o);\n+      case FLOAT:\n+        return (o1, o2) -> Float.compare((float) o1, (float) o2);\n+      case DOUBLE:\n+        return Comparator.comparingDouble(o -> (double) o);\n+      case STRING:\n+        return Comparator.comparing(o -> ((String) o));\n+      default:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2MTQxOA==", "bodyText": "(Code style)\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                private CollectorFactory.CollectorType collectorType = DEFAULT_COLLECTOR_TYPE;\n          \n          \n            \n                private CollectorFactory.CollectorType _collectorType = DEFAULT_COLLECTOR_TYPE;", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485261418", "createdAt": "2020-09-09T00:07:19Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/CollectorConfig.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import com.fasterxml.jackson.databind.annotation.JsonDeserialize;\n+import com.fasterxml.jackson.databind.annotation.JsonPOJOBuilder;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+\n+\n+/**\n+ * Config for Collector\n+ */\n+@JsonDeserialize(builder = CollectorConfig.Builder.class)\n+public class CollectorConfig {\n+  private static final CollectorFactory.CollectorType DEFAULT_COLLECTOR_TYPE = CollectorFactory.CollectorType.CONCAT;\n+\n+  private final CollectorFactory.CollectorType _collectorType;\n+  private final Map<String, ValueAggregatorFactory.ValueAggregatorType> _aggregatorTypeMap;\n+  private final List<String> _sortOrder;\n+\n+  private CollectorConfig(CollectorFactory.CollectorType collectorType,\n+      Map<String, ValueAggregatorFactory.ValueAggregatorType> aggregatorTypeMap, List<String> sortOrder) {\n+    _collectorType = collectorType;\n+    _aggregatorTypeMap = aggregatorTypeMap;\n+    _sortOrder = sortOrder;\n+  }\n+\n+  /**\n+   * The type of the Collector\n+   */\n+  public CollectorFactory.CollectorType getCollectorType() {\n+    return _collectorType;\n+  }\n+\n+  /**\n+   * Map containing aggregation types for the metrics\n+   */\n+  @Nullable\n+  public Map<String, ValueAggregatorFactory.ValueAggregatorType> getAggregatorTypeMap() {\n+    return _aggregatorTypeMap;\n+  }\n+\n+  /**\n+   * The columns on which to sort\n+   */\n+  public List<String> getSortOrder() {\n+    return _sortOrder;\n+  }\n+\n+  /**\n+   * Builder for CollectorConfig\n+   */\n+  @JsonPOJOBuilder(withPrefix = \"set\")\n+  public static class Builder {\n+    private CollectorFactory.CollectorType collectorType = DEFAULT_COLLECTOR_TYPE;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2MTQ1OQ==", "bodyText": "Avoid using this", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485261459", "createdAt": "2020-09-09T00:07:33Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/CollectorConfig.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import com.fasterxml.jackson.databind.annotation.JsonDeserialize;\n+import com.fasterxml.jackson.databind.annotation.JsonPOJOBuilder;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+\n+\n+/**\n+ * Config for Collector\n+ */\n+@JsonDeserialize(builder = CollectorConfig.Builder.class)\n+public class CollectorConfig {\n+  private static final CollectorFactory.CollectorType DEFAULT_COLLECTOR_TYPE = CollectorFactory.CollectorType.CONCAT;\n+\n+  private final CollectorFactory.CollectorType _collectorType;\n+  private final Map<String, ValueAggregatorFactory.ValueAggregatorType> _aggregatorTypeMap;\n+  private final List<String> _sortOrder;\n+\n+  private CollectorConfig(CollectorFactory.CollectorType collectorType,\n+      Map<String, ValueAggregatorFactory.ValueAggregatorType> aggregatorTypeMap, List<String> sortOrder) {\n+    _collectorType = collectorType;\n+    _aggregatorTypeMap = aggregatorTypeMap;\n+    _sortOrder = sortOrder;\n+  }\n+\n+  /**\n+   * The type of the Collector\n+   */\n+  public CollectorFactory.CollectorType getCollectorType() {\n+    return _collectorType;\n+  }\n+\n+  /**\n+   * Map containing aggregation types for the metrics\n+   */\n+  @Nullable\n+  public Map<String, ValueAggregatorFactory.ValueAggregatorType> getAggregatorTypeMap() {\n+    return _aggregatorTypeMap;\n+  }\n+\n+  /**\n+   * The columns on which to sort\n+   */\n+  public List<String> getSortOrder() {\n+    return _sortOrder;\n+  }\n+\n+  /**\n+   * Builder for CollectorConfig\n+   */\n+  @JsonPOJOBuilder(withPrefix = \"set\")\n+  public static class Builder {\n+    private CollectorFactory.CollectorType collectorType = DEFAULT_COLLECTOR_TYPE;\n+    private Map<String, ValueAggregatorFactory.ValueAggregatorType> aggregatorTypeMap;\n+    private List<String> sortOrder = new ArrayList<>();\n+\n+    public Builder setCollectorType(CollectorFactory.CollectorType collectorType) {\n+      this.collectorType = collectorType;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2MjQxOQ==", "bodyText": "Same for other places\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                private TableConfig tableConfig;\n          \n          \n            \n                private TableConfig _tableConfig;", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485262419", "createdAt": "2020-09-09T00:11:02Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentProcessorConfig.java", "diffHunk": "@@ -0,0 +1,181 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.framework;\n+\n+import com.google.common.base.Preconditions;\n+import org.apache.pinot.core.segment.processing.collector.CollectorConfig;\n+import org.apache.pinot.core.segment.processing.filter.RecordFilterConfig;\n+import org.apache.pinot.core.segment.processing.partitioner.PartitioningConfig;\n+import org.apache.pinot.core.segment.processing.transformer.RecordTransformerConfig;\n+import org.apache.pinot.spi.config.table.TableConfig;\n+import org.apache.pinot.spi.data.Schema;\n+\n+\n+/**\n+ * Config for configuring the phases of {@link SegmentProcessorFramework}\n+ */\n+public class SegmentProcessorConfig {\n+\n+  private final TableConfig _tableConfig;\n+  private final Schema _schema;\n+  private final RecordTransformerConfig _recordTransformerConfig;\n+  private final RecordFilterConfig _recordFilterConfig;\n+  private final PartitioningConfig _partitioningConfig;\n+  private final CollectorConfig _collectorConfig;\n+  private final SegmentConfig _segmentConfig;\n+\n+  private SegmentProcessorConfig(TableConfig tableConfig, Schema schema,\n+      RecordTransformerConfig recordTransformerConfig, RecordFilterConfig recordFilterConfig,\n+      PartitioningConfig partitioningConfig, CollectorConfig collectorConfig, SegmentConfig segmentConfig) {\n+    _tableConfig = tableConfig;\n+    _schema = schema;\n+    _recordTransformerConfig = recordTransformerConfig;\n+    _recordFilterConfig = recordFilterConfig;\n+    _partitioningConfig = partitioningConfig;\n+    _collectorConfig = collectorConfig;\n+    _segmentConfig = segmentConfig;\n+  }\n+\n+  /**\n+   * The Pinot table config\n+   */\n+  public TableConfig getTableConfig() {\n+    return _tableConfig;\n+  }\n+\n+  /**\n+   * The Pinot schema\n+   */\n+  public Schema getSchema() {\n+    return _schema;\n+  }\n+\n+  /**\n+   * The RecordTransformerConfig for the SegmentProcessorFramework's map phase\n+   */\n+  public RecordTransformerConfig getRecordTransformerConfig() {\n+    return _recordTransformerConfig;\n+  }\n+\n+  /**\n+   * The RecordFilterConfig to filter records\n+   */\n+  public RecordFilterConfig getRecordFilterConfig() {\n+    return _recordFilterConfig;\n+  }\n+\n+  /**\n+   * The PartitioningConfig for the SegmentProcessorFramework's map phase\n+   */\n+  public PartitioningConfig getPartitioningConfig() {\n+    return _partitioningConfig;\n+  }\n+\n+  /**\n+   * The CollectorConfig for the SegmentProcessorFramework's reduce phase\n+   */\n+  public CollectorConfig getCollectorConfig() {\n+    return _collectorConfig;\n+  }\n+\n+  /**\n+   * The SegmentConfig for the SegmentProcessorFramework's segment generation phase\n+   */\n+  public SegmentConfig getSegmentConfig() {\n+    return _segmentConfig;\n+  }\n+\n+  /**\n+   * Builder for SegmentProcessorConfig\n+   */\n+  public static class Builder {\n+    private TableConfig tableConfig;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 108}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2MjY0Ng==", "bodyText": "Same for other places\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  this.tableConfig = tableConfig;\n          \n          \n            \n                  _tableConfig = tableConfig;", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485262646", "createdAt": "2020-09-09T00:11:47Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentProcessorConfig.java", "diffHunk": "@@ -0,0 +1,181 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.framework;\n+\n+import com.google.common.base.Preconditions;\n+import org.apache.pinot.core.segment.processing.collector.CollectorConfig;\n+import org.apache.pinot.core.segment.processing.filter.RecordFilterConfig;\n+import org.apache.pinot.core.segment.processing.partitioner.PartitioningConfig;\n+import org.apache.pinot.core.segment.processing.transformer.RecordTransformerConfig;\n+import org.apache.pinot.spi.config.table.TableConfig;\n+import org.apache.pinot.spi.data.Schema;\n+\n+\n+/**\n+ * Config for configuring the phases of {@link SegmentProcessorFramework}\n+ */\n+public class SegmentProcessorConfig {\n+\n+  private final TableConfig _tableConfig;\n+  private final Schema _schema;\n+  private final RecordTransformerConfig _recordTransformerConfig;\n+  private final RecordFilterConfig _recordFilterConfig;\n+  private final PartitioningConfig _partitioningConfig;\n+  private final CollectorConfig _collectorConfig;\n+  private final SegmentConfig _segmentConfig;\n+\n+  private SegmentProcessorConfig(TableConfig tableConfig, Schema schema,\n+      RecordTransformerConfig recordTransformerConfig, RecordFilterConfig recordFilterConfig,\n+      PartitioningConfig partitioningConfig, CollectorConfig collectorConfig, SegmentConfig segmentConfig) {\n+    _tableConfig = tableConfig;\n+    _schema = schema;\n+    _recordTransformerConfig = recordTransformerConfig;\n+    _recordFilterConfig = recordFilterConfig;\n+    _partitioningConfig = partitioningConfig;\n+    _collectorConfig = collectorConfig;\n+    _segmentConfig = segmentConfig;\n+  }\n+\n+  /**\n+   * The Pinot table config\n+   */\n+  public TableConfig getTableConfig() {\n+    return _tableConfig;\n+  }\n+\n+  /**\n+   * The Pinot schema\n+   */\n+  public Schema getSchema() {\n+    return _schema;\n+  }\n+\n+  /**\n+   * The RecordTransformerConfig for the SegmentProcessorFramework's map phase\n+   */\n+  public RecordTransformerConfig getRecordTransformerConfig() {\n+    return _recordTransformerConfig;\n+  }\n+\n+  /**\n+   * The RecordFilterConfig to filter records\n+   */\n+  public RecordFilterConfig getRecordFilterConfig() {\n+    return _recordFilterConfig;\n+  }\n+\n+  /**\n+   * The PartitioningConfig for the SegmentProcessorFramework's map phase\n+   */\n+  public PartitioningConfig getPartitioningConfig() {\n+    return _partitioningConfig;\n+  }\n+\n+  /**\n+   * The CollectorConfig for the SegmentProcessorFramework's reduce phase\n+   */\n+  public CollectorConfig getCollectorConfig() {\n+    return _collectorConfig;\n+  }\n+\n+  /**\n+   * The SegmentConfig for the SegmentProcessorFramework's segment generation phase\n+   */\n+  public SegmentConfig getSegmentConfig() {\n+    return _segmentConfig;\n+  }\n+\n+  /**\n+   * Builder for SegmentProcessorConfig\n+   */\n+  public static class Builder {\n+    private TableConfig tableConfig;\n+    private Schema schema;\n+    private RecordTransformerConfig recordTransformerConfig;\n+    private RecordFilterConfig recordFilterConfig;\n+    private PartitioningConfig partitioningConfig;\n+    private CollectorConfig collectorConfig;\n+    private SegmentConfig _segmentConfig;\n+\n+    public Builder setTableConfig(TableConfig tableConfig) {\n+      this.tableConfig = tableConfig;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 117}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2NjIyMA==", "bodyText": "I feel this is not as readable as the JsonCreator annotation on the constructor. IMO We don't really need a builder for very simple config", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485266220", "createdAt": "2020-09-09T00:24:47Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/CollectorConfig.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import com.fasterxml.jackson.databind.annotation.JsonDeserialize;\n+import com.fasterxml.jackson.databind.annotation.JsonPOJOBuilder;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+\n+\n+/**\n+ * Config for Collector\n+ */\n+@JsonDeserialize(builder = CollectorConfig.Builder.class)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2NjkwMA==", "bodyText": "(nit) Make it final?", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485266900", "createdAt": "2020-09-09T00:27:18Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/ConcatCollector.java", "diffHunk": "@@ -0,0 +1,71 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * A Collector implementation for collecting and concatenating all incoming rows\n+ */\n+public class ConcatCollector implements Collector {\n+  private final List<GenericRow> _collection = new ArrayList<>();\n+  private Iterator<GenericRow> _iterator;\n+  private GenericRowSorter _sorter;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2NjkyOQ==", "bodyText": "(nit) Make it final?", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485266929", "createdAt": "2020-09-09T00:27:26Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/RollupCollector.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.MetricFieldSpec;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * A Collector that rolls up the incoming records on unique dimensions + time columns, based on provided aggregation types for metrics.\n+ * By default will use the SUM aggregation on metrics.\n+ */\n+public class RollupCollector implements Collector {\n+\n+  private final Map<Record, GenericRow> _collection = new HashMap<>();\n+  private Iterator<GenericRow> _iterator;\n+  private GenericRowSorter _sorter;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2NzkyNQ==", "bodyText": "You might want to extract number of columns from field specs as metric column can also be virtual", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485267925", "createdAt": "2020-09-09T00:31:01Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/RollupCollector.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.MetricFieldSpec;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * A Collector that rolls up the incoming records on unique dimensions + time columns, based on provided aggregation types for metrics.\n+ * By default will use the SUM aggregation on metrics.\n+ */\n+public class RollupCollector implements Collector {\n+\n+  private final Map<Record, GenericRow> _collection = new HashMap<>();\n+  private Iterator<GenericRow> _iterator;\n+  private GenericRowSorter _sorter;\n+\n+  private final int _keySize;\n+  private final int _valueSize;\n+  private final String[] _keyColumns;\n+  private final String[] _valueColumns;\n+  private final ValueAggregator[] _valueAggregators;\n+  private final MetricFieldSpec[] _metricFieldSpecs;\n+\n+  public RollupCollector(CollectorConfig collectorConfig, Schema schema) {\n+    _keySize = schema.getPhysicalColumnNames().size() - schema.getMetricNames().size();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2ODI4Mg==", "bodyText": "(nit)\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    if (fieldSpec.getFieldType().equals(FieldSpec.FieldType.METRIC)) {\n          \n          \n            \n                    if (fieldSpec.getFieldType() == FieldSpec.FieldType.METRIC) {", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485268282", "createdAt": "2020-09-09T00:32:21Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/RollupCollector.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.MetricFieldSpec;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * A Collector that rolls up the incoming records on unique dimensions + time columns, based on provided aggregation types for metrics.\n+ * By default will use the SUM aggregation on metrics.\n+ */\n+public class RollupCollector implements Collector {\n+\n+  private final Map<Record, GenericRow> _collection = new HashMap<>();\n+  private Iterator<GenericRow> _iterator;\n+  private GenericRowSorter _sorter;\n+\n+  private final int _keySize;\n+  private final int _valueSize;\n+  private final String[] _keyColumns;\n+  private final String[] _valueColumns;\n+  private final ValueAggregator[] _valueAggregators;\n+  private final MetricFieldSpec[] _metricFieldSpecs;\n+\n+  public RollupCollector(CollectorConfig collectorConfig, Schema schema) {\n+    _keySize = schema.getPhysicalColumnNames().size() - schema.getMetricNames().size();\n+    _valueSize = schema.getMetricNames().size();\n+    _keyColumns = new String[_keySize];\n+    _valueColumns = new String[_valueSize];\n+    _valueAggregators = new ValueAggregator[_valueSize];\n+    _metricFieldSpecs = new MetricFieldSpec[_valueSize];\n+\n+    Map<String, ValueAggregatorFactory.ValueAggregatorType> aggregatorTypeMap = collectorConfig.getAggregatorTypeMap();\n+    if (aggregatorTypeMap == null) {\n+      aggregatorTypeMap = Collections.emptyMap();\n+    }\n+    int valIdx = 0;\n+    int keyIdx = 0;\n+    for (FieldSpec fieldSpec : schema.getAllFieldSpecs()) {\n+      if (!fieldSpec.isVirtualColumn()) {\n+        String name = fieldSpec.getName();\n+        if (fieldSpec.getFieldType().equals(FieldSpec.FieldType.METRIC)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2OTM2Nw==", "bodyText": "Not introduced in this PR, but we should not pass in MetricFieldSpec for every aggregate() call. Instead, we should set it in constructor or add an init() method", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485269367", "createdAt": "2020-09-09T00:36:25Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/ValueAggregator.java", "diffHunk": "@@ -16,7 +16,7 @@\n  * specific language governing permissions and limitations\n  * under the License.\n  */\n-package org.apache.pinot.core.minion.rollup.aggregate;\n+package org.apache.pinot.core.segment.processing.collector;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2OTc4Mw==", "bodyText": "Should this be nullable as well?", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485269783", "createdAt": "2020-09-09T00:37:58Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/CollectorConfig.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import com.fasterxml.jackson.databind.annotation.JsonDeserialize;\n+import com.fasterxml.jackson.databind.annotation.JsonPOJOBuilder;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+\n+\n+/**\n+ * Config for Collector\n+ */\n+@JsonDeserialize(builder = CollectorConfig.Builder.class)\n+public class CollectorConfig {\n+  private static final CollectorFactory.CollectorType DEFAULT_COLLECTOR_TYPE = CollectorFactory.CollectorType.CONCAT;\n+\n+  private final CollectorFactory.CollectorType _collectorType;\n+  private final Map<String, ValueAggregatorFactory.ValueAggregatorType> _aggregatorTypeMap;\n+  private final List<String> _sortOrder;\n+\n+  private CollectorConfig(CollectorFactory.CollectorType collectorType,\n+      Map<String, ValueAggregatorFactory.ValueAggregatorType> aggregatorTypeMap, List<String> sortOrder) {\n+    _collectorType = collectorType;\n+    _aggregatorTypeMap = aggregatorTypeMap;\n+    _sortOrder = sortOrder;\n+  }\n+\n+  /**\n+   * The type of the Collector\n+   */\n+  public CollectorFactory.CollectorType getCollectorType() {\n+    return _collectorType;\n+  }\n+\n+  /**\n+   * Map containing aggregation types for the metrics\n+   */\n+  @Nullable\n+  public Map<String, ValueAggregatorFactory.ValueAggregatorType> getAggregatorTypeMap() {\n+    return _aggregatorTypeMap;\n+  }\n+\n+  /**\n+   * The columns on which to sort\n+   */\n+  public List<String> getSortOrder() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI3MjUwMQ==", "bodyText": "Should we work on GenericRecord (Avro object) instead of GenericRow (Pinot object)? We are converting them back and forth right now", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485272501", "createdAt": "2020-09-09T00:48:17Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/Collector.java", "diffHunk": "@@ -0,0 +1,55 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import java.util.Iterator;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * Collects and stores GenericRows\n+ */\n+public interface Collector {\n+\n+  /**\n+   * Collects the given GenericRow and stores it\n+   * @param genericRow the generic row to add to the collection\n+   */\n+  void collect(GenericRow genericRow);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI4NTIwOA==", "bodyText": "Use _partitionToDataFileWriterMap.get(partition) and check if the value is null to save one map lookup", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485285208", "createdAt": "2020-09-09T01:35:14Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentMapper.java", "diffHunk": "@@ -0,0 +1,135 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.framework;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.avro.Schema;\n+import org.apache.avro.file.DataFileWriter;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.pinot.core.data.readers.PinotSegmentRecordReader;\n+import org.apache.pinot.core.segment.processing.filter.RecordFilter;\n+import org.apache.pinot.core.segment.processing.filter.RecordFilterFactory;\n+import org.apache.pinot.core.segment.processing.partitioner.Partitioner;\n+import org.apache.pinot.core.segment.processing.partitioner.PartitionerFactory;\n+import org.apache.pinot.core.segment.processing.transformer.RecordTransformer;\n+import org.apache.pinot.core.segment.processing.transformer.RecordTransformerFactory;\n+import org.apache.pinot.core.segment.processing.utils.SegmentProcessorUtils;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Mapper phase of the SegmentProcessorFramework.\n+ * Reads the input segment and creates partitioned avro data files\n+ * Performs:\n+ * - record transformations\n+ * - partitioning\n+ * - partition filtering\n+ */\n+public class SegmentMapper {\n+\n+  private static final Logger LOGGER = LoggerFactory.getLogger(SegmentMapper.class);\n+  private final File _inputSegment;\n+  private final File _mapperOutputDir;\n+\n+  private final String _mapperId;\n+  private final Schema _avroSchema;\n+  private final RecordTransformer _recordTransformer;\n+  private final RecordFilter _recordFilter;\n+  private final Partitioner _partitioner;\n+  private final Map<String, DataFileWriter<GenericData.Record>> _partitionToDataFileWriterMap = new HashMap<>();\n+\n+  public SegmentMapper(String mapperId, File inputSegment, SegmentMapperConfig mapperConfig, File mapperOutputDir) {\n+    _inputSegment = inputSegment;\n+    _mapperOutputDir = mapperOutputDir;\n+\n+    _mapperId = mapperId;\n+    _avroSchema = SegmentProcessorUtils.convertPinotSchemaToAvroSchema(mapperConfig.getPinotSchema());\n+    _recordTransformer = RecordTransformerFactory.getRecordTransformer(mapperConfig.getRecordTransformerConfig());\n+    _recordFilter = RecordFilterFactory.getRecordFilter(mapperConfig.getRecordFilterConfig());\n+    _partitioner = PartitionerFactory.getPartitioner(mapperConfig.getPartitioningConfig());\n+    LOGGER.info(\n+        \"Initialized mapper with id: {}, input segment: {}, output dir: {}, recordTransformer: {}, recordFilter: {}, partitioner: {}\",\n+        _mapperId, _inputSegment, _mapperOutputDir, _recordTransformer.getClass(), _recordFilter.getClass(),\n+        _partitioner.getClass());\n+  }\n+\n+  /**\n+   * Reads the input segment and generates partitioned avro data files into the mapper output directory\n+   * Records for each partition are put into a directory of its own withing the mapper output directory, identified by the partition name\n+   */\n+  public void map()\n+      throws Exception {\n+\n+    PinotSegmentRecordReader segmentRecordReader = new PinotSegmentRecordReader(_inputSegment);\n+    GenericRow reusableRow = new GenericRow();\n+    GenericData.Record reusableRecord = new GenericData.Record(_avroSchema);\n+\n+    while (segmentRecordReader.hasNext()) {\n+      reusableRow = segmentRecordReader.next(reusableRow);\n+\n+      // Record transformation\n+      reusableRow = _recordTransformer.transformRecord(reusableRow);\n+\n+      // Record filtering\n+      if (_recordFilter.filter(reusableRow)) {\n+        continue;\n+      }\n+\n+      // Partitioning\n+      String partition = _partitioner.getPartition(reusableRow);\n+\n+      // Create writer for the partition, if not exists\n+      if (!_partitionToDataFileWriterMap.containsKey(partition)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI4Njc0Mg==", "bodyText": "Suggest leaving them as null if not set", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485286742", "createdAt": "2020-09-09T01:38:46Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentProcessorConfig.java", "diffHunk": "@@ -0,0 +1,181 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.framework;\n+\n+import com.google.common.base.Preconditions;\n+import org.apache.pinot.core.segment.processing.collector.CollectorConfig;\n+import org.apache.pinot.core.segment.processing.filter.RecordFilterConfig;\n+import org.apache.pinot.core.segment.processing.partitioner.PartitioningConfig;\n+import org.apache.pinot.core.segment.processing.transformer.RecordTransformerConfig;\n+import org.apache.pinot.spi.config.table.TableConfig;\n+import org.apache.pinot.spi.data.Schema;\n+\n+\n+/**\n+ * Config for configuring the phases of {@link SegmentProcessorFramework}\n+ */\n+public class SegmentProcessorConfig {\n+\n+  private final TableConfig _tableConfig;\n+  private final Schema _schema;\n+  private final RecordTransformerConfig _recordTransformerConfig;\n+  private final RecordFilterConfig _recordFilterConfig;\n+  private final PartitioningConfig _partitioningConfig;\n+  private final CollectorConfig _collectorConfig;\n+  private final SegmentConfig _segmentConfig;\n+\n+  private SegmentProcessorConfig(TableConfig tableConfig, Schema schema,\n+      RecordTransformerConfig recordTransformerConfig, RecordFilterConfig recordFilterConfig,\n+      PartitioningConfig partitioningConfig, CollectorConfig collectorConfig, SegmentConfig segmentConfig) {\n+    _tableConfig = tableConfig;\n+    _schema = schema;\n+    _recordTransformerConfig = recordTransformerConfig;\n+    _recordFilterConfig = recordFilterConfig;\n+    _partitioningConfig = partitioningConfig;\n+    _collectorConfig = collectorConfig;\n+    _segmentConfig = segmentConfig;\n+  }\n+\n+  /**\n+   * The Pinot table config\n+   */\n+  public TableConfig getTableConfig() {\n+    return _tableConfig;\n+  }\n+\n+  /**\n+   * The Pinot schema\n+   */\n+  public Schema getSchema() {\n+    return _schema;\n+  }\n+\n+  /**\n+   * The RecordTransformerConfig for the SegmentProcessorFramework's map phase\n+   */\n+  public RecordTransformerConfig getRecordTransformerConfig() {\n+    return _recordTransformerConfig;\n+  }\n+\n+  /**\n+   * The RecordFilterConfig to filter records\n+   */\n+  public RecordFilterConfig getRecordFilterConfig() {\n+    return _recordFilterConfig;\n+  }\n+\n+  /**\n+   * The PartitioningConfig for the SegmentProcessorFramework's map phase\n+   */\n+  public PartitioningConfig getPartitioningConfig() {\n+    return _partitioningConfig;\n+  }\n+\n+  /**\n+   * The CollectorConfig for the SegmentProcessorFramework's reduce phase\n+   */\n+  public CollectorConfig getCollectorConfig() {\n+    return _collectorConfig;\n+  }\n+\n+  /**\n+   * The SegmentConfig for the SegmentProcessorFramework's segment generation phase\n+   */\n+  public SegmentConfig getSegmentConfig() {\n+    return _segmentConfig;\n+  }\n+\n+  /**\n+   * Builder for SegmentProcessorConfig\n+   */\n+  public static class Builder {\n+    private TableConfig tableConfig;\n+    private Schema schema;\n+    private RecordTransformerConfig recordTransformerConfig;\n+    private RecordFilterConfig recordFilterConfig;\n+    private PartitioningConfig partitioningConfig;\n+    private CollectorConfig collectorConfig;\n+    private SegmentConfig _segmentConfig;\n+\n+    public Builder setTableConfig(TableConfig tableConfig) {\n+      this.tableConfig = tableConfig;\n+      return this;\n+    }\n+\n+    public Builder setSchema(Schema schema) {\n+      this.schema = schema;\n+      return this;\n+    }\n+\n+    public Builder setRecordTransformerConfig(RecordTransformerConfig recordTransformerConfig) {\n+      this.recordTransformerConfig = recordTransformerConfig;\n+      return this;\n+    }\n+\n+    public Builder setRecordFilterConfig(RecordFilterConfig recordFilterConfig) {\n+      this.recordFilterConfig = recordFilterConfig;\n+      return this;\n+    }\n+\n+    public Builder setPartitioningConfig(PartitioningConfig partitioningConfig) {\n+      this.partitioningConfig = partitioningConfig;\n+      return this;\n+    }\n+\n+    public Builder setCollectorConfig(CollectorConfig collectorConfig) {\n+      this.collectorConfig = collectorConfig;\n+      return this;\n+    }\n+\n+    public Builder setSegmentConfig(SegmentConfig segmentConfig) {\n+      this._segmentConfig = segmentConfig;\n+      return this;\n+    }\n+\n+    public SegmentProcessorConfig build() {\n+      Preconditions.checkNotNull(tableConfig, \"Must provide table config in SegmentProcessorConfig\");\n+      Preconditions.checkNotNull(schema, \"Must provide schema in SegmentProcessorConfig\");\n+      if (recordTransformerConfig == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 154}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI4Nzk0MQ==", "bodyText": "Put null values from GenericRow.getNullValueFields()?", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485287941", "createdAt": "2020-09-09T01:43:06Z", "author": {"login": "Jackie-Jiang"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/utils/SegmentProcessorUtils.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.utils;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * Helper util methods for SegmentProcessorFramework\n+ */\n+public final class SegmentProcessorUtils {\n+\n+  private SegmentProcessorUtils() {\n+  }\n+\n+  /**\n+   * Convert a GenericRow to an avro GenericRecord\n+   */\n+  public static GenericData.Record convertGenericRowToAvroRecord(GenericRow genericRow,\n+      GenericData.Record reusableRecord) {\n+    for (String field : genericRow.getFieldToValueMap().keySet()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 43}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg1NDYwMzEz", "url": "https://github.com/apache/pinot/pull/5934#pullrequestreview-485460313", "createdAt": "2020-09-09T23:05:10Z", "commit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQyMzowNToxMFrOHPdMlg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQwMTozMTozMVrOHPfsmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTk2OTA0Ng==", "bodyText": "I guess that we basically keep the entire data for a segment on JVM heap?\nIn the future, we may need to add the off-heap or file-based collector to avoid OOM error when reading large segments. (e.g. 1-2gb Pinot segment can be extremely large in row format)\nAnother way to save memory is to sort the data on all dimensions and scan at once for aggregation (but this paying a large cost for cases when the data doesn't need to be sorted)", "url": "https://github.com/apache/pinot/pull/5934#discussion_r485969046", "createdAt": "2020-09-09T23:05:10Z", "author": {"login": "snleee"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/collector/RollupCollector.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.collector;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.MetricFieldSpec;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * A Collector that rolls up the incoming records on unique dimensions + time columns, based on provided aggregation types for metrics.\n+ * By default will use the SUM aggregation on metrics.\n+ */\n+public class RollupCollector implements Collector {\n+\n+  private final Map<Record, GenericRow> _collection = new HashMap<>();\n+  private Iterator<GenericRow> _iterator;\n+  private GenericRowSorter _sorter;\n+\n+  private final int _keySize;\n+  private final int _valueSize;\n+  private final String[] _keyColumns;\n+  private final String[] _valueColumns;\n+  private final ValueAggregator[] _valueAggregators;\n+  private final MetricFieldSpec[] _metricFieldSpecs;\n+\n+  public RollupCollector(CollectorConfig collectorConfig, Schema schema) {\n+    _keySize = schema.getPhysicalColumnNames().size() - schema.getMetricNames().size();\n+    _valueSize = schema.getMetricNames().size();\n+    _keyColumns = new String[_keySize];\n+    _valueColumns = new String[_valueSize];\n+    _valueAggregators = new ValueAggregator[_valueSize];\n+    _metricFieldSpecs = new MetricFieldSpec[_valueSize];\n+\n+    Map<String, ValueAggregatorFactory.ValueAggregatorType> aggregatorTypeMap = collectorConfig.getAggregatorTypeMap();\n+    if (aggregatorTypeMap == null) {\n+      aggregatorTypeMap = Collections.emptyMap();\n+    }\n+    int valIdx = 0;\n+    int keyIdx = 0;\n+    for (FieldSpec fieldSpec : schema.getAllFieldSpecs()) {\n+      if (!fieldSpec.isVirtualColumn()) {\n+        String name = fieldSpec.getName();\n+        if (fieldSpec.getFieldType().equals(FieldSpec.FieldType.METRIC)) {\n+          _metricFieldSpecs[valIdx] = (MetricFieldSpec) fieldSpec;\n+          _valueColumns[valIdx] = name;\n+          _valueAggregators[valIdx] = ValueAggregatorFactory.getValueAggregator(\n+              aggregatorTypeMap.getOrDefault(name, ValueAggregatorFactory.ValueAggregatorType.SUM).toString());\n+          valIdx++;\n+        } else {\n+          _keyColumns[keyIdx++] = name;\n+        }\n+      }\n+    }\n+\n+    List<String> sortOrder = collectorConfig.getSortOrder();\n+    if (sortOrder.size() > 0) {\n+      _sorter = new GenericRowSorter(sortOrder, schema);\n+    }\n+  }\n+\n+  /**\n+   * If a row already exists in the collection (based on dimension + time columns), rollup the metric values, else add the row\n+   */\n+  @Override\n+  public void collect(GenericRow genericRow) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjAwNTUwNQ==", "bodyText": "No-op partitioner means that we always create a single output file?", "url": "https://github.com/apache/pinot/pull/5934#discussion_r486005505", "createdAt": "2020-09-10T01:14:37Z", "author": {"login": "snleee"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/partitioner/NoOpPartitioner.java", "diffHunk": "@@ -0,0 +1,32 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.partitioner;\n+\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * Partitioner implementation which always returns constant partition value \"0\"\n+ */\n+public class NoOpPartitioner implements Partitioner {\n+  @Override\n+  public String getPartition(GenericRow genericRow) {\n+    return \"0\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjAwNjI3Mw==", "bodyText": "Is this intended for supporting time alignment?\nWhat if the time column granularity is in seconds/hours while push frequency is DAY?\nIn that case, we may need to use TransformationPartitioner?", "url": "https://github.com/apache/pinot/pull/5934#discussion_r486006273", "createdAt": "2020-09-10T01:17:32Z", "author": {"login": "snleee"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/partitioner/ColumnValuePartitioner.java", "diffHunk": "@@ -0,0 +1,39 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.partitioner;\n+\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * Partitioner which extracts a column value as the partition\n+ */\n+public class ColumnValuePartitioner implements Partitioner {\n+\n+  private final String _columnName;\n+\n+  public ColumnValuePartitioner(String columnName) {\n+    _columnName = columnName;\n+  }\n+\n+  @Override\n+  public String getPartition(GenericRow genericRow) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjAwNjc1NA==", "bodyText": "What if I need to align data on time while the table is custom partitioned? (just trying to brainstorm how we will extend the current partitioner to support this)\nThen, we can probably add the new partitioner that combines the value from TableConfigPartitioner and TransformationPartitioner?\ne.g.  partition on memberId using murmur, need to enable segment merge so the data needs to be time aligned.\n\nUse table config partitioner to get the partition id based on murmur on memberId -> Let's day 2\nUse time align partitioner -> Let's say 2020/12/12\n\nCombine 1&2 -> 2020/12/12-2 <- example of partitionId\nWe can do something like the above?", "url": "https://github.com/apache/pinot/pull/5934#discussion_r486006754", "createdAt": "2020-09-10T01:19:28Z", "author": {"login": "snleee"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/partitioner/TableConfigPartitioner.java", "diffHunk": "@@ -0,0 +1,45 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.partitioner;\n+\n+import org.apache.pinot.core.data.partition.PartitionFunction;\n+import org.apache.pinot.core.data.partition.PartitionFunctionFactory;\n+import org.apache.pinot.spi.config.table.ColumnPartitionConfig;\n+import org.apache.pinot.spi.data.readers.GenericRow;\n+\n+\n+/**\n+ * Partitioner which computes partition values based on the ColumnPartitionConfig from the table config\n+ */\n+public class TableConfigPartitioner implements Partitioner {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjAwOTEwNg==", "bodyText": "One requirement for SegmentMergeRollup is to be able to put the custom name for the segment name (or at least need to put the prefix and the sequenced merged_XXX_0...M Where do you think it's the best place to configure those?\nYour segment framework also faces the same issue with the sequence id. So, the sequence id should be handled implicitly by the framework at least. And, we can probably add the config for the prefix.", "url": "https://github.com/apache/pinot/pull/5934#discussion_r486009106", "createdAt": "2020-09-10T01:28:08Z", "author": {"login": "snleee"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentProcessorConfig.java", "diffHunk": "@@ -0,0 +1,181 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.framework;\n+\n+import com.google.common.base.Preconditions;\n+import org.apache.pinot.core.segment.processing.collector.CollectorConfig;\n+import org.apache.pinot.core.segment.processing.filter.RecordFilterConfig;\n+import org.apache.pinot.core.segment.processing.partitioner.PartitioningConfig;\n+import org.apache.pinot.core.segment.processing.transformer.RecordTransformerConfig;\n+import org.apache.pinot.spi.config.table.TableConfig;\n+import org.apache.pinot.spi.data.Schema;\n+\n+\n+/**\n+ * Config for configuring the phases of {@link SegmentProcessorFramework}\n+ */\n+public class SegmentProcessorConfig {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjAxMDAwOQ==", "bodyText": "Did you check the output segment names when the output is more than 1 files?\nIt's possible that the final segments may have the same segment name. (e.g. <tablename>_<start>_<end>)", "url": "https://github.com/apache/pinot/pull/5934#discussion_r486010009", "createdAt": "2020-09-10T01:31:31Z", "author": {"login": "snleee"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/processing/framework/SegmentProcessorFramework.java", "diffHunk": "@@ -0,0 +1,194 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.processing.framework;\n+\n+import com.google.common.base.Preconditions;\n+import java.io.File;\n+import java.util.Arrays;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.common.utils.TarGzCompressionUtils;\n+import org.apache.pinot.core.indexsegment.generator.SegmentGeneratorConfig;\n+import org.apache.pinot.core.segment.creator.impl.SegmentIndexCreationDriverImpl;\n+import org.apache.pinot.spi.config.table.TableConfig;\n+import org.apache.pinot.spi.data.Schema;\n+import org.apache.pinot.spi.data.readers.FileFormat;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * A framework to process \"m\" given segments and convert them into \"n\" segments\n+ * The phases of the Segment Processor are\n+ * 1. Map - record transformation, partitioning, partition filtering\n+ * 2. Reduce - rollup, concat, split etc\n+ * 3. Segment generation\n+ *\n+ * This will typically be used by minion tasks, which want to perform some processing on segments\n+ * (eg task which merges segments, tasks which aligns segments per time boundaries etc)\n+ */\n+public class SegmentProcessorFramework {\n+\n+  private static final Logger LOGGER = LoggerFactory.getLogger(SegmentProcessorFramework.class);\n+\n+  private final File _inputSegmentsDir;\n+  private final File _outputSegmentsDir;\n+  private final SegmentProcessorConfig _segmentProcessorConfig;\n+\n+  private final Schema _pinotSchema;\n+  private final TableConfig _tableConfig;\n+\n+  private final File _baseDir;\n+  private final File _mapperInputDir;\n+  private final File _mapperOutputDir;\n+  private final File _reducerOutputDir;\n+\n+  /**\n+   * Initializes the Segment Processor framework with input segments, output path and processing config\n+   * @param inputSegmentsDir directory containing the input segments. These can be tarred or untarred.\n+   * @param segmentProcessorConfig config for segment processing\n+   * @param outputSegmentsDir directory for placing the resulting segments. This should already exist.\n+   */\n+  public SegmentProcessorFramework(File inputSegmentsDir, SegmentProcessorConfig segmentProcessorConfig,\n+      File outputSegmentsDir) {\n+\n+    LOGGER.info(\n+        \"Initializing SegmentProcessorFramework with input segments dir: {}, output segments dir: {} and segment processor config: {}\",\n+        inputSegmentsDir.getAbsolutePath(), outputSegmentsDir.getAbsolutePath(), segmentProcessorConfig.toString());\n+\n+    _inputSegmentsDir = inputSegmentsDir;\n+    Preconditions.checkState(_inputSegmentsDir.exists() && _inputSegmentsDir.isDirectory(),\n+        \"Input path: %s must be a directory with Pinot segments\", _inputSegmentsDir.getAbsolutePath());\n+    _outputSegmentsDir = outputSegmentsDir;\n+    Preconditions.checkState(\n+        _outputSegmentsDir.exists() && _outputSegmentsDir.isDirectory() && (_outputSegmentsDir.list().length == 0),\n+        \"Must provide existing empty output directory: %s\", _outputSegmentsDir.getAbsolutePath());\n+\n+    _segmentProcessorConfig = segmentProcessorConfig;\n+    _pinotSchema = segmentProcessorConfig.getSchema();\n+    _tableConfig = segmentProcessorConfig.getTableConfig();\n+\n+    _baseDir = new File(FileUtils.getTempDirectory(), \"segment_processor_\" + System.currentTimeMillis());\n+    FileUtils.deleteQuietly(_baseDir);\n+    Preconditions.checkState(_baseDir.mkdirs(), \"Failed to create base directory: %s for SegmentProcessor\", _baseDir);\n+    _mapperInputDir = new File(_baseDir, \"mapper_input\");\n+    Preconditions\n+        .checkState(_mapperInputDir.mkdirs(), \"Failed to create mapper input directory: %s for SegmentProcessor\",\n+            _mapperInputDir);\n+    _mapperOutputDir = new File(_baseDir, \"mapper_output\");\n+    Preconditions\n+        .checkState(_mapperOutputDir.mkdirs(), \"Failed to create mapper output directory: %s for SegmentProcessor\",\n+            _mapperOutputDir);\n+    _reducerOutputDir = new File(_baseDir, \"reducer_output\");\n+    Preconditions\n+        .checkState(_reducerOutputDir.mkdirs(), \"Failed to create reducer output directory: %s for SegmentProcessor\",\n+            _reducerOutputDir);\n+  }\n+\n+  /**\n+   * Processes segments from the input directory as per the provided configs, then puts resulting segments into the output directory\n+   */\n+  public void processSegments()\n+      throws Exception {\n+\n+    // Check for input segments\n+    File[] segmentFiles = _inputSegmentsDir.listFiles();\n+    if (segmentFiles.length == 0) {\n+      throw new IllegalStateException(\"No segments found in input dir: \" + _inputSegmentsDir.getAbsolutePath()\n+          + \". Exiting SegmentProcessorFramework.\");\n+    }\n+\n+    // Mapper phase.\n+    LOGGER.info(\"Beginning mapper phase. Processing segments: {}\", Arrays.toString(_inputSegmentsDir.list()));\n+    for (File segment : segmentFiles) {\n+\n+      String fileName = segment.getName();\n+      File mapperInput = segment;\n+\n+      // Untar the segments if needed\n+      if (!segment.isDirectory()) {\n+        if (fileName.endsWith(\".tar.gz\") || fileName.endsWith(\".tgz\")) {\n+          mapperInput = TarGzCompressionUtils.untar(segment, _mapperInputDir).get(0);\n+        } else {\n+          throw new IllegalStateException(\"Unsupported segment format: \" + segment.getAbsolutePath());\n+        }\n+      }\n+\n+      // Set mapperId as the name of the segment\n+      SegmentMapperConfig mapperConfig =\n+          new SegmentMapperConfig(_pinotSchema, _segmentProcessorConfig.getRecordTransformerConfig(),\n+              _segmentProcessorConfig.getRecordFilterConfig(), _segmentProcessorConfig.getPartitioningConfig());\n+      SegmentMapper mapper = new SegmentMapper(mapperInput.getName(), mapperInput, mapperConfig, _mapperOutputDir);\n+      mapper.map();\n+      mapper.cleanup();\n+    }\n+\n+    // Check for mapper output files\n+    File[] mapperOutputFiles = _mapperOutputDir.listFiles();\n+    if (mapperOutputFiles.length == 0) {\n+      throw new IllegalStateException(\"No files found in mapper output directory: \" + _mapperOutputDir.getAbsolutePath()\n+          + \". Exiting SegmentProcessorFramework.\");\n+    }\n+\n+    // Reducer phase.\n+    LOGGER.info(\"Beginning reducer phase. Processing files: {}\", Arrays.toString(_mapperOutputDir.list()));\n+    // Mapper output directory has 1 directory per partition, named after the partition. Each directory contains 1 or more avro files.\n+    for (File partDir : mapperOutputFiles) {\n+\n+      // Set partition as reducerId\n+      SegmentReducerConfig reducerConfig =\n+          new SegmentReducerConfig(_pinotSchema, _segmentProcessorConfig.getCollectorConfig(),\n+              _segmentProcessorConfig.getSegmentConfig().getMaxNumRecordsPerSegment());\n+      SegmentReducer reducer = new SegmentReducer(partDir.getName(), partDir, reducerConfig, _reducerOutputDir);\n+      reducer.reduce();\n+      reducer.cleanup();\n+    }\n+\n+    // Check for reducer output files\n+    File[] reducerOutputFiles = _reducerOutputDir.listFiles();\n+    if (reducerOutputFiles.length == 0) {\n+      throw new IllegalStateException(\n+          \"No files found in reducer output directory: \" + _reducerOutputDir.getAbsolutePath()\n+              + \". Exiting SegmentProcessorFramework.\");\n+    }\n+\n+    // Segment generation phase.\n+    LOGGER.info(\"Beginning segment generation phase. Processing files: {}\", Arrays.toString(_reducerOutputDir.list()));\n+    // Reducer output directory will have 1 or more avro files\n+    for (File resultFile : reducerOutputFiles) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38a8accc19144087fb2b568d097554f826e80c83"}, "originalPosition": 173}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0250c7c6fdc30e3d5f258c68c526c9e7023b08da", "author": {"user": {"login": "npawar", "name": "Neha Pawar"}}, "url": "https://github.com/apache/pinot/commit/0250c7c6fdc30e3d5f258c68c526c9e7023b08da", "committedDate": "2020-09-10T21:53:02Z", "message": "Review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2e2dabc860977772db6c44c477dc530cd21910f8", "author": {"user": {"login": "npawar", "name": "Neha Pawar"}}, "url": "https://github.com/apache/pinot/commit/2e2dabc860977772db6c44c477dc530cd21910f8", "committedDate": "2020-09-10T22:37:11Z", "message": "Use JsonCreator instead of JsonDeserialize"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8ce9e5df9a9f33bdde54d03d1dc88e3526773a90", "author": {"user": {"login": "npawar", "name": "Neha Pawar"}}, "url": "https://github.com/apache/pinot/commit/8ce9e5df9a9f33bdde54d03d1dc88e3526773a90", "committedDate": "2020-09-15T01:03:52Z", "message": "End-end test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d433fd09da426261c78260cb41b1c847986c22d7", "author": {"user": {"login": "npawar", "name": "Neha Pawar"}}, "url": "https://github.com/apache/pinot/commit/d433fd09da426261c78260cb41b1c847986c22d7", "committedDate": "2020-09-15T01:13:37Z", "message": "Add TODOs for open items"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg4ODU0NzA3", "url": "https://github.com/apache/pinot/pull/5934#pullrequestreview-488854707", "createdAt": "2020-09-15T16:21:48Z", "commit": {"oid": "d433fd09da426261c78260cb41b1c847986c22d7"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 293, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}