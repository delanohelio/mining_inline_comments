{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzYzMTEzOTAz", "number": 4987, "title": "Pinot batch ingestion hadoop", "bodyText": "refactor common codes\nadding support to run pinot batch ingestion job in hadoop\npackage plugins directory and untar them in hadoop mapper/ spark executor.\nChange PluginManager to class loader to not use system class loader", "createdAt": "2020-01-15T12:30:18Z", "url": "https://github.com/apache/pinot/pull/4987", "merged": true, "mergeCommit": {"oid": "f990656ee442973d311157bfca993db6d8218874"}, "closed": true, "closedAt": "2020-01-17T09:35:43Z", "author": {"login": "xiangfu0"}, "timelineItems": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABb6w6G9gBqjI5NTI5OTU0MDQ=", "endCursor": "Y3Vyc29yOnYyOpPPAAABb7EHCVABqjI5NTY1ODIyNzY=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "8cbf1d2298787305817c8ebd8776703db5d949b8", "author": {"user": {"login": "xiangfu0", "name": "Xiang Fu"}}, "url": "https://github.com/apache/pinot/commit/8cbf1d2298787305817c8ebd8776703db5d949b8", "committedDate": "2020-01-15T12:29:23Z", "message": "Adding pinot-batch-ingestion-hadoop module"}, "afterCommit": {"oid": "632e21efb3745dcd5ab0e6907311c9d69dafd752", "author": {"user": {"login": "xiangfu0", "name": "Xiang Fu"}}, "url": "https://github.com/apache/pinot/commit/632e21efb3745dcd5ab0e6907311c9d69dafd752", "committedDate": "2020-01-16T02:41:18Z", "message": "Adding pinot-batch-ingestion-hadoop module"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "632e21efb3745dcd5ab0e6907311c9d69dafd752", "author": {"user": {"login": "xiangfu0", "name": "Xiang Fu"}}, "url": "https://github.com/apache/pinot/commit/632e21efb3745dcd5ab0e6907311c9d69dafd752", "committedDate": "2020-01-16T02:41:18Z", "message": "Adding pinot-batch-ingestion-hadoop module"}, "afterCommit": {"oid": "c77c5d7c517a5b032246d01dbc38f994da69503f", "author": {"user": {"login": "xiangfu0", "name": "Xiang Fu"}}, "url": "https://github.com/apache/pinot/commit/c77c5d7c517a5b032246d01dbc38f994da69503f", "committedDate": "2020-01-16T05:15:20Z", "message": "Adding pinot-batch-ingestion-hadoop module"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c77c5d7c517a5b032246d01dbc38f994da69503f", "author": {"user": {"login": "xiangfu0", "name": "Xiang Fu"}}, "url": "https://github.com/apache/pinot/commit/c77c5d7c517a5b032246d01dbc38f994da69503f", "committedDate": "2020-01-16T05:15:20Z", "message": "Adding pinot-batch-ingestion-hadoop module"}, "afterCommit": {"oid": "70e42a0ef4acc2146eb8dd5d08a7aac1c5a688c0", "author": {"user": {"login": "xiangfu0", "name": "Xiang Fu"}}, "url": "https://github.com/apache/pinot/commit/70e42a0ef4acc2146eb8dd5d08a7aac1c5a688c0", "committedDate": "2020-01-16T07:13:41Z", "message": "Adding pinot-batch-ingestion-hadoop module"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "70e42a0ef4acc2146eb8dd5d08a7aac1c5a688c0", "author": {"user": {"login": "xiangfu0", "name": "Xiang Fu"}}, "url": "https://github.com/apache/pinot/commit/70e42a0ef4acc2146eb8dd5d08a7aac1c5a688c0", "committedDate": "2020-01-16T07:13:41Z", "message": "Adding pinot-batch-ingestion-hadoop module"}, "afterCommit": {"oid": "f3ea3ca93371461c81fb98394d8eb24519a0820d", "author": {"user": {"login": "xiangfu0", "name": "Xiang Fu"}}, "url": "https://github.com/apache/pinot/commit/f3ea3ca93371461c81fb98394d8eb24519a0820d", "committedDate": "2020-01-16T08:02:34Z", "message": "Adding pinot-batch-ingestion-hadoop module"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f3ea3ca93371461c81fb98394d8eb24519a0820d", "author": {"user": {"login": "xiangfu0", "name": "Xiang Fu"}}, "url": "https://github.com/apache/pinot/commit/f3ea3ca93371461c81fb98394d8eb24519a0820d", "committedDate": "2020-01-16T08:02:34Z", "message": "Adding pinot-batch-ingestion-hadoop module"}, "afterCommit": {"oid": "e14bd751f736c048a32270fc0f53eb266ad10c1c", "author": {"user": {"login": "xiangfu0", "name": "Xiang Fu"}}, "url": "https://github.com/apache/pinot/commit/e14bd751f736c048a32270fc0f53eb266ad10c1c", "committedDate": "2020-01-16T08:05:36Z", "message": "Adding pinot-batch-ingestion-hadoop module"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MjY4OTEx", "url": "https://github.com/apache/pinot/pull/4987#pullrequestreview-344268911", "createdAt": "2020-01-16T22:50:27Z", "commit": {"oid": "e14bd751f736c048a32270fc0f53eb266ad10c1c"}, "state": "APPROVED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMjo1MDoyN1rOFeqDug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMzowNjo1OFrOFeqXQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY5MDY4Mg==", "bodyText": "what does this do and any reason this is need for hadoop module", "url": "https://github.com/apache/pinot/pull/4987#discussion_r367690682", "createdAt": "2020-01-16T22:50:27Z", "author": {"login": "kishoreg"}, "path": "pinot-plugins/pinot-batch-ingestion/pinot-batch-ingestion-hadoop/pom.xml", "diffHunk": "@@ -0,0 +1,141 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<!--\n+\n+    Licensed to the Apache Software Foundation (ASF) under one\n+    or more contributor license agreements.  See the NOTICE file\n+    distributed with this work for additional information\n+    regarding copyright ownership.  The ASF licenses this file\n+    to you under the Apache License, Version 2.0 (the\n+    \"License\"); you may not use this file except in compliance\n+    with the License.  You may obtain a copy of the License at\n+\n+      http://www.apache.org/licenses/LICENSE-2.0\n+\n+    Unless required by applicable law or agreed to in writing,\n+    software distributed under the License is distributed on an\n+    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+    KIND, either express or implied.  See the License for the\n+    specific language governing permissions and limitations\n+    under the License.\n+\n+-->\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n+         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n+         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+  <parent>\n+    <artifactId>pinot-batch-ingestion</artifactId>\n+    <groupId>org.apache.pinot</groupId>\n+    <version>0.3.0-SNAPSHOT</version>\n+    <relativePath>..</relativePath>\n+  </parent>\n+  <modelVersion>4.0.0</modelVersion>\n+  <artifactId>pinot-batch-ingestion-hadoop</artifactId>\n+  <name>Pinot Batch Ingestion for Hadoop</name>\n+  <url>https://pinot.apache.org/</url>\n+  <properties>\n+    <pinot.root>${basedir}/../../..</pinot.root>\n+    <hadoop.version>2.7.0</hadoop.version>\n+  </properties>\n+  <dependencies>\n+    <dependency>\n+      <groupId>org.apache.pinot</groupId>\n+      <artifactId>pinot-batch-ingestion-common</artifactId>\n+      <version>${project.version}</version>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.yaml</groupId>\n+      <artifactId>snakeyaml</artifactId>\n+    </dependency>\n+\n+    <dependency>\n+      <groupId>org.apache.hadoop</groupId>\n+      <artifactId>hadoop-common</artifactId>\n+      <version>${hadoop.version}</version>\n+      <scope>provided</scope>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.apache.hadoop</groupId>\n+      <artifactId>hadoop-annotations</artifactId>\n+      <version>${hadoop.version}</version>\n+      <scope>provided</scope>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.apache.hadoop</groupId>\n+      <artifactId>hadoop-mapreduce-client-common</artifactId>\n+      <version>${hadoop.version}</version>\n+      <scope>provided</scope>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.apache.hadoop</groupId>\n+      <artifactId>hadoop-mapreduce-client-core</artifactId>\n+      <version>${hadoop.version}</version>\n+      <scope>provided</scope>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.apache.hadoop</groupId>\n+      <artifactId>hadoop-client</artifactId>\n+      <version>${hadoop.version}</version>\n+      <scope>provided</scope>\n+      <exclusions>\n+        <exclusion>\n+          <groupId>commons-logging</groupId>\n+          <artifactId>commons-logging</artifactId>\n+        </exclusion>\n+        <exclusion>\n+          <groupId>commons-lang</groupId>\n+          <artifactId>commons-lang</artifactId>\n+        </exclusion>\n+        <exclusion>\n+          <groupId>org.apache.commons</groupId>\n+          <artifactId>commons-math3</artifactId>\n+        </exclusion>\n+      </exclusions>\n+    </dependency>\n+    <dependency>\n+      <groupId>commons-logging</groupId>\n+      <artifactId>commons-logging</artifactId>\n+    </dependency>\n+    <dependency>\n+      <groupId>commons-lang</groupId>\n+      <artifactId>commons-lang</artifactId>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.apache.commons</groupId>\n+      <artifactId>commons-math3</artifactId>\n+      <scope>provided</scope>\n+    </dependency>\n+    <dependency>\n+      <groupId>com.google.guava</groupId>\n+      <artifactId>guava</artifactId>\n+    </dependency>\n+    <dependency>\n+      <groupId>com.fasterxml.jackson.core</groupId>\n+      <artifactId>jackson-core</artifactId>\n+    </dependency>\n+    <dependency>\n+      <groupId>com.fasterxml.jackson.core</groupId>\n+      <artifactId>jackson-databind</artifactId>\n+    </dependency>\n+    <dependency>\n+      <groupId>com.fasterxml.jackson.core</groupId>\n+      <artifactId>jackson-annotations</artifactId>\n+    </dependency>\n+  </dependencies>\n+\n+  <build>\n+    <plugins>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e14bd751f736c048a32270fc0f53eb266ad10c1c"}, "originalPosition": 126}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY5MTEyNw==", "bodyText": "I thought we enabled enforcer plugin for all modules", "url": "https://github.com/apache/pinot/pull/4987#discussion_r367691127", "createdAt": "2020-01-16T22:51:55Z", "author": {"login": "kishoreg"}, "path": "pinot-plugins/pinot-batch-ingestion/pinot-batch-ingestion-hadoop/pom.xml", "diffHunk": "@@ -0,0 +1,141 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<!--\n+\n+    Licensed to the Apache Software Foundation (ASF) under one\n+    or more contributor license agreements.  See the NOTICE file\n+    distributed with this work for additional information\n+    regarding copyright ownership.  The ASF licenses this file\n+    to you under the Apache License, Version 2.0 (the\n+    \"License\"); you may not use this file except in compliance\n+    with the License.  You may obtain a copy of the License at\n+\n+      http://www.apache.org/licenses/LICENSE-2.0\n+\n+    Unless required by applicable law or agreed to in writing,\n+    software distributed under the License is distributed on an\n+    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+    KIND, either express or implied.  See the License for the\n+    specific language governing permissions and limitations\n+    under the License.\n+\n+-->\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n+         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n+         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+  <parent>\n+    <artifactId>pinot-batch-ingestion</artifactId>\n+    <groupId>org.apache.pinot</groupId>\n+    <version>0.3.0-SNAPSHOT</version>\n+    <relativePath>..</relativePath>\n+  </parent>\n+  <modelVersion>4.0.0</modelVersion>\n+  <artifactId>pinot-batch-ingestion-hadoop</artifactId>\n+  <name>Pinot Batch Ingestion for Hadoop</name>\n+  <url>https://pinot.apache.org/</url>\n+  <properties>\n+    <pinot.root>${basedir}/../../..</pinot.root>\n+    <hadoop.version>2.7.0</hadoop.version>\n+  </properties>\n+  <dependencies>\n+    <dependency>\n+      <groupId>org.apache.pinot</groupId>\n+      <artifactId>pinot-batch-ingestion-common</artifactId>\n+      <version>${project.version}</version>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.yaml</groupId>\n+      <artifactId>snakeyaml</artifactId>\n+    </dependency>\n+\n+    <dependency>\n+      <groupId>org.apache.hadoop</groupId>\n+      <artifactId>hadoop-common</artifactId>\n+      <version>${hadoop.version}</version>\n+      <scope>provided</scope>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.apache.hadoop</groupId>\n+      <artifactId>hadoop-annotations</artifactId>\n+      <version>${hadoop.version}</version>\n+      <scope>provided</scope>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.apache.hadoop</groupId>\n+      <artifactId>hadoop-mapreduce-client-common</artifactId>\n+      <version>${hadoop.version}</version>\n+      <scope>provided</scope>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.apache.hadoop</groupId>\n+      <artifactId>hadoop-mapreduce-client-core</artifactId>\n+      <version>${hadoop.version}</version>\n+      <scope>provided</scope>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.apache.hadoop</groupId>\n+      <artifactId>hadoop-client</artifactId>\n+      <version>${hadoop.version}</version>\n+      <scope>provided</scope>\n+      <exclusions>\n+        <exclusion>\n+          <groupId>commons-logging</groupId>\n+          <artifactId>commons-logging</artifactId>\n+        </exclusion>\n+        <exclusion>\n+          <groupId>commons-lang</groupId>\n+          <artifactId>commons-lang</artifactId>\n+        </exclusion>\n+        <exclusion>\n+          <groupId>org.apache.commons</groupId>\n+          <artifactId>commons-math3</artifactId>\n+        </exclusion>\n+      </exclusions>\n+    </dependency>\n+    <dependency>\n+      <groupId>commons-logging</groupId>\n+      <artifactId>commons-logging</artifactId>\n+    </dependency>\n+    <dependency>\n+      <groupId>commons-lang</groupId>\n+      <artifactId>commons-lang</artifactId>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.apache.commons</groupId>\n+      <artifactId>commons-math3</artifactId>\n+      <scope>provided</scope>\n+    </dependency>\n+    <dependency>\n+      <groupId>com.google.guava</groupId>\n+      <artifactId>guava</artifactId>\n+    </dependency>\n+    <dependency>\n+      <groupId>com.fasterxml.jackson.core</groupId>\n+      <artifactId>jackson-core</artifactId>\n+    </dependency>\n+    <dependency>\n+      <groupId>com.fasterxml.jackson.core</groupId>\n+      <artifactId>jackson-databind</artifactId>\n+    </dependency>\n+    <dependency>\n+      <groupId>com.fasterxml.jackson.core</groupId>\n+      <artifactId>jackson-annotations</artifactId>\n+    </dependency>\n+  </dependencies>\n+\n+  <build>\n+    <plugins>\n+      <plugin>\n+        <groupId>org.apache.maven.plugins</groupId>\n+        <artifactId>maven-jar-plugin</artifactId>\n+        <version>2.5</version>\n+        <configuration>\n+          <forceCreation>true</forceCreation>\n+        </configuration>\n+      </plugin>\n+      <plugin>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e14bd751f736c048a32270fc0f53eb266ad10c1c"}, "originalPosition": 135}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY5NTUxOA==", "bodyText": "Add the logic here and some explanation on why we are compressing the plugins directory and shipping it to mapper via distributed cache.", "url": "https://github.com/apache/pinot/pull/4987#discussion_r367695518", "createdAt": "2020-01-16T23:06:23Z", "author": {"login": "kishoreg"}, "path": "pinot-plugins/pinot-batch-ingestion/pinot-batch-ingestion-hadoop/src/main/java/org/apache/pinot/plugin/ingestion/batch/hadoop/HadoopSegmentGenerationJobRunner.java", "diffHunk": "@@ -0,0 +1,313 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.plugin.ingestion.batch.hadoop;\n+\n+import com.google.common.base.Preconditions;\n+import java.io.DataOutputStream;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.net.URI;\n+import java.nio.file.FileSystems;\n+import java.nio.file.PathMatcher;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import org.apache.commons.configuration.Configuration;\n+import org.apache.commons.configuration.MapConfiguration;\n+import org.apache.hadoop.conf.Configured;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.Mapper;\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n+import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\n+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n+import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;\n+import org.apache.pinot.common.utils.StringUtil;\n+import org.apache.pinot.common.utils.TarGzCompressionUtils;\n+import org.apache.pinot.plugin.ingestion.batch.common.SegmentGenerationUtils;\n+import org.apache.pinot.spi.filesystem.PinotFS;\n+import org.apache.pinot.spi.filesystem.PinotFSFactory;\n+import org.apache.pinot.spi.ingestion.batch.runner.IngestionJobRunner;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotClusterSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotFSSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentGenerationJobSpec;\n+import org.apache.pinot.spi.plugin.PluginManager;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.yaml.snakeyaml.Yaml;\n+\n+import static org.apache.pinot.plugin.ingestion.batch.common.SegmentGenerationUtils.PINOT_PLUGINS_TAR_GZ;\n+import static org.apache.pinot.spi.plugin.PluginManager.PLUGINS_INCLUDE_PROPERTY_NAME;\n+\n+\n+public class HadoopSegmentGenerationJobRunner extends Configured implements IngestionJobRunner, Serializable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e14bd751f736c048a32270fc0f53eb266ad10c1c"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY5NTY4Mg==", "bodyText": "check for the existence of this directory.", "url": "https://github.com/apache/pinot/pull/4987#discussion_r367695682", "createdAt": "2020-01-16T23:06:58Z", "author": {"login": "kishoreg"}, "path": "pinot-plugins/pinot-batch-ingestion/pinot-batch-ingestion-hadoop/src/main/java/org/apache/pinot/plugin/ingestion/batch/hadoop/HadoopSegmentGenerationJobRunner.java", "diffHunk": "@@ -0,0 +1,313 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.plugin.ingestion.batch.hadoop;\n+\n+import com.google.common.base.Preconditions;\n+import java.io.DataOutputStream;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.net.URI;\n+import java.nio.file.FileSystems;\n+import java.nio.file.PathMatcher;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import org.apache.commons.configuration.Configuration;\n+import org.apache.commons.configuration.MapConfiguration;\n+import org.apache.hadoop.conf.Configured;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.Mapper;\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n+import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\n+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n+import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;\n+import org.apache.pinot.common.utils.StringUtil;\n+import org.apache.pinot.common.utils.TarGzCompressionUtils;\n+import org.apache.pinot.plugin.ingestion.batch.common.SegmentGenerationUtils;\n+import org.apache.pinot.spi.filesystem.PinotFS;\n+import org.apache.pinot.spi.filesystem.PinotFSFactory;\n+import org.apache.pinot.spi.ingestion.batch.runner.IngestionJobRunner;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotClusterSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotFSSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentGenerationJobSpec;\n+import org.apache.pinot.spi.plugin.PluginManager;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.yaml.snakeyaml.Yaml;\n+\n+import static org.apache.pinot.plugin.ingestion.batch.common.SegmentGenerationUtils.PINOT_PLUGINS_TAR_GZ;\n+import static org.apache.pinot.spi.plugin.PluginManager.PLUGINS_INCLUDE_PROPERTY_NAME;\n+\n+\n+public class HadoopSegmentGenerationJobRunner extends Configured implements IngestionJobRunner, Serializable {\n+\n+  public static final String SEGMENT_GENERATION_JOB_SPEC = \"segmentGenerationJobSpec\";\n+  private static final Logger LOGGER = LoggerFactory.getLogger(HadoopSegmentGenerationJobRunner.class);\n+  private static final String DEPS_JAR_DIR = \"dependencyJarDir\";\n+  private static final String STAGING_DIR = \"stagingDir\";\n+  private static final String SEGMENT_TAR_DIR = \"segmentTar\";\n+\n+  private SegmentGenerationJobSpec _spec;\n+\n+  public HadoopSegmentGenerationJobRunner() {\n+    setConf(new org.apache.hadoop.conf.Configuration());\n+    getConf().set(\"mapreduce.job.user.classpath.first\", \"true\");\n+  }\n+\n+  public HadoopSegmentGenerationJobRunner(SegmentGenerationJobSpec spec) {\n+    this();\n+    init(spec);\n+  }\n+\n+  @Override\n+  public void init(SegmentGenerationJobSpec spec) {\n+    _spec = spec;\n+    if (_spec.getInputDirURI() == null) {\n+      throw new RuntimeException(\"Missing property 'inputDirURI' in 'jobSpec' file\");\n+    }\n+    if (_spec.getOutputDirURI() == null) {\n+      throw new RuntimeException(\"Missing property 'outputDirURI' in 'jobSpec' file\");\n+    }\n+    if (_spec.getRecordReaderSpec() == null) {\n+      throw new RuntimeException(\"Missing property 'recordReaderSpec' in 'jobSpec' file\");\n+    }\n+    if (_spec.getTableSpec() == null) {\n+      throw new RuntimeException(\"Missing property 'tableSpec' in 'jobSpec' file\");\n+    }\n+    if (_spec.getTableSpec().getTableName() == null) {\n+      throw new RuntimeException(\"Missing property 'tableName' in 'tableSpec'\");\n+    }\n+    if (_spec.getTableSpec().getSchemaURI() == null) {\n+      if (_spec.getPinotClusterSpecs() == null || _spec.getPinotClusterSpecs().length == 0) {\n+        throw new RuntimeException(\"Missing property 'schemaURI' in 'tableSpec'\");\n+      }\n+      PinotClusterSpec pinotClusterSpec = _spec.getPinotClusterSpecs()[0];\n+      String schemaURI = SegmentGenerationUtils\n+          .generateSchemaURI(pinotClusterSpec.getControllerURI(), _spec.getTableSpec().getTableName());\n+      _spec.getTableSpec().setSchemaURI(schemaURI);\n+    }\n+    if (_spec.getTableSpec().getTableConfigURI() == null) {\n+      if (_spec.getPinotClusterSpecs() == null || _spec.getPinotClusterSpecs().length == 0) {\n+        throw new RuntimeException(\"Missing property 'tableConfigURI' in 'tableSpec'\");\n+      }\n+      PinotClusterSpec pinotClusterSpec = _spec.getPinotClusterSpecs()[0];\n+      String tableConfigURI = SegmentGenerationUtils\n+          .generateTableConfigURI(pinotClusterSpec.getControllerURI(), _spec.getTableSpec().getTableName());\n+      _spec.getTableSpec().setTableConfigURI(tableConfigURI);\n+    }\n+    if (_spec.getExecutionFrameworkSpec().getExtraConfigs() == null) {\n+      _spec.getExecutionFrameworkSpec().setExtraConfigs(new HashMap<>());\n+    }\n+  }\n+\n+  @Override\n+  public void run()\n+      throws Exception {\n+    //init all file systems\n+    List<PinotFSSpec> pinotFSSpecs = _spec.getPinotFSSpecs();\n+    for (PinotFSSpec pinotFSSpec : pinotFSSpecs) {\n+      Configuration config = new MapConfiguration(pinotFSSpec.getConfigs());\n+      PinotFSFactory.register(pinotFSSpec.getScheme(), pinotFSSpec.getClassName(), config);\n+    }\n+\n+    //Get pinotFS for input\n+    URI inputDirURI = new URI(_spec.getInputDirURI());\n+    if (inputDirURI.getScheme() == null) {\n+      inputDirURI = new File(_spec.getInputDirURI()).toURI();\n+    }\n+    PinotFS inputDirFS = PinotFSFactory.create(inputDirURI.getScheme());\n+\n+    //Get outputFS for writing output pinot segments\n+    URI outputDirURI = new URI(_spec.getOutputDirURI());\n+    if (outputDirURI.getScheme() == null) {\n+      outputDirURI = new File(_spec.getOutputDirURI()).toURI();\n+    }\n+    PinotFS outputDirFS = PinotFSFactory.create(outputDirURI.getScheme());\n+    outputDirFS.mkdir(outputDirURI);\n+\n+    //Get staging directory for temporary output pinot segments\n+    String stagingDir = _spec.getExecutionFrameworkSpec().getExtraConfigs().get(STAGING_DIR);\n+    Preconditions.checkNotNull(stagingDir, \"Please set config: stagingDir under 'executionFrameworkSpec.extraConfigs'\");\n+    URI stagingDirURI = URI.create(stagingDir);\n+    if (stagingDirURI.getScheme() == null) {\n+      stagingDirURI = new File(stagingDir).toURI();\n+    }\n+    if (!outputDirURI.getScheme().equals(stagingDirURI.getScheme())) {\n+      throw new RuntimeException(String\n+          .format(\"The scheme of staging directory URI [%s] and output directory URI [%s] has to be same.\",\n+              stagingDirURI, outputDirURI));\n+    }\n+    outputDirFS.mkdir(stagingDirURI);\n+    Path stagingInputDir = new Path(stagingDirURI.toString(), \"input\");\n+    outputDirFS.mkdir(stagingInputDir.toUri());\n+    Path stagingSegmentTarUri = new Path(stagingDirURI.toString(), SEGMENT_TAR_DIR);\n+    outputDirFS.mkdir(stagingSegmentTarUri.toUri());\n+\n+    //Get list of files to process\n+    String[] files = inputDirFS.listFiles(inputDirURI, true);\n+\n+    //TODO: sort input files based on creation time\n+    List<String> filteredFiles = new ArrayList<>();\n+    PathMatcher includeFilePathMatcher = null;\n+    if (_spec.getIncludeFileNamePattern() != null) {\n+      includeFilePathMatcher = FileSystems.getDefault().getPathMatcher(_spec.getIncludeFileNamePattern());\n+    }\n+    PathMatcher excludeFilePathMatcher = null;\n+    if (_spec.getExcludeFileNamePattern() != null) {\n+      excludeFilePathMatcher = FileSystems.getDefault().getPathMatcher(_spec.getExcludeFileNamePattern());\n+    }\n+\n+    for (String file : files) {\n+      if (includeFilePathMatcher != null) {\n+        if (!includeFilePathMatcher.matches(Paths.get(file))) {\n+          continue;\n+        }\n+      }\n+      if (excludeFilePathMatcher != null) {\n+        if (excludeFilePathMatcher.matches(Paths.get(file))) {\n+          continue;\n+        }\n+      }\n+      if (!inputDirFS.isDirectory(new URI(file))) {\n+        filteredFiles.add(file);\n+      }\n+    }\n+\n+    int numDataFiles = filteredFiles.size();\n+    if (numDataFiles == 0) {\n+      String errorMessage = String\n+          .format(\"No data file founded in [%s], with include file pattern: [%s] and exclude file  pattern [%s]\",\n+              _spec.getInputDirURI(), _spec.getIncludeFileNamePattern(), _spec.getExcludeFileNamePattern());\n+      LOGGER.error(errorMessage);\n+      throw new RuntimeException(errorMessage);\n+    } else {\n+      LOGGER.info(\"Creating segments with data files: {}\", filteredFiles);\n+      for (int i = 0; i < numDataFiles; i++) {\n+        String dataFilePath = filteredFiles.get(i);\n+\n+        File localFile = new File(\"tmp\");\n+        try (DataOutputStream dataOutputStream = new DataOutputStream(new FileOutputStream(localFile))) {\n+          dataOutputStream.write(StringUtil.encodeUtf8(dataFilePath + \" \" + i));\n+          dataOutputStream.flush();\n+          outputDirFS.copyFromLocalFile(localFile, new Path(stagingInputDir, Integer.toString(i)).toUri());\n+        }\n+      }\n+    }\n+\n+    try {\n+      // Set up the job\n+      Job job = Job.getInstance(getConf());\n+      job.setJarByClass(getClass());\n+      job.setJobName(getClass().getName());\n+\n+      org.apache.hadoop.conf.Configuration jobConf = job.getConfiguration();\n+      String hadoopTokenFileLocation = System.getenv(\"HADOOP_TOKEN_FILE_LOCATION\");\n+      if (hadoopTokenFileLocation != null) {\n+        jobConf.set(\"mapreduce.job.credentials.binary\", hadoopTokenFileLocation);\n+      }\n+      jobConf.setInt(JobContext.NUM_MAPS, numDataFiles);\n+\n+      packPluginsToDistributedCache(job);\n+      // Add dependency jars\n+      if (_spec.getExecutionFrameworkSpec().getExtraConfigs().containsKey(DEPS_JAR_DIR)) {\n+        addDepsJarToDistributedCache(job, _spec.getExecutionFrameworkSpec().getExtraConfigs().get(DEPS_JAR_DIR));\n+      }\n+\n+      _spec.setOutputDirURI(stagingSegmentTarUri.toUri().toString());\n+      jobConf.set(SEGMENT_GENERATION_JOB_SPEC, new Yaml().dump(_spec));\n+      _spec.setOutputDirURI(outputDirURI.toString());\n+\n+      job.setMapperClass(getMapperClass());\n+      job.setNumReduceTasks(0);\n+\n+      job.setInputFormatClass(TextInputFormat.class);\n+      job.setOutputFormatClass(TextOutputFormat.class);\n+\n+      job.setMapOutputKeyClass(LongWritable.class);\n+      job.setMapOutputValueClass(Text.class);\n+\n+      FileInputFormat.addInputPath(job, stagingInputDir);\n+      FileOutputFormat.setOutputPath(job, new Path(stagingDir, \"output\"));\n+\n+      // Submit the job\n+      job.waitForCompletion(true);\n+      if (!job.isSuccessful()) {\n+        throw new RuntimeException(\"Job failed: \" + job);\n+      }\n+\n+      LOGGER.info(\"Trying to copy segment tars from staging directory: [{}] to output directory [{}]\", stagingDirURI,\n+          outputDirURI);\n+      outputDirFS.copy(new Path(stagingDir, SEGMENT_TAR_DIR).toUri(), outputDirURI);\n+    } finally {\n+      LOGGER.info(\"Trying to clean up staging directory: [{}]\", stagingDirURI);\n+      outputDirFS.delete(stagingDirURI, true);\n+    }\n+  }\n+\n+  /**\n+   * Can be overridden to plug in custom mapper.\n+   */\n+  protected Class<? extends Mapper<LongWritable, Text, LongWritable, Text>> getMapperClass() {\n+    return HadoopSegmentCreationMapper.class;\n+  }\n+\n+  protected void packPluginsToDistributedCache(Job job) {\n+    String pluginsRootDir = PluginManager.get().getPluginsRootDir();\n+    File pluginsTarGzFile = new File(PINOT_PLUGINS_TAR_GZ);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e14bd751f736c048a32270fc0f53eb266ad10c1c"}, "originalPosition": 279}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4d98456410a44ea2f1bb24f940edf28ab95e4736", "author": {"user": {"login": "xiangfu0", "name": "Xiang Fu"}}, "url": "https://github.com/apache/pinot/commit/4d98456410a44ea2f1bb24f940edf28ab95e4736", "committedDate": "2020-01-17T01:03:35Z", "message": "refactor common utils"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2751cf528cc028a0acc18f051dd2e9acc080bd6c", "author": {"user": {"login": "xiangfu0", "name": "Xiang Fu"}}, "url": "https://github.com/apache/pinot/commit/2751cf528cc028a0acc18f051dd2e9acc080bd6c", "committedDate": "2020-01-17T01:03:35Z", "message": "Adding pinot-batch-ingestion-hadoop module"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c48d41ea81811dc704a22ab62d86f7f848159cf2", "author": {"user": {"login": "xiangfu0", "name": "Xiang Fu"}}, "url": "https://github.com/apache/pinot/commit/c48d41ea81811dc704a22ab62d86f7f848159cf2", "committedDate": "2020-01-17T01:03:35Z", "message": "Address comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e14bd751f736c048a32270fc0f53eb266ad10c1c", "author": {"user": {"login": "xiangfu0", "name": "Xiang Fu"}}, "url": "https://github.com/apache/pinot/commit/e14bd751f736c048a32270fc0f53eb266ad10c1c", "committedDate": "2020-01-16T08:05:36Z", "message": "Adding pinot-batch-ingestion-hadoop module"}, "afterCommit": {"oid": "c48d41ea81811dc704a22ab62d86f7f848159cf2", "author": {"user": {"login": "xiangfu0", "name": "Xiang Fu"}}, "url": "https://github.com/apache/pinot/commit/c48d41ea81811dc704a22ab62d86f7f848159cf2", "committedDate": "2020-01-17T01:03:35Z", "message": "Address comments"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1475, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}