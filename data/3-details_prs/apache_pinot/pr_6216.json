{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTEzNTIyNDU3", "number": 6216, "title": "Nested Field (JSON) Indexing ", "bodyText": "Add the support for json index\nSample table config:\n{\n  \"tableIndexConfig\": {\n    \"jsonIndexColumns\": [\n      \"person\"\n    ],\n    ...\n  },\n  ...\n}\n\nSample Row for person column\n{\n\u00a0\u00a0\"name\" : \"adam\",\n\u00a0\u00a0\"age\" : 30,\n\u00a0\u00a0\"country\" : \"us\",\n\u00a0\u00a0\"addresses\u201d : [\n{\n\u00a0\u00a0\u00a0\u00a0\"number\" : 112,\n\u00a0\u00a0\u00a0\u00a0\" Street\" : \"main st\",\n\u00a0\u00a0\u00a0\u00a0\" country\" : \"us\"\n\u00a0\u00a0},\n{\n\u00a0\u00a0\u00a0\u00a0\"number\" : 2,\n\u00a0\u00a0\u00a0\u00a0\" Street\" : \u201csecond st\",\n\u00a0\u00a0\u00a0\u00a0\" country\" : \"us\"\n\u00a0\u00a0},\n{\n\u00a0\u00a0\u00a0\u00a0\"number\" : 3,\n\u00a0\u00a0\u00a0\u00a0\" Street\" : \u201cthird st\",\n\u00a0\u00a0\u00a0\u00a0\" country\" : \"us\"\n\u00a0\u00a0}\n]\n}\n\nSample queries:\nSELECT ... FROM table WHERE JSON_MATCH(person, 'addresses.street = ''main st'' AND name = ''adam''')\n\nSELECT ... FROM table WHERE JSON_MATCH(person, 'name != ''adam''')\n\nSELECT JSON_EXTRACT_SCALAR(person, '$.name', 'STRING') FROM table WHERE \nJSON_MATCH(person, 'addresses.street = ''main st'') LIMIT 10\n\nPerformance\n10 million rows\nSELECT ... FROM table WHERE JSON_MATCH(person, 'name != ''adam''')\nWithout JSON Index: 11 seconds\nWith Index: 10 ms", "createdAt": "2020-11-01T01:15:58Z", "url": "https://github.com/apache/pinot/pull/6216", "merged": true, "mergeCommit": {"oid": "ea0bfa04fc548f24de248b0d1eca7f8ef7515b54"}, "closed": true, "closedAt": "2020-12-28T22:44:38Z", "author": {"login": "kishoreg"}, "timelineItems": {"totalCount": 15, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdYUqP-AFqTUyMTI1MDkwNA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdqtSdsgBqjQxNTMxMzY4Njg=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIxMjUwOTA0", "url": "https://github.com/apache/pinot/pull/6216#pullrequestreview-521250904", "createdAt": "2020-11-01T18:55:40Z", "commit": {"oid": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIxMjUwNTcx", "url": "https://github.com/apache/pinot/pull/6216#pullrequestreview-521250571", "createdAt": "2020-11-01T18:51:52Z", "commit": {"oid": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d"}, "state": "COMMENTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMVQxODo1MTo1M1rOHrxPZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMVQxOToxNDo1OVrOHrxZJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1NzU3NA==", "bodyText": "clean these up", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515657574", "createdAt": "2020-11-01T18:51:53Z", "author": {"login": "yupeng9"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/operator/filter/JSONMatchFilterOperator.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.operator.filter;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.apache.pinot.core.operator.blocks.FilterBlock;\n+import org.apache.pinot.core.operator.docidsets.BitmapDocIdSet;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.predicate.EqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.InPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.NotEqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.NotInPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.segment.creator.impl.inv.JSONIndexCreator;\n+import org.apache.pinot.core.segment.index.readers.JSONIndexReader;\n+import org.roaringbitmap.buffer.ImmutableRoaringBitmap;\n+import org.roaringbitmap.buffer.MutableRoaringBitmap;\n+\n+\n+@SuppressWarnings(\"rawtypes\")\n+public class JSONMatchFilterOperator extends BaseFilterOperator {\n+  private static final String OPERATOR_NAME = \"JSONMatchFilterOperator\";\n+\n+  private final JSONIndexReader _jsonIndexReader;\n+  private final int _numDocs;\n+  private String _column;\n+  private final FilterContext _filterContext;\n+\n+  public JSONMatchFilterOperator(String column, FilterContext filterContext, JSONIndexReader jsonIndexReader,\n+      int numDocs) {\n+    _column = column;\n+    _filterContext = filterContext;\n+    _jsonIndexReader = jsonIndexReader;\n+    _numDocs = numDocs;\n+  }\n+\n+  @Override\n+  protected FilterBlock getNextBlock() {\n+    ImmutableRoaringBitmap flattenedDocIds = process(_filterContext);\n+//    System.out.println(\"flattenedDocIds = \" + flattenedDocIds);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1NzcxNw==", "bodyText": "add a handling of default", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515657717", "createdAt": "2020-11-01T18:53:17Z", "author": {"login": "yupeng9"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/operator/filter/JSONMatchFilterOperator.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.operator.filter;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.apache.pinot.core.operator.blocks.FilterBlock;\n+import org.apache.pinot.core.operator.docidsets.BitmapDocIdSet;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.predicate.EqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.InPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.NotEqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.NotInPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.segment.creator.impl.inv.JSONIndexCreator;\n+import org.apache.pinot.core.segment.index.readers.JSONIndexReader;\n+import org.roaringbitmap.buffer.ImmutableRoaringBitmap;\n+import org.roaringbitmap.buffer.MutableRoaringBitmap;\n+\n+\n+@SuppressWarnings(\"rawtypes\")\n+public class JSONMatchFilterOperator extends BaseFilterOperator {\n+  private static final String OPERATOR_NAME = \"JSONMatchFilterOperator\";\n+\n+  private final JSONIndexReader _jsonIndexReader;\n+  private final int _numDocs;\n+  private String _column;\n+  private final FilterContext _filterContext;\n+\n+  public JSONMatchFilterOperator(String column, FilterContext filterContext, JSONIndexReader jsonIndexReader,\n+      int numDocs) {\n+    _column = column;\n+    _filterContext = filterContext;\n+    _jsonIndexReader = jsonIndexReader;\n+    _numDocs = numDocs;\n+  }\n+\n+  @Override\n+  protected FilterBlock getNextBlock() {\n+    ImmutableRoaringBitmap flattenedDocIds = process(_filterContext);\n+//    System.out.println(\"flattenedDocIds = \" + flattenedDocIds);\n+    MutableRoaringBitmap rootDocIds = new MutableRoaringBitmap();\n+    Iterator<Integer> iterator = flattenedDocIds.iterator();\n+    while (iterator.hasNext()) {\n+      int flattenedDocId = iterator.next();\n+      rootDocIds.add(_jsonIndexReader.getRootDocId(flattenedDocId));\n+    }\n+//    System.out.println(\"rootDocIds = \" + rootDocIds);\n+\n+    return new FilterBlock(new BitmapDocIdSet(rootDocIds, _numDocs));\n+  }\n+\n+  private MutableRoaringBitmap process(FilterContext filterContext) {\n+    List<FilterContext> children = _filterContext.getChildren();\n+    MutableRoaringBitmap resultBitmap = null;\n+\n+    switch (filterContext.getType()) {\n+      case AND:\n+        for (FilterContext child : children) {\n+          if (resultBitmap == null) {\n+            resultBitmap = process(child);\n+          } else {\n+            resultBitmap.and(process(child));\n+          }\n+        }\n+        break;\n+      case OR:\n+        for (FilterContext child : children) {\n+          if (resultBitmap == null) {\n+            resultBitmap = process(child);\n+          } else {\n+            resultBitmap.or(process(child));\n+          }\n+        }\n+        break;\n+      case PREDICATE:\n+        Predicate predicate = filterContext.getPredicate();\n+        Predicate newPredicate = null;\n+        switch (predicate.getType()) {\n+\n+          case EQ:\n+            EqPredicate eqPredicate = (EqPredicate) predicate;\n+            newPredicate = new EqPredicate(ExpressionContext.forIdentifier(_column),\n+                eqPredicate.getLhs().getIdentifier() + JSONIndexCreator.POSTING_LIST_KEY_SEPARATOR + eqPredicate\n+                    .getValue());\n+            break;\n+          case NOT_EQ:\n+            NotEqPredicate nEqPredicate = (NotEqPredicate) predicate;\n+            newPredicate = new NotEqPredicate(ExpressionContext.forIdentifier(_column),\n+                nEqPredicate.getLhs().getIdentifier() + JSONIndexCreator.POSTING_LIST_KEY_SEPARATOR\n+                    + nEqPredicate.getValue());\n+            break;\n+          case IN:\n+            InPredicate inPredicate = (InPredicate) predicate;\n+            List<String> newInValues = inPredicate.getValues().stream().map(\n+                value -> inPredicate.getLhs().getIdentifier() + JSONIndexCreator.POSTING_LIST_KEY_SEPARATOR\n+                    + value).collect(Collectors.toList());\n+            newPredicate = new InPredicate(ExpressionContext.forIdentifier(_column), newInValues);\n+            break;\n+          case NOT_IN:\n+            NotInPredicate notInPredicate = (NotInPredicate) predicate;\n+            List<String> newNotInValues = notInPredicate.getValues().stream().map(\n+                value -> notInPredicate.getLhs().getIdentifier() + JSONIndexCreator.POSTING_LIST_KEY_SEPARATOR\n+                    + value).collect(Collectors.toList());\n+            newPredicate = new InPredicate(ExpressionContext.forIdentifier(_column), newNotInValues);\n+            break;\n+          case IS_NULL:\n+            newPredicate = predicate;\n+            break;\n+          case IS_NOT_NULL:\n+            newPredicate = predicate;\n+            break;\n+          case RANGE:\n+          case REGEXP_LIKE:\n+          case TEXT_MATCH:\n+            throw new UnsupportedOperationException(\"JSON Match does not support RANGE, REGEXP or TEXTMATCH\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d"}, "originalPosition": 134}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1Nzc1Mw==", "bodyText": "revert", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515657753", "createdAt": "2020-11-01T18:53:36Z", "author": {"login": "yupeng9"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/operator/transform/function/CastTransformFunction.java", "diffHunk": "@@ -30,7 +30,7 @@\n import org.apache.pinot.core.plan.DocIdSetPlanNode;\n \n \n-public class CastTransformFunction extends BaseTransformFunction {\n+  public class CastTransformFunction extends BaseTransformFunction {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1Nzk5MQ==", "bodyText": "add some javadoc, plz", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515657991", "createdAt": "2020-11-01T18:55:28Z", "author": {"login": "yupeng9"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/creator/impl/inv/JSONIndexCreator.java", "diffHunk": "@@ -0,0 +1,658 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.creator.impl.inv;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.github.wnameless.json.flattener.JsonFlattener;\n+import com.google.common.io.Files;\n+import com.google.common.primitives.UnsignedBytes;\n+import java.io.BufferedInputStream;\n+import java.io.BufferedOutputStream;\n+import java.io.Closeable;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.TreeMap;\n+import java.util.stream.Collectors;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.commons.lang3.tuple.ImmutablePair;\n+import org.apache.pinot.core.common.BlockDocIdIterator;\n+import org.apache.pinot.core.io.util.VarLengthBytesValueReaderWriter;\n+import org.apache.pinot.core.operator.blocks.FilterBlock;\n+import org.apache.pinot.core.operator.filter.JSONMatchFilterOperator;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.predicate.EqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.QueryContextConverterUtils;\n+import org.apache.pinot.core.segment.index.readers.JSONIndexReader;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+import org.apache.pinot.spi.data.DimensionFieldSpec;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.sql.parsers.CalciteSqlParser;\n+import org.roaringbitmap.RoaringBitmap;\n+import org.roaringbitmap.buffer.MutableRoaringBitmap;\n+\n+import static org.apache.pinot.core.segment.creator.impl.V1Constants.Indexes.JSON_INDEX_FILE_EXTENSION;\n+\n+\n+public class JSONIndexCreator implements Closeable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1ODEzMg==", "bodyText": "define these as constants", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515658132", "createdAt": "2020-11-01T18:56:43Z", "author": {"login": "yupeng9"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/creator/impl/inv/JSONIndexCreator.java", "diffHunk": "@@ -0,0 +1,658 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.creator.impl.inv;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.github.wnameless.json.flattener.JsonFlattener;\n+import com.google.common.io.Files;\n+import com.google.common.primitives.UnsignedBytes;\n+import java.io.BufferedInputStream;\n+import java.io.BufferedOutputStream;\n+import java.io.Closeable;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.TreeMap;\n+import java.util.stream.Collectors;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.commons.lang3.tuple.ImmutablePair;\n+import org.apache.pinot.core.common.BlockDocIdIterator;\n+import org.apache.pinot.core.io.util.VarLengthBytesValueReaderWriter;\n+import org.apache.pinot.core.operator.blocks.FilterBlock;\n+import org.apache.pinot.core.operator.filter.JSONMatchFilterOperator;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.predicate.EqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.QueryContextConverterUtils;\n+import org.apache.pinot.core.segment.index.readers.JSONIndexReader;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+import org.apache.pinot.spi.data.DimensionFieldSpec;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.sql.parsers.CalciteSqlParser;\n+import org.roaringbitmap.RoaringBitmap;\n+import org.roaringbitmap.buffer.MutableRoaringBitmap;\n+\n+import static org.apache.pinot.core.segment.creator.impl.V1Constants.Indexes.JSON_INDEX_FILE_EXTENSION;\n+\n+\n+public class JSONIndexCreator implements Closeable {\n+\n+  //separator used to join the key and value to create posting list key\n+  public static String POSTING_LIST_KEY_SEPARATOR = \"|\";\n+  static int FLUSH_THRESHOLD = 50_000;\n+  static int VERSION = 1;\n+  private final File flattenedDocId2RootDocIdMappingFile;\n+  private final File postingListFile;\n+  private File dictionaryheaderFile;\n+  private File dictionaryOffsetFile;\n+  private File dictionaryFile;\n+  private File invertedIndexOffsetFile;\n+  private File invertedIndexFile;\n+  private File outputIndexFile;\n+\n+  private int docId = 0;\n+  private int numFlatennedDocId = 0;\n+  int chunkId = 0;\n+\n+  private DataOutputStream postingListWriter;\n+  private DataOutputStream flattenedDocId2RootDocIdWriter;\n+\n+  Map<String, List<Integer>> postingListMap = new TreeMap<>();\n+  List<Integer> flattenedDocIdList = new ArrayList<>();\n+  List<Integer> postingListChunkOffsets = new ArrayList<>();\n+  List<Integer> chunkLengths = new ArrayList<>();\n+  private FieldSpec fieldSpec;\n+\n+  public JSONIndexCreator(File indexDir, FieldSpec fieldSpec)\n+      throws IOException {\n+    this.fieldSpec = fieldSpec;\n+    System.out.println(\"indexDir = \" + indexDir);\n+\n+    String name = fieldSpec.getName();\n+    postingListFile = new File(indexDir + name + \"_postingList.buf\");\n+    postingListWriter = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(postingListFile)));\n+    postingListChunkOffsets.add(postingListWriter.size());\n+\n+    dictionaryheaderFile = new File(indexDir, name + \"_dictionaryHeader.buf\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1OTA0Mw==", "bodyText": "do you think this header is future-proof? Will it ever evolve and change?", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515659043", "createdAt": "2020-11-01T19:05:23Z", "author": {"login": "yupeng9"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/creator/impl/inv/JSONIndexCreator.java", "diffHunk": "@@ -0,0 +1,658 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.creator.impl.inv;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.github.wnameless.json.flattener.JsonFlattener;\n+import com.google.common.io.Files;\n+import com.google.common.primitives.UnsignedBytes;\n+import java.io.BufferedInputStream;\n+import java.io.BufferedOutputStream;\n+import java.io.Closeable;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.TreeMap;\n+import java.util.stream.Collectors;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.commons.lang3.tuple.ImmutablePair;\n+import org.apache.pinot.core.common.BlockDocIdIterator;\n+import org.apache.pinot.core.io.util.VarLengthBytesValueReaderWriter;\n+import org.apache.pinot.core.operator.blocks.FilterBlock;\n+import org.apache.pinot.core.operator.filter.JSONMatchFilterOperator;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.predicate.EqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.QueryContextConverterUtils;\n+import org.apache.pinot.core.segment.index.readers.JSONIndexReader;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+import org.apache.pinot.spi.data.DimensionFieldSpec;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.sql.parsers.CalciteSqlParser;\n+import org.roaringbitmap.RoaringBitmap;\n+import org.roaringbitmap.buffer.MutableRoaringBitmap;\n+\n+import static org.apache.pinot.core.segment.creator.impl.V1Constants.Indexes.JSON_INDEX_FILE_EXTENSION;\n+\n+\n+public class JSONIndexCreator implements Closeable {\n+\n+  //separator used to join the key and value to create posting list key\n+  public static String POSTING_LIST_KEY_SEPARATOR = \"|\";\n+  static int FLUSH_THRESHOLD = 50_000;\n+  static int VERSION = 1;\n+  private final File flattenedDocId2RootDocIdMappingFile;\n+  private final File postingListFile;\n+  private File dictionaryheaderFile;\n+  private File dictionaryOffsetFile;\n+  private File dictionaryFile;\n+  private File invertedIndexOffsetFile;\n+  private File invertedIndexFile;\n+  private File outputIndexFile;\n+\n+  private int docId = 0;\n+  private int numFlatennedDocId = 0;\n+  int chunkId = 0;\n+\n+  private DataOutputStream postingListWriter;\n+  private DataOutputStream flattenedDocId2RootDocIdWriter;\n+\n+  Map<String, List<Integer>> postingListMap = new TreeMap<>();\n+  List<Integer> flattenedDocIdList = new ArrayList<>();\n+  List<Integer> postingListChunkOffsets = new ArrayList<>();\n+  List<Integer> chunkLengths = new ArrayList<>();\n+  private FieldSpec fieldSpec;\n+\n+  public JSONIndexCreator(File indexDir, FieldSpec fieldSpec)\n+      throws IOException {\n+    this.fieldSpec = fieldSpec;\n+    System.out.println(\"indexDir = \" + indexDir);\n+\n+    String name = fieldSpec.getName();\n+    postingListFile = new File(indexDir + name + \"_postingList.buf\");\n+    postingListWriter = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(postingListFile)));\n+    postingListChunkOffsets.add(postingListWriter.size());\n+\n+    dictionaryheaderFile = new File(indexDir, name + \"_dictionaryHeader.buf\");\n+    dictionaryOffsetFile = new File(indexDir, name + \"_dictionaryOffset.buf\");\n+    dictionaryFile = new File(indexDir, name + \"_dictionary.buf\");\n+    invertedIndexOffsetFile = new File(indexDir, name + \"_invertedIndexOffset.buf\");\n+    invertedIndexFile = new File(indexDir, name + \"_invertedIndex.buf\");\n+    flattenedDocId2RootDocIdMappingFile = new File(indexDir, name + \"_flattenedDocId.buf\");\n+    flattenedDocId2RootDocIdWriter =\n+        new DataOutputStream(new BufferedOutputStream(new FileOutputStream(flattenedDocId2RootDocIdMappingFile)));\n+\n+    //output file\n+    outputIndexFile = new File(indexDir, name + JSON_INDEX_FILE_EXTENSION);\n+  }\n+\n+  public void add(byte[] data)\n+      throws IOException {\n+\n+    JsonNode jsonNode = new ObjectMapper().readTree(data);\n+    List<Map<String, String>> flattenedMapList = unnestJson(jsonNode);\n+    for (Map<String, String> map : flattenedMapList) {\n+      //\n+      for (Map.Entry<String, String> entry : map.entrySet()) {\n+        //handle key posting list\n+        String key = entry.getKey();\n+\n+        List<Integer> keyPostingList = postingListMap.get(key);\n+        if (keyPostingList == null) {\n+          keyPostingList = new ArrayList<>();\n+          postingListMap.put(key, keyPostingList);\n+        }\n+        keyPostingList.add(numFlatennedDocId);\n+\n+        //handle keyvalue posting list\n+        String keyValue = key + POSTING_LIST_KEY_SEPARATOR + entry.getValue();\n+        List<Integer> keyValuePostingList = postingListMap.get(keyValue);\n+        if (keyValuePostingList == null) {\n+          keyValuePostingList = new ArrayList<>();\n+          postingListMap.put(keyValue, keyValuePostingList);\n+        }\n+        keyValuePostingList.add(numFlatennedDocId);\n+      }\n+      //flattenedDocId2RootDocIdMapping\n+      flattenedDocIdList.add(docId);\n+\n+      numFlatennedDocId++;\n+    }\n+    docId++;\n+\n+    //flush data\n+    if (docId % FLUSH_THRESHOLD == 0) {\n+      flush();\n+    }\n+  }\n+\n+  /**\n+   * Multi value\n+   * @param dataArray\n+   * @param length\n+   * @throws IOException\n+   */\n+  public void add(byte[][] dataArray, int length)\n+      throws IOException {\n+\n+    for (int i = 0; i < length; i++) {\n+      byte[] data = dataArray[i];\n+      JsonNode jsonNode = new ObjectMapper().readTree(data);\n+      List<Map<String, String>> flattenedMapList = unnestJson(jsonNode);\n+      for (Map<String, String> map : flattenedMapList) {\n+        //\n+        for (Map.Entry<String, String> entry : map.entrySet()) {\n+          //handle key posting list\n+          String key = entry.getKey();\n+\n+          List<Integer> keyPostingList = postingListMap.get(key);\n+          if (keyPostingList == null) {\n+            keyPostingList = new ArrayList<>();\n+            postingListMap.put(key, keyPostingList);\n+          }\n+          keyPostingList.add(numFlatennedDocId);\n+\n+          //handle keyvalue posting list\n+          String keyValue = key + POSTING_LIST_KEY_SEPARATOR + entry.getValue();\n+          List<Integer> keyValuePostingList = postingListMap.get(keyValue);\n+          if (keyValuePostingList == null) {\n+            keyValuePostingList = new ArrayList<>();\n+            postingListMap.put(keyValue, keyValuePostingList);\n+          }\n+          keyValuePostingList.add(numFlatennedDocId);\n+        }\n+        //flattenedDocId2RootDocIdMapping\n+        flattenedDocIdList.add(numFlatennedDocId);\n+\n+        numFlatennedDocId++;\n+      }\n+    }\n+    docId++;\n+\n+    //flush data\n+    if (docId % FLUSH_THRESHOLD == 0) {\n+      flush();\n+    }\n+  }\n+\n+  public void seal()\n+      throws IOException {\n+\n+    flush();\n+\n+    flattenedDocId2RootDocIdWriter.close();\n+    postingListWriter.close();\n+\n+    //key posting list merging\n+    System.out.println(\"InvertedIndex\");\n+    System.out.println(\"=================\");\n+\n+    int maxKeyLength = createInvertedIndex(postingListFile, postingListChunkOffsets, chunkLengths);\n+    System.out.println(\"=================\");\n+\n+    int flattenedDocid = 0;\n+    DataInputStream flattenedDocId2RootDocIdReader =\n+        new DataInputStream(new BufferedInputStream(new FileInputStream(flattenedDocId2RootDocIdMappingFile)));\n+    int[] rootDocIdArray = new int[numFlatennedDocId];\n+    while (flattenedDocid < numFlatennedDocId) {\n+      rootDocIdArray[flattenedDocid++] = flattenedDocId2RootDocIdReader.readInt();\n+    }\n+    System.out.println(\"FlattenedDocId  to RootDocId Mapping = \");\n+    System.out.println(Arrays.toString(rootDocIdArray));\n+\n+    //PUT all contents into one file\n+\n+    //header\n+    // version + maxDictionaryLength + [store the offsets + length for each one (dictionary offset file, dictionaryFile, index offset file, index file, flattened docId to rootDocId file)]\n+    long headerSize = 2 * Integer.BYTES + 6 * 2 * Long.BYTES;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d"}, "originalPosition": 238}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1OTQyMA==", "bodyText": "how do you handle the key that contains .?", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515659420", "createdAt": "2020-11-01T19:09:10Z", "author": {"login": "yupeng9"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/creator/impl/inv/JSONIndexCreator.java", "diffHunk": "@@ -0,0 +1,658 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.creator.impl.inv;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.github.wnameless.json.flattener.JsonFlattener;\n+import com.google.common.io.Files;\n+import com.google.common.primitives.UnsignedBytes;\n+import java.io.BufferedInputStream;\n+import java.io.BufferedOutputStream;\n+import java.io.Closeable;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.TreeMap;\n+import java.util.stream.Collectors;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.commons.lang3.tuple.ImmutablePair;\n+import org.apache.pinot.core.common.BlockDocIdIterator;\n+import org.apache.pinot.core.io.util.VarLengthBytesValueReaderWriter;\n+import org.apache.pinot.core.operator.blocks.FilterBlock;\n+import org.apache.pinot.core.operator.filter.JSONMatchFilterOperator;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.predicate.EqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.QueryContextConverterUtils;\n+import org.apache.pinot.core.segment.index.readers.JSONIndexReader;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+import org.apache.pinot.spi.data.DimensionFieldSpec;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.sql.parsers.CalciteSqlParser;\n+import org.roaringbitmap.RoaringBitmap;\n+import org.roaringbitmap.buffer.MutableRoaringBitmap;\n+\n+import static org.apache.pinot.core.segment.creator.impl.V1Constants.Indexes.JSON_INDEX_FILE_EXTENSION;\n+\n+\n+public class JSONIndexCreator implements Closeable {\n+\n+  //separator used to join the key and value to create posting list key\n+  public static String POSTING_LIST_KEY_SEPARATOR = \"|\";\n+  static int FLUSH_THRESHOLD = 50_000;\n+  static int VERSION = 1;\n+  private final File flattenedDocId2RootDocIdMappingFile;\n+  private final File postingListFile;\n+  private File dictionaryheaderFile;\n+  private File dictionaryOffsetFile;\n+  private File dictionaryFile;\n+  private File invertedIndexOffsetFile;\n+  private File invertedIndexFile;\n+  private File outputIndexFile;\n+\n+  private int docId = 0;\n+  private int numFlatennedDocId = 0;\n+  int chunkId = 0;\n+\n+  private DataOutputStream postingListWriter;\n+  private DataOutputStream flattenedDocId2RootDocIdWriter;\n+\n+  Map<String, List<Integer>> postingListMap = new TreeMap<>();\n+  List<Integer> flattenedDocIdList = new ArrayList<>();\n+  List<Integer> postingListChunkOffsets = new ArrayList<>();\n+  List<Integer> chunkLengths = new ArrayList<>();\n+  private FieldSpec fieldSpec;\n+\n+  public JSONIndexCreator(File indexDir, FieldSpec fieldSpec)\n+      throws IOException {\n+    this.fieldSpec = fieldSpec;\n+    System.out.println(\"indexDir = \" + indexDir);\n+\n+    String name = fieldSpec.getName();\n+    postingListFile = new File(indexDir + name + \"_postingList.buf\");\n+    postingListWriter = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(postingListFile)));\n+    postingListChunkOffsets.add(postingListWriter.size());\n+\n+    dictionaryheaderFile = new File(indexDir, name + \"_dictionaryHeader.buf\");\n+    dictionaryOffsetFile = new File(indexDir, name + \"_dictionaryOffset.buf\");\n+    dictionaryFile = new File(indexDir, name + \"_dictionary.buf\");\n+    invertedIndexOffsetFile = new File(indexDir, name + \"_invertedIndexOffset.buf\");\n+    invertedIndexFile = new File(indexDir, name + \"_invertedIndex.buf\");\n+    flattenedDocId2RootDocIdMappingFile = new File(indexDir, name + \"_flattenedDocId.buf\");\n+    flattenedDocId2RootDocIdWriter =\n+        new DataOutputStream(new BufferedOutputStream(new FileOutputStream(flattenedDocId2RootDocIdMappingFile)));\n+\n+    //output file\n+    outputIndexFile = new File(indexDir, name + JSON_INDEX_FILE_EXTENSION);\n+  }\n+\n+  public void add(byte[] data)\n+      throws IOException {\n+\n+    JsonNode jsonNode = new ObjectMapper().readTree(data);\n+    List<Map<String, String>> flattenedMapList = unnestJson(jsonNode);\n+    for (Map<String, String> map : flattenedMapList) {\n+      //\n+      for (Map.Entry<String, String> entry : map.entrySet()) {\n+        //handle key posting list\n+        String key = entry.getKey();\n+\n+        List<Integer> keyPostingList = postingListMap.get(key);\n+        if (keyPostingList == null) {\n+          keyPostingList = new ArrayList<>();\n+          postingListMap.put(key, keyPostingList);\n+        }\n+        keyPostingList.add(numFlatennedDocId);\n+\n+        //handle keyvalue posting list\n+        String keyValue = key + POSTING_LIST_KEY_SEPARATOR + entry.getValue();\n+        List<Integer> keyValuePostingList = postingListMap.get(keyValue);\n+        if (keyValuePostingList == null) {\n+          keyValuePostingList = new ArrayList<>();\n+          postingListMap.put(keyValue, keyValuePostingList);\n+        }\n+        keyValuePostingList.add(numFlatennedDocId);\n+      }\n+      //flattenedDocId2RootDocIdMapping\n+      flattenedDocIdList.add(docId);\n+\n+      numFlatennedDocId++;\n+    }\n+    docId++;\n+\n+    //flush data\n+    if (docId % FLUSH_THRESHOLD == 0) {\n+      flush();\n+    }\n+  }\n+\n+  /**\n+   * Multi value\n+   * @param dataArray\n+   * @param length\n+   * @throws IOException\n+   */\n+  public void add(byte[][] dataArray, int length)\n+      throws IOException {\n+\n+    for (int i = 0; i < length; i++) {\n+      byte[] data = dataArray[i];\n+      JsonNode jsonNode = new ObjectMapper().readTree(data);\n+      List<Map<String, String>> flattenedMapList = unnestJson(jsonNode);\n+      for (Map<String, String> map : flattenedMapList) {\n+        //\n+        for (Map.Entry<String, String> entry : map.entrySet()) {\n+          //handle key posting list\n+          String key = entry.getKey();\n+\n+          List<Integer> keyPostingList = postingListMap.get(key);\n+          if (keyPostingList == null) {\n+            keyPostingList = new ArrayList<>();\n+            postingListMap.put(key, keyPostingList);\n+          }\n+          keyPostingList.add(numFlatennedDocId);\n+\n+          //handle keyvalue posting list\n+          String keyValue = key + POSTING_LIST_KEY_SEPARATOR + entry.getValue();\n+          List<Integer> keyValuePostingList = postingListMap.get(keyValue);\n+          if (keyValuePostingList == null) {\n+            keyValuePostingList = new ArrayList<>();\n+            postingListMap.put(keyValue, keyValuePostingList);\n+          }\n+          keyValuePostingList.add(numFlatennedDocId);\n+        }\n+        //flattenedDocId2RootDocIdMapping\n+        flattenedDocIdList.add(numFlatennedDocId);\n+\n+        numFlatennedDocId++;\n+      }\n+    }\n+    docId++;\n+\n+    //flush data\n+    if (docId % FLUSH_THRESHOLD == 0) {\n+      flush();\n+    }\n+  }\n+\n+  public void seal()\n+      throws IOException {\n+\n+    flush();\n+\n+    flattenedDocId2RootDocIdWriter.close();\n+    postingListWriter.close();\n+\n+    //key posting list merging\n+    System.out.println(\"InvertedIndex\");\n+    System.out.println(\"=================\");\n+\n+    int maxKeyLength = createInvertedIndex(postingListFile, postingListChunkOffsets, chunkLengths);\n+    System.out.println(\"=================\");\n+\n+    int flattenedDocid = 0;\n+    DataInputStream flattenedDocId2RootDocIdReader =\n+        new DataInputStream(new BufferedInputStream(new FileInputStream(flattenedDocId2RootDocIdMappingFile)));\n+    int[] rootDocIdArray = new int[numFlatennedDocId];\n+    while (flattenedDocid < numFlatennedDocId) {\n+      rootDocIdArray[flattenedDocid++] = flattenedDocId2RootDocIdReader.readInt();\n+    }\n+    System.out.println(\"FlattenedDocId  to RootDocId Mapping = \");\n+    System.out.println(Arrays.toString(rootDocIdArray));\n+\n+    //PUT all contents into one file\n+\n+    //header\n+    // version + maxDictionaryLength + [store the offsets + length for each one (dictionary offset file, dictionaryFile, index offset file, index file, flattened docId to rootDocId file)]\n+    long headerSize = 2 * Integer.BYTES + 6 * 2 * Long.BYTES;\n+\n+    long dataSize =\n+        dictionaryheaderFile.length() + dictionaryOffsetFile.length() + dictionaryFile.length() + invertedIndexFile\n+            .length() + invertedIndexOffsetFile.length() + flattenedDocId2RootDocIdMappingFile.length();\n+\n+    long totalSize = headerSize + dataSize;\n+    PinotDataBuffer pinotDataBuffer =\n+        PinotDataBuffer.mapFile(outputIndexFile, false, 0, totalSize, ByteOrder.BIG_ENDIAN, \"Nested inverted index\");\n+\n+    pinotDataBuffer.putInt(0, VERSION);\n+    pinotDataBuffer.putInt(1 * Integer.BYTES, maxKeyLength);\n+    long writtenBytes = headerSize;\n+\n+    //add dictionary header\n+    int bufferId = 0;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, dictionaryheaderFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, dictionaryheaderFile, 0, dictionaryheaderFile.length());\n+    writtenBytes += dictionaryheaderFile.length();\n+\n+    //add dictionary offset\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, dictionaryOffsetFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, dictionaryOffsetFile, 0, dictionaryOffsetFile.length());\n+    writtenBytes += dictionaryOffsetFile.length();\n+\n+    //add dictionary\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, dictionaryFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, dictionaryFile, 0, dictionaryFile.length());\n+    writtenBytes += dictionaryFile.length();\n+\n+    //add index offset\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, invertedIndexOffsetFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, invertedIndexOffsetFile, 0, invertedIndexOffsetFile.length());\n+    writtenBytes += invertedIndexOffsetFile.length();\n+\n+    //add index data\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, invertedIndexFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, invertedIndexFile, 0, invertedIndexFile.length());\n+    writtenBytes += invertedIndexFile.length();\n+\n+    //add flattened docid to root doc id mapping\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, flattenedDocId2RootDocIdMappingFile.length());\n+    pinotDataBuffer\n+        .readFrom(writtenBytes, flattenedDocId2RootDocIdMappingFile, 0, flattenedDocId2RootDocIdMappingFile.length());\n+    writtenBytes += flattenedDocId2RootDocIdMappingFile.length();\n+  }\n+\n+  private long getBufferStartOffset(int bufferId) {\n+    return 2 * Integer.BYTES + 2 * bufferId * Long.BYTES;\n+  }\n+\n+  private int createInvertedIndex(File postingListFile, List<Integer> postingListChunkOffsets,\n+      List<Integer> chunkLengthList)\n+      throws IOException {\n+\n+    List<Iterator<ImmutablePair<byte[], int[]>>> chunkIterators = new ArrayList<>();\n+\n+    for (int i = 0; i < chunkLengthList.size(); i++) {\n+\n+      final DataInputStream postingListFileReader =\n+          new DataInputStream(new BufferedInputStream(new FileInputStream(postingListFile)));\n+      postingListFileReader.skipBytes(postingListChunkOffsets.get(i));\n+      final int length = chunkLengthList.get(i);\n+      chunkIterators.add(new Iterator<ImmutablePair<byte[], int[]>>() {\n+        int index = 0;\n+\n+        @Override\n+        public boolean hasNext() {\n+          return index < length;\n+        }\n+\n+        @Override\n+        public ImmutablePair<byte[], int[]> next() {\n+          try {\n+            int keyLength = postingListFileReader.readInt();\n+            byte[] keyBytes = new byte[keyLength];\n+            postingListFileReader.read(keyBytes);\n+\n+            int postingListLength = postingListFileReader.readInt();\n+            int[] postingList = new int[postingListLength];\n+            for (int i = 0; i < postingListLength; i++) {\n+              postingList[i] = postingListFileReader.readInt();\n+            }\n+            index++;\n+            return ImmutablePair.of(keyBytes, postingList);\n+          } catch (Exception e) {\n+            throw new RuntimeException(e);\n+          }\n+        }\n+      });\n+    }\n+    final Comparator<byte[]> byteArrayComparator = UnsignedBytes.lexicographicalComparator();\n+\n+    PriorityQueue<ImmutablePair<Integer, ImmutablePair<byte[], int[]>>> queue =\n+        new PriorityQueue<>(chunkLengthList.size(),\n+            (o1, o2) -> byteArrayComparator.compare(o1.getRight().getLeft(), o2.getRight().getLeft()));\n+    for (int i = 0; i < chunkIterators.size(); i++) {\n+      Iterator<ImmutablePair<byte[], int[]>> iterator = chunkIterators.get(i);\n+      if (iterator.hasNext()) {\n+        queue.offer(ImmutablePair.of(i, iterator.next()));\n+      }\n+    }\n+    byte[] prevKey = null;\n+    RoaringBitmap roaringBitmap = new RoaringBitmap();\n+\n+    Writer writer = new Writer(dictionaryheaderFile, dictionaryOffsetFile, dictionaryFile, invertedIndexOffsetFile,\n+        invertedIndexFile);\n+    while (!queue.isEmpty()) {\n+      ImmutablePair<Integer, ImmutablePair<byte[], int[]>> poll = queue.poll();\n+      byte[] currKey = poll.getRight().getLeft();\n+      if (prevKey != null && byteArrayComparator.compare(prevKey, currKey) != 0) {\n+        System.out.println(new String(prevKey) + \":\" + roaringBitmap);\n+        writer.add(prevKey, roaringBitmap);\n+        roaringBitmap.clear();\n+      }\n+\n+      roaringBitmap.add(poll.getRight().getRight());\n+      prevKey = currKey;\n+\n+      //add the next key from the chunk where the currKey was removed from\n+      Iterator<ImmutablePair<byte[], int[]>> iterator = chunkIterators.get(poll.getLeft());\n+      if (iterator.hasNext()) {\n+        queue.offer(ImmutablePair.of(poll.getLeft(), iterator.next()));\n+      }\n+    }\n+\n+    if (prevKey != null) {\n+      writer.add(prevKey, roaringBitmap);\n+    }\n+    writer.finish();\n+    return writer.getMaxDictionaryValueLength();\n+  }\n+\n+  private void flush()\n+      throws IOException {\n+    //write the key (length|actual bytes) - posting list(length, flattenedDocIds)\n+    System.out.println(\"postingListMap = \" + postingListMap);\n+    for (Map.Entry<String, List<Integer>> entry : postingListMap.entrySet()) {\n+      byte[] keyBytes = entry.getKey().getBytes(Charset.forName(\"UTF-8\"));\n+      postingListWriter.writeInt(keyBytes.length);\n+      postingListWriter.write(keyBytes);\n+      List<Integer> flattenedDocIdList = entry.getValue();\n+      postingListWriter.writeInt(flattenedDocIdList.size());\n+      for (int flattenedDocId : flattenedDocIdList) {\n+        postingListWriter.writeInt(flattenedDocId);\n+      }\n+    }\n+\n+    //write flattened doc id to root docId mapping\n+    for (int rootDocId : flattenedDocIdList) {\n+      flattenedDocId2RootDocIdWriter.writeInt(rootDocId);\n+    }\n+    chunkLengths.add(postingListMap.size());\n+    postingListChunkOffsets.add(postingListWriter.size());\n+    postingListMap.clear();\n+    flattenedDocIdList.clear();\n+  }\n+\n+  private static List<Map<String, String>> unnestJson(JsonNode root) {\n+    Iterator<Map.Entry<String, JsonNode>> fields = root.fields();\n+    Map<String, String> flattenedSingleValuesMap = new TreeMap<>();\n+    Map<String, JsonNode> arrNodes = new TreeMap<>();\n+    Map<String, JsonNode> objectNodes = new TreeMap<>();\n+    List<Map<String, String>> resultList = new ArrayList<>();\n+    List<Map<String, String>> tempResultList = new ArrayList<>();\n+    while (fields.hasNext()) {\n+      Map.Entry<String, JsonNode> child = fields.next();\n+      if (child.getValue().isValueNode()) {\n+        //Normal value node\n+        flattenedSingleValuesMap.put(child.getKey(), child.getValue().asText());\n+      } else if (child.getValue().isArray()) {\n+        //Array Node: Process these nodes later\n+        arrNodes.put(child.getKey(), child.getValue());\n+      } else {\n+        //Object Node\n+        objectNodes.put(child.getKey(), child.getValue());\n+      }\n+    }\n+    for (String objectNodeKey : objectNodes.keySet()) {\n+      JsonNode objectNode = objectNodes.get(objectNodeKey);\n+      modifyKeysInMap(flattenedSingleValuesMap, tempResultList, objectNodeKey, objectNode);\n+    }\n+    if (tempResultList.isEmpty()) {\n+      tempResultList.add(flattenedSingleValuesMap);\n+    }\n+    if (!arrNodes.isEmpty()) {\n+      for (Map<String, String> flattenedMapElement : tempResultList) {\n+        for (String arrNodeKey : arrNodes.keySet()) {\n+          JsonNode arrNode = arrNodes.get(arrNodeKey);\n+          for (JsonNode arrNodeElement : arrNode) {\n+            modifyKeysInMap(flattenedMapElement, resultList, arrNodeKey, arrNodeElement);\n+          }\n+        }\n+      }\n+    } else {\n+      resultList.addAll(tempResultList);\n+    }\n+    return resultList;\n+  }\n+\n+  private static void modifyKeysInMap(Map<String, String> flattenedMap, List<Map<String, String>> resultList,\n+      String arrNodeKey, JsonNode arrNode) {\n+    List<Map<String, String>> objectResult = unnestJson(arrNode);\n+    for (Map<String, String> flattenedObject : objectResult) {\n+      Map<String, String> flattenedObjectCopy = new TreeMap<>(flattenedMap);\n+      for (Map.Entry<String, String> entry : flattenedObject.entrySet()) {\n+        flattenedObjectCopy.put(arrNodeKey + \".\" + entry.getKey(), entry.getValue());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d"}, "originalPosition": 455}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1OTc4NA==", "bodyText": "why do not support number type?", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515659784", "createdAt": "2020-11-01T19:12:14Z", "author": {"login": "yupeng9"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/index/loader/invertedindex/JSONIndexHandler.java", "diffHunk": "@@ -0,0 +1,216 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.index.loader.invertedindex;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.HashSet;\n+import java.util.Set;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.core.indexsegment.generator.SegmentVersion;\n+import org.apache.pinot.core.segment.creator.impl.inv.JSONIndexCreator;\n+import org.apache.pinot.core.segment.index.column.PhysicalColumnIndexContainer;\n+import org.apache.pinot.core.segment.index.loader.IndexLoadingConfig;\n+import org.apache.pinot.core.segment.index.loader.LoaderUtils;\n+import org.apache.pinot.core.segment.index.metadata.ColumnMetadata;\n+import org.apache.pinot.core.segment.index.metadata.SegmentMetadataImpl;\n+import org.apache.pinot.core.segment.index.readers.BaseImmutableDictionary;\n+import org.apache.pinot.core.segment.index.readers.Dictionary;\n+import org.apache.pinot.core.segment.index.readers.ForwardIndexReader;\n+import org.apache.pinot.core.segment.index.readers.ForwardIndexReaderContext;\n+import org.apache.pinot.core.segment.index.readers.forward.FixedBitMVForwardIndexReader;\n+import org.apache.pinot.core.segment.index.readers.forward.FixedBitSVForwardIndexReader;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+import org.apache.pinot.core.segment.store.ColumnIndexType;\n+import org.apache.pinot.core.segment.store.SegmentDirectory;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.pinot.core.segment.creator.impl.V1Constants.Indexes.JSON_INDEX_FILE_EXTENSION;\n+\n+\n+@SuppressWarnings({\"rawtypes\", \"unchecked\"})\n+public class JSONIndexHandler {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(JSONIndexHandler.class);\n+\n+  private final File _indexDir;\n+  private final SegmentDirectory.Writer _segmentWriter;\n+  private final String _segmentName;\n+  private final SegmentVersion _segmentVersion;\n+  private final Set<ColumnMetadata> _jsonIndexColumns = new HashSet<>();\n+\n+  public JSONIndexHandler(File indexDir, SegmentMetadataImpl segmentMetadata, IndexLoadingConfig indexLoadingConfig,\n+      SegmentDirectory.Writer segmentWriter) {\n+    _indexDir = indexDir;\n+    _segmentWriter = segmentWriter;\n+    _segmentName = segmentMetadata.getName();\n+    _segmentVersion = SegmentVersion.valueOf(segmentMetadata.getVersion());\n+\n+    // Only create json index on dictionary-encoded unsorted columns\n+    for (String column : indexLoadingConfig.getJsonIndexColumns()) {\n+      ColumnMetadata columnMetadata = segmentMetadata.getColumnMetadataFor(column);\n+      if (columnMetadata != null && !columnMetadata.isSorted()) {\n+        _jsonIndexColumns.add(columnMetadata);\n+      }\n+    }\n+  }\n+\n+  public void createJsonIndices()\n+      throws IOException {\n+    for (ColumnMetadata columnMetadata : _jsonIndexColumns) {\n+      createJSONIndexForColumn(columnMetadata);\n+    }\n+  }\n+\n+  private void createJSONIndexForColumn(ColumnMetadata columnMetadata)\n+      throws IOException {\n+    String column = columnMetadata.getColumnName();\n+\n+    File inProgress = new File(_indexDir, column + JSON_INDEX_FILE_EXTENSION + \".inprogress\");\n+    File jsonIndexFile = new File(_indexDir, column + JSON_INDEX_FILE_EXTENSION);\n+\n+    if (!inProgress.exists()) {\n+      // Marker file does not exist, which means last run ended normally.\n+\n+      if (_segmentWriter.hasIndexFor(column, ColumnIndexType.JSON_INDEX)) {\n+        // Skip creating json index if already exists.\n+\n+        LOGGER.info(\"Found json index for segment: {}, column: {}\", _segmentName, column);\n+        return;\n+      }\n+\n+      // Create a marker file.\n+      FileUtils.touch(inProgress);\n+    } else {\n+      // Marker file exists, which means last run gets interrupted.\n+\n+      // Remove json index if exists.\n+      // For v1 and v2, it's the actual json index. For v3, it's the temporary json index.\n+      FileUtils.deleteQuietly(jsonIndexFile);\n+    }\n+\n+    // Create new json index for the column.\n+    LOGGER.info(\"Creating new json index for segment: {}, column: {}\", _segmentName, column);\n+    if (columnMetadata.hasDictionary()) {\n+      handleDictionaryBasedColumn(columnMetadata);\n+    } else {\n+      handleNonDictionaryBasedColumn(columnMetadata);\n+    }\n+\n+    // For v3, write the generated json index file into the single file and remove it.\n+    if (_segmentVersion == SegmentVersion.v3) {\n+      LoaderUtils.writeIndexToV3Format(_segmentWriter, column, jsonIndexFile, ColumnIndexType.JSON_INDEX);\n+    }\n+\n+    // Delete the marker file.\n+    FileUtils.deleteQuietly(inProgress);\n+\n+    LOGGER.info(\"Created json index for segment: {}, column: {}\", _segmentName, column);\n+  }\n+\n+  private void handleDictionaryBasedColumn(ColumnMetadata columnMetadata)\n+      throws IOException {\n+    int numDocs = columnMetadata.getTotalDocs();\n+    try (ForwardIndexReader forwardIndexReader = getForwardIndexReader(columnMetadata, _segmentWriter);\n+        ForwardIndexReaderContext readerContext = forwardIndexReader.createContext();\n+        Dictionary dictionary = getDictionaryReader(columnMetadata, _segmentWriter);\n+        JSONIndexCreator jsonIndexCreator = new JSONIndexCreator(_indexDir, columnMetadata.getFieldSpec())) {\n+      if (columnMetadata.isSingleValue()) {\n+        switch (columnMetadata.getDataType()) {\n+          case STRING:\n+            for (int i = 0; i < numDocs; i++) {\n+              int dictId = forwardIndexReader.getDictId(i, readerContext);\n+              jsonIndexCreator.add(dictionary.getStringValue(dictId).getBytes(\"UTF-8\"));\n+            }\n+            break;\n+          case BYTES:\n+            // Single-value column\n+            for (int i = 0; i < numDocs; i++) {\n+              int dictId = forwardIndexReader.getDictId(i, readerContext);\n+              jsonIndexCreator.add(dictionary.getBytesValue(dictId));\n+            }\n+            break;\n+          default:\n+            throw new IllegalStateException(\"Unsupported data type: \" + columnMetadata.getDataType());\n+        }\n+      } else {\n+        // Multi-value column\n+        throw new IllegalStateException(\"JSON Indexing is not supported on multi-valued columns \");\n+      }\n+      jsonIndexCreator.seal();\n+    }\n+  }\n+\n+  private void handleNonDictionaryBasedColumn(ColumnMetadata columnMetadata)\n+      throws IOException {\n+    FieldSpec.DataType dataType = columnMetadata.getDataType();\n+    if (dataType != FieldSpec.DataType.BYTES || dataType != FieldSpec.DataType.STRING) {\n+      throw new UnsupportedOperationException(\n+          \"JSON indexing is only supported for STRING/BYTES datatype but found: \" + dataType);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d"}, "originalPosition": 166}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1OTk1Ng==", "bodyText": "can you explain why special handling of one id?", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515659956", "createdAt": "2020-11-01T19:14:01Z", "author": {"login": "yupeng9"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/index/readers/JSONIndexReader.java", "diffHunk": "@@ -0,0 +1,160 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.index.readers;\n+\n+import com.google.common.base.Preconditions;\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import org.apache.calcite.sql.parser.SqlParseException;\n+import org.apache.pinot.common.utils.StringUtil;\n+import org.apache.pinot.core.operator.blocks.FilterBlock;\n+import org.apache.pinot.core.operator.docidsets.BitmapDocIdSet;\n+import org.apache.pinot.core.operator.filter.BitmapBasedFilterOperator;\n+import org.apache.pinot.core.operator.filter.predicate.PredicateEvaluator;\n+import org.apache.pinot.core.operator.filter.predicate.PredicateEvaluatorProvider;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.QueryContextConverterUtils;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.sql.parsers.CalciteSqlParser;\n+import org.roaringbitmap.buffer.ImmutableRoaringBitmap;\n+import org.roaringbitmap.buffer.MutableRoaringBitmap;\n+\n+\n+public class JSONIndexReader implements Closeable {\n+\n+  private static int EXPECTED_VERSION = 1;\n+  private static int DICT_HEADER_INDEX = 0;\n+  private static int DICT_OFFSET_INDEX = 1;\n+  private static int DICT_DATA_INDEX = 2;\n+  private static int INV_OFFSET_INDEX = 3;\n+  private static int INV_DATA_INDEX = 4;\n+  private static int FLATTENED_2_ROOT_INDEX = 5;\n+\n+  private final BitmapInvertedIndexReader invertedIndexReader;\n+  private final StringDictionary dictionary;\n+  private final long cardinality;\n+  private final long numFlattenedDocs;\n+  private final PinotDataBuffer flattened2RootDocIdBuffer;\n+\n+  public JSONIndexReader(PinotDataBuffer pinotDataBuffer) {\n+\n+    int version = pinotDataBuffer.getInt(0);\n+    int maxKeyLength = pinotDataBuffer.getInt(1 * Integer.BYTES);\n+\n+    Preconditions.checkState(version == EXPECTED_VERSION, String\n+        .format(\"Index version:{} is not supported by this reader. expected version:{}\", version, EXPECTED_VERSION));\n+\n+    // dictionaryHeaderFile, dictionaryOffsetFile, dictionaryFile, invIndexOffsetFile, invIndexFile, FlattenedDocId2DocIdMappingFile\n+    int numBuffers = 6;\n+    long bufferStartOffsets[] = new long[numBuffers];\n+    long bufferSizeArray[] = new long[numBuffers];\n+    for (int i = 0; i < numBuffers; i++) {\n+      bufferStartOffsets[i] = pinotDataBuffer.getLong(2 * Integer.BYTES + 2 * i * Long.BYTES);\n+      bufferSizeArray[i] = pinotDataBuffer.getLong(2 * Integer.BYTES + 2 * i * Long.BYTES + Long.BYTES);\n+    }\n+    cardinality = bufferSizeArray[DICT_OFFSET_INDEX] / Integer.BYTES - 1;\n+\n+    long dictionaryStartOffset = bufferStartOffsets[DICT_HEADER_INDEX];\n+    long dictionarySize =\n+        bufferSizeArray[DICT_HEADER_INDEX] + bufferSizeArray[DICT_OFFSET_INDEX] + bufferSizeArray[DICT_DATA_INDEX];\n+\n+    //TODO: REMOVE DEBUG START\n+    byte[] dictHeaderBytes = new byte[(int) bufferSizeArray[DICT_HEADER_INDEX]];\n+    pinotDataBuffer.copyTo(bufferStartOffsets[DICT_HEADER_INDEX], dictHeaderBytes);\n+    System.out.println(\"Arrays.toString(dictHeaderBytes) = \" + Arrays.toString(dictHeaderBytes));\n+    //TODO: REMOVE DEBUG  END\n+\n+    PinotDataBuffer dictionaryBuffer =\n+        pinotDataBuffer.view(dictionaryStartOffset, dictionaryStartOffset + dictionarySize);\n+    dictionary = new StringDictionary(dictionaryBuffer, (int) cardinality, maxKeyLength, Byte.valueOf(\"0\"));\n+\n+    long invIndexStartOffset = bufferStartOffsets[INV_OFFSET_INDEX];\n+    long invIndexSize = bufferSizeArray[INV_OFFSET_INDEX] + bufferSizeArray[INV_DATA_INDEX];\n+\n+    PinotDataBuffer invIndexBuffer = pinotDataBuffer.view(invIndexStartOffset, invIndexStartOffset + invIndexSize);\n+    invertedIndexReader = new BitmapInvertedIndexReader(invIndexBuffer, (int) cardinality);\n+\n+    //TODO: REMOVE DEBUG START\n+    for (int dictId = 0; dictId < dictionary.length(); dictId++) {\n+      System.out.println(\"Key = \" + new String(dictionary.getBytes(dictId)));\n+      System.out.println(\"Posting List = \" + invertedIndexReader.getDocIds(dictId));\n+    }\n+    //TODO: REMOVE DEBUG  END\n+\n+    long flattened2RootDocIdStartOffset = bufferStartOffsets[FLATTENED_2_ROOT_INDEX];\n+    long flattened2RootDocIdBufferSize = bufferSizeArray[FLATTENED_2_ROOT_INDEX];\n+    numFlattenedDocs = bufferSizeArray[FLATTENED_2_ROOT_INDEX] / Integer.BYTES;\n+    flattened2RootDocIdBuffer = pinotDataBuffer.view(flattened2RootDocIdStartOffset, flattened2RootDocIdStartOffset + flattened2RootDocIdBufferSize);\n+    //TODO: REMOVE DEBUG START\n+    for (int i = 0; i < numFlattenedDocs; i++) {\n+      System.out.println(\"flattenedDocId: \" + i + \" rootDodId:\" + flattened2RootDocIdBuffer.getInt(i * Integer.BYTES));\n+    }\n+    //TODO: REMOVE DEBUG  END\n+  }\n+\n+  /**\n+   * Returns the matching document ids for the given search query.\n+   */\n+  public MutableRoaringBitmap getMatchingDocIds(Predicate predicate) {\n+\n+    PredicateEvaluator predicateEvaluator =\n+        PredicateEvaluatorProvider.getPredicateEvaluator(predicate, dictionary, FieldSpec.DataType.BYTES);\n+    boolean exclusive = predicateEvaluator.isExclusive();\n+    int[] dictIds = exclusive ? predicateEvaluator.getNonMatchingDictIds() : predicateEvaluator.getMatchingDictIds();\n+    int numDictIds = dictIds.length;\n+\n+    if (numDictIds == 1) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d"}, "originalPosition": 125}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2MDA3MA==", "bodyText": "also need to ramp up test coverage of this class", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515660070", "createdAt": "2020-11-01T19:14:59Z", "author": {"login": "yupeng9"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/creator/impl/inv/JSONIndexCreator.java", "diffHunk": "@@ -0,0 +1,658 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.creator.impl.inv;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.github.wnameless.json.flattener.JsonFlattener;\n+import com.google.common.io.Files;\n+import com.google.common.primitives.UnsignedBytes;\n+import java.io.BufferedInputStream;\n+import java.io.BufferedOutputStream;\n+import java.io.Closeable;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.TreeMap;\n+import java.util.stream.Collectors;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.commons.lang3.tuple.ImmutablePair;\n+import org.apache.pinot.core.common.BlockDocIdIterator;\n+import org.apache.pinot.core.io.util.VarLengthBytesValueReaderWriter;\n+import org.apache.pinot.core.operator.blocks.FilterBlock;\n+import org.apache.pinot.core.operator.filter.JSONMatchFilterOperator;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.predicate.EqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.QueryContextConverterUtils;\n+import org.apache.pinot.core.segment.index.readers.JSONIndexReader;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+import org.apache.pinot.spi.data.DimensionFieldSpec;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.sql.parsers.CalciteSqlParser;\n+import org.roaringbitmap.RoaringBitmap;\n+import org.roaringbitmap.buffer.MutableRoaringBitmap;\n+\n+import static org.apache.pinot.core.segment.creator.impl.V1Constants.Indexes.JSON_INDEX_FILE_EXTENSION;\n+\n+\n+public class JSONIndexCreator implements Closeable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1Nzk5MQ=="}, "originalCommit": {"oid": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d"}, "originalPosition": 69}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIxMjU5MzU5", "url": "https://github.com/apache/pinot/pull/6216#pullrequestreview-521259359", "createdAt": "2020-11-01T20:44:01Z", "commit": {"oid": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d"}, "state": "COMMENTED", "comments": {"totalCount": 19, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMVQyMDo0NDowMVrOHrx-iw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMVQyMToxNTowM1rOHryLdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2OTY0Mw==", "bodyText": "Should we support this with PQL? It is ok to do so, as long as it does not create additional non-trivial work for SQL migration.", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515669643", "createdAt": "2020-11-01T20:44:01Z", "author": {"login": "mayankshriv"}, "path": "pinot-common/src/main/antlr4/org/apache/pinot/pql/parsers/PQL2.g4", "diffHunk": "@@ -78,6 +78,7 @@ predicate:\n   | isClause                              # IsPredicate\n   | regexpLikeClause                      # RegexpLikePredicate\n   | textMatchClause                       # TextMatchPredicate\n+  | jsonMatchClause                       # JsonMatchPredicate", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2OTc3MA==", "bodyText": "Any reason to use a different version of Thrift compiler?", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515669770", "createdAt": "2020-11-01T20:45:02Z", "author": {"login": "mayankshriv"}, "path": "pinot-common/src/main/java/org/apache/pinot/common/request/AggregationInfo.java", "diffHunk": "@@ -17,7 +17,7 @@\n  * under the License.\n  */\n /**\n- * Autogenerated by Thrift Compiler (0.12.0)\n+ * Autogenerated by Thrift Compiler (0.13.0)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MDQ1NQ==", "bodyText": "Good dependency to take?", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515670455", "createdAt": "2020-11-01T20:51:32Z", "author": {"login": "mayankshriv"}, "path": "pinot-core/pom.xml", "diffHunk": "@@ -163,6 +163,11 @@\n       <groupId>com.jayway.jsonpath</groupId>\n       <artifactId>json-path</artifactId>\n     </dependency>\n+    <dependency>\n+      <groupId>com.github.wnameless.json</groupId>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MDg3Ng==", "bodyText": "Why do these need to become public?", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515670876", "createdAt": "2020-11-01T20:55:58Z", "author": {"login": "mayankshriv"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/io/util/VarLengthBytesValueReaderWriter.java", "diffHunk": "@@ -63,19 +66,19 @@\n   /**\n    * Magic bytes used to identify the dictionary files written in variable length bytes format.\n    */\n-  private static final byte[] MAGIC_BYTES = StringUtil.encodeUtf8(\".vl;\");\n+  public static final byte[] MAGIC_BYTES = StringUtil.encodeUtf8(\".vl;\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MDk5MA==", "bodyText": "Where is this used?", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515670990", "createdAt": "2020-11-01T20:57:06Z", "author": {"login": "mayankshriv"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/io/util/VarLengthBytesValueReaderWriter.java", "diffHunk": "@@ -144,6 +147,19 @@ public static boolean isVarLengthBytesDictBuffer(PinotDataBuffer buffer) {\n     return false;\n   }\n \n+  public static byte[] getHeaderBytes(int numElements)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MTEwMA==", "bodyText": "+1 for passing InvertedIndexReader instead of DataSource.", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515671100", "createdAt": "2020-11-01T20:58:44Z", "author": {"login": "mayankshriv"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/operator/filter/BitmapBasedFilterOperator.java", "diffHunk": "@@ -38,9 +38,9 @@\n   private final boolean _exclusive;\n   private final int _numDocs;\n \n-  BitmapBasedFilterOperator(PredicateEvaluator predicateEvaluator, DataSource dataSource, int numDocs) {\n+  public BitmapBasedFilterOperator(PredicateEvaluator predicateEvaluator, InvertedIndexReader invertedIndexReader, int numDocs) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MTIyNQ==", "bodyText": "Add javadoc?", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515671225", "createdAt": "2020-11-01T20:59:21Z", "author": {"login": "mayankshriv"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/operator/filter/JSONMatchFilterOperator.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.operator.filter;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.apache.pinot.core.operator.blocks.FilterBlock;\n+import org.apache.pinot.core.operator.docidsets.BitmapDocIdSet;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.predicate.EqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.InPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.NotEqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.NotInPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.segment.creator.impl.inv.JSONIndexCreator;\n+import org.apache.pinot.core.segment.index.readers.JSONIndexReader;\n+import org.roaringbitmap.buffer.ImmutableRoaringBitmap;\n+import org.roaringbitmap.buffer.MutableRoaringBitmap;\n+\n+\n+@SuppressWarnings(\"rawtypes\")\n+public class JSONMatchFilterOperator extends BaseFilterOperator {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MTc4Nw==", "bodyText": "+1 to both above.", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515671787", "createdAt": "2020-11-01T21:04:40Z", "author": {"login": "mayankshriv"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/creator/impl/inv/JSONIndexCreator.java", "diffHunk": "@@ -0,0 +1,658 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.creator.impl.inv;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.github.wnameless.json.flattener.JsonFlattener;\n+import com.google.common.io.Files;\n+import com.google.common.primitives.UnsignedBytes;\n+import java.io.BufferedInputStream;\n+import java.io.BufferedOutputStream;\n+import java.io.Closeable;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.TreeMap;\n+import java.util.stream.Collectors;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.commons.lang3.tuple.ImmutablePair;\n+import org.apache.pinot.core.common.BlockDocIdIterator;\n+import org.apache.pinot.core.io.util.VarLengthBytesValueReaderWriter;\n+import org.apache.pinot.core.operator.blocks.FilterBlock;\n+import org.apache.pinot.core.operator.filter.JSONMatchFilterOperator;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.predicate.EqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.QueryContextConverterUtils;\n+import org.apache.pinot.core.segment.index.readers.JSONIndexReader;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+import org.apache.pinot.spi.data.DimensionFieldSpec;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.sql.parsers.CalciteSqlParser;\n+import org.roaringbitmap.RoaringBitmap;\n+import org.roaringbitmap.buffer.MutableRoaringBitmap;\n+\n+import static org.apache.pinot.core.segment.creator.impl.V1Constants.Indexes.JSON_INDEX_FILE_EXTENSION;\n+\n+\n+public class JSONIndexCreator implements Closeable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1Nzk5MQ=="}, "originalCommit": {"oid": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MTgyMw==", "bodyText": "private?", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515671823", "createdAt": "2020-11-01T21:05:01Z", "author": {"login": "mayankshriv"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/creator/impl/inv/JSONIndexCreator.java", "diffHunk": "@@ -0,0 +1,658 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.creator.impl.inv;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.github.wnameless.json.flattener.JsonFlattener;\n+import com.google.common.io.Files;\n+import com.google.common.primitives.UnsignedBytes;\n+import java.io.BufferedInputStream;\n+import java.io.BufferedOutputStream;\n+import java.io.Closeable;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.TreeMap;\n+import java.util.stream.Collectors;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.commons.lang3.tuple.ImmutablePair;\n+import org.apache.pinot.core.common.BlockDocIdIterator;\n+import org.apache.pinot.core.io.util.VarLengthBytesValueReaderWriter;\n+import org.apache.pinot.core.operator.blocks.FilterBlock;\n+import org.apache.pinot.core.operator.filter.JSONMatchFilterOperator;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.predicate.EqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.QueryContextConverterUtils;\n+import org.apache.pinot.core.segment.index.readers.JSONIndexReader;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+import org.apache.pinot.spi.data.DimensionFieldSpec;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.sql.parsers.CalciteSqlParser;\n+import org.roaringbitmap.RoaringBitmap;\n+import org.roaringbitmap.buffer.MutableRoaringBitmap;\n+\n+import static org.apache.pinot.core.segment.creator.impl.V1Constants.Indexes.JSON_INDEX_FILE_EXTENSION;\n+\n+\n+public class JSONIndexCreator implements Closeable {\n+\n+  //separator used to join the key and value to create posting list key\n+  public static String POSTING_LIST_KEY_SEPARATOR = \"|\";\n+  static int FLUSH_THRESHOLD = 50_000;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MjAyNw==", "bodyText": "Static finals for _dictionaryOffset.buf, and others?", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515672027", "createdAt": "2020-11-01T21:06:19Z", "author": {"login": "mayankshriv"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/creator/impl/inv/JSONIndexCreator.java", "diffHunk": "@@ -0,0 +1,658 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.creator.impl.inv;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.github.wnameless.json.flattener.JsonFlattener;\n+import com.google.common.io.Files;\n+import com.google.common.primitives.UnsignedBytes;\n+import java.io.BufferedInputStream;\n+import java.io.BufferedOutputStream;\n+import java.io.Closeable;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.TreeMap;\n+import java.util.stream.Collectors;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.commons.lang3.tuple.ImmutablePair;\n+import org.apache.pinot.core.common.BlockDocIdIterator;\n+import org.apache.pinot.core.io.util.VarLengthBytesValueReaderWriter;\n+import org.apache.pinot.core.operator.blocks.FilterBlock;\n+import org.apache.pinot.core.operator.filter.JSONMatchFilterOperator;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.predicate.EqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.QueryContextConverterUtils;\n+import org.apache.pinot.core.segment.index.readers.JSONIndexReader;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+import org.apache.pinot.spi.data.DimensionFieldSpec;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.sql.parsers.CalciteSqlParser;\n+import org.roaringbitmap.RoaringBitmap;\n+import org.roaringbitmap.buffer.MutableRoaringBitmap;\n+\n+import static org.apache.pinot.core.segment.creator.impl.V1Constants.Indexes.JSON_INDEX_FILE_EXTENSION;\n+\n+\n+public class JSONIndexCreator implements Closeable {\n+\n+  //separator used to join the key and value to create posting list key\n+  public static String POSTING_LIST_KEY_SEPARATOR = \"|\";\n+  static int FLUSH_THRESHOLD = 50_000;\n+  static int VERSION = 1;\n+  private final File flattenedDocId2RootDocIdMappingFile;\n+  private final File postingListFile;\n+  private File dictionaryheaderFile;\n+  private File dictionaryOffsetFile;\n+  private File dictionaryFile;\n+  private File invertedIndexOffsetFile;\n+  private File invertedIndexFile;\n+  private File outputIndexFile;\n+\n+  private int docId = 0;\n+  private int numFlatennedDocId = 0;\n+  int chunkId = 0;\n+\n+  private DataOutputStream postingListWriter;\n+  private DataOutputStream flattenedDocId2RootDocIdWriter;\n+\n+  Map<String, List<Integer>> postingListMap = new TreeMap<>();\n+  List<Integer> flattenedDocIdList = new ArrayList<>();\n+  List<Integer> postingListChunkOffsets = new ArrayList<>();\n+  List<Integer> chunkLengths = new ArrayList<>();\n+  private FieldSpec fieldSpec;\n+\n+  public JSONIndexCreator(File indexDir, FieldSpec fieldSpec)\n+      throws IOException {\n+    this.fieldSpec = fieldSpec;\n+    System.out.println(\"indexDir = \" + indexDir);\n+\n+    String name = fieldSpec.getName();\n+    postingListFile = new File(indexDir + name + \"_postingList.buf\");\n+    postingListWriter = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(postingListFile)));\n+    postingListChunkOffsets.add(postingListWriter.size());\n+\n+    dictionaryheaderFile = new File(indexDir, name + \"_dictionaryHeader.buf\");\n+    dictionaryOffsetFile = new File(indexDir, name + \"_dictionaryOffset.buf\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d"}, "originalPosition": 108}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MjE0OQ==", "bodyText": "Reuse new ObjectMapper() as static final?", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515672149", "createdAt": "2020-11-01T21:07:04Z", "author": {"login": "mayankshriv"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/creator/impl/inv/JSONIndexCreator.java", "diffHunk": "@@ -0,0 +1,658 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.creator.impl.inv;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.github.wnameless.json.flattener.JsonFlattener;\n+import com.google.common.io.Files;\n+import com.google.common.primitives.UnsignedBytes;\n+import java.io.BufferedInputStream;\n+import java.io.BufferedOutputStream;\n+import java.io.Closeable;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.TreeMap;\n+import java.util.stream.Collectors;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.commons.lang3.tuple.ImmutablePair;\n+import org.apache.pinot.core.common.BlockDocIdIterator;\n+import org.apache.pinot.core.io.util.VarLengthBytesValueReaderWriter;\n+import org.apache.pinot.core.operator.blocks.FilterBlock;\n+import org.apache.pinot.core.operator.filter.JSONMatchFilterOperator;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.predicate.EqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.QueryContextConverterUtils;\n+import org.apache.pinot.core.segment.index.readers.JSONIndexReader;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+import org.apache.pinot.spi.data.DimensionFieldSpec;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.sql.parsers.CalciteSqlParser;\n+import org.roaringbitmap.RoaringBitmap;\n+import org.roaringbitmap.buffer.MutableRoaringBitmap;\n+\n+import static org.apache.pinot.core.segment.creator.impl.V1Constants.Indexes.JSON_INDEX_FILE_EXTENSION;\n+\n+\n+public class JSONIndexCreator implements Closeable {\n+\n+  //separator used to join the key and value to create posting list key\n+  public static String POSTING_LIST_KEY_SEPARATOR = \"|\";\n+  static int FLUSH_THRESHOLD = 50_000;\n+  static int VERSION = 1;\n+  private final File flattenedDocId2RootDocIdMappingFile;\n+  private final File postingListFile;\n+  private File dictionaryheaderFile;\n+  private File dictionaryOffsetFile;\n+  private File dictionaryFile;\n+  private File invertedIndexOffsetFile;\n+  private File invertedIndexFile;\n+  private File outputIndexFile;\n+\n+  private int docId = 0;\n+  private int numFlatennedDocId = 0;\n+  int chunkId = 0;\n+\n+  private DataOutputStream postingListWriter;\n+  private DataOutputStream flattenedDocId2RootDocIdWriter;\n+\n+  Map<String, List<Integer>> postingListMap = new TreeMap<>();\n+  List<Integer> flattenedDocIdList = new ArrayList<>();\n+  List<Integer> postingListChunkOffsets = new ArrayList<>();\n+  List<Integer> chunkLengths = new ArrayList<>();\n+  private FieldSpec fieldSpec;\n+\n+  public JSONIndexCreator(File indexDir, FieldSpec fieldSpec)\n+      throws IOException {\n+    this.fieldSpec = fieldSpec;\n+    System.out.println(\"indexDir = \" + indexDir);\n+\n+    String name = fieldSpec.getName();\n+    postingListFile = new File(indexDir + name + \"_postingList.buf\");\n+    postingListWriter = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(postingListFile)));\n+    postingListChunkOffsets.add(postingListWriter.size());\n+\n+    dictionaryheaderFile = new File(indexDir, name + \"_dictionaryHeader.buf\");\n+    dictionaryOffsetFile = new File(indexDir, name + \"_dictionaryOffset.buf\");\n+    dictionaryFile = new File(indexDir, name + \"_dictionary.buf\");\n+    invertedIndexOffsetFile = new File(indexDir, name + \"_invertedIndexOffset.buf\");\n+    invertedIndexFile = new File(indexDir, name + \"_invertedIndex.buf\");\n+    flattenedDocId2RootDocIdMappingFile = new File(indexDir, name + \"_flattenedDocId.buf\");\n+    flattenedDocId2RootDocIdWriter =\n+        new DataOutputStream(new BufferedOutputStream(new FileOutputStream(flattenedDocId2RootDocIdMappingFile)));\n+\n+    //output file\n+    outputIndexFile = new File(indexDir, name + JSON_INDEX_FILE_EXTENSION);\n+  }\n+\n+  public void add(byte[] data)\n+      throws IOException {\n+\n+    JsonNode jsonNode = new ObjectMapper().readTree(data);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d"}, "originalPosition": 123}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MjIzMg==", "bodyText": "Same here, for new ObjectMapper().", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515672232", "createdAt": "2020-11-01T21:08:00Z", "author": {"login": "mayankshriv"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/creator/impl/inv/JSONIndexCreator.java", "diffHunk": "@@ -0,0 +1,658 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.creator.impl.inv;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.github.wnameless.json.flattener.JsonFlattener;\n+import com.google.common.io.Files;\n+import com.google.common.primitives.UnsignedBytes;\n+import java.io.BufferedInputStream;\n+import java.io.BufferedOutputStream;\n+import java.io.Closeable;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.TreeMap;\n+import java.util.stream.Collectors;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.commons.lang3.tuple.ImmutablePair;\n+import org.apache.pinot.core.common.BlockDocIdIterator;\n+import org.apache.pinot.core.io.util.VarLengthBytesValueReaderWriter;\n+import org.apache.pinot.core.operator.blocks.FilterBlock;\n+import org.apache.pinot.core.operator.filter.JSONMatchFilterOperator;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.predicate.EqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.QueryContextConverterUtils;\n+import org.apache.pinot.core.segment.index.readers.JSONIndexReader;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+import org.apache.pinot.spi.data.DimensionFieldSpec;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.sql.parsers.CalciteSqlParser;\n+import org.roaringbitmap.RoaringBitmap;\n+import org.roaringbitmap.buffer.MutableRoaringBitmap;\n+\n+import static org.apache.pinot.core.segment.creator.impl.V1Constants.Indexes.JSON_INDEX_FILE_EXTENSION;\n+\n+\n+public class JSONIndexCreator implements Closeable {\n+\n+  //separator used to join the key and value to create posting list key\n+  public static String POSTING_LIST_KEY_SEPARATOR = \"|\";\n+  static int FLUSH_THRESHOLD = 50_000;\n+  static int VERSION = 1;\n+  private final File flattenedDocId2RootDocIdMappingFile;\n+  private final File postingListFile;\n+  private File dictionaryheaderFile;\n+  private File dictionaryOffsetFile;\n+  private File dictionaryFile;\n+  private File invertedIndexOffsetFile;\n+  private File invertedIndexFile;\n+  private File outputIndexFile;\n+\n+  private int docId = 0;\n+  private int numFlatennedDocId = 0;\n+  int chunkId = 0;\n+\n+  private DataOutputStream postingListWriter;\n+  private DataOutputStream flattenedDocId2RootDocIdWriter;\n+\n+  Map<String, List<Integer>> postingListMap = new TreeMap<>();\n+  List<Integer> flattenedDocIdList = new ArrayList<>();\n+  List<Integer> postingListChunkOffsets = new ArrayList<>();\n+  List<Integer> chunkLengths = new ArrayList<>();\n+  private FieldSpec fieldSpec;\n+\n+  public JSONIndexCreator(File indexDir, FieldSpec fieldSpec)\n+      throws IOException {\n+    this.fieldSpec = fieldSpec;\n+    System.out.println(\"indexDir = \" + indexDir);\n+\n+    String name = fieldSpec.getName();\n+    postingListFile = new File(indexDir + name + \"_postingList.buf\");\n+    postingListWriter = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(postingListFile)));\n+    postingListChunkOffsets.add(postingListWriter.size());\n+\n+    dictionaryheaderFile = new File(indexDir, name + \"_dictionaryHeader.buf\");\n+    dictionaryOffsetFile = new File(indexDir, name + \"_dictionaryOffset.buf\");\n+    dictionaryFile = new File(indexDir, name + \"_dictionary.buf\");\n+    invertedIndexOffsetFile = new File(indexDir, name + \"_invertedIndexOffset.buf\");\n+    invertedIndexFile = new File(indexDir, name + \"_invertedIndex.buf\");\n+    flattenedDocId2RootDocIdMappingFile = new File(indexDir, name + \"_flattenedDocId.buf\");\n+    flattenedDocId2RootDocIdWriter =\n+        new DataOutputStream(new BufferedOutputStream(new FileOutputStream(flattenedDocId2RootDocIdMappingFile)));\n+\n+    //output file\n+    outputIndexFile = new File(indexDir, name + JSON_INDEX_FILE_EXTENSION);\n+  }\n+\n+  public void add(byte[] data)\n+      throws IOException {\n+\n+    JsonNode jsonNode = new ObjectMapper().readTree(data);\n+    List<Map<String, String>> flattenedMapList = unnestJson(jsonNode);\n+    for (Map<String, String> map : flattenedMapList) {\n+      //\n+      for (Map.Entry<String, String> entry : map.entrySet()) {\n+        //handle key posting list\n+        String key = entry.getKey();\n+\n+        List<Integer> keyPostingList = postingListMap.get(key);\n+        if (keyPostingList == null) {\n+          keyPostingList = new ArrayList<>();\n+          postingListMap.put(key, keyPostingList);\n+        }\n+        keyPostingList.add(numFlatennedDocId);\n+\n+        //handle keyvalue posting list\n+        String keyValue = key + POSTING_LIST_KEY_SEPARATOR + entry.getValue();\n+        List<Integer> keyValuePostingList = postingListMap.get(keyValue);\n+        if (keyValuePostingList == null) {\n+          keyValuePostingList = new ArrayList<>();\n+          postingListMap.put(keyValue, keyValuePostingList);\n+        }\n+        keyValuePostingList.add(numFlatennedDocId);\n+      }\n+      //flattenedDocId2RootDocIdMapping\n+      flattenedDocIdList.add(docId);\n+\n+      numFlatennedDocId++;\n+    }\n+    docId++;\n+\n+    //flush data\n+    if (docId % FLUSH_THRESHOLD == 0) {\n+      flush();\n+    }\n+  }\n+\n+  /**\n+   * Multi value\n+   * @param dataArray\n+   * @param length\n+   * @throws IOException\n+   */\n+  public void add(byte[][] dataArray, int length)\n+      throws IOException {\n+\n+    for (int i = 0; i < length; i++) {\n+      byte[] data = dataArray[i];\n+      JsonNode jsonNode = new ObjectMapper().readTree(data);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d"}, "originalPosition": 171}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MjM5MQ==", "bodyText": "Are these debugging messages, or expected to be part of code?", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515672391", "createdAt": "2020-11-01T21:09:25Z", "author": {"login": "mayankshriv"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/creator/impl/inv/JSONIndexCreator.java", "diffHunk": "@@ -0,0 +1,658 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.creator.impl.inv;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.github.wnameless.json.flattener.JsonFlattener;\n+import com.google.common.io.Files;\n+import com.google.common.primitives.UnsignedBytes;\n+import java.io.BufferedInputStream;\n+import java.io.BufferedOutputStream;\n+import java.io.Closeable;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.TreeMap;\n+import java.util.stream.Collectors;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.commons.lang3.tuple.ImmutablePair;\n+import org.apache.pinot.core.common.BlockDocIdIterator;\n+import org.apache.pinot.core.io.util.VarLengthBytesValueReaderWriter;\n+import org.apache.pinot.core.operator.blocks.FilterBlock;\n+import org.apache.pinot.core.operator.filter.JSONMatchFilterOperator;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.predicate.EqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.QueryContextConverterUtils;\n+import org.apache.pinot.core.segment.index.readers.JSONIndexReader;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+import org.apache.pinot.spi.data.DimensionFieldSpec;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.sql.parsers.CalciteSqlParser;\n+import org.roaringbitmap.RoaringBitmap;\n+import org.roaringbitmap.buffer.MutableRoaringBitmap;\n+\n+import static org.apache.pinot.core.segment.creator.impl.V1Constants.Indexes.JSON_INDEX_FILE_EXTENSION;\n+\n+\n+public class JSONIndexCreator implements Closeable {\n+\n+  //separator used to join the key and value to create posting list key\n+  public static String POSTING_LIST_KEY_SEPARATOR = \"|\";\n+  static int FLUSH_THRESHOLD = 50_000;\n+  static int VERSION = 1;\n+  private final File flattenedDocId2RootDocIdMappingFile;\n+  private final File postingListFile;\n+  private File dictionaryheaderFile;\n+  private File dictionaryOffsetFile;\n+  private File dictionaryFile;\n+  private File invertedIndexOffsetFile;\n+  private File invertedIndexFile;\n+  private File outputIndexFile;\n+\n+  private int docId = 0;\n+  private int numFlatennedDocId = 0;\n+  int chunkId = 0;\n+\n+  private DataOutputStream postingListWriter;\n+  private DataOutputStream flattenedDocId2RootDocIdWriter;\n+\n+  Map<String, List<Integer>> postingListMap = new TreeMap<>();\n+  List<Integer> flattenedDocIdList = new ArrayList<>();\n+  List<Integer> postingListChunkOffsets = new ArrayList<>();\n+  List<Integer> chunkLengths = new ArrayList<>();\n+  private FieldSpec fieldSpec;\n+\n+  public JSONIndexCreator(File indexDir, FieldSpec fieldSpec)\n+      throws IOException {\n+    this.fieldSpec = fieldSpec;\n+    System.out.println(\"indexDir = \" + indexDir);\n+\n+    String name = fieldSpec.getName();\n+    postingListFile = new File(indexDir + name + \"_postingList.buf\");\n+    postingListWriter = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(postingListFile)));\n+    postingListChunkOffsets.add(postingListWriter.size());\n+\n+    dictionaryheaderFile = new File(indexDir, name + \"_dictionaryHeader.buf\");\n+    dictionaryOffsetFile = new File(indexDir, name + \"_dictionaryOffset.buf\");\n+    dictionaryFile = new File(indexDir, name + \"_dictionary.buf\");\n+    invertedIndexOffsetFile = new File(indexDir, name + \"_invertedIndexOffset.buf\");\n+    invertedIndexFile = new File(indexDir, name + \"_invertedIndex.buf\");\n+    flattenedDocId2RootDocIdMappingFile = new File(indexDir, name + \"_flattenedDocId.buf\");\n+    flattenedDocId2RootDocIdWriter =\n+        new DataOutputStream(new BufferedOutputStream(new FileOutputStream(flattenedDocId2RootDocIdMappingFile)));\n+\n+    //output file\n+    outputIndexFile = new File(indexDir, name + JSON_INDEX_FILE_EXTENSION);\n+  }\n+\n+  public void add(byte[] data)\n+      throws IOException {\n+\n+    JsonNode jsonNode = new ObjectMapper().readTree(data);\n+    List<Map<String, String>> flattenedMapList = unnestJson(jsonNode);\n+    for (Map<String, String> map : flattenedMapList) {\n+      //\n+      for (Map.Entry<String, String> entry : map.entrySet()) {\n+        //handle key posting list\n+        String key = entry.getKey();\n+\n+        List<Integer> keyPostingList = postingListMap.get(key);\n+        if (keyPostingList == null) {\n+          keyPostingList = new ArrayList<>();\n+          postingListMap.put(key, keyPostingList);\n+        }\n+        keyPostingList.add(numFlatennedDocId);\n+\n+        //handle keyvalue posting list\n+        String keyValue = key + POSTING_LIST_KEY_SEPARATOR + entry.getValue();\n+        List<Integer> keyValuePostingList = postingListMap.get(keyValue);\n+        if (keyValuePostingList == null) {\n+          keyValuePostingList = new ArrayList<>();\n+          postingListMap.put(keyValue, keyValuePostingList);\n+        }\n+        keyValuePostingList.add(numFlatennedDocId);\n+      }\n+      //flattenedDocId2RootDocIdMapping\n+      flattenedDocIdList.add(docId);\n+\n+      numFlatennedDocId++;\n+    }\n+    docId++;\n+\n+    //flush data\n+    if (docId % FLUSH_THRESHOLD == 0) {\n+      flush();\n+    }\n+  }\n+\n+  /**\n+   * Multi value\n+   * @param dataArray\n+   * @param length\n+   * @throws IOException\n+   */\n+  public void add(byte[][] dataArray, int length)\n+      throws IOException {\n+\n+    for (int i = 0; i < length; i++) {\n+      byte[] data = dataArray[i];\n+      JsonNode jsonNode = new ObjectMapper().readTree(data);\n+      List<Map<String, String>> flattenedMapList = unnestJson(jsonNode);\n+      for (Map<String, String> map : flattenedMapList) {\n+        //\n+        for (Map.Entry<String, String> entry : map.entrySet()) {\n+          //handle key posting list\n+          String key = entry.getKey();\n+\n+          List<Integer> keyPostingList = postingListMap.get(key);\n+          if (keyPostingList == null) {\n+            keyPostingList = new ArrayList<>();\n+            postingListMap.put(key, keyPostingList);\n+          }\n+          keyPostingList.add(numFlatennedDocId);\n+\n+          //handle keyvalue posting list\n+          String keyValue = key + POSTING_LIST_KEY_SEPARATOR + entry.getValue();\n+          List<Integer> keyValuePostingList = postingListMap.get(keyValue);\n+          if (keyValuePostingList == null) {\n+            keyValuePostingList = new ArrayList<>();\n+            postingListMap.put(keyValue, keyValuePostingList);\n+          }\n+          keyValuePostingList.add(numFlatennedDocId);\n+        }\n+        //flattenedDocId2RootDocIdMapping\n+        flattenedDocIdList.add(numFlatennedDocId);\n+\n+        numFlatennedDocId++;\n+      }\n+    }\n+    docId++;\n+\n+    //flush data\n+    if (docId % FLUSH_THRESHOLD == 0) {\n+      flush();\n+    }\n+  }\n+\n+  public void seal()\n+      throws IOException {\n+\n+    flush();\n+\n+    flattenedDocId2RootDocIdWriter.close();\n+    postingListWriter.close();\n+\n+    //key posting list merging\n+    System.out.println(\"InvertedIndex\");\n+    System.out.println(\"=================\");\n+\n+    int maxKeyLength = createInvertedIndex(postingListFile, postingListChunkOffsets, chunkLengths);\n+    System.out.println(\"=================\");\n+\n+    int flattenedDocid = 0;\n+    DataInputStream flattenedDocId2RootDocIdReader =\n+        new DataInputStream(new BufferedInputStream(new FileInputStream(flattenedDocId2RootDocIdMappingFile)));\n+    int[] rootDocIdArray = new int[numFlatennedDocId];\n+    while (flattenedDocid < numFlatennedDocId) {\n+      rootDocIdArray[flattenedDocid++] = flattenedDocId2RootDocIdReader.readInt();\n+    }\n+    System.out.println(\"FlattenedDocId  to RootDocId Mapping = \");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d"}, "originalPosition": 231}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MjUwMg==", "bodyText": "Perhaps adding an offset to payload/data-start makes header expandable for future and helps with compatibility.", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515672502", "createdAt": "2020-11-01T21:10:30Z", "author": {"login": "mayankshriv"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/creator/impl/inv/JSONIndexCreator.java", "diffHunk": "@@ -0,0 +1,658 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.creator.impl.inv;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.github.wnameless.json.flattener.JsonFlattener;\n+import com.google.common.io.Files;\n+import com.google.common.primitives.UnsignedBytes;\n+import java.io.BufferedInputStream;\n+import java.io.BufferedOutputStream;\n+import java.io.Closeable;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.TreeMap;\n+import java.util.stream.Collectors;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.commons.lang3.tuple.ImmutablePair;\n+import org.apache.pinot.core.common.BlockDocIdIterator;\n+import org.apache.pinot.core.io.util.VarLengthBytesValueReaderWriter;\n+import org.apache.pinot.core.operator.blocks.FilterBlock;\n+import org.apache.pinot.core.operator.filter.JSONMatchFilterOperator;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.predicate.EqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.QueryContextConverterUtils;\n+import org.apache.pinot.core.segment.index.readers.JSONIndexReader;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+import org.apache.pinot.spi.data.DimensionFieldSpec;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.sql.parsers.CalciteSqlParser;\n+import org.roaringbitmap.RoaringBitmap;\n+import org.roaringbitmap.buffer.MutableRoaringBitmap;\n+\n+import static org.apache.pinot.core.segment.creator.impl.V1Constants.Indexes.JSON_INDEX_FILE_EXTENSION;\n+\n+\n+public class JSONIndexCreator implements Closeable {\n+\n+  //separator used to join the key and value to create posting list key\n+  public static String POSTING_LIST_KEY_SEPARATOR = \"|\";\n+  static int FLUSH_THRESHOLD = 50_000;\n+  static int VERSION = 1;\n+  private final File flattenedDocId2RootDocIdMappingFile;\n+  private final File postingListFile;\n+  private File dictionaryheaderFile;\n+  private File dictionaryOffsetFile;\n+  private File dictionaryFile;\n+  private File invertedIndexOffsetFile;\n+  private File invertedIndexFile;\n+  private File outputIndexFile;\n+\n+  private int docId = 0;\n+  private int numFlatennedDocId = 0;\n+  int chunkId = 0;\n+\n+  private DataOutputStream postingListWriter;\n+  private DataOutputStream flattenedDocId2RootDocIdWriter;\n+\n+  Map<String, List<Integer>> postingListMap = new TreeMap<>();\n+  List<Integer> flattenedDocIdList = new ArrayList<>();\n+  List<Integer> postingListChunkOffsets = new ArrayList<>();\n+  List<Integer> chunkLengths = new ArrayList<>();\n+  private FieldSpec fieldSpec;\n+\n+  public JSONIndexCreator(File indexDir, FieldSpec fieldSpec)\n+      throws IOException {\n+    this.fieldSpec = fieldSpec;\n+    System.out.println(\"indexDir = \" + indexDir);\n+\n+    String name = fieldSpec.getName();\n+    postingListFile = new File(indexDir + name + \"_postingList.buf\");\n+    postingListWriter = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(postingListFile)));\n+    postingListChunkOffsets.add(postingListWriter.size());\n+\n+    dictionaryheaderFile = new File(indexDir, name + \"_dictionaryHeader.buf\");\n+    dictionaryOffsetFile = new File(indexDir, name + \"_dictionaryOffset.buf\");\n+    dictionaryFile = new File(indexDir, name + \"_dictionary.buf\");\n+    invertedIndexOffsetFile = new File(indexDir, name + \"_invertedIndexOffset.buf\");\n+    invertedIndexFile = new File(indexDir, name + \"_invertedIndex.buf\");\n+    flattenedDocId2RootDocIdMappingFile = new File(indexDir, name + \"_flattenedDocId.buf\");\n+    flattenedDocId2RootDocIdWriter =\n+        new DataOutputStream(new BufferedOutputStream(new FileOutputStream(flattenedDocId2RootDocIdMappingFile)));\n+\n+    //output file\n+    outputIndexFile = new File(indexDir, name + JSON_INDEX_FILE_EXTENSION);\n+  }\n+\n+  public void add(byte[] data)\n+      throws IOException {\n+\n+    JsonNode jsonNode = new ObjectMapper().readTree(data);\n+    List<Map<String, String>> flattenedMapList = unnestJson(jsonNode);\n+    for (Map<String, String> map : flattenedMapList) {\n+      //\n+      for (Map.Entry<String, String> entry : map.entrySet()) {\n+        //handle key posting list\n+        String key = entry.getKey();\n+\n+        List<Integer> keyPostingList = postingListMap.get(key);\n+        if (keyPostingList == null) {\n+          keyPostingList = new ArrayList<>();\n+          postingListMap.put(key, keyPostingList);\n+        }\n+        keyPostingList.add(numFlatennedDocId);\n+\n+        //handle keyvalue posting list\n+        String keyValue = key + POSTING_LIST_KEY_SEPARATOR + entry.getValue();\n+        List<Integer> keyValuePostingList = postingListMap.get(keyValue);\n+        if (keyValuePostingList == null) {\n+          keyValuePostingList = new ArrayList<>();\n+          postingListMap.put(keyValue, keyValuePostingList);\n+        }\n+        keyValuePostingList.add(numFlatennedDocId);\n+      }\n+      //flattenedDocId2RootDocIdMapping\n+      flattenedDocIdList.add(docId);\n+\n+      numFlatennedDocId++;\n+    }\n+    docId++;\n+\n+    //flush data\n+    if (docId % FLUSH_THRESHOLD == 0) {\n+      flush();\n+    }\n+  }\n+\n+  /**\n+   * Multi value\n+   * @param dataArray\n+   * @param length\n+   * @throws IOException\n+   */\n+  public void add(byte[][] dataArray, int length)\n+      throws IOException {\n+\n+    for (int i = 0; i < length; i++) {\n+      byte[] data = dataArray[i];\n+      JsonNode jsonNode = new ObjectMapper().readTree(data);\n+      List<Map<String, String>> flattenedMapList = unnestJson(jsonNode);\n+      for (Map<String, String> map : flattenedMapList) {\n+        //\n+        for (Map.Entry<String, String> entry : map.entrySet()) {\n+          //handle key posting list\n+          String key = entry.getKey();\n+\n+          List<Integer> keyPostingList = postingListMap.get(key);\n+          if (keyPostingList == null) {\n+            keyPostingList = new ArrayList<>();\n+            postingListMap.put(key, keyPostingList);\n+          }\n+          keyPostingList.add(numFlatennedDocId);\n+\n+          //handle keyvalue posting list\n+          String keyValue = key + POSTING_LIST_KEY_SEPARATOR + entry.getValue();\n+          List<Integer> keyValuePostingList = postingListMap.get(keyValue);\n+          if (keyValuePostingList == null) {\n+            keyValuePostingList = new ArrayList<>();\n+            postingListMap.put(keyValue, keyValuePostingList);\n+          }\n+          keyValuePostingList.add(numFlatennedDocId);\n+        }\n+        //flattenedDocId2RootDocIdMapping\n+        flattenedDocIdList.add(numFlatennedDocId);\n+\n+        numFlatennedDocId++;\n+      }\n+    }\n+    docId++;\n+\n+    //flush data\n+    if (docId % FLUSH_THRESHOLD == 0) {\n+      flush();\n+    }\n+  }\n+\n+  public void seal()\n+      throws IOException {\n+\n+    flush();\n+\n+    flattenedDocId2RootDocIdWriter.close();\n+    postingListWriter.close();\n+\n+    //key posting list merging\n+    System.out.println(\"InvertedIndex\");\n+    System.out.println(\"=================\");\n+\n+    int maxKeyLength = createInvertedIndex(postingListFile, postingListChunkOffsets, chunkLengths);\n+    System.out.println(\"=================\");\n+\n+    int flattenedDocid = 0;\n+    DataInputStream flattenedDocId2RootDocIdReader =\n+        new DataInputStream(new BufferedInputStream(new FileInputStream(flattenedDocId2RootDocIdMappingFile)));\n+    int[] rootDocIdArray = new int[numFlatennedDocId];\n+    while (flattenedDocid < numFlatennedDocId) {\n+      rootDocIdArray[flattenedDocid++] = flattenedDocId2RootDocIdReader.readInt();\n+    }\n+    System.out.println(\"FlattenedDocId  to RootDocId Mapping = \");\n+    System.out.println(Arrays.toString(rootDocIdArray));\n+\n+    //PUT all contents into one file\n+\n+    //header\n+    // version + maxDictionaryLength + [store the offsets + length for each one (dictionary offset file, dictionaryFile, index offset file, index file, flattened docId to rootDocId file)]\n+    long headerSize = 2 * Integer.BYTES + 6 * 2 * Long.BYTES;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1OTA0Mw=="}, "originalCommit": {"oid": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d"}, "originalPosition": 238}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MjYxMA==", "bodyText": "Clean up debug messages?", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515672610", "createdAt": "2020-11-01T21:11:38Z", "author": {"login": "mayankshriv"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/creator/impl/inv/JSONIndexCreator.java", "diffHunk": "@@ -0,0 +1,658 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.creator.impl.inv;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.github.wnameless.json.flattener.JsonFlattener;\n+import com.google.common.io.Files;\n+import com.google.common.primitives.UnsignedBytes;\n+import java.io.BufferedInputStream;\n+import java.io.BufferedOutputStream;\n+import java.io.Closeable;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.TreeMap;\n+import java.util.stream.Collectors;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.commons.lang3.tuple.ImmutablePair;\n+import org.apache.pinot.core.common.BlockDocIdIterator;\n+import org.apache.pinot.core.io.util.VarLengthBytesValueReaderWriter;\n+import org.apache.pinot.core.operator.blocks.FilterBlock;\n+import org.apache.pinot.core.operator.filter.JSONMatchFilterOperator;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.predicate.EqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.QueryContextConverterUtils;\n+import org.apache.pinot.core.segment.index.readers.JSONIndexReader;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+import org.apache.pinot.spi.data.DimensionFieldSpec;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.sql.parsers.CalciteSqlParser;\n+import org.roaringbitmap.RoaringBitmap;\n+import org.roaringbitmap.buffer.MutableRoaringBitmap;\n+\n+import static org.apache.pinot.core.segment.creator.impl.V1Constants.Indexes.JSON_INDEX_FILE_EXTENSION;\n+\n+\n+public class JSONIndexCreator implements Closeable {\n+\n+  //separator used to join the key and value to create posting list key\n+  public static String POSTING_LIST_KEY_SEPARATOR = \"|\";\n+  static int FLUSH_THRESHOLD = 50_000;\n+  static int VERSION = 1;\n+  private final File flattenedDocId2RootDocIdMappingFile;\n+  private final File postingListFile;\n+  private File dictionaryheaderFile;\n+  private File dictionaryOffsetFile;\n+  private File dictionaryFile;\n+  private File invertedIndexOffsetFile;\n+  private File invertedIndexFile;\n+  private File outputIndexFile;\n+\n+  private int docId = 0;\n+  private int numFlatennedDocId = 0;\n+  int chunkId = 0;\n+\n+  private DataOutputStream postingListWriter;\n+  private DataOutputStream flattenedDocId2RootDocIdWriter;\n+\n+  Map<String, List<Integer>> postingListMap = new TreeMap<>();\n+  List<Integer> flattenedDocIdList = new ArrayList<>();\n+  List<Integer> postingListChunkOffsets = new ArrayList<>();\n+  List<Integer> chunkLengths = new ArrayList<>();\n+  private FieldSpec fieldSpec;\n+\n+  public JSONIndexCreator(File indexDir, FieldSpec fieldSpec)\n+      throws IOException {\n+    this.fieldSpec = fieldSpec;\n+    System.out.println(\"indexDir = \" + indexDir);\n+\n+    String name = fieldSpec.getName();\n+    postingListFile = new File(indexDir + name + \"_postingList.buf\");\n+    postingListWriter = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(postingListFile)));\n+    postingListChunkOffsets.add(postingListWriter.size());\n+\n+    dictionaryheaderFile = new File(indexDir, name + \"_dictionaryHeader.buf\");\n+    dictionaryOffsetFile = new File(indexDir, name + \"_dictionaryOffset.buf\");\n+    dictionaryFile = new File(indexDir, name + \"_dictionary.buf\");\n+    invertedIndexOffsetFile = new File(indexDir, name + \"_invertedIndexOffset.buf\");\n+    invertedIndexFile = new File(indexDir, name + \"_invertedIndex.buf\");\n+    flattenedDocId2RootDocIdMappingFile = new File(indexDir, name + \"_flattenedDocId.buf\");\n+    flattenedDocId2RootDocIdWriter =\n+        new DataOutputStream(new BufferedOutputStream(new FileOutputStream(flattenedDocId2RootDocIdMappingFile)));\n+\n+    //output file\n+    outputIndexFile = new File(indexDir, name + JSON_INDEX_FILE_EXTENSION);\n+  }\n+\n+  public void add(byte[] data)\n+      throws IOException {\n+\n+    JsonNode jsonNode = new ObjectMapper().readTree(data);\n+    List<Map<String, String>> flattenedMapList = unnestJson(jsonNode);\n+    for (Map<String, String> map : flattenedMapList) {\n+      //\n+      for (Map.Entry<String, String> entry : map.entrySet()) {\n+        //handle key posting list\n+        String key = entry.getKey();\n+\n+        List<Integer> keyPostingList = postingListMap.get(key);\n+        if (keyPostingList == null) {\n+          keyPostingList = new ArrayList<>();\n+          postingListMap.put(key, keyPostingList);\n+        }\n+        keyPostingList.add(numFlatennedDocId);\n+\n+        //handle keyvalue posting list\n+        String keyValue = key + POSTING_LIST_KEY_SEPARATOR + entry.getValue();\n+        List<Integer> keyValuePostingList = postingListMap.get(keyValue);\n+        if (keyValuePostingList == null) {\n+          keyValuePostingList = new ArrayList<>();\n+          postingListMap.put(keyValue, keyValuePostingList);\n+        }\n+        keyValuePostingList.add(numFlatennedDocId);\n+      }\n+      //flattenedDocId2RootDocIdMapping\n+      flattenedDocIdList.add(docId);\n+\n+      numFlatennedDocId++;\n+    }\n+    docId++;\n+\n+    //flush data\n+    if (docId % FLUSH_THRESHOLD == 0) {\n+      flush();\n+    }\n+  }\n+\n+  /**\n+   * Multi value\n+   * @param dataArray\n+   * @param length\n+   * @throws IOException\n+   */\n+  public void add(byte[][] dataArray, int length)\n+      throws IOException {\n+\n+    for (int i = 0; i < length; i++) {\n+      byte[] data = dataArray[i];\n+      JsonNode jsonNode = new ObjectMapper().readTree(data);\n+      List<Map<String, String>> flattenedMapList = unnestJson(jsonNode);\n+      for (Map<String, String> map : flattenedMapList) {\n+        //\n+        for (Map.Entry<String, String> entry : map.entrySet()) {\n+          //handle key posting list\n+          String key = entry.getKey();\n+\n+          List<Integer> keyPostingList = postingListMap.get(key);\n+          if (keyPostingList == null) {\n+            keyPostingList = new ArrayList<>();\n+            postingListMap.put(key, keyPostingList);\n+          }\n+          keyPostingList.add(numFlatennedDocId);\n+\n+          //handle keyvalue posting list\n+          String keyValue = key + POSTING_LIST_KEY_SEPARATOR + entry.getValue();\n+          List<Integer> keyValuePostingList = postingListMap.get(keyValue);\n+          if (keyValuePostingList == null) {\n+            keyValuePostingList = new ArrayList<>();\n+            postingListMap.put(keyValue, keyValuePostingList);\n+          }\n+          keyValuePostingList.add(numFlatennedDocId);\n+        }\n+        //flattenedDocId2RootDocIdMapping\n+        flattenedDocIdList.add(numFlatennedDocId);\n+\n+        numFlatennedDocId++;\n+      }\n+    }\n+    docId++;\n+\n+    //flush data\n+    if (docId % FLUSH_THRESHOLD == 0) {\n+      flush();\n+    }\n+  }\n+\n+  public void seal()\n+      throws IOException {\n+\n+    flush();\n+\n+    flattenedDocId2RootDocIdWriter.close();\n+    postingListWriter.close();\n+\n+    //key posting list merging\n+    System.out.println(\"InvertedIndex\");\n+    System.out.println(\"=================\");\n+\n+    int maxKeyLength = createInvertedIndex(postingListFile, postingListChunkOffsets, chunkLengths);\n+    System.out.println(\"=================\");\n+\n+    int flattenedDocid = 0;\n+    DataInputStream flattenedDocId2RootDocIdReader =\n+        new DataInputStream(new BufferedInputStream(new FileInputStream(flattenedDocId2RootDocIdMappingFile)));\n+    int[] rootDocIdArray = new int[numFlatennedDocId];\n+    while (flattenedDocid < numFlatennedDocId) {\n+      rootDocIdArray[flattenedDocid++] = flattenedDocId2RootDocIdReader.readInt();\n+    }\n+    System.out.println(\"FlattenedDocId  to RootDocId Mapping = \");\n+    System.out.println(Arrays.toString(rootDocIdArray));\n+\n+    //PUT all contents into one file\n+\n+    //header\n+    // version + maxDictionaryLength + [store the offsets + length for each one (dictionary offset file, dictionaryFile, index offset file, index file, flattened docId to rootDocId file)]\n+    long headerSize = 2 * Integer.BYTES + 6 * 2 * Long.BYTES;\n+\n+    long dataSize =\n+        dictionaryheaderFile.length() + dictionaryOffsetFile.length() + dictionaryFile.length() + invertedIndexFile\n+            .length() + invertedIndexOffsetFile.length() + flattenedDocId2RootDocIdMappingFile.length();\n+\n+    long totalSize = headerSize + dataSize;\n+    PinotDataBuffer pinotDataBuffer =\n+        PinotDataBuffer.mapFile(outputIndexFile, false, 0, totalSize, ByteOrder.BIG_ENDIAN, \"Nested inverted index\");\n+\n+    pinotDataBuffer.putInt(0, VERSION);\n+    pinotDataBuffer.putInt(1 * Integer.BYTES, maxKeyLength);\n+    long writtenBytes = headerSize;\n+\n+    //add dictionary header\n+    int bufferId = 0;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, dictionaryheaderFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, dictionaryheaderFile, 0, dictionaryheaderFile.length());\n+    writtenBytes += dictionaryheaderFile.length();\n+\n+    //add dictionary offset\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, dictionaryOffsetFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, dictionaryOffsetFile, 0, dictionaryOffsetFile.length());\n+    writtenBytes += dictionaryOffsetFile.length();\n+\n+    //add dictionary\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, dictionaryFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, dictionaryFile, 0, dictionaryFile.length());\n+    writtenBytes += dictionaryFile.length();\n+\n+    //add index offset\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, invertedIndexOffsetFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, invertedIndexOffsetFile, 0, invertedIndexOffsetFile.length());\n+    writtenBytes += invertedIndexOffsetFile.length();\n+\n+    //add index data\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, invertedIndexFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, invertedIndexFile, 0, invertedIndexFile.length());\n+    writtenBytes += invertedIndexFile.length();\n+\n+    //add flattened docid to root doc id mapping\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, flattenedDocId2RootDocIdMappingFile.length());\n+    pinotDataBuffer\n+        .readFrom(writtenBytes, flattenedDocId2RootDocIdMappingFile, 0, flattenedDocId2RootDocIdMappingFile.length());\n+    writtenBytes += flattenedDocId2RootDocIdMappingFile.length();\n+  }\n+\n+  private long getBufferStartOffset(int bufferId) {\n+    return 2 * Integer.BYTES + 2 * bufferId * Long.BYTES;\n+  }\n+\n+  private int createInvertedIndex(File postingListFile, List<Integer> postingListChunkOffsets,\n+      List<Integer> chunkLengthList)\n+      throws IOException {\n+\n+    List<Iterator<ImmutablePair<byte[], int[]>>> chunkIterators = new ArrayList<>();\n+\n+    for (int i = 0; i < chunkLengthList.size(); i++) {\n+\n+      final DataInputStream postingListFileReader =\n+          new DataInputStream(new BufferedInputStream(new FileInputStream(postingListFile)));\n+      postingListFileReader.skipBytes(postingListChunkOffsets.get(i));\n+      final int length = chunkLengthList.get(i);\n+      chunkIterators.add(new Iterator<ImmutablePair<byte[], int[]>>() {\n+        int index = 0;\n+\n+        @Override\n+        public boolean hasNext() {\n+          return index < length;\n+        }\n+\n+        @Override\n+        public ImmutablePair<byte[], int[]> next() {\n+          try {\n+            int keyLength = postingListFileReader.readInt();\n+            byte[] keyBytes = new byte[keyLength];\n+            postingListFileReader.read(keyBytes);\n+\n+            int postingListLength = postingListFileReader.readInt();\n+            int[] postingList = new int[postingListLength];\n+            for (int i = 0; i < postingListLength; i++) {\n+              postingList[i] = postingListFileReader.readInt();\n+            }\n+            index++;\n+            return ImmutablePair.of(keyBytes, postingList);\n+          } catch (Exception e) {\n+            throw new RuntimeException(e);\n+          }\n+        }\n+      });\n+    }\n+    final Comparator<byte[]> byteArrayComparator = UnsignedBytes.lexicographicalComparator();\n+\n+    PriorityQueue<ImmutablePair<Integer, ImmutablePair<byte[], int[]>>> queue =\n+        new PriorityQueue<>(chunkLengthList.size(),\n+            (o1, o2) -> byteArrayComparator.compare(o1.getRight().getLeft(), o2.getRight().getLeft()));\n+    for (int i = 0; i < chunkIterators.size(); i++) {\n+      Iterator<ImmutablePair<byte[], int[]>> iterator = chunkIterators.get(i);\n+      if (iterator.hasNext()) {\n+        queue.offer(ImmutablePair.of(i, iterator.next()));\n+      }\n+    }\n+    byte[] prevKey = null;\n+    RoaringBitmap roaringBitmap = new RoaringBitmap();\n+\n+    Writer writer = new Writer(dictionaryheaderFile, dictionaryOffsetFile, dictionaryFile, invertedIndexOffsetFile,\n+        invertedIndexFile);\n+    while (!queue.isEmpty()) {\n+      ImmutablePair<Integer, ImmutablePair<byte[], int[]>> poll = queue.poll();\n+      byte[] currKey = poll.getRight().getLeft();\n+      if (prevKey != null && byteArrayComparator.compare(prevKey, currKey) != 0) {\n+        System.out.println(new String(prevKey) + \":\" + roaringBitmap);\n+        writer.add(prevKey, roaringBitmap);\n+        roaringBitmap.clear();\n+      }\n+\n+      roaringBitmap.add(poll.getRight().getRight());\n+      prevKey = currKey;\n+\n+      //add the next key from the chunk where the currKey was removed from\n+      Iterator<ImmutablePair<byte[], int[]>> iterator = chunkIterators.get(poll.getLeft());\n+      if (iterator.hasNext()) {\n+        queue.offer(ImmutablePair.of(poll.getLeft(), iterator.next()));\n+      }\n+    }\n+\n+    if (prevKey != null) {\n+      writer.add(prevKey, roaringBitmap);\n+    }\n+    writer.finish();\n+    return writer.getMaxDictionaryValueLength();\n+  }\n+\n+  private void flush()\n+      throws IOException {\n+    //write the key (length|actual bytes) - posting list(length, flattenedDocIds)\n+    System.out.println(\"postingListMap = \" + postingListMap);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d"}, "originalPosition": 385}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MjcyNQ==", "bodyText": "Same here, clean up debug messages.", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515672725", "createdAt": "2020-11-01T21:12:59Z", "author": {"login": "mayankshriv"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/creator/impl/inv/JSONIndexCreator.java", "diffHunk": "@@ -0,0 +1,658 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.creator.impl.inv;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.github.wnameless.json.flattener.JsonFlattener;\n+import com.google.common.io.Files;\n+import com.google.common.primitives.UnsignedBytes;\n+import java.io.BufferedInputStream;\n+import java.io.BufferedOutputStream;\n+import java.io.Closeable;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.TreeMap;\n+import java.util.stream.Collectors;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.commons.lang3.tuple.ImmutablePair;\n+import org.apache.pinot.core.common.BlockDocIdIterator;\n+import org.apache.pinot.core.io.util.VarLengthBytesValueReaderWriter;\n+import org.apache.pinot.core.operator.blocks.FilterBlock;\n+import org.apache.pinot.core.operator.filter.JSONMatchFilterOperator;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.predicate.EqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.QueryContextConverterUtils;\n+import org.apache.pinot.core.segment.index.readers.JSONIndexReader;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+import org.apache.pinot.spi.data.DimensionFieldSpec;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.sql.parsers.CalciteSqlParser;\n+import org.roaringbitmap.RoaringBitmap;\n+import org.roaringbitmap.buffer.MutableRoaringBitmap;\n+\n+import static org.apache.pinot.core.segment.creator.impl.V1Constants.Indexes.JSON_INDEX_FILE_EXTENSION;\n+\n+\n+public class JSONIndexCreator implements Closeable {\n+\n+  //separator used to join the key and value to create posting list key\n+  public static String POSTING_LIST_KEY_SEPARATOR = \"|\";\n+  static int FLUSH_THRESHOLD = 50_000;\n+  static int VERSION = 1;\n+  private final File flattenedDocId2RootDocIdMappingFile;\n+  private final File postingListFile;\n+  private File dictionaryheaderFile;\n+  private File dictionaryOffsetFile;\n+  private File dictionaryFile;\n+  private File invertedIndexOffsetFile;\n+  private File invertedIndexFile;\n+  private File outputIndexFile;\n+\n+  private int docId = 0;\n+  private int numFlatennedDocId = 0;\n+  int chunkId = 0;\n+\n+  private DataOutputStream postingListWriter;\n+  private DataOutputStream flattenedDocId2RootDocIdWriter;\n+\n+  Map<String, List<Integer>> postingListMap = new TreeMap<>();\n+  List<Integer> flattenedDocIdList = new ArrayList<>();\n+  List<Integer> postingListChunkOffsets = new ArrayList<>();\n+  List<Integer> chunkLengths = new ArrayList<>();\n+  private FieldSpec fieldSpec;\n+\n+  public JSONIndexCreator(File indexDir, FieldSpec fieldSpec)\n+      throws IOException {\n+    this.fieldSpec = fieldSpec;\n+    System.out.println(\"indexDir = \" + indexDir);\n+\n+    String name = fieldSpec.getName();\n+    postingListFile = new File(indexDir + name + \"_postingList.buf\");\n+    postingListWriter = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(postingListFile)));\n+    postingListChunkOffsets.add(postingListWriter.size());\n+\n+    dictionaryheaderFile = new File(indexDir, name + \"_dictionaryHeader.buf\");\n+    dictionaryOffsetFile = new File(indexDir, name + \"_dictionaryOffset.buf\");\n+    dictionaryFile = new File(indexDir, name + \"_dictionary.buf\");\n+    invertedIndexOffsetFile = new File(indexDir, name + \"_invertedIndexOffset.buf\");\n+    invertedIndexFile = new File(indexDir, name + \"_invertedIndex.buf\");\n+    flattenedDocId2RootDocIdMappingFile = new File(indexDir, name + \"_flattenedDocId.buf\");\n+    flattenedDocId2RootDocIdWriter =\n+        new DataOutputStream(new BufferedOutputStream(new FileOutputStream(flattenedDocId2RootDocIdMappingFile)));\n+\n+    //output file\n+    outputIndexFile = new File(indexDir, name + JSON_INDEX_FILE_EXTENSION);\n+  }\n+\n+  public void add(byte[] data)\n+      throws IOException {\n+\n+    JsonNode jsonNode = new ObjectMapper().readTree(data);\n+    List<Map<String, String>> flattenedMapList = unnestJson(jsonNode);\n+    for (Map<String, String> map : flattenedMapList) {\n+      //\n+      for (Map.Entry<String, String> entry : map.entrySet()) {\n+        //handle key posting list\n+        String key = entry.getKey();\n+\n+        List<Integer> keyPostingList = postingListMap.get(key);\n+        if (keyPostingList == null) {\n+          keyPostingList = new ArrayList<>();\n+          postingListMap.put(key, keyPostingList);\n+        }\n+        keyPostingList.add(numFlatennedDocId);\n+\n+        //handle keyvalue posting list\n+        String keyValue = key + POSTING_LIST_KEY_SEPARATOR + entry.getValue();\n+        List<Integer> keyValuePostingList = postingListMap.get(keyValue);\n+        if (keyValuePostingList == null) {\n+          keyValuePostingList = new ArrayList<>();\n+          postingListMap.put(keyValue, keyValuePostingList);\n+        }\n+        keyValuePostingList.add(numFlatennedDocId);\n+      }\n+      //flattenedDocId2RootDocIdMapping\n+      flattenedDocIdList.add(docId);\n+\n+      numFlatennedDocId++;\n+    }\n+    docId++;\n+\n+    //flush data\n+    if (docId % FLUSH_THRESHOLD == 0) {\n+      flush();\n+    }\n+  }\n+\n+  /**\n+   * Multi value\n+   * @param dataArray\n+   * @param length\n+   * @throws IOException\n+   */\n+  public void add(byte[][] dataArray, int length)\n+      throws IOException {\n+\n+    for (int i = 0; i < length; i++) {\n+      byte[] data = dataArray[i];\n+      JsonNode jsonNode = new ObjectMapper().readTree(data);\n+      List<Map<String, String>> flattenedMapList = unnestJson(jsonNode);\n+      for (Map<String, String> map : flattenedMapList) {\n+        //\n+        for (Map.Entry<String, String> entry : map.entrySet()) {\n+          //handle key posting list\n+          String key = entry.getKey();\n+\n+          List<Integer> keyPostingList = postingListMap.get(key);\n+          if (keyPostingList == null) {\n+            keyPostingList = new ArrayList<>();\n+            postingListMap.put(key, keyPostingList);\n+          }\n+          keyPostingList.add(numFlatennedDocId);\n+\n+          //handle keyvalue posting list\n+          String keyValue = key + POSTING_LIST_KEY_SEPARATOR + entry.getValue();\n+          List<Integer> keyValuePostingList = postingListMap.get(keyValue);\n+          if (keyValuePostingList == null) {\n+            keyValuePostingList = new ArrayList<>();\n+            postingListMap.put(keyValue, keyValuePostingList);\n+          }\n+          keyValuePostingList.add(numFlatennedDocId);\n+        }\n+        //flattenedDocId2RootDocIdMapping\n+        flattenedDocIdList.add(numFlatennedDocId);\n+\n+        numFlatennedDocId++;\n+      }\n+    }\n+    docId++;\n+\n+    //flush data\n+    if (docId % FLUSH_THRESHOLD == 0) {\n+      flush();\n+    }\n+  }\n+\n+  public void seal()\n+      throws IOException {\n+\n+    flush();\n+\n+    flattenedDocId2RootDocIdWriter.close();\n+    postingListWriter.close();\n+\n+    //key posting list merging\n+    System.out.println(\"InvertedIndex\");\n+    System.out.println(\"=================\");\n+\n+    int maxKeyLength = createInvertedIndex(postingListFile, postingListChunkOffsets, chunkLengths);\n+    System.out.println(\"=================\");\n+\n+    int flattenedDocid = 0;\n+    DataInputStream flattenedDocId2RootDocIdReader =\n+        new DataInputStream(new BufferedInputStream(new FileInputStream(flattenedDocId2RootDocIdMappingFile)));\n+    int[] rootDocIdArray = new int[numFlatennedDocId];\n+    while (flattenedDocid < numFlatennedDocId) {\n+      rootDocIdArray[flattenedDocid++] = flattenedDocId2RootDocIdReader.readInt();\n+    }\n+    System.out.println(\"FlattenedDocId  to RootDocId Mapping = \");\n+    System.out.println(Arrays.toString(rootDocIdArray));\n+\n+    //PUT all contents into one file\n+\n+    //header\n+    // version + maxDictionaryLength + [store the offsets + length for each one (dictionary offset file, dictionaryFile, index offset file, index file, flattened docId to rootDocId file)]\n+    long headerSize = 2 * Integer.BYTES + 6 * 2 * Long.BYTES;\n+\n+    long dataSize =\n+        dictionaryheaderFile.length() + dictionaryOffsetFile.length() + dictionaryFile.length() + invertedIndexFile\n+            .length() + invertedIndexOffsetFile.length() + flattenedDocId2RootDocIdMappingFile.length();\n+\n+    long totalSize = headerSize + dataSize;\n+    PinotDataBuffer pinotDataBuffer =\n+        PinotDataBuffer.mapFile(outputIndexFile, false, 0, totalSize, ByteOrder.BIG_ENDIAN, \"Nested inverted index\");\n+\n+    pinotDataBuffer.putInt(0, VERSION);\n+    pinotDataBuffer.putInt(1 * Integer.BYTES, maxKeyLength);\n+    long writtenBytes = headerSize;\n+\n+    //add dictionary header\n+    int bufferId = 0;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, dictionaryheaderFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, dictionaryheaderFile, 0, dictionaryheaderFile.length());\n+    writtenBytes += dictionaryheaderFile.length();\n+\n+    //add dictionary offset\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, dictionaryOffsetFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, dictionaryOffsetFile, 0, dictionaryOffsetFile.length());\n+    writtenBytes += dictionaryOffsetFile.length();\n+\n+    //add dictionary\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, dictionaryFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, dictionaryFile, 0, dictionaryFile.length());\n+    writtenBytes += dictionaryFile.length();\n+\n+    //add index offset\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, invertedIndexOffsetFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, invertedIndexOffsetFile, 0, invertedIndexOffsetFile.length());\n+    writtenBytes += invertedIndexOffsetFile.length();\n+\n+    //add index data\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, invertedIndexFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, invertedIndexFile, 0, invertedIndexFile.length());\n+    writtenBytes += invertedIndexFile.length();\n+\n+    //add flattened docid to root doc id mapping\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, flattenedDocId2RootDocIdMappingFile.length());\n+    pinotDataBuffer\n+        .readFrom(writtenBytes, flattenedDocId2RootDocIdMappingFile, 0, flattenedDocId2RootDocIdMappingFile.length());\n+    writtenBytes += flattenedDocId2RootDocIdMappingFile.length();\n+  }\n+\n+  private long getBufferStartOffset(int bufferId) {\n+    return 2 * Integer.BYTES + 2 * bufferId * Long.BYTES;\n+  }\n+\n+  private int createInvertedIndex(File postingListFile, List<Integer> postingListChunkOffsets,\n+      List<Integer> chunkLengthList)\n+      throws IOException {\n+\n+    List<Iterator<ImmutablePair<byte[], int[]>>> chunkIterators = new ArrayList<>();\n+\n+    for (int i = 0; i < chunkLengthList.size(); i++) {\n+\n+      final DataInputStream postingListFileReader =\n+          new DataInputStream(new BufferedInputStream(new FileInputStream(postingListFile)));\n+      postingListFileReader.skipBytes(postingListChunkOffsets.get(i));\n+      final int length = chunkLengthList.get(i);\n+      chunkIterators.add(new Iterator<ImmutablePair<byte[], int[]>>() {\n+        int index = 0;\n+\n+        @Override\n+        public boolean hasNext() {\n+          return index < length;\n+        }\n+\n+        @Override\n+        public ImmutablePair<byte[], int[]> next() {\n+          try {\n+            int keyLength = postingListFileReader.readInt();\n+            byte[] keyBytes = new byte[keyLength];\n+            postingListFileReader.read(keyBytes);\n+\n+            int postingListLength = postingListFileReader.readInt();\n+            int[] postingList = new int[postingListLength];\n+            for (int i = 0; i < postingListLength; i++) {\n+              postingList[i] = postingListFileReader.readInt();\n+            }\n+            index++;\n+            return ImmutablePair.of(keyBytes, postingList);\n+          } catch (Exception e) {\n+            throw new RuntimeException(e);\n+          }\n+        }\n+      });\n+    }\n+    final Comparator<byte[]> byteArrayComparator = UnsignedBytes.lexicographicalComparator();\n+\n+    PriorityQueue<ImmutablePair<Integer, ImmutablePair<byte[], int[]>>> queue =\n+        new PriorityQueue<>(chunkLengthList.size(),\n+            (o1, o2) -> byteArrayComparator.compare(o1.getRight().getLeft(), o2.getRight().getLeft()));\n+    for (int i = 0; i < chunkIterators.size(); i++) {\n+      Iterator<ImmutablePair<byte[], int[]>> iterator = chunkIterators.get(i);\n+      if (iterator.hasNext()) {\n+        queue.offer(ImmutablePair.of(i, iterator.next()));\n+      }\n+    }\n+    byte[] prevKey = null;\n+    RoaringBitmap roaringBitmap = new RoaringBitmap();\n+\n+    Writer writer = new Writer(dictionaryheaderFile, dictionaryOffsetFile, dictionaryFile, invertedIndexOffsetFile,\n+        invertedIndexFile);\n+    while (!queue.isEmpty()) {\n+      ImmutablePair<Integer, ImmutablePair<byte[], int[]>> poll = queue.poll();\n+      byte[] currKey = poll.getRight().getLeft();\n+      if (prevKey != null && byteArrayComparator.compare(prevKey, currKey) != 0) {\n+        System.out.println(new String(prevKey) + \":\" + roaringBitmap);\n+        writer.add(prevKey, roaringBitmap);\n+        roaringBitmap.clear();\n+      }\n+\n+      roaringBitmap.add(poll.getRight().getRight());\n+      prevKey = currKey;\n+\n+      //add the next key from the chunk where the currKey was removed from\n+      Iterator<ImmutablePair<byte[], int[]>> iterator = chunkIterators.get(poll.getLeft());\n+      if (iterator.hasNext()) {\n+        queue.offer(ImmutablePair.of(poll.getLeft(), iterator.next()));\n+      }\n+    }\n+\n+    if (prevKey != null) {\n+      writer.add(prevKey, roaringBitmap);\n+    }\n+    writer.finish();\n+    return writer.getMaxDictionaryValueLength();\n+  }\n+\n+  private void flush()\n+      throws IOException {\n+    //write the key (length|actual bytes) - posting list(length, flattenedDocIds)\n+    System.out.println(\"postingListMap = \" + postingListMap);\n+    for (Map.Entry<String, List<Integer>> entry : postingListMap.entrySet()) {\n+      byte[] keyBytes = entry.getKey().getBytes(Charset.forName(\"UTF-8\"));\n+      postingListWriter.writeInt(keyBytes.length);\n+      postingListWriter.write(keyBytes);\n+      List<Integer> flattenedDocIdList = entry.getValue();\n+      postingListWriter.writeInt(flattenedDocIdList.size());\n+      for (int flattenedDocId : flattenedDocIdList) {\n+        postingListWriter.writeInt(flattenedDocId);\n+      }\n+    }\n+\n+    //write flattened doc id to root docId mapping\n+    for (int rootDocId : flattenedDocIdList) {\n+      flattenedDocId2RootDocIdWriter.writeInt(rootDocId);\n+    }\n+    chunkLengths.add(postingListMap.size());\n+    postingListChunkOffsets.add(postingListWriter.size());\n+    postingListMap.clear();\n+    flattenedDocIdList.clear();\n+  }\n+\n+  private static List<Map<String, String>> unnestJson(JsonNode root) {\n+    Iterator<Map.Entry<String, JsonNode>> fields = root.fields();\n+    Map<String, String> flattenedSingleValuesMap = new TreeMap<>();\n+    Map<String, JsonNode> arrNodes = new TreeMap<>();\n+    Map<String, JsonNode> objectNodes = new TreeMap<>();\n+    List<Map<String, String>> resultList = new ArrayList<>();\n+    List<Map<String, String>> tempResultList = new ArrayList<>();\n+    while (fields.hasNext()) {\n+      Map.Entry<String, JsonNode> child = fields.next();\n+      if (child.getValue().isValueNode()) {\n+        //Normal value node\n+        flattenedSingleValuesMap.put(child.getKey(), child.getValue().asText());\n+      } else if (child.getValue().isArray()) {\n+        //Array Node: Process these nodes later\n+        arrNodes.put(child.getKey(), child.getValue());\n+      } else {\n+        //Object Node\n+        objectNodes.put(child.getKey(), child.getValue());\n+      }\n+    }\n+    for (String objectNodeKey : objectNodes.keySet()) {\n+      JsonNode objectNode = objectNodes.get(objectNodeKey);\n+      modifyKeysInMap(flattenedSingleValuesMap, tempResultList, objectNodeKey, objectNode);\n+    }\n+    if (tempResultList.isEmpty()) {\n+      tempResultList.add(flattenedSingleValuesMap);\n+    }\n+    if (!arrNodes.isEmpty()) {\n+      for (Map<String, String> flattenedMapElement : tempResultList) {\n+        for (String arrNodeKey : arrNodes.keySet()) {\n+          JsonNode arrNode = arrNodes.get(arrNodeKey);\n+          for (JsonNode arrNodeElement : arrNode) {\n+            modifyKeysInMap(flattenedMapElement, resultList, arrNodeKey, arrNodeElement);\n+          }\n+        }\n+      }\n+    } else {\n+      resultList.addAll(tempResultList);\n+    }\n+    return resultList;\n+  }\n+\n+  private static void modifyKeysInMap(Map<String, String> flattenedMap, List<Map<String, String>> resultList,\n+      String arrNodeKey, JsonNode arrNode) {\n+    List<Map<String, String>> objectResult = unnestJson(arrNode);\n+    for (Map<String, String> flattenedObject : objectResult) {\n+      Map<String, String> flattenedObjectCopy = new TreeMap<>(flattenedMap);\n+      for (Map.Entry<String, String> entry : flattenedObject.entrySet()) {\n+        flattenedObjectCopy.put(arrNodeKey + \".\" + entry.getKey(), entry.getValue());\n+      }\n+      resultList.add(flattenedObjectCopy);\n+    }\n+  }\n+\n+  @Override\n+  public void close()\n+      throws IOException {\n+\n+  }\n+\n+  private class Writer {\n+    private DataOutputStream _dictionaryHeaderWriter;\n+    private DataOutputStream _dictionaryOffsetWriter;\n+    private File _dictionaryOffsetFile;\n+    private DataOutputStream _dictionaryWriter;\n+    private DataOutputStream _invertedIndexOffsetWriter;\n+    private File _invertedIndexOffsetFile;\n+    private DataOutputStream _invertedIndexWriter;\n+    private int _dictId;\n+    private int _dictOffset;\n+    private int _invertedIndexOffset;\n+    int _maxDictionaryValueLength = Integer.MIN_VALUE;\n+\n+    public Writer(File dictionaryheaderFile, File dictionaryOffsetFile, File dictionaryFile,\n+        File invertedIndexOffsetFile, File invertedIndexFile)\n+        throws IOException {\n+      _dictionaryHeaderWriter =\n+          new DataOutputStream(new BufferedOutputStream(new FileOutputStream(dictionaryheaderFile)));\n+\n+      _dictionaryOffsetWriter =\n+          new DataOutputStream(new BufferedOutputStream(new FileOutputStream(dictionaryOffsetFile)));\n+      _dictionaryOffsetFile = dictionaryOffsetFile;\n+      _dictionaryWriter = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(dictionaryFile)));\n+      _invertedIndexOffsetWriter =\n+          new DataOutputStream(new BufferedOutputStream(new FileOutputStream(invertedIndexOffsetFile)));\n+      _invertedIndexOffsetFile = invertedIndexOffsetFile;\n+      _invertedIndexWriter = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(invertedIndexFile)));\n+      _dictId = 0;\n+      _dictOffset = 0;\n+      _invertedIndexOffset = 0;\n+    }\n+\n+    public void add(byte[] key, RoaringBitmap roaringBitmap)\n+        throws IOException {\n+      if (key.length > _maxDictionaryValueLength) {\n+        _maxDictionaryValueLength = key.length;\n+      }\n+      //write the key to dictionary\n+      _dictionaryOffsetWriter.writeInt(_dictOffset);\n+      _dictionaryWriter.write(key);\n+\n+      //write the roaringBitmap to inverted index\n+      _invertedIndexOffsetWriter.writeInt(_invertedIndexOffset);\n+\n+      int serializedSizeInBytes = roaringBitmap.serializedSizeInBytes();\n+      byte[] serializedRoaringBitmap = new byte[serializedSizeInBytes];\n+      ByteBuffer serializedRoaringBitmapBuffer = ByteBuffer.wrap(serializedRoaringBitmap);\n+      roaringBitmap.serialize(serializedRoaringBitmapBuffer);\n+      _invertedIndexWriter.write(serializedRoaringBitmap);\n+      System.out.println(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d"}, "originalPosition": 516}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3Mjc0Ng==", "bodyText": "Remove?", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515672746", "createdAt": "2020-11-01T21:13:28Z", "author": {"login": "mayankshriv"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/creator/impl/inv/JSONIndexCreator.java", "diffHunk": "@@ -0,0 +1,658 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.creator.impl.inv;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.github.wnameless.json.flattener.JsonFlattener;\n+import com.google.common.io.Files;\n+import com.google.common.primitives.UnsignedBytes;\n+import java.io.BufferedInputStream;\n+import java.io.BufferedOutputStream;\n+import java.io.Closeable;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.TreeMap;\n+import java.util.stream.Collectors;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.commons.lang3.tuple.ImmutablePair;\n+import org.apache.pinot.core.common.BlockDocIdIterator;\n+import org.apache.pinot.core.io.util.VarLengthBytesValueReaderWriter;\n+import org.apache.pinot.core.operator.blocks.FilterBlock;\n+import org.apache.pinot.core.operator.filter.JSONMatchFilterOperator;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.predicate.EqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.QueryContextConverterUtils;\n+import org.apache.pinot.core.segment.index.readers.JSONIndexReader;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+import org.apache.pinot.spi.data.DimensionFieldSpec;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.sql.parsers.CalciteSqlParser;\n+import org.roaringbitmap.RoaringBitmap;\n+import org.roaringbitmap.buffer.MutableRoaringBitmap;\n+\n+import static org.apache.pinot.core.segment.creator.impl.V1Constants.Indexes.JSON_INDEX_FILE_EXTENSION;\n+\n+\n+public class JSONIndexCreator implements Closeable {\n+\n+  //separator used to join the key and value to create posting list key\n+  public static String POSTING_LIST_KEY_SEPARATOR = \"|\";\n+  static int FLUSH_THRESHOLD = 50_000;\n+  static int VERSION = 1;\n+  private final File flattenedDocId2RootDocIdMappingFile;\n+  private final File postingListFile;\n+  private File dictionaryheaderFile;\n+  private File dictionaryOffsetFile;\n+  private File dictionaryFile;\n+  private File invertedIndexOffsetFile;\n+  private File invertedIndexFile;\n+  private File outputIndexFile;\n+\n+  private int docId = 0;\n+  private int numFlatennedDocId = 0;\n+  int chunkId = 0;\n+\n+  private DataOutputStream postingListWriter;\n+  private DataOutputStream flattenedDocId2RootDocIdWriter;\n+\n+  Map<String, List<Integer>> postingListMap = new TreeMap<>();\n+  List<Integer> flattenedDocIdList = new ArrayList<>();\n+  List<Integer> postingListChunkOffsets = new ArrayList<>();\n+  List<Integer> chunkLengths = new ArrayList<>();\n+  private FieldSpec fieldSpec;\n+\n+  public JSONIndexCreator(File indexDir, FieldSpec fieldSpec)\n+      throws IOException {\n+    this.fieldSpec = fieldSpec;\n+    System.out.println(\"indexDir = \" + indexDir);\n+\n+    String name = fieldSpec.getName();\n+    postingListFile = new File(indexDir + name + \"_postingList.buf\");\n+    postingListWriter = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(postingListFile)));\n+    postingListChunkOffsets.add(postingListWriter.size());\n+\n+    dictionaryheaderFile = new File(indexDir, name + \"_dictionaryHeader.buf\");\n+    dictionaryOffsetFile = new File(indexDir, name + \"_dictionaryOffset.buf\");\n+    dictionaryFile = new File(indexDir, name + \"_dictionary.buf\");\n+    invertedIndexOffsetFile = new File(indexDir, name + \"_invertedIndexOffset.buf\");\n+    invertedIndexFile = new File(indexDir, name + \"_invertedIndex.buf\");\n+    flattenedDocId2RootDocIdMappingFile = new File(indexDir, name + \"_flattenedDocId.buf\");\n+    flattenedDocId2RootDocIdWriter =\n+        new DataOutputStream(new BufferedOutputStream(new FileOutputStream(flattenedDocId2RootDocIdMappingFile)));\n+\n+    //output file\n+    outputIndexFile = new File(indexDir, name + JSON_INDEX_FILE_EXTENSION);\n+  }\n+\n+  public void add(byte[] data)\n+      throws IOException {\n+\n+    JsonNode jsonNode = new ObjectMapper().readTree(data);\n+    List<Map<String, String>> flattenedMapList = unnestJson(jsonNode);\n+    for (Map<String, String> map : flattenedMapList) {\n+      //\n+      for (Map.Entry<String, String> entry : map.entrySet()) {\n+        //handle key posting list\n+        String key = entry.getKey();\n+\n+        List<Integer> keyPostingList = postingListMap.get(key);\n+        if (keyPostingList == null) {\n+          keyPostingList = new ArrayList<>();\n+          postingListMap.put(key, keyPostingList);\n+        }\n+        keyPostingList.add(numFlatennedDocId);\n+\n+        //handle keyvalue posting list\n+        String keyValue = key + POSTING_LIST_KEY_SEPARATOR + entry.getValue();\n+        List<Integer> keyValuePostingList = postingListMap.get(keyValue);\n+        if (keyValuePostingList == null) {\n+          keyValuePostingList = new ArrayList<>();\n+          postingListMap.put(keyValue, keyValuePostingList);\n+        }\n+        keyValuePostingList.add(numFlatennedDocId);\n+      }\n+      //flattenedDocId2RootDocIdMapping\n+      flattenedDocIdList.add(docId);\n+\n+      numFlatennedDocId++;\n+    }\n+    docId++;\n+\n+    //flush data\n+    if (docId % FLUSH_THRESHOLD == 0) {\n+      flush();\n+    }\n+  }\n+\n+  /**\n+   * Multi value\n+   * @param dataArray\n+   * @param length\n+   * @throws IOException\n+   */\n+  public void add(byte[][] dataArray, int length)\n+      throws IOException {\n+\n+    for (int i = 0; i < length; i++) {\n+      byte[] data = dataArray[i];\n+      JsonNode jsonNode = new ObjectMapper().readTree(data);\n+      List<Map<String, String>> flattenedMapList = unnestJson(jsonNode);\n+      for (Map<String, String> map : flattenedMapList) {\n+        //\n+        for (Map.Entry<String, String> entry : map.entrySet()) {\n+          //handle key posting list\n+          String key = entry.getKey();\n+\n+          List<Integer> keyPostingList = postingListMap.get(key);\n+          if (keyPostingList == null) {\n+            keyPostingList = new ArrayList<>();\n+            postingListMap.put(key, keyPostingList);\n+          }\n+          keyPostingList.add(numFlatennedDocId);\n+\n+          //handle keyvalue posting list\n+          String keyValue = key + POSTING_LIST_KEY_SEPARATOR + entry.getValue();\n+          List<Integer> keyValuePostingList = postingListMap.get(keyValue);\n+          if (keyValuePostingList == null) {\n+            keyValuePostingList = new ArrayList<>();\n+            postingListMap.put(keyValue, keyValuePostingList);\n+          }\n+          keyValuePostingList.add(numFlatennedDocId);\n+        }\n+        //flattenedDocId2RootDocIdMapping\n+        flattenedDocIdList.add(numFlatennedDocId);\n+\n+        numFlatennedDocId++;\n+      }\n+    }\n+    docId++;\n+\n+    //flush data\n+    if (docId % FLUSH_THRESHOLD == 0) {\n+      flush();\n+    }\n+  }\n+\n+  public void seal()\n+      throws IOException {\n+\n+    flush();\n+\n+    flattenedDocId2RootDocIdWriter.close();\n+    postingListWriter.close();\n+\n+    //key posting list merging\n+    System.out.println(\"InvertedIndex\");\n+    System.out.println(\"=================\");\n+\n+    int maxKeyLength = createInvertedIndex(postingListFile, postingListChunkOffsets, chunkLengths);\n+    System.out.println(\"=================\");\n+\n+    int flattenedDocid = 0;\n+    DataInputStream flattenedDocId2RootDocIdReader =\n+        new DataInputStream(new BufferedInputStream(new FileInputStream(flattenedDocId2RootDocIdMappingFile)));\n+    int[] rootDocIdArray = new int[numFlatennedDocId];\n+    while (flattenedDocid < numFlatennedDocId) {\n+      rootDocIdArray[flattenedDocid++] = flattenedDocId2RootDocIdReader.readInt();\n+    }\n+    System.out.println(\"FlattenedDocId  to RootDocId Mapping = \");\n+    System.out.println(Arrays.toString(rootDocIdArray));\n+\n+    //PUT all contents into one file\n+\n+    //header\n+    // version + maxDictionaryLength + [store the offsets + length for each one (dictionary offset file, dictionaryFile, index offset file, index file, flattened docId to rootDocId file)]\n+    long headerSize = 2 * Integer.BYTES + 6 * 2 * Long.BYTES;\n+\n+    long dataSize =\n+        dictionaryheaderFile.length() + dictionaryOffsetFile.length() + dictionaryFile.length() + invertedIndexFile\n+            .length() + invertedIndexOffsetFile.length() + flattenedDocId2RootDocIdMappingFile.length();\n+\n+    long totalSize = headerSize + dataSize;\n+    PinotDataBuffer pinotDataBuffer =\n+        PinotDataBuffer.mapFile(outputIndexFile, false, 0, totalSize, ByteOrder.BIG_ENDIAN, \"Nested inverted index\");\n+\n+    pinotDataBuffer.putInt(0, VERSION);\n+    pinotDataBuffer.putInt(1 * Integer.BYTES, maxKeyLength);\n+    long writtenBytes = headerSize;\n+\n+    //add dictionary header\n+    int bufferId = 0;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, dictionaryheaderFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, dictionaryheaderFile, 0, dictionaryheaderFile.length());\n+    writtenBytes += dictionaryheaderFile.length();\n+\n+    //add dictionary offset\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, dictionaryOffsetFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, dictionaryOffsetFile, 0, dictionaryOffsetFile.length());\n+    writtenBytes += dictionaryOffsetFile.length();\n+\n+    //add dictionary\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, dictionaryFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, dictionaryFile, 0, dictionaryFile.length());\n+    writtenBytes += dictionaryFile.length();\n+\n+    //add index offset\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, invertedIndexOffsetFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, invertedIndexOffsetFile, 0, invertedIndexOffsetFile.length());\n+    writtenBytes += invertedIndexOffsetFile.length();\n+\n+    //add index data\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, invertedIndexFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, invertedIndexFile, 0, invertedIndexFile.length());\n+    writtenBytes += invertedIndexFile.length();\n+\n+    //add flattened docid to root doc id mapping\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, flattenedDocId2RootDocIdMappingFile.length());\n+    pinotDataBuffer\n+        .readFrom(writtenBytes, flattenedDocId2RootDocIdMappingFile, 0, flattenedDocId2RootDocIdMappingFile.length());\n+    writtenBytes += flattenedDocId2RootDocIdMappingFile.length();\n+  }\n+\n+  private long getBufferStartOffset(int bufferId) {\n+    return 2 * Integer.BYTES + 2 * bufferId * Long.BYTES;\n+  }\n+\n+  private int createInvertedIndex(File postingListFile, List<Integer> postingListChunkOffsets,\n+      List<Integer> chunkLengthList)\n+      throws IOException {\n+\n+    List<Iterator<ImmutablePair<byte[], int[]>>> chunkIterators = new ArrayList<>();\n+\n+    for (int i = 0; i < chunkLengthList.size(); i++) {\n+\n+      final DataInputStream postingListFileReader =\n+          new DataInputStream(new BufferedInputStream(new FileInputStream(postingListFile)));\n+      postingListFileReader.skipBytes(postingListChunkOffsets.get(i));\n+      final int length = chunkLengthList.get(i);\n+      chunkIterators.add(new Iterator<ImmutablePair<byte[], int[]>>() {\n+        int index = 0;\n+\n+        @Override\n+        public boolean hasNext() {\n+          return index < length;\n+        }\n+\n+        @Override\n+        public ImmutablePair<byte[], int[]> next() {\n+          try {\n+            int keyLength = postingListFileReader.readInt();\n+            byte[] keyBytes = new byte[keyLength];\n+            postingListFileReader.read(keyBytes);\n+\n+            int postingListLength = postingListFileReader.readInt();\n+            int[] postingList = new int[postingListLength];\n+            for (int i = 0; i < postingListLength; i++) {\n+              postingList[i] = postingListFileReader.readInt();\n+            }\n+            index++;\n+            return ImmutablePair.of(keyBytes, postingList);\n+          } catch (Exception e) {\n+            throw new RuntimeException(e);\n+          }\n+        }\n+      });\n+    }\n+    final Comparator<byte[]> byteArrayComparator = UnsignedBytes.lexicographicalComparator();\n+\n+    PriorityQueue<ImmutablePair<Integer, ImmutablePair<byte[], int[]>>> queue =\n+        new PriorityQueue<>(chunkLengthList.size(),\n+            (o1, o2) -> byteArrayComparator.compare(o1.getRight().getLeft(), o2.getRight().getLeft()));\n+    for (int i = 0; i < chunkIterators.size(); i++) {\n+      Iterator<ImmutablePair<byte[], int[]>> iterator = chunkIterators.get(i);\n+      if (iterator.hasNext()) {\n+        queue.offer(ImmutablePair.of(i, iterator.next()));\n+      }\n+    }\n+    byte[] prevKey = null;\n+    RoaringBitmap roaringBitmap = new RoaringBitmap();\n+\n+    Writer writer = new Writer(dictionaryheaderFile, dictionaryOffsetFile, dictionaryFile, invertedIndexOffsetFile,\n+        invertedIndexFile);\n+    while (!queue.isEmpty()) {\n+      ImmutablePair<Integer, ImmutablePair<byte[], int[]>> poll = queue.poll();\n+      byte[] currKey = poll.getRight().getLeft();\n+      if (prevKey != null && byteArrayComparator.compare(prevKey, currKey) != 0) {\n+        System.out.println(new String(prevKey) + \":\" + roaringBitmap);\n+        writer.add(prevKey, roaringBitmap);\n+        roaringBitmap.clear();\n+      }\n+\n+      roaringBitmap.add(poll.getRight().getRight());\n+      prevKey = currKey;\n+\n+      //add the next key from the chunk where the currKey was removed from\n+      Iterator<ImmutablePair<byte[], int[]>> iterator = chunkIterators.get(poll.getLeft());\n+      if (iterator.hasNext()) {\n+        queue.offer(ImmutablePair.of(poll.getLeft(), iterator.next()));\n+      }\n+    }\n+\n+    if (prevKey != null) {\n+      writer.add(prevKey, roaringBitmap);\n+    }\n+    writer.finish();\n+    return writer.getMaxDictionaryValueLength();\n+  }\n+\n+  private void flush()\n+      throws IOException {\n+    //write the key (length|actual bytes) - posting list(length, flattenedDocIds)\n+    System.out.println(\"postingListMap = \" + postingListMap);\n+    for (Map.Entry<String, List<Integer>> entry : postingListMap.entrySet()) {\n+      byte[] keyBytes = entry.getKey().getBytes(Charset.forName(\"UTF-8\"));\n+      postingListWriter.writeInt(keyBytes.length);\n+      postingListWriter.write(keyBytes);\n+      List<Integer> flattenedDocIdList = entry.getValue();\n+      postingListWriter.writeInt(flattenedDocIdList.size());\n+      for (int flattenedDocId : flattenedDocIdList) {\n+        postingListWriter.writeInt(flattenedDocId);\n+      }\n+    }\n+\n+    //write flattened doc id to root docId mapping\n+    for (int rootDocId : flattenedDocIdList) {\n+      flattenedDocId2RootDocIdWriter.writeInt(rootDocId);\n+    }\n+    chunkLengths.add(postingListMap.size());\n+    postingListChunkOffsets.add(postingListWriter.size());\n+    postingListMap.clear();\n+    flattenedDocIdList.clear();\n+  }\n+\n+  private static List<Map<String, String>> unnestJson(JsonNode root) {\n+    Iterator<Map.Entry<String, JsonNode>> fields = root.fields();\n+    Map<String, String> flattenedSingleValuesMap = new TreeMap<>();\n+    Map<String, JsonNode> arrNodes = new TreeMap<>();\n+    Map<String, JsonNode> objectNodes = new TreeMap<>();\n+    List<Map<String, String>> resultList = new ArrayList<>();\n+    List<Map<String, String>> tempResultList = new ArrayList<>();\n+    while (fields.hasNext()) {\n+      Map.Entry<String, JsonNode> child = fields.next();\n+      if (child.getValue().isValueNode()) {\n+        //Normal value node\n+        flattenedSingleValuesMap.put(child.getKey(), child.getValue().asText());\n+      } else if (child.getValue().isArray()) {\n+        //Array Node: Process these nodes later\n+        arrNodes.put(child.getKey(), child.getValue());\n+      } else {\n+        //Object Node\n+        objectNodes.put(child.getKey(), child.getValue());\n+      }\n+    }\n+    for (String objectNodeKey : objectNodes.keySet()) {\n+      JsonNode objectNode = objectNodes.get(objectNodeKey);\n+      modifyKeysInMap(flattenedSingleValuesMap, tempResultList, objectNodeKey, objectNode);\n+    }\n+    if (tempResultList.isEmpty()) {\n+      tempResultList.add(flattenedSingleValuesMap);\n+    }\n+    if (!arrNodes.isEmpty()) {\n+      for (Map<String, String> flattenedMapElement : tempResultList) {\n+        for (String arrNodeKey : arrNodes.keySet()) {\n+          JsonNode arrNode = arrNodes.get(arrNodeKey);\n+          for (JsonNode arrNodeElement : arrNode) {\n+            modifyKeysInMap(flattenedMapElement, resultList, arrNodeKey, arrNodeElement);\n+          }\n+        }\n+      }\n+    } else {\n+      resultList.addAll(tempResultList);\n+    }\n+    return resultList;\n+  }\n+\n+  private static void modifyKeysInMap(Map<String, String> flattenedMap, List<Map<String, String>> resultList,\n+      String arrNodeKey, JsonNode arrNode) {\n+    List<Map<String, String>> objectResult = unnestJson(arrNode);\n+    for (Map<String, String> flattenedObject : objectResult) {\n+      Map<String, String> flattenedObjectCopy = new TreeMap<>(flattenedMap);\n+      for (Map.Entry<String, String> entry : flattenedObject.entrySet()) {\n+        flattenedObjectCopy.put(arrNodeKey + \".\" + entry.getKey(), entry.getValue());\n+      }\n+      resultList.add(flattenedObjectCopy);\n+    }\n+  }\n+\n+  @Override\n+  public void close()\n+      throws IOException {\n+\n+  }\n+\n+  private class Writer {\n+    private DataOutputStream _dictionaryHeaderWriter;\n+    private DataOutputStream _dictionaryOffsetWriter;\n+    private File _dictionaryOffsetFile;\n+    private DataOutputStream _dictionaryWriter;\n+    private DataOutputStream _invertedIndexOffsetWriter;\n+    private File _invertedIndexOffsetFile;\n+    private DataOutputStream _invertedIndexWriter;\n+    private int _dictId;\n+    private int _dictOffset;\n+    private int _invertedIndexOffset;\n+    int _maxDictionaryValueLength = Integer.MIN_VALUE;\n+\n+    public Writer(File dictionaryheaderFile, File dictionaryOffsetFile, File dictionaryFile,\n+        File invertedIndexOffsetFile, File invertedIndexFile)\n+        throws IOException {\n+      _dictionaryHeaderWriter =\n+          new DataOutputStream(new BufferedOutputStream(new FileOutputStream(dictionaryheaderFile)));\n+\n+      _dictionaryOffsetWriter =\n+          new DataOutputStream(new BufferedOutputStream(new FileOutputStream(dictionaryOffsetFile)));\n+      _dictionaryOffsetFile = dictionaryOffsetFile;\n+      _dictionaryWriter = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(dictionaryFile)));\n+      _invertedIndexOffsetWriter =\n+          new DataOutputStream(new BufferedOutputStream(new FileOutputStream(invertedIndexOffsetFile)));\n+      _invertedIndexOffsetFile = invertedIndexOffsetFile;\n+      _invertedIndexWriter = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(invertedIndexFile)));\n+      _dictId = 0;\n+      _dictOffset = 0;\n+      _invertedIndexOffset = 0;\n+    }\n+\n+    public void add(byte[] key, RoaringBitmap roaringBitmap)\n+        throws IOException {\n+      if (key.length > _maxDictionaryValueLength) {\n+        _maxDictionaryValueLength = key.length;\n+      }\n+      //write the key to dictionary\n+      _dictionaryOffsetWriter.writeInt(_dictOffset);\n+      _dictionaryWriter.write(key);\n+\n+      //write the roaringBitmap to inverted index\n+      _invertedIndexOffsetWriter.writeInt(_invertedIndexOffset);\n+\n+      int serializedSizeInBytes = roaringBitmap.serializedSizeInBytes();\n+      byte[] serializedRoaringBitmap = new byte[serializedSizeInBytes];\n+      ByteBuffer serializedRoaringBitmapBuffer = ByteBuffer.wrap(serializedRoaringBitmap);\n+      roaringBitmap.serialize(serializedRoaringBitmapBuffer);\n+      _invertedIndexWriter.write(serializedRoaringBitmap);\n+      System.out.println(\n+          \"dictId = \" + _dictId + \", dict offset:\" + _dictOffset + \", valueLength:\" + key.length + \", inv offset:\"\n+              + _invertedIndexOffset + \", serializedSizeInBytes:\" + serializedSizeInBytes);\n+\n+      //increment offsets\n+      _dictOffset = _dictOffset + key.length;\n+      _invertedIndexOffset = _invertedIndexOffset + serializedSizeInBytes;\n+      //increment the dictionary id\n+      _dictId = _dictId + 1;\n+    }\n+\n+    void finish()\n+        throws IOException {\n+      //InvertedIndexReader and VarlengthBytesValueReaderWriter needs one extra entry for offsets since it computes the length for index i using offset[i+1] - offset[i]\n+      _invertedIndexOffsetWriter.writeInt(_invertedIndexOffset);\n+      _dictionaryOffsetWriter.writeInt(_dictOffset);\n+\n+      byte[] headerBytes = VarLengthBytesValueReaderWriter.getHeaderBytes(_dictId);\n+      _dictionaryHeaderWriter.write(headerBytes);\n+      System.out.println(\"headerBytes = \" + Arrays.toString(headerBytes));\n+\n+      _dictionaryHeaderWriter.close();\n+      _dictionaryOffsetWriter.close();\n+      _dictionaryWriter.close();\n+      _invertedIndexOffsetWriter.close();\n+      _invertedIndexWriter.close();\n+\n+      //data offsets started with zero but the actual dictionary and index will contain (header + offsets + data). so all the offsets must be adjusted ( i.e add size(header) + size(offset) to each offset value)\n+      PinotDataBuffer dictionaryOffsetBuffer = PinotDataBuffer\n+          .mapFile(dictionaryOffsetFile, false, 0, _dictionaryOffsetFile.length(), ByteOrder.BIG_ENDIAN,\n+              \"dictionary offset file\");\n+      int dictOffsetBase = _dictionaryHeaderWriter.size() + _dictionaryOffsetWriter.size();\n+      for (int i = 0; i < _dictId + 1; i++) {\n+        int offset = dictionaryOffsetBuffer.getInt(i * Integer.BYTES);\n+        int newOffset = offset + dictOffsetBase;\n+        dictionaryOffsetBuffer.putInt(i * Integer.BYTES, offset + dictOffsetBase);\n+        System.out.println(\"dictId = \" + i + \", offset = \" + offset + \", newOffset = \" + newOffset);\n+      }\n+\n+      PinotDataBuffer invIndexOffsetBuffer = PinotDataBuffer\n+          .mapFile(invertedIndexOffsetFile, false, 0, invertedIndexOffsetFile.length(), ByteOrder.BIG_ENDIAN,\n+              \"invertedIndexOffsetFile\");\n+      int invIndexOffsetBase = _invertedIndexOffsetWriter.size();\n+      for (int i = 0; i < _dictId + 1; i++) {\n+        int offset = invIndexOffsetBuffer.getInt(i * Integer.BYTES);\n+        int newOffset = offset + invIndexOffsetBase;\n+        System.out.println(\"offset = \" + offset + \", newOffset = \" + newOffset);\n+\n+        invIndexOffsetBuffer.putInt(i * Integer.BYTES, newOffset);\n+      }\n+\n+      invIndexOffsetBuffer.close();\n+      dictionaryOffsetBuffer.close();\n+    }\n+\n+    public int getMaxDictionaryValueLength() {\n+      return _maxDictionaryValueLength;\n+    }\n+  }\n+\n+  public static void main(String[] args)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d"}, "originalPosition": 576}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MjgwNQ==", "bodyText": "Add javadoc.", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515672805", "createdAt": "2020-11-01T21:14:06Z", "author": {"login": "mayankshriv"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/index/loader/invertedindex/JSONIndexHandler.java", "diffHunk": "@@ -0,0 +1,216 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.index.loader.invertedindex;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.HashSet;\n+import java.util.Set;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.core.indexsegment.generator.SegmentVersion;\n+import org.apache.pinot.core.segment.creator.impl.inv.JSONIndexCreator;\n+import org.apache.pinot.core.segment.index.column.PhysicalColumnIndexContainer;\n+import org.apache.pinot.core.segment.index.loader.IndexLoadingConfig;\n+import org.apache.pinot.core.segment.index.loader.LoaderUtils;\n+import org.apache.pinot.core.segment.index.metadata.ColumnMetadata;\n+import org.apache.pinot.core.segment.index.metadata.SegmentMetadataImpl;\n+import org.apache.pinot.core.segment.index.readers.BaseImmutableDictionary;\n+import org.apache.pinot.core.segment.index.readers.Dictionary;\n+import org.apache.pinot.core.segment.index.readers.ForwardIndexReader;\n+import org.apache.pinot.core.segment.index.readers.ForwardIndexReaderContext;\n+import org.apache.pinot.core.segment.index.readers.forward.FixedBitMVForwardIndexReader;\n+import org.apache.pinot.core.segment.index.readers.forward.FixedBitSVForwardIndexReader;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+import org.apache.pinot.core.segment.store.ColumnIndexType;\n+import org.apache.pinot.core.segment.store.SegmentDirectory;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.pinot.core.segment.creator.impl.V1Constants.Indexes.JSON_INDEX_FILE_EXTENSION;\n+\n+\n+@SuppressWarnings({\"rawtypes\", \"unchecked\"})\n+public class JSONIndexHandler {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(JSONIndexHandler.class);\n+\n+  private final File _indexDir;\n+  private final SegmentDirectory.Writer _segmentWriter;\n+  private final String _segmentName;\n+  private final SegmentVersion _segmentVersion;\n+  private final Set<ColumnMetadata> _jsonIndexColumns = new HashSet<>();\n+\n+  public JSONIndexHandler(File indexDir, SegmentMetadataImpl segmentMetadata, IndexLoadingConfig indexLoadingConfig,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3Mjk0OA==", "bodyText": "I think it is the data-type in which JSON content is stored, as opposed to the data type of JSON fields?", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515672948", "createdAt": "2020-11-01T21:15:03Z", "author": {"login": "mayankshriv"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/index/loader/invertedindex/JSONIndexHandler.java", "diffHunk": "@@ -0,0 +1,216 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.index.loader.invertedindex;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.HashSet;\n+import java.util.Set;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.core.indexsegment.generator.SegmentVersion;\n+import org.apache.pinot.core.segment.creator.impl.inv.JSONIndexCreator;\n+import org.apache.pinot.core.segment.index.column.PhysicalColumnIndexContainer;\n+import org.apache.pinot.core.segment.index.loader.IndexLoadingConfig;\n+import org.apache.pinot.core.segment.index.loader.LoaderUtils;\n+import org.apache.pinot.core.segment.index.metadata.ColumnMetadata;\n+import org.apache.pinot.core.segment.index.metadata.SegmentMetadataImpl;\n+import org.apache.pinot.core.segment.index.readers.BaseImmutableDictionary;\n+import org.apache.pinot.core.segment.index.readers.Dictionary;\n+import org.apache.pinot.core.segment.index.readers.ForwardIndexReader;\n+import org.apache.pinot.core.segment.index.readers.ForwardIndexReaderContext;\n+import org.apache.pinot.core.segment.index.readers.forward.FixedBitMVForwardIndexReader;\n+import org.apache.pinot.core.segment.index.readers.forward.FixedBitSVForwardIndexReader;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+import org.apache.pinot.core.segment.store.ColumnIndexType;\n+import org.apache.pinot.core.segment.store.SegmentDirectory;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.pinot.core.segment.creator.impl.V1Constants.Indexes.JSON_INDEX_FILE_EXTENSION;\n+\n+\n+@SuppressWarnings({\"rawtypes\", \"unchecked\"})\n+public class JSONIndexHandler {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(JSONIndexHandler.class);\n+\n+  private final File _indexDir;\n+  private final SegmentDirectory.Writer _segmentWriter;\n+  private final String _segmentName;\n+  private final SegmentVersion _segmentVersion;\n+  private final Set<ColumnMetadata> _jsonIndexColumns = new HashSet<>();\n+\n+  public JSONIndexHandler(File indexDir, SegmentMetadataImpl segmentMetadata, IndexLoadingConfig indexLoadingConfig,\n+      SegmentDirectory.Writer segmentWriter) {\n+    _indexDir = indexDir;\n+    _segmentWriter = segmentWriter;\n+    _segmentName = segmentMetadata.getName();\n+    _segmentVersion = SegmentVersion.valueOf(segmentMetadata.getVersion());\n+\n+    // Only create json index on dictionary-encoded unsorted columns\n+    for (String column : indexLoadingConfig.getJsonIndexColumns()) {\n+      ColumnMetadata columnMetadata = segmentMetadata.getColumnMetadataFor(column);\n+      if (columnMetadata != null && !columnMetadata.isSorted()) {\n+        _jsonIndexColumns.add(columnMetadata);\n+      }\n+    }\n+  }\n+\n+  public void createJsonIndices()\n+      throws IOException {\n+    for (ColumnMetadata columnMetadata : _jsonIndexColumns) {\n+      createJSONIndexForColumn(columnMetadata);\n+    }\n+  }\n+\n+  private void createJSONIndexForColumn(ColumnMetadata columnMetadata)\n+      throws IOException {\n+    String column = columnMetadata.getColumnName();\n+\n+    File inProgress = new File(_indexDir, column + JSON_INDEX_FILE_EXTENSION + \".inprogress\");\n+    File jsonIndexFile = new File(_indexDir, column + JSON_INDEX_FILE_EXTENSION);\n+\n+    if (!inProgress.exists()) {\n+      // Marker file does not exist, which means last run ended normally.\n+\n+      if (_segmentWriter.hasIndexFor(column, ColumnIndexType.JSON_INDEX)) {\n+        // Skip creating json index if already exists.\n+\n+        LOGGER.info(\"Found json index for segment: {}, column: {}\", _segmentName, column);\n+        return;\n+      }\n+\n+      // Create a marker file.\n+      FileUtils.touch(inProgress);\n+    } else {\n+      // Marker file exists, which means last run gets interrupted.\n+\n+      // Remove json index if exists.\n+      // For v1 and v2, it's the actual json index. For v3, it's the temporary json index.\n+      FileUtils.deleteQuietly(jsonIndexFile);\n+    }\n+\n+    // Create new json index for the column.\n+    LOGGER.info(\"Creating new json index for segment: {}, column: {}\", _segmentName, column);\n+    if (columnMetadata.hasDictionary()) {\n+      handleDictionaryBasedColumn(columnMetadata);\n+    } else {\n+      handleNonDictionaryBasedColumn(columnMetadata);\n+    }\n+\n+    // For v3, write the generated json index file into the single file and remove it.\n+    if (_segmentVersion == SegmentVersion.v3) {\n+      LoaderUtils.writeIndexToV3Format(_segmentWriter, column, jsonIndexFile, ColumnIndexType.JSON_INDEX);\n+    }\n+\n+    // Delete the marker file.\n+    FileUtils.deleteQuietly(inProgress);\n+\n+    LOGGER.info(\"Created json index for segment: {}, column: {}\", _segmentName, column);\n+  }\n+\n+  private void handleDictionaryBasedColumn(ColumnMetadata columnMetadata)\n+      throws IOException {\n+    int numDocs = columnMetadata.getTotalDocs();\n+    try (ForwardIndexReader forwardIndexReader = getForwardIndexReader(columnMetadata, _segmentWriter);\n+        ForwardIndexReaderContext readerContext = forwardIndexReader.createContext();\n+        Dictionary dictionary = getDictionaryReader(columnMetadata, _segmentWriter);\n+        JSONIndexCreator jsonIndexCreator = new JSONIndexCreator(_indexDir, columnMetadata.getFieldSpec())) {\n+      if (columnMetadata.isSingleValue()) {\n+        switch (columnMetadata.getDataType()) {\n+          case STRING:\n+            for (int i = 0; i < numDocs; i++) {\n+              int dictId = forwardIndexReader.getDictId(i, readerContext);\n+              jsonIndexCreator.add(dictionary.getStringValue(dictId).getBytes(\"UTF-8\"));\n+            }\n+            break;\n+          case BYTES:\n+            // Single-value column\n+            for (int i = 0; i < numDocs; i++) {\n+              int dictId = forwardIndexReader.getDictId(i, readerContext);\n+              jsonIndexCreator.add(dictionary.getBytesValue(dictId));\n+            }\n+            break;\n+          default:\n+            throw new IllegalStateException(\"Unsupported data type: \" + columnMetadata.getDataType());\n+        }\n+      } else {\n+        // Multi-value column\n+        throw new IllegalStateException(\"JSON Indexing is not supported on multi-valued columns \");\n+      }\n+      jsonIndexCreator.seal();\n+    }\n+  }\n+\n+  private void handleNonDictionaryBasedColumn(ColumnMetadata columnMetadata)\n+      throws IOException {\n+    FieldSpec.DataType dataType = columnMetadata.getDataType();\n+    if (dataType != FieldSpec.DataType.BYTES || dataType != FieldSpec.DataType.STRING) {\n+      throw new UnsupportedOperationException(\n+          \"JSON indexing is only supported for STRING/BYTES datatype but found: \" + dataType);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1OTc4NA=="}, "originalCommit": {"oid": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d"}, "originalPosition": 166}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d", "author": {"user": {"login": "kishoreg", "name": "Kishore Gopalakrishna"}}, "url": "https://github.com/apache/pinot/commit/ee6714c2ac4b6eea174d48e862fb7020494b6c1d", "committedDate": "2020-11-01T01:09:34Z", "message": "Adding index creator and reader"}, "afterCommit": {"oid": "9e6f83785d3e9886572d6d1bc7594ac8366c4031", "author": {"user": {"login": "kishoreg", "name": "Kishore Gopalakrishna"}}, "url": "https://github.com/apache/pinot/commit/9e6f83785d3e9886572d6d1bc7594ac8366c4031", "committedDate": "2020-11-03T04:16:28Z", "message": "adding example data for json"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9e6f83785d3e9886572d6d1bc7594ac8366c4031", "author": {"user": {"login": "kishoreg", "name": "Kishore Gopalakrishna"}}, "url": "https://github.com/apache/pinot/commit/9e6f83785d3e9886572d6d1bc7594ac8366c4031", "committedDate": "2020-11-03T04:16:28Z", "message": "adding example data for json"}, "afterCommit": {"oid": "64dba072325aef552c8b8ccc91b014fc7c46056e", "author": {"user": {"login": "kishoreg", "name": "Kishore Gopalakrishna"}}, "url": "https://github.com/apache/pinot/commit/64dba072325aef552c8b8ccc91b014fc7c46056e", "committedDate": "2020-11-03T19:56:52Z", "message": "adding example data for json"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "045f7ef200744ba2ca0403ed92d47c0e70f30321", "author": {"user": {"login": "kishoreg", "name": "Kishore Gopalakrishna"}}, "url": "https://github.com/apache/pinot/commit/045f7ef200744ba2ca0403ed92d47c0e70f30321", "committedDate": "2020-11-03T20:59:08Z", "message": "Fixing table config"}, "afterCommit": {"oid": "bd6b9ccf4a8c253d449e313128c5df5b41905b40", "author": {"user": {"login": "kishoreg", "name": "Kishore Gopalakrishna"}}, "url": "https://github.com/apache/pinot/commit/bd6b9ccf4a8c253d449e313128c5df5b41905b40", "committedDate": "2020-11-24T19:48:16Z", "message": "adding support querying based on array index"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "bd6b9ccf4a8c253d449e313128c5df5b41905b40", "author": {"user": {"login": "kishoreg", "name": "Kishore Gopalakrishna"}}, "url": "https://github.com/apache/pinot/commit/bd6b9ccf4a8c253d449e313128c5df5b41905b40", "committedDate": "2020-11-24T19:48:16Z", "message": "adding support querying based on array index"}, "afterCommit": {"oid": "36c968568cfb482af474048851c7fae000d07dcc", "author": {"user": {"login": "Jackie-Jiang", "name": "Xiaotian (Jackie) Jiang"}}, "url": "https://github.com/apache/pinot/commit/36c968568cfb482af474048851c7fae000d07dcc", "committedDate": "2020-12-11T03:29:24Z", "message": "Add json index support"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "36c968568cfb482af474048851c7fae000d07dcc", "author": {"user": {"login": "Jackie-Jiang", "name": "Xiaotian (Jackie) Jiang"}}, "url": "https://github.com/apache/pinot/commit/36c968568cfb482af474048851c7fae000d07dcc", "committedDate": "2020-12-11T03:29:24Z", "message": "Add json index support"}, "afterCommit": {"oid": "c0e5f33ec01d80cf76ccc39539872f9e613d0e67", "author": {"user": {"login": "Jackie-Jiang", "name": "Xiaotian (Jackie) Jiang"}}, "url": "https://github.com/apache/pinot/commit/c0e5f33ec01d80cf76ccc39539872f9e613d0e67", "committedDate": "2020-12-11T20:14:58Z", "message": "Add json index support"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c0e5f33ec01d80cf76ccc39539872f9e613d0e67", "author": {"user": {"login": "Jackie-Jiang", "name": "Xiaotian (Jackie) Jiang"}}, "url": "https://github.com/apache/pinot/commit/c0e5f33ec01d80cf76ccc39539872f9e613d0e67", "committedDate": "2020-12-11T20:14:58Z", "message": "Add json index support"}, "afterCommit": {"oid": "65457539f7e3cf4d4e6f27e58d2cb9f8bb3896ac", "author": {"user": {"login": "Jackie-Jiang", "name": "Xiaotian (Jackie) Jiang"}}, "url": "https://github.com/apache/pinot/commit/65457539f7e3cf4d4e6f27e58d2cb9f8bb3896ac", "committedDate": "2020-12-11T21:52:11Z", "message": "Add json index support"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "65457539f7e3cf4d4e6f27e58d2cb9f8bb3896ac", "author": {"user": {"login": "Jackie-Jiang", "name": "Xiaotian (Jackie) Jiang"}}, "url": "https://github.com/apache/pinot/commit/65457539f7e3cf4d4e6f27e58d2cb9f8bb3896ac", "committedDate": "2020-12-11T21:52:11Z", "message": "Add json index support"}, "afterCommit": {"oid": "7be263b8cd11340c7189fc0880aefb37264dbe0f", "author": {"user": {"login": "Jackie-Jiang", "name": "Xiaotian (Jackie) Jiang"}}, "url": "https://github.com/apache/pinot/commit/7be263b8cd11340c7189fc0880aefb37264dbe0f", "committedDate": "2020-12-14T18:54:40Z", "message": "Add json index support"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUxNzk1NDA4", "url": "https://github.com/apache/pinot/pull/6216#pullrequestreview-551795408", "createdAt": "2020-12-14T18:52:42Z", "commit": {"oid": "65457539f7e3cf4d4e6f27e58d2cb9f8bb3896ac"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNFQxODo1Mjo0MlrOIFgLoQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNFQxODo1NzozMVrOIFgg7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjY0MTA1Nw==", "bodyText": "Do we not have an existing index creator interface (for example, text index creator could have used the same interface)?", "url": "https://github.com/apache/pinot/pull/6216#discussion_r542641057", "createdAt": "2020-12-14T18:52:42Z", "author": {"login": "mayankshriv"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/creator/JsonIndexCreator.java", "diffHunk": "@@ -0,0 +1,42 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.creator;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+\n+\n+/**\n+ * Index creator for json index.\n+ */\n+public interface JsonIndexCreator extends Closeable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "65457539f7e3cf4d4e6f27e58d2cb9f8bb3896ac"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjY0NDY2MA==", "bodyText": "Add javadoc for file-format? Also, consider adding header - version, data-start-offset, etc, to help with maintaining compatability.", "url": "https://github.com/apache/pinot/pull/6216#discussion_r542644660", "createdAt": "2020-12-14T18:55:52Z", "author": {"login": "mayankshriv"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/creator/impl/inv/BitmapInvertedIndexWriter.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.creator.impl.inv;\n+\n+import java.io.Closeable;\n+import java.io.File;\n+import java.io.IOException;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.channels.FileChannel;\n+import org.apache.pinot.core.util.CleanerUtil;\n+import org.roaringbitmap.RoaringBitmap;\n+\n+\n+/**\n+ * Writer for bitmap inverted index file.\n+ */\n+public final class BitmapInvertedIndexWriter implements Closeable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7be263b8cd11340c7189fc0880aefb37264dbe0f"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjY0NTg2Mg==", "bodyText": "This should be a function of total doc size, not just num docs?", "url": "https://github.com/apache/pinot/pull/6216#discussion_r542645862", "createdAt": "2020-12-14T18:56:55Z", "author": {"login": "mayankshriv"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/creator/impl/inv/json/OffHeapJsonIndexCreator.java", "diffHunk": "@@ -0,0 +1,279 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.creator.impl.inv.json;\n+\n+import it.unimi.dsi.fastutil.longs.LongArrayList;\n+import it.unimi.dsi.fastutil.longs.LongList;\n+import java.io.BufferedOutputStream;\n+import java.io.DataOutputStream;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteOrder;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import org.apache.pinot.core.io.util.VarLengthValueWriter;\n+import org.apache.pinot.core.segment.creator.impl.inv.BitmapInvertedIndexWriter;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+import org.apache.pinot.spi.utils.StringUtils;\n+import org.roaringbitmap.RoaringBitmap;\n+import org.roaringbitmap.RoaringBitmapWriter;\n+import org.roaringbitmap.buffer.ImmutableRoaringBitmap;\n+import org.roaringbitmap.buffer.MutableRoaringBitmap;\n+\n+\n+/**\n+ * Implementation of {@link org.apache.pinot.core.segment.creator.JsonIndexCreator} that uses off-heap memory.\n+ * <p>The posting lists (map from value to doc ids) are initially stored in a TreeMap, then flushed into a file for\n+ * every 50000 documents (unflattened records) added. After all the documents are added, we read all the posting lists\n+ * from the file and merge them using a priority queue to calculate the final posting lists. Then we generate the string\n+ * dictionary and inverted index from the final posting lists and create the json index on top of them.\n+ */\n+public class OffHeapJsonIndexCreator extends BaseJsonIndexCreator {\n+  private static final int FLUSH_THRESHOLD = 50_000;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7be263b8cd11340c7189fc0880aefb37264dbe0f"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjY0NjUxMQ==", "bodyText": "Perhaps specify when to use on-heap vs off-heap?", "url": "https://github.com/apache/pinot/pull/6216#discussion_r542646511", "createdAt": "2020-12-14T18:57:31Z", "author": {"login": "mayankshriv"}, "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/creator/impl/inv/json/OnHeapJsonIndexCreator.java", "diffHunk": "@@ -0,0 +1,57 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.creator.impl.inv.json;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.pinot.core.io.util.VarLengthValueWriter;\n+import org.apache.pinot.core.segment.creator.impl.inv.BitmapInvertedIndexWriter;\n+import org.apache.pinot.spi.utils.StringUtils;\n+import org.roaringbitmap.RoaringBitmap;\n+import org.roaringbitmap.RoaringBitmapWriter;\n+\n+\n+/**\n+ * Implementation of {@link org.apache.pinot.core.segment.creator.JsonIndexCreator} that uses on-heap memory.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7be263b8cd11340c7189fc0880aefb37264dbe0f"}, "originalPosition": 32}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7be263b8cd11340c7189fc0880aefb37264dbe0f", "author": {"user": {"login": "Jackie-Jiang", "name": "Xiaotian (Jackie) Jiang"}}, "url": "https://github.com/apache/pinot/commit/7be263b8cd11340c7189fc0880aefb37264dbe0f", "committedDate": "2020-12-14T18:54:40Z", "message": "Add json index support"}, "afterCommit": {"oid": "b8aab328e686c460b30d8b32fb516dd84d26c712", "author": {"user": {"login": "Jackie-Jiang", "name": "Xiaotian (Jackie) Jiang"}}, "url": "https://github.com/apache/pinot/commit/b8aab328e686c460b30d8b32fb516dd84d26c712", "committedDate": "2020-12-14T21:04:21Z", "message": "Add json index support"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU4OTUwNzAx", "url": "https://github.com/apache/pinot/pull/6216#pullrequestreview-558950701", "createdAt": "2020-12-27T18:17:24Z", "commit": {"oid": "b8aab328e686c460b30d8b32fb516dd84d26c712"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c217c9e15e6147af07e9c8a70616ab3660696ffa", "author": {"user": {"login": "Jackie-Jiang", "name": "Xiaotian (Jackie) Jiang"}}, "url": "https://github.com/apache/pinot/commit/c217c9e15e6147af07e9c8a70616ab3660696ffa", "committedDate": "2020-12-28T21:47:31Z", "message": "Add json index support"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "b8aab328e686c460b30d8b32fb516dd84d26c712", "author": {"user": {"login": "Jackie-Jiang", "name": "Xiaotian (Jackie) Jiang"}}, "url": "https://github.com/apache/pinot/commit/b8aab328e686c460b30d8b32fb516dd84d26c712", "committedDate": "2020-12-14T21:04:21Z", "message": "Add json index support"}, "afterCommit": {"oid": "c217c9e15e6147af07e9c8a70616ab3660696ffa", "author": {"user": {"login": "Jackie-Jiang", "name": "Xiaotian (Jackie) Jiang"}}, "url": "https://github.com/apache/pinot/commit/c217c9e15e6147af07e9c8a70616ab3660696ffa", "committedDate": "2020-12-28T21:47:31Z", "message": "Add json index support"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1719, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}