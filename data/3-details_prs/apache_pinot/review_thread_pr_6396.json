{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQ2ODI0NjY4", "number": 6396, "reviewThreads": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQxNjoxMjo1OVrOFK9BfA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQwNDozNjo0MVrOFP2R7g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ3MDI5ODg0OnYy", "diffSide": "RIGHT", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/helix/ControllerRequestURLBuilder.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQxNjoxMjo1OVrOIN3nvw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQyMTo0Nzo1NlrOIPXDmg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQxMzY5NQ==", "bodyText": "Better to use File.separator than /?", "url": "https://github.com/apache/pinot/pull/6396#discussion_r551413695", "createdAt": "2021-01-04T16:12:59Z", "author": {"login": "mayankshriv"}, "path": "pinot-controller/src/main/java/org/apache/pinot/controller/helix/ControllerRequestURLBuilder.java", "diffHunk": "@@ -211,6 +211,9 @@ public String forTableView(String tableName, String view, @Nullable String table\n     }\n     return url;\n   }\n+  public String forTableSchemaGet(String tableName) {\n+    return StringUtil.join(\"/\", _baseUrl, \"tables\", tableName, \"schema\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f924f2fa0a4e3c0460c6ae6d91b7b359357e57ba"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjk3NzMwNg==", "bodyText": "I feel it should be / since it's for constructing URL, not filesystem path.", "url": "https://github.com/apache/pinot/pull/6396#discussion_r552977306", "createdAt": "2021-01-06T21:47:56Z", "author": {"login": "xiangfu0"}, "path": "pinot-controller/src/main/java/org/apache/pinot/controller/helix/ControllerRequestURLBuilder.java", "diffHunk": "@@ -211,6 +211,9 @@ public String forTableView(String tableName, String view, @Nullable String table\n     }\n     return url;\n   }\n+  public String forTableSchemaGet(String tableName) {\n+    return StringUtil.join(\"/\", _baseUrl, \"tables\", tableName, \"schema\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQxMzY5NQ=="}, "originalCommit": {"oid": "f924f2fa0a4e3c0460c6ae6d91b7b359357e57ba"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ3MDMyODg3OnYy", "diffSide": "RIGHT", "path": "pinot-tools/src/main/java/org/apache/pinot/tools/admin/command/ImportDataCommand.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQxNjoyMDowMVrOIN35dw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wN1QwMTo1NToxNlrOIPcbOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQxODIzMQ==", "bodyText": "File system to use should come from input argument?", "url": "https://github.com/apache/pinot/pull/6396#discussion_r551418231", "createdAt": "2021-01-04T16:20:01Z", "author": {"login": "mayankshriv"}, "path": "pinot-tools/src/main/java/org/apache/pinot/tools/admin/command/ImportDataCommand.java", "diffHunk": "@@ -0,0 +1,340 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.tools.admin.command;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableMap;\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.commons.codec.digest.DigestUtils;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.controller.helix.ControllerRequestURLBuilder;\n+import org.apache.pinot.spi.data.readers.FileFormat;\n+import org.apache.pinot.spi.ingestion.batch.BatchConfigProperties;\n+import org.apache.pinot.spi.ingestion.batch.IngestionJobLauncher;\n+import org.apache.pinot.spi.ingestion.batch.spec.ExecutionFrameworkSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotClusterSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotFSSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PushJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.RecordReaderSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentGenerationJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentNameGeneratorSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.TableSpec;\n+import org.apache.pinot.spi.utils.IngestionConfigUtils;\n+import org.apache.pinot.tools.Command;\n+import org.kohsuke.args4j.Option;\n+import org.kohsuke.args4j.spi.StringArrayOptionHandler;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Class to implement ImportData command.\n+ */\n+@SuppressWarnings(\"unused\")\n+public class ImportDataCommand extends AbstractBaseAdminCommand implements Command {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(ImportDataCommand.class);\n+  private static final String SEGMENT_NAME = \"segment.name\";\n+\n+  @Option(name = \"-dataFilePath\", required = true, metaVar = \"<string>\", usage = \"data file path.\")\n+  private String _dataFilePath;\n+\n+  @Option(name = \"-format\", required = true, metaVar = \"<AVRO/CSV/JSON/THRIFT/PARQUET/ORC>\", usage = \"Input data format.\")\n+  private FileFormat _format;\n+\n+  @Option(name = \"-table\", required = true, metaVar = \"<string>\", usage = \"Table name.\")\n+  private String _table;\n+\n+  @Option(name = \"-controllerURI\", metaVar = \"<string>\", usage = \"Pinot Controller URI.\")\n+  private String _controllerURI = \"http://localhost:9000\";\n+\n+  @Option(name = \"-tempDir\", metaVar = \"<string>\", usage = \"Temporary directory used to hold data during segment creation.\")\n+  private String _tempDir = new File(FileUtils.getTempDirectory(), getClass().getSimpleName()).getAbsolutePath();\n+\n+  @Option(name = \"-extraConfigs\", metaVar = \"<extra configs>\", handler = StringArrayOptionHandler.class, usage = \"Extra configs to be set.\")\n+  private List<String> _extraConfigs;\n+\n+  @SuppressWarnings(\"FieldCanBeLocal\")\n+  @Option(name = \"-help\", help = true, aliases = {\"-h\", \"--h\", \"--help\"}, usage = \"Print this message.\")\n+  private boolean _help = false;\n+\n+  public ImportDataCommand setDataFilePath(String dataFilePath) {\n+    _dataFilePath = dataFilePath;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setFormat(FileFormat format) {\n+    _format = format;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setTable(String table) {\n+    _table = table;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setControllerURI(String controllerURI) {\n+    _controllerURI = controllerURI;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setTempDir(String tempDir) {\n+    _tempDir = tempDir;\n+    return this;\n+  }\n+\n+  public List<String> getExtraConfigs() {\n+    return _extraConfigs;\n+  }\n+\n+  public ImportDataCommand setExtraConfigs(List<String> extraConfigs) {\n+    _extraConfigs = extraConfigs;\n+    return this;\n+  }\n+\n+  public String getDataFilePath() {\n+    return _dataFilePath;\n+  }\n+\n+  public FileFormat getFormat() {\n+    return _format;\n+  }\n+\n+  public String getTable() {\n+    return _table;\n+  }\n+\n+  public String getControllerURI() {\n+    return _controllerURI;\n+  }\n+\n+  public String getTempDir() {\n+    return _tempDir;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    String results = String\n+        .format(\"InsertData -dataFilePath %s -format %s -table %s -controllerURI %s -tempDir %s\", _dataFilePath,\n+            _format, _table, _controllerURI, _tempDir);\n+    if (_extraConfigs != null) {\n+      results += \" -extraConfigs \" + Arrays.toString(_extraConfigs.toArray());\n+    }\n+    return results;\n+  }\n+\n+  @Override\n+  public final String getName() {\n+    return \"InsertData\";\n+  }\n+\n+  @Override\n+  public String description() {\n+    return \"Insert data into Pinot cluster.\";\n+  }\n+\n+  @Override\n+  public boolean getHelp() {\n+    return _help;\n+  }\n+\n+  @Override\n+  public boolean execute()\n+      throws IOException {\n+    LOGGER.info(\"Executing command: {}\", toString());\n+    Preconditions.checkArgument(_table != null, \"'table' must be specified\");\n+    Preconditions.checkArgument(_format != null, \"'format' must be specified\");\n+    Preconditions.checkArgument(_dataFilePath != null, \"'dataFilePath' must be specified\");\n+\n+    try {\n+\n+      URI dataFileURI = URI.create(_dataFilePath);\n+      if ((dataFileURI.getScheme() == null)) {\n+        File dataFile = new File(_dataFilePath);\n+        Preconditions.checkArgument(dataFile.exists(), \"'dataFile': '%s' doesn't exist\", dataFile);\n+        LOGGER.info(\"Found data files: {} of format: {}\", dataFile, _format);\n+      }\n+\n+      initTempDir();\n+      IngestionJobLauncher.runIngestionJob(generateSegmentGenerationJobSpec());\n+      LOGGER.info(\"Successfully load data from {} to Pinot.\", _dataFilePath);\n+      return true;\n+    } catch (Exception e) {\n+      throw e;\n+    } finally {\n+      FileUtils.deleteQuietly(new File(_tempDir));\n+    }\n+  }\n+\n+  private void initTempDir()\n+      throws IOException {\n+    File tempDir = new File(_tempDir);\n+    if (tempDir.exists()) {\n+      LOGGER.info(\"Deleting the existing 'tempDir': {}\", tempDir);\n+      FileUtils.forceDelete(tempDir);\n+    }\n+    FileUtils.forceMkdir(tempDir);\n+  }\n+\n+  private SegmentGenerationJobSpec generateSegmentGenerationJobSpec() {\n+    final Map<String, String> extraConfigs = getExtraConfigs(_extraConfigs);\n+\n+    SegmentGenerationJobSpec spec = new SegmentGenerationJobSpec();\n+    URI dataFileURI = URI.create(_dataFilePath);\n+    URI parent = dataFileURI.getPath().endsWith(\"/\") ? dataFileURI.resolve(\"..\") : dataFileURI.resolve(\".\");\n+    spec.setInputDirURI(parent.toString());\n+    spec.setIncludeFileNamePattern(\"glob:**\" + dataFileURI.getPath());\n+    spec.setOutputDirURI(_tempDir);\n+    spec.setCleanUpOutputDir(true);\n+    spec.setOverwriteOutput(true);\n+    spec.setJobType(\"SegmentCreationAndTarPush\");\n+\n+    // set ExecutionFrameworkSpec\n+    ExecutionFrameworkSpec executionFrameworkSpec = new ExecutionFrameworkSpec();\n+    executionFrameworkSpec.setName(\"standalone\");\n+    executionFrameworkSpec.setSegmentGenerationJobRunnerClassName(\n+        \"org.apache.pinot.plugin.ingestion.batch.standalone.SegmentGenerationJobRunner\");\n+    executionFrameworkSpec.setSegmentTarPushJobRunnerClassName(\n+        \"org.apache.pinot.plugin.ingestion.batch.standalone.SegmentTarPushJobRunner\");\n+    spec.setExecutionFrameworkSpec(executionFrameworkSpec);\n+\n+    // set PinotFSSpecs\n+    List<PinotFSSpec> pinotFSSpecs = new ArrayList<>();\n+    pinotFSSpecs.add(getPinotFSSpec(\"file\", \"org.apache.pinot.spi.filesystem.LocalPinotFS\", Collections.emptyMap()));\n+    pinotFSSpecs\n+        .add(getPinotFSSpec(\"s3\", \"org.apache.pinot.plugin.filesystem.S3PinotFS\", getS3PinotFSConfigs(extraConfigs)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f924f2fa0a4e3c0460c6ae6d91b7b359357e57ba"}, "originalPosition": 228}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzA2NTI3Mw==", "bodyText": "I want to infer it and user can set extra configs using -extraConfigs", "url": "https://github.com/apache/pinot/pull/6396#discussion_r553065273", "createdAt": "2021-01-07T01:55:16Z", "author": {"login": "xiangfu0"}, "path": "pinot-tools/src/main/java/org/apache/pinot/tools/admin/command/ImportDataCommand.java", "diffHunk": "@@ -0,0 +1,340 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.tools.admin.command;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableMap;\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.commons.codec.digest.DigestUtils;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.controller.helix.ControllerRequestURLBuilder;\n+import org.apache.pinot.spi.data.readers.FileFormat;\n+import org.apache.pinot.spi.ingestion.batch.BatchConfigProperties;\n+import org.apache.pinot.spi.ingestion.batch.IngestionJobLauncher;\n+import org.apache.pinot.spi.ingestion.batch.spec.ExecutionFrameworkSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotClusterSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotFSSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PushJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.RecordReaderSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentGenerationJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentNameGeneratorSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.TableSpec;\n+import org.apache.pinot.spi.utils.IngestionConfigUtils;\n+import org.apache.pinot.tools.Command;\n+import org.kohsuke.args4j.Option;\n+import org.kohsuke.args4j.spi.StringArrayOptionHandler;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Class to implement ImportData command.\n+ */\n+@SuppressWarnings(\"unused\")\n+public class ImportDataCommand extends AbstractBaseAdminCommand implements Command {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(ImportDataCommand.class);\n+  private static final String SEGMENT_NAME = \"segment.name\";\n+\n+  @Option(name = \"-dataFilePath\", required = true, metaVar = \"<string>\", usage = \"data file path.\")\n+  private String _dataFilePath;\n+\n+  @Option(name = \"-format\", required = true, metaVar = \"<AVRO/CSV/JSON/THRIFT/PARQUET/ORC>\", usage = \"Input data format.\")\n+  private FileFormat _format;\n+\n+  @Option(name = \"-table\", required = true, metaVar = \"<string>\", usage = \"Table name.\")\n+  private String _table;\n+\n+  @Option(name = \"-controllerURI\", metaVar = \"<string>\", usage = \"Pinot Controller URI.\")\n+  private String _controllerURI = \"http://localhost:9000\";\n+\n+  @Option(name = \"-tempDir\", metaVar = \"<string>\", usage = \"Temporary directory used to hold data during segment creation.\")\n+  private String _tempDir = new File(FileUtils.getTempDirectory(), getClass().getSimpleName()).getAbsolutePath();\n+\n+  @Option(name = \"-extraConfigs\", metaVar = \"<extra configs>\", handler = StringArrayOptionHandler.class, usage = \"Extra configs to be set.\")\n+  private List<String> _extraConfigs;\n+\n+  @SuppressWarnings(\"FieldCanBeLocal\")\n+  @Option(name = \"-help\", help = true, aliases = {\"-h\", \"--h\", \"--help\"}, usage = \"Print this message.\")\n+  private boolean _help = false;\n+\n+  public ImportDataCommand setDataFilePath(String dataFilePath) {\n+    _dataFilePath = dataFilePath;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setFormat(FileFormat format) {\n+    _format = format;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setTable(String table) {\n+    _table = table;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setControllerURI(String controllerURI) {\n+    _controllerURI = controllerURI;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setTempDir(String tempDir) {\n+    _tempDir = tempDir;\n+    return this;\n+  }\n+\n+  public List<String> getExtraConfigs() {\n+    return _extraConfigs;\n+  }\n+\n+  public ImportDataCommand setExtraConfigs(List<String> extraConfigs) {\n+    _extraConfigs = extraConfigs;\n+    return this;\n+  }\n+\n+  public String getDataFilePath() {\n+    return _dataFilePath;\n+  }\n+\n+  public FileFormat getFormat() {\n+    return _format;\n+  }\n+\n+  public String getTable() {\n+    return _table;\n+  }\n+\n+  public String getControllerURI() {\n+    return _controllerURI;\n+  }\n+\n+  public String getTempDir() {\n+    return _tempDir;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    String results = String\n+        .format(\"InsertData -dataFilePath %s -format %s -table %s -controllerURI %s -tempDir %s\", _dataFilePath,\n+            _format, _table, _controllerURI, _tempDir);\n+    if (_extraConfigs != null) {\n+      results += \" -extraConfigs \" + Arrays.toString(_extraConfigs.toArray());\n+    }\n+    return results;\n+  }\n+\n+  @Override\n+  public final String getName() {\n+    return \"InsertData\";\n+  }\n+\n+  @Override\n+  public String description() {\n+    return \"Insert data into Pinot cluster.\";\n+  }\n+\n+  @Override\n+  public boolean getHelp() {\n+    return _help;\n+  }\n+\n+  @Override\n+  public boolean execute()\n+      throws IOException {\n+    LOGGER.info(\"Executing command: {}\", toString());\n+    Preconditions.checkArgument(_table != null, \"'table' must be specified\");\n+    Preconditions.checkArgument(_format != null, \"'format' must be specified\");\n+    Preconditions.checkArgument(_dataFilePath != null, \"'dataFilePath' must be specified\");\n+\n+    try {\n+\n+      URI dataFileURI = URI.create(_dataFilePath);\n+      if ((dataFileURI.getScheme() == null)) {\n+        File dataFile = new File(_dataFilePath);\n+        Preconditions.checkArgument(dataFile.exists(), \"'dataFile': '%s' doesn't exist\", dataFile);\n+        LOGGER.info(\"Found data files: {} of format: {}\", dataFile, _format);\n+      }\n+\n+      initTempDir();\n+      IngestionJobLauncher.runIngestionJob(generateSegmentGenerationJobSpec());\n+      LOGGER.info(\"Successfully load data from {} to Pinot.\", _dataFilePath);\n+      return true;\n+    } catch (Exception e) {\n+      throw e;\n+    } finally {\n+      FileUtils.deleteQuietly(new File(_tempDir));\n+    }\n+  }\n+\n+  private void initTempDir()\n+      throws IOException {\n+    File tempDir = new File(_tempDir);\n+    if (tempDir.exists()) {\n+      LOGGER.info(\"Deleting the existing 'tempDir': {}\", tempDir);\n+      FileUtils.forceDelete(tempDir);\n+    }\n+    FileUtils.forceMkdir(tempDir);\n+  }\n+\n+  private SegmentGenerationJobSpec generateSegmentGenerationJobSpec() {\n+    final Map<String, String> extraConfigs = getExtraConfigs(_extraConfigs);\n+\n+    SegmentGenerationJobSpec spec = new SegmentGenerationJobSpec();\n+    URI dataFileURI = URI.create(_dataFilePath);\n+    URI parent = dataFileURI.getPath().endsWith(\"/\") ? dataFileURI.resolve(\"..\") : dataFileURI.resolve(\".\");\n+    spec.setInputDirURI(parent.toString());\n+    spec.setIncludeFileNamePattern(\"glob:**\" + dataFileURI.getPath());\n+    spec.setOutputDirURI(_tempDir);\n+    spec.setCleanUpOutputDir(true);\n+    spec.setOverwriteOutput(true);\n+    spec.setJobType(\"SegmentCreationAndTarPush\");\n+\n+    // set ExecutionFrameworkSpec\n+    ExecutionFrameworkSpec executionFrameworkSpec = new ExecutionFrameworkSpec();\n+    executionFrameworkSpec.setName(\"standalone\");\n+    executionFrameworkSpec.setSegmentGenerationJobRunnerClassName(\n+        \"org.apache.pinot.plugin.ingestion.batch.standalone.SegmentGenerationJobRunner\");\n+    executionFrameworkSpec.setSegmentTarPushJobRunnerClassName(\n+        \"org.apache.pinot.plugin.ingestion.batch.standalone.SegmentTarPushJobRunner\");\n+    spec.setExecutionFrameworkSpec(executionFrameworkSpec);\n+\n+    // set PinotFSSpecs\n+    List<PinotFSSpec> pinotFSSpecs = new ArrayList<>();\n+    pinotFSSpecs.add(getPinotFSSpec(\"file\", \"org.apache.pinot.spi.filesystem.LocalPinotFS\", Collections.emptyMap()));\n+    pinotFSSpecs\n+        .add(getPinotFSSpec(\"s3\", \"org.apache.pinot.plugin.filesystem.S3PinotFS\", getS3PinotFSConfigs(extraConfigs)));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQxODIzMQ=="}, "originalCommit": {"oid": "f924f2fa0a4e3c0460c6ae6d91b7b359357e57ba"}, "originalPosition": 228}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ3MDMzMTI1OnYy", "diffSide": "RIGHT", "path": "pinot-tools/src/main/java/org/apache/pinot/tools/admin/command/ImportDataCommand.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQxNjoyMDo0MFrOIN368A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wN1QwMTo1NDozNlrOIPcaaw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQxODYwOA==", "bodyText": "SegmentNameGenerator should also come from input arg?", "url": "https://github.com/apache/pinot/pull/6396#discussion_r551418608", "createdAt": "2021-01-04T16:20:40Z", "author": {"login": "mayankshriv"}, "path": "pinot-tools/src/main/java/org/apache/pinot/tools/admin/command/ImportDataCommand.java", "diffHunk": "@@ -0,0 +1,340 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.tools.admin.command;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableMap;\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.commons.codec.digest.DigestUtils;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.controller.helix.ControllerRequestURLBuilder;\n+import org.apache.pinot.spi.data.readers.FileFormat;\n+import org.apache.pinot.spi.ingestion.batch.BatchConfigProperties;\n+import org.apache.pinot.spi.ingestion.batch.IngestionJobLauncher;\n+import org.apache.pinot.spi.ingestion.batch.spec.ExecutionFrameworkSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotClusterSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotFSSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PushJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.RecordReaderSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentGenerationJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentNameGeneratorSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.TableSpec;\n+import org.apache.pinot.spi.utils.IngestionConfigUtils;\n+import org.apache.pinot.tools.Command;\n+import org.kohsuke.args4j.Option;\n+import org.kohsuke.args4j.spi.StringArrayOptionHandler;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Class to implement ImportData command.\n+ */\n+@SuppressWarnings(\"unused\")\n+public class ImportDataCommand extends AbstractBaseAdminCommand implements Command {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(ImportDataCommand.class);\n+  private static final String SEGMENT_NAME = \"segment.name\";\n+\n+  @Option(name = \"-dataFilePath\", required = true, metaVar = \"<string>\", usage = \"data file path.\")\n+  private String _dataFilePath;\n+\n+  @Option(name = \"-format\", required = true, metaVar = \"<AVRO/CSV/JSON/THRIFT/PARQUET/ORC>\", usage = \"Input data format.\")\n+  private FileFormat _format;\n+\n+  @Option(name = \"-table\", required = true, metaVar = \"<string>\", usage = \"Table name.\")\n+  private String _table;\n+\n+  @Option(name = \"-controllerURI\", metaVar = \"<string>\", usage = \"Pinot Controller URI.\")\n+  private String _controllerURI = \"http://localhost:9000\";\n+\n+  @Option(name = \"-tempDir\", metaVar = \"<string>\", usage = \"Temporary directory used to hold data during segment creation.\")\n+  private String _tempDir = new File(FileUtils.getTempDirectory(), getClass().getSimpleName()).getAbsolutePath();\n+\n+  @Option(name = \"-extraConfigs\", metaVar = \"<extra configs>\", handler = StringArrayOptionHandler.class, usage = \"Extra configs to be set.\")\n+  private List<String> _extraConfigs;\n+\n+  @SuppressWarnings(\"FieldCanBeLocal\")\n+  @Option(name = \"-help\", help = true, aliases = {\"-h\", \"--h\", \"--help\"}, usage = \"Print this message.\")\n+  private boolean _help = false;\n+\n+  public ImportDataCommand setDataFilePath(String dataFilePath) {\n+    _dataFilePath = dataFilePath;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setFormat(FileFormat format) {\n+    _format = format;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setTable(String table) {\n+    _table = table;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setControllerURI(String controllerURI) {\n+    _controllerURI = controllerURI;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setTempDir(String tempDir) {\n+    _tempDir = tempDir;\n+    return this;\n+  }\n+\n+  public List<String> getExtraConfigs() {\n+    return _extraConfigs;\n+  }\n+\n+  public ImportDataCommand setExtraConfigs(List<String> extraConfigs) {\n+    _extraConfigs = extraConfigs;\n+    return this;\n+  }\n+\n+  public String getDataFilePath() {\n+    return _dataFilePath;\n+  }\n+\n+  public FileFormat getFormat() {\n+    return _format;\n+  }\n+\n+  public String getTable() {\n+    return _table;\n+  }\n+\n+  public String getControllerURI() {\n+    return _controllerURI;\n+  }\n+\n+  public String getTempDir() {\n+    return _tempDir;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    String results = String\n+        .format(\"InsertData -dataFilePath %s -format %s -table %s -controllerURI %s -tempDir %s\", _dataFilePath,\n+            _format, _table, _controllerURI, _tempDir);\n+    if (_extraConfigs != null) {\n+      results += \" -extraConfigs \" + Arrays.toString(_extraConfigs.toArray());\n+    }\n+    return results;\n+  }\n+\n+  @Override\n+  public final String getName() {\n+    return \"InsertData\";\n+  }\n+\n+  @Override\n+  public String description() {\n+    return \"Insert data into Pinot cluster.\";\n+  }\n+\n+  @Override\n+  public boolean getHelp() {\n+    return _help;\n+  }\n+\n+  @Override\n+  public boolean execute()\n+      throws IOException {\n+    LOGGER.info(\"Executing command: {}\", toString());\n+    Preconditions.checkArgument(_table != null, \"'table' must be specified\");\n+    Preconditions.checkArgument(_format != null, \"'format' must be specified\");\n+    Preconditions.checkArgument(_dataFilePath != null, \"'dataFilePath' must be specified\");\n+\n+    try {\n+\n+      URI dataFileURI = URI.create(_dataFilePath);\n+      if ((dataFileURI.getScheme() == null)) {\n+        File dataFile = new File(_dataFilePath);\n+        Preconditions.checkArgument(dataFile.exists(), \"'dataFile': '%s' doesn't exist\", dataFile);\n+        LOGGER.info(\"Found data files: {} of format: {}\", dataFile, _format);\n+      }\n+\n+      initTempDir();\n+      IngestionJobLauncher.runIngestionJob(generateSegmentGenerationJobSpec());\n+      LOGGER.info(\"Successfully load data from {} to Pinot.\", _dataFilePath);\n+      return true;\n+    } catch (Exception e) {\n+      throw e;\n+    } finally {\n+      FileUtils.deleteQuietly(new File(_tempDir));\n+    }\n+  }\n+\n+  private void initTempDir()\n+      throws IOException {\n+    File tempDir = new File(_tempDir);\n+    if (tempDir.exists()) {\n+      LOGGER.info(\"Deleting the existing 'tempDir': {}\", tempDir);\n+      FileUtils.forceDelete(tempDir);\n+    }\n+    FileUtils.forceMkdir(tempDir);\n+  }\n+\n+  private SegmentGenerationJobSpec generateSegmentGenerationJobSpec() {\n+    final Map<String, String> extraConfigs = getExtraConfigs(_extraConfigs);\n+\n+    SegmentGenerationJobSpec spec = new SegmentGenerationJobSpec();\n+    URI dataFileURI = URI.create(_dataFilePath);\n+    URI parent = dataFileURI.getPath().endsWith(\"/\") ? dataFileURI.resolve(\"..\") : dataFileURI.resolve(\".\");\n+    spec.setInputDirURI(parent.toString());\n+    spec.setIncludeFileNamePattern(\"glob:**\" + dataFileURI.getPath());\n+    spec.setOutputDirURI(_tempDir);\n+    spec.setCleanUpOutputDir(true);\n+    spec.setOverwriteOutput(true);\n+    spec.setJobType(\"SegmentCreationAndTarPush\");\n+\n+    // set ExecutionFrameworkSpec\n+    ExecutionFrameworkSpec executionFrameworkSpec = new ExecutionFrameworkSpec();\n+    executionFrameworkSpec.setName(\"standalone\");\n+    executionFrameworkSpec.setSegmentGenerationJobRunnerClassName(\n+        \"org.apache.pinot.plugin.ingestion.batch.standalone.SegmentGenerationJobRunner\");\n+    executionFrameworkSpec.setSegmentTarPushJobRunnerClassName(\n+        \"org.apache.pinot.plugin.ingestion.batch.standalone.SegmentTarPushJobRunner\");\n+    spec.setExecutionFrameworkSpec(executionFrameworkSpec);\n+\n+    // set PinotFSSpecs\n+    List<PinotFSSpec> pinotFSSpecs = new ArrayList<>();\n+    pinotFSSpecs.add(getPinotFSSpec(\"file\", \"org.apache.pinot.spi.filesystem.LocalPinotFS\", Collections.emptyMap()));\n+    pinotFSSpecs\n+        .add(getPinotFSSpec(\"s3\", \"org.apache.pinot.plugin.filesystem.S3PinotFS\", getS3PinotFSConfigs(extraConfigs)));\n+    spec.setPinotFSSpecs(pinotFSSpecs);\n+\n+    // set RecordReaderSpec\n+    RecordReaderSpec recordReaderSpec = new RecordReaderSpec();\n+    recordReaderSpec.setDataFormat(_format.name());\n+    recordReaderSpec.setClassName(getRecordReaderClass(_format));\n+    recordReaderSpec.setConfigClassName(getRecordReaderConfigClass(_format));\n+    recordReaderSpec.setConfigs(IngestionConfigUtils.getRecordReaderProps(extraConfigs));\n+    spec.setRecordReaderSpec(recordReaderSpec);\n+\n+    // set TableSpec\n+    TableSpec tableSpec = new TableSpec();\n+    tableSpec.setTableName(_table);\n+    tableSpec.setSchemaURI(ControllerRequestURLBuilder.baseUrl(_controllerURI).forTableSchemaGet(_table));\n+    tableSpec.setTableConfigURI(ControllerRequestURLBuilder.baseUrl(_controllerURI).forTableGet(_table));\n+    spec.setTableSpec(tableSpec);\n+\n+    // set SegmentNameGeneratorSpec\n+    SegmentNameGeneratorSpec segmentNameGeneratorSpec = new SegmentNameGeneratorSpec();\n+    segmentNameGeneratorSpec\n+        .setType(org.apache.pinot.spi.ingestion.batch.BatchConfigProperties.SegmentNameGeneratorType.FIXED);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f924f2fa0a4e3c0460c6ae6d91b7b359357e57ba"}, "originalPosition": 249}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzA2NTA2Nw==", "bodyText": "Added an optional param", "url": "https://github.com/apache/pinot/pull/6396#discussion_r553065067", "createdAt": "2021-01-07T01:54:36Z", "author": {"login": "xiangfu0"}, "path": "pinot-tools/src/main/java/org/apache/pinot/tools/admin/command/ImportDataCommand.java", "diffHunk": "@@ -0,0 +1,340 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.tools.admin.command;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableMap;\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.commons.codec.digest.DigestUtils;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.controller.helix.ControllerRequestURLBuilder;\n+import org.apache.pinot.spi.data.readers.FileFormat;\n+import org.apache.pinot.spi.ingestion.batch.BatchConfigProperties;\n+import org.apache.pinot.spi.ingestion.batch.IngestionJobLauncher;\n+import org.apache.pinot.spi.ingestion.batch.spec.ExecutionFrameworkSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotClusterSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotFSSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PushJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.RecordReaderSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentGenerationJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentNameGeneratorSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.TableSpec;\n+import org.apache.pinot.spi.utils.IngestionConfigUtils;\n+import org.apache.pinot.tools.Command;\n+import org.kohsuke.args4j.Option;\n+import org.kohsuke.args4j.spi.StringArrayOptionHandler;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Class to implement ImportData command.\n+ */\n+@SuppressWarnings(\"unused\")\n+public class ImportDataCommand extends AbstractBaseAdminCommand implements Command {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(ImportDataCommand.class);\n+  private static final String SEGMENT_NAME = \"segment.name\";\n+\n+  @Option(name = \"-dataFilePath\", required = true, metaVar = \"<string>\", usage = \"data file path.\")\n+  private String _dataFilePath;\n+\n+  @Option(name = \"-format\", required = true, metaVar = \"<AVRO/CSV/JSON/THRIFT/PARQUET/ORC>\", usage = \"Input data format.\")\n+  private FileFormat _format;\n+\n+  @Option(name = \"-table\", required = true, metaVar = \"<string>\", usage = \"Table name.\")\n+  private String _table;\n+\n+  @Option(name = \"-controllerURI\", metaVar = \"<string>\", usage = \"Pinot Controller URI.\")\n+  private String _controllerURI = \"http://localhost:9000\";\n+\n+  @Option(name = \"-tempDir\", metaVar = \"<string>\", usage = \"Temporary directory used to hold data during segment creation.\")\n+  private String _tempDir = new File(FileUtils.getTempDirectory(), getClass().getSimpleName()).getAbsolutePath();\n+\n+  @Option(name = \"-extraConfigs\", metaVar = \"<extra configs>\", handler = StringArrayOptionHandler.class, usage = \"Extra configs to be set.\")\n+  private List<String> _extraConfigs;\n+\n+  @SuppressWarnings(\"FieldCanBeLocal\")\n+  @Option(name = \"-help\", help = true, aliases = {\"-h\", \"--h\", \"--help\"}, usage = \"Print this message.\")\n+  private boolean _help = false;\n+\n+  public ImportDataCommand setDataFilePath(String dataFilePath) {\n+    _dataFilePath = dataFilePath;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setFormat(FileFormat format) {\n+    _format = format;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setTable(String table) {\n+    _table = table;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setControllerURI(String controllerURI) {\n+    _controllerURI = controllerURI;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setTempDir(String tempDir) {\n+    _tempDir = tempDir;\n+    return this;\n+  }\n+\n+  public List<String> getExtraConfigs() {\n+    return _extraConfigs;\n+  }\n+\n+  public ImportDataCommand setExtraConfigs(List<String> extraConfigs) {\n+    _extraConfigs = extraConfigs;\n+    return this;\n+  }\n+\n+  public String getDataFilePath() {\n+    return _dataFilePath;\n+  }\n+\n+  public FileFormat getFormat() {\n+    return _format;\n+  }\n+\n+  public String getTable() {\n+    return _table;\n+  }\n+\n+  public String getControllerURI() {\n+    return _controllerURI;\n+  }\n+\n+  public String getTempDir() {\n+    return _tempDir;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    String results = String\n+        .format(\"InsertData -dataFilePath %s -format %s -table %s -controllerURI %s -tempDir %s\", _dataFilePath,\n+            _format, _table, _controllerURI, _tempDir);\n+    if (_extraConfigs != null) {\n+      results += \" -extraConfigs \" + Arrays.toString(_extraConfigs.toArray());\n+    }\n+    return results;\n+  }\n+\n+  @Override\n+  public final String getName() {\n+    return \"InsertData\";\n+  }\n+\n+  @Override\n+  public String description() {\n+    return \"Insert data into Pinot cluster.\";\n+  }\n+\n+  @Override\n+  public boolean getHelp() {\n+    return _help;\n+  }\n+\n+  @Override\n+  public boolean execute()\n+      throws IOException {\n+    LOGGER.info(\"Executing command: {}\", toString());\n+    Preconditions.checkArgument(_table != null, \"'table' must be specified\");\n+    Preconditions.checkArgument(_format != null, \"'format' must be specified\");\n+    Preconditions.checkArgument(_dataFilePath != null, \"'dataFilePath' must be specified\");\n+\n+    try {\n+\n+      URI dataFileURI = URI.create(_dataFilePath);\n+      if ((dataFileURI.getScheme() == null)) {\n+        File dataFile = new File(_dataFilePath);\n+        Preconditions.checkArgument(dataFile.exists(), \"'dataFile': '%s' doesn't exist\", dataFile);\n+        LOGGER.info(\"Found data files: {} of format: {}\", dataFile, _format);\n+      }\n+\n+      initTempDir();\n+      IngestionJobLauncher.runIngestionJob(generateSegmentGenerationJobSpec());\n+      LOGGER.info(\"Successfully load data from {} to Pinot.\", _dataFilePath);\n+      return true;\n+    } catch (Exception e) {\n+      throw e;\n+    } finally {\n+      FileUtils.deleteQuietly(new File(_tempDir));\n+    }\n+  }\n+\n+  private void initTempDir()\n+      throws IOException {\n+    File tempDir = new File(_tempDir);\n+    if (tempDir.exists()) {\n+      LOGGER.info(\"Deleting the existing 'tempDir': {}\", tempDir);\n+      FileUtils.forceDelete(tempDir);\n+    }\n+    FileUtils.forceMkdir(tempDir);\n+  }\n+\n+  private SegmentGenerationJobSpec generateSegmentGenerationJobSpec() {\n+    final Map<String, String> extraConfigs = getExtraConfigs(_extraConfigs);\n+\n+    SegmentGenerationJobSpec spec = new SegmentGenerationJobSpec();\n+    URI dataFileURI = URI.create(_dataFilePath);\n+    URI parent = dataFileURI.getPath().endsWith(\"/\") ? dataFileURI.resolve(\"..\") : dataFileURI.resolve(\".\");\n+    spec.setInputDirURI(parent.toString());\n+    spec.setIncludeFileNamePattern(\"glob:**\" + dataFileURI.getPath());\n+    spec.setOutputDirURI(_tempDir);\n+    spec.setCleanUpOutputDir(true);\n+    spec.setOverwriteOutput(true);\n+    spec.setJobType(\"SegmentCreationAndTarPush\");\n+\n+    // set ExecutionFrameworkSpec\n+    ExecutionFrameworkSpec executionFrameworkSpec = new ExecutionFrameworkSpec();\n+    executionFrameworkSpec.setName(\"standalone\");\n+    executionFrameworkSpec.setSegmentGenerationJobRunnerClassName(\n+        \"org.apache.pinot.plugin.ingestion.batch.standalone.SegmentGenerationJobRunner\");\n+    executionFrameworkSpec.setSegmentTarPushJobRunnerClassName(\n+        \"org.apache.pinot.plugin.ingestion.batch.standalone.SegmentTarPushJobRunner\");\n+    spec.setExecutionFrameworkSpec(executionFrameworkSpec);\n+\n+    // set PinotFSSpecs\n+    List<PinotFSSpec> pinotFSSpecs = new ArrayList<>();\n+    pinotFSSpecs.add(getPinotFSSpec(\"file\", \"org.apache.pinot.spi.filesystem.LocalPinotFS\", Collections.emptyMap()));\n+    pinotFSSpecs\n+        .add(getPinotFSSpec(\"s3\", \"org.apache.pinot.plugin.filesystem.S3PinotFS\", getS3PinotFSConfigs(extraConfigs)));\n+    spec.setPinotFSSpecs(pinotFSSpecs);\n+\n+    // set RecordReaderSpec\n+    RecordReaderSpec recordReaderSpec = new RecordReaderSpec();\n+    recordReaderSpec.setDataFormat(_format.name());\n+    recordReaderSpec.setClassName(getRecordReaderClass(_format));\n+    recordReaderSpec.setConfigClassName(getRecordReaderConfigClass(_format));\n+    recordReaderSpec.setConfigs(IngestionConfigUtils.getRecordReaderProps(extraConfigs));\n+    spec.setRecordReaderSpec(recordReaderSpec);\n+\n+    // set TableSpec\n+    TableSpec tableSpec = new TableSpec();\n+    tableSpec.setTableName(_table);\n+    tableSpec.setSchemaURI(ControllerRequestURLBuilder.baseUrl(_controllerURI).forTableSchemaGet(_table));\n+    tableSpec.setTableConfigURI(ControllerRequestURLBuilder.baseUrl(_controllerURI).forTableGet(_table));\n+    spec.setTableSpec(tableSpec);\n+\n+    // set SegmentNameGeneratorSpec\n+    SegmentNameGeneratorSpec segmentNameGeneratorSpec = new SegmentNameGeneratorSpec();\n+    segmentNameGeneratorSpec\n+        .setType(org.apache.pinot.spi.ingestion.batch.BatchConfigProperties.SegmentNameGeneratorType.FIXED);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQxODYwOA=="}, "originalCommit": {"oid": "f924f2fa0a4e3c0460c6ae6d91b7b359357e57ba"}, "originalPosition": 249}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ3MDMzNTQwOnYy", "diffSide": "RIGHT", "path": "pinot-tools/src/main/java/org/apache/pinot/tools/admin/command/ImportDataCommand.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQxNjoyMTo0N1rOIN39lg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQyMTo1NTo1M1rOIPXPzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQxOTI4Ng==", "bodyText": "Hmm, should these be hardcoded?", "url": "https://github.com/apache/pinot/pull/6396#discussion_r551419286", "createdAt": "2021-01-04T16:21:47Z", "author": {"login": "mayankshriv"}, "path": "pinot-tools/src/main/java/org/apache/pinot/tools/admin/command/ImportDataCommand.java", "diffHunk": "@@ -0,0 +1,340 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.tools.admin.command;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableMap;\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.commons.codec.digest.DigestUtils;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.controller.helix.ControllerRequestURLBuilder;\n+import org.apache.pinot.spi.data.readers.FileFormat;\n+import org.apache.pinot.spi.ingestion.batch.BatchConfigProperties;\n+import org.apache.pinot.spi.ingestion.batch.IngestionJobLauncher;\n+import org.apache.pinot.spi.ingestion.batch.spec.ExecutionFrameworkSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotClusterSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotFSSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PushJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.RecordReaderSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentGenerationJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentNameGeneratorSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.TableSpec;\n+import org.apache.pinot.spi.utils.IngestionConfigUtils;\n+import org.apache.pinot.tools.Command;\n+import org.kohsuke.args4j.Option;\n+import org.kohsuke.args4j.spi.StringArrayOptionHandler;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Class to implement ImportData command.\n+ */\n+@SuppressWarnings(\"unused\")\n+public class ImportDataCommand extends AbstractBaseAdminCommand implements Command {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(ImportDataCommand.class);\n+  private static final String SEGMENT_NAME = \"segment.name\";\n+\n+  @Option(name = \"-dataFilePath\", required = true, metaVar = \"<string>\", usage = \"data file path.\")\n+  private String _dataFilePath;\n+\n+  @Option(name = \"-format\", required = true, metaVar = \"<AVRO/CSV/JSON/THRIFT/PARQUET/ORC>\", usage = \"Input data format.\")\n+  private FileFormat _format;\n+\n+  @Option(name = \"-table\", required = true, metaVar = \"<string>\", usage = \"Table name.\")\n+  private String _table;\n+\n+  @Option(name = \"-controllerURI\", metaVar = \"<string>\", usage = \"Pinot Controller URI.\")\n+  private String _controllerURI = \"http://localhost:9000\";\n+\n+  @Option(name = \"-tempDir\", metaVar = \"<string>\", usage = \"Temporary directory used to hold data during segment creation.\")\n+  private String _tempDir = new File(FileUtils.getTempDirectory(), getClass().getSimpleName()).getAbsolutePath();\n+\n+  @Option(name = \"-extraConfigs\", metaVar = \"<extra configs>\", handler = StringArrayOptionHandler.class, usage = \"Extra configs to be set.\")\n+  private List<String> _extraConfigs;\n+\n+  @SuppressWarnings(\"FieldCanBeLocal\")\n+  @Option(name = \"-help\", help = true, aliases = {\"-h\", \"--h\", \"--help\"}, usage = \"Print this message.\")\n+  private boolean _help = false;\n+\n+  public ImportDataCommand setDataFilePath(String dataFilePath) {\n+    _dataFilePath = dataFilePath;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setFormat(FileFormat format) {\n+    _format = format;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setTable(String table) {\n+    _table = table;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setControllerURI(String controllerURI) {\n+    _controllerURI = controllerURI;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setTempDir(String tempDir) {\n+    _tempDir = tempDir;\n+    return this;\n+  }\n+\n+  public List<String> getExtraConfigs() {\n+    return _extraConfigs;\n+  }\n+\n+  public ImportDataCommand setExtraConfigs(List<String> extraConfigs) {\n+    _extraConfigs = extraConfigs;\n+    return this;\n+  }\n+\n+  public String getDataFilePath() {\n+    return _dataFilePath;\n+  }\n+\n+  public FileFormat getFormat() {\n+    return _format;\n+  }\n+\n+  public String getTable() {\n+    return _table;\n+  }\n+\n+  public String getControllerURI() {\n+    return _controllerURI;\n+  }\n+\n+  public String getTempDir() {\n+    return _tempDir;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    String results = String\n+        .format(\"InsertData -dataFilePath %s -format %s -table %s -controllerURI %s -tempDir %s\", _dataFilePath,\n+            _format, _table, _controllerURI, _tempDir);\n+    if (_extraConfigs != null) {\n+      results += \" -extraConfigs \" + Arrays.toString(_extraConfigs.toArray());\n+    }\n+    return results;\n+  }\n+\n+  @Override\n+  public final String getName() {\n+    return \"InsertData\";\n+  }\n+\n+  @Override\n+  public String description() {\n+    return \"Insert data into Pinot cluster.\";\n+  }\n+\n+  @Override\n+  public boolean getHelp() {\n+    return _help;\n+  }\n+\n+  @Override\n+  public boolean execute()\n+      throws IOException {\n+    LOGGER.info(\"Executing command: {}\", toString());\n+    Preconditions.checkArgument(_table != null, \"'table' must be specified\");\n+    Preconditions.checkArgument(_format != null, \"'format' must be specified\");\n+    Preconditions.checkArgument(_dataFilePath != null, \"'dataFilePath' must be specified\");\n+\n+    try {\n+\n+      URI dataFileURI = URI.create(_dataFilePath);\n+      if ((dataFileURI.getScheme() == null)) {\n+        File dataFile = new File(_dataFilePath);\n+        Preconditions.checkArgument(dataFile.exists(), \"'dataFile': '%s' doesn't exist\", dataFile);\n+        LOGGER.info(\"Found data files: {} of format: {}\", dataFile, _format);\n+      }\n+\n+      initTempDir();\n+      IngestionJobLauncher.runIngestionJob(generateSegmentGenerationJobSpec());\n+      LOGGER.info(\"Successfully load data from {} to Pinot.\", _dataFilePath);\n+      return true;\n+    } catch (Exception e) {\n+      throw e;\n+    } finally {\n+      FileUtils.deleteQuietly(new File(_tempDir));\n+    }\n+  }\n+\n+  private void initTempDir()\n+      throws IOException {\n+    File tempDir = new File(_tempDir);\n+    if (tempDir.exists()) {\n+      LOGGER.info(\"Deleting the existing 'tempDir': {}\", tempDir);\n+      FileUtils.forceDelete(tempDir);\n+    }\n+    FileUtils.forceMkdir(tempDir);\n+  }\n+\n+  private SegmentGenerationJobSpec generateSegmentGenerationJobSpec() {\n+    final Map<String, String> extraConfigs = getExtraConfigs(_extraConfigs);\n+\n+    SegmentGenerationJobSpec spec = new SegmentGenerationJobSpec();\n+    URI dataFileURI = URI.create(_dataFilePath);\n+    URI parent = dataFileURI.getPath().endsWith(\"/\") ? dataFileURI.resolve(\"..\") : dataFileURI.resolve(\".\");\n+    spec.setInputDirURI(parent.toString());\n+    spec.setIncludeFileNamePattern(\"glob:**\" + dataFileURI.getPath());\n+    spec.setOutputDirURI(_tempDir);\n+    spec.setCleanUpOutputDir(true);\n+    spec.setOverwriteOutput(true);\n+    spec.setJobType(\"SegmentCreationAndTarPush\");\n+\n+    // set ExecutionFrameworkSpec\n+    ExecutionFrameworkSpec executionFrameworkSpec = new ExecutionFrameworkSpec();\n+    executionFrameworkSpec.setName(\"standalone\");\n+    executionFrameworkSpec.setSegmentGenerationJobRunnerClassName(\n+        \"org.apache.pinot.plugin.ingestion.batch.standalone.SegmentGenerationJobRunner\");\n+    executionFrameworkSpec.setSegmentTarPushJobRunnerClassName(\n+        \"org.apache.pinot.plugin.ingestion.batch.standalone.SegmentTarPushJobRunner\");\n+    spec.setExecutionFrameworkSpec(executionFrameworkSpec);\n+\n+    // set PinotFSSpecs\n+    List<PinotFSSpec> pinotFSSpecs = new ArrayList<>();\n+    pinotFSSpecs.add(getPinotFSSpec(\"file\", \"org.apache.pinot.spi.filesystem.LocalPinotFS\", Collections.emptyMap()));\n+    pinotFSSpecs\n+        .add(getPinotFSSpec(\"s3\", \"org.apache.pinot.plugin.filesystem.S3PinotFS\", getS3PinotFSConfigs(extraConfigs)));\n+    spec.setPinotFSSpecs(pinotFSSpecs);\n+\n+    // set RecordReaderSpec\n+    RecordReaderSpec recordReaderSpec = new RecordReaderSpec();\n+    recordReaderSpec.setDataFormat(_format.name());\n+    recordReaderSpec.setClassName(getRecordReaderClass(_format));\n+    recordReaderSpec.setConfigClassName(getRecordReaderConfigClass(_format));\n+    recordReaderSpec.setConfigs(IngestionConfigUtils.getRecordReaderProps(extraConfigs));\n+    spec.setRecordReaderSpec(recordReaderSpec);\n+\n+    // set TableSpec\n+    TableSpec tableSpec = new TableSpec();\n+    tableSpec.setTableName(_table);\n+    tableSpec.setSchemaURI(ControllerRequestURLBuilder.baseUrl(_controllerURI).forTableSchemaGet(_table));\n+    tableSpec.setTableConfigURI(ControllerRequestURLBuilder.baseUrl(_controllerURI).forTableGet(_table));\n+    spec.setTableSpec(tableSpec);\n+\n+    // set SegmentNameGeneratorSpec\n+    SegmentNameGeneratorSpec segmentNameGeneratorSpec = new SegmentNameGeneratorSpec();\n+    segmentNameGeneratorSpec\n+        .setType(org.apache.pinot.spi.ingestion.batch.BatchConfigProperties.SegmentNameGeneratorType.FIXED);\n+    String segmentName = (extraConfigs.containsKey(SEGMENT_NAME)) ? extraConfigs.get(SEGMENT_NAME)\n+        : String.format(\"%s_%s\", _table, DigestUtils.sha256Hex(_dataFilePath));\n+    segmentNameGeneratorSpec.setConfigs(ImmutableMap.of(SEGMENT_NAME, segmentName));\n+    spec.setSegmentNameGeneratorSpec(segmentNameGeneratorSpec);\n+\n+    // set PinotClusterSpecs\n+    PinotClusterSpec pinotClusterSpec = new PinotClusterSpec();\n+    pinotClusterSpec.setControllerURI(_controllerURI);\n+    PinotClusterSpec[] pinotClusterSpecs = new PinotClusterSpec[]{pinotClusterSpec};\n+    spec.setPinotClusterSpecs(pinotClusterSpecs);\n+\n+    // set PushJobSpec\n+    PushJobSpec pushJobSpec = new PushJobSpec();\n+    pushJobSpec.setPushAttempts(3);\n+    pushJobSpec.setPushRetryIntervalMillis(10000);\n+    spec.setPushJobSpec(pushJobSpec);\n+\n+    return spec;\n+  }\n+\n+  private Map<String, String> getS3PinotFSConfigs(Map<String, String> extraConfigs) {\n+    Map<String, String> s3PinotFSConfigs = new HashMap<>();\n+    s3PinotFSConfigs.put(\"region\", System.getProperty(\"AWS_REGION\", \"us-west-2\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f924f2fa0a4e3c0460c6ae6d91b7b359357e57ba"}, "originalPosition": 272}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjk4MDQyOA==", "bodyText": "this is just a default value in case not set at all.", "url": "https://github.com/apache/pinot/pull/6396#discussion_r552980428", "createdAt": "2021-01-06T21:55:53Z", "author": {"login": "xiangfu0"}, "path": "pinot-tools/src/main/java/org/apache/pinot/tools/admin/command/ImportDataCommand.java", "diffHunk": "@@ -0,0 +1,340 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.tools.admin.command;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableMap;\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.commons.codec.digest.DigestUtils;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.controller.helix.ControllerRequestURLBuilder;\n+import org.apache.pinot.spi.data.readers.FileFormat;\n+import org.apache.pinot.spi.ingestion.batch.BatchConfigProperties;\n+import org.apache.pinot.spi.ingestion.batch.IngestionJobLauncher;\n+import org.apache.pinot.spi.ingestion.batch.spec.ExecutionFrameworkSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotClusterSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotFSSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PushJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.RecordReaderSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentGenerationJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentNameGeneratorSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.TableSpec;\n+import org.apache.pinot.spi.utils.IngestionConfigUtils;\n+import org.apache.pinot.tools.Command;\n+import org.kohsuke.args4j.Option;\n+import org.kohsuke.args4j.spi.StringArrayOptionHandler;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Class to implement ImportData command.\n+ */\n+@SuppressWarnings(\"unused\")\n+public class ImportDataCommand extends AbstractBaseAdminCommand implements Command {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(ImportDataCommand.class);\n+  private static final String SEGMENT_NAME = \"segment.name\";\n+\n+  @Option(name = \"-dataFilePath\", required = true, metaVar = \"<string>\", usage = \"data file path.\")\n+  private String _dataFilePath;\n+\n+  @Option(name = \"-format\", required = true, metaVar = \"<AVRO/CSV/JSON/THRIFT/PARQUET/ORC>\", usage = \"Input data format.\")\n+  private FileFormat _format;\n+\n+  @Option(name = \"-table\", required = true, metaVar = \"<string>\", usage = \"Table name.\")\n+  private String _table;\n+\n+  @Option(name = \"-controllerURI\", metaVar = \"<string>\", usage = \"Pinot Controller URI.\")\n+  private String _controllerURI = \"http://localhost:9000\";\n+\n+  @Option(name = \"-tempDir\", metaVar = \"<string>\", usage = \"Temporary directory used to hold data during segment creation.\")\n+  private String _tempDir = new File(FileUtils.getTempDirectory(), getClass().getSimpleName()).getAbsolutePath();\n+\n+  @Option(name = \"-extraConfigs\", metaVar = \"<extra configs>\", handler = StringArrayOptionHandler.class, usage = \"Extra configs to be set.\")\n+  private List<String> _extraConfigs;\n+\n+  @SuppressWarnings(\"FieldCanBeLocal\")\n+  @Option(name = \"-help\", help = true, aliases = {\"-h\", \"--h\", \"--help\"}, usage = \"Print this message.\")\n+  private boolean _help = false;\n+\n+  public ImportDataCommand setDataFilePath(String dataFilePath) {\n+    _dataFilePath = dataFilePath;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setFormat(FileFormat format) {\n+    _format = format;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setTable(String table) {\n+    _table = table;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setControllerURI(String controllerURI) {\n+    _controllerURI = controllerURI;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setTempDir(String tempDir) {\n+    _tempDir = tempDir;\n+    return this;\n+  }\n+\n+  public List<String> getExtraConfigs() {\n+    return _extraConfigs;\n+  }\n+\n+  public ImportDataCommand setExtraConfigs(List<String> extraConfigs) {\n+    _extraConfigs = extraConfigs;\n+    return this;\n+  }\n+\n+  public String getDataFilePath() {\n+    return _dataFilePath;\n+  }\n+\n+  public FileFormat getFormat() {\n+    return _format;\n+  }\n+\n+  public String getTable() {\n+    return _table;\n+  }\n+\n+  public String getControllerURI() {\n+    return _controllerURI;\n+  }\n+\n+  public String getTempDir() {\n+    return _tempDir;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    String results = String\n+        .format(\"InsertData -dataFilePath %s -format %s -table %s -controllerURI %s -tempDir %s\", _dataFilePath,\n+            _format, _table, _controllerURI, _tempDir);\n+    if (_extraConfigs != null) {\n+      results += \" -extraConfigs \" + Arrays.toString(_extraConfigs.toArray());\n+    }\n+    return results;\n+  }\n+\n+  @Override\n+  public final String getName() {\n+    return \"InsertData\";\n+  }\n+\n+  @Override\n+  public String description() {\n+    return \"Insert data into Pinot cluster.\";\n+  }\n+\n+  @Override\n+  public boolean getHelp() {\n+    return _help;\n+  }\n+\n+  @Override\n+  public boolean execute()\n+      throws IOException {\n+    LOGGER.info(\"Executing command: {}\", toString());\n+    Preconditions.checkArgument(_table != null, \"'table' must be specified\");\n+    Preconditions.checkArgument(_format != null, \"'format' must be specified\");\n+    Preconditions.checkArgument(_dataFilePath != null, \"'dataFilePath' must be specified\");\n+\n+    try {\n+\n+      URI dataFileURI = URI.create(_dataFilePath);\n+      if ((dataFileURI.getScheme() == null)) {\n+        File dataFile = new File(_dataFilePath);\n+        Preconditions.checkArgument(dataFile.exists(), \"'dataFile': '%s' doesn't exist\", dataFile);\n+        LOGGER.info(\"Found data files: {} of format: {}\", dataFile, _format);\n+      }\n+\n+      initTempDir();\n+      IngestionJobLauncher.runIngestionJob(generateSegmentGenerationJobSpec());\n+      LOGGER.info(\"Successfully load data from {} to Pinot.\", _dataFilePath);\n+      return true;\n+    } catch (Exception e) {\n+      throw e;\n+    } finally {\n+      FileUtils.deleteQuietly(new File(_tempDir));\n+    }\n+  }\n+\n+  private void initTempDir()\n+      throws IOException {\n+    File tempDir = new File(_tempDir);\n+    if (tempDir.exists()) {\n+      LOGGER.info(\"Deleting the existing 'tempDir': {}\", tempDir);\n+      FileUtils.forceDelete(tempDir);\n+    }\n+    FileUtils.forceMkdir(tempDir);\n+  }\n+\n+  private SegmentGenerationJobSpec generateSegmentGenerationJobSpec() {\n+    final Map<String, String> extraConfigs = getExtraConfigs(_extraConfigs);\n+\n+    SegmentGenerationJobSpec spec = new SegmentGenerationJobSpec();\n+    URI dataFileURI = URI.create(_dataFilePath);\n+    URI parent = dataFileURI.getPath().endsWith(\"/\") ? dataFileURI.resolve(\"..\") : dataFileURI.resolve(\".\");\n+    spec.setInputDirURI(parent.toString());\n+    spec.setIncludeFileNamePattern(\"glob:**\" + dataFileURI.getPath());\n+    spec.setOutputDirURI(_tempDir);\n+    spec.setCleanUpOutputDir(true);\n+    spec.setOverwriteOutput(true);\n+    spec.setJobType(\"SegmentCreationAndTarPush\");\n+\n+    // set ExecutionFrameworkSpec\n+    ExecutionFrameworkSpec executionFrameworkSpec = new ExecutionFrameworkSpec();\n+    executionFrameworkSpec.setName(\"standalone\");\n+    executionFrameworkSpec.setSegmentGenerationJobRunnerClassName(\n+        \"org.apache.pinot.plugin.ingestion.batch.standalone.SegmentGenerationJobRunner\");\n+    executionFrameworkSpec.setSegmentTarPushJobRunnerClassName(\n+        \"org.apache.pinot.plugin.ingestion.batch.standalone.SegmentTarPushJobRunner\");\n+    spec.setExecutionFrameworkSpec(executionFrameworkSpec);\n+\n+    // set PinotFSSpecs\n+    List<PinotFSSpec> pinotFSSpecs = new ArrayList<>();\n+    pinotFSSpecs.add(getPinotFSSpec(\"file\", \"org.apache.pinot.spi.filesystem.LocalPinotFS\", Collections.emptyMap()));\n+    pinotFSSpecs\n+        .add(getPinotFSSpec(\"s3\", \"org.apache.pinot.plugin.filesystem.S3PinotFS\", getS3PinotFSConfigs(extraConfigs)));\n+    spec.setPinotFSSpecs(pinotFSSpecs);\n+\n+    // set RecordReaderSpec\n+    RecordReaderSpec recordReaderSpec = new RecordReaderSpec();\n+    recordReaderSpec.setDataFormat(_format.name());\n+    recordReaderSpec.setClassName(getRecordReaderClass(_format));\n+    recordReaderSpec.setConfigClassName(getRecordReaderConfigClass(_format));\n+    recordReaderSpec.setConfigs(IngestionConfigUtils.getRecordReaderProps(extraConfigs));\n+    spec.setRecordReaderSpec(recordReaderSpec);\n+\n+    // set TableSpec\n+    TableSpec tableSpec = new TableSpec();\n+    tableSpec.setTableName(_table);\n+    tableSpec.setSchemaURI(ControllerRequestURLBuilder.baseUrl(_controllerURI).forTableSchemaGet(_table));\n+    tableSpec.setTableConfigURI(ControllerRequestURLBuilder.baseUrl(_controllerURI).forTableGet(_table));\n+    spec.setTableSpec(tableSpec);\n+\n+    // set SegmentNameGeneratorSpec\n+    SegmentNameGeneratorSpec segmentNameGeneratorSpec = new SegmentNameGeneratorSpec();\n+    segmentNameGeneratorSpec\n+        .setType(org.apache.pinot.spi.ingestion.batch.BatchConfigProperties.SegmentNameGeneratorType.FIXED);\n+    String segmentName = (extraConfigs.containsKey(SEGMENT_NAME)) ? extraConfigs.get(SEGMENT_NAME)\n+        : String.format(\"%s_%s\", _table, DigestUtils.sha256Hex(_dataFilePath));\n+    segmentNameGeneratorSpec.setConfigs(ImmutableMap.of(SEGMENT_NAME, segmentName));\n+    spec.setSegmentNameGeneratorSpec(segmentNameGeneratorSpec);\n+\n+    // set PinotClusterSpecs\n+    PinotClusterSpec pinotClusterSpec = new PinotClusterSpec();\n+    pinotClusterSpec.setControllerURI(_controllerURI);\n+    PinotClusterSpec[] pinotClusterSpecs = new PinotClusterSpec[]{pinotClusterSpec};\n+    spec.setPinotClusterSpecs(pinotClusterSpecs);\n+\n+    // set PushJobSpec\n+    PushJobSpec pushJobSpec = new PushJobSpec();\n+    pushJobSpec.setPushAttempts(3);\n+    pushJobSpec.setPushRetryIntervalMillis(10000);\n+    spec.setPushJobSpec(pushJobSpec);\n+\n+    return spec;\n+  }\n+\n+  private Map<String, String> getS3PinotFSConfigs(Map<String, String> extraConfigs) {\n+    Map<String, String> s3PinotFSConfigs = new HashMap<>();\n+    s3PinotFSConfigs.put(\"region\", System.getProperty(\"AWS_REGION\", \"us-west-2\"));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQxOTI4Ng=="}, "originalCommit": {"oid": "f924f2fa0a4e3c0460c6ae6d91b7b359357e57ba"}, "originalPosition": 272}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ3MDMzOTIzOnYy", "diffSide": "RIGHT", "path": "pinot-tools/src/main/java/org/apache/pinot/tools/admin/command/ImportDataCommand.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQxNjoyMjo0NFrOIN3_1w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQwNDozNjowNFrOIVZuIg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQxOTg2Mw==", "bodyText": "Do we not have a RecordReaderFactory?", "url": "https://github.com/apache/pinot/pull/6396#discussion_r551419863", "createdAt": "2021-01-04T16:22:44Z", "author": {"login": "mayankshriv"}, "path": "pinot-tools/src/main/java/org/apache/pinot/tools/admin/command/ImportDataCommand.java", "diffHunk": "@@ -0,0 +1,340 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.tools.admin.command;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableMap;\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.commons.codec.digest.DigestUtils;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.controller.helix.ControllerRequestURLBuilder;\n+import org.apache.pinot.spi.data.readers.FileFormat;\n+import org.apache.pinot.spi.ingestion.batch.BatchConfigProperties;\n+import org.apache.pinot.spi.ingestion.batch.IngestionJobLauncher;\n+import org.apache.pinot.spi.ingestion.batch.spec.ExecutionFrameworkSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotClusterSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotFSSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PushJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.RecordReaderSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentGenerationJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentNameGeneratorSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.TableSpec;\n+import org.apache.pinot.spi.utils.IngestionConfigUtils;\n+import org.apache.pinot.tools.Command;\n+import org.kohsuke.args4j.Option;\n+import org.kohsuke.args4j.spi.StringArrayOptionHandler;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Class to implement ImportData command.\n+ */\n+@SuppressWarnings(\"unused\")\n+public class ImportDataCommand extends AbstractBaseAdminCommand implements Command {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(ImportDataCommand.class);\n+  private static final String SEGMENT_NAME = \"segment.name\";\n+\n+  @Option(name = \"-dataFilePath\", required = true, metaVar = \"<string>\", usage = \"data file path.\")\n+  private String _dataFilePath;\n+\n+  @Option(name = \"-format\", required = true, metaVar = \"<AVRO/CSV/JSON/THRIFT/PARQUET/ORC>\", usage = \"Input data format.\")\n+  private FileFormat _format;\n+\n+  @Option(name = \"-table\", required = true, metaVar = \"<string>\", usage = \"Table name.\")\n+  private String _table;\n+\n+  @Option(name = \"-controllerURI\", metaVar = \"<string>\", usage = \"Pinot Controller URI.\")\n+  private String _controllerURI = \"http://localhost:9000\";\n+\n+  @Option(name = \"-tempDir\", metaVar = \"<string>\", usage = \"Temporary directory used to hold data during segment creation.\")\n+  private String _tempDir = new File(FileUtils.getTempDirectory(), getClass().getSimpleName()).getAbsolutePath();\n+\n+  @Option(name = \"-extraConfigs\", metaVar = \"<extra configs>\", handler = StringArrayOptionHandler.class, usage = \"Extra configs to be set.\")\n+  private List<String> _extraConfigs;\n+\n+  @SuppressWarnings(\"FieldCanBeLocal\")\n+  @Option(name = \"-help\", help = true, aliases = {\"-h\", \"--h\", \"--help\"}, usage = \"Print this message.\")\n+  private boolean _help = false;\n+\n+  public ImportDataCommand setDataFilePath(String dataFilePath) {\n+    _dataFilePath = dataFilePath;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setFormat(FileFormat format) {\n+    _format = format;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setTable(String table) {\n+    _table = table;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setControllerURI(String controllerURI) {\n+    _controllerURI = controllerURI;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setTempDir(String tempDir) {\n+    _tempDir = tempDir;\n+    return this;\n+  }\n+\n+  public List<String> getExtraConfigs() {\n+    return _extraConfigs;\n+  }\n+\n+  public ImportDataCommand setExtraConfigs(List<String> extraConfigs) {\n+    _extraConfigs = extraConfigs;\n+    return this;\n+  }\n+\n+  public String getDataFilePath() {\n+    return _dataFilePath;\n+  }\n+\n+  public FileFormat getFormat() {\n+    return _format;\n+  }\n+\n+  public String getTable() {\n+    return _table;\n+  }\n+\n+  public String getControllerURI() {\n+    return _controllerURI;\n+  }\n+\n+  public String getTempDir() {\n+    return _tempDir;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    String results = String\n+        .format(\"InsertData -dataFilePath %s -format %s -table %s -controllerURI %s -tempDir %s\", _dataFilePath,\n+            _format, _table, _controllerURI, _tempDir);\n+    if (_extraConfigs != null) {\n+      results += \" -extraConfigs \" + Arrays.toString(_extraConfigs.toArray());\n+    }\n+    return results;\n+  }\n+\n+  @Override\n+  public final String getName() {\n+    return \"InsertData\";\n+  }\n+\n+  @Override\n+  public String description() {\n+    return \"Insert data into Pinot cluster.\";\n+  }\n+\n+  @Override\n+  public boolean getHelp() {\n+    return _help;\n+  }\n+\n+  @Override\n+  public boolean execute()\n+      throws IOException {\n+    LOGGER.info(\"Executing command: {}\", toString());\n+    Preconditions.checkArgument(_table != null, \"'table' must be specified\");\n+    Preconditions.checkArgument(_format != null, \"'format' must be specified\");\n+    Preconditions.checkArgument(_dataFilePath != null, \"'dataFilePath' must be specified\");\n+\n+    try {\n+\n+      URI dataFileURI = URI.create(_dataFilePath);\n+      if ((dataFileURI.getScheme() == null)) {\n+        File dataFile = new File(_dataFilePath);\n+        Preconditions.checkArgument(dataFile.exists(), \"'dataFile': '%s' doesn't exist\", dataFile);\n+        LOGGER.info(\"Found data files: {} of format: {}\", dataFile, _format);\n+      }\n+\n+      initTempDir();\n+      IngestionJobLauncher.runIngestionJob(generateSegmentGenerationJobSpec());\n+      LOGGER.info(\"Successfully load data from {} to Pinot.\", _dataFilePath);\n+      return true;\n+    } catch (Exception e) {\n+      throw e;\n+    } finally {\n+      FileUtils.deleteQuietly(new File(_tempDir));\n+    }\n+  }\n+\n+  private void initTempDir()\n+      throws IOException {\n+    File tempDir = new File(_tempDir);\n+    if (tempDir.exists()) {\n+      LOGGER.info(\"Deleting the existing 'tempDir': {}\", tempDir);\n+      FileUtils.forceDelete(tempDir);\n+    }\n+    FileUtils.forceMkdir(tempDir);\n+  }\n+\n+  private SegmentGenerationJobSpec generateSegmentGenerationJobSpec() {\n+    final Map<String, String> extraConfigs = getExtraConfigs(_extraConfigs);\n+\n+    SegmentGenerationJobSpec spec = new SegmentGenerationJobSpec();\n+    URI dataFileURI = URI.create(_dataFilePath);\n+    URI parent = dataFileURI.getPath().endsWith(\"/\") ? dataFileURI.resolve(\"..\") : dataFileURI.resolve(\".\");\n+    spec.setInputDirURI(parent.toString());\n+    spec.setIncludeFileNamePattern(\"glob:**\" + dataFileURI.getPath());\n+    spec.setOutputDirURI(_tempDir);\n+    spec.setCleanUpOutputDir(true);\n+    spec.setOverwriteOutput(true);\n+    spec.setJobType(\"SegmentCreationAndTarPush\");\n+\n+    // set ExecutionFrameworkSpec\n+    ExecutionFrameworkSpec executionFrameworkSpec = new ExecutionFrameworkSpec();\n+    executionFrameworkSpec.setName(\"standalone\");\n+    executionFrameworkSpec.setSegmentGenerationJobRunnerClassName(\n+        \"org.apache.pinot.plugin.ingestion.batch.standalone.SegmentGenerationJobRunner\");\n+    executionFrameworkSpec.setSegmentTarPushJobRunnerClassName(\n+        \"org.apache.pinot.plugin.ingestion.batch.standalone.SegmentTarPushJobRunner\");\n+    spec.setExecutionFrameworkSpec(executionFrameworkSpec);\n+\n+    // set PinotFSSpecs\n+    List<PinotFSSpec> pinotFSSpecs = new ArrayList<>();\n+    pinotFSSpecs.add(getPinotFSSpec(\"file\", \"org.apache.pinot.spi.filesystem.LocalPinotFS\", Collections.emptyMap()));\n+    pinotFSSpecs\n+        .add(getPinotFSSpec(\"s3\", \"org.apache.pinot.plugin.filesystem.S3PinotFS\", getS3PinotFSConfigs(extraConfigs)));\n+    spec.setPinotFSSpecs(pinotFSSpecs);\n+\n+    // set RecordReaderSpec\n+    RecordReaderSpec recordReaderSpec = new RecordReaderSpec();\n+    recordReaderSpec.setDataFormat(_format.name());\n+    recordReaderSpec.setClassName(getRecordReaderClass(_format));\n+    recordReaderSpec.setConfigClassName(getRecordReaderConfigClass(_format));\n+    recordReaderSpec.setConfigs(IngestionConfigUtils.getRecordReaderProps(extraConfigs));\n+    spec.setRecordReaderSpec(recordReaderSpec);\n+\n+    // set TableSpec\n+    TableSpec tableSpec = new TableSpec();\n+    tableSpec.setTableName(_table);\n+    tableSpec.setSchemaURI(ControllerRequestURLBuilder.baseUrl(_controllerURI).forTableSchemaGet(_table));\n+    tableSpec.setTableConfigURI(ControllerRequestURLBuilder.baseUrl(_controllerURI).forTableGet(_table));\n+    spec.setTableSpec(tableSpec);\n+\n+    // set SegmentNameGeneratorSpec\n+    SegmentNameGeneratorSpec segmentNameGeneratorSpec = new SegmentNameGeneratorSpec();\n+    segmentNameGeneratorSpec\n+        .setType(org.apache.pinot.spi.ingestion.batch.BatchConfigProperties.SegmentNameGeneratorType.FIXED);\n+    String segmentName = (extraConfigs.containsKey(SEGMENT_NAME)) ? extraConfigs.get(SEGMENT_NAME)\n+        : String.format(\"%s_%s\", _table, DigestUtils.sha256Hex(_dataFilePath));\n+    segmentNameGeneratorSpec.setConfigs(ImmutableMap.of(SEGMENT_NAME, segmentName));\n+    spec.setSegmentNameGeneratorSpec(segmentNameGeneratorSpec);\n+\n+    // set PinotClusterSpecs\n+    PinotClusterSpec pinotClusterSpec = new PinotClusterSpec();\n+    pinotClusterSpec.setControllerURI(_controllerURI);\n+    PinotClusterSpec[] pinotClusterSpecs = new PinotClusterSpec[]{pinotClusterSpec};\n+    spec.setPinotClusterSpecs(pinotClusterSpecs);\n+\n+    // set PushJobSpec\n+    PushJobSpec pushJobSpec = new PushJobSpec();\n+    pushJobSpec.setPushAttempts(3);\n+    pushJobSpec.setPushRetryIntervalMillis(10000);\n+    spec.setPushJobSpec(pushJobSpec);\n+\n+    return spec;\n+  }\n+\n+  private Map<String, String> getS3PinotFSConfigs(Map<String, String> extraConfigs) {\n+    Map<String, String> s3PinotFSConfigs = new HashMap<>();\n+    s3PinotFSConfigs.put(\"region\", System.getProperty(\"AWS_REGION\", \"us-west-2\"));\n+    s3PinotFSConfigs.putAll(IngestionConfigUtils.getConfigMapWithPrefix(extraConfigs,\n+        BatchConfigProperties.INPUT_FS_PROP_PREFIX + IngestionConfigUtils.DOT_SEPARATOR));\n+    return s3PinotFSConfigs;\n+  }\n+\n+  private PinotFSSpec getPinotFSSpec(String scheme, String className, Map<String, String> configs) {\n+    PinotFSSpec pinotFSSpec = new PinotFSSpec();\n+    pinotFSSpec.setScheme(scheme);\n+    pinotFSSpec.setClassName(className);\n+    pinotFSSpec.setConfigs(configs);\n+    return pinotFSSpec;\n+  }\n+\n+  private Map<String, String> getExtraConfigs(List<String> extraConfigs) {\n+    if (extraConfigs == null) {\n+      return Collections.emptyMap();\n+    }\n+    Map<String, String> recordReaderConfigs = new HashMap<>();\n+    for (String kvPair : extraConfigs) {\n+      String[] splits = kvPair.split(\"=\", 2);\n+      if ((splits.length == 2) && (splits[0] != null) && (splits[1] != null)) {\n+        recordReaderConfigs.put(splits[0], splits[1]);\n+      }\n+    }\n+    return recordReaderConfigs;\n+  }\n+\n+  private String getRecordReaderConfigClass(FileFormat format) {\n+    switch (format) {\n+      case CSV:\n+        return \"org.apache.pinot.plugin.inputformat.csv.CSVRecordReaderConfig\";\n+      case PROTO:\n+        return \"org.apache.pinot.plugin.inputformat.protobuf.ProtoBufRecordReaderConfig\";\n+      case THRIFT:\n+        return \"org.apache.pinot.plugin.inputformat.thrift.ThriftRecordReaderConfig\";\n+      case ORC:\n+      case JSON:\n+      case AVRO:\n+      case GZIPPED_AVRO:\n+      case PARQUET:\n+        return null;\n+      default:\n+        throw new IllegalArgumentException(\"Unsupported file format - \" + format);\n+    }\n+  }\n+\n+  private String getRecordReaderClass(FileFormat format) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f924f2fa0a4e3c0460c6ae6d91b7b359357e57ba"}, "originalPosition": 319}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjk3OTE4NQ==", "bodyText": "RecordReaderFactory is used to register class but doesn't have default mappings from fileformat to RecordReaderClass. Do you think we should move this default mapping method to the Factory class?", "url": "https://github.com/apache/pinot/pull/6396#discussion_r552979185", "createdAt": "2021-01-06T21:52:59Z", "author": {"login": "xiangfu0"}, "path": "pinot-tools/src/main/java/org/apache/pinot/tools/admin/command/ImportDataCommand.java", "diffHunk": "@@ -0,0 +1,340 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.tools.admin.command;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableMap;\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.commons.codec.digest.DigestUtils;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.controller.helix.ControllerRequestURLBuilder;\n+import org.apache.pinot.spi.data.readers.FileFormat;\n+import org.apache.pinot.spi.ingestion.batch.BatchConfigProperties;\n+import org.apache.pinot.spi.ingestion.batch.IngestionJobLauncher;\n+import org.apache.pinot.spi.ingestion.batch.spec.ExecutionFrameworkSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotClusterSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotFSSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PushJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.RecordReaderSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentGenerationJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentNameGeneratorSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.TableSpec;\n+import org.apache.pinot.spi.utils.IngestionConfigUtils;\n+import org.apache.pinot.tools.Command;\n+import org.kohsuke.args4j.Option;\n+import org.kohsuke.args4j.spi.StringArrayOptionHandler;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Class to implement ImportData command.\n+ */\n+@SuppressWarnings(\"unused\")\n+public class ImportDataCommand extends AbstractBaseAdminCommand implements Command {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(ImportDataCommand.class);\n+  private static final String SEGMENT_NAME = \"segment.name\";\n+\n+  @Option(name = \"-dataFilePath\", required = true, metaVar = \"<string>\", usage = \"data file path.\")\n+  private String _dataFilePath;\n+\n+  @Option(name = \"-format\", required = true, metaVar = \"<AVRO/CSV/JSON/THRIFT/PARQUET/ORC>\", usage = \"Input data format.\")\n+  private FileFormat _format;\n+\n+  @Option(name = \"-table\", required = true, metaVar = \"<string>\", usage = \"Table name.\")\n+  private String _table;\n+\n+  @Option(name = \"-controllerURI\", metaVar = \"<string>\", usage = \"Pinot Controller URI.\")\n+  private String _controllerURI = \"http://localhost:9000\";\n+\n+  @Option(name = \"-tempDir\", metaVar = \"<string>\", usage = \"Temporary directory used to hold data during segment creation.\")\n+  private String _tempDir = new File(FileUtils.getTempDirectory(), getClass().getSimpleName()).getAbsolutePath();\n+\n+  @Option(name = \"-extraConfigs\", metaVar = \"<extra configs>\", handler = StringArrayOptionHandler.class, usage = \"Extra configs to be set.\")\n+  private List<String> _extraConfigs;\n+\n+  @SuppressWarnings(\"FieldCanBeLocal\")\n+  @Option(name = \"-help\", help = true, aliases = {\"-h\", \"--h\", \"--help\"}, usage = \"Print this message.\")\n+  private boolean _help = false;\n+\n+  public ImportDataCommand setDataFilePath(String dataFilePath) {\n+    _dataFilePath = dataFilePath;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setFormat(FileFormat format) {\n+    _format = format;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setTable(String table) {\n+    _table = table;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setControllerURI(String controllerURI) {\n+    _controllerURI = controllerURI;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setTempDir(String tempDir) {\n+    _tempDir = tempDir;\n+    return this;\n+  }\n+\n+  public List<String> getExtraConfigs() {\n+    return _extraConfigs;\n+  }\n+\n+  public ImportDataCommand setExtraConfigs(List<String> extraConfigs) {\n+    _extraConfigs = extraConfigs;\n+    return this;\n+  }\n+\n+  public String getDataFilePath() {\n+    return _dataFilePath;\n+  }\n+\n+  public FileFormat getFormat() {\n+    return _format;\n+  }\n+\n+  public String getTable() {\n+    return _table;\n+  }\n+\n+  public String getControllerURI() {\n+    return _controllerURI;\n+  }\n+\n+  public String getTempDir() {\n+    return _tempDir;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    String results = String\n+        .format(\"InsertData -dataFilePath %s -format %s -table %s -controllerURI %s -tempDir %s\", _dataFilePath,\n+            _format, _table, _controllerURI, _tempDir);\n+    if (_extraConfigs != null) {\n+      results += \" -extraConfigs \" + Arrays.toString(_extraConfigs.toArray());\n+    }\n+    return results;\n+  }\n+\n+  @Override\n+  public final String getName() {\n+    return \"InsertData\";\n+  }\n+\n+  @Override\n+  public String description() {\n+    return \"Insert data into Pinot cluster.\";\n+  }\n+\n+  @Override\n+  public boolean getHelp() {\n+    return _help;\n+  }\n+\n+  @Override\n+  public boolean execute()\n+      throws IOException {\n+    LOGGER.info(\"Executing command: {}\", toString());\n+    Preconditions.checkArgument(_table != null, \"'table' must be specified\");\n+    Preconditions.checkArgument(_format != null, \"'format' must be specified\");\n+    Preconditions.checkArgument(_dataFilePath != null, \"'dataFilePath' must be specified\");\n+\n+    try {\n+\n+      URI dataFileURI = URI.create(_dataFilePath);\n+      if ((dataFileURI.getScheme() == null)) {\n+        File dataFile = new File(_dataFilePath);\n+        Preconditions.checkArgument(dataFile.exists(), \"'dataFile': '%s' doesn't exist\", dataFile);\n+        LOGGER.info(\"Found data files: {} of format: {}\", dataFile, _format);\n+      }\n+\n+      initTempDir();\n+      IngestionJobLauncher.runIngestionJob(generateSegmentGenerationJobSpec());\n+      LOGGER.info(\"Successfully load data from {} to Pinot.\", _dataFilePath);\n+      return true;\n+    } catch (Exception e) {\n+      throw e;\n+    } finally {\n+      FileUtils.deleteQuietly(new File(_tempDir));\n+    }\n+  }\n+\n+  private void initTempDir()\n+      throws IOException {\n+    File tempDir = new File(_tempDir);\n+    if (tempDir.exists()) {\n+      LOGGER.info(\"Deleting the existing 'tempDir': {}\", tempDir);\n+      FileUtils.forceDelete(tempDir);\n+    }\n+    FileUtils.forceMkdir(tempDir);\n+  }\n+\n+  private SegmentGenerationJobSpec generateSegmentGenerationJobSpec() {\n+    final Map<String, String> extraConfigs = getExtraConfigs(_extraConfigs);\n+\n+    SegmentGenerationJobSpec spec = new SegmentGenerationJobSpec();\n+    URI dataFileURI = URI.create(_dataFilePath);\n+    URI parent = dataFileURI.getPath().endsWith(\"/\") ? dataFileURI.resolve(\"..\") : dataFileURI.resolve(\".\");\n+    spec.setInputDirURI(parent.toString());\n+    spec.setIncludeFileNamePattern(\"glob:**\" + dataFileURI.getPath());\n+    spec.setOutputDirURI(_tempDir);\n+    spec.setCleanUpOutputDir(true);\n+    spec.setOverwriteOutput(true);\n+    spec.setJobType(\"SegmentCreationAndTarPush\");\n+\n+    // set ExecutionFrameworkSpec\n+    ExecutionFrameworkSpec executionFrameworkSpec = new ExecutionFrameworkSpec();\n+    executionFrameworkSpec.setName(\"standalone\");\n+    executionFrameworkSpec.setSegmentGenerationJobRunnerClassName(\n+        \"org.apache.pinot.plugin.ingestion.batch.standalone.SegmentGenerationJobRunner\");\n+    executionFrameworkSpec.setSegmentTarPushJobRunnerClassName(\n+        \"org.apache.pinot.plugin.ingestion.batch.standalone.SegmentTarPushJobRunner\");\n+    spec.setExecutionFrameworkSpec(executionFrameworkSpec);\n+\n+    // set PinotFSSpecs\n+    List<PinotFSSpec> pinotFSSpecs = new ArrayList<>();\n+    pinotFSSpecs.add(getPinotFSSpec(\"file\", \"org.apache.pinot.spi.filesystem.LocalPinotFS\", Collections.emptyMap()));\n+    pinotFSSpecs\n+        .add(getPinotFSSpec(\"s3\", \"org.apache.pinot.plugin.filesystem.S3PinotFS\", getS3PinotFSConfigs(extraConfigs)));\n+    spec.setPinotFSSpecs(pinotFSSpecs);\n+\n+    // set RecordReaderSpec\n+    RecordReaderSpec recordReaderSpec = new RecordReaderSpec();\n+    recordReaderSpec.setDataFormat(_format.name());\n+    recordReaderSpec.setClassName(getRecordReaderClass(_format));\n+    recordReaderSpec.setConfigClassName(getRecordReaderConfigClass(_format));\n+    recordReaderSpec.setConfigs(IngestionConfigUtils.getRecordReaderProps(extraConfigs));\n+    spec.setRecordReaderSpec(recordReaderSpec);\n+\n+    // set TableSpec\n+    TableSpec tableSpec = new TableSpec();\n+    tableSpec.setTableName(_table);\n+    tableSpec.setSchemaURI(ControllerRequestURLBuilder.baseUrl(_controllerURI).forTableSchemaGet(_table));\n+    tableSpec.setTableConfigURI(ControllerRequestURLBuilder.baseUrl(_controllerURI).forTableGet(_table));\n+    spec.setTableSpec(tableSpec);\n+\n+    // set SegmentNameGeneratorSpec\n+    SegmentNameGeneratorSpec segmentNameGeneratorSpec = new SegmentNameGeneratorSpec();\n+    segmentNameGeneratorSpec\n+        .setType(org.apache.pinot.spi.ingestion.batch.BatchConfigProperties.SegmentNameGeneratorType.FIXED);\n+    String segmentName = (extraConfigs.containsKey(SEGMENT_NAME)) ? extraConfigs.get(SEGMENT_NAME)\n+        : String.format(\"%s_%s\", _table, DigestUtils.sha256Hex(_dataFilePath));\n+    segmentNameGeneratorSpec.setConfigs(ImmutableMap.of(SEGMENT_NAME, segmentName));\n+    spec.setSegmentNameGeneratorSpec(segmentNameGeneratorSpec);\n+\n+    // set PinotClusterSpecs\n+    PinotClusterSpec pinotClusterSpec = new PinotClusterSpec();\n+    pinotClusterSpec.setControllerURI(_controllerURI);\n+    PinotClusterSpec[] pinotClusterSpecs = new PinotClusterSpec[]{pinotClusterSpec};\n+    spec.setPinotClusterSpecs(pinotClusterSpecs);\n+\n+    // set PushJobSpec\n+    PushJobSpec pushJobSpec = new PushJobSpec();\n+    pushJobSpec.setPushAttempts(3);\n+    pushJobSpec.setPushRetryIntervalMillis(10000);\n+    spec.setPushJobSpec(pushJobSpec);\n+\n+    return spec;\n+  }\n+\n+  private Map<String, String> getS3PinotFSConfigs(Map<String, String> extraConfigs) {\n+    Map<String, String> s3PinotFSConfigs = new HashMap<>();\n+    s3PinotFSConfigs.put(\"region\", System.getProperty(\"AWS_REGION\", \"us-west-2\"));\n+    s3PinotFSConfigs.putAll(IngestionConfigUtils.getConfigMapWithPrefix(extraConfigs,\n+        BatchConfigProperties.INPUT_FS_PROP_PREFIX + IngestionConfigUtils.DOT_SEPARATOR));\n+    return s3PinotFSConfigs;\n+  }\n+\n+  private PinotFSSpec getPinotFSSpec(String scheme, String className, Map<String, String> configs) {\n+    PinotFSSpec pinotFSSpec = new PinotFSSpec();\n+    pinotFSSpec.setScheme(scheme);\n+    pinotFSSpec.setClassName(className);\n+    pinotFSSpec.setConfigs(configs);\n+    return pinotFSSpec;\n+  }\n+\n+  private Map<String, String> getExtraConfigs(List<String> extraConfigs) {\n+    if (extraConfigs == null) {\n+      return Collections.emptyMap();\n+    }\n+    Map<String, String> recordReaderConfigs = new HashMap<>();\n+    for (String kvPair : extraConfigs) {\n+      String[] splits = kvPair.split(\"=\", 2);\n+      if ((splits.length == 2) && (splits[0] != null) && (splits[1] != null)) {\n+        recordReaderConfigs.put(splits[0], splits[1]);\n+      }\n+    }\n+    return recordReaderConfigs;\n+  }\n+\n+  private String getRecordReaderConfigClass(FileFormat format) {\n+    switch (format) {\n+      case CSV:\n+        return \"org.apache.pinot.plugin.inputformat.csv.CSVRecordReaderConfig\";\n+      case PROTO:\n+        return \"org.apache.pinot.plugin.inputformat.protobuf.ProtoBufRecordReaderConfig\";\n+      case THRIFT:\n+        return \"org.apache.pinot.plugin.inputformat.thrift.ThriftRecordReaderConfig\";\n+      case ORC:\n+      case JSON:\n+      case AVRO:\n+      case GZIPPED_AVRO:\n+      case PARQUET:\n+        return null;\n+      default:\n+        throw new IllegalArgumentException(\"Unsupported file format - \" + format);\n+    }\n+  }\n+\n+  private String getRecordReaderClass(FileFormat format) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQxOTg2Mw=="}, "originalCommit": {"oid": "f924f2fa0a4e3c0460c6ae6d91b7b359357e57ba"}, "originalPosition": 319}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTMxMjQxOA==", "bodyText": "Yeah, perhaps can be done outside of this PR.", "url": "https://github.com/apache/pinot/pull/6396#discussion_r559312418", "createdAt": "2021-01-18T04:36:04Z", "author": {"login": "mayankshriv"}, "path": "pinot-tools/src/main/java/org/apache/pinot/tools/admin/command/ImportDataCommand.java", "diffHunk": "@@ -0,0 +1,340 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.tools.admin.command;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableMap;\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.commons.codec.digest.DigestUtils;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.controller.helix.ControllerRequestURLBuilder;\n+import org.apache.pinot.spi.data.readers.FileFormat;\n+import org.apache.pinot.spi.ingestion.batch.BatchConfigProperties;\n+import org.apache.pinot.spi.ingestion.batch.IngestionJobLauncher;\n+import org.apache.pinot.spi.ingestion.batch.spec.ExecutionFrameworkSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotClusterSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotFSSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PushJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.RecordReaderSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentGenerationJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentNameGeneratorSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.TableSpec;\n+import org.apache.pinot.spi.utils.IngestionConfigUtils;\n+import org.apache.pinot.tools.Command;\n+import org.kohsuke.args4j.Option;\n+import org.kohsuke.args4j.spi.StringArrayOptionHandler;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Class to implement ImportData command.\n+ */\n+@SuppressWarnings(\"unused\")\n+public class ImportDataCommand extends AbstractBaseAdminCommand implements Command {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(ImportDataCommand.class);\n+  private static final String SEGMENT_NAME = \"segment.name\";\n+\n+  @Option(name = \"-dataFilePath\", required = true, metaVar = \"<string>\", usage = \"data file path.\")\n+  private String _dataFilePath;\n+\n+  @Option(name = \"-format\", required = true, metaVar = \"<AVRO/CSV/JSON/THRIFT/PARQUET/ORC>\", usage = \"Input data format.\")\n+  private FileFormat _format;\n+\n+  @Option(name = \"-table\", required = true, metaVar = \"<string>\", usage = \"Table name.\")\n+  private String _table;\n+\n+  @Option(name = \"-controllerURI\", metaVar = \"<string>\", usage = \"Pinot Controller URI.\")\n+  private String _controllerURI = \"http://localhost:9000\";\n+\n+  @Option(name = \"-tempDir\", metaVar = \"<string>\", usage = \"Temporary directory used to hold data during segment creation.\")\n+  private String _tempDir = new File(FileUtils.getTempDirectory(), getClass().getSimpleName()).getAbsolutePath();\n+\n+  @Option(name = \"-extraConfigs\", metaVar = \"<extra configs>\", handler = StringArrayOptionHandler.class, usage = \"Extra configs to be set.\")\n+  private List<String> _extraConfigs;\n+\n+  @SuppressWarnings(\"FieldCanBeLocal\")\n+  @Option(name = \"-help\", help = true, aliases = {\"-h\", \"--h\", \"--help\"}, usage = \"Print this message.\")\n+  private boolean _help = false;\n+\n+  public ImportDataCommand setDataFilePath(String dataFilePath) {\n+    _dataFilePath = dataFilePath;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setFormat(FileFormat format) {\n+    _format = format;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setTable(String table) {\n+    _table = table;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setControllerURI(String controllerURI) {\n+    _controllerURI = controllerURI;\n+    return this;\n+  }\n+\n+  public ImportDataCommand setTempDir(String tempDir) {\n+    _tempDir = tempDir;\n+    return this;\n+  }\n+\n+  public List<String> getExtraConfigs() {\n+    return _extraConfigs;\n+  }\n+\n+  public ImportDataCommand setExtraConfigs(List<String> extraConfigs) {\n+    _extraConfigs = extraConfigs;\n+    return this;\n+  }\n+\n+  public String getDataFilePath() {\n+    return _dataFilePath;\n+  }\n+\n+  public FileFormat getFormat() {\n+    return _format;\n+  }\n+\n+  public String getTable() {\n+    return _table;\n+  }\n+\n+  public String getControllerURI() {\n+    return _controllerURI;\n+  }\n+\n+  public String getTempDir() {\n+    return _tempDir;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    String results = String\n+        .format(\"InsertData -dataFilePath %s -format %s -table %s -controllerURI %s -tempDir %s\", _dataFilePath,\n+            _format, _table, _controllerURI, _tempDir);\n+    if (_extraConfigs != null) {\n+      results += \" -extraConfigs \" + Arrays.toString(_extraConfigs.toArray());\n+    }\n+    return results;\n+  }\n+\n+  @Override\n+  public final String getName() {\n+    return \"InsertData\";\n+  }\n+\n+  @Override\n+  public String description() {\n+    return \"Insert data into Pinot cluster.\";\n+  }\n+\n+  @Override\n+  public boolean getHelp() {\n+    return _help;\n+  }\n+\n+  @Override\n+  public boolean execute()\n+      throws IOException {\n+    LOGGER.info(\"Executing command: {}\", toString());\n+    Preconditions.checkArgument(_table != null, \"'table' must be specified\");\n+    Preconditions.checkArgument(_format != null, \"'format' must be specified\");\n+    Preconditions.checkArgument(_dataFilePath != null, \"'dataFilePath' must be specified\");\n+\n+    try {\n+\n+      URI dataFileURI = URI.create(_dataFilePath);\n+      if ((dataFileURI.getScheme() == null)) {\n+        File dataFile = new File(_dataFilePath);\n+        Preconditions.checkArgument(dataFile.exists(), \"'dataFile': '%s' doesn't exist\", dataFile);\n+        LOGGER.info(\"Found data files: {} of format: {}\", dataFile, _format);\n+      }\n+\n+      initTempDir();\n+      IngestionJobLauncher.runIngestionJob(generateSegmentGenerationJobSpec());\n+      LOGGER.info(\"Successfully load data from {} to Pinot.\", _dataFilePath);\n+      return true;\n+    } catch (Exception e) {\n+      throw e;\n+    } finally {\n+      FileUtils.deleteQuietly(new File(_tempDir));\n+    }\n+  }\n+\n+  private void initTempDir()\n+      throws IOException {\n+    File tempDir = new File(_tempDir);\n+    if (tempDir.exists()) {\n+      LOGGER.info(\"Deleting the existing 'tempDir': {}\", tempDir);\n+      FileUtils.forceDelete(tempDir);\n+    }\n+    FileUtils.forceMkdir(tempDir);\n+  }\n+\n+  private SegmentGenerationJobSpec generateSegmentGenerationJobSpec() {\n+    final Map<String, String> extraConfigs = getExtraConfigs(_extraConfigs);\n+\n+    SegmentGenerationJobSpec spec = new SegmentGenerationJobSpec();\n+    URI dataFileURI = URI.create(_dataFilePath);\n+    URI parent = dataFileURI.getPath().endsWith(\"/\") ? dataFileURI.resolve(\"..\") : dataFileURI.resolve(\".\");\n+    spec.setInputDirURI(parent.toString());\n+    spec.setIncludeFileNamePattern(\"glob:**\" + dataFileURI.getPath());\n+    spec.setOutputDirURI(_tempDir);\n+    spec.setCleanUpOutputDir(true);\n+    spec.setOverwriteOutput(true);\n+    spec.setJobType(\"SegmentCreationAndTarPush\");\n+\n+    // set ExecutionFrameworkSpec\n+    ExecutionFrameworkSpec executionFrameworkSpec = new ExecutionFrameworkSpec();\n+    executionFrameworkSpec.setName(\"standalone\");\n+    executionFrameworkSpec.setSegmentGenerationJobRunnerClassName(\n+        \"org.apache.pinot.plugin.ingestion.batch.standalone.SegmentGenerationJobRunner\");\n+    executionFrameworkSpec.setSegmentTarPushJobRunnerClassName(\n+        \"org.apache.pinot.plugin.ingestion.batch.standalone.SegmentTarPushJobRunner\");\n+    spec.setExecutionFrameworkSpec(executionFrameworkSpec);\n+\n+    // set PinotFSSpecs\n+    List<PinotFSSpec> pinotFSSpecs = new ArrayList<>();\n+    pinotFSSpecs.add(getPinotFSSpec(\"file\", \"org.apache.pinot.spi.filesystem.LocalPinotFS\", Collections.emptyMap()));\n+    pinotFSSpecs\n+        .add(getPinotFSSpec(\"s3\", \"org.apache.pinot.plugin.filesystem.S3PinotFS\", getS3PinotFSConfigs(extraConfigs)));\n+    spec.setPinotFSSpecs(pinotFSSpecs);\n+\n+    // set RecordReaderSpec\n+    RecordReaderSpec recordReaderSpec = new RecordReaderSpec();\n+    recordReaderSpec.setDataFormat(_format.name());\n+    recordReaderSpec.setClassName(getRecordReaderClass(_format));\n+    recordReaderSpec.setConfigClassName(getRecordReaderConfigClass(_format));\n+    recordReaderSpec.setConfigs(IngestionConfigUtils.getRecordReaderProps(extraConfigs));\n+    spec.setRecordReaderSpec(recordReaderSpec);\n+\n+    // set TableSpec\n+    TableSpec tableSpec = new TableSpec();\n+    tableSpec.setTableName(_table);\n+    tableSpec.setSchemaURI(ControllerRequestURLBuilder.baseUrl(_controllerURI).forTableSchemaGet(_table));\n+    tableSpec.setTableConfigURI(ControllerRequestURLBuilder.baseUrl(_controllerURI).forTableGet(_table));\n+    spec.setTableSpec(tableSpec);\n+\n+    // set SegmentNameGeneratorSpec\n+    SegmentNameGeneratorSpec segmentNameGeneratorSpec = new SegmentNameGeneratorSpec();\n+    segmentNameGeneratorSpec\n+        .setType(org.apache.pinot.spi.ingestion.batch.BatchConfigProperties.SegmentNameGeneratorType.FIXED);\n+    String segmentName = (extraConfigs.containsKey(SEGMENT_NAME)) ? extraConfigs.get(SEGMENT_NAME)\n+        : String.format(\"%s_%s\", _table, DigestUtils.sha256Hex(_dataFilePath));\n+    segmentNameGeneratorSpec.setConfigs(ImmutableMap.of(SEGMENT_NAME, segmentName));\n+    spec.setSegmentNameGeneratorSpec(segmentNameGeneratorSpec);\n+\n+    // set PinotClusterSpecs\n+    PinotClusterSpec pinotClusterSpec = new PinotClusterSpec();\n+    pinotClusterSpec.setControllerURI(_controllerURI);\n+    PinotClusterSpec[] pinotClusterSpecs = new PinotClusterSpec[]{pinotClusterSpec};\n+    spec.setPinotClusterSpecs(pinotClusterSpecs);\n+\n+    // set PushJobSpec\n+    PushJobSpec pushJobSpec = new PushJobSpec();\n+    pushJobSpec.setPushAttempts(3);\n+    pushJobSpec.setPushRetryIntervalMillis(10000);\n+    spec.setPushJobSpec(pushJobSpec);\n+\n+    return spec;\n+  }\n+\n+  private Map<String, String> getS3PinotFSConfigs(Map<String, String> extraConfigs) {\n+    Map<String, String> s3PinotFSConfigs = new HashMap<>();\n+    s3PinotFSConfigs.put(\"region\", System.getProperty(\"AWS_REGION\", \"us-west-2\"));\n+    s3PinotFSConfigs.putAll(IngestionConfigUtils.getConfigMapWithPrefix(extraConfigs,\n+        BatchConfigProperties.INPUT_FS_PROP_PREFIX + IngestionConfigUtils.DOT_SEPARATOR));\n+    return s3PinotFSConfigs;\n+  }\n+\n+  private PinotFSSpec getPinotFSSpec(String scheme, String className, Map<String, String> configs) {\n+    PinotFSSpec pinotFSSpec = new PinotFSSpec();\n+    pinotFSSpec.setScheme(scheme);\n+    pinotFSSpec.setClassName(className);\n+    pinotFSSpec.setConfigs(configs);\n+    return pinotFSSpec;\n+  }\n+\n+  private Map<String, String> getExtraConfigs(List<String> extraConfigs) {\n+    if (extraConfigs == null) {\n+      return Collections.emptyMap();\n+    }\n+    Map<String, String> recordReaderConfigs = new HashMap<>();\n+    for (String kvPair : extraConfigs) {\n+      String[] splits = kvPair.split(\"=\", 2);\n+      if ((splits.length == 2) && (splits[0] != null) && (splits[1] != null)) {\n+        recordReaderConfigs.put(splits[0], splits[1]);\n+      }\n+    }\n+    return recordReaderConfigs;\n+  }\n+\n+  private String getRecordReaderConfigClass(FileFormat format) {\n+    switch (format) {\n+      case CSV:\n+        return \"org.apache.pinot.plugin.inputformat.csv.CSVRecordReaderConfig\";\n+      case PROTO:\n+        return \"org.apache.pinot.plugin.inputformat.protobuf.ProtoBufRecordReaderConfig\";\n+      case THRIFT:\n+        return \"org.apache.pinot.plugin.inputformat.thrift.ThriftRecordReaderConfig\";\n+      case ORC:\n+      case JSON:\n+      case AVRO:\n+      case GZIPPED_AVRO:\n+      case PARQUET:\n+        return null;\n+      default:\n+        throw new IllegalArgumentException(\"Unsupported file format - \" + format);\n+    }\n+  }\n+\n+  private String getRecordReaderClass(FileFormat format) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQxOTg2Mw=="}, "originalCommit": {"oid": "f924f2fa0a4e3c0460c6ae6d91b7b359357e57ba"}, "originalPosition": 319}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ3MDM0Mjk5OnYy", "diffSide": "RIGHT", "path": "pinot-tools/src/main/java/org/apache/pinot/tools/admin/command/ImportDataCommand.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQxNjoyMzozN1rOIN4CGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQyMTo1MTowOVrOIPXIZQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQyMDQ0MQ==", "bodyText": "Does this also support dataDir that contains multiple data files to be imported?", "url": "https://github.com/apache/pinot/pull/6396#discussion_r551420441", "createdAt": "2021-01-04T16:23:37Z", "author": {"login": "mayankshriv"}, "path": "pinot-tools/src/main/java/org/apache/pinot/tools/admin/command/ImportDataCommand.java", "diffHunk": "@@ -0,0 +1,340 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.tools.admin.command;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableMap;\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.commons.codec.digest.DigestUtils;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.controller.helix.ControllerRequestURLBuilder;\n+import org.apache.pinot.spi.data.readers.FileFormat;\n+import org.apache.pinot.spi.ingestion.batch.BatchConfigProperties;\n+import org.apache.pinot.spi.ingestion.batch.IngestionJobLauncher;\n+import org.apache.pinot.spi.ingestion.batch.spec.ExecutionFrameworkSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotClusterSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotFSSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PushJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.RecordReaderSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentGenerationJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentNameGeneratorSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.TableSpec;\n+import org.apache.pinot.spi.utils.IngestionConfigUtils;\n+import org.apache.pinot.tools.Command;\n+import org.kohsuke.args4j.Option;\n+import org.kohsuke.args4j.spi.StringArrayOptionHandler;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Class to implement ImportData command.\n+ */\n+@SuppressWarnings(\"unused\")\n+public class ImportDataCommand extends AbstractBaseAdminCommand implements Command {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(ImportDataCommand.class);\n+  private static final String SEGMENT_NAME = \"segment.name\";\n+\n+  @Option(name = \"-dataFilePath\", required = true, metaVar = \"<string>\", usage = \"data file path.\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f924f2fa0a4e3c0460c6ae6d91b7b359357e57ba"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjk3ODUzMw==", "bodyText": "Not for now, it can be as simple as writing a bash to loop all the files then call this cmd.", "url": "https://github.com/apache/pinot/pull/6396#discussion_r552978533", "createdAt": "2021-01-06T21:51:09Z", "author": {"login": "xiangfu0"}, "path": "pinot-tools/src/main/java/org/apache/pinot/tools/admin/command/ImportDataCommand.java", "diffHunk": "@@ -0,0 +1,340 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.tools.admin.command;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableMap;\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.commons.codec.digest.DigestUtils;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.controller.helix.ControllerRequestURLBuilder;\n+import org.apache.pinot.spi.data.readers.FileFormat;\n+import org.apache.pinot.spi.ingestion.batch.BatchConfigProperties;\n+import org.apache.pinot.spi.ingestion.batch.IngestionJobLauncher;\n+import org.apache.pinot.spi.ingestion.batch.spec.ExecutionFrameworkSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotClusterSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotFSSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PushJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.RecordReaderSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentGenerationJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentNameGeneratorSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.TableSpec;\n+import org.apache.pinot.spi.utils.IngestionConfigUtils;\n+import org.apache.pinot.tools.Command;\n+import org.kohsuke.args4j.Option;\n+import org.kohsuke.args4j.spi.StringArrayOptionHandler;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Class to implement ImportData command.\n+ */\n+@SuppressWarnings(\"unused\")\n+public class ImportDataCommand extends AbstractBaseAdminCommand implements Command {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(ImportDataCommand.class);\n+  private static final String SEGMENT_NAME = \"segment.name\";\n+\n+  @Option(name = \"-dataFilePath\", required = true, metaVar = \"<string>\", usage = \"data file path.\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQyMDQ0MQ=="}, "originalCommit": {"oid": "f924f2fa0a4e3c0460c6ae6d91b7b359357e57ba"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUyMTYyMjg2OnYy", "diffSide": "RIGHT", "path": "pinot-tools/src/main/java/org/apache/pinot/tools/admin/command/ImportDataCommand.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQwNDozNjo0MVrOIVZulQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQwNDo0ODozNFrOIVZ47g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTMxMjUzMw==", "bodyText": "-additionalConfigs?", "url": "https://github.com/apache/pinot/pull/6396#discussion_r559312533", "createdAt": "2021-01-18T04:36:41Z", "author": {"login": "mayankshriv"}, "path": "pinot-tools/src/main/java/org/apache/pinot/tools/admin/command/ImportDataCommand.java", "diffHunk": "@@ -0,0 +1,390 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.tools.admin.command;\n+\n+import com.google.common.base.Preconditions;\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.commons.codec.digest.DigestUtils;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.controller.helix.ControllerRequestURLBuilder;\n+import org.apache.pinot.spi.data.readers.FileFormat;\n+import org.apache.pinot.spi.filesystem.PinotFSFactory;\n+import org.apache.pinot.spi.ingestion.batch.BatchConfigProperties;\n+import org.apache.pinot.spi.ingestion.batch.IngestionJobLauncher;\n+import org.apache.pinot.spi.ingestion.batch.spec.ExecutionFrameworkSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotClusterSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotFSSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PushJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.RecordReaderSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentGenerationJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentNameGeneratorSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.TableSpec;\n+import org.apache.pinot.spi.utils.IngestionConfigUtils;\n+import org.apache.pinot.tools.Command;\n+import org.kohsuke.args4j.Option;\n+import org.kohsuke.args4j.spi.StringArrayOptionHandler;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Class to implement ImportData command.\n+ */\n+@SuppressWarnings(\"unused\")\n+public class ImportDataCommand extends AbstractBaseAdminCommand implements Command {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(ImportDataCommand.class);\n+  private static final String SEGMENT_NAME = \"segment.name\";\n+\n+  @Option(name = \"-dataFilePath\", required = true, metaVar = \"<string>\", usage = \"data file path.\")\n+  private String _dataFilePath;\n+\n+  @Option(name = \"-format\", required = true, metaVar = \"<AVRO/CSV/JSON/THRIFT/PARQUET/ORC>\", usage = \"Input data format.\")\n+  private FileFormat _format;\n+\n+  @Option(name = \"-segmentNameGeneratorType\", metaVar = \"<AVRO/CSV/JSON/THRIFT/PARQUET/ORC>\", usage = \"Segment name generator type, default to FIXED type.\")\n+  private String _segmentNameGeneratorType = BatchConfigProperties.SegmentNameGeneratorType.FIXED;\n+\n+  @Option(name = \"-table\", required = true, metaVar = \"<string>\", usage = \"Table name.\")\n+  private String _table;\n+\n+  @Option(name = \"-controllerURI\", metaVar = \"<string>\", usage = \"Pinot Controller URI.\")\n+  private String _controllerURI = \"http://localhost:9000\";\n+\n+  @Option(name = \"-tempDir\", metaVar = \"<string>\", usage = \"Temporary directory used to hold data during segment creation.\")\n+  private String _tempDir = new File(FileUtils.getTempDirectory(), getClass().getSimpleName()).getAbsolutePath();\n+\n+  @Option(name = \"-extraConfigs\", metaVar = \"<extra configs>\", handler = StringArrayOptionHandler.class, usage = \"Extra configs to be set.\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f966c1ecc4086b3399d191ea882c4ac7ba89d338"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTMxNTE4Mg==", "bodyText": "done", "url": "https://github.com/apache/pinot/pull/6396#discussion_r559315182", "createdAt": "2021-01-18T04:48:34Z", "author": {"login": "xiangfu0"}, "path": "pinot-tools/src/main/java/org/apache/pinot/tools/admin/command/ImportDataCommand.java", "diffHunk": "@@ -0,0 +1,390 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.tools.admin.command;\n+\n+import com.google.common.base.Preconditions;\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.commons.codec.digest.DigestUtils;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.controller.helix.ControllerRequestURLBuilder;\n+import org.apache.pinot.spi.data.readers.FileFormat;\n+import org.apache.pinot.spi.filesystem.PinotFSFactory;\n+import org.apache.pinot.spi.ingestion.batch.BatchConfigProperties;\n+import org.apache.pinot.spi.ingestion.batch.IngestionJobLauncher;\n+import org.apache.pinot.spi.ingestion.batch.spec.ExecutionFrameworkSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotClusterSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotFSSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PushJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.RecordReaderSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentGenerationJobSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentNameGeneratorSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.TableSpec;\n+import org.apache.pinot.spi.utils.IngestionConfigUtils;\n+import org.apache.pinot.tools.Command;\n+import org.kohsuke.args4j.Option;\n+import org.kohsuke.args4j.spi.StringArrayOptionHandler;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Class to implement ImportData command.\n+ */\n+@SuppressWarnings(\"unused\")\n+public class ImportDataCommand extends AbstractBaseAdminCommand implements Command {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(ImportDataCommand.class);\n+  private static final String SEGMENT_NAME = \"segment.name\";\n+\n+  @Option(name = \"-dataFilePath\", required = true, metaVar = \"<string>\", usage = \"data file path.\")\n+  private String _dataFilePath;\n+\n+  @Option(name = \"-format\", required = true, metaVar = \"<AVRO/CSV/JSON/THRIFT/PARQUET/ORC>\", usage = \"Input data format.\")\n+  private FileFormat _format;\n+\n+  @Option(name = \"-segmentNameGeneratorType\", metaVar = \"<AVRO/CSV/JSON/THRIFT/PARQUET/ORC>\", usage = \"Segment name generator type, default to FIXED type.\")\n+  private String _segmentNameGeneratorType = BatchConfigProperties.SegmentNameGeneratorType.FIXED;\n+\n+  @Option(name = \"-table\", required = true, metaVar = \"<string>\", usage = \"Table name.\")\n+  private String _table;\n+\n+  @Option(name = \"-controllerURI\", metaVar = \"<string>\", usage = \"Pinot Controller URI.\")\n+  private String _controllerURI = \"http://localhost:9000\";\n+\n+  @Option(name = \"-tempDir\", metaVar = \"<string>\", usage = \"Temporary directory used to hold data during segment creation.\")\n+  private String _tempDir = new File(FileUtils.getTempDirectory(), getClass().getSimpleName()).getAbsolutePath();\n+\n+  @Option(name = \"-extraConfigs\", metaVar = \"<extra configs>\", handler = StringArrayOptionHandler.class, usage = \"Extra configs to be set.\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTMxMjUzMw=="}, "originalCommit": {"oid": "f966c1ecc4086b3399d191ea882c4ac7ba89d338"}, "originalPosition": 80}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3066, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}