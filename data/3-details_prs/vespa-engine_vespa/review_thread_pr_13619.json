{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM1OTI2MTc2", "number": 13619, "reviewThreads": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxNjoyOTo0MFrOEGbO3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxODo1NToxMVrOEGefKg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1MTczMDg3OnYy", "diffSide": "RIGHT", "path": "node-repository/src/main/java/com/yahoo/vespa/hosted/provision/NodeList.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxNjoyOTo0MFrOGlNqXg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QyMToyMzozNVrOGlX7AQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTY3NDMzNA==", "bodyText": "Why not just use .state(Node.State.active)? Same for the .hosts() method - .nodeType(NodeType.hosts)", "url": "https://github.com/vespa-engine/vespa/pull/13619#discussion_r441674334", "createdAt": "2020-06-17T16:29:40Z", "author": {"login": "freva"}, "path": "node-repository/src/main/java/com/yahoo/vespa/hosted/provision/NodeList.java", "diffHunk": "@@ -133,6 +146,16 @@ public NodeList state(Collection<Node.State> nodeStates) {\n         return matching(node -> nodeStates.contains(node.state()));\n     }\n \n+    /** Returns the subset of nodes in the active state */\n+    public NodeList active() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "28cf15bac333eadb4f3d133c9102c32bd0d72770"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTgyOTE4NA==", "bodyText": "Imho the only purpose of this class is to make node filtering statements shorter, so why not go all in.", "url": "https://github.com/vespa-engine/vespa/pull/13619#discussion_r441829184", "createdAt": "2020-06-17T20:56:06Z", "author": {"login": "bratseth"}, "path": "node-repository/src/main/java/com/yahoo/vespa/hosted/provision/NodeList.java", "diffHunk": "@@ -133,6 +146,16 @@ public NodeList state(Collection<Node.State> nodeStates) {\n         return matching(node -> nodeStates.contains(node.state()));\n     }\n \n+    /** Returns the subset of nodes in the active state */\n+    public NodeList active() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTY3NDMzNA=="}, "originalCommit": {"oid": "28cf15bac333eadb4f3d133c9102c32bd0d72770"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTgzODY1Ng==", "bodyText": "Aside from polluting the interface, it also has disadvantage with code search. I often search for usage of those enum values to see which cases I might need to consider. If we end up having a method for every/most states, that just slows that process down where you would also need to search for usage of those methods.", "url": "https://github.com/vespa-engine/vespa/pull/13619#discussion_r441838656", "createdAt": "2020-06-17T21:15:11Z", "author": {"login": "freva"}, "path": "node-repository/src/main/java/com/yahoo/vespa/hosted/provision/NodeList.java", "diffHunk": "@@ -133,6 +146,16 @@ public NodeList state(Collection<Node.State> nodeStates) {\n         return matching(node -> nodeStates.contains(node.state()));\n     }\n \n+    /** Returns the subset of nodes in the active state */\n+    public NodeList active() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTY3NDMzNA=="}, "originalCommit": {"oid": "28cf15bac333eadb4f3d133c9102c32bd0d72770"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTg0MjQzMw==", "bodyText": "Don't agree with those and the latter is really an argument against all library code, but as a compromise I removed active() since it was used once, but not hosts() since it was used twice :-)", "url": "https://github.com/vespa-engine/vespa/pull/13619#discussion_r441842433", "createdAt": "2020-06-17T21:23:35Z", "author": {"login": "bratseth"}, "path": "node-repository/src/main/java/com/yahoo/vespa/hosted/provision/NodeList.java", "diffHunk": "@@ -133,6 +146,16 @@ public NodeList state(Collection<Node.State> nodeStates) {\n         return matching(node -> nodeStates.contains(node.state()));\n     }\n \n+    /** Returns the subset of nodes in the active state */\n+    public NodeList active() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTY3NDMzNA=="}, "originalCommit": {"oid": "28cf15bac333eadb4f3d133c9102c32bd0d72770"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1MTg2NDIwOnYy", "diffSide": "RIGHT", "path": "node-repository/src/main/java/com/yahoo/vespa/hosted/provision/maintenance/MaintenanceDeployment.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxNzowNjowMVrOGlPB7A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QyMDo1NjoyMVrOGlXH2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTY5Njc0OA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                               .ifPresent(node -> nodeRepository.setDirty(node, Agent.Rebalancer, \"Expired by \" + agent));\n          \n          \n            \n                                               .ifPresent(node -> nodeRepository.setDirty(node, agent, \"Expired by \" + agent));", "url": "https://github.com/vespa-engine/vespa/pull/13619#discussion_r441696748", "createdAt": "2020-06-17T17:06:01Z", "author": {"login": "freva"}, "path": "node-repository/src/main/java/com/yahoo/vespa/hosted/provision/maintenance/MaintenanceDeployment.java", "diffHunk": "@@ -128,4 +131,106 @@ public String toString() {\n         return \"deployment of \" + application;\n     }\n \n+    public static class Move {\n+\n+        private final Node node;\n+        private final Node fromHost, toHost;\n+\n+        Move(Node node, Node fromHost, Node toHost) {\n+            this.node = node;\n+            this.fromHost = fromHost;\n+            this.toHost = toHost;\n+        }\n+\n+        public Node node() { return node; }\n+        public Node fromHost() { return fromHost; }\n+        public Node toHost() { return toHost; }\n+\n+        /**\n+         * Try to deploy to make this move.\n+         *\n+         * @param verifyTarget true to only make this move if the node ends up at the expected target host,\n+         *                     false if we should perform it as long as it moves from the source host\n+         * @return true if the move was done, false if it couldn't be\n+         */\n+        public boolean execute(boolean verifyTarget,\n+                               Agent agent, Deployer deployer, Metric metric, NodeRepository nodeRepository) {\n+            if (isEmpty()) return false;\n+            ApplicationId application = node.allocation().get().owner();\n+            try (MaintenanceDeployment deployment = new MaintenanceDeployment(application, deployer, metric, nodeRepository)) {\n+                if ( ! deployment.isValid()) return false;\n+\n+                boolean couldMarkRetiredNow = markWantToRetire(node, true, agent, nodeRepository);\n+                if ( ! couldMarkRetiredNow) return false;\n+\n+                Optional<Node> expectedNewNode = Optional.empty();\n+                try {\n+                    if ( ! deployment.prepare()) return false;\n+                    if (verifyTarget) {\n+                        expectedNewNode =\n+                                nodeRepository.getNodes(application, Node.State.reserved).stream()\n+                                              .filter(n -> !n.hostname().equals(node.hostname()))\n+                                              .filter(n -> n.allocation().get().membership().cluster().id().equals(node.allocation().get().membership().cluster().id()))\n+                                              .findAny();\n+                        if (expectedNewNode.isEmpty()) return false;\n+                        if (!expectedNewNode.get().hasParent(toHost.hostname())) return false;\n+                    }\n+                    if ( ! deployment.activate()) return false;\n+\n+                    log.info(agent + \" redeployed \" + application + \" to \" +\n+                             ( verifyTarget ? this : \"move \" + (node.hostname() + \" from \" + fromHost)));\n+                    return true;\n+                }\n+                finally {\n+                    markWantToRetire(node, false, agent, nodeRepository); // Necessary if this failed, no-op otherwise\n+\n+                    // Immediately clean up if we reserved the node but could not activate or reserved a node on the wrong host\n+                    expectedNewNode.flatMap(node -> nodeRepository.getNode(node.hostname(), Node.State.reserved))\n+                                   .ifPresent(node -> nodeRepository.setDirty(node, Agent.Rebalancer, \"Expired by \" + agent));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "28cf15bac333eadb4f3d133c9102c32bd0d72770"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTgyOTMzOA==", "bodyText": "Thanks!", "url": "https://github.com/vespa-engine/vespa/pull/13619#discussion_r441829338", "createdAt": "2020-06-17T20:56:21Z", "author": {"login": "bratseth"}, "path": "node-repository/src/main/java/com/yahoo/vespa/hosted/provision/maintenance/MaintenanceDeployment.java", "diffHunk": "@@ -128,4 +131,106 @@ public String toString() {\n         return \"deployment of \" + application;\n     }\n \n+    public static class Move {\n+\n+        private final Node node;\n+        private final Node fromHost, toHost;\n+\n+        Move(Node node, Node fromHost, Node toHost) {\n+            this.node = node;\n+            this.fromHost = fromHost;\n+            this.toHost = toHost;\n+        }\n+\n+        public Node node() { return node; }\n+        public Node fromHost() { return fromHost; }\n+        public Node toHost() { return toHost; }\n+\n+        /**\n+         * Try to deploy to make this move.\n+         *\n+         * @param verifyTarget true to only make this move if the node ends up at the expected target host,\n+         *                     false if we should perform it as long as it moves from the source host\n+         * @return true if the move was done, false if it couldn't be\n+         */\n+        public boolean execute(boolean verifyTarget,\n+                               Agent agent, Deployer deployer, Metric metric, NodeRepository nodeRepository) {\n+            if (isEmpty()) return false;\n+            ApplicationId application = node.allocation().get().owner();\n+            try (MaintenanceDeployment deployment = new MaintenanceDeployment(application, deployer, metric, nodeRepository)) {\n+                if ( ! deployment.isValid()) return false;\n+\n+                boolean couldMarkRetiredNow = markWantToRetire(node, true, agent, nodeRepository);\n+                if ( ! couldMarkRetiredNow) return false;\n+\n+                Optional<Node> expectedNewNode = Optional.empty();\n+                try {\n+                    if ( ! deployment.prepare()) return false;\n+                    if (verifyTarget) {\n+                        expectedNewNode =\n+                                nodeRepository.getNodes(application, Node.State.reserved).stream()\n+                                              .filter(n -> !n.hostname().equals(node.hostname()))\n+                                              .filter(n -> n.allocation().get().membership().cluster().id().equals(node.allocation().get().membership().cluster().id()))\n+                                              .findAny();\n+                        if (expectedNewNode.isEmpty()) return false;\n+                        if (!expectedNewNode.get().hasParent(toHost.hostname())) return false;\n+                    }\n+                    if ( ! deployment.activate()) return false;\n+\n+                    log.info(agent + \" redeployed \" + application + \" to \" +\n+                             ( verifyTarget ? this : \"move \" + (node.hostname() + \" from \" + fromHost)));\n+                    return true;\n+                }\n+                finally {\n+                    markWantToRetire(node, false, agent, nodeRepository); // Necessary if this failed, no-op otherwise\n+\n+                    // Immediately clean up if we reserved the node but could not activate or reserved a node on the wrong host\n+                    expectedNewNode.flatMap(node -> nodeRepository.getNode(node.hostname(), Node.State.reserved))\n+                                   .ifPresent(node -> nodeRepository.setDirty(node, Agent.Rebalancer, \"Expired by \" + agent));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTY5Njc0OA=="}, "originalCommit": {"oid": "28cf15bac333eadb4f3d133c9102c32bd0d72770"}, "originalPosition": 73}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1MTg3MjA4OnYy", "diffSide": "RIGHT", "path": "node-repository/src/main/java/com/yahoo/vespa/hosted/provision/node/Agent.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxNzowODowN1rOGlPHCg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QyMDo1ODo0N1rOGlXMnw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTY5ODA1OA==", "bodyText": "Also add to \n  \n    \n      vespa/controller-api/src/main/java/com/yahoo/vespa/hosted/controller/api/integration/noderepository/NodeHistory.java\n    \n    \n         Line 35\n      in\n      06ed2a1\n    \n    \n    \n    \n\n        \n          \n           public enum Agent {", "url": "https://github.com/vespa-engine/vespa/pull/13619#discussion_r441698058", "createdAt": "2020-06-17T17:08:07Z", "author": {"login": "freva"}, "path": "node-repository/src/main/java/com/yahoo/vespa/hosted/provision/node/Agent.java", "diffHunk": "@@ -21,6 +21,7 @@\n     ProvisionedExpirer,\n     ReservationExpirer,\n     DynamicProvisioningMaintainer,\n-    RetiringUpgrader;\n+    RetiringUpgrader,\n+    SpareCapacityMaintainer", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "28cf15bac333eadb4f3d133c9102c32bd0d72770"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTgzMDU1OQ==", "bodyText": "Done", "url": "https://github.com/vespa-engine/vespa/pull/13619#discussion_r441830559", "createdAt": "2020-06-17T20:58:47Z", "author": {"login": "bratseth"}, "path": "node-repository/src/main/java/com/yahoo/vespa/hosted/provision/node/Agent.java", "diffHunk": "@@ -21,6 +21,7 @@\n     ProvisionedExpirer,\n     ReservationExpirer,\n     DynamicProvisioningMaintainer,\n-    RetiringUpgrader;\n+    RetiringUpgrader,\n+    SpareCapacityMaintainer", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTY5ODA1OA=="}, "originalCommit": {"oid": "28cf15bac333eadb4f3d133c9102c32bd0d72770"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1MTk2MjA3OnYy", "diffSide": "RIGHT", "path": "node-repository/src/main/java/com/yahoo/vespa/hosted/provision/provisioning/HostCapacity.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxNzozMzozNlrOGlQBZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QyMToyMzowNlrOGlX6Gg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTcxMjk5OA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * Spare hosts are the two hosts in the system with the most free capacity.\n          \n          \n            \n                 * Spare hosts are the hosts in the system with the most free capacity.", "url": "https://github.com/vespa-engine/vespa/pull/13619#discussion_r441712998", "createdAt": "2020-06-17T17:33:36Z", "author": {"login": "freva"}, "path": "node-repository/src/main/java/com/yahoo/vespa/hosted/provision/provisioning/HostCapacity.java", "diffHunk": "@@ -16,17 +20,38 @@\n  *\n  * @author smorgrav\n  */\n-public class DockerHostCapacity {\n+public class HostCapacity {\n \n     private final NodeList allNodes;\n     private final HostResourcesCalculator hostResourcesCalculator;\n \n-    public DockerHostCapacity(NodeList allNodes, HostResourcesCalculator hostResourcesCalculator) {\n+    public HostCapacity(NodeList allNodes, HostResourcesCalculator hostResourcesCalculator) {\n         this.allNodes = Objects.requireNonNull(allNodes, \"allNodes must be non-null\");\n         this.hostResourcesCalculator = Objects.requireNonNull(hostResourcesCalculator, \"hostResourcesCalculator must be non-null\");\n     }\n \n-    int compareWithoutInactive(Node hostA, Node hostB) {\n+    public NodeList allNodes() { return allNodes; }\n+\n+    /**\n+     * Spare hosts are the two hosts in the system with the most free capacity.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "28cf15bac333eadb4f3d133c9102c32bd0d72770"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTgzMDk5OA==", "bodyText": "It's really the two  ...", "url": "https://github.com/vespa-engine/vespa/pull/13619#discussion_r441830998", "createdAt": "2020-06-17T20:59:44Z", "author": {"login": "bratseth"}, "path": "node-repository/src/main/java/com/yahoo/vespa/hosted/provision/provisioning/HostCapacity.java", "diffHunk": "@@ -16,17 +20,38 @@\n  *\n  * @author smorgrav\n  */\n-public class DockerHostCapacity {\n+public class HostCapacity {\n \n     private final NodeList allNodes;\n     private final HostResourcesCalculator hostResourcesCalculator;\n \n-    public DockerHostCapacity(NodeList allNodes, HostResourcesCalculator hostResourcesCalculator) {\n+    public HostCapacity(NodeList allNodes, HostResourcesCalculator hostResourcesCalculator) {\n         this.allNodes = Objects.requireNonNull(allNodes, \"allNodes must be non-null\");\n         this.hostResourcesCalculator = Objects.requireNonNull(hostResourcesCalculator, \"hostResourcesCalculator must be non-null\");\n     }\n \n-    int compareWithoutInactive(Node hostA, Node hostB) {\n+    public NodeList allNodes() { return allNodes; }\n+\n+    /**\n+     * Spare hosts are the two hosts in the system with the most free capacity.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTcxMjk5OA=="}, "originalCommit": {"oid": "28cf15bac333eadb4f3d133c9102c32bd0d72770"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTgzNDIzNg==", "bodyText": "As far as this method is concerned, it's the count hosts?", "url": "https://github.com/vespa-engine/vespa/pull/13619#discussion_r441834236", "createdAt": "2020-06-17T21:06:06Z", "author": {"login": "freva"}, "path": "node-repository/src/main/java/com/yahoo/vespa/hosted/provision/provisioning/HostCapacity.java", "diffHunk": "@@ -16,17 +20,38 @@\n  *\n  * @author smorgrav\n  */\n-public class DockerHostCapacity {\n+public class HostCapacity {\n \n     private final NodeList allNodes;\n     private final HostResourcesCalculator hostResourcesCalculator;\n \n-    public DockerHostCapacity(NodeList allNodes, HostResourcesCalculator hostResourcesCalculator) {\n+    public HostCapacity(NodeList allNodes, HostResourcesCalculator hostResourcesCalculator) {\n         this.allNodes = Objects.requireNonNull(allNodes, \"allNodes must be non-null\");\n         this.hostResourcesCalculator = Objects.requireNonNull(hostResourcesCalculator, \"hostResourcesCalculator must be non-null\");\n     }\n \n-    int compareWithoutInactive(Node hostA, Node hostB) {\n+    public NodeList allNodes() { return allNodes; }\n+\n+    /**\n+     * Spare hosts are the two hosts in the system with the most free capacity.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTcxMjk5OA=="}, "originalCommit": {"oid": "28cf15bac333eadb4f3d133c9102c32bd0d72770"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTg0MjIwMg==", "bodyText": "Also, don't we need to set these to 2:\n\n  \n    \n      vespa/node-repository/src/main/java/com/yahoo/vespa/hosted/provision/provisioning/NodeRepositoryProvisioner.java\n    \n    \n         Line 49\n      in\n      193c96e\n    \n    \n    \n    \n\n        \n          \n           private static final int SPARE_CAPACITY_PROD = 0; \n        \n    \n  \n\n\n?", "url": "https://github.com/vespa-engine/vespa/pull/13619#discussion_r441842202", "createdAt": "2020-06-17T21:23:06Z", "author": {"login": "freva"}, "path": "node-repository/src/main/java/com/yahoo/vespa/hosted/provision/provisioning/HostCapacity.java", "diffHunk": "@@ -16,17 +20,38 @@\n  *\n  * @author smorgrav\n  */\n-public class DockerHostCapacity {\n+public class HostCapacity {\n \n     private final NodeList allNodes;\n     private final HostResourcesCalculator hostResourcesCalculator;\n \n-    public DockerHostCapacity(NodeList allNodes, HostResourcesCalculator hostResourcesCalculator) {\n+    public HostCapacity(NodeList allNodes, HostResourcesCalculator hostResourcesCalculator) {\n         this.allNodes = Objects.requireNonNull(allNodes, \"allNodes must be non-null\");\n         this.hostResourcesCalculator = Objects.requireNonNull(hostResourcesCalculator, \"hostResourcesCalculator must be non-null\");\n     }\n \n-    int compareWithoutInactive(Node hostA, Node hostB) {\n+    public NodeList allNodes() { return allNodes; }\n+\n+    /**\n+     * Spare hosts are the two hosts in the system with the most free capacity.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTcxMjk5OA=="}, "originalCommit": {"oid": "28cf15bac333eadb4f3d133c9102c32bd0d72770"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1MTk5ODUxOnYy", "diffSide": "RIGHT", "path": "node-repository/src/main/java/com/yahoo/vespa/hosted/provision/maintenance/SpareCapacityMaintainer.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxNzo0Mzo1MFrOGlQYtg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QyMToxNzowMVrOGlXvRA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTcxODk2Ng==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            resources = resources.add(move.fromHost().resources());\n          \n          \n            \n                            resources = resources.add(move.node().resources());", "url": "https://github.com/vespa-engine/vespa/pull/13619#discussion_r441718966", "createdAt": "2020-06-17T17:43:50Z", "author": {"login": "freva"}, "path": "node-repository/src/main/java/com/yahoo/vespa/hosted/provision/maintenance/SpareCapacityMaintainer.java", "diffHunk": "@@ -0,0 +1,346 @@\n+// Copyright Verizon Media. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.\n+package com.yahoo.vespa.hosted.provision.maintenance;\n+\n+import com.yahoo.config.provision.Deployer;\n+import com.yahoo.config.provision.NodeResources;\n+import com.yahoo.jdisc.Metric;\n+import com.yahoo.vespa.hosted.provision.Node;\n+import com.yahoo.vespa.hosted.provision.NodeList;\n+import com.yahoo.vespa.hosted.provision.NodeRepository;\n+import com.yahoo.vespa.hosted.provision.maintenance.MaintenanceDeployment.Move;\n+import com.yahoo.vespa.hosted.provision.node.Agent;\n+import com.yahoo.vespa.hosted.provision.provisioning.HostCapacity;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.logging.Level;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * A maintainer which attempts to ensure there is spare capacity available in chunks which can fit\n+ * all node resource configuration in use, such that the system is able to quickly replace a failed node\n+ * if necessary.\n+ *\n+ * This also emits the following metrics:\n+ * - Overcommitted hosts: Hosts whose capacity is less than the sum of its children's\n+ * - Spare host capacity, or how many hosts the repository can stand to lose without ending up in a situation where it's\n+ *   unable to find a new home for orphaned tenants.\n+ *\n+ * @author mgimle\n+ * @author bratseth\n+ */\n+public class SpareCapacityMaintainer extends NodeRepositoryMaintainer {\n+\n+    private final int maxIterations;\n+    private final Deployer deployer;\n+    private final Metric metric;\n+\n+    public SpareCapacityMaintainer(Deployer deployer,\n+                                   NodeRepository nodeRepository,\n+                                   Metric metric,\n+                                   Duration interval) {\n+        this(deployer, nodeRepository, metric, interval,\n+             10_000 // Should take less than a few minutes\n+        );\n+    }\n+\n+    public SpareCapacityMaintainer(Deployer deployer,\n+                                   NodeRepository nodeRepository,\n+                                   Metric metric,\n+                                   Duration interval,\n+                                   int maxIterations) {\n+        super(nodeRepository, interval);\n+        this.deployer = deployer;\n+        this.metric = metric;\n+        this.maxIterations = maxIterations;\n+    }\n+\n+    @Override\n+    protected void maintain() {\n+        if ( ! nodeRepository().zone().getCloud().allowHostSharing()) return;\n+\n+        CapacityChecker capacityChecker = new CapacityChecker(nodeRepository());\n+\n+        List<Node> overcommittedHosts = capacityChecker.findOvercommittedHosts();\n+        if (overcommittedHosts.size() != 0) {\n+            log.log(Level.WARNING, String.format(\"%d nodes are overcommitted! [ %s ]\",\n+                                                 overcommittedHosts.size(),\n+                                                 overcommittedHosts.stream().map(Node::hostname).collect(Collectors.joining(\", \"))));\n+        }\n+        metric.set(\"overcommittedHosts\", overcommittedHosts.size(), null);\n+\n+        Optional<CapacityChecker.HostFailurePath> failurePath = capacityChecker.worstCaseHostLossLeadingToFailure();\n+        if (failurePath.isPresent()) {\n+            int spareHostCapacity = failurePath.get().hostsCausingFailure.size() - 1;\n+            if (spareHostCapacity == 0) {\n+                Move move = findMitigatingMove(failurePath.get());\n+                if (moving(move)) {\n+                    // We succeeded or are in the process of taking a step to mitigate.\n+                    // Report with the assumption this will eventually succeed to avoid alerting before we're stuck\n+                    spareHostCapacity++;\n+                }\n+            }\n+            metric.set(\"spareHostCapacity\", spareHostCapacity, null);\n+        }\n+    }\n+\n+    private boolean moving(Move move) {\n+        if (move.isEmpty()) return false;\n+        if (move.node().allocation().get().membership().retired()) return true; // Move already in progress\n+        return move.execute(false, Agent.SpareCapacityMaintainer, deployer, metric, nodeRepository());\n+    }\n+\n+    private Move findMitigatingMove(CapacityChecker.HostFailurePath failurePath) {\n+        Optional<Node> nodeWhichCantMove = failurePath.failureReason.tenant;\n+        if (nodeWhichCantMove.isEmpty()) return Move.empty();\n+\n+        Node node = nodeWhichCantMove.get();\n+        NodeList allNodes = nodeRepository().list();\n+        // Allocation will assign the two most empty nodes as \"spares\", which will not be allocated on\n+        // unless needed for node failing. Our goal here is to make room on these spares for the given node\n+        HostCapacity hostCapacity = new HostCapacity(allNodes, nodeRepository().resourcesCalculator());\n+        Set<Node> spareHosts = hostCapacity.findSpareHosts(allNodes.hosts().satisfies(node.resources()).asList(), 2);\n+        List<Node> hosts = allNodes.hosts().except(spareHosts).asList();\n+\n+        CapacitySolver capacitySolver = new CapacitySolver(hostCapacity, maxIterations);\n+        List<Move> shortestMitigation = null;\n+        for (Node spareHost : spareHosts) {\n+            List<Move> mitigation = capacitySolver.makeRoomFor(node, spareHost, hosts, List.of(), List.of());\n+            if (mitigation == null) continue;\n+            if (shortestMitigation == null || shortestMitigation.size() > mitigation.size())\n+                shortestMitigation = mitigation;\n+        }\n+        if (shortestMitigation == null || shortestMitigation.isEmpty()) return Move.empty();\n+        return shortestMitigation.get(0);\n+    }\n+\n+    private static class CapacitySolver {\n+\n+        private final HostCapacity hostCapacity;\n+        private final int maxIterations;\n+\n+        private int iterations = 0;\n+\n+        CapacitySolver(HostCapacity hostCapacity, int maxIterations) {\n+            this.hostCapacity = hostCapacity;\n+            this.maxIterations = maxIterations;\n+        }\n+\n+        /** The map of subproblem solutions already found. The value is null when there is no solution. */\n+        private Map<SolutionKey, List<Move>> solutions = new HashMap<>();\n+\n+        /**\n+         * Finds the shortest sequence of moves which makes room for the given node on the given host,\n+         * assuming the given moves already made over the given hosts' current allocation.\n+         *\n+         * @param node the node to make room for\n+         * @param host the target host to make room on\n+         * @param hosts the hosts onto which we can move nodes\n+         * @param movesConsidered the moves already being considered to add as part of this scenario\n+         *                        (after any moves made by this)\n+         * @param movesMade the moves already made in this scenario\n+         * @return the list of movesMade with the moves needed for this appended, in the order they should be performed,\n+         *         or null if no sequence could be found\n+         */\n+        List<Move> makeRoomFor(Node node, Node host, List<Node> hosts, List<Move> movesConsidered, List<Move> movesMade) {\n+            SolutionKey solutionKey = new SolutionKey(node, host, movesConsidered, movesMade);\n+            List<Move> solution = solutions.get(solutionKey);\n+            if (solution == null) {\n+                solution = findRoomFor(node, host, hosts, movesConsidered, movesMade);\n+                solutions.put(solutionKey, solution);\n+            }\n+            return solution;\n+        }\n+\n+        private List<Move> findRoomFor(Node node, Node host, List<Node> hosts,\n+                                       List<Move> movesConsidered, List<Move> movesMade) {\n+            if (iterations++ > maxIterations)\n+                return null;\n+\n+            if ( ! host.resources().satisfies(node.resources())) return null;\n+            NodeResources freeCapacity = freeCapacityWith(movesMade, host);\n+            if (freeCapacity.satisfies(node.resources())) return List.of();\n+\n+            List<Move> shortest = null;\n+            for (var i = subsets(hostCapacity.allNodes().childrenOf(host), 5); i.hasNext(); ) {\n+                List<Node> childrenToMove = i.next();\n+                if ( ! addResourcesOf(childrenToMove, freeCapacity).satisfies(node.resources())) continue;\n+                List<Move> moves = move(childrenToMove, host, hosts, movesConsidered, movesMade);\n+                if (moves == null) continue;\n+\n+                if (shortest == null || moves.size() < shortest.size())\n+                    shortest = moves;\n+            }\n+            if (shortest == null) return null;\n+            return append(movesMade, shortest);\n+        }\n+\n+        private List<Move> move(List<Node> nodes, Node host, List<Node> hosts, List<Move> movesConsidered, List<Move> movesMade) {\n+            List<Move> moves = new ArrayList<>();\n+            for (Node childToMove : nodes) {\n+                List<Move> childMoves = move(childToMove, host, hosts, movesConsidered, append(movesMade, moves));\n+                if (childMoves == null) return null;\n+                moves.addAll(childMoves);\n+            }\n+            return moves;\n+        }\n+\n+        private List<Move> move(Node node, Node host, List<Node> hosts, List<Move> movesConsidered, List<Move> movesMade) {\n+            if (contains(node, movesConsidered)) return null;\n+            if (contains(node, movesMade)) return null;\n+            List<Move> shortest = null;\n+            for (Node target : hosts) {\n+                if (target.equals(host)) continue;\n+                Move move = new Move(node, host, target);\n+                List<Move> childMoves = makeRoomFor(node, target, hosts, append(movesConsidered, move), movesMade);\n+                if (childMoves == null) continue;\n+                if (shortest == null || shortest.size() > childMoves.size() + 1) {\n+                    shortest = new ArrayList<>(childMoves);\n+                    shortest.add(move);\n+                }\n+            }\n+            return shortest;\n+        }\n+\n+        private boolean contains(Node node, List<Move> moves) {\n+            return moves.stream().anyMatch(move -> move.node().equals(node));\n+        }\n+\n+        private NodeResources addResourcesOf(List<Node> nodes, NodeResources resources) {\n+            for (Node node : nodes)\n+                resources = resources.add(node.resources());\n+            return resources;\n+        }\n+\n+        private Iterator<List<Node>> subsets(NodeList nodes, int maxSize) {\n+            return new SubsetIterator(nodes.asList(), maxSize);\n+        }\n+\n+        private List<Move> append(List<Move> a, List<Move> b) {\n+            List<Move> list = new ArrayList<>();\n+            list.addAll(a);\n+            list.addAll(b);\n+            return list;\n+        }\n+\n+        private List<Move> append(List<Move> moves, Move move) {\n+            List<Move> list = new ArrayList<>(moves);\n+            list.add(move);\n+            return list;\n+        }\n+\n+        private NodeResources freeCapacityWith(List<Move> moves, Node host) {\n+            NodeResources resources = hostCapacity.freeCapacityOf(host);\n+            for (Move move : moves) {\n+                if ( ! move.toHost().equals(host)) continue;\n+                resources = resources.subtract(move.node().resources());\n+            }\n+            for (Move move : moves) {\n+                if ( ! move.fromHost().equals(host)) continue;\n+                resources = resources.add(move.fromHost().resources());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "28cf15bac333eadb4f3d133c9102c32bd0d72770"}, "originalPosition": 247}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTgzOTQyOA==", "bodyText": "\ud83d\udcaf thanks!", "url": "https://github.com/vespa-engine/vespa/pull/13619#discussion_r441839428", "createdAt": "2020-06-17T21:17:01Z", "author": {"login": "bratseth"}, "path": "node-repository/src/main/java/com/yahoo/vespa/hosted/provision/maintenance/SpareCapacityMaintainer.java", "diffHunk": "@@ -0,0 +1,346 @@\n+// Copyright Verizon Media. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.\n+package com.yahoo.vespa.hosted.provision.maintenance;\n+\n+import com.yahoo.config.provision.Deployer;\n+import com.yahoo.config.provision.NodeResources;\n+import com.yahoo.jdisc.Metric;\n+import com.yahoo.vespa.hosted.provision.Node;\n+import com.yahoo.vespa.hosted.provision.NodeList;\n+import com.yahoo.vespa.hosted.provision.NodeRepository;\n+import com.yahoo.vespa.hosted.provision.maintenance.MaintenanceDeployment.Move;\n+import com.yahoo.vespa.hosted.provision.node.Agent;\n+import com.yahoo.vespa.hosted.provision.provisioning.HostCapacity;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.logging.Level;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * A maintainer which attempts to ensure there is spare capacity available in chunks which can fit\n+ * all node resource configuration in use, such that the system is able to quickly replace a failed node\n+ * if necessary.\n+ *\n+ * This also emits the following metrics:\n+ * - Overcommitted hosts: Hosts whose capacity is less than the sum of its children's\n+ * - Spare host capacity, or how many hosts the repository can stand to lose without ending up in a situation where it's\n+ *   unable to find a new home for orphaned tenants.\n+ *\n+ * @author mgimle\n+ * @author bratseth\n+ */\n+public class SpareCapacityMaintainer extends NodeRepositoryMaintainer {\n+\n+    private final int maxIterations;\n+    private final Deployer deployer;\n+    private final Metric metric;\n+\n+    public SpareCapacityMaintainer(Deployer deployer,\n+                                   NodeRepository nodeRepository,\n+                                   Metric metric,\n+                                   Duration interval) {\n+        this(deployer, nodeRepository, metric, interval,\n+             10_000 // Should take less than a few minutes\n+        );\n+    }\n+\n+    public SpareCapacityMaintainer(Deployer deployer,\n+                                   NodeRepository nodeRepository,\n+                                   Metric metric,\n+                                   Duration interval,\n+                                   int maxIterations) {\n+        super(nodeRepository, interval);\n+        this.deployer = deployer;\n+        this.metric = metric;\n+        this.maxIterations = maxIterations;\n+    }\n+\n+    @Override\n+    protected void maintain() {\n+        if ( ! nodeRepository().zone().getCloud().allowHostSharing()) return;\n+\n+        CapacityChecker capacityChecker = new CapacityChecker(nodeRepository());\n+\n+        List<Node> overcommittedHosts = capacityChecker.findOvercommittedHosts();\n+        if (overcommittedHosts.size() != 0) {\n+            log.log(Level.WARNING, String.format(\"%d nodes are overcommitted! [ %s ]\",\n+                                                 overcommittedHosts.size(),\n+                                                 overcommittedHosts.stream().map(Node::hostname).collect(Collectors.joining(\", \"))));\n+        }\n+        metric.set(\"overcommittedHosts\", overcommittedHosts.size(), null);\n+\n+        Optional<CapacityChecker.HostFailurePath> failurePath = capacityChecker.worstCaseHostLossLeadingToFailure();\n+        if (failurePath.isPresent()) {\n+            int spareHostCapacity = failurePath.get().hostsCausingFailure.size() - 1;\n+            if (spareHostCapacity == 0) {\n+                Move move = findMitigatingMove(failurePath.get());\n+                if (moving(move)) {\n+                    // We succeeded or are in the process of taking a step to mitigate.\n+                    // Report with the assumption this will eventually succeed to avoid alerting before we're stuck\n+                    spareHostCapacity++;\n+                }\n+            }\n+            metric.set(\"spareHostCapacity\", spareHostCapacity, null);\n+        }\n+    }\n+\n+    private boolean moving(Move move) {\n+        if (move.isEmpty()) return false;\n+        if (move.node().allocation().get().membership().retired()) return true; // Move already in progress\n+        return move.execute(false, Agent.SpareCapacityMaintainer, deployer, metric, nodeRepository());\n+    }\n+\n+    private Move findMitigatingMove(CapacityChecker.HostFailurePath failurePath) {\n+        Optional<Node> nodeWhichCantMove = failurePath.failureReason.tenant;\n+        if (nodeWhichCantMove.isEmpty()) return Move.empty();\n+\n+        Node node = nodeWhichCantMove.get();\n+        NodeList allNodes = nodeRepository().list();\n+        // Allocation will assign the two most empty nodes as \"spares\", which will not be allocated on\n+        // unless needed for node failing. Our goal here is to make room on these spares for the given node\n+        HostCapacity hostCapacity = new HostCapacity(allNodes, nodeRepository().resourcesCalculator());\n+        Set<Node> spareHosts = hostCapacity.findSpareHosts(allNodes.hosts().satisfies(node.resources()).asList(), 2);\n+        List<Node> hosts = allNodes.hosts().except(spareHosts).asList();\n+\n+        CapacitySolver capacitySolver = new CapacitySolver(hostCapacity, maxIterations);\n+        List<Move> shortestMitigation = null;\n+        for (Node spareHost : spareHosts) {\n+            List<Move> mitigation = capacitySolver.makeRoomFor(node, spareHost, hosts, List.of(), List.of());\n+            if (mitigation == null) continue;\n+            if (shortestMitigation == null || shortestMitigation.size() > mitigation.size())\n+                shortestMitigation = mitigation;\n+        }\n+        if (shortestMitigation == null || shortestMitigation.isEmpty()) return Move.empty();\n+        return shortestMitigation.get(0);\n+    }\n+\n+    private static class CapacitySolver {\n+\n+        private final HostCapacity hostCapacity;\n+        private final int maxIterations;\n+\n+        private int iterations = 0;\n+\n+        CapacitySolver(HostCapacity hostCapacity, int maxIterations) {\n+            this.hostCapacity = hostCapacity;\n+            this.maxIterations = maxIterations;\n+        }\n+\n+        /** The map of subproblem solutions already found. The value is null when there is no solution. */\n+        private Map<SolutionKey, List<Move>> solutions = new HashMap<>();\n+\n+        /**\n+         * Finds the shortest sequence of moves which makes room for the given node on the given host,\n+         * assuming the given moves already made over the given hosts' current allocation.\n+         *\n+         * @param node the node to make room for\n+         * @param host the target host to make room on\n+         * @param hosts the hosts onto which we can move nodes\n+         * @param movesConsidered the moves already being considered to add as part of this scenario\n+         *                        (after any moves made by this)\n+         * @param movesMade the moves already made in this scenario\n+         * @return the list of movesMade with the moves needed for this appended, in the order they should be performed,\n+         *         or null if no sequence could be found\n+         */\n+        List<Move> makeRoomFor(Node node, Node host, List<Node> hosts, List<Move> movesConsidered, List<Move> movesMade) {\n+            SolutionKey solutionKey = new SolutionKey(node, host, movesConsidered, movesMade);\n+            List<Move> solution = solutions.get(solutionKey);\n+            if (solution == null) {\n+                solution = findRoomFor(node, host, hosts, movesConsidered, movesMade);\n+                solutions.put(solutionKey, solution);\n+            }\n+            return solution;\n+        }\n+\n+        private List<Move> findRoomFor(Node node, Node host, List<Node> hosts,\n+                                       List<Move> movesConsidered, List<Move> movesMade) {\n+            if (iterations++ > maxIterations)\n+                return null;\n+\n+            if ( ! host.resources().satisfies(node.resources())) return null;\n+            NodeResources freeCapacity = freeCapacityWith(movesMade, host);\n+            if (freeCapacity.satisfies(node.resources())) return List.of();\n+\n+            List<Move> shortest = null;\n+            for (var i = subsets(hostCapacity.allNodes().childrenOf(host), 5); i.hasNext(); ) {\n+                List<Node> childrenToMove = i.next();\n+                if ( ! addResourcesOf(childrenToMove, freeCapacity).satisfies(node.resources())) continue;\n+                List<Move> moves = move(childrenToMove, host, hosts, movesConsidered, movesMade);\n+                if (moves == null) continue;\n+\n+                if (shortest == null || moves.size() < shortest.size())\n+                    shortest = moves;\n+            }\n+            if (shortest == null) return null;\n+            return append(movesMade, shortest);\n+        }\n+\n+        private List<Move> move(List<Node> nodes, Node host, List<Node> hosts, List<Move> movesConsidered, List<Move> movesMade) {\n+            List<Move> moves = new ArrayList<>();\n+            for (Node childToMove : nodes) {\n+                List<Move> childMoves = move(childToMove, host, hosts, movesConsidered, append(movesMade, moves));\n+                if (childMoves == null) return null;\n+                moves.addAll(childMoves);\n+            }\n+            return moves;\n+        }\n+\n+        private List<Move> move(Node node, Node host, List<Node> hosts, List<Move> movesConsidered, List<Move> movesMade) {\n+            if (contains(node, movesConsidered)) return null;\n+            if (contains(node, movesMade)) return null;\n+            List<Move> shortest = null;\n+            for (Node target : hosts) {\n+                if (target.equals(host)) continue;\n+                Move move = new Move(node, host, target);\n+                List<Move> childMoves = makeRoomFor(node, target, hosts, append(movesConsidered, move), movesMade);\n+                if (childMoves == null) continue;\n+                if (shortest == null || shortest.size() > childMoves.size() + 1) {\n+                    shortest = new ArrayList<>(childMoves);\n+                    shortest.add(move);\n+                }\n+            }\n+            return shortest;\n+        }\n+\n+        private boolean contains(Node node, List<Move> moves) {\n+            return moves.stream().anyMatch(move -> move.node().equals(node));\n+        }\n+\n+        private NodeResources addResourcesOf(List<Node> nodes, NodeResources resources) {\n+            for (Node node : nodes)\n+                resources = resources.add(node.resources());\n+            return resources;\n+        }\n+\n+        private Iterator<List<Node>> subsets(NodeList nodes, int maxSize) {\n+            return new SubsetIterator(nodes.asList(), maxSize);\n+        }\n+\n+        private List<Move> append(List<Move> a, List<Move> b) {\n+            List<Move> list = new ArrayList<>();\n+            list.addAll(a);\n+            list.addAll(b);\n+            return list;\n+        }\n+\n+        private List<Move> append(List<Move> moves, Move move) {\n+            List<Move> list = new ArrayList<>(moves);\n+            list.add(move);\n+            return list;\n+        }\n+\n+        private NodeResources freeCapacityWith(List<Move> moves, Node host) {\n+            NodeResources resources = hostCapacity.freeCapacityOf(host);\n+            for (Move move : moves) {\n+                if ( ! move.toHost().equals(host)) continue;\n+                resources = resources.subtract(move.node().resources());\n+            }\n+            for (Move move : moves) {\n+                if ( ! move.fromHost().equals(host)) continue;\n+                resources = resources.add(move.fromHost().resources());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTcxODk2Ng=="}, "originalCommit": {"oid": "28cf15bac333eadb4f3d133c9102c32bd0d72770"}, "originalPosition": 247}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1MjA0Mzg4OnYy", "diffSide": "RIGHT", "path": "node-repository/src/main/java/com/yahoo/vespa/hosted/provision/maintenance/SpareCapacityMaintainer.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxNzo1NjoxMlrOGlQ1uw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QyMTowMjoxOFrOGlXTlg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTcyNjM5NQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        int ones = 0;\n          \n          \n            \n                        for (int position = 0; Math.pow(2, position) <= number; position++) {\n          \n          \n            \n                            if (hasOneAtPosition(position, number))\n          \n          \n            \n                                ones++;\n          \n          \n            \n                        }\n          \n          \n            \n                        return ones;\n          \n          \n            \n                        return Integer.bitCount(number);", "url": "https://github.com/vespa-engine/vespa/pull/13619#discussion_r441726395", "createdAt": "2020-06-17T17:56:12Z", "author": {"login": "freva"}, "path": "node-repository/src/main/java/com/yahoo/vespa/hosted/provision/maintenance/SpareCapacityMaintainer.java", "diffHunk": "@@ -0,0 +1,346 @@\n+// Copyright Verizon Media. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.\n+package com.yahoo.vespa.hosted.provision.maintenance;\n+\n+import com.yahoo.config.provision.Deployer;\n+import com.yahoo.config.provision.NodeResources;\n+import com.yahoo.jdisc.Metric;\n+import com.yahoo.vespa.hosted.provision.Node;\n+import com.yahoo.vespa.hosted.provision.NodeList;\n+import com.yahoo.vespa.hosted.provision.NodeRepository;\n+import com.yahoo.vespa.hosted.provision.maintenance.MaintenanceDeployment.Move;\n+import com.yahoo.vespa.hosted.provision.node.Agent;\n+import com.yahoo.vespa.hosted.provision.provisioning.HostCapacity;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.logging.Level;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * A maintainer which attempts to ensure there is spare capacity available in chunks which can fit\n+ * all node resource configuration in use, such that the system is able to quickly replace a failed node\n+ * if necessary.\n+ *\n+ * This also emits the following metrics:\n+ * - Overcommitted hosts: Hosts whose capacity is less than the sum of its children's\n+ * - Spare host capacity, or how many hosts the repository can stand to lose without ending up in a situation where it's\n+ *   unable to find a new home for orphaned tenants.\n+ *\n+ * @author mgimle\n+ * @author bratseth\n+ */\n+public class SpareCapacityMaintainer extends NodeRepositoryMaintainer {\n+\n+    private final int maxIterations;\n+    private final Deployer deployer;\n+    private final Metric metric;\n+\n+    public SpareCapacityMaintainer(Deployer deployer,\n+                                   NodeRepository nodeRepository,\n+                                   Metric metric,\n+                                   Duration interval) {\n+        this(deployer, nodeRepository, metric, interval,\n+             10_000 // Should take less than a few minutes\n+        );\n+    }\n+\n+    public SpareCapacityMaintainer(Deployer deployer,\n+                                   NodeRepository nodeRepository,\n+                                   Metric metric,\n+                                   Duration interval,\n+                                   int maxIterations) {\n+        super(nodeRepository, interval);\n+        this.deployer = deployer;\n+        this.metric = metric;\n+        this.maxIterations = maxIterations;\n+    }\n+\n+    @Override\n+    protected void maintain() {\n+        if ( ! nodeRepository().zone().getCloud().allowHostSharing()) return;\n+\n+        CapacityChecker capacityChecker = new CapacityChecker(nodeRepository());\n+\n+        List<Node> overcommittedHosts = capacityChecker.findOvercommittedHosts();\n+        if (overcommittedHosts.size() != 0) {\n+            log.log(Level.WARNING, String.format(\"%d nodes are overcommitted! [ %s ]\",\n+                                                 overcommittedHosts.size(),\n+                                                 overcommittedHosts.stream().map(Node::hostname).collect(Collectors.joining(\", \"))));\n+        }\n+        metric.set(\"overcommittedHosts\", overcommittedHosts.size(), null);\n+\n+        Optional<CapacityChecker.HostFailurePath> failurePath = capacityChecker.worstCaseHostLossLeadingToFailure();\n+        if (failurePath.isPresent()) {\n+            int spareHostCapacity = failurePath.get().hostsCausingFailure.size() - 1;\n+            if (spareHostCapacity == 0) {\n+                Move move = findMitigatingMove(failurePath.get());\n+                if (moving(move)) {\n+                    // We succeeded or are in the process of taking a step to mitigate.\n+                    // Report with the assumption this will eventually succeed to avoid alerting before we're stuck\n+                    spareHostCapacity++;\n+                }\n+            }\n+            metric.set(\"spareHostCapacity\", spareHostCapacity, null);\n+        }\n+    }\n+\n+    private boolean moving(Move move) {\n+        if (move.isEmpty()) return false;\n+        if (move.node().allocation().get().membership().retired()) return true; // Move already in progress\n+        return move.execute(false, Agent.SpareCapacityMaintainer, deployer, metric, nodeRepository());\n+    }\n+\n+    private Move findMitigatingMove(CapacityChecker.HostFailurePath failurePath) {\n+        Optional<Node> nodeWhichCantMove = failurePath.failureReason.tenant;\n+        if (nodeWhichCantMove.isEmpty()) return Move.empty();\n+\n+        Node node = nodeWhichCantMove.get();\n+        NodeList allNodes = nodeRepository().list();\n+        // Allocation will assign the two most empty nodes as \"spares\", which will not be allocated on\n+        // unless needed for node failing. Our goal here is to make room on these spares for the given node\n+        HostCapacity hostCapacity = new HostCapacity(allNodes, nodeRepository().resourcesCalculator());\n+        Set<Node> spareHosts = hostCapacity.findSpareHosts(allNodes.hosts().satisfies(node.resources()).asList(), 2);\n+        List<Node> hosts = allNodes.hosts().except(spareHosts).asList();\n+\n+        CapacitySolver capacitySolver = new CapacitySolver(hostCapacity, maxIterations);\n+        List<Move> shortestMitigation = null;\n+        for (Node spareHost : spareHosts) {\n+            List<Move> mitigation = capacitySolver.makeRoomFor(node, spareHost, hosts, List.of(), List.of());\n+            if (mitigation == null) continue;\n+            if (shortestMitigation == null || shortestMitigation.size() > mitigation.size())\n+                shortestMitigation = mitigation;\n+        }\n+        if (shortestMitigation == null || shortestMitigation.isEmpty()) return Move.empty();\n+        return shortestMitigation.get(0);\n+    }\n+\n+    private static class CapacitySolver {\n+\n+        private final HostCapacity hostCapacity;\n+        private final int maxIterations;\n+\n+        private int iterations = 0;\n+\n+        CapacitySolver(HostCapacity hostCapacity, int maxIterations) {\n+            this.hostCapacity = hostCapacity;\n+            this.maxIterations = maxIterations;\n+        }\n+\n+        /** The map of subproblem solutions already found. The value is null when there is no solution. */\n+        private Map<SolutionKey, List<Move>> solutions = new HashMap<>();\n+\n+        /**\n+         * Finds the shortest sequence of moves which makes room for the given node on the given host,\n+         * assuming the given moves already made over the given hosts' current allocation.\n+         *\n+         * @param node the node to make room for\n+         * @param host the target host to make room on\n+         * @param hosts the hosts onto which we can move nodes\n+         * @param movesConsidered the moves already being considered to add as part of this scenario\n+         *                        (after any moves made by this)\n+         * @param movesMade the moves already made in this scenario\n+         * @return the list of movesMade with the moves needed for this appended, in the order they should be performed,\n+         *         or null if no sequence could be found\n+         */\n+        List<Move> makeRoomFor(Node node, Node host, List<Node> hosts, List<Move> movesConsidered, List<Move> movesMade) {\n+            SolutionKey solutionKey = new SolutionKey(node, host, movesConsidered, movesMade);\n+            List<Move> solution = solutions.get(solutionKey);\n+            if (solution == null) {\n+                solution = findRoomFor(node, host, hosts, movesConsidered, movesMade);\n+                solutions.put(solutionKey, solution);\n+            }\n+            return solution;\n+        }\n+\n+        private List<Move> findRoomFor(Node node, Node host, List<Node> hosts,\n+                                       List<Move> movesConsidered, List<Move> movesMade) {\n+            if (iterations++ > maxIterations)\n+                return null;\n+\n+            if ( ! host.resources().satisfies(node.resources())) return null;\n+            NodeResources freeCapacity = freeCapacityWith(movesMade, host);\n+            if (freeCapacity.satisfies(node.resources())) return List.of();\n+\n+            List<Move> shortest = null;\n+            for (var i = subsets(hostCapacity.allNodes().childrenOf(host), 5); i.hasNext(); ) {\n+                List<Node> childrenToMove = i.next();\n+                if ( ! addResourcesOf(childrenToMove, freeCapacity).satisfies(node.resources())) continue;\n+                List<Move> moves = move(childrenToMove, host, hosts, movesConsidered, movesMade);\n+                if (moves == null) continue;\n+\n+                if (shortest == null || moves.size() < shortest.size())\n+                    shortest = moves;\n+            }\n+            if (shortest == null) return null;\n+            return append(movesMade, shortest);\n+        }\n+\n+        private List<Move> move(List<Node> nodes, Node host, List<Node> hosts, List<Move> movesConsidered, List<Move> movesMade) {\n+            List<Move> moves = new ArrayList<>();\n+            for (Node childToMove : nodes) {\n+                List<Move> childMoves = move(childToMove, host, hosts, movesConsidered, append(movesMade, moves));\n+                if (childMoves == null) return null;\n+                moves.addAll(childMoves);\n+            }\n+            return moves;\n+        }\n+\n+        private List<Move> move(Node node, Node host, List<Node> hosts, List<Move> movesConsidered, List<Move> movesMade) {\n+            if (contains(node, movesConsidered)) return null;\n+            if (contains(node, movesMade)) return null;\n+            List<Move> shortest = null;\n+            for (Node target : hosts) {\n+                if (target.equals(host)) continue;\n+                Move move = new Move(node, host, target);\n+                List<Move> childMoves = makeRoomFor(node, target, hosts, append(movesConsidered, move), movesMade);\n+                if (childMoves == null) continue;\n+                if (shortest == null || shortest.size() > childMoves.size() + 1) {\n+                    shortest = new ArrayList<>(childMoves);\n+                    shortest.add(move);\n+                }\n+            }\n+            return shortest;\n+        }\n+\n+        private boolean contains(Node node, List<Move> moves) {\n+            return moves.stream().anyMatch(move -> move.node().equals(node));\n+        }\n+\n+        private NodeResources addResourcesOf(List<Node> nodes, NodeResources resources) {\n+            for (Node node : nodes)\n+                resources = resources.add(node.resources());\n+            return resources;\n+        }\n+\n+        private Iterator<List<Node>> subsets(NodeList nodes, int maxSize) {\n+            return new SubsetIterator(nodes.asList(), maxSize);\n+        }\n+\n+        private List<Move> append(List<Move> a, List<Move> b) {\n+            List<Move> list = new ArrayList<>();\n+            list.addAll(a);\n+            list.addAll(b);\n+            return list;\n+        }\n+\n+        private List<Move> append(List<Move> moves, Move move) {\n+            List<Move> list = new ArrayList<>(moves);\n+            list.add(move);\n+            return list;\n+        }\n+\n+        private NodeResources freeCapacityWith(List<Move> moves, Node host) {\n+            NodeResources resources = hostCapacity.freeCapacityOf(host);\n+            for (Move move : moves) {\n+                if ( ! move.toHost().equals(host)) continue;\n+                resources = resources.subtract(move.node().resources());\n+            }\n+            for (Move move : moves) {\n+                if ( ! move.fromHost().equals(host)) continue;\n+                resources = resources.add(move.fromHost().resources());\n+            }\n+            return resources;\n+        }\n+\n+    }\n+\n+    private static class SolutionKey {\n+\n+        private final Node node;\n+        private final Node host;\n+        private final List<Move> movesConsidered;\n+        private final List<Move> movesMade;\n+\n+        private final int hash;\n+\n+        public SolutionKey(Node node, Node host, List<Move> movesConsidered, List<Move> movesMade) {\n+            this.node = node;\n+            this.host = host;\n+            this.movesConsidered = movesConsidered;\n+            this.movesMade = movesMade;\n+\n+            hash = Objects.hash(node, host, movesConsidered, movesMade);\n+        }\n+\n+        @Override\n+        public int hashCode() { return hash; }\n+\n+        @Override\n+        public boolean equals(Object o) {\n+            if (o == this) return true;\n+            if (o == null || o.getClass() != this.getClass()) return false;\n+\n+            SolutionKey other = (SolutionKey)o;\n+            if ( ! other.node.equals(this.node)) return false;\n+            if ( ! other.host.equals(this.host)) return false;\n+            if ( ! other.movesConsidered.equals(this.movesConsidered)) return false;\n+            if ( ! other.movesMade.equals(this.movesMade)) return false;\n+            return true;\n+        }\n+\n+    }\n+\n+    private static class SubsetIterator implements Iterator<List<Node>> {\n+\n+        private final List<Node> nodes;\n+        private final int maxLength;\n+\n+        // A number whose binary representation determines which items of list we'll include\n+        private int i = 0; // first \"previous\" = 0 -> skip the empty set\n+        private List<Node> next = null;\n+\n+        public SubsetIterator(List<Node> nodes, int maxLength) {\n+            this.nodes = new ArrayList<>(nodes.subList(0, Math.min(nodes.size(), 31)));\n+            this.maxLength = maxLength;\n+        }\n+\n+        @Override\n+        public boolean hasNext() {\n+            if (next != null) return true;\n+\n+            // find next\n+            while (++i < 1<<nodes.size()) {\n+                int ones = onesIn(i);\n+                if (ones > maxLength) continue;\n+\n+                next = new ArrayList<>(ones);\n+                for (int position = 0; position < nodes.size(); position++) {\n+                    if (hasOneAtPosition(position, i))\n+                        next.add(nodes.get(position));\n+                }\n+                return true;\n+            }\n+            return false;\n+        }\n+\n+        @Override\n+        public List<Node> next() {\n+            if ( ! hasNext()) throw new IllegalStateException(\"No more elements\");\n+            var current = next;\n+            next = null;\n+            return current;\n+        }\n+\n+        private boolean hasOneAtPosition(int position, int number) {\n+            return (number & (1 << position)) > 0;\n+        }\n+\n+        private int onesIn(int number) {\n+            int ones = 0;\n+            for (int position = 0; Math.pow(2, position) <= number; position++) {\n+                if (hasOneAtPosition(position, number))\n+                    ones++;\n+            }\n+            return ones;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "28cf15bac333eadb4f3d133c9102c32bd0d72770"}, "originalPosition": 341}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTgzMjM0Mg==", "bodyText": "Didn't know about bitCount - thanks!", "url": "https://github.com/vespa-engine/vespa/pull/13619#discussion_r441832342", "createdAt": "2020-06-17T21:02:18Z", "author": {"login": "bratseth"}, "path": "node-repository/src/main/java/com/yahoo/vespa/hosted/provision/maintenance/SpareCapacityMaintainer.java", "diffHunk": "@@ -0,0 +1,346 @@\n+// Copyright Verizon Media. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.\n+package com.yahoo.vespa.hosted.provision.maintenance;\n+\n+import com.yahoo.config.provision.Deployer;\n+import com.yahoo.config.provision.NodeResources;\n+import com.yahoo.jdisc.Metric;\n+import com.yahoo.vespa.hosted.provision.Node;\n+import com.yahoo.vespa.hosted.provision.NodeList;\n+import com.yahoo.vespa.hosted.provision.NodeRepository;\n+import com.yahoo.vespa.hosted.provision.maintenance.MaintenanceDeployment.Move;\n+import com.yahoo.vespa.hosted.provision.node.Agent;\n+import com.yahoo.vespa.hosted.provision.provisioning.HostCapacity;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.logging.Level;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * A maintainer which attempts to ensure there is spare capacity available in chunks which can fit\n+ * all node resource configuration in use, such that the system is able to quickly replace a failed node\n+ * if necessary.\n+ *\n+ * This also emits the following metrics:\n+ * - Overcommitted hosts: Hosts whose capacity is less than the sum of its children's\n+ * - Spare host capacity, or how many hosts the repository can stand to lose without ending up in a situation where it's\n+ *   unable to find a new home for orphaned tenants.\n+ *\n+ * @author mgimle\n+ * @author bratseth\n+ */\n+public class SpareCapacityMaintainer extends NodeRepositoryMaintainer {\n+\n+    private final int maxIterations;\n+    private final Deployer deployer;\n+    private final Metric metric;\n+\n+    public SpareCapacityMaintainer(Deployer deployer,\n+                                   NodeRepository nodeRepository,\n+                                   Metric metric,\n+                                   Duration interval) {\n+        this(deployer, nodeRepository, metric, interval,\n+             10_000 // Should take less than a few minutes\n+        );\n+    }\n+\n+    public SpareCapacityMaintainer(Deployer deployer,\n+                                   NodeRepository nodeRepository,\n+                                   Metric metric,\n+                                   Duration interval,\n+                                   int maxIterations) {\n+        super(nodeRepository, interval);\n+        this.deployer = deployer;\n+        this.metric = metric;\n+        this.maxIterations = maxIterations;\n+    }\n+\n+    @Override\n+    protected void maintain() {\n+        if ( ! nodeRepository().zone().getCloud().allowHostSharing()) return;\n+\n+        CapacityChecker capacityChecker = new CapacityChecker(nodeRepository());\n+\n+        List<Node> overcommittedHosts = capacityChecker.findOvercommittedHosts();\n+        if (overcommittedHosts.size() != 0) {\n+            log.log(Level.WARNING, String.format(\"%d nodes are overcommitted! [ %s ]\",\n+                                                 overcommittedHosts.size(),\n+                                                 overcommittedHosts.stream().map(Node::hostname).collect(Collectors.joining(\", \"))));\n+        }\n+        metric.set(\"overcommittedHosts\", overcommittedHosts.size(), null);\n+\n+        Optional<CapacityChecker.HostFailurePath> failurePath = capacityChecker.worstCaseHostLossLeadingToFailure();\n+        if (failurePath.isPresent()) {\n+            int spareHostCapacity = failurePath.get().hostsCausingFailure.size() - 1;\n+            if (spareHostCapacity == 0) {\n+                Move move = findMitigatingMove(failurePath.get());\n+                if (moving(move)) {\n+                    // We succeeded or are in the process of taking a step to mitigate.\n+                    // Report with the assumption this will eventually succeed to avoid alerting before we're stuck\n+                    spareHostCapacity++;\n+                }\n+            }\n+            metric.set(\"spareHostCapacity\", spareHostCapacity, null);\n+        }\n+    }\n+\n+    private boolean moving(Move move) {\n+        if (move.isEmpty()) return false;\n+        if (move.node().allocation().get().membership().retired()) return true; // Move already in progress\n+        return move.execute(false, Agent.SpareCapacityMaintainer, deployer, metric, nodeRepository());\n+    }\n+\n+    private Move findMitigatingMove(CapacityChecker.HostFailurePath failurePath) {\n+        Optional<Node> nodeWhichCantMove = failurePath.failureReason.tenant;\n+        if (nodeWhichCantMove.isEmpty()) return Move.empty();\n+\n+        Node node = nodeWhichCantMove.get();\n+        NodeList allNodes = nodeRepository().list();\n+        // Allocation will assign the two most empty nodes as \"spares\", which will not be allocated on\n+        // unless needed for node failing. Our goal here is to make room on these spares for the given node\n+        HostCapacity hostCapacity = new HostCapacity(allNodes, nodeRepository().resourcesCalculator());\n+        Set<Node> spareHosts = hostCapacity.findSpareHosts(allNodes.hosts().satisfies(node.resources()).asList(), 2);\n+        List<Node> hosts = allNodes.hosts().except(spareHosts).asList();\n+\n+        CapacitySolver capacitySolver = new CapacitySolver(hostCapacity, maxIterations);\n+        List<Move> shortestMitigation = null;\n+        for (Node spareHost : spareHosts) {\n+            List<Move> mitigation = capacitySolver.makeRoomFor(node, spareHost, hosts, List.of(), List.of());\n+            if (mitigation == null) continue;\n+            if (shortestMitigation == null || shortestMitigation.size() > mitigation.size())\n+                shortestMitigation = mitigation;\n+        }\n+        if (shortestMitigation == null || shortestMitigation.isEmpty()) return Move.empty();\n+        return shortestMitigation.get(0);\n+    }\n+\n+    private static class CapacitySolver {\n+\n+        private final HostCapacity hostCapacity;\n+        private final int maxIterations;\n+\n+        private int iterations = 0;\n+\n+        CapacitySolver(HostCapacity hostCapacity, int maxIterations) {\n+            this.hostCapacity = hostCapacity;\n+            this.maxIterations = maxIterations;\n+        }\n+\n+        /** The map of subproblem solutions already found. The value is null when there is no solution. */\n+        private Map<SolutionKey, List<Move>> solutions = new HashMap<>();\n+\n+        /**\n+         * Finds the shortest sequence of moves which makes room for the given node on the given host,\n+         * assuming the given moves already made over the given hosts' current allocation.\n+         *\n+         * @param node the node to make room for\n+         * @param host the target host to make room on\n+         * @param hosts the hosts onto which we can move nodes\n+         * @param movesConsidered the moves already being considered to add as part of this scenario\n+         *                        (after any moves made by this)\n+         * @param movesMade the moves already made in this scenario\n+         * @return the list of movesMade with the moves needed for this appended, in the order they should be performed,\n+         *         or null if no sequence could be found\n+         */\n+        List<Move> makeRoomFor(Node node, Node host, List<Node> hosts, List<Move> movesConsidered, List<Move> movesMade) {\n+            SolutionKey solutionKey = new SolutionKey(node, host, movesConsidered, movesMade);\n+            List<Move> solution = solutions.get(solutionKey);\n+            if (solution == null) {\n+                solution = findRoomFor(node, host, hosts, movesConsidered, movesMade);\n+                solutions.put(solutionKey, solution);\n+            }\n+            return solution;\n+        }\n+\n+        private List<Move> findRoomFor(Node node, Node host, List<Node> hosts,\n+                                       List<Move> movesConsidered, List<Move> movesMade) {\n+            if (iterations++ > maxIterations)\n+                return null;\n+\n+            if ( ! host.resources().satisfies(node.resources())) return null;\n+            NodeResources freeCapacity = freeCapacityWith(movesMade, host);\n+            if (freeCapacity.satisfies(node.resources())) return List.of();\n+\n+            List<Move> shortest = null;\n+            for (var i = subsets(hostCapacity.allNodes().childrenOf(host), 5); i.hasNext(); ) {\n+                List<Node> childrenToMove = i.next();\n+                if ( ! addResourcesOf(childrenToMove, freeCapacity).satisfies(node.resources())) continue;\n+                List<Move> moves = move(childrenToMove, host, hosts, movesConsidered, movesMade);\n+                if (moves == null) continue;\n+\n+                if (shortest == null || moves.size() < shortest.size())\n+                    shortest = moves;\n+            }\n+            if (shortest == null) return null;\n+            return append(movesMade, shortest);\n+        }\n+\n+        private List<Move> move(List<Node> nodes, Node host, List<Node> hosts, List<Move> movesConsidered, List<Move> movesMade) {\n+            List<Move> moves = new ArrayList<>();\n+            for (Node childToMove : nodes) {\n+                List<Move> childMoves = move(childToMove, host, hosts, movesConsidered, append(movesMade, moves));\n+                if (childMoves == null) return null;\n+                moves.addAll(childMoves);\n+            }\n+            return moves;\n+        }\n+\n+        private List<Move> move(Node node, Node host, List<Node> hosts, List<Move> movesConsidered, List<Move> movesMade) {\n+            if (contains(node, movesConsidered)) return null;\n+            if (contains(node, movesMade)) return null;\n+            List<Move> shortest = null;\n+            for (Node target : hosts) {\n+                if (target.equals(host)) continue;\n+                Move move = new Move(node, host, target);\n+                List<Move> childMoves = makeRoomFor(node, target, hosts, append(movesConsidered, move), movesMade);\n+                if (childMoves == null) continue;\n+                if (shortest == null || shortest.size() > childMoves.size() + 1) {\n+                    shortest = new ArrayList<>(childMoves);\n+                    shortest.add(move);\n+                }\n+            }\n+            return shortest;\n+        }\n+\n+        private boolean contains(Node node, List<Move> moves) {\n+            return moves.stream().anyMatch(move -> move.node().equals(node));\n+        }\n+\n+        private NodeResources addResourcesOf(List<Node> nodes, NodeResources resources) {\n+            for (Node node : nodes)\n+                resources = resources.add(node.resources());\n+            return resources;\n+        }\n+\n+        private Iterator<List<Node>> subsets(NodeList nodes, int maxSize) {\n+            return new SubsetIterator(nodes.asList(), maxSize);\n+        }\n+\n+        private List<Move> append(List<Move> a, List<Move> b) {\n+            List<Move> list = new ArrayList<>();\n+            list.addAll(a);\n+            list.addAll(b);\n+            return list;\n+        }\n+\n+        private List<Move> append(List<Move> moves, Move move) {\n+            List<Move> list = new ArrayList<>(moves);\n+            list.add(move);\n+            return list;\n+        }\n+\n+        private NodeResources freeCapacityWith(List<Move> moves, Node host) {\n+            NodeResources resources = hostCapacity.freeCapacityOf(host);\n+            for (Move move : moves) {\n+                if ( ! move.toHost().equals(host)) continue;\n+                resources = resources.subtract(move.node().resources());\n+            }\n+            for (Move move : moves) {\n+                if ( ! move.fromHost().equals(host)) continue;\n+                resources = resources.add(move.fromHost().resources());\n+            }\n+            return resources;\n+        }\n+\n+    }\n+\n+    private static class SolutionKey {\n+\n+        private final Node node;\n+        private final Node host;\n+        private final List<Move> movesConsidered;\n+        private final List<Move> movesMade;\n+\n+        private final int hash;\n+\n+        public SolutionKey(Node node, Node host, List<Move> movesConsidered, List<Move> movesMade) {\n+            this.node = node;\n+            this.host = host;\n+            this.movesConsidered = movesConsidered;\n+            this.movesMade = movesMade;\n+\n+            hash = Objects.hash(node, host, movesConsidered, movesMade);\n+        }\n+\n+        @Override\n+        public int hashCode() { return hash; }\n+\n+        @Override\n+        public boolean equals(Object o) {\n+            if (o == this) return true;\n+            if (o == null || o.getClass() != this.getClass()) return false;\n+\n+            SolutionKey other = (SolutionKey)o;\n+            if ( ! other.node.equals(this.node)) return false;\n+            if ( ! other.host.equals(this.host)) return false;\n+            if ( ! other.movesConsidered.equals(this.movesConsidered)) return false;\n+            if ( ! other.movesMade.equals(this.movesMade)) return false;\n+            return true;\n+        }\n+\n+    }\n+\n+    private static class SubsetIterator implements Iterator<List<Node>> {\n+\n+        private final List<Node> nodes;\n+        private final int maxLength;\n+\n+        // A number whose binary representation determines which items of list we'll include\n+        private int i = 0; // first \"previous\" = 0 -> skip the empty set\n+        private List<Node> next = null;\n+\n+        public SubsetIterator(List<Node> nodes, int maxLength) {\n+            this.nodes = new ArrayList<>(nodes.subList(0, Math.min(nodes.size(), 31)));\n+            this.maxLength = maxLength;\n+        }\n+\n+        @Override\n+        public boolean hasNext() {\n+            if (next != null) return true;\n+\n+            // find next\n+            while (++i < 1<<nodes.size()) {\n+                int ones = onesIn(i);\n+                if (ones > maxLength) continue;\n+\n+                next = new ArrayList<>(ones);\n+                for (int position = 0; position < nodes.size(); position++) {\n+                    if (hasOneAtPosition(position, i))\n+                        next.add(nodes.get(position));\n+                }\n+                return true;\n+            }\n+            return false;\n+        }\n+\n+        @Override\n+        public List<Node> next() {\n+            if ( ! hasNext()) throw new IllegalStateException(\"No more elements\");\n+            var current = next;\n+            next = null;\n+            return current;\n+        }\n+\n+        private boolean hasOneAtPosition(int position, int number) {\n+            return (number & (1 << position)) > 0;\n+        }\n+\n+        private int onesIn(int number) {\n+            int ones = 0;\n+            for (int position = 0; Math.pow(2, position) <= number; position++) {\n+                if (hasOneAtPosition(position, number))\n+                    ones++;\n+            }\n+            return ones;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTcyNjM5NQ=="}, "originalCommit": {"oid": "28cf15bac333eadb4f3d133c9102c32bd0d72770"}, "originalPosition": 341}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1MjE5MzgxOnYy", "diffSide": "RIGHT", "path": "node-repository/src/test/java/com/yahoo/vespa/hosted/provision/maintenance/SpareCapacityMaintainerTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxODo0MDoxMVrOGlSW-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QyMTowODo1OFrOGlXgrQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTc1MTI4OA==", "bodyText": "Why is this needed?", "url": "https://github.com/vespa-engine/vespa/pull/13619#discussion_r441751288", "createdAt": "2020-06-17T18:40:11Z", "author": {"login": "freva"}, "path": "node-repository/src/test/java/com/yahoo/vespa/hosted/provision/maintenance/SpareCapacityMaintainerTest.java", "diffHunk": "@@ -0,0 +1,327 @@\n+// Copyright Verizon Media. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.\n+package com.yahoo.vespa.hosted.provision.maintenance;\n+\n+import com.yahoo.config.provision.ApplicationId;\n+import com.yahoo.config.provision.ClusterMembership;\n+import com.yahoo.config.provision.ClusterSpec;\n+import com.yahoo.config.provision.DockerImage;\n+import com.yahoo.config.provision.Environment;\n+import com.yahoo.config.provision.Flavor;\n+import com.yahoo.config.provision.NodeFlavors;\n+import com.yahoo.config.provision.NodeResources;\n+import com.yahoo.config.provision.NodeType;\n+import com.yahoo.config.provision.RegionName;\n+import com.yahoo.config.provision.Zone;\n+import com.yahoo.test.ManualClock;\n+import com.yahoo.transaction.NestedTransaction;\n+import com.yahoo.vespa.curator.mock.MockCurator;\n+import com.yahoo.vespa.hosted.provision.Node;\n+import com.yahoo.vespa.hosted.provision.NodeRepository;\n+import com.yahoo.vespa.hosted.provision.node.Agent;\n+import com.yahoo.vespa.hosted.provision.node.IP;\n+import com.yahoo.vespa.hosted.provision.provisioning.EmptyProvisionServiceProvider;\n+import com.yahoo.vespa.hosted.provision.provisioning.FlavorConfigBuilder;\n+import com.yahoo.vespa.hosted.provision.testutils.MockDeployer;\n+import com.yahoo.vespa.hosted.provision.testutils.MockNameResolver;\n+import org.junit.Ignore;\n+import org.junit.Test;\n+\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+/**\n+ * @author bratseth\n+ */\n+public class SpareCapacityMaintainerTest {\n+\n+    @Test\n+    public void testEmpty() {\n+        var tester = new SpareCapacityMaintainerTester();\n+        tester.maintainer.maintain();\n+        assertEquals(0, tester.deployer.redeployments);\n+        assertEquals(0, tester.nodeRepository.list().retired().size());\n+    }\n+\n+    @Test\n+    public void testOneSpare() {\n+        var tester = new SpareCapacityMaintainerTester();\n+        tester.addHosts(2, new NodeResources(10, 100, 1000, 1));\n+        tester.addNodes(0, 1, new NodeResources(10, 100, 1000, 1), 0);\n+        tester.maintainer.maintain();\n+        assertEquals(0, tester.deployer.redeployments);\n+        assertEquals(0, tester.nodeRepository.list().retired().size());\n+        assertEquals(1, tester.metric.values.get(\"spareHostCapacity\"));\n+    }\n+\n+    @Test\n+    public void testTwoSpares() {\n+        var tester = new SpareCapacityMaintainerTester();\n+        tester.addHosts(3, new NodeResources(10, 100, 1000, 1));\n+        tester.addNodes(0, 1, new NodeResources(10, 100, 1000, 1), 0);\n+        tester.maintainer.maintain();\n+        assertEquals(0, tester.deployer.redeployments);\n+        assertEquals(0, tester.nodeRepository.list().retired().size());\n+        assertEquals(2, tester.metric.values.get(\"spareHostCapacity\"));\n+    }\n+\n+    @Test\n+    public void testNoSpares() {\n+        var tester = new SpareCapacityMaintainerTester();\n+        tester.addHosts(2, new NodeResources(10, 100, 1000, 1));\n+        tester.addNodes(0, 2, new NodeResources(10, 100, 1000, 1), 0);\n+        tester.maintainer.maintain();\n+        assertEquals(0, tester.deployer.redeployments);\n+        assertEquals(0, tester.nodeRepository.list().retired().size());\n+        assertEquals(0, tester.metric.values.get(\"spareHostCapacity\"));\n+    }\n+\n+    @Test\n+    public void testAllWorksAsSpares() {\n+        var tester = new SpareCapacityMaintainerTester();\n+        tester.addHosts(4, new NodeResources(10, 100, 1000, 1));\n+        tester.addNodes(0, 2, new NodeResources(5, 50, 500, 0.5), 0);\n+        tester.addNodes(1, 2, new NodeResources(5, 50, 500, 0.5), 2);\n+        tester.maintainer.maintain();\n+        assertEquals(0, tester.deployer.redeployments);\n+        assertEquals(0, tester.nodeRepository.list().retired().size());\n+        assertEquals(2, tester.metric.values.get(\"spareHostCapacity\"));\n+    }\n+\n+    @Test\n+    public void testMoveIsNeeded() {\n+        // Moving application id 1 and 2 to the same nodes frees up spares for application 0\n+        var tester = new SpareCapacityMaintainerTester();\n+        tester.addHosts(6, new NodeResources(10, 100, 1000, 1));\n+        tester.addNodes(0, 2, new NodeResources(10, 100, 1000, 1), 0);\n+        tester.addNodes(1, 2, new NodeResources(5, 50, 500, 0.5), 2);\n+        tester.addNodes(2, 2, new NodeResources(5, 50, 500, 0.5), 4);\n+        tester.maintainer.maintain();\n+        assertEquals(1, tester.deployer.redeployments);\n+        assertEquals(1, tester.nodeRepository.list().retired().size());\n+        assertEquals(1, tester.metric.values.get(\"spareHostCapacity\"));\n+\n+        // Maintaining again is a no-op since the node to move is already retired\n+        tester.maintainer.maintain();\n+        assertEquals(1, tester.deployer.redeployments);\n+        assertEquals(1, tester.nodeRepository.list().retired().size());\n+        assertEquals(1, tester.metric.values.get(\"spareHostCapacity\"));\n+    }\n+\n+    @Test\n+    public void testMultipleMovesAreNeeded() {\n+        // Moving application id 1 and 2 to the same nodes frees up spares for application 0\n+        // so that it can be moved from size 12 to size 10 hosts, clearing up spare room for the size 12 application\n+        var tester = new SpareCapacityMaintainerTester();\n+        tester.addHosts(4, new NodeResources(12, 120, 1200, 1.2));\n+        tester.addHosts(4, new NodeResources(10, 100, 1000, 1));\n+        tester.addNodes(0, 2, new NodeResources(10, 100, 1000, 1.0), 0);\n+        tester.addNodes(1, 2, new NodeResources(12, 120, 1200, 1.2), 2);\n+        tester.addNodes(2, 2, new NodeResources(5, 50, 500, 0.5), 4);\n+        tester.addNodes(3, 2, new NodeResources(5, 50, 500, 0.5), 6);\n+        tester.maintainer.maintain();\n+        assertEquals(1, tester.deployer.redeployments);\n+        assertEquals(1, tester.nodeRepository.list().retired().size());\n+        assertEquals(1, tester.metric.values.get(\"spareHostCapacity\"));\n+    }\n+\n+    @Test\n+    public void testMultipleNodesMustMoveFromOneHost() {\n+        // By moving the 4 small nodes from host 2 we free up sufficient space on the third host to act as a spare for\n+        // application 0\n+        var tester = new SpareCapacityMaintainerTester();\n+        setupMultipleHosts(tester, 5);\n+\n+        tester.maintainer.maintain();\n+        assertEquals(1, tester.deployer.redeployments);\n+        assertEquals(1, tester.nodeRepository.list().retired().size());\n+        assertEquals(1, tester.metric.values.get(\"spareHostCapacity\"));\n+    }\n+\n+    @Test\n+    public void testMultipleNodesMustMoveFromOneHostButInsufficientCapacity() {\n+        var tester = new SpareCapacityMaintainerTester();\n+        setupMultipleHosts(tester, 4);\n+\n+        tester.maintainer.maintain();\n+        assertEquals(0, tester.deployer.redeployments);\n+        assertEquals(0, tester.nodeRepository.list().retired().size());\n+        assertEquals(0, tester.metric.values.get(\"spareHostCapacity\"));\n+    }\n+\n+    private void setupMultipleHosts(SpareCapacityMaintainerTester tester, int smallNodeCount) {\n+        tester.addHosts(2, new NodeResources(10, 100, 1000, 1));\n+        tester.addNodes(0, 2, new NodeResources(10, 100, 1000, 1.0), 0);\n+\n+        tester.addHosts(1, new NodeResources(16, 160, 1600, 1.6));\n+        tester.addNodes(1, 1, new NodeResources(1, 10, 100, 0.1), 2);\n+        tester.addNodes(2, 1, new NodeResources(1, 10, 100, 0.1), 2);\n+        tester.addNodes(3, 1, new NodeResources(1, 10, 100, 0.1), 2);\n+        tester.addNodes(4, 1, new NodeResources(1, 10, 100, 0.1), 2);\n+        tester.addNodes(5, 1, new NodeResources(2, 20, 200, 2.0), 2);\n+        tester.addNodes(6, 1, new NodeResources(2, 20, 200, 2.0), 2);\n+        tester.addNodes(7, 1, new NodeResources(2, 20, 200, 2.0), 2);\n+\n+        tester.addHosts(smallNodeCount, new NodeResources(2, 20, 200, 2.0));\n+    }\n+\n+    @Test\n+    public void testTooManyIterationsAreNeeded() {\n+        // 6 nodes must move to the next host, which is more than the max limit\n+        var tester = new SpareCapacityMaintainerTester(5);\n+\n+        tester.addHosts(2, new NodeResources(10, 100, 1000, 1));\n+        tester.addHosts(1, new NodeResources(9, 90, 900, 0.9));\n+        tester.addHosts(1, new NodeResources(8, 80, 800, 0.8));\n+        tester.addHosts(1, new NodeResources(7, 70, 700, 0.7));\n+        tester.addHosts(1, new NodeResources(6, 60, 600, 0.6));\n+        tester.addHosts(1, new NodeResources(5, 50, 500, 0.5));\n+        tester.addHosts(1, new NodeResources(4, 40, 400, 0.4));\n+\n+        tester.addNodes(0, 1, new NodeResources(10, 100, 1000, 1.0), 0);\n+        tester.addNodes(1, 1, new NodeResources( 9, 90, 900, 0.9), 1);\n+        tester.addNodes(2, 1, new NodeResources( 8, 80, 800, 0.8), 2);\n+        tester.addNodes(3, 1, new NodeResources( 7, 70, 700, 0.7), 3);\n+        tester.addNodes(4, 1, new NodeResources( 6, 60, 600, 0.6), 4);\n+        tester.addNodes(5, 1, new NodeResources( 5, 50, 500, 0.5), 5);\n+        tester.addNodes(6, 1, new NodeResources( 4, 40, 400, 0.4), 6);\n+\n+        tester.maintainer.maintain();\n+        assertEquals(0, tester.deployer.redeployments);\n+        assertEquals(0, tester.nodeRepository.list().retired().size());\n+        assertEquals(0, tester.metric.values.get(\"spareHostCapacity\"));\n+    }\n+\n+    /** Microbenchmark */\n+    @Test\n+    @Ignore\n+    public void testLargeNodeRepo() {\n+        // Completely fill 200 hosts with 2000 nodes\n+        int hosts = 200;\n+        var tester = new SpareCapacityMaintainerTester();\n+        tester.addHosts(hosts, new NodeResources(100, 1000, 10000, 10));\n+        int hostOffset = 0;\n+        for (int i = 0; i < 200; i++) {\n+            int applicationSize = 10;\n+            int resourceSize = 10;\n+            tester.addNodes(i, applicationSize, new NodeResources(resourceSize, resourceSize * 10, resourceSize * 100, 0.1), hostOffset);\n+            hostOffset = (hostOffset + applicationSize) % hosts;\n+        }\n+        long startTime = System.currentTimeMillis();\n+        tester.maintainer.maintain();\n+        long totalTime = System.currentTimeMillis() - startTime;\n+        System.out.println(\"Complete in \" + ( totalTime / 1000) + \" seconds\");\n+        assertEquals(0, tester.deployer.redeployments);\n+        assertEquals(0, tester.nodeRepository.list().retired().size());\n+        assertEquals(0, tester.metric.values.get(\"spareHostCapacity\"));\n+    }\n+\n+    private static class SpareCapacityMaintainerTester {\n+\n+        NodeRepository nodeRepository;\n+        MockDeployer deployer;\n+        TestMetric metric = new TestMetric();\n+        SpareCapacityMaintainer maintainer;\n+        private int hostIndex = 0;\n+        private int nodeIndex = 0;\n+\n+        private SpareCapacityMaintainerTester() {\n+            this(1000);\n+        }\n+\n+        private SpareCapacityMaintainerTester(int maxIterations) {\n+            NodeFlavors flavors = new NodeFlavors(new FlavorConfigBuilder().build());\n+            nodeRepository = new NodeRepository(flavors,\n+                                                new EmptyProvisionServiceProvider().getHostResourcesCalculator(),\n+                                                new MockCurator(),\n+                                                new ManualClock(),\n+                                                new Zone(Environment.prod, RegionName.from(\"us-east-3\")),\n+                                                new MockNameResolver().mockAnyLookup(),\n+                                                DockerImage.fromString(\"docker-registry.domain.tld:8080/dist/vespa\"), true, false);\n+            deployer = new MockDeployer(nodeRepository);\n+            maintainer = new SpareCapacityMaintainer(deployer, nodeRepository, metric, Duration.ofDays(1), maxIterations);\n+        }\n+\n+        private void addHosts(int count, NodeResources resources) {\n+            List<Node> hosts = new ArrayList<>();\n+            for (int i = 0; i < count; i++) {\n+                Node host = nodeRepository.createNode(\"host\" + hostIndex,\n+                                                      \"host\" + hostIndex + \".yahoo.com\",\n+                                                      ipConfig(hostIndex + nodeIndex, true),\n+                                                      Optional.empty(),\n+                                                      new Flavor(resources),\n+                                                      Optional.empty(),\n+                                                      NodeType.host);\n+                hosts.add(host);\n+                hostIndex++;\n+            }\n+            hosts = nodeRepository.addNodes(hosts, Agent.system);\n+            hosts = nodeRepository.setReady(hosts, Agent.system, \"Test\");\n+            var transaction = new NestedTransaction();\n+            nodeRepository.activate(hosts, transaction);\n+            transaction.commit();\n+        }\n+\n+        private void addNodes(int id, int count, NodeResources resources, int hostOffset) {\n+            List<Node> nodes = new ArrayList<>();\n+            ApplicationId application = ApplicationId.from(\"tenant\" + id, \"application\" + id, \"default\");\n+            for (int i = 0; i < count; i++) {\n+                ClusterMembership membership = ClusterMembership.from(ClusterSpec.specification(ClusterSpec.Type.content, ClusterSpec.Id.from(\"cluster\" + id))\n+                                                                                 .group(ClusterSpec.Group.from(0))\n+                                                                                 .vespaVersion(\"7\")\n+                                                                                 .build(),\n+                                                                      i);\n+                Node node = nodeRepository.createNode(\"node\" + nodeIndex,\n+                                                      \"node\" + nodeIndex + \".yahoo.com\",\n+                                                      ipConfig(hostIndex + nodeIndex, false),\n+                                                      Optional.of(\"host\" + ( hostOffset + i) + \".yahoo.com\"),\n+                                                      new Flavor(resources),\n+                                                      Optional.empty(),\n+                                                      NodeType.tenant);\n+                node = node.allocate(application, membership, node.resources(), Instant.now());\n+                nodes.add(node);\n+                nodeIndex++;\n+            }\n+            nodes = nodeRepository.addNodes(nodes, Agent.system);\n+            for (int i = 0; i < count; i++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "28cf15bac333eadb4f3d133c9102c32bd0d72770"}, "originalPosition": 293}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTgzNTY5Mw==", "bodyText": "The second loop? This whole thing is in lieu of allocating properly, because here I want full control over where nodes are placed, without being dependent on constraints applied during allocation. The second loop is needed because setting nodes ready will remove the allocation if I set it at creation.", "url": "https://github.com/vespa-engine/vespa/pull/13619#discussion_r441835693", "createdAt": "2020-06-17T21:08:58Z", "author": {"login": "bratseth"}, "path": "node-repository/src/test/java/com/yahoo/vespa/hosted/provision/maintenance/SpareCapacityMaintainerTest.java", "diffHunk": "@@ -0,0 +1,327 @@\n+// Copyright Verizon Media. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.\n+package com.yahoo.vespa.hosted.provision.maintenance;\n+\n+import com.yahoo.config.provision.ApplicationId;\n+import com.yahoo.config.provision.ClusterMembership;\n+import com.yahoo.config.provision.ClusterSpec;\n+import com.yahoo.config.provision.DockerImage;\n+import com.yahoo.config.provision.Environment;\n+import com.yahoo.config.provision.Flavor;\n+import com.yahoo.config.provision.NodeFlavors;\n+import com.yahoo.config.provision.NodeResources;\n+import com.yahoo.config.provision.NodeType;\n+import com.yahoo.config.provision.RegionName;\n+import com.yahoo.config.provision.Zone;\n+import com.yahoo.test.ManualClock;\n+import com.yahoo.transaction.NestedTransaction;\n+import com.yahoo.vespa.curator.mock.MockCurator;\n+import com.yahoo.vespa.hosted.provision.Node;\n+import com.yahoo.vespa.hosted.provision.NodeRepository;\n+import com.yahoo.vespa.hosted.provision.node.Agent;\n+import com.yahoo.vespa.hosted.provision.node.IP;\n+import com.yahoo.vespa.hosted.provision.provisioning.EmptyProvisionServiceProvider;\n+import com.yahoo.vespa.hosted.provision.provisioning.FlavorConfigBuilder;\n+import com.yahoo.vespa.hosted.provision.testutils.MockDeployer;\n+import com.yahoo.vespa.hosted.provision.testutils.MockNameResolver;\n+import org.junit.Ignore;\n+import org.junit.Test;\n+\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+/**\n+ * @author bratseth\n+ */\n+public class SpareCapacityMaintainerTest {\n+\n+    @Test\n+    public void testEmpty() {\n+        var tester = new SpareCapacityMaintainerTester();\n+        tester.maintainer.maintain();\n+        assertEquals(0, tester.deployer.redeployments);\n+        assertEquals(0, tester.nodeRepository.list().retired().size());\n+    }\n+\n+    @Test\n+    public void testOneSpare() {\n+        var tester = new SpareCapacityMaintainerTester();\n+        tester.addHosts(2, new NodeResources(10, 100, 1000, 1));\n+        tester.addNodes(0, 1, new NodeResources(10, 100, 1000, 1), 0);\n+        tester.maintainer.maintain();\n+        assertEquals(0, tester.deployer.redeployments);\n+        assertEquals(0, tester.nodeRepository.list().retired().size());\n+        assertEquals(1, tester.metric.values.get(\"spareHostCapacity\"));\n+    }\n+\n+    @Test\n+    public void testTwoSpares() {\n+        var tester = new SpareCapacityMaintainerTester();\n+        tester.addHosts(3, new NodeResources(10, 100, 1000, 1));\n+        tester.addNodes(0, 1, new NodeResources(10, 100, 1000, 1), 0);\n+        tester.maintainer.maintain();\n+        assertEquals(0, tester.deployer.redeployments);\n+        assertEquals(0, tester.nodeRepository.list().retired().size());\n+        assertEquals(2, tester.metric.values.get(\"spareHostCapacity\"));\n+    }\n+\n+    @Test\n+    public void testNoSpares() {\n+        var tester = new SpareCapacityMaintainerTester();\n+        tester.addHosts(2, new NodeResources(10, 100, 1000, 1));\n+        tester.addNodes(0, 2, new NodeResources(10, 100, 1000, 1), 0);\n+        tester.maintainer.maintain();\n+        assertEquals(0, tester.deployer.redeployments);\n+        assertEquals(0, tester.nodeRepository.list().retired().size());\n+        assertEquals(0, tester.metric.values.get(\"spareHostCapacity\"));\n+    }\n+\n+    @Test\n+    public void testAllWorksAsSpares() {\n+        var tester = new SpareCapacityMaintainerTester();\n+        tester.addHosts(4, new NodeResources(10, 100, 1000, 1));\n+        tester.addNodes(0, 2, new NodeResources(5, 50, 500, 0.5), 0);\n+        tester.addNodes(1, 2, new NodeResources(5, 50, 500, 0.5), 2);\n+        tester.maintainer.maintain();\n+        assertEquals(0, tester.deployer.redeployments);\n+        assertEquals(0, tester.nodeRepository.list().retired().size());\n+        assertEquals(2, tester.metric.values.get(\"spareHostCapacity\"));\n+    }\n+\n+    @Test\n+    public void testMoveIsNeeded() {\n+        // Moving application id 1 and 2 to the same nodes frees up spares for application 0\n+        var tester = new SpareCapacityMaintainerTester();\n+        tester.addHosts(6, new NodeResources(10, 100, 1000, 1));\n+        tester.addNodes(0, 2, new NodeResources(10, 100, 1000, 1), 0);\n+        tester.addNodes(1, 2, new NodeResources(5, 50, 500, 0.5), 2);\n+        tester.addNodes(2, 2, new NodeResources(5, 50, 500, 0.5), 4);\n+        tester.maintainer.maintain();\n+        assertEquals(1, tester.deployer.redeployments);\n+        assertEquals(1, tester.nodeRepository.list().retired().size());\n+        assertEquals(1, tester.metric.values.get(\"spareHostCapacity\"));\n+\n+        // Maintaining again is a no-op since the node to move is already retired\n+        tester.maintainer.maintain();\n+        assertEquals(1, tester.deployer.redeployments);\n+        assertEquals(1, tester.nodeRepository.list().retired().size());\n+        assertEquals(1, tester.metric.values.get(\"spareHostCapacity\"));\n+    }\n+\n+    @Test\n+    public void testMultipleMovesAreNeeded() {\n+        // Moving application id 1 and 2 to the same nodes frees up spares for application 0\n+        // so that it can be moved from size 12 to size 10 hosts, clearing up spare room for the size 12 application\n+        var tester = new SpareCapacityMaintainerTester();\n+        tester.addHosts(4, new NodeResources(12, 120, 1200, 1.2));\n+        tester.addHosts(4, new NodeResources(10, 100, 1000, 1));\n+        tester.addNodes(0, 2, new NodeResources(10, 100, 1000, 1.0), 0);\n+        tester.addNodes(1, 2, new NodeResources(12, 120, 1200, 1.2), 2);\n+        tester.addNodes(2, 2, new NodeResources(5, 50, 500, 0.5), 4);\n+        tester.addNodes(3, 2, new NodeResources(5, 50, 500, 0.5), 6);\n+        tester.maintainer.maintain();\n+        assertEquals(1, tester.deployer.redeployments);\n+        assertEquals(1, tester.nodeRepository.list().retired().size());\n+        assertEquals(1, tester.metric.values.get(\"spareHostCapacity\"));\n+    }\n+\n+    @Test\n+    public void testMultipleNodesMustMoveFromOneHost() {\n+        // By moving the 4 small nodes from host 2 we free up sufficient space on the third host to act as a spare for\n+        // application 0\n+        var tester = new SpareCapacityMaintainerTester();\n+        setupMultipleHosts(tester, 5);\n+\n+        tester.maintainer.maintain();\n+        assertEquals(1, tester.deployer.redeployments);\n+        assertEquals(1, tester.nodeRepository.list().retired().size());\n+        assertEquals(1, tester.metric.values.get(\"spareHostCapacity\"));\n+    }\n+\n+    @Test\n+    public void testMultipleNodesMustMoveFromOneHostButInsufficientCapacity() {\n+        var tester = new SpareCapacityMaintainerTester();\n+        setupMultipleHosts(tester, 4);\n+\n+        tester.maintainer.maintain();\n+        assertEquals(0, tester.deployer.redeployments);\n+        assertEquals(0, tester.nodeRepository.list().retired().size());\n+        assertEquals(0, tester.metric.values.get(\"spareHostCapacity\"));\n+    }\n+\n+    private void setupMultipleHosts(SpareCapacityMaintainerTester tester, int smallNodeCount) {\n+        tester.addHosts(2, new NodeResources(10, 100, 1000, 1));\n+        tester.addNodes(0, 2, new NodeResources(10, 100, 1000, 1.0), 0);\n+\n+        tester.addHosts(1, new NodeResources(16, 160, 1600, 1.6));\n+        tester.addNodes(1, 1, new NodeResources(1, 10, 100, 0.1), 2);\n+        tester.addNodes(2, 1, new NodeResources(1, 10, 100, 0.1), 2);\n+        tester.addNodes(3, 1, new NodeResources(1, 10, 100, 0.1), 2);\n+        tester.addNodes(4, 1, new NodeResources(1, 10, 100, 0.1), 2);\n+        tester.addNodes(5, 1, new NodeResources(2, 20, 200, 2.0), 2);\n+        tester.addNodes(6, 1, new NodeResources(2, 20, 200, 2.0), 2);\n+        tester.addNodes(7, 1, new NodeResources(2, 20, 200, 2.0), 2);\n+\n+        tester.addHosts(smallNodeCount, new NodeResources(2, 20, 200, 2.0));\n+    }\n+\n+    @Test\n+    public void testTooManyIterationsAreNeeded() {\n+        // 6 nodes must move to the next host, which is more than the max limit\n+        var tester = new SpareCapacityMaintainerTester(5);\n+\n+        tester.addHosts(2, new NodeResources(10, 100, 1000, 1));\n+        tester.addHosts(1, new NodeResources(9, 90, 900, 0.9));\n+        tester.addHosts(1, new NodeResources(8, 80, 800, 0.8));\n+        tester.addHosts(1, new NodeResources(7, 70, 700, 0.7));\n+        tester.addHosts(1, new NodeResources(6, 60, 600, 0.6));\n+        tester.addHosts(1, new NodeResources(5, 50, 500, 0.5));\n+        tester.addHosts(1, new NodeResources(4, 40, 400, 0.4));\n+\n+        tester.addNodes(0, 1, new NodeResources(10, 100, 1000, 1.0), 0);\n+        tester.addNodes(1, 1, new NodeResources( 9, 90, 900, 0.9), 1);\n+        tester.addNodes(2, 1, new NodeResources( 8, 80, 800, 0.8), 2);\n+        tester.addNodes(3, 1, new NodeResources( 7, 70, 700, 0.7), 3);\n+        tester.addNodes(4, 1, new NodeResources( 6, 60, 600, 0.6), 4);\n+        tester.addNodes(5, 1, new NodeResources( 5, 50, 500, 0.5), 5);\n+        tester.addNodes(6, 1, new NodeResources( 4, 40, 400, 0.4), 6);\n+\n+        tester.maintainer.maintain();\n+        assertEquals(0, tester.deployer.redeployments);\n+        assertEquals(0, tester.nodeRepository.list().retired().size());\n+        assertEquals(0, tester.metric.values.get(\"spareHostCapacity\"));\n+    }\n+\n+    /** Microbenchmark */\n+    @Test\n+    @Ignore\n+    public void testLargeNodeRepo() {\n+        // Completely fill 200 hosts with 2000 nodes\n+        int hosts = 200;\n+        var tester = new SpareCapacityMaintainerTester();\n+        tester.addHosts(hosts, new NodeResources(100, 1000, 10000, 10));\n+        int hostOffset = 0;\n+        for (int i = 0; i < 200; i++) {\n+            int applicationSize = 10;\n+            int resourceSize = 10;\n+            tester.addNodes(i, applicationSize, new NodeResources(resourceSize, resourceSize * 10, resourceSize * 100, 0.1), hostOffset);\n+            hostOffset = (hostOffset + applicationSize) % hosts;\n+        }\n+        long startTime = System.currentTimeMillis();\n+        tester.maintainer.maintain();\n+        long totalTime = System.currentTimeMillis() - startTime;\n+        System.out.println(\"Complete in \" + ( totalTime / 1000) + \" seconds\");\n+        assertEquals(0, tester.deployer.redeployments);\n+        assertEquals(0, tester.nodeRepository.list().retired().size());\n+        assertEquals(0, tester.metric.values.get(\"spareHostCapacity\"));\n+    }\n+\n+    private static class SpareCapacityMaintainerTester {\n+\n+        NodeRepository nodeRepository;\n+        MockDeployer deployer;\n+        TestMetric metric = new TestMetric();\n+        SpareCapacityMaintainer maintainer;\n+        private int hostIndex = 0;\n+        private int nodeIndex = 0;\n+\n+        private SpareCapacityMaintainerTester() {\n+            this(1000);\n+        }\n+\n+        private SpareCapacityMaintainerTester(int maxIterations) {\n+            NodeFlavors flavors = new NodeFlavors(new FlavorConfigBuilder().build());\n+            nodeRepository = new NodeRepository(flavors,\n+                                                new EmptyProvisionServiceProvider().getHostResourcesCalculator(),\n+                                                new MockCurator(),\n+                                                new ManualClock(),\n+                                                new Zone(Environment.prod, RegionName.from(\"us-east-3\")),\n+                                                new MockNameResolver().mockAnyLookup(),\n+                                                DockerImage.fromString(\"docker-registry.domain.tld:8080/dist/vespa\"), true, false);\n+            deployer = new MockDeployer(nodeRepository);\n+            maintainer = new SpareCapacityMaintainer(deployer, nodeRepository, metric, Duration.ofDays(1), maxIterations);\n+        }\n+\n+        private void addHosts(int count, NodeResources resources) {\n+            List<Node> hosts = new ArrayList<>();\n+            for (int i = 0; i < count; i++) {\n+                Node host = nodeRepository.createNode(\"host\" + hostIndex,\n+                                                      \"host\" + hostIndex + \".yahoo.com\",\n+                                                      ipConfig(hostIndex + nodeIndex, true),\n+                                                      Optional.empty(),\n+                                                      new Flavor(resources),\n+                                                      Optional.empty(),\n+                                                      NodeType.host);\n+                hosts.add(host);\n+                hostIndex++;\n+            }\n+            hosts = nodeRepository.addNodes(hosts, Agent.system);\n+            hosts = nodeRepository.setReady(hosts, Agent.system, \"Test\");\n+            var transaction = new NestedTransaction();\n+            nodeRepository.activate(hosts, transaction);\n+            transaction.commit();\n+        }\n+\n+        private void addNodes(int id, int count, NodeResources resources, int hostOffset) {\n+            List<Node> nodes = new ArrayList<>();\n+            ApplicationId application = ApplicationId.from(\"tenant\" + id, \"application\" + id, \"default\");\n+            for (int i = 0; i < count; i++) {\n+                ClusterMembership membership = ClusterMembership.from(ClusterSpec.specification(ClusterSpec.Type.content, ClusterSpec.Id.from(\"cluster\" + id))\n+                                                                                 .group(ClusterSpec.Group.from(0))\n+                                                                                 .vespaVersion(\"7\")\n+                                                                                 .build(),\n+                                                                      i);\n+                Node node = nodeRepository.createNode(\"node\" + nodeIndex,\n+                                                      \"node\" + nodeIndex + \".yahoo.com\",\n+                                                      ipConfig(hostIndex + nodeIndex, false),\n+                                                      Optional.of(\"host\" + ( hostOffset + i) + \".yahoo.com\"),\n+                                                      new Flavor(resources),\n+                                                      Optional.empty(),\n+                                                      NodeType.tenant);\n+                node = node.allocate(application, membership, node.resources(), Instant.now());\n+                nodes.add(node);\n+                nodeIndex++;\n+            }\n+            nodes = nodeRepository.addNodes(nodes, Agent.system);\n+            for (int i = 0; i < count; i++) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTc1MTI4OA=="}, "originalCommit": {"oid": "28cf15bac333eadb4f3d133c9102c32bd0d72770"}, "originalPosition": 293}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1MjI2NDEwOnYy", "diffSide": "RIGHT", "path": "node-repository/src/main/java/com/yahoo/vespa/hosted/provision/maintenance/SpareCapacityMaintainer.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxODo1NToxMVrOGlTGfg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQwNjoyMTozM1rOGlhGzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTc2MzQ1NA==", "bodyText": "It's not obvious to me that this return null/empty list always ends being correct, why not always return non-null?\nAlso, add a check here to ignore empty mitigation if we already have a shortestMitigation candidate, after all doing something that takes longer should be better than doing nothing?", "url": "https://github.com/vespa-engine/vespa/pull/13619#discussion_r441763454", "createdAt": "2020-06-17T18:55:11Z", "author": {"login": "freva"}, "path": "node-repository/src/main/java/com/yahoo/vespa/hosted/provision/maintenance/SpareCapacityMaintainer.java", "diffHunk": "@@ -0,0 +1,346 @@\n+// Copyright Verizon Media. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.\n+package com.yahoo.vespa.hosted.provision.maintenance;\n+\n+import com.yahoo.config.provision.Deployer;\n+import com.yahoo.config.provision.NodeResources;\n+import com.yahoo.jdisc.Metric;\n+import com.yahoo.vespa.hosted.provision.Node;\n+import com.yahoo.vespa.hosted.provision.NodeList;\n+import com.yahoo.vespa.hosted.provision.NodeRepository;\n+import com.yahoo.vespa.hosted.provision.maintenance.MaintenanceDeployment.Move;\n+import com.yahoo.vespa.hosted.provision.node.Agent;\n+import com.yahoo.vespa.hosted.provision.provisioning.HostCapacity;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.logging.Level;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * A maintainer which attempts to ensure there is spare capacity available in chunks which can fit\n+ * all node resource configuration in use, such that the system is able to quickly replace a failed node\n+ * if necessary.\n+ *\n+ * This also emits the following metrics:\n+ * - Overcommitted hosts: Hosts whose capacity is less than the sum of its children's\n+ * - Spare host capacity, or how many hosts the repository can stand to lose without ending up in a situation where it's\n+ *   unable to find a new home for orphaned tenants.\n+ *\n+ * @author mgimle\n+ * @author bratseth\n+ */\n+public class SpareCapacityMaintainer extends NodeRepositoryMaintainer {\n+\n+    private final int maxIterations;\n+    private final Deployer deployer;\n+    private final Metric metric;\n+\n+    public SpareCapacityMaintainer(Deployer deployer,\n+                                   NodeRepository nodeRepository,\n+                                   Metric metric,\n+                                   Duration interval) {\n+        this(deployer, nodeRepository, metric, interval,\n+             10_000 // Should take less than a few minutes\n+        );\n+    }\n+\n+    public SpareCapacityMaintainer(Deployer deployer,\n+                                   NodeRepository nodeRepository,\n+                                   Metric metric,\n+                                   Duration interval,\n+                                   int maxIterations) {\n+        super(nodeRepository, interval);\n+        this.deployer = deployer;\n+        this.metric = metric;\n+        this.maxIterations = maxIterations;\n+    }\n+\n+    @Override\n+    protected void maintain() {\n+        if ( ! nodeRepository().zone().getCloud().allowHostSharing()) return;\n+\n+        CapacityChecker capacityChecker = new CapacityChecker(nodeRepository());\n+\n+        List<Node> overcommittedHosts = capacityChecker.findOvercommittedHosts();\n+        if (overcommittedHosts.size() != 0) {\n+            log.log(Level.WARNING, String.format(\"%d nodes are overcommitted! [ %s ]\",\n+                                                 overcommittedHosts.size(),\n+                                                 overcommittedHosts.stream().map(Node::hostname).collect(Collectors.joining(\", \"))));\n+        }\n+        metric.set(\"overcommittedHosts\", overcommittedHosts.size(), null);\n+\n+        Optional<CapacityChecker.HostFailurePath> failurePath = capacityChecker.worstCaseHostLossLeadingToFailure();\n+        if (failurePath.isPresent()) {\n+            int spareHostCapacity = failurePath.get().hostsCausingFailure.size() - 1;\n+            if (spareHostCapacity == 0) {\n+                Move move = findMitigatingMove(failurePath.get());\n+                if (moving(move)) {\n+                    // We succeeded or are in the process of taking a step to mitigate.\n+                    // Report with the assumption this will eventually succeed to avoid alerting before we're stuck\n+                    spareHostCapacity++;\n+                }\n+            }\n+            metric.set(\"spareHostCapacity\", spareHostCapacity, null);\n+        }\n+    }\n+\n+    private boolean moving(Move move) {\n+        if (move.isEmpty()) return false;\n+        if (move.node().allocation().get().membership().retired()) return true; // Move already in progress\n+        return move.execute(false, Agent.SpareCapacityMaintainer, deployer, metric, nodeRepository());\n+    }\n+\n+    private Move findMitigatingMove(CapacityChecker.HostFailurePath failurePath) {\n+        Optional<Node> nodeWhichCantMove = failurePath.failureReason.tenant;\n+        if (nodeWhichCantMove.isEmpty()) return Move.empty();\n+\n+        Node node = nodeWhichCantMove.get();\n+        NodeList allNodes = nodeRepository().list();\n+        // Allocation will assign the two most empty nodes as \"spares\", which will not be allocated on\n+        // unless needed for node failing. Our goal here is to make room on these spares for the given node\n+        HostCapacity hostCapacity = new HostCapacity(allNodes, nodeRepository().resourcesCalculator());\n+        Set<Node> spareHosts = hostCapacity.findSpareHosts(allNodes.hosts().satisfies(node.resources()).asList(), 2);\n+        List<Node> hosts = allNodes.hosts().except(spareHosts).asList();\n+\n+        CapacitySolver capacitySolver = new CapacitySolver(hostCapacity, maxIterations);\n+        List<Move> shortestMitigation = null;\n+        for (Node spareHost : spareHosts) {\n+            List<Move> mitigation = capacitySolver.makeRoomFor(node, spareHost, hosts, List.of(), List.of());\n+            if (mitigation == null) continue;\n+            if (shortestMitigation == null || shortestMitigation.size() > mitigation.size())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "28cf15bac333eadb4f3d133c9102c32bd0d72770"}, "originalPosition": 117}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTgzNzA5Ng==", "bodyText": "Empty list means the solution needs no moves.\nnull means there are no solutions.\nI don't understand the second paragraph - if it's null we continue because we found no solution. Otherwise we compare it to the current best, if any.", "url": "https://github.com/vespa-engine/vespa/pull/13619#discussion_r441837096", "createdAt": "2020-06-17T21:11:58Z", "author": {"login": "bratseth"}, "path": "node-repository/src/main/java/com/yahoo/vespa/hosted/provision/maintenance/SpareCapacityMaintainer.java", "diffHunk": "@@ -0,0 +1,346 @@\n+// Copyright Verizon Media. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.\n+package com.yahoo.vespa.hosted.provision.maintenance;\n+\n+import com.yahoo.config.provision.Deployer;\n+import com.yahoo.config.provision.NodeResources;\n+import com.yahoo.jdisc.Metric;\n+import com.yahoo.vespa.hosted.provision.Node;\n+import com.yahoo.vespa.hosted.provision.NodeList;\n+import com.yahoo.vespa.hosted.provision.NodeRepository;\n+import com.yahoo.vespa.hosted.provision.maintenance.MaintenanceDeployment.Move;\n+import com.yahoo.vespa.hosted.provision.node.Agent;\n+import com.yahoo.vespa.hosted.provision.provisioning.HostCapacity;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.logging.Level;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * A maintainer which attempts to ensure there is spare capacity available in chunks which can fit\n+ * all node resource configuration in use, such that the system is able to quickly replace a failed node\n+ * if necessary.\n+ *\n+ * This also emits the following metrics:\n+ * - Overcommitted hosts: Hosts whose capacity is less than the sum of its children's\n+ * - Spare host capacity, or how many hosts the repository can stand to lose without ending up in a situation where it's\n+ *   unable to find a new home for orphaned tenants.\n+ *\n+ * @author mgimle\n+ * @author bratseth\n+ */\n+public class SpareCapacityMaintainer extends NodeRepositoryMaintainer {\n+\n+    private final int maxIterations;\n+    private final Deployer deployer;\n+    private final Metric metric;\n+\n+    public SpareCapacityMaintainer(Deployer deployer,\n+                                   NodeRepository nodeRepository,\n+                                   Metric metric,\n+                                   Duration interval) {\n+        this(deployer, nodeRepository, metric, interval,\n+             10_000 // Should take less than a few minutes\n+        );\n+    }\n+\n+    public SpareCapacityMaintainer(Deployer deployer,\n+                                   NodeRepository nodeRepository,\n+                                   Metric metric,\n+                                   Duration interval,\n+                                   int maxIterations) {\n+        super(nodeRepository, interval);\n+        this.deployer = deployer;\n+        this.metric = metric;\n+        this.maxIterations = maxIterations;\n+    }\n+\n+    @Override\n+    protected void maintain() {\n+        if ( ! nodeRepository().zone().getCloud().allowHostSharing()) return;\n+\n+        CapacityChecker capacityChecker = new CapacityChecker(nodeRepository());\n+\n+        List<Node> overcommittedHosts = capacityChecker.findOvercommittedHosts();\n+        if (overcommittedHosts.size() != 0) {\n+            log.log(Level.WARNING, String.format(\"%d nodes are overcommitted! [ %s ]\",\n+                                                 overcommittedHosts.size(),\n+                                                 overcommittedHosts.stream().map(Node::hostname).collect(Collectors.joining(\", \"))));\n+        }\n+        metric.set(\"overcommittedHosts\", overcommittedHosts.size(), null);\n+\n+        Optional<CapacityChecker.HostFailurePath> failurePath = capacityChecker.worstCaseHostLossLeadingToFailure();\n+        if (failurePath.isPresent()) {\n+            int spareHostCapacity = failurePath.get().hostsCausingFailure.size() - 1;\n+            if (spareHostCapacity == 0) {\n+                Move move = findMitigatingMove(failurePath.get());\n+                if (moving(move)) {\n+                    // We succeeded or are in the process of taking a step to mitigate.\n+                    // Report with the assumption this will eventually succeed to avoid alerting before we're stuck\n+                    spareHostCapacity++;\n+                }\n+            }\n+            metric.set(\"spareHostCapacity\", spareHostCapacity, null);\n+        }\n+    }\n+\n+    private boolean moving(Move move) {\n+        if (move.isEmpty()) return false;\n+        if (move.node().allocation().get().membership().retired()) return true; // Move already in progress\n+        return move.execute(false, Agent.SpareCapacityMaintainer, deployer, metric, nodeRepository());\n+    }\n+\n+    private Move findMitigatingMove(CapacityChecker.HostFailurePath failurePath) {\n+        Optional<Node> nodeWhichCantMove = failurePath.failureReason.tenant;\n+        if (nodeWhichCantMove.isEmpty()) return Move.empty();\n+\n+        Node node = nodeWhichCantMove.get();\n+        NodeList allNodes = nodeRepository().list();\n+        // Allocation will assign the two most empty nodes as \"spares\", which will not be allocated on\n+        // unless needed for node failing. Our goal here is to make room on these spares for the given node\n+        HostCapacity hostCapacity = new HostCapacity(allNodes, nodeRepository().resourcesCalculator());\n+        Set<Node> spareHosts = hostCapacity.findSpareHosts(allNodes.hosts().satisfies(node.resources()).asList(), 2);\n+        List<Node> hosts = allNodes.hosts().except(spareHosts).asList();\n+\n+        CapacitySolver capacitySolver = new CapacitySolver(hostCapacity, maxIterations);\n+        List<Move> shortestMitigation = null;\n+        for (Node spareHost : spareHosts) {\n+            List<Move> mitigation = capacitySolver.makeRoomFor(node, spareHost, hosts, List.of(), List.of());\n+            if (mitigation == null) continue;\n+            if (shortestMitigation == null || shortestMitigation.size() > mitigation.size())", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTc2MzQ1NA=="}, "originalCommit": {"oid": "28cf15bac333eadb4f3d133c9102c32bd0d72770"}, "originalPosition": 117}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTg0MDY1Ng==", "bodyText": "If makeRoomFor() returns empty list, it will be set as the shortestMitigation since shortestMitigation.size() > mitigation.size() == shortestMitigation.size() > 0, but then 2 lines below, if shortestMitigation.isEmpty() we return Move.empty(). So wouldn't it be better to change this if to\nif (shortestMitigation == null || (shortestMitigation.size() > mitigation.size() && !mitigation.isEmpty()))\n\n?", "url": "https://github.com/vespa-engine/vespa/pull/13619#discussion_r441840656", "createdAt": "2020-06-17T21:19:46Z", "author": {"login": "freva"}, "path": "node-repository/src/main/java/com/yahoo/vespa/hosted/provision/maintenance/SpareCapacityMaintainer.java", "diffHunk": "@@ -0,0 +1,346 @@\n+// Copyright Verizon Media. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.\n+package com.yahoo.vespa.hosted.provision.maintenance;\n+\n+import com.yahoo.config.provision.Deployer;\n+import com.yahoo.config.provision.NodeResources;\n+import com.yahoo.jdisc.Metric;\n+import com.yahoo.vespa.hosted.provision.Node;\n+import com.yahoo.vespa.hosted.provision.NodeList;\n+import com.yahoo.vespa.hosted.provision.NodeRepository;\n+import com.yahoo.vespa.hosted.provision.maintenance.MaintenanceDeployment.Move;\n+import com.yahoo.vespa.hosted.provision.node.Agent;\n+import com.yahoo.vespa.hosted.provision.provisioning.HostCapacity;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.logging.Level;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * A maintainer which attempts to ensure there is spare capacity available in chunks which can fit\n+ * all node resource configuration in use, such that the system is able to quickly replace a failed node\n+ * if necessary.\n+ *\n+ * This also emits the following metrics:\n+ * - Overcommitted hosts: Hosts whose capacity is less than the sum of its children's\n+ * - Spare host capacity, or how many hosts the repository can stand to lose without ending up in a situation where it's\n+ *   unable to find a new home for orphaned tenants.\n+ *\n+ * @author mgimle\n+ * @author bratseth\n+ */\n+public class SpareCapacityMaintainer extends NodeRepositoryMaintainer {\n+\n+    private final int maxIterations;\n+    private final Deployer deployer;\n+    private final Metric metric;\n+\n+    public SpareCapacityMaintainer(Deployer deployer,\n+                                   NodeRepository nodeRepository,\n+                                   Metric metric,\n+                                   Duration interval) {\n+        this(deployer, nodeRepository, metric, interval,\n+             10_000 // Should take less than a few minutes\n+        );\n+    }\n+\n+    public SpareCapacityMaintainer(Deployer deployer,\n+                                   NodeRepository nodeRepository,\n+                                   Metric metric,\n+                                   Duration interval,\n+                                   int maxIterations) {\n+        super(nodeRepository, interval);\n+        this.deployer = deployer;\n+        this.metric = metric;\n+        this.maxIterations = maxIterations;\n+    }\n+\n+    @Override\n+    protected void maintain() {\n+        if ( ! nodeRepository().zone().getCloud().allowHostSharing()) return;\n+\n+        CapacityChecker capacityChecker = new CapacityChecker(nodeRepository());\n+\n+        List<Node> overcommittedHosts = capacityChecker.findOvercommittedHosts();\n+        if (overcommittedHosts.size() != 0) {\n+            log.log(Level.WARNING, String.format(\"%d nodes are overcommitted! [ %s ]\",\n+                                                 overcommittedHosts.size(),\n+                                                 overcommittedHosts.stream().map(Node::hostname).collect(Collectors.joining(\", \"))));\n+        }\n+        metric.set(\"overcommittedHosts\", overcommittedHosts.size(), null);\n+\n+        Optional<CapacityChecker.HostFailurePath> failurePath = capacityChecker.worstCaseHostLossLeadingToFailure();\n+        if (failurePath.isPresent()) {\n+            int spareHostCapacity = failurePath.get().hostsCausingFailure.size() - 1;\n+            if (spareHostCapacity == 0) {\n+                Move move = findMitigatingMove(failurePath.get());\n+                if (moving(move)) {\n+                    // We succeeded or are in the process of taking a step to mitigate.\n+                    // Report with the assumption this will eventually succeed to avoid alerting before we're stuck\n+                    spareHostCapacity++;\n+                }\n+            }\n+            metric.set(\"spareHostCapacity\", spareHostCapacity, null);\n+        }\n+    }\n+\n+    private boolean moving(Move move) {\n+        if (move.isEmpty()) return false;\n+        if (move.node().allocation().get().membership().retired()) return true; // Move already in progress\n+        return move.execute(false, Agent.SpareCapacityMaintainer, deployer, metric, nodeRepository());\n+    }\n+\n+    private Move findMitigatingMove(CapacityChecker.HostFailurePath failurePath) {\n+        Optional<Node> nodeWhichCantMove = failurePath.failureReason.tenant;\n+        if (nodeWhichCantMove.isEmpty()) return Move.empty();\n+\n+        Node node = nodeWhichCantMove.get();\n+        NodeList allNodes = nodeRepository().list();\n+        // Allocation will assign the two most empty nodes as \"spares\", which will not be allocated on\n+        // unless needed for node failing. Our goal here is to make room on these spares for the given node\n+        HostCapacity hostCapacity = new HostCapacity(allNodes, nodeRepository().resourcesCalculator());\n+        Set<Node> spareHosts = hostCapacity.findSpareHosts(allNodes.hosts().satisfies(node.resources()).asList(), 2);\n+        List<Node> hosts = allNodes.hosts().except(spareHosts).asList();\n+\n+        CapacitySolver capacitySolver = new CapacitySolver(hostCapacity, maxIterations);\n+        List<Move> shortestMitigation = null;\n+        for (Node spareHost : spareHosts) {\n+            List<Move> mitigation = capacitySolver.makeRoomFor(node, spareHost, hosts, List.of(), List.of());\n+            if (mitigation == null) continue;\n+            if (shortestMitigation == null || shortestMitigation.size() > mitigation.size())", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTc2MzQ1NA=="}, "originalCommit": {"oid": "28cf15bac333eadb4f3d133c9102c32bd0d72770"}, "originalPosition": 117}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTk5MjkwOA==", "bodyText": "If the shortest mitigation is empty it means that the situation can be mitigated in 0 moves, i.e it is already fine.\nThis is an inconsistency that will only arise if some change happened to mitigate it between when the nodes were read by CapacityChecker and by this, or if either the CapacityChecker or this has a bug. I don't think we should prefer an unnecessary non-empty mitigation in this case, but arguably we could to signal it upwards such that we don't emit the alert in this case (and perhaps also log it). I'm uncertain on whether it's worth the extra complication.", "url": "https://github.com/vespa-engine/vespa/pull/13619#discussion_r441992908", "createdAt": "2020-06-18T06:21:33Z", "author": {"login": "bratseth"}, "path": "node-repository/src/main/java/com/yahoo/vespa/hosted/provision/maintenance/SpareCapacityMaintainer.java", "diffHunk": "@@ -0,0 +1,346 @@\n+// Copyright Verizon Media. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.\n+package com.yahoo.vespa.hosted.provision.maintenance;\n+\n+import com.yahoo.config.provision.Deployer;\n+import com.yahoo.config.provision.NodeResources;\n+import com.yahoo.jdisc.Metric;\n+import com.yahoo.vespa.hosted.provision.Node;\n+import com.yahoo.vespa.hosted.provision.NodeList;\n+import com.yahoo.vespa.hosted.provision.NodeRepository;\n+import com.yahoo.vespa.hosted.provision.maintenance.MaintenanceDeployment.Move;\n+import com.yahoo.vespa.hosted.provision.node.Agent;\n+import com.yahoo.vespa.hosted.provision.provisioning.HostCapacity;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.logging.Level;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * A maintainer which attempts to ensure there is spare capacity available in chunks which can fit\n+ * all node resource configuration in use, such that the system is able to quickly replace a failed node\n+ * if necessary.\n+ *\n+ * This also emits the following metrics:\n+ * - Overcommitted hosts: Hosts whose capacity is less than the sum of its children's\n+ * - Spare host capacity, or how many hosts the repository can stand to lose without ending up in a situation where it's\n+ *   unable to find a new home for orphaned tenants.\n+ *\n+ * @author mgimle\n+ * @author bratseth\n+ */\n+public class SpareCapacityMaintainer extends NodeRepositoryMaintainer {\n+\n+    private final int maxIterations;\n+    private final Deployer deployer;\n+    private final Metric metric;\n+\n+    public SpareCapacityMaintainer(Deployer deployer,\n+                                   NodeRepository nodeRepository,\n+                                   Metric metric,\n+                                   Duration interval) {\n+        this(deployer, nodeRepository, metric, interval,\n+             10_000 // Should take less than a few minutes\n+        );\n+    }\n+\n+    public SpareCapacityMaintainer(Deployer deployer,\n+                                   NodeRepository nodeRepository,\n+                                   Metric metric,\n+                                   Duration interval,\n+                                   int maxIterations) {\n+        super(nodeRepository, interval);\n+        this.deployer = deployer;\n+        this.metric = metric;\n+        this.maxIterations = maxIterations;\n+    }\n+\n+    @Override\n+    protected void maintain() {\n+        if ( ! nodeRepository().zone().getCloud().allowHostSharing()) return;\n+\n+        CapacityChecker capacityChecker = new CapacityChecker(nodeRepository());\n+\n+        List<Node> overcommittedHosts = capacityChecker.findOvercommittedHosts();\n+        if (overcommittedHosts.size() != 0) {\n+            log.log(Level.WARNING, String.format(\"%d nodes are overcommitted! [ %s ]\",\n+                                                 overcommittedHosts.size(),\n+                                                 overcommittedHosts.stream().map(Node::hostname).collect(Collectors.joining(\", \"))));\n+        }\n+        metric.set(\"overcommittedHosts\", overcommittedHosts.size(), null);\n+\n+        Optional<CapacityChecker.HostFailurePath> failurePath = capacityChecker.worstCaseHostLossLeadingToFailure();\n+        if (failurePath.isPresent()) {\n+            int spareHostCapacity = failurePath.get().hostsCausingFailure.size() - 1;\n+            if (spareHostCapacity == 0) {\n+                Move move = findMitigatingMove(failurePath.get());\n+                if (moving(move)) {\n+                    // We succeeded or are in the process of taking a step to mitigate.\n+                    // Report with the assumption this will eventually succeed to avoid alerting before we're stuck\n+                    spareHostCapacity++;\n+                }\n+            }\n+            metric.set(\"spareHostCapacity\", spareHostCapacity, null);\n+        }\n+    }\n+\n+    private boolean moving(Move move) {\n+        if (move.isEmpty()) return false;\n+        if (move.node().allocation().get().membership().retired()) return true; // Move already in progress\n+        return move.execute(false, Agent.SpareCapacityMaintainer, deployer, metric, nodeRepository());\n+    }\n+\n+    private Move findMitigatingMove(CapacityChecker.HostFailurePath failurePath) {\n+        Optional<Node> nodeWhichCantMove = failurePath.failureReason.tenant;\n+        if (nodeWhichCantMove.isEmpty()) return Move.empty();\n+\n+        Node node = nodeWhichCantMove.get();\n+        NodeList allNodes = nodeRepository().list();\n+        // Allocation will assign the two most empty nodes as \"spares\", which will not be allocated on\n+        // unless needed for node failing. Our goal here is to make room on these spares for the given node\n+        HostCapacity hostCapacity = new HostCapacity(allNodes, nodeRepository().resourcesCalculator());\n+        Set<Node> spareHosts = hostCapacity.findSpareHosts(allNodes.hosts().satisfies(node.resources()).asList(), 2);\n+        List<Node> hosts = allNodes.hosts().except(spareHosts).asList();\n+\n+        CapacitySolver capacitySolver = new CapacitySolver(hostCapacity, maxIterations);\n+        List<Move> shortestMitigation = null;\n+        for (Node spareHost : spareHosts) {\n+            List<Move> mitigation = capacitySolver.makeRoomFor(node, spareHost, hosts, List.of(), List.of());\n+            if (mitigation == null) continue;\n+            if (shortestMitigation == null || shortestMitigation.size() > mitigation.size())", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTc2MzQ1NA=="}, "originalCommit": {"oid": "28cf15bac333eadb4f3d133c9102c32bd0d72770"}, "originalPosition": 117}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1957, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}