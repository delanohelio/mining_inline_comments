{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM1MDM0OTM4", "number": 2076, "title": "Hadoop 16961. ABFS: Adding metrics to AbfsInputStream", "bodyText": "Test run by: mvn -T 1C -Dparallel-tests=abfs clean verify\nRegion: East US, West US\n[INFO] Results:\n[INFO]\n[INFO] Tests run: 77, Failures: 0, Errors: 0, Skipped: 0\n\n[INFO] Results:\n[INFO]\n[WARNING] Tests run: 443, Failures: 0, Errors: 0, Skipped: 70\n\n[INFO] Results:\n[INFO]\n[WARNING] Tests run: 206, Failures: 0, Errors: 0, Skipped: 29\n\nRelates to #1946", "createdAt": "2020-06-16T07:45:28Z", "url": "https://github.com/apache/hadoop/pull/2076", "merged": true, "mergeCommit": {"oid": "3b5c9a90c07e6360007f3f4aa357aa665b47ca3a"}, "closed": true, "closedAt": "2020-07-03T10:41:36Z", "author": {"login": "mehakmeet"}, "timelineItems": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcrDTGOAH2gAyNDM1MDM0OTM4OjdkMWY5MjBmNGM0ZDEzZjcyNzFjMTE1ZWNhM2RjZDA0Mzg0ODc1Zjk=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcu75DhgH2gAyNDM1MDM0OTM4OmRhNDgyY2FmYzYyNTcwY2E4NGY1NTcxMWQ2YTY5MmQwNWNiMzEyNTU=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "7d1f920f4c4d13f7271c115eca3dcd04384875f9", "author": {"user": {"login": "bgaborg", "name": "Gabor Bota"}}, "url": "https://github.com/apache/hadoop/commit/7d1f920f4c4d13f7271c115eca3dcd04384875f9", "committedDate": "2020-06-14T03:15:24Z", "message": "HADOOP-16961. ABFS: Adding metrics to AbfsInputStream\n\nChange-Id: I034b771533b8314364a3762034439e323758ee09"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "eee59b8d8fc92e1a6abc7cab4942fcbd02924586", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/eee59b8d8fc92e1a6abc7cab4942fcbd02924586", "committedDate": "2020-06-16T07:41:51Z", "message": "HADOOP-16961. AbfsInputStreamStatistics via AbfsInputStreamContext and adding tests."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMxNDI1OTEz", "url": "https://github.com/apache/hadoop/pull/2076#pullrequestreview-431425913", "createdAt": "2020-06-16T11:53:57Z", "commit": {"oid": "eee59b8d8fc92e1a6abc7cab4942fcbd02924586"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxMTo1Mzo1N1rOGkXzaw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxMTo1ODozN1rOGkX9ng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc5MTkxNQ==", "bodyText": "needs check for stats == null", "url": "https://github.com/apache/hadoop/pull/2076#discussion_r440791915", "createdAt": "2020-06-16T11:53:57Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -443,4 +488,26 @@ protected void setCachedSasToken(final CachedSASToken cachedSasToken) {\n     this.cachedSasToken = cachedSasToken;\n   }\n \n+  /**\n+   * Getter for AbfsInputStreamStatistics.\n+   *\n+   * @return an instance of AbfsInputStreamStatistics.\n+   */\n+  @VisibleForTesting\n+  public AbfsInputStreamStatistics getStreamStatistics() {\n+    return streamStatistics;\n+  }\n+\n+  /**\n+   * Get the statistics of the stream.\n+   * @return a string value.\n+   */\n+  @Override\n+  public String toString() {\n+    final StringBuilder sb = new StringBuilder(super.toString());\n+    sb.append(\"AbfsInputStream@(\").append(this.hashCode()).append(\"){\");\n+    sb.append(streamStatistics.toString());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eee59b8d8fc92e1a6abc7cab4942fcbd02924586"}, "originalPosition": 178}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc5MjMyNw==", "bodyText": "not needed; remove", "url": "https://github.com/apache/hadoop/pull/2076#discussion_r440792327", "createdAt": "2020-06-16T11:54:38Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStreamStatistics.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.classification.InterfaceStability;\n+\n+/**\n+ * Interface for statistics for the AbfsInputStream.\n+ */\n+@InterfaceStability.Unstable\n+public interface AbfsInputStreamStatistics {\n+  /**\n+   * Seek backwards, incrementing the seek and backward seek counters.\n+   *\n+   * @param negativeOffset how far was the seek?\n+   *                       This is expected to be negative.\n+   */\n+  void seekBackwards(long negativeOffset);\n+\n+  /**\n+   * Record a forward seek, adding a seek operation, a forward\n+   * seek operation, and any bytes skipped.\n+   *\n+   * @param skipped number of bytes skipped by reading from the stream.\n+   *                If the seek was implemented by a close + reopen, set this to zero.\n+   */\n+  void seekForwards(long skipped);\n+\n+  /**\n+   * Record a forward or backward seek, adding a seek operation, a forward or\n+   * a backward seek operation, and number of bytes skipped.\n+   *\n+   * @param seekTo     seek to the position.\n+   * @param currentPos current position.\n+   */\n+  void seek(long seekTo, long currentPos);\n+\n+  /**\n+   * Increment the bytes read counter by the number of bytes;\n+   * no-op if the argument is negative.\n+   *\n+   * @param bytes number of bytes read.\n+   */\n+  void bytesRead(long bytes);\n+\n+  /**\n+   * Record the total bytes read from buffer.\n+   *\n+   * @param bytes number of bytes that are read from buffer.\n+   */\n+  void bytesReadFromBuffer(long bytes);\n+\n+  /**\n+   * Records the total number of seeks done in the buffer.\n+   */\n+  void seekInBuffer();\n+\n+  /**\n+   * A {@code read(byte[] buf, int off, int len)} operation has started.\n+   *\n+   * @param pos starting position of the read.\n+   * @param len length of bytes to read.\n+   */\n+  void readOperationStarted(long pos, long len);\n+\n+  /**\n+   * Records a successful remote read operation.\n+   */\n+  void remoteReadOperation();\n+\n+  /**\n+   * Makes the string of all the AbfsInputStream statistics.\n+   * @return the string with all the statistics.\n+   */\n+  @Override\n+  String toString();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eee59b8d8fc92e1a6abc7cab4942fcbd02924586"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc5Mzg3Mw==", "bodyText": "close() the stream and ask for the stats again, to verify they are still readable\ncall toString on opened and closed streams.", "url": "https://github.com/apache/hadoop/pull/2076#discussion_r440793873", "createdAt": "2020-06-16T11:57:25Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsInputStreamStatistics.java", "diffHunk": "@@ -0,0 +1,231 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+\n+import org.junit.Test;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsInputStream;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsInputStreamStatisticsImpl;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.io.IOUtils;\n+\n+public class ITestAbfsInputStreamStatistics\n+    extends AbstractAbfsIntegrationTest {\n+  private static final int OPERATIONS = 10;\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(ITestAbfsInputStreamStatistics.class);\n+  private static final int ONE_MB = 1024 * 1024;\n+  private byte[] defBuffer = new byte[ONE_MB];\n+\n+  public ITestAbfsInputStreamStatistics() throws Exception {\n+  }\n+\n+  /**\n+   * Test to check the initial values of the AbfsInputStream statistics.\n+   */\n+  @Test\n+  public void testInitValues() throws IOException {\n+    describe(\"Testing the initial values of AbfsInputStream Statistics\");\n+\n+    AzureBlobFileSystem fs = getFileSystem();\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    Path initValuesPath = path(getMethodName());\n+    AbfsOutputStream outputStream = null;\n+    AbfsInputStream inputStream = null;\n+\n+    try {\n+\n+      outputStream = createAbfsOutputStreamWithFlushEnabled(fs, initValuesPath);\n+      inputStream = abfss.openFileForRead(initValuesPath, fs.getFsStatistics());\n+\n+      AbfsInputStreamStatisticsImpl stats =\n+          (AbfsInputStreamStatisticsImpl) inputStream.getStreamStatistics();\n+\n+      checkInitValue(stats.getSeekOperations(), \"seekOps\");\n+      checkInitValue(stats.getForwardSeekOperations(), \"forwardSeekOps\");\n+      checkInitValue(stats.getBackwardSeekOperations(), \"backwardSeekOps\");\n+      checkInitValue(stats.getBytesRead(), \"bytesRead\");\n+      checkInitValue(stats.getBytesSkippedOnSeek(), \"bytesSkippedOnSeek\");\n+      checkInitValue(stats.getBytesBackwardsOnSeek(), \"bytesBackwardsOnSeek\");\n+      checkInitValue(stats.getSeekInBuffer(), \"seekInBuffer\");\n+      checkInitValue(stats.getReadOperations(), \"readOps\");\n+      checkInitValue(stats.getBytesReadFromBuffer(), \"bytesReadFromBuffer\");\n+      checkInitValue(stats.getRemoteReadOperations(), \"remoteReadOps\");\n+\n+    } finally {\n+      IOUtils.cleanupWithLogger(LOG, outputStream, inputStream);\n+    }\n+  }\n+\n+  /**\n+   * Test to check statistics from seek operation in AbfsInputStream.\n+   */\n+  @Test\n+  public void testSeekStatistics() throws IOException {\n+    describe(\"Testing the values of statistics from seek operations in \"\n+        + \"AbfsInputStream\");\n+\n+    AzureBlobFileSystem fs = getFileSystem();\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    Path seekStatPath = path(getMethodName());\n+\n+    AbfsOutputStream out = null;\n+    AbfsInputStream in = null;\n+\n+    try {\n+      out = createAbfsOutputStreamWithFlushEnabled(fs, seekStatPath);\n+\n+      //Writing a default buffer in a file.\n+      out.write(defBuffer);\n+      out.hflush();\n+      in = abfss.openFileForRead(seekStatPath, fs.getFsStatistics());\n+\n+      /*\n+       * Writing 1MB buffer to the file, this would make the fCursor(Current\n+       * position of cursor) to the end of file.\n+       */\n+      int result = in.read(defBuffer, 0, ONE_MB);\n+      LOG.info(\"Result of read : {}\", result);\n+\n+      /*\n+       * Seeking to start of file and then back to end would result in a\n+       * backward and a forward seek respectively 10 times.\n+       */\n+      for (int i = 0; i < OPERATIONS; i++) {\n+        in.seek(0);\n+        in.seek(ONE_MB);\n+      }\n+\n+      AbfsInputStreamStatisticsImpl stats =\n+          (AbfsInputStreamStatisticsImpl) in.getStreamStatistics();\n+      /*\n+       * seekOps - Since we are doing backward and forward seek OPERATIONS\n+       * times, total seeks would be 2 * OPERATIONS.\n+       *\n+       * backwardSeekOps - Since we are doing a backward seek inside a loop\n+       * for OPERATION times, total backward seeks would be OPERATIONS.\n+       *\n+       * forwardSeekOps - Since we are doing a forward seek inside a loop\n+       * for OPERATION times, total forward seeks would be OPERATIONS.\n+       *\n+       * bytesBackwardsOnSeek - Since we are doing backward seeks from end of\n+       * file in a ONE_MB file each time, this would mean the bytes from\n+       * backward seek would be OPERATIONS * ONE_MB. Since this is backward\n+       * seek this value is expected be to be negative.\n+       *\n+       * bytesSkippedOnSeek - Since, we move from start to end in seek, but\n+       * our fCursor(position of cursor) always remain at end of file, this\n+       * would mean no bytes were skipped on seek. Since, all forward seeks\n+       * are in buffer.\n+       *\n+       * seekInBuffer - Since all seeks were in buffer, the seekInBuffer\n+       * would be equal to 2 * OPERATIONS.\n+       *\n+       */\n+      assertEquals(\"Mismatch in seekOps value\", 2 * OPERATIONS,\n+          stats.getSeekOperations());\n+      assertEquals(\"Mismatch in backwardSeekOps value\", OPERATIONS,\n+          stats.getBackwardSeekOperations());\n+      assertEquals(\"Mismatch in forwardSeekOps value\", OPERATIONS,\n+          stats.getForwardSeekOperations());\n+      assertEquals(\"Mismatch in bytesBackwardsOnSeek value\",\n+          -1 * OPERATIONS * ONE_MB, stats.getBytesBackwardsOnSeek());\n+      assertEquals(\"Mismatch in bytesSkippedOnSeek value\",\n+          0, stats.getBytesSkippedOnSeek());\n+      assertEquals(\"Mismatch in seekInBuffer value\", 2 * OPERATIONS,\n+          stats.getSeekInBuffer());\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eee59b8d8fc92e1a6abc7cab4942fcbd02924586"}, "originalPosition": 158}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc5NDUyNg==", "bodyText": "close() the stream and ask for the stats again, to verify they are still readable\ncall toString on opened and closed streams.", "url": "https://github.com/apache/hadoop/pull/2076#discussion_r440794526", "createdAt": "2020-06-16T11:58:37Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsInputStreamStatistics.java", "diffHunk": "@@ -0,0 +1,231 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+\n+import org.junit.Test;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsInputStream;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsInputStreamStatisticsImpl;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.io.IOUtils;\n+\n+public class ITestAbfsInputStreamStatistics\n+    extends AbstractAbfsIntegrationTest {\n+  private static final int OPERATIONS = 10;\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(ITestAbfsInputStreamStatistics.class);\n+  private static final int ONE_MB = 1024 * 1024;\n+  private byte[] defBuffer = new byte[ONE_MB];\n+\n+  public ITestAbfsInputStreamStatistics() throws Exception {\n+  }\n+\n+  /**\n+   * Test to check the initial values of the AbfsInputStream statistics.\n+   */\n+  @Test\n+  public void testInitValues() throws IOException {\n+    describe(\"Testing the initial values of AbfsInputStream Statistics\");\n+\n+    AzureBlobFileSystem fs = getFileSystem();\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    Path initValuesPath = path(getMethodName());\n+    AbfsOutputStream outputStream = null;\n+    AbfsInputStream inputStream = null;\n+\n+    try {\n+\n+      outputStream = createAbfsOutputStreamWithFlushEnabled(fs, initValuesPath);\n+      inputStream = abfss.openFileForRead(initValuesPath, fs.getFsStatistics());\n+\n+      AbfsInputStreamStatisticsImpl stats =\n+          (AbfsInputStreamStatisticsImpl) inputStream.getStreamStatistics();\n+\n+      checkInitValue(stats.getSeekOperations(), \"seekOps\");\n+      checkInitValue(stats.getForwardSeekOperations(), \"forwardSeekOps\");\n+      checkInitValue(stats.getBackwardSeekOperations(), \"backwardSeekOps\");\n+      checkInitValue(stats.getBytesRead(), \"bytesRead\");\n+      checkInitValue(stats.getBytesSkippedOnSeek(), \"bytesSkippedOnSeek\");\n+      checkInitValue(stats.getBytesBackwardsOnSeek(), \"bytesBackwardsOnSeek\");\n+      checkInitValue(stats.getSeekInBuffer(), \"seekInBuffer\");\n+      checkInitValue(stats.getReadOperations(), \"readOps\");\n+      checkInitValue(stats.getBytesReadFromBuffer(), \"bytesReadFromBuffer\");\n+      checkInitValue(stats.getRemoteReadOperations(), \"remoteReadOps\");\n+\n+    } finally {\n+      IOUtils.cleanupWithLogger(LOG, outputStream, inputStream);\n+    }\n+  }\n+\n+  /**\n+   * Test to check statistics from seek operation in AbfsInputStream.\n+   */\n+  @Test\n+  public void testSeekStatistics() throws IOException {\n+    describe(\"Testing the values of statistics from seek operations in \"\n+        + \"AbfsInputStream\");\n+\n+    AzureBlobFileSystem fs = getFileSystem();\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    Path seekStatPath = path(getMethodName());\n+\n+    AbfsOutputStream out = null;\n+    AbfsInputStream in = null;\n+\n+    try {\n+      out = createAbfsOutputStreamWithFlushEnabled(fs, seekStatPath);\n+\n+      //Writing a default buffer in a file.\n+      out.write(defBuffer);\n+      out.hflush();\n+      in = abfss.openFileForRead(seekStatPath, fs.getFsStatistics());\n+\n+      /*\n+       * Writing 1MB buffer to the file, this would make the fCursor(Current\n+       * position of cursor) to the end of file.\n+       */\n+      int result = in.read(defBuffer, 0, ONE_MB);\n+      LOG.info(\"Result of read : {}\", result);\n+\n+      /*\n+       * Seeking to start of file and then back to end would result in a\n+       * backward and a forward seek respectively 10 times.\n+       */\n+      for (int i = 0; i < OPERATIONS; i++) {\n+        in.seek(0);\n+        in.seek(ONE_MB);\n+      }\n+\n+      AbfsInputStreamStatisticsImpl stats =\n+          (AbfsInputStreamStatisticsImpl) in.getStreamStatistics();\n+      /*\n+       * seekOps - Since we are doing backward and forward seek OPERATIONS\n+       * times, total seeks would be 2 * OPERATIONS.\n+       *\n+       * backwardSeekOps - Since we are doing a backward seek inside a loop\n+       * for OPERATION times, total backward seeks would be OPERATIONS.\n+       *\n+       * forwardSeekOps - Since we are doing a forward seek inside a loop\n+       * for OPERATION times, total forward seeks would be OPERATIONS.\n+       *\n+       * bytesBackwardsOnSeek - Since we are doing backward seeks from end of\n+       * file in a ONE_MB file each time, this would mean the bytes from\n+       * backward seek would be OPERATIONS * ONE_MB. Since this is backward\n+       * seek this value is expected be to be negative.\n+       *\n+       * bytesSkippedOnSeek - Since, we move from start to end in seek, but\n+       * our fCursor(position of cursor) always remain at end of file, this\n+       * would mean no bytes were skipped on seek. Since, all forward seeks\n+       * are in buffer.\n+       *\n+       * seekInBuffer - Since all seeks were in buffer, the seekInBuffer\n+       * would be equal to 2 * OPERATIONS.\n+       *\n+       */\n+      assertEquals(\"Mismatch in seekOps value\", 2 * OPERATIONS,\n+          stats.getSeekOperations());\n+      assertEquals(\"Mismatch in backwardSeekOps value\", OPERATIONS,\n+          stats.getBackwardSeekOperations());\n+      assertEquals(\"Mismatch in forwardSeekOps value\", OPERATIONS,\n+          stats.getForwardSeekOperations());\n+      assertEquals(\"Mismatch in bytesBackwardsOnSeek value\",\n+          -1 * OPERATIONS * ONE_MB, stats.getBytesBackwardsOnSeek());\n+      assertEquals(\"Mismatch in bytesSkippedOnSeek value\",\n+          0, stats.getBytesSkippedOnSeek());\n+      assertEquals(\"Mismatch in seekInBuffer value\", 2 * OPERATIONS,\n+          stats.getSeekInBuffer());\n+\n+    } finally {\n+      IOUtils.cleanupWithLogger(LOG, out, in);\n+    }\n+  }\n+\n+  /**\n+   * Test to check statistics value from read operation in AbfsInputStream.\n+   */\n+  @Test\n+  public void testReadStatistics() throws IOException {\n+    describe(\"Testing the values of statistics from read operation in \"\n+        + \"AbfsInputStream\");\n+\n+    AzureBlobFileSystem fs = getFileSystem();\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    Path readStatPath = path(getMethodName());\n+\n+    AbfsOutputStream out = null;\n+    AbfsInputStream in = null;\n+\n+    try {\n+      out = createAbfsOutputStreamWithFlushEnabled(fs, readStatPath);\n+\n+      /*\n+       * Writing 1MB buffer to the file.\n+       */\n+      out.write(defBuffer);\n+      out.hflush();\n+      in = abfss.openFileForRead(readStatPath, fs.getFsStatistics());\n+\n+      /*\n+       * Doing file read 10 times.\n+       */\n+      for (int i = 0; i < OPERATIONS; i++) {\n+        in.read();\n+      }\n+\n+      AbfsInputStreamStatisticsImpl stats =\n+          (AbfsInputStreamStatisticsImpl) in.getStreamStatistics();\n+\n+      /*\n+       * bytesRead - Since each time a single byte is read, total\n+       * bytes read would be equal to OPERATIONS.\n+       *\n+       * readOps - Since each time read operation is performed OPERATIONS\n+       * times, total number of read operations would be equal to OPERATIONS.\n+       *\n+       * remoteReadOps - Only a single remote read operation is done. Hence,\n+       * total remote read ops is 1.\n+       *\n+       */\n+      assertEquals(\"Mismatch in bytesRead value\", OPERATIONS,\n+          stats.getBytesRead());\n+      assertEquals(\"Mismatch in readOps value\", OPERATIONS,\n+          stats.getReadOperations());\n+      assertEquals(\"Mismatch in remoteReadOps value\", 1,\n+          stats.getRemoteReadOperations());\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eee59b8d8fc92e1a6abc7cab4942fcbd02924586"}, "originalPosition": 216}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMyMjE3MjY2", "url": "https://github.com/apache/hadoop/pull/2076#pullrequestreview-432217266", "createdAt": "2020-06-17T09:22:18Z", "commit": {"oid": "eee59b8d8fc92e1a6abc7cab4942fcbd02924586"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e07bd4a942b6f4b307fdc75a7bd3d31f0b0bad0f", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/e07bd4a942b6f4b307fdc75a7bd3d31f0b0bad0f", "committedDate": "2020-06-22T07:10:43Z", "message": "HADOOP-16961. Review Comments, Statistics Readability with closed stream and null statistics test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "da482cafc62570ca84f55711d6a692d05cb31255", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/da482cafc62570ca84f55711d6a692d05cb31255", "committedDate": "2020-06-26T04:53:19Z", "message": "HADOOP-16961. null statistics test with no errors and bytesReadFromBuffer at correct place."}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4028, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}