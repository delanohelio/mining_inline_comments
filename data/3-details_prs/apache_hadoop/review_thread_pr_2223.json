{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY3MTM5OTk0", "number": 2223, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMlQwMDoyMDo1NlrOEiuZKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMlQwNDo1NDoxNFrOEivsCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0ODQ3MTQ3OnYy", "diffSide": "LEFT", "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMlQwMDoyMDo1NlrOHQw-Mw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMlQwMDoyMDo1NlrOHQw-Mw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzM0MTYxOQ==", "bodyText": "The bug is when the policies are empty it won't clean up the existing policies.", "url": "https://github.com/apache/hadoop/pull/2223#discussion_r487341619", "createdAt": "2020-09-12T00:20:56Z", "author": {"login": "JohnZZGithub"}, "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java", "diffHunk": "@@ -1450,26 +1450,33 @@ public static void setArchiveSharedCacheUploadPolicies(Configuration conf,\n    */\n   private static void setSharedCacheUploadPolicies(Configuration conf,\n       Map<String, Boolean> policies, boolean areFiles) {\n-    if (policies != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a15e59c06c9cd5e2e9f52d48d3693592255b7465"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0ODY3OTk1OnYy", "diffSide": "RIGHT", "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMlQwNDo0OTowMVrOHQykWg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMlQwNDo1MTowNlrOHQylFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzM2Nzc3MA==", "bodyText": "I know following code was mostly borrowed from the existing code, but since we are in Java 8 for Hadoop 3, should we simplify this a bit using this chance?\n    Iterator<Map.Entry<String, Boolean>> it = policies.entrySet().iterator();\n    Map.Entry<String, Boolean> e;\n    if (it.hasNext()) {\n      e = it.next();\n      sb.append(e.getKey() + DELIM + e.getValue());\n    }\n    while (it.hasNext()) {\n      e = it.next();\n      sb.append(\",\" + e.getKey() + DELIM + e.getValue());\n    }\n\ncan be shorter and clearer statement, for e.g.\n    policies.forEach((k,v) -> sb.append(k).append(DELIM).append(v).append(\",\"));\n    sb.deleteCharAt(sb.length() - 1); // do we need this, or it is just fine?\n\nThoughts?", "url": "https://github.com/apache/hadoop/pull/2223#discussion_r487367770", "createdAt": "2020-09-12T04:49:01Z", "author": {"login": "liuml07"}, "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java", "diffHunk": "@@ -1450,26 +1450,33 @@ public static void setArchiveSharedCacheUploadPolicies(Configuration conf,\n    */\n   private static void setSharedCacheUploadPolicies(Configuration conf,\n       Map<String, Boolean> policies, boolean areFiles) {\n-    if (policies != null) {\n-      StringBuilder sb = new StringBuilder();\n-      Iterator<Map.Entry<String, Boolean>> it = policies.entrySet().iterator();\n-      Map.Entry<String, Boolean> e;\n-      if (it.hasNext()) {\n-        e = it.next();\n-        sb.append(e.getKey() + DELIM + e.getValue());\n-      } else {\n-        // policies is an empty map, just skip setting the parameter\n-        return;\n-      }\n-      while (it.hasNext()) {\n-        e = it.next();\n-        sb.append(\",\" + e.getKey() + DELIM + e.getValue());\n-      }\n-      String confParam =\n-          areFiles ? MRJobConfig.CACHE_FILES_SHARED_CACHE_UPLOAD_POLICIES\n-              : MRJobConfig.CACHE_ARCHIVES_SHARED_CACHE_UPLOAD_POLICIES;\n-      conf.set(confParam, sb.toString());\n+    String confParam = areFiles ?\n+        MRJobConfig.CACHE_FILES_SHARED_CACHE_UPLOAD_POLICIES :\n+        MRJobConfig.CACHE_ARCHIVES_SHARED_CACHE_UPLOAD_POLICIES;\n+    conf.set(confParam, populateSharedCacheUploadPolicies(policies));\n+  }\n+\n+  private static String populateSharedCacheUploadPolicies(\n+      Map<String, Boolean> policies) {\n+    // If policies are an empty map or null, we will set EMPTY_STRING.\n+    // In other words, cleaning up existing policies. This is useful when we\n+    // try to clean up shared cache upload policies for non-application\n+    // master tasks. See YARN-10398 for details.\n+    if (policies == null || policies.size() == 0) {\n+      return \"\";\n+    }\n+    StringBuilder sb = new StringBuilder();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a15e59c06c9cd5e2e9f52d48d3693592255b7465"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzM2Nzk1OA==", "bodyText": "Also, given the code is so simple, maybe we can save one extra private helper method, and move the logic back to setSharedCacheUploadPolicies() method, which itself has only several lines of code.", "url": "https://github.com/apache/hadoop/pull/2223#discussion_r487367958", "createdAt": "2020-09-12T04:51:06Z", "author": {"login": "liuml07"}, "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java", "diffHunk": "@@ -1450,26 +1450,33 @@ public static void setArchiveSharedCacheUploadPolicies(Configuration conf,\n    */\n   private static void setSharedCacheUploadPolicies(Configuration conf,\n       Map<String, Boolean> policies, boolean areFiles) {\n-    if (policies != null) {\n-      StringBuilder sb = new StringBuilder();\n-      Iterator<Map.Entry<String, Boolean>> it = policies.entrySet().iterator();\n-      Map.Entry<String, Boolean> e;\n-      if (it.hasNext()) {\n-        e = it.next();\n-        sb.append(e.getKey() + DELIM + e.getValue());\n-      } else {\n-        // policies is an empty map, just skip setting the parameter\n-        return;\n-      }\n-      while (it.hasNext()) {\n-        e = it.next();\n-        sb.append(\",\" + e.getKey() + DELIM + e.getValue());\n-      }\n-      String confParam =\n-          areFiles ? MRJobConfig.CACHE_FILES_SHARED_CACHE_UPLOAD_POLICIES\n-              : MRJobConfig.CACHE_ARCHIVES_SHARED_CACHE_UPLOAD_POLICIES;\n-      conf.set(confParam, sb.toString());\n+    String confParam = areFiles ?\n+        MRJobConfig.CACHE_FILES_SHARED_CACHE_UPLOAD_POLICIES :\n+        MRJobConfig.CACHE_ARCHIVES_SHARED_CACHE_UPLOAD_POLICIES;\n+    conf.set(confParam, populateSharedCacheUploadPolicies(policies));\n+  }\n+\n+  private static String populateSharedCacheUploadPolicies(\n+      Map<String, Boolean> policies) {\n+    // If policies are an empty map or null, we will set EMPTY_STRING.\n+    // In other words, cleaning up existing policies. This is useful when we\n+    // try to clean up shared cache upload policies for non-application\n+    // master tasks. See YARN-10398 for details.\n+    if (policies == null || policies.size() == 0) {\n+      return \"\";\n+    }\n+    StringBuilder sb = new StringBuilder();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzM2Nzc3MA=="}, "originalCommit": {"oid": "a15e59c06c9cd5e2e9f52d48d3693592255b7465"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0ODY4MjQ2OnYy", "diffSide": "RIGHT", "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMlQwNDo1MjoxNFrOHQyleQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMlQwNDo1MjoxNFrOHQyleQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzM2ODA1Nw==", "bodyText": "Since this JIRA has been moved from YARN to MAPREDUCE project, should we replace the YARN-10398 in comment with the new JIRA number MAPREDUCE-7294?", "url": "https://github.com/apache/hadoop/pull/2223#discussion_r487368057", "createdAt": "2020-09-12T04:52:14Z", "author": {"login": "liuml07"}, "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java", "diffHunk": "@@ -1450,26 +1450,33 @@ public static void setArchiveSharedCacheUploadPolicies(Configuration conf,\n    */\n   private static void setSharedCacheUploadPolicies(Configuration conf,\n       Map<String, Boolean> policies, boolean areFiles) {\n-    if (policies != null) {\n-      StringBuilder sb = new StringBuilder();\n-      Iterator<Map.Entry<String, Boolean>> it = policies.entrySet().iterator();\n-      Map.Entry<String, Boolean> e;\n-      if (it.hasNext()) {\n-        e = it.next();\n-        sb.append(e.getKey() + DELIM + e.getValue());\n-      } else {\n-        // policies is an empty map, just skip setting the parameter\n-        return;\n-      }\n-      while (it.hasNext()) {\n-        e = it.next();\n-        sb.append(\",\" + e.getKey() + DELIM + e.getValue());\n-      }\n-      String confParam =\n-          areFiles ? MRJobConfig.CACHE_FILES_SHARED_CACHE_UPLOAD_POLICIES\n-              : MRJobConfig.CACHE_ARCHIVES_SHARED_CACHE_UPLOAD_POLICIES;\n-      conf.set(confParam, sb.toString());\n+    String confParam = areFiles ?\n+        MRJobConfig.CACHE_FILES_SHARED_CACHE_UPLOAD_POLICIES :\n+        MRJobConfig.CACHE_ARCHIVES_SHARED_CACHE_UPLOAD_POLICIES;\n+    conf.set(confParam, populateSharedCacheUploadPolicies(policies));\n+  }\n+\n+  private static String populateSharedCacheUploadPolicies(\n+      Map<String, Boolean> policies) {\n+    // If policies are an empty map or null, we will set EMPTY_STRING.\n+    // In other words, cleaning up existing policies. This is useful when we\n+    // try to clean up shared cache upload policies for non-application\n+    // master tasks. See YARN-10398 for details.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a15e59c06c9cd5e2e9f52d48d3693592255b7465"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0ODY4MzYwOnYy", "diffSide": "RIGHT", "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMlQwNDo1NDoxNFrOHQyl_Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMlQwNDo1NDoxNFrOHQyl_Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzM2ODE4OQ==", "bodyText": "nit: this sentence can be:\n// If no policy is provided, we will reset the config by setting an empty string value.", "url": "https://github.com/apache/hadoop/pull/2223#discussion_r487368189", "createdAt": "2020-09-12T04:54:14Z", "author": {"login": "liuml07"}, "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java", "diffHunk": "@@ -1450,26 +1450,33 @@ public static void setArchiveSharedCacheUploadPolicies(Configuration conf,\n    */\n   private static void setSharedCacheUploadPolicies(Configuration conf,\n       Map<String, Boolean> policies, boolean areFiles) {\n-    if (policies != null) {\n-      StringBuilder sb = new StringBuilder();\n-      Iterator<Map.Entry<String, Boolean>> it = policies.entrySet().iterator();\n-      Map.Entry<String, Boolean> e;\n-      if (it.hasNext()) {\n-        e = it.next();\n-        sb.append(e.getKey() + DELIM + e.getValue());\n-      } else {\n-        // policies is an empty map, just skip setting the parameter\n-        return;\n-      }\n-      while (it.hasNext()) {\n-        e = it.next();\n-        sb.append(\",\" + e.getKey() + DELIM + e.getValue());\n-      }\n-      String confParam =\n-          areFiles ? MRJobConfig.CACHE_FILES_SHARED_CACHE_UPLOAD_POLICIES\n-              : MRJobConfig.CACHE_ARCHIVES_SHARED_CACHE_UPLOAD_POLICIES;\n-      conf.set(confParam, sb.toString());\n+    String confParam = areFiles ?\n+        MRJobConfig.CACHE_FILES_SHARED_CACHE_UPLOAD_POLICIES :\n+        MRJobConfig.CACHE_ARCHIVES_SHARED_CACHE_UPLOAD_POLICIES;\n+    conf.set(confParam, populateSharedCacheUploadPolicies(policies));\n+  }\n+\n+  private static String populateSharedCacheUploadPolicies(\n+      Map<String, Boolean> policies) {\n+    // If policies are an empty map or null, we will set EMPTY_STRING.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a15e59c06c9cd5e2e9f52d48d3693592255b7465"}, "originalPosition": 31}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3385, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}