{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzcxNzg3ODcz", "number": 1832, "reviewThreads": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNlQyMTozMTozN1rODd1mHw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQxNzo0NjowOVrODejANA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMyNjEzNDA3OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterHttpServer.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNlQyMTozMTozN1rOFmq6FQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNlQyMTozMTozN1rOFmq6FQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjA5MzIwNQ==", "bodyText": "I think we can remove this now.", "url": "https://github.com/apache/hadoop/pull/1832#discussion_r376093205", "createdAt": "2020-02-06T21:31:37Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterHttpServer.java", "diffHunk": "@@ -118,10 +118,14 @@ protected void serviceStop() throws Exception {\n \n   private static void setupServlets(\n       HttpServer2 httpServer, Configuration conf) {\n-    // TODO Add servlets for FSCK, etc\n+    // TODO: Add more required servlets", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d697bee4bb006190297e2fa4d918773caac5c93c"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMyNjEzNjE3OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterFsck.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNlQyMTozMjoyNFrOFmq7cg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNlQyMTozMjoyNFrOFmq7cg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjA5MzU1NA==", "bodyText": "Let's make an static import for assertTrue and add a message with the out.", "url": "https://github.com/apache/hadoop/pull/1832#discussion_r376093554", "createdAt": "2020-02-06T21:32:24Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterFsck.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.federation.router;\n+\n+import java.io.IOException;\n+import java.net.InetSocketAddress;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Collections;\n+\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.server.federation.MiniRouterDFSCluster;\n+import org.apache.hadoop.hdfs.server.federation.RouterConfigBuilder;\n+import org.apache.hadoop.hdfs.server.federation.StateStoreDFSCluster;\n+import org.apache.hadoop.hdfs.server.federation.resolver.MountTableManager;\n+import org.apache.hadoop.hdfs.server.federation.resolver.MountTableResolver;\n+import org.apache.hadoop.hdfs.server.federation.store.protocol.AddMountTableEntryRequest;\n+import org.apache.hadoop.hdfs.server.federation.store.protocol.AddMountTableEntryResponse;\n+import org.apache.hadoop.hdfs.server.federation.store.protocol.GetMountTableEntriesRequest;\n+import org.apache.hadoop.hdfs.server.federation.store.protocol.GetMountTableEntriesResponse;\n+import org.apache.hadoop.hdfs.server.federation.store.protocol.RemoveMountTableEntryRequest;\n+import org.apache.hadoop.hdfs.server.federation.store.records.MountTable;\n+import org.apache.http.HttpStatus;\n+import org.apache.http.client.methods.CloseableHttpResponse;\n+import org.apache.http.client.methods.HttpGet;\n+import org.apache.http.impl.client.CloseableHttpClient;\n+import org.apache.http.impl.client.HttpClients;\n+import org.apache.http.util.EntityUtils;\n+\n+/**\n+ * End-to-end tests for fsck via DFSRouter\n+ */\n+public class TestRouterFsck {\n+\n+  private static StateStoreDFSCluster cluster;\n+  private static MiniRouterDFSCluster.RouterContext routerContext;\n+  private static MountTableResolver mountTable;\n+  private static FileSystem routerFs;\n+  private static InetSocketAddress webAddress;\n+\n+  @BeforeClass\n+  public static void globalSetUp() throws Exception {\n+    // Build and start a federated cluster\n+    cluster = new StateStoreDFSCluster(false, 2);\n+    Configuration conf = new RouterConfigBuilder()\n+        .stateStore()\n+        .admin()\n+        .rpc()\n+        .http()\n+        .build();\n+    cluster.addRouterOverrides(conf);\n+    cluster.startCluster();\n+    cluster.startRouters();\n+    cluster.waitClusterUp();\n+\n+    // Get the end points\n+    routerContext = cluster.getRandomRouter();\n+    routerFs = routerContext.getFileSystem();\n+    Router router = routerContext.getRouter();\n+    mountTable = (MountTableResolver) router.getSubclusterResolver();\n+    webAddress = router.getHttpServerAddress();\n+    Assert.assertNotNull(webAddress);\n+  }\n+\n+  @AfterClass\n+  public static void tearDown() {\n+    if (cluster != null) {\n+      cluster.stopRouter(routerContext);\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n+  }\n+\n+  @After\n+  public void clearMountTable() throws IOException {\n+    RouterClient client = routerContext.getAdminClient();\n+    MountTableManager mountTableManager = client.getMountTableManager();\n+    GetMountTableEntriesRequest req1 =\n+        GetMountTableEntriesRequest.newInstance(\"/\");\n+    GetMountTableEntriesResponse response =\n+        mountTableManager.getMountTableEntries(req1);\n+    for (MountTable entry : response.getEntries()) {\n+      RemoveMountTableEntryRequest req2 =\n+          RemoveMountTableEntryRequest.newInstance(entry.getSourcePath());\n+      mountTableManager.removeMountTableEntry(req2);\n+    }\n+  }\n+\n+  private boolean addMountTable(final MountTable entry) throws IOException {\n+    RouterClient client = routerContext.getAdminClient();\n+    MountTableManager mountTableManager = client.getMountTableManager();\n+    AddMountTableEntryRequest addRequest =\n+        AddMountTableEntryRequest.newInstance(entry);\n+    AddMountTableEntryResponse addResponse =\n+        mountTableManager.addMountTableEntry(addRequest);\n+    // Reload the Router cache\n+    mountTable.loadCache(true);\n+    return addResponse.getStatus();\n+  }\n+\n+  @Test\n+  public void testFsck() throws Exception {\n+    MountTable addEntry = MountTable.newInstance(\"/testdir\",\n+        Collections.singletonMap(\"ns0\", \"/testdir\"));\n+    Assert.assertTrue(addMountTable(addEntry));\n+    addEntry = MountTable.newInstance(\"/testdir2\",\n+        Collections.singletonMap(\"ns1\", \"/testdir2\"));\n+    Assert.assertTrue(addMountTable(addEntry));\n+    // create 1 file on ns0\n+    routerFs.createNewFile(new Path(\"/testdir/testfile\"));\n+    // create 3 files on ns1\n+    routerFs.createNewFile(new Path(\"/testdir2/testfile2\"));\n+    routerFs.createNewFile(new Path(\"/testdir2/testfile3\"));\n+    routerFs.createNewFile(new Path(\"/testdir2/testfile4\"));\n+\n+    try (CloseableHttpClient httpClient = HttpClients.createDefault()) {\n+      // TODO: support https\n+      HttpGet httpGet = new HttpGet(\"http://\" + webAddress.getHostName() +\n+              \":\" + webAddress.getPort() + \"/fsck\");\n+      try (CloseableHttpResponse httpResponse = httpClient.execute(httpGet)) {\n+        Assert.assertEquals(HttpStatus.SC_OK,\n+            httpResponse.getStatusLine().getStatusCode());\n+        String out = EntityUtils.toString(\n+            httpResponse.getEntity(), StandardCharsets.UTF_8);\n+        System.out.println(out);\n+        Assert.assertTrue(out.contains(\"Federated FSCK started\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d697bee4bb006190297e2fa4d918773caac5c93c"}, "originalPosition": 148}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMyNjEzNjQxOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterFsck.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNlQyMTozMjozMVrOFmq7nA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNlQyMTozMjozMVrOFmq7nA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjA5MzU5Ng==", "bodyText": "LOG?", "url": "https://github.com/apache/hadoop/pull/1832#discussion_r376093596", "createdAt": "2020-02-06T21:32:31Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterFsck.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.federation.router;\n+\n+import java.io.IOException;\n+import java.net.InetSocketAddress;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Collections;\n+\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.server.federation.MiniRouterDFSCluster;\n+import org.apache.hadoop.hdfs.server.federation.RouterConfigBuilder;\n+import org.apache.hadoop.hdfs.server.federation.StateStoreDFSCluster;\n+import org.apache.hadoop.hdfs.server.federation.resolver.MountTableManager;\n+import org.apache.hadoop.hdfs.server.federation.resolver.MountTableResolver;\n+import org.apache.hadoop.hdfs.server.federation.store.protocol.AddMountTableEntryRequest;\n+import org.apache.hadoop.hdfs.server.federation.store.protocol.AddMountTableEntryResponse;\n+import org.apache.hadoop.hdfs.server.federation.store.protocol.GetMountTableEntriesRequest;\n+import org.apache.hadoop.hdfs.server.federation.store.protocol.GetMountTableEntriesResponse;\n+import org.apache.hadoop.hdfs.server.federation.store.protocol.RemoveMountTableEntryRequest;\n+import org.apache.hadoop.hdfs.server.federation.store.records.MountTable;\n+import org.apache.http.HttpStatus;\n+import org.apache.http.client.methods.CloseableHttpResponse;\n+import org.apache.http.client.methods.HttpGet;\n+import org.apache.http.impl.client.CloseableHttpClient;\n+import org.apache.http.impl.client.HttpClients;\n+import org.apache.http.util.EntityUtils;\n+\n+/**\n+ * End-to-end tests for fsck via DFSRouter\n+ */\n+public class TestRouterFsck {\n+\n+  private static StateStoreDFSCluster cluster;\n+  private static MiniRouterDFSCluster.RouterContext routerContext;\n+  private static MountTableResolver mountTable;\n+  private static FileSystem routerFs;\n+  private static InetSocketAddress webAddress;\n+\n+  @BeforeClass\n+  public static void globalSetUp() throws Exception {\n+    // Build and start a federated cluster\n+    cluster = new StateStoreDFSCluster(false, 2);\n+    Configuration conf = new RouterConfigBuilder()\n+        .stateStore()\n+        .admin()\n+        .rpc()\n+        .http()\n+        .build();\n+    cluster.addRouterOverrides(conf);\n+    cluster.startCluster();\n+    cluster.startRouters();\n+    cluster.waitClusterUp();\n+\n+    // Get the end points\n+    routerContext = cluster.getRandomRouter();\n+    routerFs = routerContext.getFileSystem();\n+    Router router = routerContext.getRouter();\n+    mountTable = (MountTableResolver) router.getSubclusterResolver();\n+    webAddress = router.getHttpServerAddress();\n+    Assert.assertNotNull(webAddress);\n+  }\n+\n+  @AfterClass\n+  public static void tearDown() {\n+    if (cluster != null) {\n+      cluster.stopRouter(routerContext);\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n+  }\n+\n+  @After\n+  public void clearMountTable() throws IOException {\n+    RouterClient client = routerContext.getAdminClient();\n+    MountTableManager mountTableManager = client.getMountTableManager();\n+    GetMountTableEntriesRequest req1 =\n+        GetMountTableEntriesRequest.newInstance(\"/\");\n+    GetMountTableEntriesResponse response =\n+        mountTableManager.getMountTableEntries(req1);\n+    for (MountTable entry : response.getEntries()) {\n+      RemoveMountTableEntryRequest req2 =\n+          RemoveMountTableEntryRequest.newInstance(entry.getSourcePath());\n+      mountTableManager.removeMountTableEntry(req2);\n+    }\n+  }\n+\n+  private boolean addMountTable(final MountTable entry) throws IOException {\n+    RouterClient client = routerContext.getAdminClient();\n+    MountTableManager mountTableManager = client.getMountTableManager();\n+    AddMountTableEntryRequest addRequest =\n+        AddMountTableEntryRequest.newInstance(entry);\n+    AddMountTableEntryResponse addResponse =\n+        mountTableManager.addMountTableEntry(addRequest);\n+    // Reload the Router cache\n+    mountTable.loadCache(true);\n+    return addResponse.getStatus();\n+  }\n+\n+  @Test\n+  public void testFsck() throws Exception {\n+    MountTable addEntry = MountTable.newInstance(\"/testdir\",\n+        Collections.singletonMap(\"ns0\", \"/testdir\"));\n+    Assert.assertTrue(addMountTable(addEntry));\n+    addEntry = MountTable.newInstance(\"/testdir2\",\n+        Collections.singletonMap(\"ns1\", \"/testdir2\"));\n+    Assert.assertTrue(addMountTable(addEntry));\n+    // create 1 file on ns0\n+    routerFs.createNewFile(new Path(\"/testdir/testfile\"));\n+    // create 3 files on ns1\n+    routerFs.createNewFile(new Path(\"/testdir2/testfile2\"));\n+    routerFs.createNewFile(new Path(\"/testdir2/testfile3\"));\n+    routerFs.createNewFile(new Path(\"/testdir2/testfile4\"));\n+\n+    try (CloseableHttpClient httpClient = HttpClients.createDefault()) {\n+      // TODO: support https\n+      HttpGet httpGet = new HttpGet(\"http://\" + webAddress.getHostName() +\n+              \":\" + webAddress.getPort() + \"/fsck\");\n+      try (CloseableHttpResponse httpResponse = httpClient.execute(httpGet)) {\n+        Assert.assertEquals(HttpStatus.SC_OK,\n+            httpResponse.getStatusLine().getStatusCode());\n+        String out = EntityUtils.toString(\n+            httpResponse.getEntity(), StandardCharsets.UTF_8);\n+        System.out.println(out);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d697bee4bb006190297e2fa4d918773caac5c93c"}, "originalPosition": 147}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMyNjEzOTY3OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterFsck.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNlQyMTozMzozMFrOFmq9jA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QwNDo1MjoxMlrOFmyQ_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjA5NDA5Mg==", "bodyText": "Can we check for this with the webAddress in the unit test?", "url": "https://github.com/apache/hadoop/pull/1832#discussion_r376094092", "createdAt": "2020-02-06T21:33:30Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterFsck.java", "diffHunk": "@@ -0,0 +1,157 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.federation.router;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.io.PrintWriter;\n+import java.net.InetAddress;\n+import java.net.URL;\n+import java.net.URLConnection;\n+import java.util.Collections;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.hdfs.server.federation.resolver.FederationNamenodeServiceState;\n+import org.apache.hadoop.hdfs.server.federation.store.MembershipStore;\n+import org.apache.hadoop.hdfs.server.federation.store.StateStoreService;\n+import org.apache.hadoop.hdfs.server.federation.store.protocol.GetNamenodeRegistrationsRequest;\n+import org.apache.hadoop.hdfs.server.federation.store.protocol.GetNamenodeRegistrationsResponse;\n+import org.apache.hadoop.hdfs.server.federation.store.records.MembershipState;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.util.Time;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Wrapper for the Router to offer the Namenode FSCK.\n+ */\n+@InterfaceAudience.Private\n+public class RouterFsck {\n+\n+  public static final Logger LOG =\n+      LoggerFactory.getLogger(RouterFsck.class.getName());\n+\n+  private final Router router;\n+  private final InetAddress remoteAddress;\n+  private final PrintWriter out;\n+  private final Map<String, String[]> pmap;\n+\n+  public RouterFsck(Router router, Map<String, String[]> pmap,\n+                    PrintWriter out, InetAddress remoteAddress) {\n+    this.router = router;\n+    this.remoteAddress = remoteAddress;\n+    this.out = out;\n+    this.pmap = pmap;\n+  }\n+\n+  public void fsck() {\n+    final long startTime = Time.monotonicNow();\n+    try {\n+      String msg = \"Federated FSCK started by \" +\n+          UserGroupInformation.getCurrentUser() + \" from \" + remoteAddress +\n+          \" at \" + new Date();\n+      LOG.info(msg);\n+      out.println(msg);\n+\n+      // Check each Namenode in the federation\n+      StateStoreService stateStore = router.getStateStore();\n+      MembershipStore membership =\n+          stateStore.getRegisteredRecordStore(MembershipStore.class);\n+      GetNamenodeRegistrationsRequest request =\n+          GetNamenodeRegistrationsRequest.newInstance();\n+      GetNamenodeRegistrationsResponse response =\n+          membership.getNamenodeRegistrations(request);\n+      List<MembershipState> memberships = response.getNamenodeMemberships();\n+      Collections.sort(memberships);\n+      for (MembershipState nn : memberships) {\n+        if (nn.getState() == FederationNamenodeServiceState.ACTIVE) {\n+          try {\n+            String webAddress = nn.getWebAddress();\n+            out.write(\"Checking \" + nn + \" at \" + webAddress + \"\\n\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d697bee4bb006190297e2fa4d918773caac5c93c"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjIxMzc1Ng==", "bodyText": "Added the check in the unit test", "url": "https://github.com/apache/hadoop/pull/1832#discussion_r376213756", "createdAt": "2020-02-07T04:52:12Z", "author": {"login": "aajisaka"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterFsck.java", "diffHunk": "@@ -0,0 +1,157 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.federation.router;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.io.PrintWriter;\n+import java.net.InetAddress;\n+import java.net.URL;\n+import java.net.URLConnection;\n+import java.util.Collections;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.hdfs.server.federation.resolver.FederationNamenodeServiceState;\n+import org.apache.hadoop.hdfs.server.federation.store.MembershipStore;\n+import org.apache.hadoop.hdfs.server.federation.store.StateStoreService;\n+import org.apache.hadoop.hdfs.server.federation.store.protocol.GetNamenodeRegistrationsRequest;\n+import org.apache.hadoop.hdfs.server.federation.store.protocol.GetNamenodeRegistrationsResponse;\n+import org.apache.hadoop.hdfs.server.federation.store.records.MembershipState;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.util.Time;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Wrapper for the Router to offer the Namenode FSCK.\n+ */\n+@InterfaceAudience.Private\n+public class RouterFsck {\n+\n+  public static final Logger LOG =\n+      LoggerFactory.getLogger(RouterFsck.class.getName());\n+\n+  private final Router router;\n+  private final InetAddress remoteAddress;\n+  private final PrintWriter out;\n+  private final Map<String, String[]> pmap;\n+\n+  public RouterFsck(Router router, Map<String, String[]> pmap,\n+                    PrintWriter out, InetAddress remoteAddress) {\n+    this.router = router;\n+    this.remoteAddress = remoteAddress;\n+    this.out = out;\n+    this.pmap = pmap;\n+  }\n+\n+  public void fsck() {\n+    final long startTime = Time.monotonicNow();\n+    try {\n+      String msg = \"Federated FSCK started by \" +\n+          UserGroupInformation.getCurrentUser() + \" from \" + remoteAddress +\n+          \" at \" + new Date();\n+      LOG.info(msg);\n+      out.println(msg);\n+\n+      // Check each Namenode in the federation\n+      StateStoreService stateStore = router.getStateStore();\n+      MembershipStore membership =\n+          stateStore.getRegisteredRecordStore(MembershipStore.class);\n+      GetNamenodeRegistrationsRequest request =\n+          GetNamenodeRegistrationsRequest.newInstance();\n+      GetNamenodeRegistrationsResponse response =\n+          membership.getNamenodeRegistrations(request);\n+      List<MembershipState> memberships = response.getNamenodeMemberships();\n+      Collections.sort(memberships);\n+      for (MembershipState nn : memberships) {\n+        if (nn.getState() == FederationNamenodeServiceState.ACTIVE) {\n+          try {\n+            String webAddress = nn.getWebAddress();\n+            out.write(\"Checking \" + nn + \" at \" + webAddress + \"\\n\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjA5NDA5Mg=="}, "originalCommit": {"oid": "d697bee4bb006190297e2fa4d918773caac5c93c"}, "originalPosition": 91}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMyOTEzOTY0OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterFsck.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QxOToxOTowOFrOFnHi5w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QxOToxOTowOFrOFnHi5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjU2MjQwNw==", "bodyText": "Too long.", "url": "https://github.com/apache/hadoop/pull/1832#discussion_r376562407", "createdAt": "2020-02-07T19:19:08Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterFsck.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.federation.router;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.io.PrintWriter;\n+import java.net.InetAddress;\n+import java.net.URL;\n+import java.net.URLConnection;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Collections;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.hdfs.server.federation.resolver.FederationNamenodeServiceState;\n+import org.apache.hadoop.hdfs.server.federation.store.MembershipStore;\n+import org.apache.hadoop.hdfs.server.federation.store.StateStoreService;\n+import org.apache.hadoop.hdfs.server.federation.store.protocol.GetNamenodeRegistrationsRequest;\n+import org.apache.hadoop.hdfs.server.federation.store.protocol.GetNamenodeRegistrationsResponse;\n+import org.apache.hadoop.hdfs.server.federation.store.records.MembershipState;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.util.Time;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Wrapper for the Router to offer the Namenode FSCK.\n+ */\n+@InterfaceAudience.Private\n+public class RouterFsck {\n+\n+  public static final Logger LOG =\n+      LoggerFactory.getLogger(RouterFsck.class.getName());\n+\n+  private final Router router;\n+  private final InetAddress remoteAddress;\n+  private final PrintWriter out;\n+  private final Map<String, String[]> pmap;\n+\n+  public RouterFsck(Router router, Map<String, String[]> pmap,\n+                    PrintWriter out, InetAddress remoteAddress) {\n+    this.router = router;\n+    this.remoteAddress = remoteAddress;\n+    this.out = out;\n+    this.pmap = pmap;\n+  }\n+\n+  public void fsck() {\n+    final long startTime = Time.monotonicNow();\n+    try {\n+      String msg = \"Federated FSCK started by \" +\n+          UserGroupInformation.getCurrentUser() + \" from \" + remoteAddress +\n+          \" at \" + new Date();\n+      LOG.info(msg);\n+      out.println(msg);\n+\n+      // Check each Namenode in the federation\n+      StateStoreService stateStore = router.getStateStore();\n+      MembershipStore membership =\n+          stateStore.getRegisteredRecordStore(MembershipStore.class);\n+      GetNamenodeRegistrationsRequest request =\n+          GetNamenodeRegistrationsRequest.newInstance();\n+      GetNamenodeRegistrationsResponse response =\n+          membership.getNamenodeRegistrations(request);\n+      List<MembershipState> memberships = response.getNamenodeMemberships();\n+      Collections.sort(memberships);\n+      for (MembershipState nn : memberships) {\n+        if (nn.getState() == FederationNamenodeServiceState.ACTIVE) {\n+          try {\n+            String webAddress = nn.getWebAddress();\n+            out.write(\"Checking \" + nn + \" at \" + webAddress + \"\\n\");\n+            remoteFsck(nn);\n+          } catch (IOException ioe) {\n+            out.println(\"Cannot query \" + nn + \": \" + ioe.getMessage() + \"\\n\");\n+          }\n+        }\n+      }\n+\n+      out.println(\"Federated FSCK ended at \" + new Date() + \" in \"\n+          + (Time.monotonicNow() - startTime + \" milliseconds\"));\n+    } catch (Exception e) {\n+      String errMsg = \"Fsck \" + e.getMessage();\n+      LOG.warn(errMsg, e);\n+      out.println(\"Federated FSCK ended at \" + new Date() + \" in \"\n+          + (Time.monotonicNow() - startTime + \" milliseconds\"));\n+      out.println(e.getMessage());\n+      out.print(\"\\n\\n\" + errMsg);\n+    } finally {\n+      out.close();\n+    }\n+  }\n+\n+  /**\n+   * Perform FSCK in a remote Namenode.\n+   *\n+   * @param nn The state of the remote NameNode\n+   * @throws IOException Failed to fsck in a remote NameNode\n+   */\n+  private void remoteFsck(MembershipState nn) throws IOException {\n+    final String scheme = nn.getWebScheme();\n+    final String webAddress = nn.getWebAddress();\n+    final String args = getURLArguments(pmap);\n+    final URL url = new URL(scheme + \"://\" + webAddress + \"/fsck?\" + args);\n+\n+    // Connect to the Namenode and output\n+    final URLConnection conn = url.openConnection();\n+    try (InputStream is = conn.getInputStream();\n+         InputStreamReader isr = new InputStreamReader(is, StandardCharsets.UTF_8);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d8e25fa7cba84c2b350c4b97f22524b29cee2fa"}, "originalPosition": 129}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMyOTE0MDI4OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterFsckServlet.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QxOToxOToyNFrOFnHjVQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QxOToxOToyNFrOFnHjVQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjU2MjUxNw==", "bodyText": "First sentence should end with a period. [JavadocStyle]", "url": "https://github.com/apache/hadoop/pull/1832#discussion_r376562517", "createdAt": "2020-02-07T19:19:24Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterFsckServlet.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.federation.router;\n+\n+import java.io.IOException;\n+import java.io.PrintWriter;\n+import java.net.InetAddress;\n+import java.security.PrivilegedExceptionAction;\n+import java.util.Map;\n+\n+import javax.servlet.ServletContext;\n+import javax.servlet.http.HttpServlet;\n+import javax.servlet.http.HttpServletRequest;\n+import javax.servlet.http.HttpServletResponse;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hdfs.server.common.JspHelper;\n+import org.apache.hadoop.security.UserGroupInformation;\n+\n+/**\n+ * This class is used in Namesystem's web server to do fsck on namenode.\n+ */\n+@InterfaceAudience.Private\n+public class RouterFsckServlet extends HttpServlet {\n+  /** for java.io.Serializable */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d8e25fa7cba84c2b350c4b97f22524b29cee2fa"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMyOTE0MTQ5OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterFsckServlet.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QxOToxOTo1MlrOFnHkJw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QxOToxOTo1MlrOFnHkJw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjU2MjcyNw==", "bodyText": "Bad indent.\nCheck other checkstyles:\nhttps://builds.apache.org/job/hadoop-multibranch/job/PR-1832/2/artifact/out/diff-checkstyle-hadoop-hdfs-project.txt", "url": "https://github.com/apache/hadoop/pull/1832#discussion_r376562727", "createdAt": "2020-02-07T19:19:52Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterFsckServlet.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.federation.router;\n+\n+import java.io.IOException;\n+import java.io.PrintWriter;\n+import java.net.InetAddress;\n+import java.security.PrivilegedExceptionAction;\n+import java.util.Map;\n+\n+import javax.servlet.ServletContext;\n+import javax.servlet.http.HttpServlet;\n+import javax.servlet.http.HttpServletRequest;\n+import javax.servlet.http.HttpServletResponse;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hdfs.server.common.JspHelper;\n+import org.apache.hadoop.security.UserGroupInformation;\n+\n+/**\n+ * This class is used in Namesystem's web server to do fsck on namenode.\n+ */\n+@InterfaceAudience.Private\n+public class RouterFsckServlet extends HttpServlet {\n+  /** for java.io.Serializable */\n+  private static final long serialVersionUID = 1L;\n+\n+  public static final String SERVLET_NAME = \"fsck\";\n+  public static final String PATH_SPEC = \"/fsck\";\n+\n+  /** Handle fsck request */\n+  @Override\n+  public void doGet(HttpServletRequest request, HttpServletResponse response\n+      ) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d8e25fa7cba84c2b350c4b97f22524b29cee2fa"}, "originalPosition": 50}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzMzU3MzY0OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterFsckServlet.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQxNzo0NjowOVrOFnvfUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQxNzo0NzoxM1rOFnvhvA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzIxNjg0OQ==", "bodyText": "HttpURLConnection.HTTP_BAD_REQUEST in java.net.", "url": "https://github.com/apache/hadoop/pull/1832#discussion_r377216849", "createdAt": "2020-02-10T17:46:09Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterFsckServlet.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.federation.router;\n+\n+import java.io.IOException;\n+import java.io.PrintWriter;\n+import java.net.InetAddress;\n+import java.security.PrivilegedExceptionAction;\n+import java.util.Map;\n+\n+import javax.servlet.ServletContext;\n+import javax.servlet.http.HttpServlet;\n+import javax.servlet.http.HttpServletRequest;\n+import javax.servlet.http.HttpServletResponse;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hdfs.server.common.JspHelper;\n+import org.apache.hadoop.security.UserGroupInformation;\n+\n+/**\n+ * This class is used in Namesystem's web server to do fsck on namenode.\n+ */\n+@InterfaceAudience.Private\n+public class RouterFsckServlet extends HttpServlet {\n+  /** for java.io.Serializable. */\n+  private static final long serialVersionUID = 1L;\n+\n+  public static final String SERVLET_NAME = \"fsck\";\n+  public static final String PATH_SPEC = \"/fsck\";\n+\n+  /** Handle fsck request. */\n+  @Override\n+  public void doGet(HttpServletRequest request, HttpServletResponse response)\n+      throws IOException {\n+    final Map<String, String[]> pmap = request.getParameterMap();\n+    final PrintWriter out = response.getWriter();\n+    final InetAddress remoteAddress =\n+        InetAddress.getByName(request.getRemoteAddr());\n+    final ServletContext context = getServletContext();\n+    final Configuration conf = RouterHttpServer.getConfFromContext(context);\n+    final UserGroupInformation ugi = getUGI(request, conf);\n+    try {\n+      ugi.doAs((PrivilegedExceptionAction<Object>) () -> {\n+        Router router = RouterHttpServer.getRouterFromContext(context);\n+        new RouterFsck(router, pmap, out, remoteAddress).fsck();\n+        return null;\n+      });\n+    } catch (InterruptedException e) {\n+      response.sendError(400, e.getMessage());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "699ac37e698ef96062e5012b6b1301f8d1f53c81"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzIxNzQ2OA==", "bodyText": "Or HttpStatus.SC_BAD_REQUEST if you want to follow the same as the unit test.", "url": "https://github.com/apache/hadoop/pull/1832#discussion_r377217468", "createdAt": "2020-02-10T17:47:13Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterFsckServlet.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.federation.router;\n+\n+import java.io.IOException;\n+import java.io.PrintWriter;\n+import java.net.InetAddress;\n+import java.security.PrivilegedExceptionAction;\n+import java.util.Map;\n+\n+import javax.servlet.ServletContext;\n+import javax.servlet.http.HttpServlet;\n+import javax.servlet.http.HttpServletRequest;\n+import javax.servlet.http.HttpServletResponse;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hdfs.server.common.JspHelper;\n+import org.apache.hadoop.security.UserGroupInformation;\n+\n+/**\n+ * This class is used in Namesystem's web server to do fsck on namenode.\n+ */\n+@InterfaceAudience.Private\n+public class RouterFsckServlet extends HttpServlet {\n+  /** for java.io.Serializable. */\n+  private static final long serialVersionUID = 1L;\n+\n+  public static final String SERVLET_NAME = \"fsck\";\n+  public static final String PATH_SPEC = \"/fsck\";\n+\n+  /** Handle fsck request. */\n+  @Override\n+  public void doGet(HttpServletRequest request, HttpServletResponse response)\n+      throws IOException {\n+    final Map<String, String[]> pmap = request.getParameterMap();\n+    final PrintWriter out = response.getWriter();\n+    final InetAddress remoteAddress =\n+        InetAddress.getByName(request.getRemoteAddr());\n+    final ServletContext context = getServletContext();\n+    final Configuration conf = RouterHttpServer.getConfFromContext(context);\n+    final UserGroupInformation ugi = getUGI(request, conf);\n+    try {\n+      ugi.doAs((PrivilegedExceptionAction<Object>) () -> {\n+        Router router = RouterHttpServer.getRouterFromContext(context);\n+        new RouterFsck(router, pmap, out, remoteAddress).fsck();\n+        return null;\n+      });\n+    } catch (InterruptedException e) {\n+      response.sendError(400, e.getMessage());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzIxNjg0OQ=="}, "originalCommit": {"oid": "699ac37e698ef96062e5012b6b1301f8d1f53c81"}, "originalPosition": 65}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3699, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}