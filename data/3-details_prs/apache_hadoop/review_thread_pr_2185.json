{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDYxOTMzNDM4", "number": 2185, "reviewThreads": {"totalCount": 34, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNzozMzo1N1rOEWTEPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQwNDo1NzowNlrOEh8vGA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxODE2NTExOnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/InodeTree.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNzozMzo1N1rOG9iM1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNzozMzo1N1rOG9iM1Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE3NjY2MQ==", "bodyText": "It looks complex. But the major change here is simple.\nBefore:\nif (key.startsWith(mountTablePrefix)) {\n// hundred line of code\n}\nAfter:\nif (!key.startsWith(mountTablePrefix)) {\ncontinue\n}\n// hundred line of code", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r467176661", "createdAt": "2020-08-07T17:33:57Z", "author": {"login": "JohnZZGithub"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/InodeTree.java", "diffHunk": "@@ -486,84 +506,113 @@ protected InodeTree(final Configuration config, final String viewName,\n     final UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\n     for (Entry<String, String> si : config) {\n       final String key = si.getKey();\n-      if (key.startsWith(mountTablePrefix)) {\n-        gotMountTableEntry = true;\n-        LinkType linkType;\n-        String src = key.substring(mountTablePrefix.length());\n-        String settings = null;\n-        if (src.startsWith(linkPrefix)) {\n-          src = src.substring(linkPrefix.length());\n-          if (src.equals(SlashPath.toString())) {\n-            throw new UnsupportedFileSystemException(\"Unexpected mount table \"\n-                + \"link entry '\" + key + \"'. Use \"\n-                + Constants.CONFIG_VIEWFS_LINK_MERGE_SLASH  + \" instead!\");\n-          }\n-          linkType = LinkType.SINGLE;\n-        } else if (src.startsWith(linkFallbackPrefix)) {\n-          if (src.length() != linkFallbackPrefix.length()) {\n-            throw new IOException(\"ViewFs: Mount points initialization error.\" +\n-                \" Invalid \" + Constants.CONFIG_VIEWFS_LINK_FALLBACK +\n-                \" entry in config: \" + src);\n-          }\n-          linkType = LinkType.SINGLE_FALLBACK;\n-        } else if (src.startsWith(linkMergePrefix)) { // A merge link\n-          src = src.substring(linkMergePrefix.length());\n-          linkType = LinkType.MERGE;\n-        } else if (src.startsWith(linkMergeSlashPrefix)) {\n-          // This is a LinkMergeSlash entry. This entry should\n-          // not have any additional source path.\n-          if (src.length() != linkMergeSlashPrefix.length()) {\n-            throw new IOException(\"ViewFs: Mount points initialization error.\" +\n-                \" Invalid \" + Constants.CONFIG_VIEWFS_LINK_MERGE_SLASH +\n-                \" entry in config: \" + src);\n-          }\n-          linkType = LinkType.MERGE_SLASH;\n-        } else if (src.startsWith(Constants.CONFIG_VIEWFS_LINK_NFLY)) {\n-          // prefix.settings.src\n-          src = src.substring(Constants.CONFIG_VIEWFS_LINK_NFLY.length() + 1);\n-          // settings.src\n-          settings = src.substring(0, src.indexOf('.'));\n-          // settings\n-\n-          // settings.src\n-          src = src.substring(settings.length() + 1);\n-          // src\n-\n-          linkType = LinkType.NFLY;\n-        } else if (src.startsWith(Constants.CONFIG_VIEWFS_HOMEDIR)) {\n-          // ignore - we set home dir from config\n-          continue;\n-        } else {\n-          throw new IOException(\"ViewFs: Cannot initialize: Invalid entry in \" +\n-              \"Mount table in config: \" + src);\n-        }\n+      if (!key.startsWith(mountTablePrefix)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d3e5b8331000f9189c7d2073d08a281532f36a8d"}, "originalPosition": 153}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyMjA3NjU5OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/InodeTree.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwNjozNjo0NFrOG-Cy8Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNzoyNDoyNFrOHCenHg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzcxMDcwNQ==", "bodyText": "you mean fs.viewfs.mounttable.<mnt_tbl_name>.linkRegex ?", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r467710705", "createdAt": "2020-08-10T06:36:44Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/InodeTree.java", "diffHunk": "@@ -226,7 +239,14 @@ void addLink(final String pathComponent, final INodeLink<T> link)\n      * Config prefix: fs.viewfs.mounttable.<mnt_tbl_name>.linkNfly\n      * Refer: {@link Constants#CONFIG_VIEWFS_LINK_NFLY}\n      */\n-    NFLY;\n+    NFLY,\n+    /**\n+     * Link entry which source are regex exrepssions and target refer matched\n+     * group from source\n+     * Config prefix: fs.viewfs.mounttable.<mnt_tbl_name>.linkMerge", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d3e5b8331000f9189c7d2073d08a281532f36a8d"}, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjM2MDczNA==", "bodyText": "Yes, nice catch.", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r472360734", "createdAt": "2020-08-18T17:24:24Z", "author": {"login": "JohnZZGithub"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/InodeTree.java", "diffHunk": "@@ -226,7 +239,14 @@ void addLink(final String pathComponent, final INodeLink<T> link)\n      * Config prefix: fs.viewfs.mounttable.<mnt_tbl_name>.linkNfly\n      * Refer: {@link Constants#CONFIG_VIEWFS_LINK_NFLY}\n      */\n-    NFLY;\n+    NFLY,\n+    /**\n+     * Link entry which source are regex exrepssions and target refer matched\n+     * group from source\n+     * Config prefix: fs.viewfs.mounttable.<mnt_tbl_name>.linkMerge", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzcxMDcwNQ=="}, "originalCommit": {"oid": "d3e5b8331000f9189c7d2073d08a281532f36a8d"}, "originalPosition": 82}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0NTYwMTEyOnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QwNjo1OTo1MVrOHBb7qQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QyMzoxODo0MFrOHB99pA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTI2ODI2NQ==", "bodyText": "Below params javadoc does not help anything I think. If you want to add, please add description( most of that params seems self explanatory ), otherwise just remove params part.", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471268265", "createdAt": "2020-08-17T06:59:51Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java", "diffHunk": "@@ -166,6 +166,41 @@ public static void addLinkNfly(final Configuration conf, final String src,\n     addLinkNfly(conf, getDefaultMountTableName(conf), src, null, targets);\n   }\n \n+\n+  /**\n+   * Add a LinkRegex to the config for the specified mount table.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c58b945a32a978c470be07ef47e9ed7edfff428c"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTgyNTgyOA==", "bodyText": "Make sense, thanks.", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471825828", "createdAt": "2020-08-17T23:18:40Z", "author": {"login": "JohnZZGithub"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java", "diffHunk": "@@ -166,6 +166,41 @@ public static void addLinkNfly(final Configuration conf, final String src,\n     addLinkNfly(conf, getDefaultMountTableName(conf), src, null, targets);\n   }\n \n+\n+  /**\n+   * Add a LinkRegex to the config for the specified mount table.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTI2ODI2NQ=="}, "originalCommit": {"oid": "c58b945a32a978c470be07ef47e9ed7edfff428c"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0NTg0NTg2OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/InodeTree.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QwNzo1Nzo1NFrOHBePvw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QwNzo1Nzo1NFrOHBePvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMwNjE3NQ==", "bodyText": "below key construction can be outside of lock?", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471306175", "createdAt": "2020-08-17T07:57:54Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/InodeTree.java", "diffHunk": "@@ -646,102 +714,222 @@ boolean isInternalDir() {\n   }\n \n   /**\n-   * Resolve the pathname p relative to root InodeDir\n+   * Resolve the pathname p relative to root InodeDir.\n    * @param p - input path\n    * @param resolveLastComponent\n    * @return ResolveResult which allows further resolution of the remaining path\n    * @throws FileNotFoundException\n    */\n   ResolveResult<T> resolve(final String p, final boolean resolveLastComponent)\n       throws FileNotFoundException {\n-    String[] path = breakIntoPathComponents(p);\n-    if (path.length <= 1) { // special case for when path is \"/\"\n-      T targetFs = root.isInternalDir() ?\n-          getRootDir().getInternalDirFs() : getRootLink().getTargetFileSystem();\n-      ResolveResult<T> res = new ResolveResult<T>(ResultKind.INTERNAL_DIR,\n-          targetFs, root.fullPath, SlashPath);\n-      return res;\n-    }\n+    ResolveResult<T> resolveResult = null;\n+    resolveResult = getResolveResultFromCache(p, resolveLastComponent);\n+    if (resolveResult != null) {\n+      return resolveResult;\n+    }\n+\n+    try {\n+      String[] path = breakIntoPathComponents(p);\n+      if (path.length <= 1) { // special case for when path is \"/\"\n+        T targetFs = root.isInternalDir() ?\n+            getRootDir().getInternalDirFs()\n+            : getRootLink().getTargetFileSystem();\n+        resolveResult = new ResolveResult<T>(ResultKind.INTERNAL_DIR,\n+            targetFs, root.fullPath, SlashPath);\n+        return resolveResult;\n+      }\n \n-    /**\n-     * linkMergeSlash has been configured. The root of this mount table has\n-     * been linked to the root directory of a file system.\n-     * The first non-slash path component should be name of the mount table.\n-     */\n-    if (root.isLink()) {\n-      Path remainingPath;\n-      StringBuilder remainingPathStr = new StringBuilder();\n-      // ignore first slash\n-      for (int i = 1; i < path.length; i++) {\n-        remainingPathStr.append(\"/\").append(path[i]);\n+      /**\n+       * linkMergeSlash has been configured. The root of this mount table has\n+       * been linked to the root directory of a file system.\n+       * The first non-slash path component should be name of the mount table.\n+       */\n+      if (root.isLink()) {\n+        Path remainingPath;\n+        StringBuilder remainingPathStr = new StringBuilder();\n+        // ignore first slash\n+        for (int i = 1; i < path.length; i++) {\n+          remainingPathStr.append(\"/\").append(path[i]);\n+        }\n+        remainingPath = new Path(remainingPathStr.toString());\n+        resolveResult = new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n+            getRootLink().getTargetFileSystem(), root.fullPath, remainingPath);\n+        return resolveResult;\n       }\n-      remainingPath = new Path(remainingPathStr.toString());\n-      ResolveResult<T> res = new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n-          getRootLink().getTargetFileSystem(), root.fullPath, remainingPath);\n-      return res;\n-    }\n-    Preconditions.checkState(root.isInternalDir());\n-    INodeDir<T> curInode = getRootDir();\n+      Preconditions.checkState(root.isInternalDir());\n+      INodeDir<T> curInode = getRootDir();\n \n-    int i;\n-    // ignore first slash\n-    for (i = 1; i < path.length - (resolveLastComponent ? 0 : 1); i++) {\n-      INode<T> nextInode = curInode.resolveInternal(path[i]);\n-      if (nextInode == null) {\n-        if (hasFallbackLink()) {\n-          return new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n-              getRootFallbackLink().getTargetFileSystem(),\n-              root.fullPath, new Path(p));\n-        } else {\n-          StringBuilder failedAt = new StringBuilder(path[0]);\n-          for (int j = 1; j <= i; ++j) {\n-            failedAt.append('/').append(path[j]);\n+      // Try to resolve path in the regex mount point\n+      resolveResult = tryResolveInRegexMountpoint(p, resolveLastComponent);\n+      if (resolveResult != null) {\n+        return resolveResult;\n+      }\n+\n+      int i;\n+      // ignore first slash\n+      for (i = 1; i < path.length - (resolveLastComponent ? 0 : 1); i++) {\n+        INode<T> nextInode = curInode.resolveInternal(path[i]);\n+        if (nextInode == null) {\n+          if (hasFallbackLink()) {\n+            resolveResult = new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n+                getRootFallbackLink().getTargetFileSystem(), root.fullPath,\n+                new Path(p));\n+            return resolveResult;\n+          } else {\n+            StringBuilder failedAt = new StringBuilder(path[0]);\n+            for (int j = 1; j <= i; ++j) {\n+              failedAt.append('/').append(path[j]);\n+            }\n+            throw (new FileNotFoundException(\n+                \"File/Directory does not exist: \" + failedAt.toString()));\n           }\n-          throw (new FileNotFoundException(\n-              \"File/Directory does not exist: \" + failedAt.toString()));\n         }\n-      }\n \n-      if (nextInode.isLink()) {\n-        final INodeLink<T> link = (INodeLink<T>) nextInode;\n-        final Path remainingPath;\n-        if (i >= path.length - 1) {\n-          remainingPath = SlashPath;\n-        } else {\n-          StringBuilder remainingPathStr = new StringBuilder(\"/\" + path[i + 1]);\n-          for (int j = i + 2; j < path.length; ++j) {\n-            remainingPathStr.append('/').append(path[j]);\n+        if (nextInode.isLink()) {\n+          final INodeLink<T> link = (INodeLink<T>) nextInode;\n+          final Path remainingPath;\n+          if (i >= path.length - 1) {\n+            remainingPath = SlashPath;\n+          } else {\n+            StringBuilder remainingPathStr =\n+                new StringBuilder(\"/\" + path[i + 1]);\n+            for (int j = i + 2; j < path.length; ++j) {\n+              remainingPathStr.append('/').append(path[j]);\n+            }\n+            remainingPath = new Path(remainingPathStr.toString());\n           }\n-          remainingPath = new Path(remainingPathStr.toString());\n+          resolveResult = new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n+              link.getTargetFileSystem(), nextInode.fullPath, remainingPath);\n+          return resolveResult;\n+        } else if (nextInode.isInternalDir()) {\n+          curInode = (INodeDir<T>) nextInode;\n         }\n-        final ResolveResult<T> res =\n-            new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n-                link.getTargetFileSystem(), nextInode.fullPath, remainingPath);\n-        return res;\n-      } else if (nextInode.isInternalDir()) {\n-        curInode = (INodeDir<T>) nextInode;\n+      }\n+\n+      // We have resolved to an internal dir in mount table.\n+      Path remainingPath;\n+      if (resolveLastComponent) {\n+        remainingPath = SlashPath;\n+      } else {\n+        // note we have taken care of when path is \"/\" above\n+        // for internal dirs rem-path does not start with / since the lookup\n+        // that follows will do a children.get(remaningPath) and will have to\n+        // strip-out the initial /\n+        StringBuilder remainingPathStr = new StringBuilder(\"/\" + path[i]);\n+        for (int j = i + 1; j < path.length; ++j) {\n+          remainingPathStr.append('/').append(path[j]);\n+        }\n+        remainingPath = new Path(remainingPathStr.toString());\n+      }\n+      resolveResult = new ResolveResult<T>(ResultKind.INTERNAL_DIR,\n+          curInode.getInternalDirFs(), curInode.fullPath, remainingPath);\n+      return resolveResult;\n+    } finally {\n+      if (pathResolutionCacheCapacity > 0 && resolveResult != null) {\n+        addResolveResultToCache(p, resolveLastComponent, resolveResult);\n       }\n     }\n+  }\n \n-    // We have resolved to an internal dir in mount table.\n-    Path remainingPath;\n-    if (resolveLastComponent) {\n-      remainingPath = SlashPath;\n-    } else {\n-      // note we have taken care of when path is \"/\" above\n-      // for internal dirs rem-path does not start with / since the lookup\n-      // that follows will do a children.get(remaningPath) and will have to\n-      // strip-out the initial /\n-      StringBuilder remainingPathStr = new StringBuilder(\"/\" + path[i]);\n-      for (int j = i + 1; j < path.length; ++j) {\n-        remainingPathStr.append('/').append(path[j]);\n+  /**\n+   * Walk through all regex mount points to see\n+   * whether the path match any regex expressions.\n+   *\n+   * @param srcPath\n+   * @param resolveLastComponent\n+   * @return\n+   */\n+  protected ResolveResult<T> tryResolveInRegexMountpoint(final String srcPath,\n+      final boolean resolveLastComponent) {\n+    for (RegexMountPoint regexMountPoint : regexMountPointList) {\n+      ResolveResult resolveResult =\n+          regexMountPoint.resolve(srcPath, resolveLastComponent);\n+      if (resolveResult != null) {\n+        return resolveResult;\n       }\n-      remainingPath = new Path(remainingPathStr.toString());\n     }\n-    final ResolveResult<T> res =\n-        new ResolveResult<T>(ResultKind.INTERNAL_DIR,\n-            curInode.getInternalDirFs(), curInode.fullPath, remainingPath);\n-    return res;\n+    return null;\n+  }\n+\n+  /**\n+   * Build resolve result return to caller.\n+   *\n+   * @param resultKind\n+   * @param resolvedPathStr\n+   * @param targetOfResolvedPathStr\n+   * @param remainingPath\n+   * @return\n+   */\n+  protected ResolveResult<T> buildResolveResultForRegexMountPoint(\n+      ResultKind resultKind, String resolvedPathStr,\n+      String targetOfResolvedPathStr, Path remainingPath) {\n+    try {\n+      T targetFs = getTargetFileSystem(new URI(targetOfResolvedPathStr));\n+      return new ResolveResult<T>(resultKind, targetFs, resolvedPathStr,\n+          remainingPath);\n+    } catch (IOException ex) {\n+      LOGGER.error(String.format(\n+          \"Got Exception while build resolve result.\"\n+              + \" ResultKind:%s, resolvedPathStr:%s,\"\n+              + \" targetOfResolvedPathStr:%s, remainingPath:%s,\"\n+              + \" will return null.\",\n+          resultKind, resolvedPathStr, targetOfResolvedPathStr, remainingPath),\n+          ex);\n+      return null;\n+    } catch (URISyntaxException uex) {\n+      LOGGER.error(String.format(\n+          \"Got Exception while build resolve result.\"\n+              + \" ResultKind:%s, resolvedPathStr:%s,\"\n+              + \" targetOfResolvedPathStr:%s, remainingPath:%s,\"\n+              + \" will return null.\",\n+          resultKind, resolvedPathStr, targetOfResolvedPathStr, remainingPath),\n+          uex);\n+      return null;\n+    }\n+  }\n+\n+  /**\n+   * Return resolution cache capacity.\n+   *\n+   * @return\n+   */\n+  public int getPathResolutionCacheCapacity() {\n+    return pathResolutionCacheCapacity;\n+  }\n+\n+  private void addResolveResultToCache(final String pathStr,\n+      final Boolean resolveLastComponent,\n+      final ResolveResult<T> resolveResult) {\n+    try {\n+      cacheRWLock.writeLock().lock();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c58b945a32a978c470be07ef47e9ed7edfff428c"}, "originalPosition": 596}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0NTg0NzIzOnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/InodeTree.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QwNzo1ODoyMFrOHBeQkw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QwNzo1ODoyMFrOHBeQkw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMwNjM4Nw==", "bodyText": "same as above.", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471306387", "createdAt": "2020-08-17T07:58:20Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/InodeTree.java", "diffHunk": "@@ -646,102 +714,222 @@ boolean isInternalDir() {\n   }\n \n   /**\n-   * Resolve the pathname p relative to root InodeDir\n+   * Resolve the pathname p relative to root InodeDir.\n    * @param p - input path\n    * @param resolveLastComponent\n    * @return ResolveResult which allows further resolution of the remaining path\n    * @throws FileNotFoundException\n    */\n   ResolveResult<T> resolve(final String p, final boolean resolveLastComponent)\n       throws FileNotFoundException {\n-    String[] path = breakIntoPathComponents(p);\n-    if (path.length <= 1) { // special case for when path is \"/\"\n-      T targetFs = root.isInternalDir() ?\n-          getRootDir().getInternalDirFs() : getRootLink().getTargetFileSystem();\n-      ResolveResult<T> res = new ResolveResult<T>(ResultKind.INTERNAL_DIR,\n-          targetFs, root.fullPath, SlashPath);\n-      return res;\n-    }\n+    ResolveResult<T> resolveResult = null;\n+    resolveResult = getResolveResultFromCache(p, resolveLastComponent);\n+    if (resolveResult != null) {\n+      return resolveResult;\n+    }\n+\n+    try {\n+      String[] path = breakIntoPathComponents(p);\n+      if (path.length <= 1) { // special case for when path is \"/\"\n+        T targetFs = root.isInternalDir() ?\n+            getRootDir().getInternalDirFs()\n+            : getRootLink().getTargetFileSystem();\n+        resolveResult = new ResolveResult<T>(ResultKind.INTERNAL_DIR,\n+            targetFs, root.fullPath, SlashPath);\n+        return resolveResult;\n+      }\n \n-    /**\n-     * linkMergeSlash has been configured. The root of this mount table has\n-     * been linked to the root directory of a file system.\n-     * The first non-slash path component should be name of the mount table.\n-     */\n-    if (root.isLink()) {\n-      Path remainingPath;\n-      StringBuilder remainingPathStr = new StringBuilder();\n-      // ignore first slash\n-      for (int i = 1; i < path.length; i++) {\n-        remainingPathStr.append(\"/\").append(path[i]);\n+      /**\n+       * linkMergeSlash has been configured. The root of this mount table has\n+       * been linked to the root directory of a file system.\n+       * The first non-slash path component should be name of the mount table.\n+       */\n+      if (root.isLink()) {\n+        Path remainingPath;\n+        StringBuilder remainingPathStr = new StringBuilder();\n+        // ignore first slash\n+        for (int i = 1; i < path.length; i++) {\n+          remainingPathStr.append(\"/\").append(path[i]);\n+        }\n+        remainingPath = new Path(remainingPathStr.toString());\n+        resolveResult = new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n+            getRootLink().getTargetFileSystem(), root.fullPath, remainingPath);\n+        return resolveResult;\n       }\n-      remainingPath = new Path(remainingPathStr.toString());\n-      ResolveResult<T> res = new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n-          getRootLink().getTargetFileSystem(), root.fullPath, remainingPath);\n-      return res;\n-    }\n-    Preconditions.checkState(root.isInternalDir());\n-    INodeDir<T> curInode = getRootDir();\n+      Preconditions.checkState(root.isInternalDir());\n+      INodeDir<T> curInode = getRootDir();\n \n-    int i;\n-    // ignore first slash\n-    for (i = 1; i < path.length - (resolveLastComponent ? 0 : 1); i++) {\n-      INode<T> nextInode = curInode.resolveInternal(path[i]);\n-      if (nextInode == null) {\n-        if (hasFallbackLink()) {\n-          return new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n-              getRootFallbackLink().getTargetFileSystem(),\n-              root.fullPath, new Path(p));\n-        } else {\n-          StringBuilder failedAt = new StringBuilder(path[0]);\n-          for (int j = 1; j <= i; ++j) {\n-            failedAt.append('/').append(path[j]);\n+      // Try to resolve path in the regex mount point\n+      resolveResult = tryResolveInRegexMountpoint(p, resolveLastComponent);\n+      if (resolveResult != null) {\n+        return resolveResult;\n+      }\n+\n+      int i;\n+      // ignore first slash\n+      for (i = 1; i < path.length - (resolveLastComponent ? 0 : 1); i++) {\n+        INode<T> nextInode = curInode.resolveInternal(path[i]);\n+        if (nextInode == null) {\n+          if (hasFallbackLink()) {\n+            resolveResult = new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n+                getRootFallbackLink().getTargetFileSystem(), root.fullPath,\n+                new Path(p));\n+            return resolveResult;\n+          } else {\n+            StringBuilder failedAt = new StringBuilder(path[0]);\n+            for (int j = 1; j <= i; ++j) {\n+              failedAt.append('/').append(path[j]);\n+            }\n+            throw (new FileNotFoundException(\n+                \"File/Directory does not exist: \" + failedAt.toString()));\n           }\n-          throw (new FileNotFoundException(\n-              \"File/Directory does not exist: \" + failedAt.toString()));\n         }\n-      }\n \n-      if (nextInode.isLink()) {\n-        final INodeLink<T> link = (INodeLink<T>) nextInode;\n-        final Path remainingPath;\n-        if (i >= path.length - 1) {\n-          remainingPath = SlashPath;\n-        } else {\n-          StringBuilder remainingPathStr = new StringBuilder(\"/\" + path[i + 1]);\n-          for (int j = i + 2; j < path.length; ++j) {\n-            remainingPathStr.append('/').append(path[j]);\n+        if (nextInode.isLink()) {\n+          final INodeLink<T> link = (INodeLink<T>) nextInode;\n+          final Path remainingPath;\n+          if (i >= path.length - 1) {\n+            remainingPath = SlashPath;\n+          } else {\n+            StringBuilder remainingPathStr =\n+                new StringBuilder(\"/\" + path[i + 1]);\n+            for (int j = i + 2; j < path.length; ++j) {\n+              remainingPathStr.append('/').append(path[j]);\n+            }\n+            remainingPath = new Path(remainingPathStr.toString());\n           }\n-          remainingPath = new Path(remainingPathStr.toString());\n+          resolveResult = new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n+              link.getTargetFileSystem(), nextInode.fullPath, remainingPath);\n+          return resolveResult;\n+        } else if (nextInode.isInternalDir()) {\n+          curInode = (INodeDir<T>) nextInode;\n         }\n-        final ResolveResult<T> res =\n-            new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n-                link.getTargetFileSystem(), nextInode.fullPath, remainingPath);\n-        return res;\n-      } else if (nextInode.isInternalDir()) {\n-        curInode = (INodeDir<T>) nextInode;\n+      }\n+\n+      // We have resolved to an internal dir in mount table.\n+      Path remainingPath;\n+      if (resolveLastComponent) {\n+        remainingPath = SlashPath;\n+      } else {\n+        // note we have taken care of when path is \"/\" above\n+        // for internal dirs rem-path does not start with / since the lookup\n+        // that follows will do a children.get(remaningPath) and will have to\n+        // strip-out the initial /\n+        StringBuilder remainingPathStr = new StringBuilder(\"/\" + path[i]);\n+        for (int j = i + 1; j < path.length; ++j) {\n+          remainingPathStr.append('/').append(path[j]);\n+        }\n+        remainingPath = new Path(remainingPathStr.toString());\n+      }\n+      resolveResult = new ResolveResult<T>(ResultKind.INTERNAL_DIR,\n+          curInode.getInternalDirFs(), curInode.fullPath, remainingPath);\n+      return resolveResult;\n+    } finally {\n+      if (pathResolutionCacheCapacity > 0 && resolveResult != null) {\n+        addResolveResultToCache(p, resolveLastComponent, resolveResult);\n       }\n     }\n+  }\n \n-    // We have resolved to an internal dir in mount table.\n-    Path remainingPath;\n-    if (resolveLastComponent) {\n-      remainingPath = SlashPath;\n-    } else {\n-      // note we have taken care of when path is \"/\" above\n-      // for internal dirs rem-path does not start with / since the lookup\n-      // that follows will do a children.get(remaningPath) and will have to\n-      // strip-out the initial /\n-      StringBuilder remainingPathStr = new StringBuilder(\"/\" + path[i]);\n-      for (int j = i + 1; j < path.length; ++j) {\n-        remainingPathStr.append('/').append(path[j]);\n+  /**\n+   * Walk through all regex mount points to see\n+   * whether the path match any regex expressions.\n+   *\n+   * @param srcPath\n+   * @param resolveLastComponent\n+   * @return\n+   */\n+  protected ResolveResult<T> tryResolveInRegexMountpoint(final String srcPath,\n+      final boolean resolveLastComponent) {\n+    for (RegexMountPoint regexMountPoint : regexMountPointList) {\n+      ResolveResult resolveResult =\n+          regexMountPoint.resolve(srcPath, resolveLastComponent);\n+      if (resolveResult != null) {\n+        return resolveResult;\n       }\n-      remainingPath = new Path(remainingPathStr.toString());\n     }\n-    final ResolveResult<T> res =\n-        new ResolveResult<T>(ResultKind.INTERNAL_DIR,\n-            curInode.getInternalDirFs(), curInode.fullPath, remainingPath);\n-    return res;\n+    return null;\n+  }\n+\n+  /**\n+   * Build resolve result return to caller.\n+   *\n+   * @param resultKind\n+   * @param resolvedPathStr\n+   * @param targetOfResolvedPathStr\n+   * @param remainingPath\n+   * @return\n+   */\n+  protected ResolveResult<T> buildResolveResultForRegexMountPoint(\n+      ResultKind resultKind, String resolvedPathStr,\n+      String targetOfResolvedPathStr, Path remainingPath) {\n+    try {\n+      T targetFs = getTargetFileSystem(new URI(targetOfResolvedPathStr));\n+      return new ResolveResult<T>(resultKind, targetFs, resolvedPathStr,\n+          remainingPath);\n+    } catch (IOException ex) {\n+      LOGGER.error(String.format(\n+          \"Got Exception while build resolve result.\"\n+              + \" ResultKind:%s, resolvedPathStr:%s,\"\n+              + \" targetOfResolvedPathStr:%s, remainingPath:%s,\"\n+              + \" will return null.\",\n+          resultKind, resolvedPathStr, targetOfResolvedPathStr, remainingPath),\n+          ex);\n+      return null;\n+    } catch (URISyntaxException uex) {\n+      LOGGER.error(String.format(\n+          \"Got Exception while build resolve result.\"\n+              + \" ResultKind:%s, resolvedPathStr:%s,\"\n+              + \" targetOfResolvedPathStr:%s, remainingPath:%s,\"\n+              + \" will return null.\",\n+          resultKind, resolvedPathStr, targetOfResolvedPathStr, remainingPath),\n+          uex);\n+      return null;\n+    }\n+  }\n+\n+  /**\n+   * Return resolution cache capacity.\n+   *\n+   * @return\n+   */\n+  public int getPathResolutionCacheCapacity() {\n+    return pathResolutionCacheCapacity;\n+  }\n+\n+  private void addResolveResultToCache(final String pathStr,\n+      final Boolean resolveLastComponent,\n+      final ResolveResult<T> resolveResult) {\n+    try {\n+      cacheRWLock.writeLock().lock();\n+      String key = getResolveCacheKeyStr(pathStr, resolveLastComponent);\n+      pathResolutionCache.put(key, resolveResult);\n+    } finally {\n+      cacheRWLock.writeLock().unlock();\n+    }\n+  }\n+\n+  private ResolveResult<T> getResolveResultFromCache(final String pathStr,\n+      final Boolean resolveLastComponent) {\n+    if (pathResolutionCacheCapacity <= 0) {\n+      return null;\n+    }\n+    try {\n+      cacheRWLock.readLock().lock();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c58b945a32a978c470be07ef47e9ed7edfff428c"}, "originalPosition": 610}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0NTkxNDIyOnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/InodeTree.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QwODoxNzoyN1rOHBe3sw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QwODoxNzoyN1rOHBe3sw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMxNjQwMw==", "bodyText": "Please move annotation to separate line below.", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471316403", "createdAt": "2020-08-17T08:17:27Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/InodeTree.java", "diffHunk": "@@ -646,102 +714,222 @@ boolean isInternalDir() {\n   }\n \n   /**\n-   * Resolve the pathname p relative to root InodeDir\n+   * Resolve the pathname p relative to root InodeDir.\n    * @param p - input path\n    * @param resolveLastComponent\n    * @return ResolveResult which allows further resolution of the remaining path\n    * @throws FileNotFoundException\n    */\n   ResolveResult<T> resolve(final String p, final boolean resolveLastComponent)\n       throws FileNotFoundException {\n-    String[] path = breakIntoPathComponents(p);\n-    if (path.length <= 1) { // special case for when path is \"/\"\n-      T targetFs = root.isInternalDir() ?\n-          getRootDir().getInternalDirFs() : getRootLink().getTargetFileSystem();\n-      ResolveResult<T> res = new ResolveResult<T>(ResultKind.INTERNAL_DIR,\n-          targetFs, root.fullPath, SlashPath);\n-      return res;\n-    }\n+    ResolveResult<T> resolveResult = null;\n+    resolveResult = getResolveResultFromCache(p, resolveLastComponent);\n+    if (resolveResult != null) {\n+      return resolveResult;\n+    }\n+\n+    try {\n+      String[] path = breakIntoPathComponents(p);\n+      if (path.length <= 1) { // special case for when path is \"/\"\n+        T targetFs = root.isInternalDir() ?\n+            getRootDir().getInternalDirFs()\n+            : getRootLink().getTargetFileSystem();\n+        resolveResult = new ResolveResult<T>(ResultKind.INTERNAL_DIR,\n+            targetFs, root.fullPath, SlashPath);\n+        return resolveResult;\n+      }\n \n-    /**\n-     * linkMergeSlash has been configured. The root of this mount table has\n-     * been linked to the root directory of a file system.\n-     * The first non-slash path component should be name of the mount table.\n-     */\n-    if (root.isLink()) {\n-      Path remainingPath;\n-      StringBuilder remainingPathStr = new StringBuilder();\n-      // ignore first slash\n-      for (int i = 1; i < path.length; i++) {\n-        remainingPathStr.append(\"/\").append(path[i]);\n+      /**\n+       * linkMergeSlash has been configured. The root of this mount table has\n+       * been linked to the root directory of a file system.\n+       * The first non-slash path component should be name of the mount table.\n+       */\n+      if (root.isLink()) {\n+        Path remainingPath;\n+        StringBuilder remainingPathStr = new StringBuilder();\n+        // ignore first slash\n+        for (int i = 1; i < path.length; i++) {\n+          remainingPathStr.append(\"/\").append(path[i]);\n+        }\n+        remainingPath = new Path(remainingPathStr.toString());\n+        resolveResult = new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n+            getRootLink().getTargetFileSystem(), root.fullPath, remainingPath);\n+        return resolveResult;\n       }\n-      remainingPath = new Path(remainingPathStr.toString());\n-      ResolveResult<T> res = new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n-          getRootLink().getTargetFileSystem(), root.fullPath, remainingPath);\n-      return res;\n-    }\n-    Preconditions.checkState(root.isInternalDir());\n-    INodeDir<T> curInode = getRootDir();\n+      Preconditions.checkState(root.isInternalDir());\n+      INodeDir<T> curInode = getRootDir();\n \n-    int i;\n-    // ignore first slash\n-    for (i = 1; i < path.length - (resolveLastComponent ? 0 : 1); i++) {\n-      INode<T> nextInode = curInode.resolveInternal(path[i]);\n-      if (nextInode == null) {\n-        if (hasFallbackLink()) {\n-          return new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n-              getRootFallbackLink().getTargetFileSystem(),\n-              root.fullPath, new Path(p));\n-        } else {\n-          StringBuilder failedAt = new StringBuilder(path[0]);\n-          for (int j = 1; j <= i; ++j) {\n-            failedAt.append('/').append(path[j]);\n+      // Try to resolve path in the regex mount point\n+      resolveResult = tryResolveInRegexMountpoint(p, resolveLastComponent);\n+      if (resolveResult != null) {\n+        return resolveResult;\n+      }\n+\n+      int i;\n+      // ignore first slash\n+      for (i = 1; i < path.length - (resolveLastComponent ? 0 : 1); i++) {\n+        INode<T> nextInode = curInode.resolveInternal(path[i]);\n+        if (nextInode == null) {\n+          if (hasFallbackLink()) {\n+            resolveResult = new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n+                getRootFallbackLink().getTargetFileSystem(), root.fullPath,\n+                new Path(p));\n+            return resolveResult;\n+          } else {\n+            StringBuilder failedAt = new StringBuilder(path[0]);\n+            for (int j = 1; j <= i; ++j) {\n+              failedAt.append('/').append(path[j]);\n+            }\n+            throw (new FileNotFoundException(\n+                \"File/Directory does not exist: \" + failedAt.toString()));\n           }\n-          throw (new FileNotFoundException(\n-              \"File/Directory does not exist: \" + failedAt.toString()));\n         }\n-      }\n \n-      if (nextInode.isLink()) {\n-        final INodeLink<T> link = (INodeLink<T>) nextInode;\n-        final Path remainingPath;\n-        if (i >= path.length - 1) {\n-          remainingPath = SlashPath;\n-        } else {\n-          StringBuilder remainingPathStr = new StringBuilder(\"/\" + path[i + 1]);\n-          for (int j = i + 2; j < path.length; ++j) {\n-            remainingPathStr.append('/').append(path[j]);\n+        if (nextInode.isLink()) {\n+          final INodeLink<T> link = (INodeLink<T>) nextInode;\n+          final Path remainingPath;\n+          if (i >= path.length - 1) {\n+            remainingPath = SlashPath;\n+          } else {\n+            StringBuilder remainingPathStr =\n+                new StringBuilder(\"/\" + path[i + 1]);\n+            for (int j = i + 2; j < path.length; ++j) {\n+              remainingPathStr.append('/').append(path[j]);\n+            }\n+            remainingPath = new Path(remainingPathStr.toString());\n           }\n-          remainingPath = new Path(remainingPathStr.toString());\n+          resolveResult = new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n+              link.getTargetFileSystem(), nextInode.fullPath, remainingPath);\n+          return resolveResult;\n+        } else if (nextInode.isInternalDir()) {\n+          curInode = (INodeDir<T>) nextInode;\n         }\n-        final ResolveResult<T> res =\n-            new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n-                link.getTargetFileSystem(), nextInode.fullPath, remainingPath);\n-        return res;\n-      } else if (nextInode.isInternalDir()) {\n-        curInode = (INodeDir<T>) nextInode;\n+      }\n+\n+      // We have resolved to an internal dir in mount table.\n+      Path remainingPath;\n+      if (resolveLastComponent) {\n+        remainingPath = SlashPath;\n+      } else {\n+        // note we have taken care of when path is \"/\" above\n+        // for internal dirs rem-path does not start with / since the lookup\n+        // that follows will do a children.get(remaningPath) and will have to\n+        // strip-out the initial /\n+        StringBuilder remainingPathStr = new StringBuilder(\"/\" + path[i]);\n+        for (int j = i + 1; j < path.length; ++j) {\n+          remainingPathStr.append('/').append(path[j]);\n+        }\n+        remainingPath = new Path(remainingPathStr.toString());\n+      }\n+      resolveResult = new ResolveResult<T>(ResultKind.INTERNAL_DIR,\n+          curInode.getInternalDirFs(), curInode.fullPath, remainingPath);\n+      return resolveResult;\n+    } finally {\n+      if (pathResolutionCacheCapacity > 0 && resolveResult != null) {\n+        addResolveResultToCache(p, resolveLastComponent, resolveResult);\n       }\n     }\n+  }\n \n-    // We have resolved to an internal dir in mount table.\n-    Path remainingPath;\n-    if (resolveLastComponent) {\n-      remainingPath = SlashPath;\n-    } else {\n-      // note we have taken care of when path is \"/\" above\n-      // for internal dirs rem-path does not start with / since the lookup\n-      // that follows will do a children.get(remaningPath) and will have to\n-      // strip-out the initial /\n-      StringBuilder remainingPathStr = new StringBuilder(\"/\" + path[i]);\n-      for (int j = i + 1; j < path.length; ++j) {\n-        remainingPathStr.append('/').append(path[j]);\n+  /**\n+   * Walk through all regex mount points to see\n+   * whether the path match any regex expressions.\n+   *\n+   * @param srcPath\n+   * @param resolveLastComponent\n+   * @return\n+   */\n+  protected ResolveResult<T> tryResolveInRegexMountpoint(final String srcPath,\n+      final boolean resolveLastComponent) {\n+    for (RegexMountPoint regexMountPoint : regexMountPointList) {\n+      ResolveResult resolveResult =\n+          regexMountPoint.resolve(srcPath, resolveLastComponent);\n+      if (resolveResult != null) {\n+        return resolveResult;\n       }\n-      remainingPath = new Path(remainingPathStr.toString());\n     }\n-    final ResolveResult<T> res =\n-        new ResolveResult<T>(ResultKind.INTERNAL_DIR,\n-            curInode.getInternalDirFs(), curInode.fullPath, remainingPath);\n-    return res;\n+    return null;\n+  }\n+\n+  /**\n+   * Build resolve result return to caller.\n+   *\n+   * @param resultKind\n+   * @param resolvedPathStr\n+   * @param targetOfResolvedPathStr\n+   * @param remainingPath\n+   * @return\n+   */\n+  protected ResolveResult<T> buildResolveResultForRegexMountPoint(\n+      ResultKind resultKind, String resolvedPathStr,\n+      String targetOfResolvedPathStr, Path remainingPath) {\n+    try {\n+      T targetFs = getTargetFileSystem(new URI(targetOfResolvedPathStr));\n+      return new ResolveResult<T>(resultKind, targetFs, resolvedPathStr,\n+          remainingPath);\n+    } catch (IOException ex) {\n+      LOGGER.error(String.format(\n+          \"Got Exception while build resolve result.\"\n+              + \" ResultKind:%s, resolvedPathStr:%s,\"\n+              + \" targetOfResolvedPathStr:%s, remainingPath:%s,\"\n+              + \" will return null.\",\n+          resultKind, resolvedPathStr, targetOfResolvedPathStr, remainingPath),\n+          ex);\n+      return null;\n+    } catch (URISyntaxException uex) {\n+      LOGGER.error(String.format(\n+          \"Got Exception while build resolve result.\"\n+              + \" ResultKind:%s, resolvedPathStr:%s,\"\n+              + \" targetOfResolvedPathStr:%s, remainingPath:%s,\"\n+              + \" will return null.\",\n+          resultKind, resolvedPathStr, targetOfResolvedPathStr, remainingPath),\n+          uex);\n+      return null;\n+    }\n+  }\n+\n+  /**\n+   * Return resolution cache capacity.\n+   *\n+   * @return\n+   */\n+  public int getPathResolutionCacheCapacity() {\n+    return pathResolutionCacheCapacity;\n+  }\n+\n+  private void addResolveResultToCache(final String pathStr,\n+      final Boolean resolveLastComponent,\n+      final ResolveResult<T> resolveResult) {\n+    try {\n+      cacheRWLock.writeLock().lock();\n+      String key = getResolveCacheKeyStr(pathStr, resolveLastComponent);\n+      pathResolutionCache.put(key, resolveResult);\n+    } finally {\n+      cacheRWLock.writeLock().unlock();\n+    }\n+  }\n+\n+  private ResolveResult<T> getResolveResultFromCache(final String pathStr,\n+      final Boolean resolveLastComponent) {\n+    if (pathResolutionCacheCapacity <= 0) {\n+      return null;\n+    }\n+    try {\n+      cacheRWLock.readLock().lock();\n+      String key = getResolveCacheKeyStr(pathStr, resolveLastComponent);\n+      return (ResolveResult<T>) pathResolutionCache.get(key);\n+    } finally {\n+      cacheRWLock.readLock().unlock();\n+    }\n+  }\n+\n+  public static String getResolveCacheKeyStr(final String path,\n+      Boolean resolveLastComp) {\n+    return path + \",resolveLastComp\" + resolveLastComp;\n+  }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c58b945a32a978c470be07ef47e9ed7edfff428c"}, "originalPosition": 622}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0NTkyNjgyOnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/RegexMountPoint.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QwODoyMDo1NlrOHBe-_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QwODoyMDo1NlrOHBe-_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMxODI3MA==", "bodyText": "nit: typo: give space after comma", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471318270", "createdAt": "2020-08-17T08:20:56Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/RegexMountPoint.java", "diffHunk": "@@ -0,0 +1,244 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.regex.PatternSyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.util.StringUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.fs.viewfs.InodeTree.SlashPath;\n+\n+/**\n+ * Regex mount point is build to implement regex based mount point.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Unstable\n+class RegexMountPoint<T> {\n+  private static final Logger LOGGER =\n+      LoggerFactory.getLogger(RegexMountPoint.class.getName());\n+\n+  private InodeTree inodeTree;\n+  private String srcPathRegex;\n+  private Pattern srcPattern;\n+  private String dstPath;\n+  private String interceptorSettingsString;\n+  private List<RegexMountPointInterceptor> interceptorList;\n+\n+  public static final String SETTING_SRCREGEX_SEP = \"#.\";\n+  public static final char INTERCEPTOR_SEP = ';';\n+  public static final char INTERCEPTOR_INTERNAL_SEP = ':';\n+  // ${var},$var\n+  public static final Pattern VAR_PATTERN_IN_DEST =\n+      Pattern.compile(\"\\\\$((\\\\{\\\\w+\\\\})|(\\\\w+))\");\n+\n+  // key => $key or key = > ${key}\n+  private Map<String, Set<String>> varInDestPathMap;\n+\n+  public Map<String, Set<String>> getVarInDestPathMap() {\n+    return varInDestPathMap;\n+  }\n+\n+  RegexMountPoint(InodeTree inodeTree, String sourcePathRegex,\n+      String destPath, String settingsStr) {\n+    this.inodeTree = inodeTree;\n+    this.srcPathRegex = sourcePathRegex;\n+    this.dstPath = destPath;\n+    this.interceptorSettingsString = settingsStr;\n+    this.interceptorList = new ArrayList<>();\n+  }\n+\n+  /**\n+   * Initialize regex mount point.\n+   *\n+   * @throws IOException\n+   */\n+  public void initialize() throws IOException {\n+    try {\n+      srcPattern = Pattern.compile(srcPathRegex);\n+    } catch (PatternSyntaxException ex) {\n+      throw new IOException(\n+          \"Failed to initialized mount point due to bad src path regex:\"\n+              + srcPathRegex + \", dstPath:\" + dstPath, ex);\n+    }\n+    varInDestPathMap = getVarListInString(dstPath);\n+    initializeInterceptors();\n+  }\n+\n+  private void initializeInterceptors() throws IOException {\n+    if (interceptorSettingsString == null\n+        || interceptorSettingsString.isEmpty()) {\n+      return;\n+    }\n+    String[] interceptorStrArray =\n+        StringUtils.split(interceptorSettingsString, INTERCEPTOR_SEP);\n+    for (String interceptorStr : interceptorStrArray) {\n+      RegexMountPointInterceptor interceptor =\n+          RegexMountPointInterceptorFactory.create(interceptorStr);\n+      if (interceptor == null) {\n+        throw new IOException(\n+            \"Illegal settings String \" + interceptorSettingsString);\n+      }\n+      interceptor.initialize();\n+      interceptorList.add(interceptor);\n+    }\n+  }\n+\n+  /**\n+   * Get $var1 and $var2 style variables in string.\n+   *\n+   * @param input\n+   * @return\n+   */\n+  public static Map<String, Set<String>> getVarListInString(String input) {\n+    Map<String, Set<String>> varMap = new HashMap<>();\n+    Matcher matcher = VAR_PATTERN_IN_DEST.matcher(input);\n+    while (matcher.find()) {\n+      // $var or ${var}\n+      String varName = matcher.group(0);\n+      // var or {var}\n+      String strippedVarName = matcher.group(1);\n+      if (strippedVarName.startsWith(\"{\")) {\n+        // {varName} = > varName\n+        strippedVarName =\n+            strippedVarName.substring(1, strippedVarName.length() - 1);\n+      }\n+      varMap.putIfAbsent(strippedVarName, new HashSet<>());\n+      varMap.get(strippedVarName).add(varName);\n+    }\n+    return varMap;\n+  }\n+\n+  public String getSrcPathRegex() {\n+    return srcPathRegex;\n+  }\n+\n+  public Pattern getSrcPattern() {\n+    return srcPattern;\n+  }\n+\n+  public String getDstPath() {\n+    return dstPath;\n+  }\n+\n+  public static Pattern getVarPatternInDest() {\n+    return VAR_PATTERN_IN_DEST;\n+  }\n+\n+  /**\n+   * Get resolved path from regex mount points.\n+   * @param srcPath\n+   * @param resolveLastComponent\n+   * @return\n+   */\n+  public InodeTree.ResolveResult<T> resolve(final String srcPath,\n+      final boolean resolveLastComponent) {\n+    String pathStrToResolve = srcPath;\n+    if (!resolveLastComponent) {\n+      int lastSlashIndex = srcPath.lastIndexOf(SlashPath.toString());\n+      if (lastSlashIndex == -1) {\n+        return null;\n+      }\n+      pathStrToResolve = srcPath.substring(0, lastSlashIndex);\n+    }\n+    for (RegexMountPointInterceptor interceptor : interceptorList) {\n+      pathStrToResolve = interceptor.interceptSource(pathStrToResolve);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c58b945a32a978c470be07ef47e9ed7edfff428c"}, "originalPosition": 174}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0NzgxNzI1OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/InodeTree.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxNjo1MTo1MFrOHBwxFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxODowNjoxNFrOHCgHfA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTYwOTYyMA==", "bodyText": "Since this patch already big and this cache improvement seems to be generic for all cases, can we file separate Sub JIRA for this cache part?", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471609620", "createdAt": "2020-08-17T16:51:50Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/InodeTree.java", "diffHunk": "@@ -646,102 +714,222 @@ boolean isInternalDir() {\n   }\n \n   /**\n-   * Resolve the pathname p relative to root InodeDir\n+   * Resolve the pathname p relative to root InodeDir.\n    * @param p - input path\n    * @param resolveLastComponent\n    * @return ResolveResult which allows further resolution of the remaining path\n    * @throws FileNotFoundException\n    */\n   ResolveResult<T> resolve(final String p, final boolean resolveLastComponent)\n       throws FileNotFoundException {\n-    String[] path = breakIntoPathComponents(p);\n-    if (path.length <= 1) { // special case for when path is \"/\"\n-      T targetFs = root.isInternalDir() ?\n-          getRootDir().getInternalDirFs() : getRootLink().getTargetFileSystem();\n-      ResolveResult<T> res = new ResolveResult<T>(ResultKind.INTERNAL_DIR,\n-          targetFs, root.fullPath, SlashPath);\n-      return res;\n-    }\n+    ResolveResult<T> resolveResult = null;\n+    resolveResult = getResolveResultFromCache(p, resolveLastComponent);\n+    if (resolveResult != null) {\n+      return resolveResult;\n+    }\n+\n+    try {\n+      String[] path = breakIntoPathComponents(p);\n+      if (path.length <= 1) { // special case for when path is \"/\"\n+        T targetFs = root.isInternalDir() ?\n+            getRootDir().getInternalDirFs()\n+            : getRootLink().getTargetFileSystem();\n+        resolveResult = new ResolveResult<T>(ResultKind.INTERNAL_DIR,\n+            targetFs, root.fullPath, SlashPath);\n+        return resolveResult;\n+      }\n \n-    /**\n-     * linkMergeSlash has been configured. The root of this mount table has\n-     * been linked to the root directory of a file system.\n-     * The first non-slash path component should be name of the mount table.\n-     */\n-    if (root.isLink()) {\n-      Path remainingPath;\n-      StringBuilder remainingPathStr = new StringBuilder();\n-      // ignore first slash\n-      for (int i = 1; i < path.length; i++) {\n-        remainingPathStr.append(\"/\").append(path[i]);\n+      /**\n+       * linkMergeSlash has been configured. The root of this mount table has\n+       * been linked to the root directory of a file system.\n+       * The first non-slash path component should be name of the mount table.\n+       */\n+      if (root.isLink()) {\n+        Path remainingPath;\n+        StringBuilder remainingPathStr = new StringBuilder();\n+        // ignore first slash\n+        for (int i = 1; i < path.length; i++) {\n+          remainingPathStr.append(\"/\").append(path[i]);\n+        }\n+        remainingPath = new Path(remainingPathStr.toString());\n+        resolveResult = new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n+            getRootLink().getTargetFileSystem(), root.fullPath, remainingPath);\n+        return resolveResult;\n       }\n-      remainingPath = new Path(remainingPathStr.toString());\n-      ResolveResult<T> res = new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n-          getRootLink().getTargetFileSystem(), root.fullPath, remainingPath);\n-      return res;\n-    }\n-    Preconditions.checkState(root.isInternalDir());\n-    INodeDir<T> curInode = getRootDir();\n+      Preconditions.checkState(root.isInternalDir());\n+      INodeDir<T> curInode = getRootDir();\n \n-    int i;\n-    // ignore first slash\n-    for (i = 1; i < path.length - (resolveLastComponent ? 0 : 1); i++) {\n-      INode<T> nextInode = curInode.resolveInternal(path[i]);\n-      if (nextInode == null) {\n-        if (hasFallbackLink()) {\n-          return new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n-              getRootFallbackLink().getTargetFileSystem(),\n-              root.fullPath, new Path(p));\n-        } else {\n-          StringBuilder failedAt = new StringBuilder(path[0]);\n-          for (int j = 1; j <= i; ++j) {\n-            failedAt.append('/').append(path[j]);\n+      // Try to resolve path in the regex mount point\n+      resolveResult = tryResolveInRegexMountpoint(p, resolveLastComponent);\n+      if (resolveResult != null) {\n+        return resolveResult;\n+      }\n+\n+      int i;\n+      // ignore first slash\n+      for (i = 1; i < path.length - (resolveLastComponent ? 0 : 1); i++) {\n+        INode<T> nextInode = curInode.resolveInternal(path[i]);\n+        if (nextInode == null) {\n+          if (hasFallbackLink()) {\n+            resolveResult = new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n+                getRootFallbackLink().getTargetFileSystem(), root.fullPath,\n+                new Path(p));\n+            return resolveResult;\n+          } else {\n+            StringBuilder failedAt = new StringBuilder(path[0]);\n+            for (int j = 1; j <= i; ++j) {\n+              failedAt.append('/').append(path[j]);\n+            }\n+            throw (new FileNotFoundException(\n+                \"File/Directory does not exist: \" + failedAt.toString()));\n           }\n-          throw (new FileNotFoundException(\n-              \"File/Directory does not exist: \" + failedAt.toString()));\n         }\n-      }\n \n-      if (nextInode.isLink()) {\n-        final INodeLink<T> link = (INodeLink<T>) nextInode;\n-        final Path remainingPath;\n-        if (i >= path.length - 1) {\n-          remainingPath = SlashPath;\n-        } else {\n-          StringBuilder remainingPathStr = new StringBuilder(\"/\" + path[i + 1]);\n-          for (int j = i + 2; j < path.length; ++j) {\n-            remainingPathStr.append('/').append(path[j]);\n+        if (nextInode.isLink()) {\n+          final INodeLink<T> link = (INodeLink<T>) nextInode;\n+          final Path remainingPath;\n+          if (i >= path.length - 1) {\n+            remainingPath = SlashPath;\n+          } else {\n+            StringBuilder remainingPathStr =\n+                new StringBuilder(\"/\" + path[i + 1]);\n+            for (int j = i + 2; j < path.length; ++j) {\n+              remainingPathStr.append('/').append(path[j]);\n+            }\n+            remainingPath = new Path(remainingPathStr.toString());\n           }\n-          remainingPath = new Path(remainingPathStr.toString());\n+          resolveResult = new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n+              link.getTargetFileSystem(), nextInode.fullPath, remainingPath);\n+          return resolveResult;\n+        } else if (nextInode.isInternalDir()) {\n+          curInode = (INodeDir<T>) nextInode;\n         }\n-        final ResolveResult<T> res =\n-            new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n-                link.getTargetFileSystem(), nextInode.fullPath, remainingPath);\n-        return res;\n-      } else if (nextInode.isInternalDir()) {\n-        curInode = (INodeDir<T>) nextInode;\n+      }\n+\n+      // We have resolved to an internal dir in mount table.\n+      Path remainingPath;\n+      if (resolveLastComponent) {\n+        remainingPath = SlashPath;\n+      } else {\n+        // note we have taken care of when path is \"/\" above\n+        // for internal dirs rem-path does not start with / since the lookup\n+        // that follows will do a children.get(remaningPath) and will have to\n+        // strip-out the initial /\n+        StringBuilder remainingPathStr = new StringBuilder(\"/\" + path[i]);\n+        for (int j = i + 1; j < path.length; ++j) {\n+          remainingPathStr.append('/').append(path[j]);\n+        }\n+        remainingPath = new Path(remainingPathStr.toString());\n+      }\n+      resolveResult = new ResolveResult<T>(ResultKind.INTERNAL_DIR,\n+          curInode.getInternalDirFs(), curInode.fullPath, remainingPath);\n+      return resolveResult;\n+    } finally {\n+      if (pathResolutionCacheCapacity > 0 && resolveResult != null) {\n+        addResolveResultToCache(p, resolveLastComponent, resolveResult);\n       }\n     }\n+  }\n \n-    // We have resolved to an internal dir in mount table.\n-    Path remainingPath;\n-    if (resolveLastComponent) {\n-      remainingPath = SlashPath;\n-    } else {\n-      // note we have taken care of when path is \"/\" above\n-      // for internal dirs rem-path does not start with / since the lookup\n-      // that follows will do a children.get(remaningPath) and will have to\n-      // strip-out the initial /\n-      StringBuilder remainingPathStr = new StringBuilder(\"/\" + path[i]);\n-      for (int j = i + 1; j < path.length; ++j) {\n-        remainingPathStr.append('/').append(path[j]);\n+  /**\n+   * Walk through all regex mount points to see\n+   * whether the path match any regex expressions.\n+   *\n+   * @param srcPath\n+   * @param resolveLastComponent\n+   * @return\n+   */\n+  protected ResolveResult<T> tryResolveInRegexMountpoint(final String srcPath,\n+      final boolean resolveLastComponent) {\n+    for (RegexMountPoint regexMountPoint : regexMountPointList) {\n+      ResolveResult resolveResult =\n+          regexMountPoint.resolve(srcPath, resolveLastComponent);\n+      if (resolveResult != null) {\n+        return resolveResult;\n       }\n-      remainingPath = new Path(remainingPathStr.toString());\n     }\n-    final ResolveResult<T> res =\n-        new ResolveResult<T>(ResultKind.INTERNAL_DIR,\n-            curInode.getInternalDirFs(), curInode.fullPath, remainingPath);\n-    return res;\n+    return null;\n+  }\n+\n+  /**\n+   * Build resolve result return to caller.\n+   *\n+   * @param resultKind\n+   * @param resolvedPathStr\n+   * @param targetOfResolvedPathStr\n+   * @param remainingPath\n+   * @return\n+   */\n+  protected ResolveResult<T> buildResolveResultForRegexMountPoint(\n+      ResultKind resultKind, String resolvedPathStr,\n+      String targetOfResolvedPathStr, Path remainingPath) {\n+    try {\n+      T targetFs = getTargetFileSystem(new URI(targetOfResolvedPathStr));\n+      return new ResolveResult<T>(resultKind, targetFs, resolvedPathStr,\n+          remainingPath);\n+    } catch (IOException ex) {\n+      LOGGER.error(String.format(\n+          \"Got Exception while build resolve result.\"\n+              + \" ResultKind:%s, resolvedPathStr:%s,\"\n+              + \" targetOfResolvedPathStr:%s, remainingPath:%s,\"\n+              + \" will return null.\",\n+          resultKind, resolvedPathStr, targetOfResolvedPathStr, remainingPath),\n+          ex);\n+      return null;\n+    } catch (URISyntaxException uex) {\n+      LOGGER.error(String.format(\n+          \"Got Exception while build resolve result.\"\n+              + \" ResultKind:%s, resolvedPathStr:%s,\"\n+              + \" targetOfResolvedPathStr:%s, remainingPath:%s,\"\n+              + \" will return null.\",\n+          resultKind, resolvedPathStr, targetOfResolvedPathStr, remainingPath),\n+          uex);\n+      return null;\n+    }\n+  }\n+\n+  /**\n+   * Return resolution cache capacity.\n+   *\n+   * @return\n+   */\n+  public int getPathResolutionCacheCapacity() {\n+    return pathResolutionCacheCapacity;\n+  }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c58b945a32a978c470be07ef47e9ed7edfff428c"}, "originalPosition": 591}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjM4NTQwNA==", "bodyText": "Sure, make sense.", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r472385404", "createdAt": "2020-08-18T18:06:14Z", "author": {"login": "JohnZZGithub"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/InodeTree.java", "diffHunk": "@@ -646,102 +714,222 @@ boolean isInternalDir() {\n   }\n \n   /**\n-   * Resolve the pathname p relative to root InodeDir\n+   * Resolve the pathname p relative to root InodeDir.\n    * @param p - input path\n    * @param resolveLastComponent\n    * @return ResolveResult which allows further resolution of the remaining path\n    * @throws FileNotFoundException\n    */\n   ResolveResult<T> resolve(final String p, final boolean resolveLastComponent)\n       throws FileNotFoundException {\n-    String[] path = breakIntoPathComponents(p);\n-    if (path.length <= 1) { // special case for when path is \"/\"\n-      T targetFs = root.isInternalDir() ?\n-          getRootDir().getInternalDirFs() : getRootLink().getTargetFileSystem();\n-      ResolveResult<T> res = new ResolveResult<T>(ResultKind.INTERNAL_DIR,\n-          targetFs, root.fullPath, SlashPath);\n-      return res;\n-    }\n+    ResolveResult<T> resolveResult = null;\n+    resolveResult = getResolveResultFromCache(p, resolveLastComponent);\n+    if (resolveResult != null) {\n+      return resolveResult;\n+    }\n+\n+    try {\n+      String[] path = breakIntoPathComponents(p);\n+      if (path.length <= 1) { // special case for when path is \"/\"\n+        T targetFs = root.isInternalDir() ?\n+            getRootDir().getInternalDirFs()\n+            : getRootLink().getTargetFileSystem();\n+        resolveResult = new ResolveResult<T>(ResultKind.INTERNAL_DIR,\n+            targetFs, root.fullPath, SlashPath);\n+        return resolveResult;\n+      }\n \n-    /**\n-     * linkMergeSlash has been configured. The root of this mount table has\n-     * been linked to the root directory of a file system.\n-     * The first non-slash path component should be name of the mount table.\n-     */\n-    if (root.isLink()) {\n-      Path remainingPath;\n-      StringBuilder remainingPathStr = new StringBuilder();\n-      // ignore first slash\n-      for (int i = 1; i < path.length; i++) {\n-        remainingPathStr.append(\"/\").append(path[i]);\n+      /**\n+       * linkMergeSlash has been configured. The root of this mount table has\n+       * been linked to the root directory of a file system.\n+       * The first non-slash path component should be name of the mount table.\n+       */\n+      if (root.isLink()) {\n+        Path remainingPath;\n+        StringBuilder remainingPathStr = new StringBuilder();\n+        // ignore first slash\n+        for (int i = 1; i < path.length; i++) {\n+          remainingPathStr.append(\"/\").append(path[i]);\n+        }\n+        remainingPath = new Path(remainingPathStr.toString());\n+        resolveResult = new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n+            getRootLink().getTargetFileSystem(), root.fullPath, remainingPath);\n+        return resolveResult;\n       }\n-      remainingPath = new Path(remainingPathStr.toString());\n-      ResolveResult<T> res = new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n-          getRootLink().getTargetFileSystem(), root.fullPath, remainingPath);\n-      return res;\n-    }\n-    Preconditions.checkState(root.isInternalDir());\n-    INodeDir<T> curInode = getRootDir();\n+      Preconditions.checkState(root.isInternalDir());\n+      INodeDir<T> curInode = getRootDir();\n \n-    int i;\n-    // ignore first slash\n-    for (i = 1; i < path.length - (resolveLastComponent ? 0 : 1); i++) {\n-      INode<T> nextInode = curInode.resolveInternal(path[i]);\n-      if (nextInode == null) {\n-        if (hasFallbackLink()) {\n-          return new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n-              getRootFallbackLink().getTargetFileSystem(),\n-              root.fullPath, new Path(p));\n-        } else {\n-          StringBuilder failedAt = new StringBuilder(path[0]);\n-          for (int j = 1; j <= i; ++j) {\n-            failedAt.append('/').append(path[j]);\n+      // Try to resolve path in the regex mount point\n+      resolveResult = tryResolveInRegexMountpoint(p, resolveLastComponent);\n+      if (resolveResult != null) {\n+        return resolveResult;\n+      }\n+\n+      int i;\n+      // ignore first slash\n+      for (i = 1; i < path.length - (resolveLastComponent ? 0 : 1); i++) {\n+        INode<T> nextInode = curInode.resolveInternal(path[i]);\n+        if (nextInode == null) {\n+          if (hasFallbackLink()) {\n+            resolveResult = new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n+                getRootFallbackLink().getTargetFileSystem(), root.fullPath,\n+                new Path(p));\n+            return resolveResult;\n+          } else {\n+            StringBuilder failedAt = new StringBuilder(path[0]);\n+            for (int j = 1; j <= i; ++j) {\n+              failedAt.append('/').append(path[j]);\n+            }\n+            throw (new FileNotFoundException(\n+                \"File/Directory does not exist: \" + failedAt.toString()));\n           }\n-          throw (new FileNotFoundException(\n-              \"File/Directory does not exist: \" + failedAt.toString()));\n         }\n-      }\n \n-      if (nextInode.isLink()) {\n-        final INodeLink<T> link = (INodeLink<T>) nextInode;\n-        final Path remainingPath;\n-        if (i >= path.length - 1) {\n-          remainingPath = SlashPath;\n-        } else {\n-          StringBuilder remainingPathStr = new StringBuilder(\"/\" + path[i + 1]);\n-          for (int j = i + 2; j < path.length; ++j) {\n-            remainingPathStr.append('/').append(path[j]);\n+        if (nextInode.isLink()) {\n+          final INodeLink<T> link = (INodeLink<T>) nextInode;\n+          final Path remainingPath;\n+          if (i >= path.length - 1) {\n+            remainingPath = SlashPath;\n+          } else {\n+            StringBuilder remainingPathStr =\n+                new StringBuilder(\"/\" + path[i + 1]);\n+            for (int j = i + 2; j < path.length; ++j) {\n+              remainingPathStr.append('/').append(path[j]);\n+            }\n+            remainingPath = new Path(remainingPathStr.toString());\n           }\n-          remainingPath = new Path(remainingPathStr.toString());\n+          resolveResult = new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n+              link.getTargetFileSystem(), nextInode.fullPath, remainingPath);\n+          return resolveResult;\n+        } else if (nextInode.isInternalDir()) {\n+          curInode = (INodeDir<T>) nextInode;\n         }\n-        final ResolveResult<T> res =\n-            new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n-                link.getTargetFileSystem(), nextInode.fullPath, remainingPath);\n-        return res;\n-      } else if (nextInode.isInternalDir()) {\n-        curInode = (INodeDir<T>) nextInode;\n+      }\n+\n+      // We have resolved to an internal dir in mount table.\n+      Path remainingPath;\n+      if (resolveLastComponent) {\n+        remainingPath = SlashPath;\n+      } else {\n+        // note we have taken care of when path is \"/\" above\n+        // for internal dirs rem-path does not start with / since the lookup\n+        // that follows will do a children.get(remaningPath) and will have to\n+        // strip-out the initial /\n+        StringBuilder remainingPathStr = new StringBuilder(\"/\" + path[i]);\n+        for (int j = i + 1; j < path.length; ++j) {\n+          remainingPathStr.append('/').append(path[j]);\n+        }\n+        remainingPath = new Path(remainingPathStr.toString());\n+      }\n+      resolveResult = new ResolveResult<T>(ResultKind.INTERNAL_DIR,\n+          curInode.getInternalDirFs(), curInode.fullPath, remainingPath);\n+      return resolveResult;\n+    } finally {\n+      if (pathResolutionCacheCapacity > 0 && resolveResult != null) {\n+        addResolveResultToCache(p, resolveLastComponent, resolveResult);\n       }\n     }\n+  }\n \n-    // We have resolved to an internal dir in mount table.\n-    Path remainingPath;\n-    if (resolveLastComponent) {\n-      remainingPath = SlashPath;\n-    } else {\n-      // note we have taken care of when path is \"/\" above\n-      // for internal dirs rem-path does not start with / since the lookup\n-      // that follows will do a children.get(remaningPath) and will have to\n-      // strip-out the initial /\n-      StringBuilder remainingPathStr = new StringBuilder(\"/\" + path[i]);\n-      for (int j = i + 1; j < path.length; ++j) {\n-        remainingPathStr.append('/').append(path[j]);\n+  /**\n+   * Walk through all regex mount points to see\n+   * whether the path match any regex expressions.\n+   *\n+   * @param srcPath\n+   * @param resolveLastComponent\n+   * @return\n+   */\n+  protected ResolveResult<T> tryResolveInRegexMountpoint(final String srcPath,\n+      final boolean resolveLastComponent) {\n+    for (RegexMountPoint regexMountPoint : regexMountPointList) {\n+      ResolveResult resolveResult =\n+          regexMountPoint.resolve(srcPath, resolveLastComponent);\n+      if (resolveResult != null) {\n+        return resolveResult;\n       }\n-      remainingPath = new Path(remainingPathStr.toString());\n     }\n-    final ResolveResult<T> res =\n-        new ResolveResult<T>(ResultKind.INTERNAL_DIR,\n-            curInode.getInternalDirFs(), curInode.fullPath, remainingPath);\n-    return res;\n+    return null;\n+  }\n+\n+  /**\n+   * Build resolve result return to caller.\n+   *\n+   * @param resultKind\n+   * @param resolvedPathStr\n+   * @param targetOfResolvedPathStr\n+   * @param remainingPath\n+   * @return\n+   */\n+  protected ResolveResult<T> buildResolveResultForRegexMountPoint(\n+      ResultKind resultKind, String resolvedPathStr,\n+      String targetOfResolvedPathStr, Path remainingPath) {\n+    try {\n+      T targetFs = getTargetFileSystem(new URI(targetOfResolvedPathStr));\n+      return new ResolveResult<T>(resultKind, targetFs, resolvedPathStr,\n+          remainingPath);\n+    } catch (IOException ex) {\n+      LOGGER.error(String.format(\n+          \"Got Exception while build resolve result.\"\n+              + \" ResultKind:%s, resolvedPathStr:%s,\"\n+              + \" targetOfResolvedPathStr:%s, remainingPath:%s,\"\n+              + \" will return null.\",\n+          resultKind, resolvedPathStr, targetOfResolvedPathStr, remainingPath),\n+          ex);\n+      return null;\n+    } catch (URISyntaxException uex) {\n+      LOGGER.error(String.format(\n+          \"Got Exception while build resolve result.\"\n+              + \" ResultKind:%s, resolvedPathStr:%s,\"\n+              + \" targetOfResolvedPathStr:%s, remainingPath:%s,\"\n+              + \" will return null.\",\n+          resultKind, resolvedPathStr, targetOfResolvedPathStr, remainingPath),\n+          uex);\n+      return null;\n+    }\n+  }\n+\n+  /**\n+   * Return resolution cache capacity.\n+   *\n+   * @return\n+   */\n+  public int getPathResolutionCacheCapacity() {\n+    return pathResolutionCacheCapacity;\n+  }\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTYwOTYyMA=="}, "originalCommit": {"oid": "c58b945a32a978c470be07ef47e9ed7edfff428c"}, "originalPosition": 591}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0ODA0Nzc2OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/site/markdown/ViewFs.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxNzozMToyMlrOHBzL7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QyMzozNzo1MFrOHB-UoA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTY0OTI2Mg==", "bodyText": "Instead of asking users to disable, isn't it the better way is to add getTargetFileSystem(uri, userCache=true/false) method?\nSo, that the users already using other mount points need not disable for the sake if regex based mount points right?", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471649262", "createdAt": "2020-08-17T17:31:22Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/site/markdown/ViewFs.md", "diffHunk": "@@ -366,6 +366,82 @@ Don't want to change scheme or difficult to copy mount-table configurations to a\n \n Please refer to the [View File System Overload Scheme Guide](./ViewFsOverloadScheme.html)\n \n+Regex Pattern Based Mount Points\n+--------------------------------\n+\n+The view file system mount points were a Key-Value based mapping system. It is not friendly for user cases which mapping config could be abstracted to rules. E.g. Users want to provide a GCS bucket per user and there might be thousands of users in total. The old key-value based approach won't work well for several reasons:\n+\n+1. The mount table is used by FileSystem clients. There's a cost to spread the config to all clients and we should avoid it if possible. The [View File System Overload Scheme Guide](./ViewFsOverloadScheme.html) could help the distribution by central mount table management. But the mount table still have to be updated on every change. The change could be greatly avoided if provide a rule-based mount table..\n+\n+2. The client have to understand all the KVs in the mount table. This is not ideal when the mountable grows to thousands of items. E.g. thousands of file systems might be initialized even users only need one. And the config itself will become bloated at scale.\n+\n+### Understand the Difference\n+\n+In the key-value based mount table, view file system treats every mount point as a partition. There's several file system APIs which will lead to operation on all partitions. E.g. there's an HDFS cluster with multiple mount. Users want to run \u201chadoop fs -put file viewfs://hdfs.namenode.apache.org/tmp/\u201d cmd to copy data from local disk to our HDFS cluster. The cmd will trigger ViewFileSystem to call setVerifyChecksum() method which will initialize the file system for every mount point.\n+For a regex-base rule mount table entry, we couldn't know what's corresponding path until parsing. So the regex based mount table entry will be ignored on such cases and the file system will be created upon accessing. The inner cache of ViewFs is also not available for regex-base mount points now as it assumes target file system doesn't change after viewfs initialization. Please disable it if you want to use regex-base mount table. We also need to change the rename strategy to SAME_FILESYSTEM_ACROSS_MOUNTPOINT for the same reason.\n+```xml\n+<property>\n+    <name>fs.viewfs.enable.inner.cache</name>\n+    <value>false</value>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c58b945a32a978c470be07ef47e9ed7edfff428c"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTgzMTcxMg==", "bodyText": "Good call.", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471831712", "createdAt": "2020-08-17T23:37:50Z", "author": {"login": "JohnZZGithub"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/site/markdown/ViewFs.md", "diffHunk": "@@ -366,6 +366,82 @@ Don't want to change scheme or difficult to copy mount-table configurations to a\n \n Please refer to the [View File System Overload Scheme Guide](./ViewFsOverloadScheme.html)\n \n+Regex Pattern Based Mount Points\n+--------------------------------\n+\n+The view file system mount points were a Key-Value based mapping system. It is not friendly for user cases which mapping config could be abstracted to rules. E.g. Users want to provide a GCS bucket per user and there might be thousands of users in total. The old key-value based approach won't work well for several reasons:\n+\n+1. The mount table is used by FileSystem clients. There's a cost to spread the config to all clients and we should avoid it if possible. The [View File System Overload Scheme Guide](./ViewFsOverloadScheme.html) could help the distribution by central mount table management. But the mount table still have to be updated on every change. The change could be greatly avoided if provide a rule-based mount table..\n+\n+2. The client have to understand all the KVs in the mount table. This is not ideal when the mountable grows to thousands of items. E.g. thousands of file systems might be initialized even users only need one. And the config itself will become bloated at scale.\n+\n+### Understand the Difference\n+\n+In the key-value based mount table, view file system treats every mount point as a partition. There's several file system APIs which will lead to operation on all partitions. E.g. there's an HDFS cluster with multiple mount. Users want to run \u201chadoop fs -put file viewfs://hdfs.namenode.apache.org/tmp/\u201d cmd to copy data from local disk to our HDFS cluster. The cmd will trigger ViewFileSystem to call setVerifyChecksum() method which will initialize the file system for every mount point.\n+For a regex-base rule mount table entry, we couldn't know what's corresponding path until parsing. So the regex based mount table entry will be ignored on such cases and the file system will be created upon accessing. The inner cache of ViewFs is also not available for regex-base mount points now as it assumes target file system doesn't change after viewfs initialization. Please disable it if you want to use regex-base mount table. We also need to change the rename strategy to SAME_FILESYSTEM_ACROSS_MOUNTPOINT for the same reason.\n+```xml\n+<property>\n+    <name>fs.viewfs.enable.inner.cache</name>\n+    <value>false</value>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTY0OTI2Mg=="}, "originalCommit": {"oid": "c58b945a32a978c470be07ef47e9ed7edfff428c"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0ODEwNTIxOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemLinkRegex.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxNzo0ODoxNlrOHBzuqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwMTo0ODoxNVrOHCAeEg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTY1ODE1NQ==", "bodyText": "same as above comment", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471658155", "createdAt": "2020-08-17T17:48:16Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemLinkRegex.java", "diffHunk": "@@ -0,0 +1,320 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.MiniDFSNNTopology;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.fs.viewfs.Constants.CONFIG_VIEWFS_ENABLE_INNER_CACHE;\n+import static org.apache.hadoop.fs.viewfs.RegexMountPoint.INTERCEPTOR_INTERNAL_SEP;\n+\n+/**\n+ * Test linkRegex node type for view file system.\n+ */\n+public class TestViewFileSystemLinkRegex extends ViewFileSystemBaseTest {\n+  public static final Logger LOGGER =\n+      LoggerFactory.getLogger(TestViewFileSystemLinkRegex.class);\n+\n+  private static FileSystem fsDefault;\n+  private static MiniDFSCluster cluster;\n+  private static Configuration clusterConfig;\n+  private static final int NAME_SPACES_COUNT = 3;\n+  private static final int DATA_NODES_COUNT = 3;\n+  private static final int FS_INDEX_DEFAULT = 0;\n+  private static final FileSystem[] FS_HDFS = new FileSystem[NAME_SPACES_COUNT];\n+  private static final String CLUSTER_NAME =\n+      \"TestViewFileSystemLinkRegexCluster\";\n+  private static final File TEST_DIR = GenericTestUtils\n+      .getTestDir(TestViewFileSystemLinkRegex.class.getSimpleName());\n+  private static final String TEST_BASE_PATH =\n+      \"/tmp/TestViewFileSystemLinkRegex\";\n+\n+  @Override\n+  protected FileSystemTestHelper createFileSystemHelper() {\n+    return new FileSystemTestHelper(TEST_BASE_PATH);\n+  }\n+\n+  @BeforeClass\n+  public static void clusterSetupAtBeginning() throws IOException {\n+    SupportsBlocks = true;\n+    clusterConfig = ViewFileSystemTestSetup.createConfig();\n+    clusterConfig.setBoolean(\n+        DFSConfigKeys.DFS_NAMENODE_DELEGATION_TOKEN_ALWAYS_USE_KEY,\n+        true);\n+    cluster = new MiniDFSCluster.Builder(clusterConfig).nnTopology(\n+        MiniDFSNNTopology.simpleFederatedTopology(NAME_SPACES_COUNT))\n+        .numDataNodes(DATA_NODES_COUNT).build();\n+    cluster.waitClusterUp();\n+\n+    for (int i = 0; i < NAME_SPACES_COUNT; i++) {\n+      FS_HDFS[i] = cluster.getFileSystem(i);\n+    }\n+    fsDefault = FS_HDFS[FS_INDEX_DEFAULT];\n+  }\n+\n+  @AfterClass\n+  public static void clusterShutdownAtEnd() throws Exception {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  @Override\n+  @Before\n+  public void setUp() throws Exception {\n+    fsTarget = fsDefault;\n+    super.setUp();\n+  }\n+\n+  /**\n+   * Override this so that we don't set the targetTestRoot to any path under the\n+   * root of the FS, and so that we don't try to delete the test dir, but rather\n+   * only its contents.\n+   */\n+  @Override\n+  void initializeTargetTestRoot() throws IOException {\n+    targetTestRoot = fsDefault.makeQualified(new Path(\"/\"));\n+    for (FileStatus status : fsDefault.listStatus(targetTestRoot)) {\n+      fsDefault.delete(status.getPath(), true);\n+    }\n+  }\n+\n+  @Override\n+  void setupMountPoints() {\n+    super.setupMountPoints();\n+  }\n+\n+  @Override\n+  int getExpectedDelegationTokenCount() {\n+    return 1; // all point to the same fs so 1 unique token\n+  }\n+\n+  @Override\n+  int getExpectedDelegationTokenCountWithCredentials() {\n+    return 1;\n+  }\n+\n+  public String buildReplaceInterceptorSettingString(String srcRegex,\n+      String replaceString) {\n+    return\n+        RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH.getConfigName()\n+            + INTERCEPTOR_INTERNAL_SEP + srcRegex + INTERCEPTOR_INTERNAL_SEP\n+            + replaceString;\n+  }\n+\n+  public String linkInterceptorSettings(\n+      List<String> interceptorSettingStrList) {\n+    StringBuilder stringBuilder = new StringBuilder();\n+    int listSize = interceptorSettingStrList.size();\n+    for (int i = 0; i < listSize; ++i) {\n+      stringBuilder.append(interceptorSettingStrList.get(i));\n+      if (i < listSize - 1) {\n+        stringBuilder.append(RegexMountPoint.INTERCEPTOR_SEP);\n+      }\n+    }\n+    return stringBuilder.toString();\n+  }\n+\n+  @Test\n+  public void testConfLinkRegexIndexMapping() throws Exception {\n+    conf.setBoolean(CONFIG_VIEWFS_ENABLE_INNER_CACHE, false);\n+    // (^/(\\w+),/targetTestRoot/$1)\n+    // => /targetTestRoot/testConfLinkRegexIndexMapping1\n+    URI viewFsUri =\n+        new URI(FsConstants.VIEWFS_SCHEME, CLUSTER_NAME, \"/\", null, null);\n+    String regexStr = \"^/(\\\\w+)\";\n+    String dstPathStr = targetTestRoot + \"$1\";\n+    Path srcPath = new Path(\"/testConfLinkRegexIndexMapping1\");\n+    Path expectedResolveResult =\n+        new Path(dstPathStr.replace(\"$1\", \"testConfLinkRegexIndexMapping1\"));\n+    FSDataOutputStream outputStream = fsTarget.create((expectedResolveResult));\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    FileSystem vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    // Test ${1} format\n+    // ^/(\\w+, /targetTestRoot/${1})\n+    // => /targetTestRoot/testConfLinkRegexIndexMapping2\n+    dstPathStr = targetTestRoot + \"${1}\";\n+    srcPath = new Path(\"/testConfLinkRegexIndexMapping2\");\n+    expectedResolveResult =\n+        new Path(dstPathStr.replace(\"${1}\", \"testConfLinkRegexIndexMapping2\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    //(^/(\\w+)/file1, /targetTestRoot/$1)\n+    // = > /targetTestRoot/testConfLinkRegexIndexMapping3/file1\n+    dstPathStr = targetTestRoot + \"$1\";\n+    srcPath = new Path(\"/testConfLinkRegexIndexMapping3/file1\");\n+    expectedResolveResult = new Path(\n+        dstPathStr.replace(\"$1\", \"testConfLinkRegexIndexMapping3/file1\"));\n+    outputStream = fsTarget.create(expectedResolveResult);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c58b945a32a978c470be07ef47e9ed7edfff428c"}, "originalPosition": 196}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg2Njg5OA==", "bodyText": "Good point", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471866898", "createdAt": "2020-08-18T01:48:15Z", "author": {"login": "JohnZZGithub"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemLinkRegex.java", "diffHunk": "@@ -0,0 +1,320 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.MiniDFSNNTopology;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.fs.viewfs.Constants.CONFIG_VIEWFS_ENABLE_INNER_CACHE;\n+import static org.apache.hadoop.fs.viewfs.RegexMountPoint.INTERCEPTOR_INTERNAL_SEP;\n+\n+/**\n+ * Test linkRegex node type for view file system.\n+ */\n+public class TestViewFileSystemLinkRegex extends ViewFileSystemBaseTest {\n+  public static final Logger LOGGER =\n+      LoggerFactory.getLogger(TestViewFileSystemLinkRegex.class);\n+\n+  private static FileSystem fsDefault;\n+  private static MiniDFSCluster cluster;\n+  private static Configuration clusterConfig;\n+  private static final int NAME_SPACES_COUNT = 3;\n+  private static final int DATA_NODES_COUNT = 3;\n+  private static final int FS_INDEX_DEFAULT = 0;\n+  private static final FileSystem[] FS_HDFS = new FileSystem[NAME_SPACES_COUNT];\n+  private static final String CLUSTER_NAME =\n+      \"TestViewFileSystemLinkRegexCluster\";\n+  private static final File TEST_DIR = GenericTestUtils\n+      .getTestDir(TestViewFileSystemLinkRegex.class.getSimpleName());\n+  private static final String TEST_BASE_PATH =\n+      \"/tmp/TestViewFileSystemLinkRegex\";\n+\n+  @Override\n+  protected FileSystemTestHelper createFileSystemHelper() {\n+    return new FileSystemTestHelper(TEST_BASE_PATH);\n+  }\n+\n+  @BeforeClass\n+  public static void clusterSetupAtBeginning() throws IOException {\n+    SupportsBlocks = true;\n+    clusterConfig = ViewFileSystemTestSetup.createConfig();\n+    clusterConfig.setBoolean(\n+        DFSConfigKeys.DFS_NAMENODE_DELEGATION_TOKEN_ALWAYS_USE_KEY,\n+        true);\n+    cluster = new MiniDFSCluster.Builder(clusterConfig).nnTopology(\n+        MiniDFSNNTopology.simpleFederatedTopology(NAME_SPACES_COUNT))\n+        .numDataNodes(DATA_NODES_COUNT).build();\n+    cluster.waitClusterUp();\n+\n+    for (int i = 0; i < NAME_SPACES_COUNT; i++) {\n+      FS_HDFS[i] = cluster.getFileSystem(i);\n+    }\n+    fsDefault = FS_HDFS[FS_INDEX_DEFAULT];\n+  }\n+\n+  @AfterClass\n+  public static void clusterShutdownAtEnd() throws Exception {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  @Override\n+  @Before\n+  public void setUp() throws Exception {\n+    fsTarget = fsDefault;\n+    super.setUp();\n+  }\n+\n+  /**\n+   * Override this so that we don't set the targetTestRoot to any path under the\n+   * root of the FS, and so that we don't try to delete the test dir, but rather\n+   * only its contents.\n+   */\n+  @Override\n+  void initializeTargetTestRoot() throws IOException {\n+    targetTestRoot = fsDefault.makeQualified(new Path(\"/\"));\n+    for (FileStatus status : fsDefault.listStatus(targetTestRoot)) {\n+      fsDefault.delete(status.getPath(), true);\n+    }\n+  }\n+\n+  @Override\n+  void setupMountPoints() {\n+    super.setupMountPoints();\n+  }\n+\n+  @Override\n+  int getExpectedDelegationTokenCount() {\n+    return 1; // all point to the same fs so 1 unique token\n+  }\n+\n+  @Override\n+  int getExpectedDelegationTokenCountWithCredentials() {\n+    return 1;\n+  }\n+\n+  public String buildReplaceInterceptorSettingString(String srcRegex,\n+      String replaceString) {\n+    return\n+        RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH.getConfigName()\n+            + INTERCEPTOR_INTERNAL_SEP + srcRegex + INTERCEPTOR_INTERNAL_SEP\n+            + replaceString;\n+  }\n+\n+  public String linkInterceptorSettings(\n+      List<String> interceptorSettingStrList) {\n+    StringBuilder stringBuilder = new StringBuilder();\n+    int listSize = interceptorSettingStrList.size();\n+    for (int i = 0; i < listSize; ++i) {\n+      stringBuilder.append(interceptorSettingStrList.get(i));\n+      if (i < listSize - 1) {\n+        stringBuilder.append(RegexMountPoint.INTERCEPTOR_SEP);\n+      }\n+    }\n+    return stringBuilder.toString();\n+  }\n+\n+  @Test\n+  public void testConfLinkRegexIndexMapping() throws Exception {\n+    conf.setBoolean(CONFIG_VIEWFS_ENABLE_INNER_CACHE, false);\n+    // (^/(\\w+),/targetTestRoot/$1)\n+    // => /targetTestRoot/testConfLinkRegexIndexMapping1\n+    URI viewFsUri =\n+        new URI(FsConstants.VIEWFS_SCHEME, CLUSTER_NAME, \"/\", null, null);\n+    String regexStr = \"^/(\\\\w+)\";\n+    String dstPathStr = targetTestRoot + \"$1\";\n+    Path srcPath = new Path(\"/testConfLinkRegexIndexMapping1\");\n+    Path expectedResolveResult =\n+        new Path(dstPathStr.replace(\"$1\", \"testConfLinkRegexIndexMapping1\"));\n+    FSDataOutputStream outputStream = fsTarget.create((expectedResolveResult));\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    FileSystem vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    // Test ${1} format\n+    // ^/(\\w+, /targetTestRoot/${1})\n+    // => /targetTestRoot/testConfLinkRegexIndexMapping2\n+    dstPathStr = targetTestRoot + \"${1}\";\n+    srcPath = new Path(\"/testConfLinkRegexIndexMapping2\");\n+    expectedResolveResult =\n+        new Path(dstPathStr.replace(\"${1}\", \"testConfLinkRegexIndexMapping2\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    //(^/(\\w+)/file1, /targetTestRoot/$1)\n+    // = > /targetTestRoot/testConfLinkRegexIndexMapping3/file1\n+    dstPathStr = targetTestRoot + \"$1\";\n+    srcPath = new Path(\"/testConfLinkRegexIndexMapping3/file1\");\n+    expectedResolveResult = new Path(\n+        dstPathStr.replace(\"$1\", \"testConfLinkRegexIndexMapping3/file1\"));\n+    outputStream = fsTarget.create(expectedResolveResult);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTY1ODE1NQ=="}, "originalCommit": {"oid": "c58b945a32a978c470be07ef47e9ed7edfff428c"}, "originalPosition": 196}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0ODEwOTQ1OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemLinkRegex.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxNzo0OToyMFrOHBzxMQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxNzo0OToyMFrOHBzxMQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTY1ODgwMQ==", "bodyText": "you may want to close vfs as well?", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471658801", "createdAt": "2020-08-17T17:49:20Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemLinkRegex.java", "diffHunk": "@@ -0,0 +1,320 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.MiniDFSNNTopology;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.fs.viewfs.Constants.CONFIG_VIEWFS_ENABLE_INNER_CACHE;\n+import static org.apache.hadoop.fs.viewfs.RegexMountPoint.INTERCEPTOR_INTERNAL_SEP;\n+\n+/**\n+ * Test linkRegex node type for view file system.\n+ */\n+public class TestViewFileSystemLinkRegex extends ViewFileSystemBaseTest {\n+  public static final Logger LOGGER =\n+      LoggerFactory.getLogger(TestViewFileSystemLinkRegex.class);\n+\n+  private static FileSystem fsDefault;\n+  private static MiniDFSCluster cluster;\n+  private static Configuration clusterConfig;\n+  private static final int NAME_SPACES_COUNT = 3;\n+  private static final int DATA_NODES_COUNT = 3;\n+  private static final int FS_INDEX_DEFAULT = 0;\n+  private static final FileSystem[] FS_HDFS = new FileSystem[NAME_SPACES_COUNT];\n+  private static final String CLUSTER_NAME =\n+      \"TestViewFileSystemLinkRegexCluster\";\n+  private static final File TEST_DIR = GenericTestUtils\n+      .getTestDir(TestViewFileSystemLinkRegex.class.getSimpleName());\n+  private static final String TEST_BASE_PATH =\n+      \"/tmp/TestViewFileSystemLinkRegex\";\n+\n+  @Override\n+  protected FileSystemTestHelper createFileSystemHelper() {\n+    return new FileSystemTestHelper(TEST_BASE_PATH);\n+  }\n+\n+  @BeforeClass\n+  public static void clusterSetupAtBeginning() throws IOException {\n+    SupportsBlocks = true;\n+    clusterConfig = ViewFileSystemTestSetup.createConfig();\n+    clusterConfig.setBoolean(\n+        DFSConfigKeys.DFS_NAMENODE_DELEGATION_TOKEN_ALWAYS_USE_KEY,\n+        true);\n+    cluster = new MiniDFSCluster.Builder(clusterConfig).nnTopology(\n+        MiniDFSNNTopology.simpleFederatedTopology(NAME_SPACES_COUNT))\n+        .numDataNodes(DATA_NODES_COUNT).build();\n+    cluster.waitClusterUp();\n+\n+    for (int i = 0; i < NAME_SPACES_COUNT; i++) {\n+      FS_HDFS[i] = cluster.getFileSystem(i);\n+    }\n+    fsDefault = FS_HDFS[FS_INDEX_DEFAULT];\n+  }\n+\n+  @AfterClass\n+  public static void clusterShutdownAtEnd() throws Exception {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  @Override\n+  @Before\n+  public void setUp() throws Exception {\n+    fsTarget = fsDefault;\n+    super.setUp();\n+  }\n+\n+  /**\n+   * Override this so that we don't set the targetTestRoot to any path under the\n+   * root of the FS, and so that we don't try to delete the test dir, but rather\n+   * only its contents.\n+   */\n+  @Override\n+  void initializeTargetTestRoot() throws IOException {\n+    targetTestRoot = fsDefault.makeQualified(new Path(\"/\"));\n+    for (FileStatus status : fsDefault.listStatus(targetTestRoot)) {\n+      fsDefault.delete(status.getPath(), true);\n+    }\n+  }\n+\n+  @Override\n+  void setupMountPoints() {\n+    super.setupMountPoints();\n+  }\n+\n+  @Override\n+  int getExpectedDelegationTokenCount() {\n+    return 1; // all point to the same fs so 1 unique token\n+  }\n+\n+  @Override\n+  int getExpectedDelegationTokenCountWithCredentials() {\n+    return 1;\n+  }\n+\n+  public String buildReplaceInterceptorSettingString(String srcRegex,\n+      String replaceString) {\n+    return\n+        RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH.getConfigName()\n+            + INTERCEPTOR_INTERNAL_SEP + srcRegex + INTERCEPTOR_INTERNAL_SEP\n+            + replaceString;\n+  }\n+\n+  public String linkInterceptorSettings(\n+      List<String> interceptorSettingStrList) {\n+    StringBuilder stringBuilder = new StringBuilder();\n+    int listSize = interceptorSettingStrList.size();\n+    for (int i = 0; i < listSize; ++i) {\n+      stringBuilder.append(interceptorSettingStrList.get(i));\n+      if (i < listSize - 1) {\n+        stringBuilder.append(RegexMountPoint.INTERCEPTOR_SEP);\n+      }\n+    }\n+    return stringBuilder.toString();\n+  }\n+\n+  @Test\n+  public void testConfLinkRegexIndexMapping() throws Exception {\n+    conf.setBoolean(CONFIG_VIEWFS_ENABLE_INNER_CACHE, false);\n+    // (^/(\\w+),/targetTestRoot/$1)\n+    // => /targetTestRoot/testConfLinkRegexIndexMapping1\n+    URI viewFsUri =\n+        new URI(FsConstants.VIEWFS_SCHEME, CLUSTER_NAME, \"/\", null, null);\n+    String regexStr = \"^/(\\\\w+)\";\n+    String dstPathStr = targetTestRoot + \"$1\";\n+    Path srcPath = new Path(\"/testConfLinkRegexIndexMapping1\");\n+    Path expectedResolveResult =\n+        new Path(dstPathStr.replace(\"$1\", \"testConfLinkRegexIndexMapping1\"));\n+    FSDataOutputStream outputStream = fsTarget.create((expectedResolveResult));\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    FileSystem vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    // Test ${1} format\n+    // ^/(\\w+, /targetTestRoot/${1})\n+    // => /targetTestRoot/testConfLinkRegexIndexMapping2\n+    dstPathStr = targetTestRoot + \"${1}\";\n+    srcPath = new Path(\"/testConfLinkRegexIndexMapping2\");\n+    expectedResolveResult =\n+        new Path(dstPathStr.replace(\"${1}\", \"testConfLinkRegexIndexMapping2\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    //(^/(\\w+)/file1, /targetTestRoot/$1)\n+    // = > /targetTestRoot/testConfLinkRegexIndexMapping3/file1\n+    dstPathStr = targetTestRoot + \"$1\";\n+    srcPath = new Path(\"/testConfLinkRegexIndexMapping3/file1\");\n+    expectedResolveResult = new Path(\n+        dstPathStr.replace(\"$1\", \"testConfLinkRegexIndexMapping3/file1\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    //(^/(\\w+)/file1, /targetTestRoot/$1/)\n+    // = > /targetTestRoot/testConfLinkRegexIndexMapping4/file1\n+    dstPathStr = targetTestRoot + \"$1/\";\n+    srcPath = new Path(\"/testConfLinkRegexIndexMapping4/file1\");\n+    expectedResolveResult = new Path(\n+        dstPathStr.replace(\"$1\", \"testConfLinkRegexIndexMapping4/file1\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    vfs = FileSystem.get(viewFsUri, conf);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c58b945a32a978c470be07ef47e9ed7edfff428c"}, "originalPosition": 214}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0ODExMjQ5OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemLinkRegex.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxNzo1MDowNFrOHBzy_Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwMTo0ODowN1rOHCAd8Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTY1OTI2MQ==", "bodyText": "same as above...please take care of other similar places.", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471659261", "createdAt": "2020-08-17T17:50:04Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemLinkRegex.java", "diffHunk": "@@ -0,0 +1,320 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.MiniDFSNNTopology;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.fs.viewfs.Constants.CONFIG_VIEWFS_ENABLE_INNER_CACHE;\n+import static org.apache.hadoop.fs.viewfs.RegexMountPoint.INTERCEPTOR_INTERNAL_SEP;\n+\n+/**\n+ * Test linkRegex node type for view file system.\n+ */\n+public class TestViewFileSystemLinkRegex extends ViewFileSystemBaseTest {\n+  public static final Logger LOGGER =\n+      LoggerFactory.getLogger(TestViewFileSystemLinkRegex.class);\n+\n+  private static FileSystem fsDefault;\n+  private static MiniDFSCluster cluster;\n+  private static Configuration clusterConfig;\n+  private static final int NAME_SPACES_COUNT = 3;\n+  private static final int DATA_NODES_COUNT = 3;\n+  private static final int FS_INDEX_DEFAULT = 0;\n+  private static final FileSystem[] FS_HDFS = new FileSystem[NAME_SPACES_COUNT];\n+  private static final String CLUSTER_NAME =\n+      \"TestViewFileSystemLinkRegexCluster\";\n+  private static final File TEST_DIR = GenericTestUtils\n+      .getTestDir(TestViewFileSystemLinkRegex.class.getSimpleName());\n+  private static final String TEST_BASE_PATH =\n+      \"/tmp/TestViewFileSystemLinkRegex\";\n+\n+  @Override\n+  protected FileSystemTestHelper createFileSystemHelper() {\n+    return new FileSystemTestHelper(TEST_BASE_PATH);\n+  }\n+\n+  @BeforeClass\n+  public static void clusterSetupAtBeginning() throws IOException {\n+    SupportsBlocks = true;\n+    clusterConfig = ViewFileSystemTestSetup.createConfig();\n+    clusterConfig.setBoolean(\n+        DFSConfigKeys.DFS_NAMENODE_DELEGATION_TOKEN_ALWAYS_USE_KEY,\n+        true);\n+    cluster = new MiniDFSCluster.Builder(clusterConfig).nnTopology(\n+        MiniDFSNNTopology.simpleFederatedTopology(NAME_SPACES_COUNT))\n+        .numDataNodes(DATA_NODES_COUNT).build();\n+    cluster.waitClusterUp();\n+\n+    for (int i = 0; i < NAME_SPACES_COUNT; i++) {\n+      FS_HDFS[i] = cluster.getFileSystem(i);\n+    }\n+    fsDefault = FS_HDFS[FS_INDEX_DEFAULT];\n+  }\n+\n+  @AfterClass\n+  public static void clusterShutdownAtEnd() throws Exception {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  @Override\n+  @Before\n+  public void setUp() throws Exception {\n+    fsTarget = fsDefault;\n+    super.setUp();\n+  }\n+\n+  /**\n+   * Override this so that we don't set the targetTestRoot to any path under the\n+   * root of the FS, and so that we don't try to delete the test dir, but rather\n+   * only its contents.\n+   */\n+  @Override\n+  void initializeTargetTestRoot() throws IOException {\n+    targetTestRoot = fsDefault.makeQualified(new Path(\"/\"));\n+    for (FileStatus status : fsDefault.listStatus(targetTestRoot)) {\n+      fsDefault.delete(status.getPath(), true);\n+    }\n+  }\n+\n+  @Override\n+  void setupMountPoints() {\n+    super.setupMountPoints();\n+  }\n+\n+  @Override\n+  int getExpectedDelegationTokenCount() {\n+    return 1; // all point to the same fs so 1 unique token\n+  }\n+\n+  @Override\n+  int getExpectedDelegationTokenCountWithCredentials() {\n+    return 1;\n+  }\n+\n+  public String buildReplaceInterceptorSettingString(String srcRegex,\n+      String replaceString) {\n+    return\n+        RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH.getConfigName()\n+            + INTERCEPTOR_INTERNAL_SEP + srcRegex + INTERCEPTOR_INTERNAL_SEP\n+            + replaceString;\n+  }\n+\n+  public String linkInterceptorSettings(\n+      List<String> interceptorSettingStrList) {\n+    StringBuilder stringBuilder = new StringBuilder();\n+    int listSize = interceptorSettingStrList.size();\n+    for (int i = 0; i < listSize; ++i) {\n+      stringBuilder.append(interceptorSettingStrList.get(i));\n+      if (i < listSize - 1) {\n+        stringBuilder.append(RegexMountPoint.INTERCEPTOR_SEP);\n+      }\n+    }\n+    return stringBuilder.toString();\n+  }\n+\n+  @Test\n+  public void testConfLinkRegexIndexMapping() throws Exception {\n+    conf.setBoolean(CONFIG_VIEWFS_ENABLE_INNER_CACHE, false);\n+    // (^/(\\w+),/targetTestRoot/$1)\n+    // => /targetTestRoot/testConfLinkRegexIndexMapping1\n+    URI viewFsUri =\n+        new URI(FsConstants.VIEWFS_SCHEME, CLUSTER_NAME, \"/\", null, null);\n+    String regexStr = \"^/(\\\\w+)\";\n+    String dstPathStr = targetTestRoot + \"$1\";\n+    Path srcPath = new Path(\"/testConfLinkRegexIndexMapping1\");\n+    Path expectedResolveResult =\n+        new Path(dstPathStr.replace(\"$1\", \"testConfLinkRegexIndexMapping1\"));\n+    FSDataOutputStream outputStream = fsTarget.create((expectedResolveResult));\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    FileSystem vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    // Test ${1} format\n+    // ^/(\\w+, /targetTestRoot/${1})\n+    // => /targetTestRoot/testConfLinkRegexIndexMapping2\n+    dstPathStr = targetTestRoot + \"${1}\";\n+    srcPath = new Path(\"/testConfLinkRegexIndexMapping2\");\n+    expectedResolveResult =\n+        new Path(dstPathStr.replace(\"${1}\", \"testConfLinkRegexIndexMapping2\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    //(^/(\\w+)/file1, /targetTestRoot/$1)\n+    // = > /targetTestRoot/testConfLinkRegexIndexMapping3/file1\n+    dstPathStr = targetTestRoot + \"$1\";\n+    srcPath = new Path(\"/testConfLinkRegexIndexMapping3/file1\");\n+    expectedResolveResult = new Path(\n+        dstPathStr.replace(\"$1\", \"testConfLinkRegexIndexMapping3/file1\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    //(^/(\\w+)/file1, /targetTestRoot/$1/)\n+    // = > /targetTestRoot/testConfLinkRegexIndexMapping4/file1\n+    dstPathStr = targetTestRoot + \"$1/\";\n+    srcPath = new Path(\"/testConfLinkRegexIndexMapping4/file1\");\n+    expectedResolveResult = new Path(\n+        dstPathStr.replace(\"$1\", \"testConfLinkRegexIndexMapping4/file1\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+  }\n+\n+  @Test\n+  public void testConfLinkRegexNamedGroupMapping() throws Exception {\n+    conf.setBoolean(CONFIG_VIEWFS_ENABLE_INNER_CACHE, false);\n+    // ^/(?<firstDir>\\\\w+) = > /targetTestRoot/$firstDir\n+    URI viewFsUri =\n+        new URI(FsConstants.VIEWFS_SCHEME, CLUSTER_NAME, \"/\", null, null);\n+    String regexStr = \"^/(?<firstDir>\\\\w+)\";\n+    String dstPathStr = targetTestRoot + \"$firstDir\";\n+    Path srcPath = new Path(\"/testConfLinkRegexNamedGroupMapping1\");\n+    Path expectedResolveResult = new Path(\n+        dstPathStr.replace(\"$firstDir\", \"testConfLinkRegexNamedGroupMapping1\"));\n+    FSDataOutputStream outputStream = fsTarget.create((expectedResolveResult));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c58b945a32a978c470be07ef47e9ed7edfff428c"}, "originalPosition": 230}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg2Njg2NQ==", "bodyText": "Good point", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471866865", "createdAt": "2020-08-18T01:48:07Z", "author": {"login": "JohnZZGithub"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemLinkRegex.java", "diffHunk": "@@ -0,0 +1,320 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.MiniDFSNNTopology;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.fs.viewfs.Constants.CONFIG_VIEWFS_ENABLE_INNER_CACHE;\n+import static org.apache.hadoop.fs.viewfs.RegexMountPoint.INTERCEPTOR_INTERNAL_SEP;\n+\n+/**\n+ * Test linkRegex node type for view file system.\n+ */\n+public class TestViewFileSystemLinkRegex extends ViewFileSystemBaseTest {\n+  public static final Logger LOGGER =\n+      LoggerFactory.getLogger(TestViewFileSystemLinkRegex.class);\n+\n+  private static FileSystem fsDefault;\n+  private static MiniDFSCluster cluster;\n+  private static Configuration clusterConfig;\n+  private static final int NAME_SPACES_COUNT = 3;\n+  private static final int DATA_NODES_COUNT = 3;\n+  private static final int FS_INDEX_DEFAULT = 0;\n+  private static final FileSystem[] FS_HDFS = new FileSystem[NAME_SPACES_COUNT];\n+  private static final String CLUSTER_NAME =\n+      \"TestViewFileSystemLinkRegexCluster\";\n+  private static final File TEST_DIR = GenericTestUtils\n+      .getTestDir(TestViewFileSystemLinkRegex.class.getSimpleName());\n+  private static final String TEST_BASE_PATH =\n+      \"/tmp/TestViewFileSystemLinkRegex\";\n+\n+  @Override\n+  protected FileSystemTestHelper createFileSystemHelper() {\n+    return new FileSystemTestHelper(TEST_BASE_PATH);\n+  }\n+\n+  @BeforeClass\n+  public static void clusterSetupAtBeginning() throws IOException {\n+    SupportsBlocks = true;\n+    clusterConfig = ViewFileSystemTestSetup.createConfig();\n+    clusterConfig.setBoolean(\n+        DFSConfigKeys.DFS_NAMENODE_DELEGATION_TOKEN_ALWAYS_USE_KEY,\n+        true);\n+    cluster = new MiniDFSCluster.Builder(clusterConfig).nnTopology(\n+        MiniDFSNNTopology.simpleFederatedTopology(NAME_SPACES_COUNT))\n+        .numDataNodes(DATA_NODES_COUNT).build();\n+    cluster.waitClusterUp();\n+\n+    for (int i = 0; i < NAME_SPACES_COUNT; i++) {\n+      FS_HDFS[i] = cluster.getFileSystem(i);\n+    }\n+    fsDefault = FS_HDFS[FS_INDEX_DEFAULT];\n+  }\n+\n+  @AfterClass\n+  public static void clusterShutdownAtEnd() throws Exception {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  @Override\n+  @Before\n+  public void setUp() throws Exception {\n+    fsTarget = fsDefault;\n+    super.setUp();\n+  }\n+\n+  /**\n+   * Override this so that we don't set the targetTestRoot to any path under the\n+   * root of the FS, and so that we don't try to delete the test dir, but rather\n+   * only its contents.\n+   */\n+  @Override\n+  void initializeTargetTestRoot() throws IOException {\n+    targetTestRoot = fsDefault.makeQualified(new Path(\"/\"));\n+    for (FileStatus status : fsDefault.listStatus(targetTestRoot)) {\n+      fsDefault.delete(status.getPath(), true);\n+    }\n+  }\n+\n+  @Override\n+  void setupMountPoints() {\n+    super.setupMountPoints();\n+  }\n+\n+  @Override\n+  int getExpectedDelegationTokenCount() {\n+    return 1; // all point to the same fs so 1 unique token\n+  }\n+\n+  @Override\n+  int getExpectedDelegationTokenCountWithCredentials() {\n+    return 1;\n+  }\n+\n+  public String buildReplaceInterceptorSettingString(String srcRegex,\n+      String replaceString) {\n+    return\n+        RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH.getConfigName()\n+            + INTERCEPTOR_INTERNAL_SEP + srcRegex + INTERCEPTOR_INTERNAL_SEP\n+            + replaceString;\n+  }\n+\n+  public String linkInterceptorSettings(\n+      List<String> interceptorSettingStrList) {\n+    StringBuilder stringBuilder = new StringBuilder();\n+    int listSize = interceptorSettingStrList.size();\n+    for (int i = 0; i < listSize; ++i) {\n+      stringBuilder.append(interceptorSettingStrList.get(i));\n+      if (i < listSize - 1) {\n+        stringBuilder.append(RegexMountPoint.INTERCEPTOR_SEP);\n+      }\n+    }\n+    return stringBuilder.toString();\n+  }\n+\n+  @Test\n+  public void testConfLinkRegexIndexMapping() throws Exception {\n+    conf.setBoolean(CONFIG_VIEWFS_ENABLE_INNER_CACHE, false);\n+    // (^/(\\w+),/targetTestRoot/$1)\n+    // => /targetTestRoot/testConfLinkRegexIndexMapping1\n+    URI viewFsUri =\n+        new URI(FsConstants.VIEWFS_SCHEME, CLUSTER_NAME, \"/\", null, null);\n+    String regexStr = \"^/(\\\\w+)\";\n+    String dstPathStr = targetTestRoot + \"$1\";\n+    Path srcPath = new Path(\"/testConfLinkRegexIndexMapping1\");\n+    Path expectedResolveResult =\n+        new Path(dstPathStr.replace(\"$1\", \"testConfLinkRegexIndexMapping1\"));\n+    FSDataOutputStream outputStream = fsTarget.create((expectedResolveResult));\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    FileSystem vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    // Test ${1} format\n+    // ^/(\\w+, /targetTestRoot/${1})\n+    // => /targetTestRoot/testConfLinkRegexIndexMapping2\n+    dstPathStr = targetTestRoot + \"${1}\";\n+    srcPath = new Path(\"/testConfLinkRegexIndexMapping2\");\n+    expectedResolveResult =\n+        new Path(dstPathStr.replace(\"${1}\", \"testConfLinkRegexIndexMapping2\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    //(^/(\\w+)/file1, /targetTestRoot/$1)\n+    // = > /targetTestRoot/testConfLinkRegexIndexMapping3/file1\n+    dstPathStr = targetTestRoot + \"$1\";\n+    srcPath = new Path(\"/testConfLinkRegexIndexMapping3/file1\");\n+    expectedResolveResult = new Path(\n+        dstPathStr.replace(\"$1\", \"testConfLinkRegexIndexMapping3/file1\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    //(^/(\\w+)/file1, /targetTestRoot/$1/)\n+    // = > /targetTestRoot/testConfLinkRegexIndexMapping4/file1\n+    dstPathStr = targetTestRoot + \"$1/\";\n+    srcPath = new Path(\"/testConfLinkRegexIndexMapping4/file1\");\n+    expectedResolveResult = new Path(\n+        dstPathStr.replace(\"$1\", \"testConfLinkRegexIndexMapping4/file1\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+  }\n+\n+  @Test\n+  public void testConfLinkRegexNamedGroupMapping() throws Exception {\n+    conf.setBoolean(CONFIG_VIEWFS_ENABLE_INNER_CACHE, false);\n+    // ^/(?<firstDir>\\\\w+) = > /targetTestRoot/$firstDir\n+    URI viewFsUri =\n+        new URI(FsConstants.VIEWFS_SCHEME, CLUSTER_NAME, \"/\", null, null);\n+    String regexStr = \"^/(?<firstDir>\\\\w+)\";\n+    String dstPathStr = targetTestRoot + \"$firstDir\";\n+    Path srcPath = new Path(\"/testConfLinkRegexNamedGroupMapping1\");\n+    Path expectedResolveResult = new Path(\n+        dstPathStr.replace(\"$firstDir\", \"testConfLinkRegexNamedGroupMapping1\"));\n+    FSDataOutputStream outputStream = fsTarget.create((expectedResolveResult));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTY1OTI2MQ=="}, "originalCommit": {"oid": "c58b945a32a978c470be07ef47e9ed7edfff428c"}, "originalPosition": 230}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0ODE2MDA3OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemLinkRegex.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxODowMzoyNlrOHB0Psg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxODowMzoyNlrOHB0Psg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTY2NjYxMA==", "bodyText": "Could we assert the results of listStatus. Probably create few files under that expected target path. So, the ls will get some file and assert to make sure it's getting from target.", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471666610", "createdAt": "2020-08-17T18:03:26Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemLinkRegex.java", "diffHunk": "@@ -0,0 +1,320 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.MiniDFSNNTopology;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.fs.viewfs.Constants.CONFIG_VIEWFS_ENABLE_INNER_CACHE;\n+import static org.apache.hadoop.fs.viewfs.RegexMountPoint.INTERCEPTOR_INTERNAL_SEP;\n+\n+/**\n+ * Test linkRegex node type for view file system.\n+ */\n+public class TestViewFileSystemLinkRegex extends ViewFileSystemBaseTest {\n+  public static final Logger LOGGER =\n+      LoggerFactory.getLogger(TestViewFileSystemLinkRegex.class);\n+\n+  private static FileSystem fsDefault;\n+  private static MiniDFSCluster cluster;\n+  private static Configuration clusterConfig;\n+  private static final int NAME_SPACES_COUNT = 3;\n+  private static final int DATA_NODES_COUNT = 3;\n+  private static final int FS_INDEX_DEFAULT = 0;\n+  private static final FileSystem[] FS_HDFS = new FileSystem[NAME_SPACES_COUNT];\n+  private static final String CLUSTER_NAME =\n+      \"TestViewFileSystemLinkRegexCluster\";\n+  private static final File TEST_DIR = GenericTestUtils\n+      .getTestDir(TestViewFileSystemLinkRegex.class.getSimpleName());\n+  private static final String TEST_BASE_PATH =\n+      \"/tmp/TestViewFileSystemLinkRegex\";\n+\n+  @Override\n+  protected FileSystemTestHelper createFileSystemHelper() {\n+    return new FileSystemTestHelper(TEST_BASE_PATH);\n+  }\n+\n+  @BeforeClass\n+  public static void clusterSetupAtBeginning() throws IOException {\n+    SupportsBlocks = true;\n+    clusterConfig = ViewFileSystemTestSetup.createConfig();\n+    clusterConfig.setBoolean(\n+        DFSConfigKeys.DFS_NAMENODE_DELEGATION_TOKEN_ALWAYS_USE_KEY,\n+        true);\n+    cluster = new MiniDFSCluster.Builder(clusterConfig).nnTopology(\n+        MiniDFSNNTopology.simpleFederatedTopology(NAME_SPACES_COUNT))\n+        .numDataNodes(DATA_NODES_COUNT).build();\n+    cluster.waitClusterUp();\n+\n+    for (int i = 0; i < NAME_SPACES_COUNT; i++) {\n+      FS_HDFS[i] = cluster.getFileSystem(i);\n+    }\n+    fsDefault = FS_HDFS[FS_INDEX_DEFAULT];\n+  }\n+\n+  @AfterClass\n+  public static void clusterShutdownAtEnd() throws Exception {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  @Override\n+  @Before\n+  public void setUp() throws Exception {\n+    fsTarget = fsDefault;\n+    super.setUp();\n+  }\n+\n+  /**\n+   * Override this so that we don't set the targetTestRoot to any path under the\n+   * root of the FS, and so that we don't try to delete the test dir, but rather\n+   * only its contents.\n+   */\n+  @Override\n+  void initializeTargetTestRoot() throws IOException {\n+    targetTestRoot = fsDefault.makeQualified(new Path(\"/\"));\n+    for (FileStatus status : fsDefault.listStatus(targetTestRoot)) {\n+      fsDefault.delete(status.getPath(), true);\n+    }\n+  }\n+\n+  @Override\n+  void setupMountPoints() {\n+    super.setupMountPoints();\n+  }\n+\n+  @Override\n+  int getExpectedDelegationTokenCount() {\n+    return 1; // all point to the same fs so 1 unique token\n+  }\n+\n+  @Override\n+  int getExpectedDelegationTokenCountWithCredentials() {\n+    return 1;\n+  }\n+\n+  public String buildReplaceInterceptorSettingString(String srcRegex,\n+      String replaceString) {\n+    return\n+        RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH.getConfigName()\n+            + INTERCEPTOR_INTERNAL_SEP + srcRegex + INTERCEPTOR_INTERNAL_SEP\n+            + replaceString;\n+  }\n+\n+  public String linkInterceptorSettings(\n+      List<String> interceptorSettingStrList) {\n+    StringBuilder stringBuilder = new StringBuilder();\n+    int listSize = interceptorSettingStrList.size();\n+    for (int i = 0; i < listSize; ++i) {\n+      stringBuilder.append(interceptorSettingStrList.get(i));\n+      if (i < listSize - 1) {\n+        stringBuilder.append(RegexMountPoint.INTERCEPTOR_SEP);\n+      }\n+    }\n+    return stringBuilder.toString();\n+  }\n+\n+  @Test\n+  public void testConfLinkRegexIndexMapping() throws Exception {\n+    conf.setBoolean(CONFIG_VIEWFS_ENABLE_INNER_CACHE, false);\n+    // (^/(\\w+),/targetTestRoot/$1)\n+    // => /targetTestRoot/testConfLinkRegexIndexMapping1\n+    URI viewFsUri =\n+        new URI(FsConstants.VIEWFS_SCHEME, CLUSTER_NAME, \"/\", null, null);\n+    String regexStr = \"^/(\\\\w+)\";\n+    String dstPathStr = targetTestRoot + \"$1\";\n+    Path srcPath = new Path(\"/testConfLinkRegexIndexMapping1\");\n+    Path expectedResolveResult =\n+        new Path(dstPathStr.replace(\"$1\", \"testConfLinkRegexIndexMapping1\"));\n+    FSDataOutputStream outputStream = fsTarget.create((expectedResolveResult));\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    FileSystem vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    // Test ${1} format\n+    // ^/(\\w+, /targetTestRoot/${1})\n+    // => /targetTestRoot/testConfLinkRegexIndexMapping2\n+    dstPathStr = targetTestRoot + \"${1}\";\n+    srcPath = new Path(\"/testConfLinkRegexIndexMapping2\");\n+    expectedResolveResult =\n+        new Path(dstPathStr.replace(\"${1}\", \"testConfLinkRegexIndexMapping2\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    //(^/(\\w+)/file1, /targetTestRoot/$1)\n+    // = > /targetTestRoot/testConfLinkRegexIndexMapping3/file1\n+    dstPathStr = targetTestRoot + \"$1\";\n+    srcPath = new Path(\"/testConfLinkRegexIndexMapping3/file1\");\n+    expectedResolveResult = new Path(\n+        dstPathStr.replace(\"$1\", \"testConfLinkRegexIndexMapping3/file1\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    //(^/(\\w+)/file1, /targetTestRoot/$1/)\n+    // = > /targetTestRoot/testConfLinkRegexIndexMapping4/file1\n+    dstPathStr = targetTestRoot + \"$1/\";\n+    srcPath = new Path(\"/testConfLinkRegexIndexMapping4/file1\");\n+    expectedResolveResult = new Path(\n+        dstPathStr.replace(\"$1\", \"testConfLinkRegexIndexMapping4/file1\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+  }\n+\n+  @Test\n+  public void testConfLinkRegexNamedGroupMapping() throws Exception {\n+    conf.setBoolean(CONFIG_VIEWFS_ENABLE_INNER_CACHE, false);\n+    // ^/(?<firstDir>\\\\w+) = > /targetTestRoot/$firstDir\n+    URI viewFsUri =\n+        new URI(FsConstants.VIEWFS_SCHEME, CLUSTER_NAME, \"/\", null, null);\n+    String regexStr = \"^/(?<firstDir>\\\\w+)\";\n+    String dstPathStr = targetTestRoot + \"$firstDir\";\n+    Path srcPath = new Path(\"/testConfLinkRegexNamedGroupMapping1\");\n+    Path expectedResolveResult = new Path(\n+        dstPathStr.replace(\"$firstDir\", \"testConfLinkRegexNamedGroupMapping1\"));\n+    FSDataOutputStream outputStream = fsTarget.create((expectedResolveResult));\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    FileSystem vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    // Test ${1} format\n+    dstPathStr = targetTestRoot + \"${firstDir}\";\n+    srcPath = new Path(\"/testConfLinkRegexNamedGroupMapping2\");\n+    expectedResolveResult = new Path(dstPathStr\n+        .replace(\"${firstDir}\", \"testConfLinkRegexNamedGroupMapping2\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c58b945a32a978c470be07ef47e9ed7edfff428c"}, "originalPosition": 244}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0ODE4NDA2OnYy", "diffSide": "LEFT", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/ViewFileSystemBaseTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxODowOToxMVrOHB0dXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxODowOToxMVrOHB0dXQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTY3MDEwOQ==", "bodyText": "seems like no change here. Can we avoid it?", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471670109", "createdAt": "2020-08-17T18:09:11Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/ViewFileSystemBaseTest.java", "diffHunk": "@@ -1417,7 +1418,8 @@ public void testGetContentSummaryWithFileInLocalFS() throws Exception {\n       fos.write(expected.getBytes());\n     }\n     ConfigUtil.addLink(conf,\n-        \"/internalDir/internalDir2/linkToLocalFile\", localFile.toURI());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c58b945a32a978c470be07ef47e9ed7edfff428c"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0ODE5MDcwOnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestRegexMountPointResolvedDstPathReplaceInterceptor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxODoxMDo0MFrOHB0g-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxODoxMDo0MFrOHB0g-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTY3MTAzMw==", "bodyText": "Nits: please add annotation in separate line above. You may want to change for all such lines below and above.", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471671033", "createdAt": "2020-08-17T18:10:40Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestRegexMountPointResolvedDstPathReplaceInterceptor.java", "diffHunk": "@@ -0,0 +1,99 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import static org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH;\n+\n+/**\n+ * Test RegexMountPointResolvedDstPathReplaceInterceptor.\n+ */\n+public class TestRegexMountPointResolvedDstPathReplaceInterceptor {\n+\n+  public String createSerializedString(String regex, String replaceString) {\n+    return REPLACE_RESOLVED_DST_PATH.getConfigName()\n+        + RegexMountPoint.INTERCEPTOR_INTERNAL_SEP + regex\n+        + RegexMountPoint.INTERCEPTOR_INTERNAL_SEP + replaceString;\n+  }\n+\n+  @Test public void testDeserializeFromStringNormalCase() throws IOException {\n+    String srcRegex = \"-\";\n+    String replaceString = \"_\";\n+    String serializedString = createSerializedString(srcRegex, replaceString);\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        RegexMountPointResolvedDstPathReplaceInterceptor\n+            .deserializeFromString(serializedString);\n+    Assert.assertTrue(interceptor.getSrcRegexString().equals(srcRegex));\n+    Assert.assertTrue(interceptor.getReplaceString().equals(replaceString));\n+    Assert.assertTrue(interceptor.getSrcRegexPattern() == null);\n+    interceptor.initialize();\n+    Assert.assertTrue(\n+        interceptor.getSrcRegexPattern().toString().equals(srcRegex));\n+  }\n+\n+  @Test public void testDeserializeFromStringBadCase() throws IOException {\n+    String srcRegex = \"-\";\n+    String replaceString = \"_\";\n+    String serializedString = createSerializedString(srcRegex, replaceString);\n+    serializedString = serializedString + \":ddd\";\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        RegexMountPointResolvedDstPathReplaceInterceptor\n+            .deserializeFromString(serializedString);\n+    Assert.assertEquals(interceptor, null);\n+  }\n+\n+  @Test public void testSerialization() {\n+    String srcRegex = \"word1\";\n+    String replaceString = \"word2\";\n+    String serializedString = createSerializedString(srcRegex, replaceString);\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        new RegexMountPointResolvedDstPathReplaceInterceptor(srcRegex,\n+            replaceString);\n+    Assert.assertEquals(interceptor.serializeToString(), serializedString);\n+  }\n+\n+  @Test public void testInterceptSource() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c58b945a32a978c470be07ef47e9ed7edfff428c"}, "originalPosition": 75}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0ODIwMzY4OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/ViewFileSystemBaseTest.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxODoxNDo0MVrOHB0pAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwMzozMDo1NFrOHCCD5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTY3MzA4OQ==", "bodyText": "Not about this test, In general on inner cache disabling: There was a suggestion for avoiding explicitly user disabling it. If that not possible, you may want to have check while analyzing mount points itself that, if cache is enabled and using regex mounts, then you may want to fail fs initialization itself?", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471673089", "createdAt": "2020-08-17T18:14:41Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/ViewFileSystemBaseTest.java", "diffHunk": "@@ -1430,4 +1432,49 @@ public void testGetContentSummaryWithFileInLocalFS() throws Exception {\n           summaryAfter.getLength());\n     }\n   }\n+\n+  @Test\n+  public void testMountPointCache() throws Exception {\n+    conf.setInt(Constants.CONFIG_VIEWFS_PATH_RESOLUTION_CACHE_CAPACITY, 1);\n+    conf.setBoolean(\"fs.viewfs.impl.disable.cache\", true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c58b945a32a978c470be07ef47e9ed7edfff428c"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTgzMDYzMA==", "bodyText": "@umamaheswararao  This is a good point. Let me add some preconditions check.\nBTW, now the inner cache assumes every filesystem is created while InodeTree is constructed and never changed. Do you think it's reasonable to change it to a concurrent hash map?", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471830630", "createdAt": "2020-08-17T23:34:12Z", "author": {"login": "JohnZZGithub"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/ViewFileSystemBaseTest.java", "diffHunk": "@@ -1430,4 +1432,49 @@ public void testGetContentSummaryWithFileInLocalFS() throws Exception {\n           summaryAfter.getLength());\n     }\n   }\n+\n+  @Test\n+  public void testMountPointCache() throws Exception {\n+    conf.setInt(Constants.CONFIG_VIEWFS_PATH_RESOLUTION_CACHE_CAPACITY, 1);\n+    conf.setBoolean(\"fs.viewfs.impl.disable.cache\", true);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTY3MzA4OQ=="}, "originalCommit": {"oid": "c58b945a32a978c470be07ef47e9ed7edfff428c"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg5Mjk2Nw==", "bodyText": "One more thing to clarify, I guess this config is for per schema level cache. Regex mount point is OK with it. Mount table is not good inner cache inside ViewFileSystem.java.", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471892967", "createdAt": "2020-08-18T03:30:54Z", "author": {"login": "JohnZZGithub"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/ViewFileSystemBaseTest.java", "diffHunk": "@@ -1430,4 +1432,49 @@ public void testGetContentSummaryWithFileInLocalFS() throws Exception {\n           summaryAfter.getLength());\n     }\n   }\n+\n+  @Test\n+  public void testMountPointCache() throws Exception {\n+    conf.setInt(Constants.CONFIG_VIEWFS_PATH_RESOLUTION_CACHE_CAPACITY, 1);\n+    conf.setBoolean(\"fs.viewfs.impl.disable.cache\", true);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTY3MzA4OQ=="}, "originalCommit": {"oid": "c58b945a32a978c470be07ef47e9ed7edfff428c"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0ODQ1MjQwOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/site/markdown/ViewFs.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxOTowNDozN1rOHB3Rcg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QyMzozNDo1NlrOHB-RYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcxNjIxMA==", "bodyText": "I agree with this issue and filed a JIRA for it. https://issues.apache.org/jira/browse/HADOOP-17028\nThis issue should be solved once implement that.", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471716210", "createdAt": "2020-08-17T19:04:37Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/site/markdown/ViewFs.md", "diffHunk": "@@ -366,6 +366,82 @@ Don't want to change scheme or difficult to copy mount-table configurations to a\n \n Please refer to the [View File System Overload Scheme Guide](./ViewFsOverloadScheme.html)\n \n+Regex Pattern Based Mount Points\n+--------------------------------\n+\n+The view file system mount points were a Key-Value based mapping system. It is not friendly for user cases which mapping config could be abstracted to rules. E.g. Users want to provide a GCS bucket per user and there might be thousands of users in total. The old key-value based approach won't work well for several reasons:\n+\n+1. The mount table is used by FileSystem clients. There's a cost to spread the config to all clients and we should avoid it if possible. The [View File System Overload Scheme Guide](./ViewFsOverloadScheme.html) could help the distribution by central mount table management. But the mount table still have to be updated on every change. The change could be greatly avoided if provide a rule-based mount table..\n+\n+2. The client have to understand all the KVs in the mount table. This is not ideal when the mountable grows to thousands of items. E.g. thousands of file systems might be initialized even users only need one. And the config itself will become bloated at scale.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c58b945a32a978c470be07ef47e9ed7edfff428c"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTgzMDg4Mg==", "bodyText": "I didn't realize that there is a jira. This is great.", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471830882", "createdAt": "2020-08-17T23:34:56Z", "author": {"login": "JohnZZGithub"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/site/markdown/ViewFs.md", "diffHunk": "@@ -366,6 +366,82 @@ Don't want to change scheme or difficult to copy mount-table configurations to a\n \n Please refer to the [View File System Overload Scheme Guide](./ViewFsOverloadScheme.html)\n \n+Regex Pattern Based Mount Points\n+--------------------------------\n+\n+The view file system mount points were a Key-Value based mapping system. It is not friendly for user cases which mapping config could be abstracted to rules. E.g. Users want to provide a GCS bucket per user and there might be thousands of users in total. The old key-value based approach won't work well for several reasons:\n+\n+1. The mount table is used by FileSystem clients. There's a cost to spread the config to all clients and we should avoid it if possible. The [View File System Overload Scheme Guide](./ViewFsOverloadScheme.html) could help the distribution by central mount table management. But the mount table still have to be updated on every change. The change could be greatly avoided if provide a rule-based mount table..\n+\n+2. The client have to understand all the KVs in the mount table. This is not ideal when the mountable grows to thousands of items. E.g. thousands of file systems might be initialized even users only need one. And the config itself will become bloated at scale.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcxNjIxMA=="}, "originalCommit": {"oid": "c58b945a32a978c470be07ef47e9ed7edfff428c"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0ODQ4MDIwOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemLinkRegex.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxOToxMjozOVrOHB3h_A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwMzoyNzoxN1rOHCCAWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcyMDQ0NA==", "bodyText": "Do you mind writing some description test steps in javadoc. It will be helpful to better understand about scenarios.", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471720444", "createdAt": "2020-08-17T19:12:39Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemLinkRegex.java", "diffHunk": "@@ -0,0 +1,320 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.MiniDFSNNTopology;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.fs.viewfs.Constants.CONFIG_VIEWFS_ENABLE_INNER_CACHE;\n+import static org.apache.hadoop.fs.viewfs.RegexMountPoint.INTERCEPTOR_INTERNAL_SEP;\n+\n+/**\n+ * Test linkRegex node type for view file system.\n+ */\n+public class TestViewFileSystemLinkRegex extends ViewFileSystemBaseTest {\n+  public static final Logger LOGGER =\n+      LoggerFactory.getLogger(TestViewFileSystemLinkRegex.class);\n+\n+  private static FileSystem fsDefault;\n+  private static MiniDFSCluster cluster;\n+  private static Configuration clusterConfig;\n+  private static final int NAME_SPACES_COUNT = 3;\n+  private static final int DATA_NODES_COUNT = 3;\n+  private static final int FS_INDEX_DEFAULT = 0;\n+  private static final FileSystem[] FS_HDFS = new FileSystem[NAME_SPACES_COUNT];\n+  private static final String CLUSTER_NAME =\n+      \"TestViewFileSystemLinkRegexCluster\";\n+  private static final File TEST_DIR = GenericTestUtils\n+      .getTestDir(TestViewFileSystemLinkRegex.class.getSimpleName());\n+  private static final String TEST_BASE_PATH =\n+      \"/tmp/TestViewFileSystemLinkRegex\";\n+\n+  @Override\n+  protected FileSystemTestHelper createFileSystemHelper() {\n+    return new FileSystemTestHelper(TEST_BASE_PATH);\n+  }\n+\n+  @BeforeClass\n+  public static void clusterSetupAtBeginning() throws IOException {\n+    SupportsBlocks = true;\n+    clusterConfig = ViewFileSystemTestSetup.createConfig();\n+    clusterConfig.setBoolean(\n+        DFSConfigKeys.DFS_NAMENODE_DELEGATION_TOKEN_ALWAYS_USE_KEY,\n+        true);\n+    cluster = new MiniDFSCluster.Builder(clusterConfig).nnTopology(\n+        MiniDFSNNTopology.simpleFederatedTopology(NAME_SPACES_COUNT))\n+        .numDataNodes(DATA_NODES_COUNT).build();\n+    cluster.waitClusterUp();\n+\n+    for (int i = 0; i < NAME_SPACES_COUNT; i++) {\n+      FS_HDFS[i] = cluster.getFileSystem(i);\n+    }\n+    fsDefault = FS_HDFS[FS_INDEX_DEFAULT];\n+  }\n+\n+  @AfterClass\n+  public static void clusterShutdownAtEnd() throws Exception {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  @Override\n+  @Before\n+  public void setUp() throws Exception {\n+    fsTarget = fsDefault;\n+    super.setUp();\n+  }\n+\n+  /**\n+   * Override this so that we don't set the targetTestRoot to any path under the\n+   * root of the FS, and so that we don't try to delete the test dir, but rather\n+   * only its contents.\n+   */\n+  @Override\n+  void initializeTargetTestRoot() throws IOException {\n+    targetTestRoot = fsDefault.makeQualified(new Path(\"/\"));\n+    for (FileStatus status : fsDefault.listStatus(targetTestRoot)) {\n+      fsDefault.delete(status.getPath(), true);\n+    }\n+  }\n+\n+  @Override\n+  void setupMountPoints() {\n+    super.setupMountPoints();\n+  }\n+\n+  @Override\n+  int getExpectedDelegationTokenCount() {\n+    return 1; // all point to the same fs so 1 unique token\n+  }\n+\n+  @Override\n+  int getExpectedDelegationTokenCountWithCredentials() {\n+    return 1;\n+  }\n+\n+  public String buildReplaceInterceptorSettingString(String srcRegex,\n+      String replaceString) {\n+    return\n+        RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH.getConfigName()\n+            + INTERCEPTOR_INTERNAL_SEP + srcRegex + INTERCEPTOR_INTERNAL_SEP\n+            + replaceString;\n+  }\n+\n+  public String linkInterceptorSettings(\n+      List<String> interceptorSettingStrList) {\n+    StringBuilder stringBuilder = new StringBuilder();\n+    int listSize = interceptorSettingStrList.size();\n+    for (int i = 0; i < listSize; ++i) {\n+      stringBuilder.append(interceptorSettingStrList.get(i));\n+      if (i < listSize - 1) {\n+        stringBuilder.append(RegexMountPoint.INTERCEPTOR_SEP);\n+      }\n+    }\n+    return stringBuilder.toString();\n+  }\n+\n+  @Test\n+  public void testConfLinkRegexIndexMapping() throws Exception {\n+    conf.setBoolean(CONFIG_VIEWFS_ENABLE_INNER_CACHE, false);\n+    // (^/(\\w+),/targetTestRoot/$1)\n+    // => /targetTestRoot/testConfLinkRegexIndexMapping1\n+    URI viewFsUri =\n+        new URI(FsConstants.VIEWFS_SCHEME, CLUSTER_NAME, \"/\", null, null);\n+    String regexStr = \"^/(\\\\w+)\";\n+    String dstPathStr = targetTestRoot + \"$1\";\n+    Path srcPath = new Path(\"/testConfLinkRegexIndexMapping1\");\n+    Path expectedResolveResult =\n+        new Path(dstPathStr.replace(\"$1\", \"testConfLinkRegexIndexMapping1\"));\n+    FSDataOutputStream outputStream = fsTarget.create((expectedResolveResult));\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    FileSystem vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    // Test ${1} format\n+    // ^/(\\w+, /targetTestRoot/${1})\n+    // => /targetTestRoot/testConfLinkRegexIndexMapping2\n+    dstPathStr = targetTestRoot + \"${1}\";\n+    srcPath = new Path(\"/testConfLinkRegexIndexMapping2\");\n+    expectedResolveResult =\n+        new Path(dstPathStr.replace(\"${1}\", \"testConfLinkRegexIndexMapping2\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    //(^/(\\w+)/file1, /targetTestRoot/$1)\n+    // = > /targetTestRoot/testConfLinkRegexIndexMapping3/file1\n+    dstPathStr = targetTestRoot + \"$1\";\n+    srcPath = new Path(\"/testConfLinkRegexIndexMapping3/file1\");\n+    expectedResolveResult = new Path(\n+        dstPathStr.replace(\"$1\", \"testConfLinkRegexIndexMapping3/file1\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    //(^/(\\w+)/file1, /targetTestRoot/$1/)\n+    // = > /targetTestRoot/testConfLinkRegexIndexMapping4/file1\n+    dstPathStr = targetTestRoot + \"$1/\";\n+    srcPath = new Path(\"/testConfLinkRegexIndexMapping4/file1\");\n+    expectedResolveResult = new Path(\n+        dstPathStr.replace(\"$1\", \"testConfLinkRegexIndexMapping4/file1\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+  }\n+\n+  @Test\n+  public void testConfLinkRegexNamedGroupMapping() throws Exception {\n+    conf.setBoolean(CONFIG_VIEWFS_ENABLE_INNER_CACHE, false);\n+    // ^/(?<firstDir>\\\\w+) = > /targetTestRoot/$firstDir\n+    URI viewFsUri =\n+        new URI(FsConstants.VIEWFS_SCHEME, CLUSTER_NAME, \"/\", null, null);\n+    String regexStr = \"^/(?<firstDir>\\\\w+)\";\n+    String dstPathStr = targetTestRoot + \"$firstDir\";\n+    Path srcPath = new Path(\"/testConfLinkRegexNamedGroupMapping1\");\n+    Path expectedResolveResult = new Path(\n+        dstPathStr.replace(\"$firstDir\", \"testConfLinkRegexNamedGroupMapping1\"));\n+    FSDataOutputStream outputStream = fsTarget.create((expectedResolveResult));\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    FileSystem vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    // Test ${1} format\n+    dstPathStr = targetTestRoot + \"${firstDir}\";\n+    srcPath = new Path(\"/testConfLinkRegexNamedGroupMapping2\");\n+    expectedResolveResult = new Path(dstPathStr\n+        .replace(\"${firstDir}\", \"testConfLinkRegexNamedGroupMapping2\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+  }\n+\n+  @Test\n+  public void testConfLinkRegexFixedDestMapping() throws Exception {\n+    conf.setBoolean(CONFIG_VIEWFS_ENABLE_INNER_CACHE, false);\n+    URI viewFsUri =\n+        new URI(FsConstants.VIEWFS_SCHEME, CLUSTER_NAME, \"/\", null, null);\n+    String regexStr = \"^/\\\\w+\";\n+    String dstPathStr =\n+        targetTestRoot + \"testConfLinkRegexFixedDestMappingFile\";\n+    Path expectedResolveResult = new Path(dstPathStr);\n+    FSDataOutputStream outputStream = fsTarget.create((expectedResolveResult));\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    FileSystem vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(\n+        expectedResolveResult.equals(vfs.resolvePath(new Path(\"/misc1\"))));\n+    Assert.assertTrue(\n+        expectedResolveResult.equals(vfs.resolvePath(new Path(\"/misc2\"))));\n+  }\n+\n+  @Test\n+  public void testConfLinkRegexWithSingleInterceptor() throws Exception {\n+    conf.setBoolean(CONFIG_VIEWFS_ENABLE_INNER_CACHE, false);\n+    URI viewFsUri =\n+        new URI(FsConstants.VIEWFS_SCHEME, CLUSTER_NAME, \"/\", null, null);\n+    String regexStr = \"^/user/(?<username>\\\\w+)\";\n+    String dstPathStr = targetTestRoot + \"$username\";\n+    // Replace \"_\" with \"-\"\n+    String settingString = buildReplaceInterceptorSettingString(\"_\", \"-\");\n+    Path srcPath = new Path(\"/user/hadoop_user1/hadoop_file1\");\n+    Path expectedResolveResult =\n+        new Path(targetTestRoot, \"hadoop-user1/hadoop_file1\");\n+    FSDataOutputStream outputStream = fsTarget.create((expectedResolveResult));\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil\n+        .addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, settingString);\n+    FileSystem vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+  }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c58b945a32a978c470be07ef47e9ed7edfff428c"}, "originalPosition": 293}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg5MjA1OA==", "bodyText": "Sure.", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471892058", "createdAt": "2020-08-18T03:27:17Z", "author": {"login": "JohnZZGithub"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemLinkRegex.java", "diffHunk": "@@ -0,0 +1,320 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.MiniDFSNNTopology;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.fs.viewfs.Constants.CONFIG_VIEWFS_ENABLE_INNER_CACHE;\n+import static org.apache.hadoop.fs.viewfs.RegexMountPoint.INTERCEPTOR_INTERNAL_SEP;\n+\n+/**\n+ * Test linkRegex node type for view file system.\n+ */\n+public class TestViewFileSystemLinkRegex extends ViewFileSystemBaseTest {\n+  public static final Logger LOGGER =\n+      LoggerFactory.getLogger(TestViewFileSystemLinkRegex.class);\n+\n+  private static FileSystem fsDefault;\n+  private static MiniDFSCluster cluster;\n+  private static Configuration clusterConfig;\n+  private static final int NAME_SPACES_COUNT = 3;\n+  private static final int DATA_NODES_COUNT = 3;\n+  private static final int FS_INDEX_DEFAULT = 0;\n+  private static final FileSystem[] FS_HDFS = new FileSystem[NAME_SPACES_COUNT];\n+  private static final String CLUSTER_NAME =\n+      \"TestViewFileSystemLinkRegexCluster\";\n+  private static final File TEST_DIR = GenericTestUtils\n+      .getTestDir(TestViewFileSystemLinkRegex.class.getSimpleName());\n+  private static final String TEST_BASE_PATH =\n+      \"/tmp/TestViewFileSystemLinkRegex\";\n+\n+  @Override\n+  protected FileSystemTestHelper createFileSystemHelper() {\n+    return new FileSystemTestHelper(TEST_BASE_PATH);\n+  }\n+\n+  @BeforeClass\n+  public static void clusterSetupAtBeginning() throws IOException {\n+    SupportsBlocks = true;\n+    clusterConfig = ViewFileSystemTestSetup.createConfig();\n+    clusterConfig.setBoolean(\n+        DFSConfigKeys.DFS_NAMENODE_DELEGATION_TOKEN_ALWAYS_USE_KEY,\n+        true);\n+    cluster = new MiniDFSCluster.Builder(clusterConfig).nnTopology(\n+        MiniDFSNNTopology.simpleFederatedTopology(NAME_SPACES_COUNT))\n+        .numDataNodes(DATA_NODES_COUNT).build();\n+    cluster.waitClusterUp();\n+\n+    for (int i = 0; i < NAME_SPACES_COUNT; i++) {\n+      FS_HDFS[i] = cluster.getFileSystem(i);\n+    }\n+    fsDefault = FS_HDFS[FS_INDEX_DEFAULT];\n+  }\n+\n+  @AfterClass\n+  public static void clusterShutdownAtEnd() throws Exception {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  @Override\n+  @Before\n+  public void setUp() throws Exception {\n+    fsTarget = fsDefault;\n+    super.setUp();\n+  }\n+\n+  /**\n+   * Override this so that we don't set the targetTestRoot to any path under the\n+   * root of the FS, and so that we don't try to delete the test dir, but rather\n+   * only its contents.\n+   */\n+  @Override\n+  void initializeTargetTestRoot() throws IOException {\n+    targetTestRoot = fsDefault.makeQualified(new Path(\"/\"));\n+    for (FileStatus status : fsDefault.listStatus(targetTestRoot)) {\n+      fsDefault.delete(status.getPath(), true);\n+    }\n+  }\n+\n+  @Override\n+  void setupMountPoints() {\n+    super.setupMountPoints();\n+  }\n+\n+  @Override\n+  int getExpectedDelegationTokenCount() {\n+    return 1; // all point to the same fs so 1 unique token\n+  }\n+\n+  @Override\n+  int getExpectedDelegationTokenCountWithCredentials() {\n+    return 1;\n+  }\n+\n+  public String buildReplaceInterceptorSettingString(String srcRegex,\n+      String replaceString) {\n+    return\n+        RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH.getConfigName()\n+            + INTERCEPTOR_INTERNAL_SEP + srcRegex + INTERCEPTOR_INTERNAL_SEP\n+            + replaceString;\n+  }\n+\n+  public String linkInterceptorSettings(\n+      List<String> interceptorSettingStrList) {\n+    StringBuilder stringBuilder = new StringBuilder();\n+    int listSize = interceptorSettingStrList.size();\n+    for (int i = 0; i < listSize; ++i) {\n+      stringBuilder.append(interceptorSettingStrList.get(i));\n+      if (i < listSize - 1) {\n+        stringBuilder.append(RegexMountPoint.INTERCEPTOR_SEP);\n+      }\n+    }\n+    return stringBuilder.toString();\n+  }\n+\n+  @Test\n+  public void testConfLinkRegexIndexMapping() throws Exception {\n+    conf.setBoolean(CONFIG_VIEWFS_ENABLE_INNER_CACHE, false);\n+    // (^/(\\w+),/targetTestRoot/$1)\n+    // => /targetTestRoot/testConfLinkRegexIndexMapping1\n+    URI viewFsUri =\n+        new URI(FsConstants.VIEWFS_SCHEME, CLUSTER_NAME, \"/\", null, null);\n+    String regexStr = \"^/(\\\\w+)\";\n+    String dstPathStr = targetTestRoot + \"$1\";\n+    Path srcPath = new Path(\"/testConfLinkRegexIndexMapping1\");\n+    Path expectedResolveResult =\n+        new Path(dstPathStr.replace(\"$1\", \"testConfLinkRegexIndexMapping1\"));\n+    FSDataOutputStream outputStream = fsTarget.create((expectedResolveResult));\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    FileSystem vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    // Test ${1} format\n+    // ^/(\\w+, /targetTestRoot/${1})\n+    // => /targetTestRoot/testConfLinkRegexIndexMapping2\n+    dstPathStr = targetTestRoot + \"${1}\";\n+    srcPath = new Path(\"/testConfLinkRegexIndexMapping2\");\n+    expectedResolveResult =\n+        new Path(dstPathStr.replace(\"${1}\", \"testConfLinkRegexIndexMapping2\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    //(^/(\\w+)/file1, /targetTestRoot/$1)\n+    // = > /targetTestRoot/testConfLinkRegexIndexMapping3/file1\n+    dstPathStr = targetTestRoot + \"$1\";\n+    srcPath = new Path(\"/testConfLinkRegexIndexMapping3/file1\");\n+    expectedResolveResult = new Path(\n+        dstPathStr.replace(\"$1\", \"testConfLinkRegexIndexMapping3/file1\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    //(^/(\\w+)/file1, /targetTestRoot/$1/)\n+    // = > /targetTestRoot/testConfLinkRegexIndexMapping4/file1\n+    dstPathStr = targetTestRoot + \"$1/\";\n+    srcPath = new Path(\"/testConfLinkRegexIndexMapping4/file1\");\n+    expectedResolveResult = new Path(\n+        dstPathStr.replace(\"$1\", \"testConfLinkRegexIndexMapping4/file1\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+  }\n+\n+  @Test\n+  public void testConfLinkRegexNamedGroupMapping() throws Exception {\n+    conf.setBoolean(CONFIG_VIEWFS_ENABLE_INNER_CACHE, false);\n+    // ^/(?<firstDir>\\\\w+) = > /targetTestRoot/$firstDir\n+    URI viewFsUri =\n+        new URI(FsConstants.VIEWFS_SCHEME, CLUSTER_NAME, \"/\", null, null);\n+    String regexStr = \"^/(?<firstDir>\\\\w+)\";\n+    String dstPathStr = targetTestRoot + \"$firstDir\";\n+    Path srcPath = new Path(\"/testConfLinkRegexNamedGroupMapping1\");\n+    Path expectedResolveResult = new Path(\n+        dstPathStr.replace(\"$firstDir\", \"testConfLinkRegexNamedGroupMapping1\"));\n+    FSDataOutputStream outputStream = fsTarget.create((expectedResolveResult));\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    FileSystem vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    // Test ${1} format\n+    dstPathStr = targetTestRoot + \"${firstDir}\";\n+    srcPath = new Path(\"/testConfLinkRegexNamedGroupMapping2\");\n+    expectedResolveResult = new Path(dstPathStr\n+        .replace(\"${firstDir}\", \"testConfLinkRegexNamedGroupMapping2\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+  }\n+\n+  @Test\n+  public void testConfLinkRegexFixedDestMapping() throws Exception {\n+    conf.setBoolean(CONFIG_VIEWFS_ENABLE_INNER_CACHE, false);\n+    URI viewFsUri =\n+        new URI(FsConstants.VIEWFS_SCHEME, CLUSTER_NAME, \"/\", null, null);\n+    String regexStr = \"^/\\\\w+\";\n+    String dstPathStr =\n+        targetTestRoot + \"testConfLinkRegexFixedDestMappingFile\";\n+    Path expectedResolveResult = new Path(dstPathStr);\n+    FSDataOutputStream outputStream = fsTarget.create((expectedResolveResult));\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    FileSystem vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(\n+        expectedResolveResult.equals(vfs.resolvePath(new Path(\"/misc1\"))));\n+    Assert.assertTrue(\n+        expectedResolveResult.equals(vfs.resolvePath(new Path(\"/misc2\"))));\n+  }\n+\n+  @Test\n+  public void testConfLinkRegexWithSingleInterceptor() throws Exception {\n+    conf.setBoolean(CONFIG_VIEWFS_ENABLE_INNER_CACHE, false);\n+    URI viewFsUri =\n+        new URI(FsConstants.VIEWFS_SCHEME, CLUSTER_NAME, \"/\", null, null);\n+    String regexStr = \"^/user/(?<username>\\\\w+)\";\n+    String dstPathStr = targetTestRoot + \"$username\";\n+    // Replace \"_\" with \"-\"\n+    String settingString = buildReplaceInterceptorSettingString(\"_\", \"-\");\n+    Path srcPath = new Path(\"/user/hadoop_user1/hadoop_file1\");\n+    Path expectedResolveResult =\n+        new Path(targetTestRoot, \"hadoop-user1/hadoop_file1\");\n+    FSDataOutputStream outputStream = fsTarget.create((expectedResolveResult));\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil\n+        .addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, settingString);\n+    FileSystem vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+  }\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcyMDQ0NA=="}, "originalCommit": {"oid": "c58b945a32a978c470be07ef47e9ed7edfff428c"}, "originalPosition": 293}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAwNjI0ODkwOnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/RegexMountPoint.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwNDo0NDozMFrOHKhazg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQxODozNToyM1rOHLDSVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDc5NTM0Mg==", "bodyText": "Below method seems like unused method? Please removed it.", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r480795342", "createdAt": "2020-09-01T04:44:30Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/RegexMountPoint.java", "diffHunk": "@@ -0,0 +1,306 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.regex.PatternSyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.util.StringUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.fs.viewfs.InodeTree.SlashPath;\n+\n+/**\n+ * Regex mount point is build to implement regex based mount point.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Unstable\n+class RegexMountPoint<T> {\n+  private static final Logger LOGGER =\n+      LoggerFactory.getLogger(RegexMountPoint.class.getName());\n+\n+  private InodeTree inodeTree;\n+  private String srcPathRegex;\n+  private Pattern srcPattern;\n+  private String dstPath;\n+  private String interceptorSettingsString;\n+  private List<RegexMountPointInterceptor> interceptorList;\n+\n+  public static final String SETTING_SRCREGEX_SEP = \"#.\";\n+  public static final char INTERCEPTOR_SEP = ';';\n+  public static final char INTERCEPTOR_INTERNAL_SEP = ':';\n+  // ${var},$var\n+  public static final Pattern VAR_PATTERN_IN_DEST =\n+      Pattern.compile(\"\\\\$((\\\\{\\\\w+\\\\})|(\\\\w+))\");\n+\n+  // Same var might have different representations.\n+  // e.g.\n+  // key => $key or key = > ${key}\n+  private Map<String, Set<String>> varInDestPathMap;\n+\n+  public Map<String, Set<String>> getVarInDestPathMap() {\n+    return varInDestPathMap;\n+  }\n+\n+  RegexMountPoint(InodeTree inodeTree, String sourcePathRegex,\n+      String destPath, String settingsStr) {\n+    this.inodeTree = inodeTree;\n+    this.srcPathRegex = sourcePathRegex;\n+    this.dstPath = destPath;\n+    this.interceptorSettingsString = settingsStr;\n+    this.interceptorList = new ArrayList<>();\n+  }\n+\n+  /**\n+   * Initialize regex mount point.\n+   *\n+   * @throws IOException\n+   */\n+  public void initialize() throws IOException {\n+    try {\n+      srcPattern = Pattern.compile(srcPathRegex);\n+    } catch (PatternSyntaxException ex) {\n+      throw new IOException(\n+          \"Failed to initialized mount point due to bad src path regex:\"\n+              + srcPathRegex + \", dstPath:\" + dstPath, ex);\n+    }\n+    varInDestPathMap = getVarListInString(dstPath);\n+    initializeInterceptors();\n+  }\n+\n+  private void initializeInterceptors() throws IOException {\n+    if (interceptorSettingsString == null\n+        || interceptorSettingsString.isEmpty()) {\n+      return;\n+    }\n+    String[] interceptorStrArray =\n+        StringUtils.split(interceptorSettingsString, INTERCEPTOR_SEP);\n+    for (String interceptorStr : interceptorStrArray) {\n+      RegexMountPointInterceptor interceptor =\n+          RegexMountPointInterceptorFactory.create(interceptorStr);\n+      if (interceptor == null) {\n+        throw new IOException(\n+            \"Illegal settings String \" + interceptorSettingsString);\n+      }\n+      interceptor.initialize();\n+      interceptorList.add(interceptor);\n+    }\n+  }\n+\n+  /**\n+   * Get $var1 and $var2 style variables in string.\n+   *\n+   * @param input - the string to be process.\n+   * @return\n+   */\n+  public static Map<String, Set<String>> getVarListInString(String input) {\n+    Map<String, Set<String>> varMap = new HashMap<>();\n+    Matcher matcher = VAR_PATTERN_IN_DEST.matcher(input);\n+    while (matcher.find()) {\n+      // $var or ${var}\n+      String varName = matcher.group(0);\n+      // var or {var}\n+      String strippedVarName = matcher.group(1);\n+      if (strippedVarName.startsWith(\"{\")) {\n+        // {varName} = > varName\n+        strippedVarName =\n+            strippedVarName.substring(1, strippedVarName.length() - 1);\n+      }\n+      varMap.putIfAbsent(strippedVarName, new HashSet<>());\n+      varMap.get(strippedVarName).add(varName);\n+    }\n+    return varMap;\n+  }\n+\n+  public String getSrcPathRegex() {\n+    return srcPathRegex;\n+  }\n+\n+  public Pattern getSrcPattern() {\n+    return srcPattern;\n+  }\n+\n+  public String getDstPath() {\n+    return dstPath;\n+  }\n+\n+  public static Pattern getVarPatternInDest() {\n+    return VAR_PATTERN_IN_DEST;\n+  }\n+\n+  /**\n+   * Get resolved path from regex mount points.\n+   *  E.g. link: ^/user/(?<username>\\\\w+) => s3://$user.apache.com/_${user}\n+   *  srcPath: is /user/hadoop/dir1\n+   *  resolveLastComponent: true\n+   *  then return value is s3://hadoop.apache.com/_hadoop\n+   * @param srcPath - the src path to resolve\n+   * @param resolveLastComponent - whether resolve the path after last `/`\n+   * @return mapped path of the mount point.\n+   */\n+  public InodeTree.ResolveResult<T> resolve(final String srcPath,\n+      final boolean resolveLastComponent) {\n+    String pathStrToResolve = getPathToResolve(srcPath, resolveLastComponent);\n+    for (RegexMountPointInterceptor interceptor : interceptorList) {\n+      pathStrToResolve = interceptor.interceptSource(pathStrToResolve);\n+    }\n+    LOGGER.debug(\"Path to resolve:\" + pathStrToResolve + \", srcPattern:\"\n+        + getSrcPathRegex());\n+    Matcher srcMatcher = getSrcPattern().matcher(pathStrToResolve);\n+    String parsedDestPath = getDstPath();\n+    int mappedCount = 0;\n+    String resolvedPathStr = \"\";\n+    while (srcMatcher.find()) {\n+      resolvedPathStr = pathStrToResolve.substring(0, srcMatcher.end());\n+      Map<String, Set<String>> varMap = getVarInDestPathMap();\n+      for (Map.Entry<String, Set<String>> entry : varMap.entrySet()) {\n+        String regexGroupNameOrIndexStr = entry.getKey();\n+        Set<String> groupRepresentationStrSetInDest = entry.getValue();\n+        parsedDestPath = replaceRegexCaptureGroupInPath(\n+            parsedDestPath, srcMatcher,\n+            regexGroupNameOrIndexStr, groupRepresentationStrSetInDest);\n+      }\n+      ++mappedCount;\n+    }\n+    if (0 == mappedCount) {\n+      return null;\n+    }\n+    Path remainingPath = getRemainingPathStr(srcPath, resolvedPathStr);\n+    for (RegexMountPointInterceptor interceptor : interceptorList) {\n+      parsedDestPath = interceptor.interceptResolvedDestPathStr(parsedDestPath);\n+      remainingPath =\n+          interceptor.interceptRemainingPath(remainingPath);\n+    }\n+    InodeTree.ResolveResult resolveResult = inodeTree\n+        .buildResolveResultForRegexMountPoint(InodeTree.ResultKind.EXTERNAL_DIR,\n+            resolvedPathStr, parsedDestPath, remainingPath);\n+    return resolveResult;\n+  }\n+\n+  private Path getRemainingPathStr(\n+      String srcPath,\n+      String resolvedPathStr) {\n+    String remainingPathStr = srcPath.substring(resolvedPathStr.length());\n+    if (!remainingPathStr.startsWith(\"/\")) {\n+      remainingPathStr = \"/\" + remainingPathStr;\n+    }\n+    return new Path(remainingPathStr);\n+  }\n+\n+  private String getPathToResolve(\n+      String srcPath, boolean resolveLastComponent) {\n+    if (resolveLastComponent) {\n+      return srcPath;\n+    }\n+    int lastSlashIndex = srcPath.lastIndexOf(SlashPath.toString());\n+    if (lastSlashIndex == -1) {\n+      return null;\n+    }\n+    return srcPath.substring(0, lastSlashIndex);\n+  }\n+\n+  /**\n+   * Use capture group named regexGroupNameOrIndexStr in mather to replace\n+   * parsedDestPath.\n+   * E.g. link: ^/user/(?<username>\\\\w+) => s3://$user.apache.com/_${user}\n+   * srcMatcher is from /user/hadoop.\n+   * Then the params will be like following.\n+   * parsedDestPath: s3://$user.apache.com/_${user},\n+   * regexGroupNameOrIndexStr: user\n+   * groupRepresentationStrSetInDest: {user:$user; user:${user}}\n+   * return value will be s3://hadoop.apache.com/_hadoop\n+   * @param parsedDestPath\n+   * @param srcMatcher\n+   * @param regexGroupNameOrIndexStr\n+   * @param groupRepresentationStrSetInDest\n+   * @return return parsedDestPath while ${var},$var replaced or\n+   * parsedDestPath nothing found.\n+   */\n+  private String replaceRegexCaptureGroupInPath(\n+      String parsedDestPath,\n+      Matcher srcMatcher,\n+      String regexGroupNameOrIndexStr,\n+      Set<String> groupRepresentationStrSetInDest) {\n+    String groupValue = getRegexGroupValueFromMather(\n+        srcMatcher, regexGroupNameOrIndexStr);\n+    if (groupValue == null) {\n+      return parsedDestPath;\n+    }\n+    for (String varName : groupRepresentationStrSetInDest) {\n+      parsedDestPath = parsedDestPath.replace(varName, groupValue);\n+      LOGGER.debug(\"parsedDestPath value is:\" + parsedDestPath);\n+    }\n+    return parsedDestPath;\n+  }\n+\n+  /**\n+   * Get matched capture group value from regex matched string. E.g.\n+   * Regex: ^/user/(?<username>\\\\w+), regexGroupNameOrIndexStr: userName\n+   * then /user/hadoop should return hadoop while call\n+   * getRegexGroupValueFromMather(matcher, usersName)\n+   * or getRegexGroupValueFromMather(matcher, 1)\n+   *\n+   * @param srcMatcher - the matcher to be use\n+   * @param regexGroupNameOrIndexStr - the regex group name or index\n+   * @return - Null if no matched group named regexGroupNameOrIndexStr found.\n+   */\n+  private String getRegexGroupValueFromMather(\n+      Matcher srcMatcher, String regexGroupNameOrIndexStr) {\n+    if (regexGroupNameOrIndexStr.matches(\"\\\\d+\")) {\n+      // group index\n+      int groupIndex = Integer.parseUnsignedInt(regexGroupNameOrIndexStr);\n+      if (groupIndex >= 0 && groupIndex <= srcMatcher.groupCount()) {\n+        return srcMatcher.group(groupIndex);\n+      }\n+    } else {\n+      // named group in regex\n+      return srcMatcher.group(regexGroupNameOrIndexStr);\n+    }\n+    return null;\n+  }\n+\n+  /**", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d9b3db8e9df4d5fe43f20880e54c22f1916fdb4"}, "originalPosition": 289}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM1MDIyOA==", "bodyText": "Good catch.", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r481350228", "createdAt": "2020-09-01T18:35:23Z", "author": {"login": "JohnZZGithub"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/RegexMountPoint.java", "diffHunk": "@@ -0,0 +1,306 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.regex.PatternSyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.util.StringUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.fs.viewfs.InodeTree.SlashPath;\n+\n+/**\n+ * Regex mount point is build to implement regex based mount point.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Unstable\n+class RegexMountPoint<T> {\n+  private static final Logger LOGGER =\n+      LoggerFactory.getLogger(RegexMountPoint.class.getName());\n+\n+  private InodeTree inodeTree;\n+  private String srcPathRegex;\n+  private Pattern srcPattern;\n+  private String dstPath;\n+  private String interceptorSettingsString;\n+  private List<RegexMountPointInterceptor> interceptorList;\n+\n+  public static final String SETTING_SRCREGEX_SEP = \"#.\";\n+  public static final char INTERCEPTOR_SEP = ';';\n+  public static final char INTERCEPTOR_INTERNAL_SEP = ':';\n+  // ${var},$var\n+  public static final Pattern VAR_PATTERN_IN_DEST =\n+      Pattern.compile(\"\\\\$((\\\\{\\\\w+\\\\})|(\\\\w+))\");\n+\n+  // Same var might have different representations.\n+  // e.g.\n+  // key => $key or key = > ${key}\n+  private Map<String, Set<String>> varInDestPathMap;\n+\n+  public Map<String, Set<String>> getVarInDestPathMap() {\n+    return varInDestPathMap;\n+  }\n+\n+  RegexMountPoint(InodeTree inodeTree, String sourcePathRegex,\n+      String destPath, String settingsStr) {\n+    this.inodeTree = inodeTree;\n+    this.srcPathRegex = sourcePathRegex;\n+    this.dstPath = destPath;\n+    this.interceptorSettingsString = settingsStr;\n+    this.interceptorList = new ArrayList<>();\n+  }\n+\n+  /**\n+   * Initialize regex mount point.\n+   *\n+   * @throws IOException\n+   */\n+  public void initialize() throws IOException {\n+    try {\n+      srcPattern = Pattern.compile(srcPathRegex);\n+    } catch (PatternSyntaxException ex) {\n+      throw new IOException(\n+          \"Failed to initialized mount point due to bad src path regex:\"\n+              + srcPathRegex + \", dstPath:\" + dstPath, ex);\n+    }\n+    varInDestPathMap = getVarListInString(dstPath);\n+    initializeInterceptors();\n+  }\n+\n+  private void initializeInterceptors() throws IOException {\n+    if (interceptorSettingsString == null\n+        || interceptorSettingsString.isEmpty()) {\n+      return;\n+    }\n+    String[] interceptorStrArray =\n+        StringUtils.split(interceptorSettingsString, INTERCEPTOR_SEP);\n+    for (String interceptorStr : interceptorStrArray) {\n+      RegexMountPointInterceptor interceptor =\n+          RegexMountPointInterceptorFactory.create(interceptorStr);\n+      if (interceptor == null) {\n+        throw new IOException(\n+            \"Illegal settings String \" + interceptorSettingsString);\n+      }\n+      interceptor.initialize();\n+      interceptorList.add(interceptor);\n+    }\n+  }\n+\n+  /**\n+   * Get $var1 and $var2 style variables in string.\n+   *\n+   * @param input - the string to be process.\n+   * @return\n+   */\n+  public static Map<String, Set<String>> getVarListInString(String input) {\n+    Map<String, Set<String>> varMap = new HashMap<>();\n+    Matcher matcher = VAR_PATTERN_IN_DEST.matcher(input);\n+    while (matcher.find()) {\n+      // $var or ${var}\n+      String varName = matcher.group(0);\n+      // var or {var}\n+      String strippedVarName = matcher.group(1);\n+      if (strippedVarName.startsWith(\"{\")) {\n+        // {varName} = > varName\n+        strippedVarName =\n+            strippedVarName.substring(1, strippedVarName.length() - 1);\n+      }\n+      varMap.putIfAbsent(strippedVarName, new HashSet<>());\n+      varMap.get(strippedVarName).add(varName);\n+    }\n+    return varMap;\n+  }\n+\n+  public String getSrcPathRegex() {\n+    return srcPathRegex;\n+  }\n+\n+  public Pattern getSrcPattern() {\n+    return srcPattern;\n+  }\n+\n+  public String getDstPath() {\n+    return dstPath;\n+  }\n+\n+  public static Pattern getVarPatternInDest() {\n+    return VAR_PATTERN_IN_DEST;\n+  }\n+\n+  /**\n+   * Get resolved path from regex mount points.\n+   *  E.g. link: ^/user/(?<username>\\\\w+) => s3://$user.apache.com/_${user}\n+   *  srcPath: is /user/hadoop/dir1\n+   *  resolveLastComponent: true\n+   *  then return value is s3://hadoop.apache.com/_hadoop\n+   * @param srcPath - the src path to resolve\n+   * @param resolveLastComponent - whether resolve the path after last `/`\n+   * @return mapped path of the mount point.\n+   */\n+  public InodeTree.ResolveResult<T> resolve(final String srcPath,\n+      final boolean resolveLastComponent) {\n+    String pathStrToResolve = getPathToResolve(srcPath, resolveLastComponent);\n+    for (RegexMountPointInterceptor interceptor : interceptorList) {\n+      pathStrToResolve = interceptor.interceptSource(pathStrToResolve);\n+    }\n+    LOGGER.debug(\"Path to resolve:\" + pathStrToResolve + \", srcPattern:\"\n+        + getSrcPathRegex());\n+    Matcher srcMatcher = getSrcPattern().matcher(pathStrToResolve);\n+    String parsedDestPath = getDstPath();\n+    int mappedCount = 0;\n+    String resolvedPathStr = \"\";\n+    while (srcMatcher.find()) {\n+      resolvedPathStr = pathStrToResolve.substring(0, srcMatcher.end());\n+      Map<String, Set<String>> varMap = getVarInDestPathMap();\n+      for (Map.Entry<String, Set<String>> entry : varMap.entrySet()) {\n+        String regexGroupNameOrIndexStr = entry.getKey();\n+        Set<String> groupRepresentationStrSetInDest = entry.getValue();\n+        parsedDestPath = replaceRegexCaptureGroupInPath(\n+            parsedDestPath, srcMatcher,\n+            regexGroupNameOrIndexStr, groupRepresentationStrSetInDest);\n+      }\n+      ++mappedCount;\n+    }\n+    if (0 == mappedCount) {\n+      return null;\n+    }\n+    Path remainingPath = getRemainingPathStr(srcPath, resolvedPathStr);\n+    for (RegexMountPointInterceptor interceptor : interceptorList) {\n+      parsedDestPath = interceptor.interceptResolvedDestPathStr(parsedDestPath);\n+      remainingPath =\n+          interceptor.interceptRemainingPath(remainingPath);\n+    }\n+    InodeTree.ResolveResult resolveResult = inodeTree\n+        .buildResolveResultForRegexMountPoint(InodeTree.ResultKind.EXTERNAL_DIR,\n+            resolvedPathStr, parsedDestPath, remainingPath);\n+    return resolveResult;\n+  }\n+\n+  private Path getRemainingPathStr(\n+      String srcPath,\n+      String resolvedPathStr) {\n+    String remainingPathStr = srcPath.substring(resolvedPathStr.length());\n+    if (!remainingPathStr.startsWith(\"/\")) {\n+      remainingPathStr = \"/\" + remainingPathStr;\n+    }\n+    return new Path(remainingPathStr);\n+  }\n+\n+  private String getPathToResolve(\n+      String srcPath, boolean resolveLastComponent) {\n+    if (resolveLastComponent) {\n+      return srcPath;\n+    }\n+    int lastSlashIndex = srcPath.lastIndexOf(SlashPath.toString());\n+    if (lastSlashIndex == -1) {\n+      return null;\n+    }\n+    return srcPath.substring(0, lastSlashIndex);\n+  }\n+\n+  /**\n+   * Use capture group named regexGroupNameOrIndexStr in mather to replace\n+   * parsedDestPath.\n+   * E.g. link: ^/user/(?<username>\\\\w+) => s3://$user.apache.com/_${user}\n+   * srcMatcher is from /user/hadoop.\n+   * Then the params will be like following.\n+   * parsedDestPath: s3://$user.apache.com/_${user},\n+   * regexGroupNameOrIndexStr: user\n+   * groupRepresentationStrSetInDest: {user:$user; user:${user}}\n+   * return value will be s3://hadoop.apache.com/_hadoop\n+   * @param parsedDestPath\n+   * @param srcMatcher\n+   * @param regexGroupNameOrIndexStr\n+   * @param groupRepresentationStrSetInDest\n+   * @return return parsedDestPath while ${var},$var replaced or\n+   * parsedDestPath nothing found.\n+   */\n+  private String replaceRegexCaptureGroupInPath(\n+      String parsedDestPath,\n+      Matcher srcMatcher,\n+      String regexGroupNameOrIndexStr,\n+      Set<String> groupRepresentationStrSetInDest) {\n+    String groupValue = getRegexGroupValueFromMather(\n+        srcMatcher, regexGroupNameOrIndexStr);\n+    if (groupValue == null) {\n+      return parsedDestPath;\n+    }\n+    for (String varName : groupRepresentationStrSetInDest) {\n+      parsedDestPath = parsedDestPath.replace(varName, groupValue);\n+      LOGGER.debug(\"parsedDestPath value is:\" + parsedDestPath);\n+    }\n+    return parsedDestPath;\n+  }\n+\n+  /**\n+   * Get matched capture group value from regex matched string. E.g.\n+   * Regex: ^/user/(?<username>\\\\w+), regexGroupNameOrIndexStr: userName\n+   * then /user/hadoop should return hadoop while call\n+   * getRegexGroupValueFromMather(matcher, usersName)\n+   * or getRegexGroupValueFromMather(matcher, 1)\n+   *\n+   * @param srcMatcher - the matcher to be use\n+   * @param regexGroupNameOrIndexStr - the regex group name or index\n+   * @return - Null if no matched group named regexGroupNameOrIndexStr found.\n+   */\n+  private String getRegexGroupValueFromMather(\n+      Matcher srcMatcher, String regexGroupNameOrIndexStr) {\n+    if (regexGroupNameOrIndexStr.matches(\"\\\\d+\")) {\n+      // group index\n+      int groupIndex = Integer.parseUnsignedInt(regexGroupNameOrIndexStr);\n+      if (groupIndex >= 0 && groupIndex <= srcMatcher.groupCount()) {\n+        return srcMatcher.group(groupIndex);\n+      }\n+    } else {\n+      // named group in regex\n+      return srcMatcher.group(regexGroupNameOrIndexStr);\n+    }\n+    return null;\n+  }\n+\n+  /**", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDc5NTM0Mg=="}, "originalCommit": {"oid": "3d9b3db8e9df4d5fe43f20880e54c22f1916fdb4"}, "originalPosition": 289}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAwNjQ5ODc2OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/Constants.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwNTozNjozM1rOHKj9Sw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwNToyODo1OVrOHO3Oig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDgzNjkzOQ==", "bodyText": "below comment can be corrected? regex base mount point will not use caching now.\nOtherwise people could confuse and may tend to disable. IIUC, even if they enable this, RegexBasedMountPoints will continue to work right?", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r480836939", "createdAt": "2020-09-01T05:36:33Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/Constants.java", "diffHunk": "@@ -86,12 +86,21 @@\n    */\n   String CONFIG_VIEWFS_LINK_MERGE_SLASH = \"linkMergeSlash\";\n \n+  /**\n+   * Config variable for specifying a regex link which uses regular expressions\n+   * as source and target could use group captured in src.\n+   * E.g. (^/(?<firstDir>\\\\w+), /prefix-${firstDir}) =>\n+   *   (/path1/file1 => /prefix-path1/file1)\n+   */\n+  String CONFIG_VIEWFS_LINK_REGEX = \"linkRegex\";\n+\n   FsPermission PERMISSION_555 = new FsPermission((short) 0555);\n \n   String CONFIG_VIEWFS_RENAME_STRATEGY = \"fs.viewfs.rename.strategy\";\n \n   /**\n    * Enable ViewFileSystem to cache all children filesystems in inner cache.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d9b3db8e9df4d5fe43f20880e54c22f1916fdb4"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTMyODkxMg==", "bodyText": "Good cache, I should clarify it. ViewFileSystem will work if people enable inner cache. However, regex based mounts won't be cache.", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r481328912", "createdAt": "2020-09-01T17:56:25Z", "author": {"login": "JohnZZGithub"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/Constants.java", "diffHunk": "@@ -86,12 +86,21 @@\n    */\n   String CONFIG_VIEWFS_LINK_MERGE_SLASH = \"linkMergeSlash\";\n \n+  /**\n+   * Config variable for specifying a regex link which uses regular expressions\n+   * as source and target could use group captured in src.\n+   * E.g. (^/(?<firstDir>\\\\w+), /prefix-${firstDir}) =>\n+   *   (/path1/file1 => /prefix-path1/file1)\n+   */\n+  String CONFIG_VIEWFS_LINK_REGEX = \"linkRegex\";\n+\n   FsPermission PERMISSION_555 = new FsPermission((short) 0555);\n \n   String CONFIG_VIEWFS_RENAME_STRATEGY = \"fs.viewfs.rename.strategy\";\n \n   /**\n    * Enable ViewFileSystem to cache all children filesystems in inner cache.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDgzNjkzOQ=="}, "originalCommit": {"oid": "3d9b3db8e9df4d5fe43f20880e54c22f1916fdb4"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTMzNTU4NA==", "bodyText": "Is the below comment is still valid?", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r485335584", "createdAt": "2020-09-09T04:48:49Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/Constants.java", "diffHunk": "@@ -86,12 +86,21 @@\n    */\n   String CONFIG_VIEWFS_LINK_MERGE_SLASH = \"linkMergeSlash\";\n \n+  /**\n+   * Config variable for specifying a regex link which uses regular expressions\n+   * as source and target could use group captured in src.\n+   * E.g. (^/(?<firstDir>\\\\w+), /prefix-${firstDir}) =>\n+   *   (/path1/file1 => /prefix-path1/file1)\n+   */\n+  String CONFIG_VIEWFS_LINK_REGEX = \"linkRegex\";\n+\n   FsPermission PERMISSION_555 = new FsPermission((short) 0555);\n \n   String CONFIG_VIEWFS_RENAME_STRATEGY = \"fs.viewfs.rename.strategy\";\n \n   /**\n    * Enable ViewFileSystem to cache all children filesystems in inner cache.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDgzNjkzOQ=="}, "originalCommit": {"oid": "3d9b3db8e9df4d5fe43f20880e54c22f1916fdb4"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTM0Njk1NA==", "bodyText": "Nice catch.", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r485346954", "createdAt": "2020-09-09T05:28:59Z", "author": {"login": "JohnZZGithub"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/Constants.java", "diffHunk": "@@ -86,12 +86,21 @@\n    */\n   String CONFIG_VIEWFS_LINK_MERGE_SLASH = \"linkMergeSlash\";\n \n+  /**\n+   * Config variable for specifying a regex link which uses regular expressions\n+   * as source and target could use group captured in src.\n+   * E.g. (^/(?<firstDir>\\\\w+), /prefix-${firstDir}) =>\n+   *   (/path1/file1 => /prefix-path1/file1)\n+   */\n+  String CONFIG_VIEWFS_LINK_REGEX = \"linkRegex\";\n+\n   FsPermission PERMISSION_555 = new FsPermission((short) 0555);\n \n   String CONFIG_VIEWFS_RENAME_STRATEGY = \"fs.viewfs.rename.strategy\";\n \n   /**\n    * Enable ViewFileSystem to cache all children filesystems in inner cache.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDgzNjkzOQ=="}, "originalCommit": {"oid": "3d9b3db8e9df4d5fe43f20880e54c22f1916fdb4"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAwNjU2NTYwOnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/RegexMountPointInterceptorFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwNTo1MzoxM1rOHKknZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwNTo1MzoxM1rOHKknZg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDg0NzcxOA==", "bodyText": "you may want to remove the below return ? or specify what it's returning.", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r480847718", "createdAt": "2020-09-01T05:53:13Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/RegexMountPointInterceptorFactory.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+\n+/**\n+ * The interceptor factory used to create RegexMountPoint interceptors.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Unstable\n+final class RegexMountPointInterceptorFactory {\n+\n+  private RegexMountPointInterceptorFactory() {\n+\n+  }\n+\n+  /**\n+   * interceptorSettingsString string should be like ${type}:${string},\n+   * e.g. replaceresolveddstpath:word1,word2.\n+   *\n+   * @param interceptorSettingsString", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d9b3db8e9df4d5fe43f20880e54c22f1916fdb4"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAwNjU4NjMwOnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestRegexMountPointResolvedDstPathReplaceInterceptor.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwNTo1ODoxMlrOHKk0TQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNlQxNjo1ODozNFrOHNqpDA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDg1MTAyMQ==", "bodyText": "What are you asserting in this case?", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r480851021", "createdAt": "2020-09-01T05:58:12Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestRegexMountPointResolvedDstPathReplaceInterceptor.java", "diffHunk": "@@ -0,0 +1,104 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import static org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH;\n+\n+/**\n+ * Test RegexMountPointResolvedDstPathReplaceInterceptor.\n+ */\n+public class TestRegexMountPointResolvedDstPathReplaceInterceptor {\n+\n+  public String createSerializedString(String regex, String replaceString) {\n+    return REPLACE_RESOLVED_DST_PATH.getConfigName()\n+        + RegexMountPoint.INTERCEPTOR_INTERNAL_SEP + regex\n+        + RegexMountPoint.INTERCEPTOR_INTERNAL_SEP + replaceString;\n+  }\n+\n+  @Test\n+  public void testDeserializeFromStringNormalCase() throws IOException {\n+    String srcRegex = \"-\";\n+    String replaceString = \"_\";\n+    String serializedString = createSerializedString(srcRegex, replaceString);\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        RegexMountPointResolvedDstPathReplaceInterceptor\n+            .deserializeFromString(serializedString);\n+    Assert.assertTrue(interceptor.getSrcRegexString().equals(srcRegex));\n+    Assert.assertTrue(interceptor.getReplaceString().equals(replaceString));\n+    Assert.assertTrue(interceptor.getSrcRegexPattern() == null);\n+    interceptor.initialize();\n+    Assert.assertTrue(\n+        interceptor.getSrcRegexPattern().toString().equals(srcRegex));\n+  }\n+\n+  @Test\n+  public void testDeserializeFromStringBadCase() throws IOException {\n+    String srcRegex = \"-\";\n+    String replaceString = \"_\";\n+    String serializedString = createSerializedString(srcRegex, replaceString);\n+    serializedString = serializedString + \":ddd\";\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        RegexMountPointResolvedDstPathReplaceInterceptor\n+            .deserializeFromString(serializedString);\n+    Assert.assertEquals(interceptor, null);\n+  }\n+\n+  @Test\n+  public void testSerialization() {\n+    String srcRegex = \"word1\";\n+    String replaceString = \"word2\";\n+    String serializedString = createSerializedString(srcRegex, replaceString);\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        new RegexMountPointResolvedDstPathReplaceInterceptor(srcRegex,\n+            replaceString);\n+    Assert.assertEquals(interceptor.serializeToString(), serializedString);\n+  }\n+\n+  @Test", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d9b3db8e9df4d5fe43f20880e54c22f1916fdb4"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTUyMTExMQ==", "bodyText": "I meant to have a safegurad when users want to change the interceptor settings format. But I'm OK to remove it.", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r481521111", "createdAt": "2020-09-02T01:14:02Z", "author": {"login": "JohnZZGithub"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestRegexMountPointResolvedDstPathReplaceInterceptor.java", "diffHunk": "@@ -0,0 +1,104 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import static org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH;\n+\n+/**\n+ * Test RegexMountPointResolvedDstPathReplaceInterceptor.\n+ */\n+public class TestRegexMountPointResolvedDstPathReplaceInterceptor {\n+\n+  public String createSerializedString(String regex, String replaceString) {\n+    return REPLACE_RESOLVED_DST_PATH.getConfigName()\n+        + RegexMountPoint.INTERCEPTOR_INTERNAL_SEP + regex\n+        + RegexMountPoint.INTERCEPTOR_INTERNAL_SEP + replaceString;\n+  }\n+\n+  @Test\n+  public void testDeserializeFromStringNormalCase() throws IOException {\n+    String srcRegex = \"-\";\n+    String replaceString = \"_\";\n+    String serializedString = createSerializedString(srcRegex, replaceString);\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        RegexMountPointResolvedDstPathReplaceInterceptor\n+            .deserializeFromString(serializedString);\n+    Assert.assertTrue(interceptor.getSrcRegexString().equals(srcRegex));\n+    Assert.assertTrue(interceptor.getReplaceString().equals(replaceString));\n+    Assert.assertTrue(interceptor.getSrcRegexPattern() == null);\n+    interceptor.initialize();\n+    Assert.assertTrue(\n+        interceptor.getSrcRegexPattern().toString().equals(srcRegex));\n+  }\n+\n+  @Test\n+  public void testDeserializeFromStringBadCase() throws IOException {\n+    String srcRegex = \"-\";\n+    String replaceString = \"_\";\n+    String serializedString = createSerializedString(srcRegex, replaceString);\n+    serializedString = serializedString + \":ddd\";\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        RegexMountPointResolvedDstPathReplaceInterceptor\n+            .deserializeFromString(serializedString);\n+    Assert.assertEquals(interceptor, null);\n+  }\n+\n+  @Test\n+  public void testSerialization() {\n+    String srcRegex = \"word1\";\n+    String replaceString = \"word2\";\n+    String serializedString = createSerializedString(srcRegex, replaceString);\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        new RegexMountPointResolvedDstPathReplaceInterceptor(srcRegex,\n+            replaceString);\n+    Assert.assertEquals(interceptor.serializeToString(), serializedString);\n+  }\n+\n+  @Test", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDg1MTAyMQ=="}, "originalCommit": {"oid": "3d9b3db8e9df4d5fe43f20880e54c22f1916fdb4"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTUzOTA0MQ==", "bodyText": "I mean we just need to assert I think. Ex: Test saying \"InterceptSource\". SO, just validate what it should return. In this case, it will return same string. It may not have much value now, but if some one changes it, it can just catch it.\nCurrently this test will never fail.", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r481539041", "createdAt": "2020-09-02T01:46:29Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestRegexMountPointResolvedDstPathReplaceInterceptor.java", "diffHunk": "@@ -0,0 +1,104 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import static org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH;\n+\n+/**\n+ * Test RegexMountPointResolvedDstPathReplaceInterceptor.\n+ */\n+public class TestRegexMountPointResolvedDstPathReplaceInterceptor {\n+\n+  public String createSerializedString(String regex, String replaceString) {\n+    return REPLACE_RESOLVED_DST_PATH.getConfigName()\n+        + RegexMountPoint.INTERCEPTOR_INTERNAL_SEP + regex\n+        + RegexMountPoint.INTERCEPTOR_INTERNAL_SEP + replaceString;\n+  }\n+\n+  @Test\n+  public void testDeserializeFromStringNormalCase() throws IOException {\n+    String srcRegex = \"-\";\n+    String replaceString = \"_\";\n+    String serializedString = createSerializedString(srcRegex, replaceString);\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        RegexMountPointResolvedDstPathReplaceInterceptor\n+            .deserializeFromString(serializedString);\n+    Assert.assertTrue(interceptor.getSrcRegexString().equals(srcRegex));\n+    Assert.assertTrue(interceptor.getReplaceString().equals(replaceString));\n+    Assert.assertTrue(interceptor.getSrcRegexPattern() == null);\n+    interceptor.initialize();\n+    Assert.assertTrue(\n+        interceptor.getSrcRegexPattern().toString().equals(srcRegex));\n+  }\n+\n+  @Test\n+  public void testDeserializeFromStringBadCase() throws IOException {\n+    String srcRegex = \"-\";\n+    String replaceString = \"_\";\n+    String serializedString = createSerializedString(srcRegex, replaceString);\n+    serializedString = serializedString + \":ddd\";\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        RegexMountPointResolvedDstPathReplaceInterceptor\n+            .deserializeFromString(serializedString);\n+    Assert.assertEquals(interceptor, null);\n+  }\n+\n+  @Test\n+  public void testSerialization() {\n+    String srcRegex = \"word1\";\n+    String replaceString = \"word2\";\n+    String serializedString = createSerializedString(srcRegex, replaceString);\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        new RegexMountPointResolvedDstPathReplaceInterceptor(srcRegex,\n+            replaceString);\n+    Assert.assertEquals(interceptor.serializeToString(), serializedString);\n+  }\n+\n+  @Test", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDg1MTAyMQ=="}, "originalCommit": {"oid": "3d9b3db8e9df4d5fe43f20880e54c22f1916fdb4"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDAxMzQxNA==", "bodyText": "Ah, gotcha. I thought the comment was for testSerialization().", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r484013414", "createdAt": "2020-09-06T02:53:17Z", "author": {"login": "JohnZZGithub"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestRegexMountPointResolvedDstPathReplaceInterceptor.java", "diffHunk": "@@ -0,0 +1,104 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import static org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH;\n+\n+/**\n+ * Test RegexMountPointResolvedDstPathReplaceInterceptor.\n+ */\n+public class TestRegexMountPointResolvedDstPathReplaceInterceptor {\n+\n+  public String createSerializedString(String regex, String replaceString) {\n+    return REPLACE_RESOLVED_DST_PATH.getConfigName()\n+        + RegexMountPoint.INTERCEPTOR_INTERNAL_SEP + regex\n+        + RegexMountPoint.INTERCEPTOR_INTERNAL_SEP + replaceString;\n+  }\n+\n+  @Test\n+  public void testDeserializeFromStringNormalCase() throws IOException {\n+    String srcRegex = \"-\";\n+    String replaceString = \"_\";\n+    String serializedString = createSerializedString(srcRegex, replaceString);\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        RegexMountPointResolvedDstPathReplaceInterceptor\n+            .deserializeFromString(serializedString);\n+    Assert.assertTrue(interceptor.getSrcRegexString().equals(srcRegex));\n+    Assert.assertTrue(interceptor.getReplaceString().equals(replaceString));\n+    Assert.assertTrue(interceptor.getSrcRegexPattern() == null);\n+    interceptor.initialize();\n+    Assert.assertTrue(\n+        interceptor.getSrcRegexPattern().toString().equals(srcRegex));\n+  }\n+\n+  @Test\n+  public void testDeserializeFromStringBadCase() throws IOException {\n+    String srcRegex = \"-\";\n+    String replaceString = \"_\";\n+    String serializedString = createSerializedString(srcRegex, replaceString);\n+    serializedString = serializedString + \":ddd\";\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        RegexMountPointResolvedDstPathReplaceInterceptor\n+            .deserializeFromString(serializedString);\n+    Assert.assertEquals(interceptor, null);\n+  }\n+\n+  @Test\n+  public void testSerialization() {\n+    String srcRegex = \"word1\";\n+    String replaceString = \"word2\";\n+    String serializedString = createSerializedString(srcRegex, replaceString);\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        new RegexMountPointResolvedDstPathReplaceInterceptor(srcRegex,\n+            replaceString);\n+    Assert.assertEquals(interceptor.serializeToString(), serializedString);\n+  }\n+\n+  @Test", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDg1MTAyMQ=="}, "originalCommit": {"oid": "3d9b3db8e9df4d5fe43f20880e54c22f1916fdb4"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5MjE3Mg==", "bodyText": "No worries. Thanks for addressing. :-)", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r484092172", "createdAt": "2020-09-06T16:58:34Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestRegexMountPointResolvedDstPathReplaceInterceptor.java", "diffHunk": "@@ -0,0 +1,104 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import static org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH;\n+\n+/**\n+ * Test RegexMountPointResolvedDstPathReplaceInterceptor.\n+ */\n+public class TestRegexMountPointResolvedDstPathReplaceInterceptor {\n+\n+  public String createSerializedString(String regex, String replaceString) {\n+    return REPLACE_RESOLVED_DST_PATH.getConfigName()\n+        + RegexMountPoint.INTERCEPTOR_INTERNAL_SEP + regex\n+        + RegexMountPoint.INTERCEPTOR_INTERNAL_SEP + replaceString;\n+  }\n+\n+  @Test\n+  public void testDeserializeFromStringNormalCase() throws IOException {\n+    String srcRegex = \"-\";\n+    String replaceString = \"_\";\n+    String serializedString = createSerializedString(srcRegex, replaceString);\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        RegexMountPointResolvedDstPathReplaceInterceptor\n+            .deserializeFromString(serializedString);\n+    Assert.assertTrue(interceptor.getSrcRegexString().equals(srcRegex));\n+    Assert.assertTrue(interceptor.getReplaceString().equals(replaceString));\n+    Assert.assertTrue(interceptor.getSrcRegexPattern() == null);\n+    interceptor.initialize();\n+    Assert.assertTrue(\n+        interceptor.getSrcRegexPattern().toString().equals(srcRegex));\n+  }\n+\n+  @Test\n+  public void testDeserializeFromStringBadCase() throws IOException {\n+    String srcRegex = \"-\";\n+    String replaceString = \"_\";\n+    String serializedString = createSerializedString(srcRegex, replaceString);\n+    serializedString = serializedString + \":ddd\";\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        RegexMountPointResolvedDstPathReplaceInterceptor\n+            .deserializeFromString(serializedString);\n+    Assert.assertEquals(interceptor, null);\n+  }\n+\n+  @Test\n+  public void testSerialization() {\n+    String srcRegex = \"word1\";\n+    String replaceString = \"word2\";\n+    String serializedString = createSerializedString(srcRegex, replaceString);\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        new RegexMountPointResolvedDstPathReplaceInterceptor(srcRegex,\n+            replaceString);\n+    Assert.assertEquals(interceptor.serializeToString(), serializedString);\n+  }\n+\n+  @Test", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDg1MTAyMQ=="}, "originalCommit": {"oid": "3d9b3db8e9df4d5fe43f20880e54c22f1916fdb4"}, "originalPosition": 78}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAwNjU5NDM5OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestRegexMountPointResolvedDstPathReplaceInterceptor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwNjowMDoxMFrOHKk5jg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwNjowMDoxMFrOHKk5jg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDg1MjM2Ng==", "bodyText": "You want to use assertEquals instead?", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r480852366", "createdAt": "2020-09-01T06:00:10Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestRegexMountPointResolvedDstPathReplaceInterceptor.java", "diffHunk": "@@ -0,0 +1,104 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import static org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH;\n+\n+/**\n+ * Test RegexMountPointResolvedDstPathReplaceInterceptor.\n+ */\n+public class TestRegexMountPointResolvedDstPathReplaceInterceptor {\n+\n+  public String createSerializedString(String regex, String replaceString) {\n+    return REPLACE_RESOLVED_DST_PATH.getConfigName()\n+        + RegexMountPoint.INTERCEPTOR_INTERNAL_SEP + regex\n+        + RegexMountPoint.INTERCEPTOR_INTERNAL_SEP + replaceString;\n+  }\n+\n+  @Test\n+  public void testDeserializeFromStringNormalCase() throws IOException {\n+    String srcRegex = \"-\";\n+    String replaceString = \"_\";\n+    String serializedString = createSerializedString(srcRegex, replaceString);\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        RegexMountPointResolvedDstPathReplaceInterceptor\n+            .deserializeFromString(serializedString);\n+    Assert.assertTrue(interceptor.getSrcRegexString().equals(srcRegex));\n+    Assert.assertTrue(interceptor.getReplaceString().equals(replaceString));\n+    Assert.assertTrue(interceptor.getSrcRegexPattern() == null);\n+    interceptor.initialize();\n+    Assert.assertTrue(\n+        interceptor.getSrcRegexPattern().toString().equals(srcRegex));\n+  }\n+\n+  @Test\n+  public void testDeserializeFromStringBadCase() throws IOException {\n+    String srcRegex = \"-\";\n+    String replaceString = \"_\";\n+    String serializedString = createSerializedString(srcRegex, replaceString);\n+    serializedString = serializedString + \":ddd\";\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        RegexMountPointResolvedDstPathReplaceInterceptor\n+            .deserializeFromString(serializedString);\n+    Assert.assertEquals(interceptor, null);\n+  }\n+\n+  @Test\n+  public void testSerialization() {\n+    String srcRegex = \"word1\";\n+    String replaceString = \"word2\";\n+    String serializedString = createSerializedString(srcRegex, replaceString);\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        new RegexMountPointResolvedDstPathReplaceInterceptor(srcRegex,\n+            replaceString);\n+    Assert.assertEquals(interceptor.serializeToString(), serializedString);\n+  }\n+\n+  @Test\n+  public void testInterceptSource() {\n+    String srcRegex = \"word1\";\n+    String replaceString = \"word2\";\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        new RegexMountPointResolvedDstPathReplaceInterceptor(srcRegex,\n+            replaceString);\n+    String sourcePath = \"/a/b/l3/dd\";\n+    sourcePath = interceptor.interceptSource(sourcePath);\n+  }\n+\n+  @Test\n+  public void testInterceptResolve() throws IOException {\n+    String pathAfterResolution = \"/user-hadoop\";\n+    Path remainingPath = new Path(\"/ad-data\");\n+\n+    String srcRegex = \"hadoop\";\n+    String replaceString = \"hdfs\";\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        new RegexMountPointResolvedDstPathReplaceInterceptor(srcRegex,\n+            replaceString);\n+    interceptor.initialize();\n+    Assert.assertTrue(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d9b3db8e9df4d5fe43f20880e54c22f1916fdb4"}, "originalPosition": 100}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAwNjU5NzUxOnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestRegexMountPointResolvedDstPathReplaceInterceptor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwNjowMDo1N1rOHKk7jA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwNjowMDo1N1rOHKk7jA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDg1Mjg3Ng==", "bodyText": "You may want to use assertEquals directly?", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r480852876", "createdAt": "2020-09-01T06:00:57Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestRegexMountPointResolvedDstPathReplaceInterceptor.java", "diffHunk": "@@ -0,0 +1,104 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import static org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH;\n+\n+/**\n+ * Test RegexMountPointResolvedDstPathReplaceInterceptor.\n+ */\n+public class TestRegexMountPointResolvedDstPathReplaceInterceptor {\n+\n+  public String createSerializedString(String regex, String replaceString) {\n+    return REPLACE_RESOLVED_DST_PATH.getConfigName()\n+        + RegexMountPoint.INTERCEPTOR_INTERNAL_SEP + regex\n+        + RegexMountPoint.INTERCEPTOR_INTERNAL_SEP + replaceString;\n+  }\n+\n+  @Test\n+  public void testDeserializeFromStringNormalCase() throws IOException {\n+    String srcRegex = \"-\";\n+    String replaceString = \"_\";\n+    String serializedString = createSerializedString(srcRegex, replaceString);\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        RegexMountPointResolvedDstPathReplaceInterceptor\n+            .deserializeFromString(serializedString);\n+    Assert.assertTrue(interceptor.getSrcRegexString().equals(srcRegex));\n+    Assert.assertTrue(interceptor.getReplaceString().equals(replaceString));\n+    Assert.assertTrue(interceptor.getSrcRegexPattern() == null);\n+    interceptor.initialize();\n+    Assert.assertTrue(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d9b3db8e9df4d5fe43f20880e54c22f1916fdb4"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAwNjYwNjk1OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestRegexMountPoint.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwNjowMzoyM1rOHKlBnQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwNjowMzoyM1rOHKlBnQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDg1NDQyOQ==", "bodyText": "You may want to use assertEquals ? The advantage would be that, when assertion fails it will tell you what's mismatching.  There are lot of asserts like this, please change if possible.", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r480854429", "createdAt": "2020-09-01T06:03:23Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestRegexMountPoint.java", "diffHunk": "@@ -0,0 +1,166 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Test Regex Mount Point.\n+ */\n+public class TestRegexMountPoint {\n+  private static final Logger LOGGER =\n+      LoggerFactory.getLogger(TestRegexMountPoint.class.getName());\n+\n+  private InodeTree inodeTree;\n+  private Configuration conf;\n+\n+  class TestRegexMountPointFileSystem {\n+    public URI getUri() {\n+      return uri;\n+    }\n+\n+    private URI uri;\n+\n+    TestRegexMountPointFileSystem(URI uri) {\n+      String uriStr = uri == null ? \"null\" : uri.toString();\n+      LOGGER.info(\"Create TestRegexMountPointFileSystem Via URI:\" + uriStr);\n+      this.uri = uri;\n+    }\n+  }\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    ConfigUtil.addLink(conf, TestRegexMountPoint.class.getName(), \"/mnt\",\n+        URI.create(\"file:///\"));\n+\n+    inodeTree = new InodeTree<TestRegexMountPointFileSystem>(conf,\n+        TestRegexMountPoint.class.getName(), null, false) {\n+      @Override\n+      protected TestRegexMountPointFileSystem getTargetFileSystem(\n+          final URI uri) {\n+        return new TestRegexMountPointFileSystem(uri);\n+      }\n+\n+      @Override\n+      protected TestRegexMountPointFileSystem getTargetFileSystem(\n+          final URI uri, boolean enableCache) {\n+        return new TestRegexMountPointFileSystem(uri);\n+      }\n+\n+      @Override\n+      protected TestRegexMountPointFileSystem getTargetFileSystem(\n+          final INodeDir<TestRegexMountPointFileSystem> dir) {\n+        return new TestRegexMountPointFileSystem(null);\n+      }\n+\n+      @Override\n+      protected TestRegexMountPointFileSystem getTargetFileSystem(\n+          final String settings, final URI[] mergeFsURIList) {\n+        return new TestRegexMountPointFileSystem(null);\n+      }\n+    };\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    inodeTree = null;\n+  }\n+\n+  @Test\n+  public void testGetVarListInString() throws IOException {\n+    String srcRegex = \"/(\\\\w+)\";\n+    String target = \"/$0/${1}/$1/${2}/${2}\";\n+    RegexMountPoint regexMountPoint =\n+        new RegexMountPoint(inodeTree, srcRegex, target, null);\n+    regexMountPoint.initialize();\n+    Map<String, Set<String>> varMap = regexMountPoint.getVarInDestPathMap();\n+    Assert.assertEquals(varMap.size(), 3);\n+    Assert.assertEquals(varMap.get(\"0\").size(), 1);\n+    Assert.assertTrue(varMap.get(\"0\").contains(\"$0\"));\n+    Assert.assertEquals(varMap.get(\"1\").size(), 2);\n+    Assert.assertTrue(varMap.get(\"1\").contains(\"${1}\"));\n+    Assert.assertTrue(varMap.get(\"1\").contains(\"$1\"));\n+    Assert.assertEquals(varMap.get(\"2\").size(), 1);\n+    Assert.assertTrue(varMap.get(\"2\").contains(\"${2}\"));\n+  }\n+\n+  @Test\n+  public void testResolve() throws IOException {\n+    String regexStr = \"^/user/(?<username>\\\\w+)\";\n+    String dstPathStr = \"/namenode1/testResolve/$username\";\n+    String settingsStr = null;\n+    RegexMountPoint regexMountPoint =\n+        new RegexMountPoint(inodeTree, regexStr, dstPathStr, settingsStr);\n+    regexMountPoint.initialize();\n+    InodeTree.ResolveResult resolveResult =\n+        regexMountPoint.resolve(\"/user/hadoop/file1\", true);\n+    Assert.assertEquals(resolveResult.kind, InodeTree.ResultKind.EXTERNAL_DIR);\n+    Assert.assertTrue(\n+        resolveResult.targetFileSystem\n+            instanceof TestRegexMountPointFileSystem);\n+    Assert.assertTrue(resolveResult.resolvedPath.equals(\"/user/hadoop\"));\n+    Assert.assertTrue(\n+        resolveResult.targetFileSystem\n+            instanceof TestRegexMountPointFileSystem);\n+    Assert.assertTrue(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d9b3db8e9df4d5fe43f20880e54c22f1916fdb4"}, "originalPosition": 132}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAwNjYxNTc5OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestRegexMountPointInterceptorFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwNjowNTozMlrOHKlHIw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwNjowNTozMlrOHKlHIw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDg1NTg0Mw==", "bodyText": "you can use assertNull", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r480855843", "createdAt": "2020-09-01T06:05:32Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestRegexMountPointInterceptorFactory.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+/**\n+ * Test Regex Mount Point Interceptor Factory.\n+ */\n+public class TestRegexMountPointInterceptorFactory {\n+\n+  @Test\n+  public void testCreateNormalCase() {\n+    String replaceInterceptorStr =\n+        RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH.getConfigName()\n+            + Character.toString(RegexMountPoint.INTERCEPTOR_INTERNAL_SEP)\n+            + \"src\" + Character\n+            .toString(RegexMountPoint.INTERCEPTOR_INTERNAL_SEP) + \"replace\";\n+    RegexMountPointInterceptor interceptor =\n+        RegexMountPointInterceptorFactory.create(replaceInterceptorStr);\n+    Assert.assertTrue(\n+        interceptor\n+            instanceof RegexMountPointResolvedDstPathReplaceInterceptor);\n+  }\n+\n+  @Test\n+  public void testCreateBadCase() {\n+    String replaceInterceptorStr =\n+        RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH.getConfigName()\n+            + \"___\" + Character\n+            .toString(RegexMountPoint.INTERCEPTOR_INTERNAL_SEP) + \"src\"\n+            + Character.toString(RegexMountPoint.INTERCEPTOR_INTERNAL_SEP)\n+            + \"replace\";\n+    RegexMountPointInterceptor interceptor =\n+        RegexMountPointInterceptorFactory.create(replaceInterceptorStr);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d9b3db8e9df4d5fe43f20880e54c22f1916fdb4"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAwNjYzNzIzOnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/RegexMountPointResolvedDstPathReplaceInterceptor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwNjoxMDo1NlrOHKlUvg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQwMTozNDozOVrOHLOWPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDg1OTMyNg==", "bodyText": "Seems like we don't do anything here. Could you please add that in Javadoc. Say src will not be intercepted in this impl, it's only for dst?", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r480859326", "createdAt": "2020-09-01T06:10:56Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/RegexMountPointResolvedDstPathReplaceInterceptor.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.regex.PatternSyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH;\n+\n+/**\n+ * Implementation of RegexMountPointResolvedDstPathReplaceInterceptor.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Unstable\n+class RegexMountPointResolvedDstPathReplaceInterceptor\n+    implements RegexMountPointInterceptor {\n+\n+  private String srcRegexString;\n+  private String replaceString;\n+  private Pattern srcRegexPattern;\n+\n+  RegexMountPointResolvedDstPathReplaceInterceptor(String srcRegex,\n+      String replaceString) {\n+    this.srcRegexString = srcRegex;\n+    this.replaceString = replaceString;\n+    this.srcRegexPattern = null;\n+  }\n+\n+  public String getSrcRegexString() {\n+    return srcRegexString;\n+  }\n+\n+  public String getReplaceString() {\n+    return replaceString;\n+  }\n+\n+  public Pattern getSrcRegexPattern() {\n+    return srcRegexPattern;\n+  }\n+\n+  @Override\n+  public void initialize() throws IOException {\n+    try {\n+      srcRegexPattern = Pattern.compile(srcRegexString);\n+    } catch (PatternSyntaxException ex) {\n+      throw new IOException(\n+          \"Initialize interceptor failed, srcRegx:\" + srcRegexString, ex);\n+    }\n+  }\n+\n+  /**\n+   * Intercept source before resolution.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d9b3db8e9df4d5fe43f20880e54c22f1916fdb4"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTUzMTQ1Mw==", "bodyText": "Sure.", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r481531453", "createdAt": "2020-09-02T01:34:39Z", "author": {"login": "JohnZZGithub"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/RegexMountPointResolvedDstPathReplaceInterceptor.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.regex.PatternSyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH;\n+\n+/**\n+ * Implementation of RegexMountPointResolvedDstPathReplaceInterceptor.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Unstable\n+class RegexMountPointResolvedDstPathReplaceInterceptor\n+    implements RegexMountPointInterceptor {\n+\n+  private String srcRegexString;\n+  private String replaceString;\n+  private Pattern srcRegexPattern;\n+\n+  RegexMountPointResolvedDstPathReplaceInterceptor(String srcRegex,\n+      String replaceString) {\n+    this.srcRegexString = srcRegex;\n+    this.replaceString = replaceString;\n+    this.srcRegexPattern = null;\n+  }\n+\n+  public String getSrcRegexString() {\n+    return srcRegexString;\n+  }\n+\n+  public String getReplaceString() {\n+    return replaceString;\n+  }\n+\n+  public Pattern getSrcRegexPattern() {\n+    return srcRegexPattern;\n+  }\n+\n+  @Override\n+  public void initialize() throws IOException {\n+    try {\n+      srcRegexPattern = Pattern.compile(srcRegexString);\n+    } catch (PatternSyntaxException ex) {\n+      throw new IOException(\n+          \"Initialize interceptor failed, srcRegx:\" + srcRegexString, ex);\n+    }\n+  }\n+\n+  /**\n+   * Intercept source before resolution.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDg1OTMyNg=="}, "originalCommit": {"oid": "3d9b3db8e9df4d5fe43f20880e54c22f1916fdb4"}, "originalPosition": 73}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAwNjY1NDAzOnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/RegexMountPointResolvedDstPathReplaceInterceptor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwNjoxNDo1MlrOHKlfKA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQwMTozNjo1OVrOHLOb0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDg2MTk5Mg==", "bodyText": "same a interceptSrc doc.", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r480861992", "createdAt": "2020-09-01T06:14:52Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/RegexMountPointResolvedDstPathReplaceInterceptor.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.regex.PatternSyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH;\n+\n+/**\n+ * Implementation of RegexMountPointResolvedDstPathReplaceInterceptor.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Unstable\n+class RegexMountPointResolvedDstPathReplaceInterceptor\n+    implements RegexMountPointInterceptor {\n+\n+  private String srcRegexString;\n+  private String replaceString;\n+  private Pattern srcRegexPattern;\n+\n+  RegexMountPointResolvedDstPathReplaceInterceptor(String srcRegex,\n+      String replaceString) {\n+    this.srcRegexString = srcRegex;\n+    this.replaceString = replaceString;\n+    this.srcRegexPattern = null;\n+  }\n+\n+  public String getSrcRegexString() {\n+    return srcRegexString;\n+  }\n+\n+  public String getReplaceString() {\n+    return replaceString;\n+  }\n+\n+  public Pattern getSrcRegexPattern() {\n+    return srcRegexPattern;\n+  }\n+\n+  @Override\n+  public void initialize() throws IOException {\n+    try {\n+      srcRegexPattern = Pattern.compile(srcRegexString);\n+    } catch (PatternSyntaxException ex) {\n+      throw new IOException(\n+          \"Initialize interceptor failed, srcRegx:\" + srcRegexString, ex);\n+    }\n+  }\n+\n+  /**\n+   * Intercept source before resolution.\n+   *\n+   * @param source\n+   * @return\n+   */\n+  @Override\n+  public String interceptSource(String source) {\n+    return source;\n+  }\n+\n+  /**\n+   * Intercept resolved path, e.g.\n+   * Mount point /^(\\\\w+)/, ${1}.hadoop.net\n+   * If incoming path is /user1/home/tmp/job1,\n+   * then the resolved path str will be user1.\n+   *\n+   * @return intercepted string\n+   */\n+  @Override public String interceptResolvedDestPathStr(\n+      String parsedDestPathStr) {\n+    Matcher matcher = srcRegexPattern.matcher(parsedDestPathStr);\n+    return matcher.replaceAll(replaceString);\n+  }\n+\n+  /**\n+   * Intercept remaining path.\n+   *", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d9b3db8e9df4d5fe43f20880e54c22f1916fdb4"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTUzMjg4Mg==", "bodyText": "Done.", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r481532882", "createdAt": "2020-09-02T01:36:59Z", "author": {"login": "JohnZZGithub"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/RegexMountPointResolvedDstPathReplaceInterceptor.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.regex.PatternSyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH;\n+\n+/**\n+ * Implementation of RegexMountPointResolvedDstPathReplaceInterceptor.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Unstable\n+class RegexMountPointResolvedDstPathReplaceInterceptor\n+    implements RegexMountPointInterceptor {\n+\n+  private String srcRegexString;\n+  private String replaceString;\n+  private Pattern srcRegexPattern;\n+\n+  RegexMountPointResolvedDstPathReplaceInterceptor(String srcRegex,\n+      String replaceString) {\n+    this.srcRegexString = srcRegex;\n+    this.replaceString = replaceString;\n+    this.srcRegexPattern = null;\n+  }\n+\n+  public String getSrcRegexString() {\n+    return srcRegexString;\n+  }\n+\n+  public String getReplaceString() {\n+    return replaceString;\n+  }\n+\n+  public Pattern getSrcRegexPattern() {\n+    return srcRegexPattern;\n+  }\n+\n+  @Override\n+  public void initialize() throws IOException {\n+    try {\n+      srcRegexPattern = Pattern.compile(srcRegexString);\n+    } catch (PatternSyntaxException ex) {\n+      throw new IOException(\n+          \"Initialize interceptor failed, srcRegx:\" + srcRegexString, ex);\n+    }\n+  }\n+\n+  /**\n+   * Intercept source before resolution.\n+   *\n+   * @param source\n+   * @return\n+   */\n+  @Override\n+  public String interceptSource(String source) {\n+    return source;\n+  }\n+\n+  /**\n+   * Intercept resolved path, e.g.\n+   * Mount point /^(\\\\w+)/, ${1}.hadoop.net\n+   * If incoming path is /user1/home/tmp/job1,\n+   * then the resolved path str will be user1.\n+   *\n+   * @return intercepted string\n+   */\n+  @Override public String interceptResolvedDestPathStr(\n+      String parsedDestPathStr) {\n+    Matcher matcher = srcRegexPattern.matcher(parsedDestPathStr);\n+    return matcher.replaceAll(replaceString);\n+  }\n+\n+  /**\n+   * Intercept remaining path.\n+   *", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDg2MTk5Mg=="}, "originalCommit": {"oid": "3d9b3db8e9df4d5fe43f20880e54c22f1916fdb4"}, "originalPosition": 99}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAwNjY2MTUyOnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/RegexMountPointResolvedDstPathReplaceInterceptor.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwNjoxNjo0MFrOHKljvw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQwODoyODozMlrOHLj0ZQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDg2MzE2Nw==", "bodyText": "shouldn't it interceptSrc and remainingPath follow same pattern to intercept as they both split from src path only?\nWhats the issue if I use same interceptSource method? Do we have some concerns? Could you explain me if I miss something here?", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r480863167", "createdAt": "2020-09-01T06:16:40Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/RegexMountPointResolvedDstPathReplaceInterceptor.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.regex.PatternSyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH;\n+\n+/**\n+ * Implementation of RegexMountPointResolvedDstPathReplaceInterceptor.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Unstable\n+class RegexMountPointResolvedDstPathReplaceInterceptor\n+    implements RegexMountPointInterceptor {\n+\n+  private String srcRegexString;\n+  private String replaceString;\n+  private Pattern srcRegexPattern;\n+\n+  RegexMountPointResolvedDstPathReplaceInterceptor(String srcRegex,\n+      String replaceString) {\n+    this.srcRegexString = srcRegex;\n+    this.replaceString = replaceString;\n+    this.srcRegexPattern = null;\n+  }\n+\n+  public String getSrcRegexString() {\n+    return srcRegexString;\n+  }\n+\n+  public String getReplaceString() {\n+    return replaceString;\n+  }\n+\n+  public Pattern getSrcRegexPattern() {\n+    return srcRegexPattern;\n+  }\n+\n+  @Override\n+  public void initialize() throws IOException {\n+    try {\n+      srcRegexPattern = Pattern.compile(srcRegexString);\n+    } catch (PatternSyntaxException ex) {\n+      throw new IOException(\n+          \"Initialize interceptor failed, srcRegx:\" + srcRegexString, ex);\n+    }\n+  }\n+\n+  /**\n+   * Intercept source before resolution.\n+   *\n+   * @param source\n+   * @return\n+   */\n+  @Override\n+  public String interceptSource(String source) {\n+    return source;\n+  }\n+\n+  /**\n+   * Intercept resolved path, e.g.\n+   * Mount point /^(\\\\w+)/, ${1}.hadoop.net\n+   * If incoming path is /user1/home/tmp/job1,\n+   * then the resolved path str will be user1.\n+   *\n+   * @return intercepted string\n+   */\n+  @Override public String interceptResolvedDestPathStr(\n+      String parsedDestPathStr) {\n+    Matcher matcher = srcRegexPattern.matcher(parsedDestPathStr);\n+    return matcher.replaceAll(replaceString);\n+  }\n+\n+  /**\n+   * Intercept remaining path.\n+   *\n+   * @return intercepted path\n+   */\n+  @Override public Path interceptRemainingPath(Path remainingPath) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d9b3db8e9df4d5fe43f20880e54c22f1916fdb4"}, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTU0MzQ0Ng==", "bodyText": "This is a great question.\nHere's an example:\n/user/$userName => s3://$userName.apache.org\n/user/hadoop/dir1/dir2/dir3/file3 will be mapped to s3://user.hadoop.apche.org/dir1/dir2/dir3/file3\ninterceptSource(/user/hadoop/) take sourcePath and won't know which part will be replaced in the final path.\nAfter mapping, the dest will be s3://user.hadoop.apche.org/dir1/dir2/dir3/file3\ninterceptRemainingPath(dir1/dir2/dir3/file3) could enable users to do process the remaining part (dir1/dir2/dir3/file3) after the mounted path. E.g. take care of bad characters.\nBut I'm good to remove it if we feel it's cleaner.", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r481543446", "createdAt": "2020-09-02T01:53:30Z", "author": {"login": "JohnZZGithub"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/RegexMountPointResolvedDstPathReplaceInterceptor.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.regex.PatternSyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH;\n+\n+/**\n+ * Implementation of RegexMountPointResolvedDstPathReplaceInterceptor.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Unstable\n+class RegexMountPointResolvedDstPathReplaceInterceptor\n+    implements RegexMountPointInterceptor {\n+\n+  private String srcRegexString;\n+  private String replaceString;\n+  private Pattern srcRegexPattern;\n+\n+  RegexMountPointResolvedDstPathReplaceInterceptor(String srcRegex,\n+      String replaceString) {\n+    this.srcRegexString = srcRegex;\n+    this.replaceString = replaceString;\n+    this.srcRegexPattern = null;\n+  }\n+\n+  public String getSrcRegexString() {\n+    return srcRegexString;\n+  }\n+\n+  public String getReplaceString() {\n+    return replaceString;\n+  }\n+\n+  public Pattern getSrcRegexPattern() {\n+    return srcRegexPattern;\n+  }\n+\n+  @Override\n+  public void initialize() throws IOException {\n+    try {\n+      srcRegexPattern = Pattern.compile(srcRegexString);\n+    } catch (PatternSyntaxException ex) {\n+      throw new IOException(\n+          \"Initialize interceptor failed, srcRegx:\" + srcRegexString, ex);\n+    }\n+  }\n+\n+  /**\n+   * Intercept source before resolution.\n+   *\n+   * @param source\n+   * @return\n+   */\n+  @Override\n+  public String interceptSource(String source) {\n+    return source;\n+  }\n+\n+  /**\n+   * Intercept resolved path, e.g.\n+   * Mount point /^(\\\\w+)/, ${1}.hadoop.net\n+   * If incoming path is /user1/home/tmp/job1,\n+   * then the resolved path str will be user1.\n+   *\n+   * @return intercepted string\n+   */\n+  @Override public String interceptResolvedDestPathStr(\n+      String parsedDestPathStr) {\n+    Matcher matcher = srcRegexPattern.matcher(parsedDestPathStr);\n+    return matcher.replaceAll(replaceString);\n+  }\n+\n+  /**\n+   * Intercept remaining path.\n+   *\n+   * @return intercepted path\n+   */\n+  @Override public Path interceptRemainingPath(Path remainingPath) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDg2MzE2Nw=="}, "originalCommit": {"oid": "3d9b3db8e9df4d5fe43f20880e54c22f1916fdb4"}, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTg4MzIzNw==", "bodyText": "The splitting of remaining part is kind implementation details I feel. User may not clearly have idea on that, unless they are very advanced users.\nMy question was what if we use same API for remaining part also?  in ur example interceptSource((dir1/dir2/dir3/file3)). What will happen here? Intercepter are just for replacing some pattern with other right. Let me know if I am missing.\nThanks for your efforts on fixing other comments..\nAlso please take a look at check-style warnings reported in latest report.", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r481883237", "createdAt": "2020-09-02T08:28:32Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/RegexMountPointResolvedDstPathReplaceInterceptor.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.regex.PatternSyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH;\n+\n+/**\n+ * Implementation of RegexMountPointResolvedDstPathReplaceInterceptor.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Unstable\n+class RegexMountPointResolvedDstPathReplaceInterceptor\n+    implements RegexMountPointInterceptor {\n+\n+  private String srcRegexString;\n+  private String replaceString;\n+  private Pattern srcRegexPattern;\n+\n+  RegexMountPointResolvedDstPathReplaceInterceptor(String srcRegex,\n+      String replaceString) {\n+    this.srcRegexString = srcRegex;\n+    this.replaceString = replaceString;\n+    this.srcRegexPattern = null;\n+  }\n+\n+  public String getSrcRegexString() {\n+    return srcRegexString;\n+  }\n+\n+  public String getReplaceString() {\n+    return replaceString;\n+  }\n+\n+  public Pattern getSrcRegexPattern() {\n+    return srcRegexPattern;\n+  }\n+\n+  @Override\n+  public void initialize() throws IOException {\n+    try {\n+      srcRegexPattern = Pattern.compile(srcRegexString);\n+    } catch (PatternSyntaxException ex) {\n+      throw new IOException(\n+          \"Initialize interceptor failed, srcRegx:\" + srcRegexString, ex);\n+    }\n+  }\n+\n+  /**\n+   * Intercept source before resolution.\n+   *\n+   * @param source\n+   * @return\n+   */\n+  @Override\n+  public String interceptSource(String source) {\n+    return source;\n+  }\n+\n+  /**\n+   * Intercept resolved path, e.g.\n+   * Mount point /^(\\\\w+)/, ${1}.hadoop.net\n+   * If incoming path is /user1/home/tmp/job1,\n+   * then the resolved path str will be user1.\n+   *\n+   * @return intercepted string\n+   */\n+  @Override public String interceptResolvedDestPathStr(\n+      String parsedDestPathStr) {\n+    Matcher matcher = srcRegexPattern.matcher(parsedDestPathStr);\n+    return matcher.replaceAll(replaceString);\n+  }\n+\n+  /**\n+   * Intercept remaining path.\n+   *\n+   * @return intercepted path\n+   */\n+  @Override public Path interceptRemainingPath(Path remainingPath) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDg2MzE2Nw=="}, "originalCommit": {"oid": "3d9b3db8e9df4d5fe43f20880e54c22f1916fdb4"}, "originalPosition": 102}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNTY1MjQyOnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwNDozNzozM1rOHO2WJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwNToyNzoxN1rOHO3Mmg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTMzMjUxOA==", "bodyText": "Do you see any issue if we make it true? If no issues, we can simply clean it on close right instead of having another config?\nSeems like this is an improvement to existing code. If you want, I am ok to file small JIRA and fix this cleanup thing.( I am assuming it's not necessarily needed with this.)", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r485332518", "createdAt": "2020-09-09T04:37:33Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java", "diffHunk": "@@ -217,6 +239,7 @@ public Path getMountedOnPath() {\n   Path homeDir = null;\n   private boolean enableInnerCache = false;\n   private InnerCache cache;\n+  private boolean evictCacheOnClose = false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8786ee8fc0e1b1d617eabd398c4d05a1ff656002"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTM0NjQ1OA==", "bodyText": "@umamaheswararao Thanks for the kindness. I didn't see a problem with making it true. Just mean to be more cautious, let me remove it.", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r485346458", "createdAt": "2020-09-09T05:27:17Z", "author": {"login": "JohnZZGithub"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java", "diffHunk": "@@ -217,6 +239,7 @@ public Path getMountedOnPath() {\n   Path homeDir = null;\n   private boolean enableInnerCache = false;\n   private InnerCache cache;\n+  private boolean evictCacheOnClose = false;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTMzMjUxOA=="}, "originalCommit": {"oid": "8786ee8fc0e1b1d617eabd398c4d05a1ff656002"}, "originalPosition": 86}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNTY1NDY2OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestRegexMountPoint.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwNDozOTowOVrOHO2XkQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwNToyNzozNlrOHO3M7g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTMzMjg4MQ==", "bodyText": "Still this asserts can use assertEquals method? Please use them all places.", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r485332881", "createdAt": "2020-09-09T04:39:09Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestRegexMountPoint.java", "diffHunk": "@@ -0,0 +1,160 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Test Regex Mount Point.\n+ */\n+public class TestRegexMountPoint {\n+  private static final Logger LOGGER =\n+      LoggerFactory.getLogger(TestRegexMountPoint.class.getName());\n+\n+  private InodeTree inodeTree;\n+  private Configuration conf;\n+\n+  class TestRegexMountPointFileSystem {\n+    public URI getUri() {\n+      return uri;\n+    }\n+\n+    private URI uri;\n+\n+    TestRegexMountPointFileSystem(URI uri) {\n+      String uriStr = uri == null ? \"null\" : uri.toString();\n+      LOGGER.info(\"Create TestRegexMountPointFileSystem Via URI:\" + uriStr);\n+      this.uri = uri;\n+    }\n+  }\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    ConfigUtil.addLink(conf, TestRegexMountPoint.class.getName(), \"/mnt\",\n+        URI.create(\"file:///\"));\n+\n+    inodeTree = new InodeTree<TestRegexMountPointFileSystem>(conf,\n+        TestRegexMountPoint.class.getName(), null, false) {\n+      @Override\n+      protected TestRegexMountPointFileSystem getTargetFileSystem(\n+          final URI uri) {\n+        return new TestRegexMountPointFileSystem(uri);\n+      }\n+\n+      @Override\n+      protected TestRegexMountPointFileSystem getTargetFileSystem(\n+          final INodeDir<TestRegexMountPointFileSystem> dir) {\n+        return new TestRegexMountPointFileSystem(null);\n+      }\n+\n+      @Override\n+      protected TestRegexMountPointFileSystem getTargetFileSystem(\n+          final String settings, final URI[] mergeFsURIList) {\n+        return new TestRegexMountPointFileSystem(null);\n+      }\n+    };\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    inodeTree = null;\n+  }\n+\n+  @Test\n+  public void testGetVarListInString() throws IOException {\n+    String srcRegex = \"/(\\\\w+)\";\n+    String target = \"/$0/${1}/$1/${2}/${2}\";\n+    RegexMountPoint regexMountPoint =\n+        new RegexMountPoint(inodeTree, srcRegex, target, null);\n+    regexMountPoint.initialize();\n+    Map<String, Set<String>> varMap = regexMountPoint.getVarInDestPathMap();\n+    Assert.assertEquals(varMap.size(), 3);\n+    Assert.assertEquals(varMap.get(\"0\").size(), 1);\n+    Assert.assertTrue(varMap.get(\"0\").contains(\"$0\"));\n+    Assert.assertEquals(varMap.get(\"1\").size(), 2);\n+    Assert.assertTrue(varMap.get(\"1\").contains(\"${1}\"));\n+    Assert.assertTrue(varMap.get(\"1\").contains(\"$1\"));\n+    Assert.assertEquals(varMap.get(\"2\").size(), 1);\n+    Assert.assertTrue(varMap.get(\"2\").contains(\"${2}\"));\n+  }\n+\n+  @Test\n+  public void testResolve() throws IOException {\n+    String regexStr = \"^/user/(?<username>\\\\w+)\";\n+    String dstPathStr = \"/namenode1/testResolve/$username\";\n+    String settingsStr = null;\n+    RegexMountPoint regexMountPoint =\n+        new RegexMountPoint(inodeTree, regexStr, dstPathStr, settingsStr);\n+    regexMountPoint.initialize();\n+    InodeTree.ResolveResult resolveResult =\n+        regexMountPoint.resolve(\"/user/hadoop/file1\", true);\n+    Assert.assertEquals(resolveResult.kind, InodeTree.ResultKind.EXTERNAL_DIR);\n+    Assert.assertTrue(\n+        resolveResult.targetFileSystem\n+            instanceof TestRegexMountPointFileSystem);\n+    Assert.assertTrue(resolveResult.resolvedPath.equals(\"/user/hadoop\"));\n+    Assert.assertTrue(\n+        resolveResult.targetFileSystem\n+            instanceof TestRegexMountPointFileSystem);\n+    Assert.assertTrue(\n+        ((TestRegexMountPointFileSystem) resolveResult.targetFileSystem)\n+            .getUri().toString().equals(\"/namenode1/testResolve/hadoop\"));\n+    Assert.assertTrue(resolveResult.remainingPath.toString().equals(\"/file1\"));\n+  }\n+\n+  @Test\n+  public void testResolveWithInterceptor() throws IOException {\n+    String regexStr = \"^/user/(?<username>\\\\w+)\";\n+    String dstPathStr = \"/namenode1/testResolve/$username\";\n+    // Replace \"_\" with \"-\"\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        new RegexMountPointResolvedDstPathReplaceInterceptor(\"_\", \"-\");\n+    // replaceresolvedpath:_:-\n+    String settingsStr = interceptor.serializeToString();\n+    RegexMountPoint regexMountPoint =\n+        new RegexMountPoint(inodeTree, regexStr, dstPathStr, settingsStr);\n+    regexMountPoint.initialize();\n+    InodeTree.ResolveResult resolveResult =\n+        regexMountPoint.resolve(\"/user/hadoop_user1/file_index\", true);\n+    Assert.assertEquals(resolveResult.kind, InodeTree.ResultKind.EXTERNAL_DIR);\n+    Assert.assertTrue(\n+        resolveResult.targetFileSystem\n+            instanceof TestRegexMountPointFileSystem);\n+    Assert.assertTrue(resolveResult.resolvedPath.equals(\"/user/hadoop_user1\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8786ee8fc0e1b1d617eabd398c4d05a1ff656002"}, "originalPosition": 150}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTM0NjU0Mg==", "bodyText": "Nice catch, let me clean them up.", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r485346542", "createdAt": "2020-09-09T05:27:36Z", "author": {"login": "JohnZZGithub"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestRegexMountPoint.java", "diffHunk": "@@ -0,0 +1,160 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Test Regex Mount Point.\n+ */\n+public class TestRegexMountPoint {\n+  private static final Logger LOGGER =\n+      LoggerFactory.getLogger(TestRegexMountPoint.class.getName());\n+\n+  private InodeTree inodeTree;\n+  private Configuration conf;\n+\n+  class TestRegexMountPointFileSystem {\n+    public URI getUri() {\n+      return uri;\n+    }\n+\n+    private URI uri;\n+\n+    TestRegexMountPointFileSystem(URI uri) {\n+      String uriStr = uri == null ? \"null\" : uri.toString();\n+      LOGGER.info(\"Create TestRegexMountPointFileSystem Via URI:\" + uriStr);\n+      this.uri = uri;\n+    }\n+  }\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    ConfigUtil.addLink(conf, TestRegexMountPoint.class.getName(), \"/mnt\",\n+        URI.create(\"file:///\"));\n+\n+    inodeTree = new InodeTree<TestRegexMountPointFileSystem>(conf,\n+        TestRegexMountPoint.class.getName(), null, false) {\n+      @Override\n+      protected TestRegexMountPointFileSystem getTargetFileSystem(\n+          final URI uri) {\n+        return new TestRegexMountPointFileSystem(uri);\n+      }\n+\n+      @Override\n+      protected TestRegexMountPointFileSystem getTargetFileSystem(\n+          final INodeDir<TestRegexMountPointFileSystem> dir) {\n+        return new TestRegexMountPointFileSystem(null);\n+      }\n+\n+      @Override\n+      protected TestRegexMountPointFileSystem getTargetFileSystem(\n+          final String settings, final URI[] mergeFsURIList) {\n+        return new TestRegexMountPointFileSystem(null);\n+      }\n+    };\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    inodeTree = null;\n+  }\n+\n+  @Test\n+  public void testGetVarListInString() throws IOException {\n+    String srcRegex = \"/(\\\\w+)\";\n+    String target = \"/$0/${1}/$1/${2}/${2}\";\n+    RegexMountPoint regexMountPoint =\n+        new RegexMountPoint(inodeTree, srcRegex, target, null);\n+    regexMountPoint.initialize();\n+    Map<String, Set<String>> varMap = regexMountPoint.getVarInDestPathMap();\n+    Assert.assertEquals(varMap.size(), 3);\n+    Assert.assertEquals(varMap.get(\"0\").size(), 1);\n+    Assert.assertTrue(varMap.get(\"0\").contains(\"$0\"));\n+    Assert.assertEquals(varMap.get(\"1\").size(), 2);\n+    Assert.assertTrue(varMap.get(\"1\").contains(\"${1}\"));\n+    Assert.assertTrue(varMap.get(\"1\").contains(\"$1\"));\n+    Assert.assertEquals(varMap.get(\"2\").size(), 1);\n+    Assert.assertTrue(varMap.get(\"2\").contains(\"${2}\"));\n+  }\n+\n+  @Test\n+  public void testResolve() throws IOException {\n+    String regexStr = \"^/user/(?<username>\\\\w+)\";\n+    String dstPathStr = \"/namenode1/testResolve/$username\";\n+    String settingsStr = null;\n+    RegexMountPoint regexMountPoint =\n+        new RegexMountPoint(inodeTree, regexStr, dstPathStr, settingsStr);\n+    regexMountPoint.initialize();\n+    InodeTree.ResolveResult resolveResult =\n+        regexMountPoint.resolve(\"/user/hadoop/file1\", true);\n+    Assert.assertEquals(resolveResult.kind, InodeTree.ResultKind.EXTERNAL_DIR);\n+    Assert.assertTrue(\n+        resolveResult.targetFileSystem\n+            instanceof TestRegexMountPointFileSystem);\n+    Assert.assertTrue(resolveResult.resolvedPath.equals(\"/user/hadoop\"));\n+    Assert.assertTrue(\n+        resolveResult.targetFileSystem\n+            instanceof TestRegexMountPointFileSystem);\n+    Assert.assertTrue(\n+        ((TestRegexMountPointFileSystem) resolveResult.targetFileSystem)\n+            .getUri().toString().equals(\"/namenode1/testResolve/hadoop\"));\n+    Assert.assertTrue(resolveResult.remainingPath.toString().equals(\"/file1\"));\n+  }\n+\n+  @Test\n+  public void testResolveWithInterceptor() throws IOException {\n+    String regexStr = \"^/user/(?<username>\\\\w+)\";\n+    String dstPathStr = \"/namenode1/testResolve/$username\";\n+    // Replace \"_\" with \"-\"\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        new RegexMountPointResolvedDstPathReplaceInterceptor(\"_\", \"-\");\n+    // replaceresolvedpath:_:-\n+    String settingsStr = interceptor.serializeToString();\n+    RegexMountPoint regexMountPoint =\n+        new RegexMountPoint(inodeTree, regexStr, dstPathStr, settingsStr);\n+    regexMountPoint.initialize();\n+    InodeTree.ResolveResult resolveResult =\n+        regexMountPoint.resolve(\"/user/hadoop_user1/file_index\", true);\n+    Assert.assertEquals(resolveResult.kind, InodeTree.ResultKind.EXTERNAL_DIR);\n+    Assert.assertTrue(\n+        resolveResult.targetFileSystem\n+            instanceof TestRegexMountPointFileSystem);\n+    Assert.assertTrue(resolveResult.resolvedPath.equals(\"/user/hadoop_user1\"));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTMzMjg4MQ=="}, "originalCommit": {"oid": "8786ee8fc0e1b1d617eabd398c4d05a1ff656002"}, "originalPosition": 150}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNTY2MTMyOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemLinkRegex.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwNDo0MjoyN1rOHO2bFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwNToyODowNVrOHO3Ngw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTMzMzc4MQ==", "bodyText": "small suggestion here:\nMove above lines out of try block. Then use try(FileSystem vfs = = FileSystem.get(viewFsUri, config)){\n}\nThis should close automatically after block. So, we can remove finally block below?", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r485333781", "createdAt": "2020-09-09T04:42:27Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemLinkRegex.java", "diffHunk": "@@ -0,0 +1,473 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.MiniDFSNNTopology;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.fs.viewfs.RegexMountPoint.INTERCEPTOR_INTERNAL_SEP;\n+import static org.junit.Assert.assertSame;\n+\n+/**\n+ * Test linkRegex node type for view file system.\n+ */\n+public class TestViewFileSystemLinkRegex extends ViewFileSystemBaseTest {\n+  public static final Logger LOGGER =\n+      LoggerFactory.getLogger(TestViewFileSystemLinkRegex.class);\n+\n+  private static FileSystem fsDefault;\n+  private static MiniDFSCluster cluster;\n+  private static Configuration clusterConfig;\n+  private static final int NAME_SPACES_COUNT = 3;\n+  private static final int DATA_NODES_COUNT = 3;\n+  private static final int FS_INDEX_DEFAULT = 0;\n+  private static final FileSystem[] FS_HDFS = new FileSystem[NAME_SPACES_COUNT];\n+  private static final String CLUSTER_NAME =\n+      \"TestViewFileSystemLinkRegexCluster\";\n+  private static final File TEST_DIR = GenericTestUtils\n+      .getTestDir(TestViewFileSystemLinkRegex.class.getSimpleName());\n+  private static final String TEST_BASE_PATH =\n+      \"/tmp/TestViewFileSystemLinkRegex\";\n+\n+  @Override\n+  protected FileSystemTestHelper createFileSystemHelper() {\n+    return new FileSystemTestHelper(TEST_BASE_PATH);\n+  }\n+\n+  @BeforeClass\n+  public static void clusterSetupAtBeginning() throws IOException {\n+    SupportsBlocks = true;\n+    clusterConfig = ViewFileSystemTestSetup.createConfig();\n+    clusterConfig.setBoolean(\n+        DFSConfigKeys.DFS_NAMENODE_DELEGATION_TOKEN_ALWAYS_USE_KEY,\n+        true);\n+    cluster = new MiniDFSCluster.Builder(clusterConfig).nnTopology(\n+        MiniDFSNNTopology.simpleFederatedTopology(NAME_SPACES_COUNT))\n+        .numDataNodes(DATA_NODES_COUNT).build();\n+    cluster.waitClusterUp();\n+\n+    for (int i = 0; i < NAME_SPACES_COUNT; i++) {\n+      FS_HDFS[i] = cluster.getFileSystem(i);\n+    }\n+    fsDefault = FS_HDFS[FS_INDEX_DEFAULT];\n+  }\n+\n+  @AfterClass\n+  public static void clusterShutdownAtEnd() throws Exception {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  @Override\n+  @Before\n+  public void setUp() throws Exception {\n+    fsTarget = fsDefault;\n+    super.setUp();\n+  }\n+\n+  /**\n+   * Override this so that we don't set the targetTestRoot to any path under the\n+   * root of the FS, and so that we don't try to delete the test dir, but rather\n+   * only its contents.\n+   */\n+  @Override\n+  void initializeTargetTestRoot() throws IOException {\n+    targetTestRoot = fsDefault.makeQualified(new Path(\"/\"));\n+    for (FileStatus status : fsDefault.listStatus(targetTestRoot)) {\n+      fsDefault.delete(status.getPath(), true);\n+    }\n+  }\n+\n+  @Override\n+  void setupMountPoints() {\n+    super.setupMountPoints();\n+  }\n+\n+  @Override\n+  int getExpectedDelegationTokenCount() {\n+    return 1; // all point to the same fs so 1 unique token\n+  }\n+\n+  @Override\n+  int getExpectedDelegationTokenCountWithCredentials() {\n+    return 1;\n+  }\n+\n+  public String buildReplaceInterceptorSettingString(String srcRegex,\n+      String replaceString) {\n+    return\n+        RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH.getConfigName()\n+            + INTERCEPTOR_INTERNAL_SEP + srcRegex + INTERCEPTOR_INTERNAL_SEP\n+            + replaceString;\n+  }\n+\n+  public String linkInterceptorSettings(\n+      List<String> interceptorSettingStrList) {\n+    StringBuilder stringBuilder = new StringBuilder();\n+    int listSize = interceptorSettingStrList.size();\n+    for (int i = 0; i < listSize; ++i) {\n+      stringBuilder.append(interceptorSettingStrList.get(i));\n+      if (i < listSize - 1) {\n+        stringBuilder.append(RegexMountPoint.INTERCEPTOR_SEP);\n+      }\n+    }\n+    return stringBuilder.toString();\n+  }\n+\n+  private void createDirWithChildren(\n+      FileSystem fileSystem, Path dir, List<Path> childrenFiles)\n+      throws IOException {\n+    Assert.assertTrue(fileSystem.mkdirs(dir));\n+    int index = 0;\n+    for (Path childFile : childrenFiles) {\n+      createFile(fileSystem, childFile, index, true);\n+    }\n+  }\n+\n+  private void createFile(\n+      FileSystem fileSystem, Path file, int dataLenToWrite, boolean overwrite)\n+      throws IOException {\n+    FSDataOutputStream outputStream = null;\n+    try {\n+      outputStream = fileSystem.create(file, overwrite);\n+      for (int i = 0; i < dataLenToWrite; ++i) {\n+        outputStream.writeByte(i);\n+      }\n+      outputStream.close();\n+    } finally {\n+      if (outputStream != null) {\n+        outputStream.close();\n+      }\n+    }\n+  }\n+\n+  private void createDirWithChildren(\n+      FileSystem fileSystem, Path dir, int childrenFilesCnt)\n+      throws IOException {\n+    List<Path> childrenFiles = new ArrayList<>(childrenFilesCnt);\n+    for (int i = 0; i < childrenFilesCnt; ++i) {\n+      childrenFiles.add(new Path(dir, \"file\" + i));\n+    }\n+    createDirWithChildren(fileSystem, dir, childrenFiles);\n+  }\n+\n+  /**\n+   * The function used to test regex mountpoints.\n+   * @param config - get mountable config from this conf\n+   * @param regexStr - the src path regex expression that applies to this config\n+   * @param dstPathStr - the string of target path\n+   * @param interceptorSettings - the serialized interceptor string to be\n+   *                           applied while resolving the mapping\n+   * @param dirPathBeforeMountPoint - the src path user passed in to be mapped.\n+   * @param expectedResolveResult - the expected path after resolve\n+   *                             dirPathBeforeMountPoint via regex mountpint.\n+   * @param childrenFilesCnt - the child files under dirPathBeforeMountPoint to\n+   *                         be created\n+   * @throws IOException\n+   * @throws URISyntaxException\n+   */\n+  private void testRegexMountpoint(\n+      Configuration config,\n+      String regexStr,\n+      String dstPathStr,\n+      String interceptorSettings,\n+      Path dirPathBeforeMountPoint,\n+      Path expectedResolveResult,\n+      int childrenFilesCnt)\n+      throws IOException, URISyntaxException {\n+    FileSystem vfs = null;\n+    try {\n+      // Set up test env\n+      createDirWithChildren(\n+          fsTarget, expectedResolveResult, childrenFilesCnt);\n+      ConfigUtil.addLinkRegex(\n+          config, CLUSTER_NAME, regexStr, dstPathStr, interceptorSettings);\n+\n+      // Asserts\n+      URI viewFsUri = new URI(\n+          FsConstants.VIEWFS_SCHEME, CLUSTER_NAME, \"/\", null, null);\n+      vfs = FileSystem.get(viewFsUri, config);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8786ee8fc0e1b1d617eabd398c4d05a1ff656002"}, "originalPosition": 229}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTM0NjY5MQ==", "bodyText": "Good call", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r485346691", "createdAt": "2020-09-09T05:28:05Z", "author": {"login": "JohnZZGithub"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemLinkRegex.java", "diffHunk": "@@ -0,0 +1,473 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.MiniDFSNNTopology;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.fs.viewfs.RegexMountPoint.INTERCEPTOR_INTERNAL_SEP;\n+import static org.junit.Assert.assertSame;\n+\n+/**\n+ * Test linkRegex node type for view file system.\n+ */\n+public class TestViewFileSystemLinkRegex extends ViewFileSystemBaseTest {\n+  public static final Logger LOGGER =\n+      LoggerFactory.getLogger(TestViewFileSystemLinkRegex.class);\n+\n+  private static FileSystem fsDefault;\n+  private static MiniDFSCluster cluster;\n+  private static Configuration clusterConfig;\n+  private static final int NAME_SPACES_COUNT = 3;\n+  private static final int DATA_NODES_COUNT = 3;\n+  private static final int FS_INDEX_DEFAULT = 0;\n+  private static final FileSystem[] FS_HDFS = new FileSystem[NAME_SPACES_COUNT];\n+  private static final String CLUSTER_NAME =\n+      \"TestViewFileSystemLinkRegexCluster\";\n+  private static final File TEST_DIR = GenericTestUtils\n+      .getTestDir(TestViewFileSystemLinkRegex.class.getSimpleName());\n+  private static final String TEST_BASE_PATH =\n+      \"/tmp/TestViewFileSystemLinkRegex\";\n+\n+  @Override\n+  protected FileSystemTestHelper createFileSystemHelper() {\n+    return new FileSystemTestHelper(TEST_BASE_PATH);\n+  }\n+\n+  @BeforeClass\n+  public static void clusterSetupAtBeginning() throws IOException {\n+    SupportsBlocks = true;\n+    clusterConfig = ViewFileSystemTestSetup.createConfig();\n+    clusterConfig.setBoolean(\n+        DFSConfigKeys.DFS_NAMENODE_DELEGATION_TOKEN_ALWAYS_USE_KEY,\n+        true);\n+    cluster = new MiniDFSCluster.Builder(clusterConfig).nnTopology(\n+        MiniDFSNNTopology.simpleFederatedTopology(NAME_SPACES_COUNT))\n+        .numDataNodes(DATA_NODES_COUNT).build();\n+    cluster.waitClusterUp();\n+\n+    for (int i = 0; i < NAME_SPACES_COUNT; i++) {\n+      FS_HDFS[i] = cluster.getFileSystem(i);\n+    }\n+    fsDefault = FS_HDFS[FS_INDEX_DEFAULT];\n+  }\n+\n+  @AfterClass\n+  public static void clusterShutdownAtEnd() throws Exception {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  @Override\n+  @Before\n+  public void setUp() throws Exception {\n+    fsTarget = fsDefault;\n+    super.setUp();\n+  }\n+\n+  /**\n+   * Override this so that we don't set the targetTestRoot to any path under the\n+   * root of the FS, and so that we don't try to delete the test dir, but rather\n+   * only its contents.\n+   */\n+  @Override\n+  void initializeTargetTestRoot() throws IOException {\n+    targetTestRoot = fsDefault.makeQualified(new Path(\"/\"));\n+    for (FileStatus status : fsDefault.listStatus(targetTestRoot)) {\n+      fsDefault.delete(status.getPath(), true);\n+    }\n+  }\n+\n+  @Override\n+  void setupMountPoints() {\n+    super.setupMountPoints();\n+  }\n+\n+  @Override\n+  int getExpectedDelegationTokenCount() {\n+    return 1; // all point to the same fs so 1 unique token\n+  }\n+\n+  @Override\n+  int getExpectedDelegationTokenCountWithCredentials() {\n+    return 1;\n+  }\n+\n+  public String buildReplaceInterceptorSettingString(String srcRegex,\n+      String replaceString) {\n+    return\n+        RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH.getConfigName()\n+            + INTERCEPTOR_INTERNAL_SEP + srcRegex + INTERCEPTOR_INTERNAL_SEP\n+            + replaceString;\n+  }\n+\n+  public String linkInterceptorSettings(\n+      List<String> interceptorSettingStrList) {\n+    StringBuilder stringBuilder = new StringBuilder();\n+    int listSize = interceptorSettingStrList.size();\n+    for (int i = 0; i < listSize; ++i) {\n+      stringBuilder.append(interceptorSettingStrList.get(i));\n+      if (i < listSize - 1) {\n+        stringBuilder.append(RegexMountPoint.INTERCEPTOR_SEP);\n+      }\n+    }\n+    return stringBuilder.toString();\n+  }\n+\n+  private void createDirWithChildren(\n+      FileSystem fileSystem, Path dir, List<Path> childrenFiles)\n+      throws IOException {\n+    Assert.assertTrue(fileSystem.mkdirs(dir));\n+    int index = 0;\n+    for (Path childFile : childrenFiles) {\n+      createFile(fileSystem, childFile, index, true);\n+    }\n+  }\n+\n+  private void createFile(\n+      FileSystem fileSystem, Path file, int dataLenToWrite, boolean overwrite)\n+      throws IOException {\n+    FSDataOutputStream outputStream = null;\n+    try {\n+      outputStream = fileSystem.create(file, overwrite);\n+      for (int i = 0; i < dataLenToWrite; ++i) {\n+        outputStream.writeByte(i);\n+      }\n+      outputStream.close();\n+    } finally {\n+      if (outputStream != null) {\n+        outputStream.close();\n+      }\n+    }\n+  }\n+\n+  private void createDirWithChildren(\n+      FileSystem fileSystem, Path dir, int childrenFilesCnt)\n+      throws IOException {\n+    List<Path> childrenFiles = new ArrayList<>(childrenFilesCnt);\n+    for (int i = 0; i < childrenFilesCnt; ++i) {\n+      childrenFiles.add(new Path(dir, \"file\" + i));\n+    }\n+    createDirWithChildren(fileSystem, dir, childrenFiles);\n+  }\n+\n+  /**\n+   * The function used to test regex mountpoints.\n+   * @param config - get mountable config from this conf\n+   * @param regexStr - the src path regex expression that applies to this config\n+   * @param dstPathStr - the string of target path\n+   * @param interceptorSettings - the serialized interceptor string to be\n+   *                           applied while resolving the mapping\n+   * @param dirPathBeforeMountPoint - the src path user passed in to be mapped.\n+   * @param expectedResolveResult - the expected path after resolve\n+   *                             dirPathBeforeMountPoint via regex mountpint.\n+   * @param childrenFilesCnt - the child files under dirPathBeforeMountPoint to\n+   *                         be created\n+   * @throws IOException\n+   * @throws URISyntaxException\n+   */\n+  private void testRegexMountpoint(\n+      Configuration config,\n+      String regexStr,\n+      String dstPathStr,\n+      String interceptorSettings,\n+      Path dirPathBeforeMountPoint,\n+      Path expectedResolveResult,\n+      int childrenFilesCnt)\n+      throws IOException, URISyntaxException {\n+    FileSystem vfs = null;\n+    try {\n+      // Set up test env\n+      createDirWithChildren(\n+          fsTarget, expectedResolveResult, childrenFilesCnt);\n+      ConfigUtil.addLinkRegex(\n+          config, CLUSTER_NAME, regexStr, dstPathStr, interceptorSettings);\n+\n+      // Asserts\n+      URI viewFsUri = new URI(\n+          FsConstants.VIEWFS_SCHEME, CLUSTER_NAME, \"/\", null, null);\n+      vfs = FileSystem.get(viewFsUri, config);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTMzMzc4MQ=="}, "originalCommit": {"oid": "8786ee8fc0e1b1d617eabd398c4d05a1ff656002"}, "originalPosition": 229}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNTY5NzA2OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/site/markdown/ViewFs.md", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwNTowMDowOVrOHO2u8g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQyMjoyMDoyMFrOHPcQug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTMzODg2Ng==", "bodyText": "For a regex-base rule mount table entry, we couldn't know what's corresponding path until parsing.\n\nWhatever we know should be added to mountPoints? So, that getMountPoints will return known fs-es? It may be a good idea to add Java doc on API level.\nI am ok to have this in followup JIRA to cover all of this aspects.", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r485338866", "createdAt": "2020-09-09T05:00:09Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/site/markdown/ViewFs.md", "diffHunk": "@@ -366,6 +366,69 @@ Don't want to change scheme or difficult to copy mount-table configurations to a\n \n Please refer to the [View File System Overload Scheme Guide](./ViewFsOverloadScheme.html)\n \n+Regex Pattern Based Mount Points\n+--------------------------------\n+\n+The view file system mount points were a Key-Value based mapping system. It is not friendly for user cases which mapping config could be abstracted to rules. E.g. Users want to provide a GCS bucket per user and there might be thousands of users in total. The old key-value based approach won't work well for several reasons:\n+\n+1. The mount table is used by FileSystem clients. There's a cost to spread the config to all clients and we should avoid it if possible. The [View File System Overload Scheme Guide](./ViewFsOverloadScheme.html) could help the distribution by central mount table management. But the mount table still have to be updated on every change. The change could be greatly avoided if provide a rule-based mount table.\n+\n+2. The client have to understand all the KVs in the mount table. This is not ideal when the mountable grows to thousands of items. E.g. thousands of file systems might be initialized even users only need one. And the config itself will become bloated at scale.\n+\n+### Understand the Difference\n+\n+In the key-value based mount table, view file system treats every mount point as a partition. There's several file system APIs which will lead to operation on all partitions. E.g. there's an HDFS cluster with multiple mount. Users want to run \u201chadoop fs -put file viewfs://hdfs.namenode.apache.org/tmp/\u201d cmd to copy data from local disk to our HDFS cluster. The cmd will trigger ViewFileSystem to call setVerifyChecksum() method which will initialize the file system for every mount point.\n+For a regex-base rule mount table entry, we couldn't know what's corresponding path until parsing. So the regex based mount table entry will be ignored on such cases. The file system (ChRootedFileSystem) will be created upon accessing. But the underlying file system will be cached by inner cache of ViewFileSystem.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8786ee8fc0e1b1d617eabd398c4d05a1ff656002"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTM1Njc5NQ==", "bodyText": "Good idea. I guess this patch didn't add parsed fs to mount point yet. Maybe it's better when we modify the code and doc at the same time. Created https://issues.apache.org/jira/browse/HADOOP-17247 to track the issue. Does it make sense? Thanks", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r485356795", "createdAt": "2020-09-09T05:59:58Z", "author": {"login": "JohnZZGithub"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/site/markdown/ViewFs.md", "diffHunk": "@@ -366,6 +366,69 @@ Don't want to change scheme or difficult to copy mount-table configurations to a\n \n Please refer to the [View File System Overload Scheme Guide](./ViewFsOverloadScheme.html)\n \n+Regex Pattern Based Mount Points\n+--------------------------------\n+\n+The view file system mount points were a Key-Value based mapping system. It is not friendly for user cases which mapping config could be abstracted to rules. E.g. Users want to provide a GCS bucket per user and there might be thousands of users in total. The old key-value based approach won't work well for several reasons:\n+\n+1. The mount table is used by FileSystem clients. There's a cost to spread the config to all clients and we should avoid it if possible. The [View File System Overload Scheme Guide](./ViewFsOverloadScheme.html) could help the distribution by central mount table management. But the mount table still have to be updated on every change. The change could be greatly avoided if provide a rule-based mount table.\n+\n+2. The client have to understand all the KVs in the mount table. This is not ideal when the mountable grows to thousands of items. E.g. thousands of file systems might be initialized even users only need one. And the config itself will become bloated at scale.\n+\n+### Understand the Difference\n+\n+In the key-value based mount table, view file system treats every mount point as a partition. There's several file system APIs which will lead to operation on all partitions. E.g. there's an HDFS cluster with multiple mount. Users want to run \u201chadoop fs -put file viewfs://hdfs.namenode.apache.org/tmp/\u201d cmd to copy data from local disk to our HDFS cluster. The cmd will trigger ViewFileSystem to call setVerifyChecksum() method which will initialize the file system for every mount point.\n+For a regex-base rule mount table entry, we couldn't know what's corresponding path until parsing. So the regex based mount table entry will be ignored on such cases. The file system (ChRootedFileSystem) will be created upon accessing. But the underlying file system will be cached by inner cache of ViewFileSystem.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTMzODg2Ng=="}, "originalCommit": {"oid": "8786ee8fc0e1b1d617eabd398c4d05a1ff656002"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTk1MzcyMg==", "bodyText": "Thanks for having separate JIRA. It make sense to me. !", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r485953722", "createdAt": "2020-09-09T22:20:20Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/site/markdown/ViewFs.md", "diffHunk": "@@ -366,6 +366,69 @@ Don't want to change scheme or difficult to copy mount-table configurations to a\n \n Please refer to the [View File System Overload Scheme Guide](./ViewFsOverloadScheme.html)\n \n+Regex Pattern Based Mount Points\n+--------------------------------\n+\n+The view file system mount points were a Key-Value based mapping system. It is not friendly for user cases which mapping config could be abstracted to rules. E.g. Users want to provide a GCS bucket per user and there might be thousands of users in total. The old key-value based approach won't work well for several reasons:\n+\n+1. The mount table is used by FileSystem clients. There's a cost to spread the config to all clients and we should avoid it if possible. The [View File System Overload Scheme Guide](./ViewFsOverloadScheme.html) could help the distribution by central mount table management. But the mount table still have to be updated on every change. The change could be greatly avoided if provide a rule-based mount table.\n+\n+2. The client have to understand all the KVs in the mount table. This is not ideal when the mountable grows to thousands of items. E.g. thousands of file systems might be initialized even users only need one. And the config itself will become bloated at scale.\n+\n+### Understand the Difference\n+\n+In the key-value based mount table, view file system treats every mount point as a partition. There's several file system APIs which will lead to operation on all partitions. E.g. there's an HDFS cluster with multiple mount. Users want to run \u201chadoop fs -put file viewfs://hdfs.namenode.apache.org/tmp/\u201d cmd to copy data from local disk to our HDFS cluster. The cmd will trigger ViewFileSystem to call setVerifyChecksum() method which will initialize the file system for every mount point.\n+For a regex-base rule mount table entry, we couldn't know what's corresponding path until parsing. So the regex based mount table entry will be ignored on such cases. The file system (ChRootedFileSystem) will be created upon accessing. But the underlying file system will be cached by inner cache of ViewFileSystem.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTMzODg2Ng=="}, "originalCommit": {"oid": "8786ee8fc0e1b1d617eabd398c4d05a1ff656002"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0MDMzNTYwOnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQwNDo1NzowNlrOHPi80A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQwNToxOToyNlrOHPjcPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjA2MzMxMg==", "bodyText": "This method not used anywhere?", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r486063312", "createdAt": "2020-09-10T04:57:06Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java", "diffHunk": "@@ -166,6 +166,42 @@ public static void addLinkNfly(final Configuration conf, final String src,\n     addLinkNfly(conf, getDefaultMountTableName(conf), src, null, targets);\n   }\n \n+\n+  /**\n+   * Add a LinkRegex to the config for the specified mount table.\n+   * @param conf - get mountable config from this conf\n+   * @param mountTableName - the mountable name of the regex config item\n+   * @param srcRegex - the src path regex expression that applies to this config\n+   * @param targetStr - the string of target path\n+   */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fa30c5ec629e54379368db91dcf51f2414b03d28"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjA3MDg5Mg==", "bodyText": "Good catch, I guess the next addLinkRegex is used but not this one.", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r486070892", "createdAt": "2020-09-10T05:17:47Z", "author": {"login": "JohnZZGithub"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java", "diffHunk": "@@ -166,6 +166,42 @@ public static void addLinkNfly(final Configuration conf, final String src,\n     addLinkNfly(conf, getDefaultMountTableName(conf), src, null, targets);\n   }\n \n+\n+  /**\n+   * Add a LinkRegex to the config for the specified mount table.\n+   * @param conf - get mountable config from this conf\n+   * @param mountTableName - the mountable name of the regex config item\n+   * @param srcRegex - the src path regex expression that applies to this config\n+   * @param targetStr - the string of target path\n+   */", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjA2MzMxMg=="}, "originalCommit": {"oid": "fa30c5ec629e54379368db91dcf51f2414b03d28"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjA3MTM1Ng==", "bodyText": "Removed.", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r486071356", "createdAt": "2020-09-10T05:19:26Z", "author": {"login": "JohnZZGithub"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java", "diffHunk": "@@ -166,6 +166,42 @@ public static void addLinkNfly(final Configuration conf, final String src,\n     addLinkNfly(conf, getDefaultMountTableName(conf), src, null, targets);\n   }\n \n+\n+  /**\n+   * Add a LinkRegex to the config for the specified mount table.\n+   * @param conf - get mountable config from this conf\n+   * @param mountTableName - the mountable name of the regex config item\n+   * @param srcRegex - the src path regex expression that applies to this config\n+   * @param targetStr - the string of target path\n+   */", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjA2MzMxMg=="}, "originalCommit": {"oid": "fa30c5ec629e54379368db91dcf51f2414b03d28"}, "originalPosition": 11}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3359, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}