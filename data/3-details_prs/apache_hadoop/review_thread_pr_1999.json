{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE0Mjk2NjQ4", "number": 1999, "reviewThreads": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNjoyNDoyNFrOD94ALQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMVQxODozODoyN1rOEBeKrQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MjA3Mjc3OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNjoyNDoyNFrOGXoewA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNjoyODozNlrOGXop4w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQzMzY2NA==", "bodyText": "nit: add a gap between org.apache imports and the others", "url": "https://github.com/apache/hadoop/pull/1999#discussion_r427433664", "createdAt": "2020-05-19T16:24:24Z", "author": {"login": "steveloughran"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java", "diffHunk": "@@ -15,86 +15,113 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n+\n package org.apache.hadoop.fs.sftp;\n \n+import java.io.EOFException;\n import java.io.IOException;\n import java.io.InputStream;\n+import java.io.UncheckedIOException;\n \n+import com.jcraft.jsch.ChannelSftp;\n+import com.jcraft.jsch.SftpATTRS;\n+import com.jcraft.jsch.SftpException;\n+import org.apache.hadoop.fs.FSExceptionMessages;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "86bdff3d89be5324a213654947a2ba7c4fd6891b"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQzNjUxNQ==", "bodyText": "sure, thank you", "url": "https://github.com/apache/hadoop/pull/1999#discussion_r427436515", "createdAt": "2020-05-19T16:28:36Z", "author": {"login": "mpryahin"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java", "diffHunk": "@@ -15,86 +15,113 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n+\n package org.apache.hadoop.fs.sftp;\n \n+import java.io.EOFException;\n import java.io.IOException;\n import java.io.InputStream;\n+import java.io.UncheckedIOException;\n \n+import com.jcraft.jsch.ChannelSftp;\n+import com.jcraft.jsch.SftpATTRS;\n+import com.jcraft.jsch.SftpException;\n+import org.apache.hadoop.fs.FSExceptionMessages;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQzMzY2NA=="}, "originalCommit": {"oid": "86bdff3d89be5324a213654947a2ba7c4fd6891b"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MjA3NjA5OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNjoyNTowOFrOGXogug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNzoxMzo0N1rOGXqbQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQzNDE3MA==", "bodyText": "why the double wrap?", "url": "https://github.com/apache/hadoop/pull/1999#discussion_r427434170", "createdAt": "2020-05-19T16:25:08Z", "author": {"login": "steveloughran"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java", "diffHunk": "@@ -15,86 +15,113 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n+\n package org.apache.hadoop.fs.sftp;\n \n+import java.io.EOFException;\n import java.io.IOException;\n import java.io.InputStream;\n+import java.io.UncheckedIOException;\n \n+import com.jcraft.jsch.ChannelSftp;\n+import com.jcraft.jsch.SftpATTRS;\n+import com.jcraft.jsch.SftpException;\n+import org.apache.hadoop.fs.FSExceptionMessages;\n import org.apache.hadoop.fs.FSInputStream;\n import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n \n /** SFTP FileSystem input stream. */\n class SFTPInputStream extends FSInputStream {\n \n-  public static final String E_SEEK_NOTSUPPORTED = \"Seek not supported\";\n-  public static final String E_NULL_INPUTSTREAM = \"Null InputStream\";\n-  public static final String E_STREAM_CLOSED = \"Stream closed\";\n-\n+  private final ChannelSftp channel;\n+  private final Path path;\n   private InputStream wrappedStream;\n   private FileSystem.Statistics stats;\n   private boolean closed;\n   private long pos;\n-\n-  SFTPInputStream(InputStream stream,  FileSystem.Statistics stats) {\n-\n-    if (stream == null) {\n-      throw new IllegalArgumentException(E_NULL_INPUTSTREAM);\n+  private long nextPos;\n+  private long contentLength;\n+\n+  SFTPInputStream(ChannelSftp channel, Path path, FileSystem.Statistics stats) {\n+    try {\n+      this.channel = channel;\n+      this.path = path;\n+      this.stats = stats;\n+      this.wrappedStream = channel.get(path.toUri().getPath());\n+      SftpATTRS stat = channel.lstat(path.toString());\n+      this.contentLength = stat.getSize();\n+    } catch (SftpException e) {\n+      throw new UncheckedIOException(new IOException(e));\n     }\n-    this.wrappedStream = stream;\n-    this.stats = stats;\n+  }\n \n-    this.pos = 0;\n-    this.closed = false;\n+  @Override\n+  public synchronized void seek(long position) throws IOException {\n+    checkNotClosed();\n+    if (position < 0) {\n+      throw new EOFException(FSExceptionMessages.NEGATIVE_SEEK);\n+    }\n+    nextPos = position;\n   }\n \n   @Override\n-  public void seek(long position) throws IOException {\n-    throw new IOException(E_SEEK_NOTSUPPORTED);\n+  public synchronized int available() throws IOException {\n+    checkNotClosed();\n+    long remaining = contentLength - nextPos;\n+    if (remaining > Integer.MAX_VALUE) {\n+      return Integer.MAX_VALUE;\n+    }\n+    return (int) remaining;\n+  }\n+\n+  private void seekInternal() throws IOException {\n+    if (pos == nextPos) {\n+      return;\n+    }\n+    if (nextPos > pos) {\n+      long skipped = wrappedStream.skip(nextPos - pos);\n+      pos = pos + skipped;\n+    }\n+    if (nextPos < pos) {\n+      wrappedStream.close();\n+      try {\n+        wrappedStream = channel.get(path.toUri().getPath());\n+        pos = wrappedStream.skip(nextPos);\n+      } catch (SftpException e) {\n+        throw new UncheckedIOException(new IOException(e));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "86bdff3d89be5324a213654947a2ba7c4fd6891b"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ2NTUzOA==", "bodyText": "my bad, i'll unwrap it. Thank you!", "url": "https://github.com/apache/hadoop/pull/1999#discussion_r427465538", "createdAt": "2020-05-19T17:13:47Z", "author": {"login": "mpryahin"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java", "diffHunk": "@@ -15,86 +15,113 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n+\n package org.apache.hadoop.fs.sftp;\n \n+import java.io.EOFException;\n import java.io.IOException;\n import java.io.InputStream;\n+import java.io.UncheckedIOException;\n \n+import com.jcraft.jsch.ChannelSftp;\n+import com.jcraft.jsch.SftpATTRS;\n+import com.jcraft.jsch.SftpException;\n+import org.apache.hadoop.fs.FSExceptionMessages;\n import org.apache.hadoop.fs.FSInputStream;\n import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n \n /** SFTP FileSystem input stream. */\n class SFTPInputStream extends FSInputStream {\n \n-  public static final String E_SEEK_NOTSUPPORTED = \"Seek not supported\";\n-  public static final String E_NULL_INPUTSTREAM = \"Null InputStream\";\n-  public static final String E_STREAM_CLOSED = \"Stream closed\";\n-\n+  private final ChannelSftp channel;\n+  private final Path path;\n   private InputStream wrappedStream;\n   private FileSystem.Statistics stats;\n   private boolean closed;\n   private long pos;\n-\n-  SFTPInputStream(InputStream stream,  FileSystem.Statistics stats) {\n-\n-    if (stream == null) {\n-      throw new IllegalArgumentException(E_NULL_INPUTSTREAM);\n+  private long nextPos;\n+  private long contentLength;\n+\n+  SFTPInputStream(ChannelSftp channel, Path path, FileSystem.Statistics stats) {\n+    try {\n+      this.channel = channel;\n+      this.path = path;\n+      this.stats = stats;\n+      this.wrappedStream = channel.get(path.toUri().getPath());\n+      SftpATTRS stat = channel.lstat(path.toString());\n+      this.contentLength = stat.getSize();\n+    } catch (SftpException e) {\n+      throw new UncheckedIOException(new IOException(e));\n     }\n-    this.wrappedStream = stream;\n-    this.stats = stats;\n+  }\n \n-    this.pos = 0;\n-    this.closed = false;\n+  @Override\n+  public synchronized void seek(long position) throws IOException {\n+    checkNotClosed();\n+    if (position < 0) {\n+      throw new EOFException(FSExceptionMessages.NEGATIVE_SEEK);\n+    }\n+    nextPos = position;\n   }\n \n   @Override\n-  public void seek(long position) throws IOException {\n-    throw new IOException(E_SEEK_NOTSUPPORTED);\n+  public synchronized int available() throws IOException {\n+    checkNotClosed();\n+    long remaining = contentLength - nextPos;\n+    if (remaining > Integer.MAX_VALUE) {\n+      return Integer.MAX_VALUE;\n+    }\n+    return (int) remaining;\n+  }\n+\n+  private void seekInternal() throws IOException {\n+    if (pos == nextPos) {\n+      return;\n+    }\n+    if (nextPos > pos) {\n+      long skipped = wrappedStream.skip(nextPos - pos);\n+      pos = pos + skipped;\n+    }\n+    if (nextPos < pos) {\n+      wrappedStream.close();\n+      try {\n+        wrappedStream = channel.get(path.toUri().getPath());\n+        pos = wrappedStream.skip(nextPos);\n+      } catch (SftpException e) {\n+        throw new UncheckedIOException(new IOException(e));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQzNDE3MA=="}, "originalCommit": {"oid": "86bdff3d89be5324a213654947a2ba7c4fd6891b"}, "originalPosition": 93}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MjA5MDE3OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNjoyODozMFrOGXopow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNzoxMTo0OFrOGXqWnQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQzNjQ1MQ==", "bodyText": "why the double wrap?", "url": "https://github.com/apache/hadoop/pull/1999#discussion_r427436451", "createdAt": "2020-05-19T16:28:30Z", "author": {"login": "steveloughran"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java", "diffHunk": "@@ -15,86 +15,113 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n+\n package org.apache.hadoop.fs.sftp;\n \n+import java.io.EOFException;\n import java.io.IOException;\n import java.io.InputStream;\n+import java.io.UncheckedIOException;\n \n+import com.jcraft.jsch.ChannelSftp;\n+import com.jcraft.jsch.SftpATTRS;\n+import com.jcraft.jsch.SftpException;\n+import org.apache.hadoop.fs.FSExceptionMessages;\n import org.apache.hadoop.fs.FSInputStream;\n import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n \n /** SFTP FileSystem input stream. */\n class SFTPInputStream extends FSInputStream {\n \n-  public static final String E_SEEK_NOTSUPPORTED = \"Seek not supported\";\n-  public static final String E_NULL_INPUTSTREAM = \"Null InputStream\";\n-  public static final String E_STREAM_CLOSED = \"Stream closed\";\n-\n+  private final ChannelSftp channel;\n+  private final Path path;\n   private InputStream wrappedStream;\n   private FileSystem.Statistics stats;\n   private boolean closed;\n   private long pos;\n-\n-  SFTPInputStream(InputStream stream,  FileSystem.Statistics stats) {\n-\n-    if (stream == null) {\n-      throw new IllegalArgumentException(E_NULL_INPUTSTREAM);\n+  private long nextPos;\n+  private long contentLength;\n+\n+  SFTPInputStream(ChannelSftp channel, Path path, FileSystem.Statistics stats) {\n+    try {\n+      this.channel = channel;\n+      this.path = path;\n+      this.stats = stats;\n+      this.wrappedStream = channel.get(path.toUri().getPath());\n+      SftpATTRS stat = channel.lstat(path.toString());\n+      this.contentLength = stat.getSize();\n+    } catch (SftpException e) {\n+      throw new UncheckedIOException(new IOException(e));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "86bdff3d89be5324a213654947a2ba7c4fd6891b"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ2NDM0OQ==", "bodyText": "I tried to keep the contract unchanged but it seems I dropped the ball:\nPreviously an inputstream instance was created in the SFTPFileSystem#open(Path f, int bufferSize) method which threw IOExpetion in case of failure and its clients might deeply rely on this behaviour. Now as an underlying inputstream creation was moved from\nSFTPFileSystem into FSDataInputStream constructor I should've added IOExpetion to the FSDataInputStream constructor signature to keep things unchanged and backward compatible. Thanks a lot for pointing out.", "url": "https://github.com/apache/hadoop/pull/1999#discussion_r427464349", "createdAt": "2020-05-19T17:11:48Z", "author": {"login": "mpryahin"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java", "diffHunk": "@@ -15,86 +15,113 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n+\n package org.apache.hadoop.fs.sftp;\n \n+import java.io.EOFException;\n import java.io.IOException;\n import java.io.InputStream;\n+import java.io.UncheckedIOException;\n \n+import com.jcraft.jsch.ChannelSftp;\n+import com.jcraft.jsch.SftpATTRS;\n+import com.jcraft.jsch.SftpException;\n+import org.apache.hadoop.fs.FSExceptionMessages;\n import org.apache.hadoop.fs.FSInputStream;\n import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n \n /** SFTP FileSystem input stream. */\n class SFTPInputStream extends FSInputStream {\n \n-  public static final String E_SEEK_NOTSUPPORTED = \"Seek not supported\";\n-  public static final String E_NULL_INPUTSTREAM = \"Null InputStream\";\n-  public static final String E_STREAM_CLOSED = \"Stream closed\";\n-\n+  private final ChannelSftp channel;\n+  private final Path path;\n   private InputStream wrappedStream;\n   private FileSystem.Statistics stats;\n   private boolean closed;\n   private long pos;\n-\n-  SFTPInputStream(InputStream stream,  FileSystem.Statistics stats) {\n-\n-    if (stream == null) {\n-      throw new IllegalArgumentException(E_NULL_INPUTSTREAM);\n+  private long nextPos;\n+  private long contentLength;\n+\n+  SFTPInputStream(ChannelSftp channel, Path path, FileSystem.Statistics stats) {\n+    try {\n+      this.channel = channel;\n+      this.path = path;\n+      this.stats = stats;\n+      this.wrappedStream = channel.get(path.toUri().getPath());\n+      SftpATTRS stat = channel.lstat(path.toString());\n+      this.contentLength = stat.getSize();\n+    } catch (SftpException e) {\n+      throw new UncheckedIOException(new IOException(e));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQzNjQ1MQ=="}, "originalCommit": {"oid": "86bdff3d89be5324a213654947a2ba7c4fd6891b"}, "originalPosition": 50}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MjA5NzgzOnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/sftp/SFTPContract.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNjozMDoxNlrOGXouXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQyMTozMzoyM1rOGXzcLg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQzNzY2MA==", "bodyText": "shouldn't this be set in core-default.xml already?\nif not, sftp:// urls would break. (yes, i know every stack overflow spark example does this, but that is just superstition)", "url": "https://github.com/apache/hadoop/pull/1999#discussion_r427437660", "createdAt": "2020-05-19T16:30:16Z", "author": {"login": "steveloughran"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/sftp/SFTPContract.java", "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *       http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.contract.sftp;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.contract.AbstractFSContract;\n+import org.apache.hadoop.fs.sftp.SFTPFileSystem;\n+import org.apache.sshd.common.NamedFactory;\n+import org.apache.sshd.server.SshServer;\n+import org.apache.sshd.server.auth.UserAuth;\n+import org.apache.sshd.server.auth.password.UserAuthPasswordFactory;\n+import org.apache.sshd.server.keyprovider.SimpleGeneratorHostKeyProvider;\n+import org.apache.sshd.server.subsystem.sftp.SftpSubsystemFactory;\n+\n+public class SFTPContract extends AbstractFSContract {\n+\n+  private String testDataDir = new FileSystemTestHelper().getTestRootDir();\n+  private Configuration conf;\n+  public static final String CONTRACT_XML = \"contract/sftp.xml\";\n+  private SshServer sshd;\n+\n+  public SFTPContract(Configuration conf) {\n+    super(conf);\n+    addConfResource(CONTRACT_XML);\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void init() throws IOException {\n+    sshd = SshServer.setUpDefaultServer();\n+    // ask OS to assign a port\n+    sshd.setPort(0);\n+    sshd.setKeyPairProvider(new SimpleGeneratorHostKeyProvider());\n+\n+    List<NamedFactory<UserAuth>> userAuthFactories = new ArrayList<>();\n+    userAuthFactories.add(new UserAuthPasswordFactory());\n+\n+    sshd.setUserAuthFactories(userAuthFactories);\n+    sshd.setPasswordAuthenticator((username, password, session) ->\n+        username.equals(\"user\") && password.equals(\"password\")\n+    );\n+\n+    sshd.setSubsystemFactories(\n+        Collections.singletonList(new SftpSubsystemFactory()));\n+\n+    sshd.start();\n+    int port = sshd.getPort();\n+\n+    conf.setClass(\"fs.sftp.impl\", SFTPFileSystem.class, FileSystem.class);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "86bdff3d89be5324a213654947a2ba7c4fd6891b"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ3NTY0OQ==", "bodyText": "it's not set in core-default.xml, and if not specified here sftp urls won't be resolved by sftp schema. Could you please clarify a bit what exactly you mean here? Thank you!", "url": "https://github.com/apache/hadoop/pull/1999#discussion_r427475649", "createdAt": "2020-05-19T17:29:38Z", "author": {"login": "mpryahin"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/sftp/SFTPContract.java", "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *       http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.contract.sftp;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.contract.AbstractFSContract;\n+import org.apache.hadoop.fs.sftp.SFTPFileSystem;\n+import org.apache.sshd.common.NamedFactory;\n+import org.apache.sshd.server.SshServer;\n+import org.apache.sshd.server.auth.UserAuth;\n+import org.apache.sshd.server.auth.password.UserAuthPasswordFactory;\n+import org.apache.sshd.server.keyprovider.SimpleGeneratorHostKeyProvider;\n+import org.apache.sshd.server.subsystem.sftp.SftpSubsystemFactory;\n+\n+public class SFTPContract extends AbstractFSContract {\n+\n+  private String testDataDir = new FileSystemTestHelper().getTestRootDir();\n+  private Configuration conf;\n+  public static final String CONTRACT_XML = \"contract/sftp.xml\";\n+  private SshServer sshd;\n+\n+  public SFTPContract(Configuration conf) {\n+    super(conf);\n+    addConfResource(CONTRACT_XML);\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void init() throws IOException {\n+    sshd = SshServer.setUpDefaultServer();\n+    // ask OS to assign a port\n+    sshd.setPort(0);\n+    sshd.setKeyPairProvider(new SimpleGeneratorHostKeyProvider());\n+\n+    List<NamedFactory<UserAuth>> userAuthFactories = new ArrayList<>();\n+    userAuthFactories.add(new UserAuthPasswordFactory());\n+\n+    sshd.setUserAuthFactories(userAuthFactories);\n+    sshd.setPasswordAuthenticator((username, password, session) ->\n+        username.equals(\"user\") && password.equals(\"password\")\n+    );\n+\n+    sshd.setSubsystemFactories(\n+        Collections.singletonList(new SftpSubsystemFactory()));\n+\n+    sshd.start();\n+    int port = sshd.getPort();\n+\n+    conf.setClass(\"fs.sftp.impl\", SFTPFileSystem.class, FileSystem.class);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQzNzY2MA=="}, "originalCommit": {"oid": "86bdff3d89be5324a213654947a2ba7c4fd6891b"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYxMzIzMA==", "bodyText": "double checked, it's quite strange. Is it a candidate for an improvement issue?", "url": "https://github.com/apache/hadoop/pull/1999#discussion_r427613230", "createdAt": "2020-05-19T21:33:23Z", "author": {"login": "mpryahin"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/sftp/SFTPContract.java", "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *       http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.contract.sftp;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.contract.AbstractFSContract;\n+import org.apache.hadoop.fs.sftp.SFTPFileSystem;\n+import org.apache.sshd.common.NamedFactory;\n+import org.apache.sshd.server.SshServer;\n+import org.apache.sshd.server.auth.UserAuth;\n+import org.apache.sshd.server.auth.password.UserAuthPasswordFactory;\n+import org.apache.sshd.server.keyprovider.SimpleGeneratorHostKeyProvider;\n+import org.apache.sshd.server.subsystem.sftp.SftpSubsystemFactory;\n+\n+public class SFTPContract extends AbstractFSContract {\n+\n+  private String testDataDir = new FileSystemTestHelper().getTestRootDir();\n+  private Configuration conf;\n+  public static final String CONTRACT_XML = \"contract/sftp.xml\";\n+  private SshServer sshd;\n+\n+  public SFTPContract(Configuration conf) {\n+    super(conf);\n+    addConfResource(CONTRACT_XML);\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void init() throws IOException {\n+    sshd = SshServer.setUpDefaultServer();\n+    // ask OS to assign a port\n+    sshd.setPort(0);\n+    sshd.setKeyPairProvider(new SimpleGeneratorHostKeyProvider());\n+\n+    List<NamedFactory<UserAuth>> userAuthFactories = new ArrayList<>();\n+    userAuthFactories.add(new UserAuthPasswordFactory());\n+\n+    sshd.setUserAuthFactories(userAuthFactories);\n+    sshd.setPasswordAuthenticator((username, password, session) ->\n+        username.equals(\"user\") && password.equals(\"password\")\n+    );\n+\n+    sshd.setSubsystemFactories(\n+        Collections.singletonList(new SftpSubsystemFactory()));\n+\n+    sshd.start();\n+    int port = sshd.getPort();\n+\n+    conf.setClass(\"fs.sftp.impl\", SFTPFileSystem.class, FileSystem.class);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQzNzY2MA=="}, "originalCommit": {"oid": "86bdff3d89be5324a213654947a2ba7c4fd6891b"}, "originalPosition": 75}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MjEwMTE0OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/sftp/SFTPContract.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNjozMDo1OVrOGXowWw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNzoyNTowMVrOGXq2wg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQzODE3MQ==", "bodyText": "nit: make a variable and reuse", "url": "https://github.com/apache/hadoop/pull/1999#discussion_r427438171", "createdAt": "2020-05-19T16:30:59Z", "author": {"login": "steveloughran"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/sftp/SFTPContract.java", "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *       http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.contract.sftp;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.contract.AbstractFSContract;\n+import org.apache.hadoop.fs.sftp.SFTPFileSystem;\n+import org.apache.sshd.common.NamedFactory;\n+import org.apache.sshd.server.SshServer;\n+import org.apache.sshd.server.auth.UserAuth;\n+import org.apache.sshd.server.auth.password.UserAuthPasswordFactory;\n+import org.apache.sshd.server.keyprovider.SimpleGeneratorHostKeyProvider;\n+import org.apache.sshd.server.subsystem.sftp.SftpSubsystemFactory;\n+\n+public class SFTPContract extends AbstractFSContract {\n+\n+  private String testDataDir = new FileSystemTestHelper().getTestRootDir();\n+  private Configuration conf;\n+  public static final String CONTRACT_XML = \"contract/sftp.xml\";\n+  private SshServer sshd;\n+\n+  public SFTPContract(Configuration conf) {\n+    super(conf);\n+    addConfResource(CONTRACT_XML);\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void init() throws IOException {\n+    sshd = SshServer.setUpDefaultServer();\n+    // ask OS to assign a port\n+    sshd.setPort(0);\n+    sshd.setKeyPairProvider(new SimpleGeneratorHostKeyProvider());\n+\n+    List<NamedFactory<UserAuth>> userAuthFactories = new ArrayList<>();\n+    userAuthFactories.add(new UserAuthPasswordFactory());\n+\n+    sshd.setUserAuthFactories(userAuthFactories);\n+    sshd.setPasswordAuthenticator((username, password, session) ->\n+        username.equals(\"user\") && password.equals(\"password\")\n+    );\n+\n+    sshd.setSubsystemFactories(\n+        Collections.singletonList(new SftpSubsystemFactory()));\n+\n+    sshd.start();\n+    int port = sshd.getPort();\n+\n+    conf.setClass(\"fs.sftp.impl\", SFTPFileSystem.class, FileSystem.class);\n+    conf.setInt(\"fs.sftp.host.port\", port);\n+    conf.setBoolean(\"fs.sftp.impl.disable.cache\", true);\n+  }\n+\n+  @Override\n+  public void teardown() throws IOException {\n+    if (sshd != null) {\n+      sshd.stop();\n+    }\n+  }\n+\n+  @Override\n+  public FileSystem getTestFileSystem() throws IOException {\n+    return FileSystem.get(URI.create(\"sftp://user:password@localhost\"), conf);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "86bdff3d89be5324a213654947a2ba7c4fd6891b"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ3MjU3OA==", "bodyText": "will do, thank you!", "url": "https://github.com/apache/hadoop/pull/1999#discussion_r427472578", "createdAt": "2020-05-19T17:25:01Z", "author": {"login": "mpryahin"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/sftp/SFTPContract.java", "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *       http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.contract.sftp;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.contract.AbstractFSContract;\n+import org.apache.hadoop.fs.sftp.SFTPFileSystem;\n+import org.apache.sshd.common.NamedFactory;\n+import org.apache.sshd.server.SshServer;\n+import org.apache.sshd.server.auth.UserAuth;\n+import org.apache.sshd.server.auth.password.UserAuthPasswordFactory;\n+import org.apache.sshd.server.keyprovider.SimpleGeneratorHostKeyProvider;\n+import org.apache.sshd.server.subsystem.sftp.SftpSubsystemFactory;\n+\n+public class SFTPContract extends AbstractFSContract {\n+\n+  private String testDataDir = new FileSystemTestHelper().getTestRootDir();\n+  private Configuration conf;\n+  public static final String CONTRACT_XML = \"contract/sftp.xml\";\n+  private SshServer sshd;\n+\n+  public SFTPContract(Configuration conf) {\n+    super(conf);\n+    addConfResource(CONTRACT_XML);\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void init() throws IOException {\n+    sshd = SshServer.setUpDefaultServer();\n+    // ask OS to assign a port\n+    sshd.setPort(0);\n+    sshd.setKeyPairProvider(new SimpleGeneratorHostKeyProvider());\n+\n+    List<NamedFactory<UserAuth>> userAuthFactories = new ArrayList<>();\n+    userAuthFactories.add(new UserAuthPasswordFactory());\n+\n+    sshd.setUserAuthFactories(userAuthFactories);\n+    sshd.setPasswordAuthenticator((username, password, session) ->\n+        username.equals(\"user\") && password.equals(\"password\")\n+    );\n+\n+    sshd.setSubsystemFactories(\n+        Collections.singletonList(new SftpSubsystemFactory()));\n+\n+    sshd.start();\n+    int port = sshd.getPort();\n+\n+    conf.setClass(\"fs.sftp.impl\", SFTPFileSystem.class, FileSystem.class);\n+    conf.setInt(\"fs.sftp.host.port\", port);\n+    conf.setBoolean(\"fs.sftp.impl.disable.cache\", true);\n+  }\n+\n+  @Override\n+  public void teardown() throws IOException {\n+    if (sshd != null) {\n+      sshd.stop();\n+    }\n+  }\n+\n+  @Override\n+  public FileSystem getTestFileSystem() throws IOException {\n+    return FileSystem.get(URI.create(\"sftp://user:password@localhost\"), conf);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQzODE3MQ=="}, "originalCommit": {"oid": "86bdff3d89be5324a213654947a2ba7c4fd6891b"}, "originalPosition": 89}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MjExNTAwOnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/sftp/SFTPContract.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNjozMzo0NlrOGXo4sA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNzozOTozMFrOGXrcAg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ0MDMwNA==", "bodyText": "shame we didn't declare this as raising an ioe; probably too late now", "url": "https://github.com/apache/hadoop/pull/1999#discussion_r427440304", "createdAt": "2020-05-19T16:33:46Z", "author": {"login": "steveloughran"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/sftp/SFTPContract.java", "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *       http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.contract.sftp;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.contract.AbstractFSContract;\n+import org.apache.hadoop.fs.sftp.SFTPFileSystem;\n+import org.apache.sshd.common.NamedFactory;\n+import org.apache.sshd.server.SshServer;\n+import org.apache.sshd.server.auth.UserAuth;\n+import org.apache.sshd.server.auth.password.UserAuthPasswordFactory;\n+import org.apache.sshd.server.keyprovider.SimpleGeneratorHostKeyProvider;\n+import org.apache.sshd.server.subsystem.sftp.SftpSubsystemFactory;\n+\n+public class SFTPContract extends AbstractFSContract {\n+\n+  private String testDataDir = new FileSystemTestHelper().getTestRootDir();\n+  private Configuration conf;\n+  public static final String CONTRACT_XML = \"contract/sftp.xml\";\n+  private SshServer sshd;\n+\n+  public SFTPContract(Configuration conf) {\n+    super(conf);\n+    addConfResource(CONTRACT_XML);\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void init() throws IOException {\n+    sshd = SshServer.setUpDefaultServer();\n+    // ask OS to assign a port\n+    sshd.setPort(0);\n+    sshd.setKeyPairProvider(new SimpleGeneratorHostKeyProvider());\n+\n+    List<NamedFactory<UserAuth>> userAuthFactories = new ArrayList<>();\n+    userAuthFactories.add(new UserAuthPasswordFactory());\n+\n+    sshd.setUserAuthFactories(userAuthFactories);\n+    sshd.setPasswordAuthenticator((username, password, session) ->\n+        username.equals(\"user\") && password.equals(\"password\")\n+    );\n+\n+    sshd.setSubsystemFactories(\n+        Collections.singletonList(new SftpSubsystemFactory()));\n+\n+    sshd.start();\n+    int port = sshd.getPort();\n+\n+    conf.setClass(\"fs.sftp.impl\", SFTPFileSystem.class, FileSystem.class);\n+    conf.setInt(\"fs.sftp.host.port\", port);\n+    conf.setBoolean(\"fs.sftp.impl.disable.cache\", true);\n+  }\n+\n+  @Override\n+  public void teardown() throws IOException {\n+    if (sshd != null) {\n+      sshd.stop();\n+    }\n+  }\n+\n+  @Override\n+  public FileSystem getTestFileSystem() throws IOException {\n+    return FileSystem.get(URI.create(\"sftp://user:password@localhost\"), conf);\n+  }\n+\n+  @Override\n+  public String getScheme() {\n+    return \"sftp\";\n+  }\n+\n+  @Override\n+  public Path getTestPath() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "86bdff3d89be5324a213654947a2ba7c4fd6891b"}, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ4MjExNA==", "bodyText": "it's not too late I suppose, would you like me to try to declare it as throwing IOE? I can issue a new Jira and fix it there.", "url": "https://github.com/apache/hadoop/pull/1999#discussion_r427482114", "createdAt": "2020-05-19T17:39:30Z", "author": {"login": "mpryahin"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/sftp/SFTPContract.java", "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *       http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.contract.sftp;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.contract.AbstractFSContract;\n+import org.apache.hadoop.fs.sftp.SFTPFileSystem;\n+import org.apache.sshd.common.NamedFactory;\n+import org.apache.sshd.server.SshServer;\n+import org.apache.sshd.server.auth.UserAuth;\n+import org.apache.sshd.server.auth.password.UserAuthPasswordFactory;\n+import org.apache.sshd.server.keyprovider.SimpleGeneratorHostKeyProvider;\n+import org.apache.sshd.server.subsystem.sftp.SftpSubsystemFactory;\n+\n+public class SFTPContract extends AbstractFSContract {\n+\n+  private String testDataDir = new FileSystemTestHelper().getTestRootDir();\n+  private Configuration conf;\n+  public static final String CONTRACT_XML = \"contract/sftp.xml\";\n+  private SshServer sshd;\n+\n+  public SFTPContract(Configuration conf) {\n+    super(conf);\n+    addConfResource(CONTRACT_XML);\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void init() throws IOException {\n+    sshd = SshServer.setUpDefaultServer();\n+    // ask OS to assign a port\n+    sshd.setPort(0);\n+    sshd.setKeyPairProvider(new SimpleGeneratorHostKeyProvider());\n+\n+    List<NamedFactory<UserAuth>> userAuthFactories = new ArrayList<>();\n+    userAuthFactories.add(new UserAuthPasswordFactory());\n+\n+    sshd.setUserAuthFactories(userAuthFactories);\n+    sshd.setPasswordAuthenticator((username, password, session) ->\n+        username.equals(\"user\") && password.equals(\"password\")\n+    );\n+\n+    sshd.setSubsystemFactories(\n+        Collections.singletonList(new SftpSubsystemFactory()));\n+\n+    sshd.start();\n+    int port = sshd.getPort();\n+\n+    conf.setClass(\"fs.sftp.impl\", SFTPFileSystem.class, FileSystem.class);\n+    conf.setInt(\"fs.sftp.host.port\", port);\n+    conf.setBoolean(\"fs.sftp.impl.disable.cache\", true);\n+  }\n+\n+  @Override\n+  public void teardown() throws IOException {\n+    if (sshd != null) {\n+      sshd.stop();\n+    }\n+  }\n+\n+  @Override\n+  public FileSystem getTestFileSystem() throws IOException {\n+    return FileSystem.get(URI.create(\"sftp://user:password@localhost\"), conf);\n+  }\n+\n+  @Override\n+  public String getScheme() {\n+    return \"sftp\";\n+  }\n+\n+  @Override\n+  public Path getTestPath() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ0MDMwNA=="}, "originalCommit": {"oid": "86bdff3d89be5324a213654947a2ba7c4fd6891b"}, "originalPosition": 98}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MjExNjQ3OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/test/resources/contract/sftp.xml", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNjozNDowOFrOGXo5lA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNzoyNToyN1rOGXq36A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ0MDUzMg==", "bodyText": "nit: sftp", "url": "https://github.com/apache/hadoop/pull/1999#discussion_r427440532", "createdAt": "2020-05-19T16:34:08Z", "author": {"login": "steveloughran"}, "path": "hadoop-common-project/hadoop-common/src/test/resources/contract/sftp.xml", "diffHunk": "@@ -0,0 +1,79 @@\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one\n+  ~  or more contributor license agreements.  See the NOTICE file\n+  ~  distributed with this work for additional information\n+  ~  regarding copyright ownership.  The ASF licenses this file\n+  ~  to you under the Apache License, Version 2.0 (the\n+  ~  \"License\"); you may not use this file except in compliance\n+  ~  with the License.  You may obtain a copy of the License at\n+  ~\n+  ~       http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~  Unless required by applicable law or agreed to in writing, software\n+  ~  distributed under the License is distributed on an \"AS IS\" BASIS,\n+  ~  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  ~  See the License for the specific language governing permissions and\n+  ~  limitations under the License.\n+  -->\n+\n+<configuration>\n+  <!--\n+  FTP -these options are for testing against a remote unix filesystem.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "86bdff3d89be5324a213654947a2ba7c4fd6891b"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ3Mjg3Mg==", "bodyText": "yep, thanks a lot.", "url": "https://github.com/apache/hadoop/pull/1999#discussion_r427472872", "createdAt": "2020-05-19T17:25:27Z", "author": {"login": "mpryahin"}, "path": "hadoop-common-project/hadoop-common/src/test/resources/contract/sftp.xml", "diffHunk": "@@ -0,0 +1,79 @@\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one\n+  ~  or more contributor license agreements.  See the NOTICE file\n+  ~  distributed with this work for additional information\n+  ~  regarding copyright ownership.  The ASF licenses this file\n+  ~  to you under the Apache License, Version 2.0 (the\n+  ~  \"License\"); you may not use this file except in compliance\n+  ~  with the License.  You may obtain a copy of the License at\n+  ~\n+  ~       http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~  Unless required by applicable law or agreed to in writing, software\n+  ~  distributed under the License is distributed on an \"AS IS\" BASIS,\n+  ~  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  ~  See the License for the specific language governing permissions and\n+  ~  limitations under the License.\n+  -->\n+\n+<configuration>\n+  <!--\n+  FTP -these options are for testing against a remote unix filesystem.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ0MDUzMg=="}, "originalCommit": {"oid": "86bdff3d89be5324a213654947a2ba7c4fd6891b"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MjEyNDM3OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNjozNjowN1rOGXo-mg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNzoxODozOFrOGXqm_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ0MTgxOA==", "bodyText": "wrap super.close() with a try/finally so the channel is always diconnected", "url": "https://github.com/apache/hadoop/pull/1999#discussion_r427441818", "createdAt": "2020-05-19T16:36:07Z", "author": {"login": "steveloughran"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java", "diffHunk": "@@ -516,16 +515,14 @@ public FSDataInputStream open(Path f, int bufferSize) throws IOException {\n       disconnect(channel);\n       throw new IOException(String.format(E_PATH_DIR, f));\n     }\n-    InputStream is;\n     try {\n       // the path could be a symbolic link, so get the real path\n       absolute = new Path(\"/\", channel.realpath(absolute.toUri().getPath()));\n-\n-      is = channel.get(absolute.toUri().getPath());\n     } catch (SftpException e) {\n       throw new IOException(e);\n     }\n-    return new FSDataInputStream(new SFTPInputStream(is, statistics)){\n+    return new FSDataInputStream(\n+        new SFTPInputStream(channel, absolute, statistics)){\n       @Override\n       public void close() throws IOException {\n         super.close();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "86bdff3d89be5324a213654947a2ba7c4fd6891b"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ2ODU0MA==", "bodyText": "thanks a lot will do", "url": "https://github.com/apache/hadoop/pull/1999#discussion_r427468540", "createdAt": "2020-05-19T17:18:38Z", "author": {"login": "mpryahin"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java", "diffHunk": "@@ -516,16 +515,14 @@ public FSDataInputStream open(Path f, int bufferSize) throws IOException {\n       disconnect(channel);\n       throw new IOException(String.format(E_PATH_DIR, f));\n     }\n-    InputStream is;\n     try {\n       // the path could be a symbolic link, so get the real path\n       absolute = new Path(\"/\", channel.realpath(absolute.toUri().getPath()));\n-\n-      is = channel.get(absolute.toUri().getPath());\n     } catch (SftpException e) {\n       throw new IOException(e);\n     }\n-    return new FSDataInputStream(new SFTPInputStream(is, statistics)){\n+    return new FSDataInputStream(\n+        new SFTPInputStream(channel, absolute, statistics)){\n       @Override\n       public void close() throws IOException {\n         super.close();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ0MTgxOA=="}, "originalCommit": {"oid": "86bdff3d89be5324a213654947a2ba7c4fd6891b"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5OTc4Mjg1OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/AbstractFSContract.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMVQxODozODoyN1rOGdVqWg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxMzo1MTo1M1rOGdyoPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzQxNjc5NA==", "bodyText": "add . so javadoc is happy", "url": "https://github.com/apache/hadoop/pull/1999#discussion_r433416794", "createdAt": "2020-06-01T18:38:27Z", "author": {"login": "steveloughran"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/AbstractFSContract.java", "diffHunk": "@@ -69,6 +69,14 @@ public void init() throws IOException {\n \n   }\n \n+  /**\n+   * Any teardown logic can go here", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b874cdf5b10d61184b6602147aa6a9ae1e6f9929"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzg5MTM4OQ==", "bodyText": "fixed, thank you.", "url": "https://github.com/apache/hadoop/pull/1999#discussion_r433891389", "createdAt": "2020-06-02T13:51:53Z", "author": {"login": "mpryahin"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/AbstractFSContract.java", "diffHunk": "@@ -69,6 +69,14 @@ public void init() throws IOException {\n \n   }\n \n+  /**\n+   * Any teardown logic can go here", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzQxNjc5NA=="}, "originalCommit": {"oid": "b874cdf5b10d61184b6602147aa6a9ae1e6f9929"}, "originalPosition": 5}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3549, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}