{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzg5NjE4ODMx", "number": 1898, "reviewThreads": {"totalCount": 17, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QxNjowMzoxNlrODoyQZA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wOC0xMVQwNzoyNjo0N1rOGgwDhA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ0MDkzMDI4OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QxNjowMzoxNlrOF3jF2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxMDo1MzoyNVrOF4pP7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc5MDkzOQ==", "bodyText": "Can you use logger format?\nLOG.trace(\"Trigger client.read for path={} position={} offset={} length={}\", path, position, offset, length);", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r393790939", "createdAt": "2020-03-17T16:03:16Z", "author": {"login": "goiri"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -234,6 +242,7 @@ int readRemote(long position, byte[] b, int offset, int length) throws IOExcepti\n     final AbfsRestOperation op;\n     AbfsPerfTracker tracker = client.getAbfsPerfTracker();\n     try (AbfsPerfInfo perfInfo = new AbfsPerfInfo(tracker, \"readRemote\", \"read\")) {\n+      LOG.trace(String.format(\"Trigger client.read for path=%s position=%s offset=%s length=%s\", path, position, offset, length));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e29d124a0e41d1553e1c7b58117f8921da6e424"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk0MDM5OQ==", "bodyText": "Done", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r394940399", "createdAt": "2020-03-19T10:53:25Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -234,6 +242,7 @@ int readRemote(long position, byte[] b, int offset, int length) throws IOExcepti\n     final AbfsRestOperation op;\n     AbfsPerfTracker tracker = client.getAbfsPerfTracker();\n     try (AbfsPerfInfo perfInfo = new AbfsPerfInfo(tracker, \"readRemote\", \"read\")) {\n+      LOG.trace(String.format(\"Trigger client.read for path=%s position=%s offset=%s length=%s\", path, position, offset, length));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc5MDkzOQ=="}, "originalCommit": {"oid": "5e29d124a0e41d1553e1c7b58117f8921da6e424"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ0MDkzMzc4OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QxNjowNDowNVrOF3jIMQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxMDo1NzowOFrOF4pX2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc5MTUzNw==", "bodyText": "Avoid.", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r393791537", "createdAt": "2020-03-17T16:04:05Z", "author": {"login": "goiri"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -101,6 +107,7 @@ void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, fi\n       if (isAlreadyQueued(stream, requestedOffset)) {\n         return; // already queued, do not queue again\n       }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e29d124a0e41d1553e1c7b58117f8921da6e424"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk0MjQyNg==", "bodyText": "Done", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r394942426", "createdAt": "2020-03-19T10:57:08Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -101,6 +107,7 @@ void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, fi\n       if (isAlreadyQueued(stream, requestedOffset)) {\n         return; // already queued, do not queue again\n       }\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc5MTUzNw=="}, "originalCommit": {"oid": "5e29d124a0e41d1553e1c7b58117f8921da6e424"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ0MDkzNDM1OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QxNjowNDoxM1rOF3jIiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxMDo1NDowNFrOF4pRgg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc5MTYyNQ==", "bodyText": "80 char limit", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r393791625", "createdAt": "2020-03-17T16:04:13Z", "author": {"login": "goiri"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -90,8 +94,10 @@ private ReadBufferManager() {\n    * @param stream          The {@link AbfsInputStream} for which to do the read-ahead\n    * @param requestedOffset The offset in the file which shoukd be read\n    * @param requestedLength The length to read\n+   * @param queueReadAheadRequestId unique queue request ID\n    */\n-  void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, final int requestedLength) {\n+  void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, final int requestedLength", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e29d124a0e41d1553e1c7b58117f8921da6e424"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk0MDgwMg==", "bodyText": "Undid any change to method signature as part of the review comments to remove queueRequestId.", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r394940802", "createdAt": "2020-03-19T10:54:04Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -90,8 +94,10 @@ private ReadBufferManager() {\n    * @param stream          The {@link AbfsInputStream} for which to do the read-ahead\n    * @param requestedOffset The offset in the file which shoukd be read\n    * @param requestedLength The length to read\n+   * @param queueReadAheadRequestId unique queue request ID\n    */\n-  void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, final int requestedLength) {\n+  void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, final int requestedLength", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc5MTYyNQ=="}, "originalCommit": {"oid": "5e29d124a0e41d1553e1c7b58117f8921da6e424"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ0MDk0MTEzOnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QxNjowNTo0OVrOF3jM6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxODozMzoxMlrOGCQ1qQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc5Mjc0NQ==", "bodyText": "What about capturing logs and checking for the messages?", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r393792745", "createdAt": "2020-03-17T16:05:49Z", "author": {"login": "goiri"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "diffHunk": "@@ -0,0 +1,438 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.io.IOException;\n+import java.util.UUID;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.TimeoutException;\n+\n+import static java.util.UUID.randomUUID;\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.Mockito.doReturn;\n+import static org.mockito.Mockito.doThrow;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+import static org.apache.hadoop.test.LambdaTestUtils.intercept;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.FORWARD_SLASH;\n+\n+/**\n+ * Unit test AbfsInputStream.\n+ */\n+public class TestAbfsInputStream extends", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e29d124a0e41d1553e1c7b58117f8921da6e424"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk0MjAwMg==", "bodyText": "If you meant capturing the exception messages from failed read ahead threads, that will not be possible as we can not predict which stub hit which of the parallely running read ahead threads. Hence asserting on the exception and no message.", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r394942002", "createdAt": "2020-03-19T10:56:17Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "diffHunk": "@@ -0,0 +1,438 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.io.IOException;\n+import java.util.UUID;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.TimeoutException;\n+\n+import static java.util.UUID.randomUUID;\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.Mockito.doReturn;\n+import static org.mockito.Mockito.doThrow;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+import static org.apache.hadoop.test.LambdaTestUtils.intercept;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.FORWARD_SLASH;\n+\n+/**\n+ * Unit test AbfsInputStream.\n+ */\n+public class TestAbfsInputStream extends", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc5Mjc0NQ=="}, "originalCommit": {"oid": "5e29d124a0e41d1553e1c7b58117f8921da6e424"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTEyODYyOQ==", "bodyText": "I was referring to check using LogCapturer", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r395128629", "createdAt": "2020-03-19T15:48:20Z", "author": {"login": "goiri"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "diffHunk": "@@ -0,0 +1,438 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.io.IOException;\n+import java.util.UUID;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.TimeoutException;\n+\n+import static java.util.UUID.randomUUID;\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.Mockito.doReturn;\n+import static org.mockito.Mockito.doThrow;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+import static org.apache.hadoop.test.LambdaTestUtils.intercept;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.FORWARD_SLASH;\n+\n+/**\n+ * Unit test AbfsInputStream.\n+ */\n+public class TestAbfsInputStream extends", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc5Mjc0NQ=="}, "originalCommit": {"oid": "5e29d124a0e41d1553e1c7b58117f8921da6e424"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAyNjIxNw==", "bodyText": "that's always been trouble in the past; major source of maintenance pain in the wasb tests. Better use the assertion", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r405026217", "createdAt": "2020-04-07T18:33:12Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "diffHunk": "@@ -0,0 +1,438 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.io.IOException;\n+import java.util.UUID;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.TimeoutException;\n+\n+import static java.util.UUID.randomUUID;\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.Mockito.doReturn;\n+import static org.mockito.Mockito.doThrow;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+import static org.apache.hadoop.test.LambdaTestUtils.intercept;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.FORWARD_SLASH;\n+\n+/**\n+ * Unit test AbfsInputStream.\n+ */\n+public class TestAbfsInputStream extends", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc5Mjc0NQ=="}, "originalCommit": {"oid": "5e29d124a0e41d1553e1c7b58117f8921da6e424"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ0MDk0NjcxOnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QxNjowNjo1MVrOF3jQOw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxMDo1Njo0OFrOF4pXLQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc5MzU5NQ==", "bodyText": "new param", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r393793595", "createdAt": "2020-03-17T16:06:51Z", "author": {"login": "goiri"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -141,7 +149,8 @@ void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, fi\n    * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n    * @return the number of bytes read", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e29d124a0e41d1553e1c7b58117f8921da6e424"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk0MjI1Mw==", "bodyText": "New param undone as part of latest iteration.", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r394942253", "createdAt": "2020-03-19T10:56:48Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -141,7 +149,8 @@ void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, fi\n    * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n    * @return the number of bytes read", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc5MzU5NQ=="}, "originalCommit": {"oid": "5e29d124a0e41d1553e1c7b58117f8921da6e424"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ0OTEyOTg5OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxNTo0ODozOFrOF40wEQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxODo0NDo1MVrOGCRRnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTEyODg0OQ==", "bodyText": "These parenthesis are confusing.", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r395128849", "createdAt": "2020-03-19T15:48:38Z", "author": {"login": "goiri"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -299,11 +327,32 @@ private void clearFromReadAheadQueue(final AbfsInputStream stream, final long re\n   }\n \n   private int getBlockFromCompletedQueue(final AbfsInputStream stream, final long position, final int length,\n-                                         final byte[] buffer) {\n-    ReadBuffer buf = getFromList(completedReadList, stream, position);\n-    if (buf == null || position >= buf.getOffset() + buf.getLength()) {\n+                                         final byte[] buffer) throws IOException {\n+    ReadBuffer buf = getBufferFromCompletedQueue(stream, position);\n+\n+    if (buf == null) {\n       return 0;\n     }\n+\n+    if (buf.getStatus() == ReadBufferStatus.READ_FAILED) {\n+      // Eviction of a read buffer is triggered only when a queue request comes in\n+      // and each eviction attempt tries to find one eligible buffer.\n+      // Hence there are chances that an old read-ahead buffer with exception is still\n+      // available. To prevent new read requests to fail due to such old buffers,\n+      // return exception only from buffers that failed within last THRESHOLD_AGE_MILLISECONDS\n+      if ((currentTimeMillis() - (buf.getTimeStamp()) < THRESHOLD_AGE_MILLISECONDS)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1cb20dbff9e353d7cd8d4bf400721a205ab3e234"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjIzMzIxNQ==", "bodyText": "If the buffer was updated with an error in last \"Threshold_age_milliseconds\", then we return the error to client.\nCan you please help me with the cause of confusion.", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r396233215", "createdAt": "2020-03-23T06:25:12Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -299,11 +327,32 @@ private void clearFromReadAheadQueue(final AbfsInputStream stream, final long re\n   }\n \n   private int getBlockFromCompletedQueue(final AbfsInputStream stream, final long position, final int length,\n-                                         final byte[] buffer) {\n-    ReadBuffer buf = getFromList(completedReadList, stream, position);\n-    if (buf == null || position >= buf.getOffset() + buf.getLength()) {\n+                                         final byte[] buffer) throws IOException {\n+    ReadBuffer buf = getBufferFromCompletedQueue(stream, position);\n+\n+    if (buf == null) {\n       return 0;\n     }\n+\n+    if (buf.getStatus() == ReadBufferStatus.READ_FAILED) {\n+      // Eviction of a read buffer is triggered only when a queue request comes in\n+      // and each eviction attempt tries to find one eligible buffer.\n+      // Hence there are chances that an old read-ahead buffer with exception is still\n+      // available. To prevent new read requests to fail due to such old buffers,\n+      // return exception only from buffers that failed within last THRESHOLD_AGE_MILLISECONDS\n+      if ((currentTimeMillis() - (buf.getTimeStamp()) < THRESHOLD_AGE_MILLISECONDS)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTEyODg0OQ=="}, "originalCommit": {"oid": "1cb20dbff9e353d7cd8d4bf400721a205ab3e234"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTcyNTYwOA==", "bodyText": "If this is to avoid the failure for new requests, then instead of checking the time window, why not  reset the buffer status before throwing the exception? Then following new requests will not be affected by the old failure.", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r399725608", "createdAt": "2020-03-29T00:14:50Z", "author": {"login": "DadanielZ"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -299,11 +327,32 @@ private void clearFromReadAheadQueue(final AbfsInputStream stream, final long re\n   }\n \n   private int getBlockFromCompletedQueue(final AbfsInputStream stream, final long position, final int length,\n-                                         final byte[] buffer) {\n-    ReadBuffer buf = getFromList(completedReadList, stream, position);\n-    if (buf == null || position >= buf.getOffset() + buf.getLength()) {\n+                                         final byte[] buffer) throws IOException {\n+    ReadBuffer buf = getBufferFromCompletedQueue(stream, position);\n+\n+    if (buf == null) {\n       return 0;\n     }\n+\n+    if (buf.getStatus() == ReadBufferStatus.READ_FAILED) {\n+      // Eviction of a read buffer is triggered only when a queue request comes in\n+      // and each eviction attempt tries to find one eligible buffer.\n+      // Hence there are chances that an old read-ahead buffer with exception is still\n+      // available. To prevent new read requests to fail due to such old buffers,\n+      // return exception only from buffers that failed within last THRESHOLD_AGE_MILLISECONDS\n+      if ((currentTimeMillis() - (buf.getTimeStamp()) < THRESHOLD_AGE_MILLISECONDS)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTEyODg0OQ=="}, "originalCommit": {"oid": "1cb20dbff9e353d7cd8d4bf400721a205ab3e234"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDExMjEyMg==", "bodyText": "Aim here is to enforce the read-ahead failure for the threshold time duration (which currently is 30 sec), i.e. any read request for that offset that can be served by the ReadBuffer needs to fail.", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r400112122", "createdAt": "2020-03-30T11:16:02Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -299,11 +327,32 @@ private void clearFromReadAheadQueue(final AbfsInputStream stream, final long re\n   }\n \n   private int getBlockFromCompletedQueue(final AbfsInputStream stream, final long position, final int length,\n-                                         final byte[] buffer) {\n-    ReadBuffer buf = getFromList(completedReadList, stream, position);\n-    if (buf == null || position >= buf.getOffset() + buf.getLength()) {\n+                                         final byte[] buffer) throws IOException {\n+    ReadBuffer buf = getBufferFromCompletedQueue(stream, position);\n+\n+    if (buf == null) {\n       return 0;\n     }\n+\n+    if (buf.getStatus() == ReadBufferStatus.READ_FAILED) {\n+      // Eviction of a read buffer is triggered only when a queue request comes in\n+      // and each eviction attempt tries to find one eligible buffer.\n+      // Hence there are chances that an old read-ahead buffer with exception is still\n+      // available. To prevent new read requests to fail due to such old buffers,\n+      // return exception only from buffers that failed within last THRESHOLD_AGE_MILLISECONDS\n+      if ((currentTimeMillis() - (buf.getTimeStamp()) < THRESHOLD_AGE_MILLISECONDS)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTEyODg0OQ=="}, "originalCommit": {"oid": "1cb20dbff9e353d7cd8d4bf400721a205ab3e234"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAzMzM3Mg==", "bodyText": "why 30s? Any way to tune this for test runs?", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r405033372", "createdAt": "2020-04-07T18:44:51Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -299,11 +327,32 @@ private void clearFromReadAheadQueue(final AbfsInputStream stream, final long re\n   }\n \n   private int getBlockFromCompletedQueue(final AbfsInputStream stream, final long position, final int length,\n-                                         final byte[] buffer) {\n-    ReadBuffer buf = getFromList(completedReadList, stream, position);\n-    if (buf == null || position >= buf.getOffset() + buf.getLength()) {\n+                                         final byte[] buffer) throws IOException {\n+    ReadBuffer buf = getBufferFromCompletedQueue(stream, position);\n+\n+    if (buf == null) {\n       return 0;\n     }\n+\n+    if (buf.getStatus() == ReadBufferStatus.READ_FAILED) {\n+      // Eviction of a read buffer is triggered only when a queue request comes in\n+      // and each eviction attempt tries to find one eligible buffer.\n+      // Hence there are chances that an old read-ahead buffer with exception is still\n+      // available. To prevent new read requests to fail due to such old buffers,\n+      // return exception only from buffers that failed within last THRESHOLD_AGE_MILLISECONDS\n+      if ((currentTimeMillis() - (buf.getTimeStamp()) < THRESHOLD_AGE_MILLISECONDS)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTEyODg0OQ=="}, "originalCommit": {"oid": "1cb20dbff9e353d7cd8d4bf400721a205ab3e234"}, "originalPosition": 86}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ3ODQ0MDkzOnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yOFQyMzo0ODo0OVrOF9NMWg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxMToxOTozMVrOF9lBSQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTcyMzYxMA==", "bodyText": "why use java.io.IOException but not IOException?", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r399723610", "createdAt": "2020-03-28T23:48:49Z", "author": {"login": "DadanielZ"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -141,7 +144,8 @@ void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, fi\n    * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n    * @return the number of bytes read\n    */\n-  int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer) {\n+  int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer)\n+      throws java.io.IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4ba43efe00c9d00d9543493ddfb502e02a58dd69"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDExMzk5Mw==", "bodyText": "Fixed", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r400113993", "createdAt": "2020-03-30T11:19:31Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -141,7 +144,8 @@ void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, fi\n    * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n    * @return the number of bytes read\n    */\n-  int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer) {\n+  int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer)\n+      throws java.io.IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTcyMzYxMA=="}, "originalCommit": {"oid": "4ba43efe00c9d00d9543493ddfb502e02a58dd69"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ3ODQ0MTk0OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBuffer.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yOFQyMzo1MDoyNFrOF9NMyw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxMToxOTo0MVrOF9lBrg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTcyMzcyMw==", "bodyText": "IOException", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r399723723", "createdAt": "2020-03-28T23:50:24Z", "author": {"login": "DadanielZ"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBuffer.java", "diffHunk": "@@ -88,12 +92,23 @@ public void setBufferindex(int bufferindex) {\n     this.bufferindex = bufferindex;\n   }\n \n+  public IOException getErrException() {\n+    return errException;\n+  }\n+\n+  public void setErrException(final java.io.IOException errException) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4ba43efe00c9d00d9543493ddfb502e02a58dd69"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDExNDA5NA==", "bodyText": "Fixed", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r400114094", "createdAt": "2020-03-30T11:19:41Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBuffer.java", "diffHunk": "@@ -88,12 +92,23 @@ public void setBufferindex(int bufferindex) {\n     this.bufferindex = bufferindex;\n   }\n \n+  public IOException getErrException() {\n+    return errException;\n+  }\n+\n+  public void setErrException(final java.io.IOException errException) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTcyMzcyMw=="}, "originalCommit": {"oid": "4ba43efe00c9d00d9543493ddfb502e02a58dd69"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMzMxNTA5OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBuffer.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxODoyOToyOVrOGCQs7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxODoyOToyOVrOGCQs7g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAyMzk4Mg==", "bodyText": "add space above this line", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r405023982", "createdAt": "2020-04-07T18:29:29Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBuffer.java", "diffHunk": "@@ -17,11 +17,13 @@\n  */\n \n package org.apache.hadoop.fs.azurebfs.services;\n-\n+import java.io.IOException;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52fba323269e6272849b89824dc372398ead4894"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMzMzNzQ2OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxODozNToxOFrOGCQ6mQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxODozNToxOFrOGCQ6mQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAyNzQ4MQ==", "bodyText": "not needed; just cut it", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r405027481", "createdAt": "2020-04-07T18:35:18Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "diffHunk": "@@ -0,0 +1,433 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.io.IOException;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.TimeoutException;\n+\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.Mockito.doReturn;\n+import static org.mockito.Mockito.doThrow;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+import static org.apache.hadoop.test.LambdaTestUtils.intercept;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.FORWARD_SLASH;\n+\n+/**\n+ * Unit test AbfsInputStream.\n+ */\n+public class TestAbfsInputStream extends\n+    AbstractAbfsIntegrationTest {\n+\n+  private static final int KILOBYTE = 1024;\n+\n+  private AbfsRestOperation getMockRestOp() {\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsHttpOperation httpOp = mock(AbfsHttpOperation.class);\n+    when(httpOp.getBytesReceived()).thenReturn(1024L);\n+    when(op.getResult()).thenReturn(httpOp);\n+    return op;\n+  }\n+\n+  private AbfsClient getMockAbfsClient() {\n+    // Mock failure for client.read()\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\n+        \"test\",\n+        this.getAccountName(),\n+        this.getConfiguration());\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+\n+    return client;\n+  }\n+\n+  private AbfsInputStream getAbfsInputStream(AbfsClient mockAbfsClient, String fileName) {\n+    // Create AbfsInputStream with the client instance\n+    AbfsInputStream inputStream = new AbfsInputStream(\n+        mockAbfsClient,\n+        null,\n+        FORWARD_SLASH + fileName,\n+        3 * KILOBYTE,\n+        1 * KILOBYTE, // Setting read ahead buffer size of 1 KB\n+        this.getConfiguration().getReadAheadQueueDepth(),\n+        this.getConfiguration().getTolerateOobAppends(),\n+        \"eTag\");\n+\n+    return inputStream;\n+  }\n+\n+  private void queueReadAheads(AbfsInputStream inputStream) {\n+    // Mimic AbfsInputStream readAhead queue requests\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 0, 1 * KILOBYTE);\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 1 * KILOBYTE, 1 * KILOBYTE);\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 2 * KILOBYTE, 1 * KILOBYTE);\n+  }\n+\n+  private void verifyReadCallCount(AbfsClient client, int count) throws\n+      AzureBlobFileSystemException, InterruptedException {\n+    // ReadAhead threads are triggered asynchronously.\n+    // Wait a second before verifying the number of total calls.\n+    Thread.sleep(1000);\n+    verify(client, times(count)).read(any(String.class), any(Long.class),\n+        any(byte[].class), any(Integer.class), any(Integer.class),\n+        any(String.class));\n+  }\n+\n+  private void checkEvictedStatus(AbfsInputStream inputStream, int position, boolean expectedToThrowException)\n+      throws Exception {\n+    // Sleep for the eviction threshold time\n+    Thread.sleep(ReadBufferManager.getBufferManager().getThresholdAgeMilliseconds() + 1000);\n+\n+    // Eviction is done only when AbfsInputStream tries to queue new items.\n+    // 1 tryEvict will remove 1 eligible item. To ensure that the current test buffer\n+    // will get evicted (considering there could be other tests running in parallel),\n+    // call tryEvict for the number of items that are there in completedReadList.\n+    int numOfCompletedReadListItems = ReadBufferManager.getBufferManager().getCompletedReadListSize();\n+    while (numOfCompletedReadListItems > 0) {\n+      ReadBufferManager.getBufferManager().callTryEvict();\n+      numOfCompletedReadListItems--;\n+    }\n+\n+    if (expectedToThrowException) {\n+      intercept(IOException.class,\n+          () -> inputStream.read(position, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE));\n+    } else {\n+      inputStream.read(position, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE);\n+    }\n+  }\n+\n+  public TestAbfsInputStream() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52fba323269e6272849b89824dc372398ead4894"}, "originalPosition": 126}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMzM0NTAxOnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxODozNzoyMVrOGCQ_fQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMjo1MjoyOVrOGXe78A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAyODczMw==", "bodyText": "is there any way for test runs to avoid these long sleeps? This might add 30s to the test run, and that adds up over the day.", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r405028733", "createdAt": "2020-04-07T18:37:21Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "diffHunk": "@@ -0,0 +1,433 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.io.IOException;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.TimeoutException;\n+\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.Mockito.doReturn;\n+import static org.mockito.Mockito.doThrow;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+import static org.apache.hadoop.test.LambdaTestUtils.intercept;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.FORWARD_SLASH;\n+\n+/**\n+ * Unit test AbfsInputStream.\n+ */\n+public class TestAbfsInputStream extends\n+    AbstractAbfsIntegrationTest {\n+\n+  private static final int KILOBYTE = 1024;\n+\n+  private AbfsRestOperation getMockRestOp() {\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsHttpOperation httpOp = mock(AbfsHttpOperation.class);\n+    when(httpOp.getBytesReceived()).thenReturn(1024L);\n+    when(op.getResult()).thenReturn(httpOp);\n+    return op;\n+  }\n+\n+  private AbfsClient getMockAbfsClient() {\n+    // Mock failure for client.read()\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\n+        \"test\",\n+        this.getAccountName(),\n+        this.getConfiguration());\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+\n+    return client;\n+  }\n+\n+  private AbfsInputStream getAbfsInputStream(AbfsClient mockAbfsClient, String fileName) {\n+    // Create AbfsInputStream with the client instance\n+    AbfsInputStream inputStream = new AbfsInputStream(\n+        mockAbfsClient,\n+        null,\n+        FORWARD_SLASH + fileName,\n+        3 * KILOBYTE,\n+        1 * KILOBYTE, // Setting read ahead buffer size of 1 KB\n+        this.getConfiguration().getReadAheadQueueDepth(),\n+        this.getConfiguration().getTolerateOobAppends(),\n+        \"eTag\");\n+\n+    return inputStream;\n+  }\n+\n+  private void queueReadAheads(AbfsInputStream inputStream) {\n+    // Mimic AbfsInputStream readAhead queue requests\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 0, 1 * KILOBYTE);\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 1 * KILOBYTE, 1 * KILOBYTE);\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 2 * KILOBYTE, 1 * KILOBYTE);\n+  }\n+\n+  private void verifyReadCallCount(AbfsClient client, int count) throws\n+      AzureBlobFileSystemException, InterruptedException {\n+    // ReadAhead threads are triggered asynchronously.\n+    // Wait a second before verifying the number of total calls.\n+    Thread.sleep(1000);\n+    verify(client, times(count)).read(any(String.class), any(Long.class),\n+        any(byte[].class), any(Integer.class), any(Integer.class),\n+        any(String.class));\n+  }\n+\n+  private void checkEvictedStatus(AbfsInputStream inputStream, int position, boolean expectedToThrowException)\n+      throws Exception {\n+    // Sleep for the eviction threshold time\n+    Thread.sleep(ReadBufferManager.getBufferManager().getThresholdAgeMilliseconds() + 1000);\n+\n+    // Eviction is done only when AbfsInputStream tries to queue new items.\n+    // 1 tryEvict will remove 1 eligible item. To ensure that the current test buffer\n+    // will get evicted (considering there could be other tests running in parallel),\n+    // call tryEvict for the number of items that are there in completedReadList.\n+    int numOfCompletedReadListItems = ReadBufferManager.getBufferManager().getCompletedReadListSize();\n+    while (numOfCompletedReadListItems > 0) {\n+      ReadBufferManager.getBufferManager().callTryEvict();\n+      numOfCompletedReadListItems--;\n+    }\n+\n+    if (expectedToThrowException) {\n+      intercept(IOException.class,\n+          () -> inputStream.read(position, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE));\n+    } else {\n+      inputStream.read(position, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE);\n+    }\n+  }\n+\n+  public TestAbfsInputStream() throws Exception {\n+    super();\n+  }\n+\n+  /**\n+   * This test expects AbfsInputStream to throw the exception that readAhead\n+   * thread received on read. The readAhead thread must be initiated from the\n+   * active read request itself.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testFailedReadAhead() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // Actual read request fails with the failure in readahead thread\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error RAH-Thread-Z\"))\n+        .doReturn(successOp) // Any extra calls to read, pass it.\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testFailedReadAhead.txt\");\n+\n+    // Scenario: ReadAhead triggered from current active read call failed\n+    // Before the change to return exception from readahead buffer,\n+    // AbfsInputStream would have triggered an extra readremote on noticing\n+    // data absent in readahead buffers\n+    // In this test, a read should trigger 3 client.read() calls as file is 3 KB\n+    // and readahead buffer size set in AbfsInputStream is 1 KB\n+    // There should only be a total of 3 client.read() in this test.\n+    intercept(IOException.class,\n+        () -> inputStream.read(new byte[1 * KILOBYTE]));\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Stub returns success for the 4th read request, if ReadBuffers still\n+    // persisted, ReadAheadManager getBlock would have returned exception.\n+    checkEvictedStatus(inputStream, 0, false);\n+  }\n+\n+  /**\n+   * The test expects AbfsInputStream to initiate a remote read request for\n+   * the request offset and length when previous read ahead on the offset had failed.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testOlderReadAheadFailure() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // First Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // A second read request will see that readahead had failed for data in\n+    // the requested offset range and also that its is an older readahead request.\n+    // So attempt a new read only for the requested range.\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Z\"))\n+        .doReturn(successOp) // pass the read for second read request\n+        .doReturn(successOp) // pass success for post eviction test\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testOlderReadAheadFailure.txt\");\n+\n+    // First read request that fails as the readahead triggered from this request failed.\n+    intercept(IOException.class,\n+        () -> inputStream.read(new byte[1 * KILOBYTE]));\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Sleep for 30 sec so that the read ahead buffer qualifies for being old.\n+    Thread.sleep(ReadBufferManager.getBufferManager().getThresholdAgeMilliseconds());\n+\n+    // Second read request should retry the read (and not issue any new readaheads)\n+    inputStream.read(1 * KILOBYTE, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE);\n+\n+    // Once created, mock will remember all interactions. So total number of read\n+    // calls will be one more from earlier (there is a reset mock which will reset the\n+    // count, but the mock stub is erased as well which needs AbsInputStream to be recreated,\n+    // which beats the purpose)\n+    verifyReadCallCount(client, 4);\n+\n+    // Stub returns success for the 5th read request, if ReadBuffers still\n+    // persisted request would have failed for position 0.\n+    checkEvictedStatus(inputStream, 0, false);\n+  }\n+\n+  /**\n+   * The test expects AbfsInputStream to utilize any data read ahead for\n+   * requested offset and length.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testSuccessfulReadAhead() throws Exception {\n+    // Mock failure for client.read()\n+    AbfsClient client = getMockAbfsClient();\n+\n+    // Success operation mock\n+    AbfsRestOperation op = getMockRestOp();\n+\n+    // Stub :\n+    // Pass all readAheads and fail the post eviction request to\n+    // prove ReadAhead buffer is used\n+    // for post eviction check, fail all read aheads\n+    doReturn(op)\n+        .doReturn(op)\n+        .doReturn(op)\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Z\"))\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testSuccessfulReadAhead.txt\");\n+\n+    // First read request that triggers readAheads.\n+    inputStream.read(new byte[1 * KILOBYTE]);\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Another read request whose requested data is already read ahead.\n+    inputStream.read(1 * KILOBYTE, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE);\n+\n+    // Once created, mock will remember all interactions.\n+    // As the above read should not have triggered any server calls, total\n+    // number of read calls made at this point will be same as last.\n+    verifyReadCallCount(client, 3);\n+\n+    // Stub will throw exception for client.read() for 4th and later calls\n+    // if not using the read-ahead buffer exception will be thrown on read\n+    checkEvictedStatus(inputStream, 0, true);\n+  }\n+\n+  /**\n+   * This test expects ReadAheadManager to throw exception if the read ahead\n+   * thread had failed within the last 30 sec.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadAheadManagerForFailedReadAhead() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // Actual read request fails with the failure in readahead thread\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error RAH-Thread-Z\"))\n+        .doReturn(successOp) // Any extra calls to read, pass it.\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testReadAheadManagerForFailedReadAhead.txt\");\n+\n+    queueReadAheads(inputStream);\n+\n+    // AbfsInputStream Read would have waited for the read-ahead for the requested offset\n+    // as we are testing from ReadAheadManager directly, sleep for a sec to\n+    // get the read ahead threads to complete\n+    Thread.sleep(1000);\n+\n+    // if readAhead failed for specific offset, getBlock should\n+    // throw exception from the ReadBuffer that failed within last 30 sec\n+    intercept(IOException.class,\n+        () -> ReadBufferManager.getBufferManager().getBlock(\n+            inputStream,\n+            0,\n+            1 * KILOBYTE,\n+            new byte[1 * KILOBYTE]));\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Stub returns success for the 4th read request, if ReadBuffers still\n+    // persisted, ReadAheadManager getBlock would have returned exception.\n+    checkEvictedStatus(inputStream, 0, false);\n+  }\n+\n+  /**\n+   * The test expects ReadAheadManager to return 0 receivedBytes when previous\n+   * read ahead on the offset had failed and not throw exception received then.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadAheadManagerForOlderReadAheadFailure() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // First Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // A second read request will see that readahead had failed for data in\n+    // the requested offset range but also that its is an older readahead request.\n+    // System issue could have resolved by now, so attempt a new read only for the requested range.\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doReturn(successOp) // pass the read for second read request\n+        .doReturn(successOp) // pass success for post eviction test\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testReadAheadManagerForOlderReadAheadFailure.txt\");\n+\n+    queueReadAheads(inputStream);\n+\n+    // AbfsInputStream Read would have waited for the read-ahead for the requested offset\n+    // as we are testing from ReadAheadManager directly, sleep for 30 secs so that\n+    // read buffer qualifies for to be an old buffer\n+    Thread.sleep(ReadBufferManager.getBufferManager().getThresholdAgeMilliseconds());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52fba323269e6272849b89824dc372398ead4894"}, "originalPosition": 356}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI3NzI5Ng==", "bodyText": "Have reduced the sleep that the test will need.", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r427277296", "createdAt": "2020-05-19T12:52:29Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "diffHunk": "@@ -0,0 +1,433 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.io.IOException;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.TimeoutException;\n+\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.Mockito.doReturn;\n+import static org.mockito.Mockito.doThrow;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+import static org.apache.hadoop.test.LambdaTestUtils.intercept;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.FORWARD_SLASH;\n+\n+/**\n+ * Unit test AbfsInputStream.\n+ */\n+public class TestAbfsInputStream extends\n+    AbstractAbfsIntegrationTest {\n+\n+  private static final int KILOBYTE = 1024;\n+\n+  private AbfsRestOperation getMockRestOp() {\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsHttpOperation httpOp = mock(AbfsHttpOperation.class);\n+    when(httpOp.getBytesReceived()).thenReturn(1024L);\n+    when(op.getResult()).thenReturn(httpOp);\n+    return op;\n+  }\n+\n+  private AbfsClient getMockAbfsClient() {\n+    // Mock failure for client.read()\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\n+        \"test\",\n+        this.getAccountName(),\n+        this.getConfiguration());\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+\n+    return client;\n+  }\n+\n+  private AbfsInputStream getAbfsInputStream(AbfsClient mockAbfsClient, String fileName) {\n+    // Create AbfsInputStream with the client instance\n+    AbfsInputStream inputStream = new AbfsInputStream(\n+        mockAbfsClient,\n+        null,\n+        FORWARD_SLASH + fileName,\n+        3 * KILOBYTE,\n+        1 * KILOBYTE, // Setting read ahead buffer size of 1 KB\n+        this.getConfiguration().getReadAheadQueueDepth(),\n+        this.getConfiguration().getTolerateOobAppends(),\n+        \"eTag\");\n+\n+    return inputStream;\n+  }\n+\n+  private void queueReadAheads(AbfsInputStream inputStream) {\n+    // Mimic AbfsInputStream readAhead queue requests\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 0, 1 * KILOBYTE);\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 1 * KILOBYTE, 1 * KILOBYTE);\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 2 * KILOBYTE, 1 * KILOBYTE);\n+  }\n+\n+  private void verifyReadCallCount(AbfsClient client, int count) throws\n+      AzureBlobFileSystemException, InterruptedException {\n+    // ReadAhead threads are triggered asynchronously.\n+    // Wait a second before verifying the number of total calls.\n+    Thread.sleep(1000);\n+    verify(client, times(count)).read(any(String.class), any(Long.class),\n+        any(byte[].class), any(Integer.class), any(Integer.class),\n+        any(String.class));\n+  }\n+\n+  private void checkEvictedStatus(AbfsInputStream inputStream, int position, boolean expectedToThrowException)\n+      throws Exception {\n+    // Sleep for the eviction threshold time\n+    Thread.sleep(ReadBufferManager.getBufferManager().getThresholdAgeMilliseconds() + 1000);\n+\n+    // Eviction is done only when AbfsInputStream tries to queue new items.\n+    // 1 tryEvict will remove 1 eligible item. To ensure that the current test buffer\n+    // will get evicted (considering there could be other tests running in parallel),\n+    // call tryEvict for the number of items that are there in completedReadList.\n+    int numOfCompletedReadListItems = ReadBufferManager.getBufferManager().getCompletedReadListSize();\n+    while (numOfCompletedReadListItems > 0) {\n+      ReadBufferManager.getBufferManager().callTryEvict();\n+      numOfCompletedReadListItems--;\n+    }\n+\n+    if (expectedToThrowException) {\n+      intercept(IOException.class,\n+          () -> inputStream.read(position, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE));\n+    } else {\n+      inputStream.read(position, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE);\n+    }\n+  }\n+\n+  public TestAbfsInputStream() throws Exception {\n+    super();\n+  }\n+\n+  /**\n+   * This test expects AbfsInputStream to throw the exception that readAhead\n+   * thread received on read. The readAhead thread must be initiated from the\n+   * active read request itself.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testFailedReadAhead() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // Actual read request fails with the failure in readahead thread\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error RAH-Thread-Z\"))\n+        .doReturn(successOp) // Any extra calls to read, pass it.\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testFailedReadAhead.txt\");\n+\n+    // Scenario: ReadAhead triggered from current active read call failed\n+    // Before the change to return exception from readahead buffer,\n+    // AbfsInputStream would have triggered an extra readremote on noticing\n+    // data absent in readahead buffers\n+    // In this test, a read should trigger 3 client.read() calls as file is 3 KB\n+    // and readahead buffer size set in AbfsInputStream is 1 KB\n+    // There should only be a total of 3 client.read() in this test.\n+    intercept(IOException.class,\n+        () -> inputStream.read(new byte[1 * KILOBYTE]));\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Stub returns success for the 4th read request, if ReadBuffers still\n+    // persisted, ReadAheadManager getBlock would have returned exception.\n+    checkEvictedStatus(inputStream, 0, false);\n+  }\n+\n+  /**\n+   * The test expects AbfsInputStream to initiate a remote read request for\n+   * the request offset and length when previous read ahead on the offset had failed.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testOlderReadAheadFailure() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // First Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // A second read request will see that readahead had failed for data in\n+    // the requested offset range and also that its is an older readahead request.\n+    // So attempt a new read only for the requested range.\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Z\"))\n+        .doReturn(successOp) // pass the read for second read request\n+        .doReturn(successOp) // pass success for post eviction test\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testOlderReadAheadFailure.txt\");\n+\n+    // First read request that fails as the readahead triggered from this request failed.\n+    intercept(IOException.class,\n+        () -> inputStream.read(new byte[1 * KILOBYTE]));\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Sleep for 30 sec so that the read ahead buffer qualifies for being old.\n+    Thread.sleep(ReadBufferManager.getBufferManager().getThresholdAgeMilliseconds());\n+\n+    // Second read request should retry the read (and not issue any new readaheads)\n+    inputStream.read(1 * KILOBYTE, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE);\n+\n+    // Once created, mock will remember all interactions. So total number of read\n+    // calls will be one more from earlier (there is a reset mock which will reset the\n+    // count, but the mock stub is erased as well which needs AbsInputStream to be recreated,\n+    // which beats the purpose)\n+    verifyReadCallCount(client, 4);\n+\n+    // Stub returns success for the 5th read request, if ReadBuffers still\n+    // persisted request would have failed for position 0.\n+    checkEvictedStatus(inputStream, 0, false);\n+  }\n+\n+  /**\n+   * The test expects AbfsInputStream to utilize any data read ahead for\n+   * requested offset and length.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testSuccessfulReadAhead() throws Exception {\n+    // Mock failure for client.read()\n+    AbfsClient client = getMockAbfsClient();\n+\n+    // Success operation mock\n+    AbfsRestOperation op = getMockRestOp();\n+\n+    // Stub :\n+    // Pass all readAheads and fail the post eviction request to\n+    // prove ReadAhead buffer is used\n+    // for post eviction check, fail all read aheads\n+    doReturn(op)\n+        .doReturn(op)\n+        .doReturn(op)\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Z\"))\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testSuccessfulReadAhead.txt\");\n+\n+    // First read request that triggers readAheads.\n+    inputStream.read(new byte[1 * KILOBYTE]);\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Another read request whose requested data is already read ahead.\n+    inputStream.read(1 * KILOBYTE, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE);\n+\n+    // Once created, mock will remember all interactions.\n+    // As the above read should not have triggered any server calls, total\n+    // number of read calls made at this point will be same as last.\n+    verifyReadCallCount(client, 3);\n+\n+    // Stub will throw exception for client.read() for 4th and later calls\n+    // if not using the read-ahead buffer exception will be thrown on read\n+    checkEvictedStatus(inputStream, 0, true);\n+  }\n+\n+  /**\n+   * This test expects ReadAheadManager to throw exception if the read ahead\n+   * thread had failed within the last 30 sec.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadAheadManagerForFailedReadAhead() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // Actual read request fails with the failure in readahead thread\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error RAH-Thread-Z\"))\n+        .doReturn(successOp) // Any extra calls to read, pass it.\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testReadAheadManagerForFailedReadAhead.txt\");\n+\n+    queueReadAheads(inputStream);\n+\n+    // AbfsInputStream Read would have waited for the read-ahead for the requested offset\n+    // as we are testing from ReadAheadManager directly, sleep for a sec to\n+    // get the read ahead threads to complete\n+    Thread.sleep(1000);\n+\n+    // if readAhead failed for specific offset, getBlock should\n+    // throw exception from the ReadBuffer that failed within last 30 sec\n+    intercept(IOException.class,\n+        () -> ReadBufferManager.getBufferManager().getBlock(\n+            inputStream,\n+            0,\n+            1 * KILOBYTE,\n+            new byte[1 * KILOBYTE]));\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Stub returns success for the 4th read request, if ReadBuffers still\n+    // persisted, ReadAheadManager getBlock would have returned exception.\n+    checkEvictedStatus(inputStream, 0, false);\n+  }\n+\n+  /**\n+   * The test expects ReadAheadManager to return 0 receivedBytes when previous\n+   * read ahead on the offset had failed and not throw exception received then.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadAheadManagerForOlderReadAheadFailure() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // First Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // A second read request will see that readahead had failed for data in\n+    // the requested offset range but also that its is an older readahead request.\n+    // System issue could have resolved by now, so attempt a new read only for the requested range.\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doReturn(successOp) // pass the read for second read request\n+        .doReturn(successOp) // pass success for post eviction test\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testReadAheadManagerForOlderReadAheadFailure.txt\");\n+\n+    queueReadAheads(inputStream);\n+\n+    // AbfsInputStream Read would have waited for the read-ahead for the requested offset\n+    // as we are testing from ReadAheadManager directly, sleep for 30 secs so that\n+    // read buffer qualifies for to be an old buffer\n+    Thread.sleep(ReadBufferManager.getBufferManager().getThresholdAgeMilliseconds());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAyODczMw=="}, "originalCommit": {"oid": "52fba323269e6272849b89824dc372398ead4894"}, "originalPosition": 356}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMzM0NjU0OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxODozNzo0MlrOGCRAZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMjo1MTo1OVrOGXe6ow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAyODk2Ng==", "bodyText": "assertEquals for all equality checks", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r405028966", "createdAt": "2020-04-07T18:37:42Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "diffHunk": "@@ -0,0 +1,433 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.io.IOException;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.TimeoutException;\n+\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.Mockito.doReturn;\n+import static org.mockito.Mockito.doThrow;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+import static org.apache.hadoop.test.LambdaTestUtils.intercept;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.FORWARD_SLASH;\n+\n+/**\n+ * Unit test AbfsInputStream.\n+ */\n+public class TestAbfsInputStream extends\n+    AbstractAbfsIntegrationTest {\n+\n+  private static final int KILOBYTE = 1024;\n+\n+  private AbfsRestOperation getMockRestOp() {\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsHttpOperation httpOp = mock(AbfsHttpOperation.class);\n+    when(httpOp.getBytesReceived()).thenReturn(1024L);\n+    when(op.getResult()).thenReturn(httpOp);\n+    return op;\n+  }\n+\n+  private AbfsClient getMockAbfsClient() {\n+    // Mock failure for client.read()\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\n+        \"test\",\n+        this.getAccountName(),\n+        this.getConfiguration());\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+\n+    return client;\n+  }\n+\n+  private AbfsInputStream getAbfsInputStream(AbfsClient mockAbfsClient, String fileName) {\n+    // Create AbfsInputStream with the client instance\n+    AbfsInputStream inputStream = new AbfsInputStream(\n+        mockAbfsClient,\n+        null,\n+        FORWARD_SLASH + fileName,\n+        3 * KILOBYTE,\n+        1 * KILOBYTE, // Setting read ahead buffer size of 1 KB\n+        this.getConfiguration().getReadAheadQueueDepth(),\n+        this.getConfiguration().getTolerateOobAppends(),\n+        \"eTag\");\n+\n+    return inputStream;\n+  }\n+\n+  private void queueReadAheads(AbfsInputStream inputStream) {\n+    // Mimic AbfsInputStream readAhead queue requests\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 0, 1 * KILOBYTE);\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 1 * KILOBYTE, 1 * KILOBYTE);\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 2 * KILOBYTE, 1 * KILOBYTE);\n+  }\n+\n+  private void verifyReadCallCount(AbfsClient client, int count) throws\n+      AzureBlobFileSystemException, InterruptedException {\n+    // ReadAhead threads are triggered asynchronously.\n+    // Wait a second before verifying the number of total calls.\n+    Thread.sleep(1000);\n+    verify(client, times(count)).read(any(String.class), any(Long.class),\n+        any(byte[].class), any(Integer.class), any(Integer.class),\n+        any(String.class));\n+  }\n+\n+  private void checkEvictedStatus(AbfsInputStream inputStream, int position, boolean expectedToThrowException)\n+      throws Exception {\n+    // Sleep for the eviction threshold time\n+    Thread.sleep(ReadBufferManager.getBufferManager().getThresholdAgeMilliseconds() + 1000);\n+\n+    // Eviction is done only when AbfsInputStream tries to queue new items.\n+    // 1 tryEvict will remove 1 eligible item. To ensure that the current test buffer\n+    // will get evicted (considering there could be other tests running in parallel),\n+    // call tryEvict for the number of items that are there in completedReadList.\n+    int numOfCompletedReadListItems = ReadBufferManager.getBufferManager().getCompletedReadListSize();\n+    while (numOfCompletedReadListItems > 0) {\n+      ReadBufferManager.getBufferManager().callTryEvict();\n+      numOfCompletedReadListItems--;\n+    }\n+\n+    if (expectedToThrowException) {\n+      intercept(IOException.class,\n+          () -> inputStream.read(position, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE));\n+    } else {\n+      inputStream.read(position, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE);\n+    }\n+  }\n+\n+  public TestAbfsInputStream() throws Exception {\n+    super();\n+  }\n+\n+  /**\n+   * This test expects AbfsInputStream to throw the exception that readAhead\n+   * thread received on read. The readAhead thread must be initiated from the\n+   * active read request itself.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testFailedReadAhead() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // Actual read request fails with the failure in readahead thread\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error RAH-Thread-Z\"))\n+        .doReturn(successOp) // Any extra calls to read, pass it.\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testFailedReadAhead.txt\");\n+\n+    // Scenario: ReadAhead triggered from current active read call failed\n+    // Before the change to return exception from readahead buffer,\n+    // AbfsInputStream would have triggered an extra readremote on noticing\n+    // data absent in readahead buffers\n+    // In this test, a read should trigger 3 client.read() calls as file is 3 KB\n+    // and readahead buffer size set in AbfsInputStream is 1 KB\n+    // There should only be a total of 3 client.read() in this test.\n+    intercept(IOException.class,\n+        () -> inputStream.read(new byte[1 * KILOBYTE]));\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Stub returns success for the 4th read request, if ReadBuffers still\n+    // persisted, ReadAheadManager getBlock would have returned exception.\n+    checkEvictedStatus(inputStream, 0, false);\n+  }\n+\n+  /**\n+   * The test expects AbfsInputStream to initiate a remote read request for\n+   * the request offset and length when previous read ahead on the offset had failed.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testOlderReadAheadFailure() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // First Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // A second read request will see that readahead had failed for data in\n+    // the requested offset range and also that its is an older readahead request.\n+    // So attempt a new read only for the requested range.\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Z\"))\n+        .doReturn(successOp) // pass the read for second read request\n+        .doReturn(successOp) // pass success for post eviction test\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testOlderReadAheadFailure.txt\");\n+\n+    // First read request that fails as the readahead triggered from this request failed.\n+    intercept(IOException.class,\n+        () -> inputStream.read(new byte[1 * KILOBYTE]));\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Sleep for 30 sec so that the read ahead buffer qualifies for being old.\n+    Thread.sleep(ReadBufferManager.getBufferManager().getThresholdAgeMilliseconds());\n+\n+    // Second read request should retry the read (and not issue any new readaheads)\n+    inputStream.read(1 * KILOBYTE, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE);\n+\n+    // Once created, mock will remember all interactions. So total number of read\n+    // calls will be one more from earlier (there is a reset mock which will reset the\n+    // count, but the mock stub is erased as well which needs AbsInputStream to be recreated,\n+    // which beats the purpose)\n+    verifyReadCallCount(client, 4);\n+\n+    // Stub returns success for the 5th read request, if ReadBuffers still\n+    // persisted request would have failed for position 0.\n+    checkEvictedStatus(inputStream, 0, false);\n+  }\n+\n+  /**\n+   * The test expects AbfsInputStream to utilize any data read ahead for\n+   * requested offset and length.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testSuccessfulReadAhead() throws Exception {\n+    // Mock failure for client.read()\n+    AbfsClient client = getMockAbfsClient();\n+\n+    // Success operation mock\n+    AbfsRestOperation op = getMockRestOp();\n+\n+    // Stub :\n+    // Pass all readAheads and fail the post eviction request to\n+    // prove ReadAhead buffer is used\n+    // for post eviction check, fail all read aheads\n+    doReturn(op)\n+        .doReturn(op)\n+        .doReturn(op)\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Z\"))\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testSuccessfulReadAhead.txt\");\n+\n+    // First read request that triggers readAheads.\n+    inputStream.read(new byte[1 * KILOBYTE]);\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Another read request whose requested data is already read ahead.\n+    inputStream.read(1 * KILOBYTE, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE);\n+\n+    // Once created, mock will remember all interactions.\n+    // As the above read should not have triggered any server calls, total\n+    // number of read calls made at this point will be same as last.\n+    verifyReadCallCount(client, 3);\n+\n+    // Stub will throw exception for client.read() for 4th and later calls\n+    // if not using the read-ahead buffer exception will be thrown on read\n+    checkEvictedStatus(inputStream, 0, true);\n+  }\n+\n+  /**\n+   * This test expects ReadAheadManager to throw exception if the read ahead\n+   * thread had failed within the last 30 sec.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadAheadManagerForFailedReadAhead() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // Actual read request fails with the failure in readahead thread\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error RAH-Thread-Z\"))\n+        .doReturn(successOp) // Any extra calls to read, pass it.\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testReadAheadManagerForFailedReadAhead.txt\");\n+\n+    queueReadAheads(inputStream);\n+\n+    // AbfsInputStream Read would have waited for the read-ahead for the requested offset\n+    // as we are testing from ReadAheadManager directly, sleep for a sec to\n+    // get the read ahead threads to complete\n+    Thread.sleep(1000);\n+\n+    // if readAhead failed for specific offset, getBlock should\n+    // throw exception from the ReadBuffer that failed within last 30 sec\n+    intercept(IOException.class,\n+        () -> ReadBufferManager.getBufferManager().getBlock(\n+            inputStream,\n+            0,\n+            1 * KILOBYTE,\n+            new byte[1 * KILOBYTE]));\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Stub returns success for the 4th read request, if ReadBuffers still\n+    // persisted, ReadAheadManager getBlock would have returned exception.\n+    checkEvictedStatus(inputStream, 0, false);\n+  }\n+\n+  /**\n+   * The test expects ReadAheadManager to return 0 receivedBytes when previous\n+   * read ahead on the offset had failed and not throw exception received then.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadAheadManagerForOlderReadAheadFailure() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // First Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // A second read request will see that readahead had failed for data in\n+    // the requested offset range but also that its is an older readahead request.\n+    // System issue could have resolved by now, so attempt a new read only for the requested range.\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doReturn(successOp) // pass the read for second read request\n+        .doReturn(successOp) // pass success for post eviction test\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testReadAheadManagerForOlderReadAheadFailure.txt\");\n+\n+    queueReadAheads(inputStream);\n+\n+    // AbfsInputStream Read would have waited for the read-ahead for the requested offset\n+    // as we are testing from ReadAheadManager directly, sleep for 30 secs so that\n+    // read buffer qualifies for to be an old buffer\n+    Thread.sleep(ReadBufferManager.getBufferManager().getThresholdAgeMilliseconds());\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // getBlock from a new read request should return 0 if there is a failure\n+    // 30 sec before in read ahead buffer for respective offset.\n+    int bytesRead = ReadBufferManager.getBufferManager().getBlock(\n+        inputStream,\n+        1 * KILOBYTE,\n+        1 * KILOBYTE,\n+        new byte[1 * KILOBYTE]);\n+    Assert.assertTrue(\"bytesRead should be zero when previously read \"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52fba323269e6272849b89824dc372398ead4894"}, "originalPosition": 368}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI3Njk2Mw==", "bodyText": "Done", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r427276963", "createdAt": "2020-05-19T12:51:59Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "diffHunk": "@@ -0,0 +1,433 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.io.IOException;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.TimeoutException;\n+\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.Mockito.doReturn;\n+import static org.mockito.Mockito.doThrow;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+import static org.apache.hadoop.test.LambdaTestUtils.intercept;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.FORWARD_SLASH;\n+\n+/**\n+ * Unit test AbfsInputStream.\n+ */\n+public class TestAbfsInputStream extends\n+    AbstractAbfsIntegrationTest {\n+\n+  private static final int KILOBYTE = 1024;\n+\n+  private AbfsRestOperation getMockRestOp() {\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsHttpOperation httpOp = mock(AbfsHttpOperation.class);\n+    when(httpOp.getBytesReceived()).thenReturn(1024L);\n+    when(op.getResult()).thenReturn(httpOp);\n+    return op;\n+  }\n+\n+  private AbfsClient getMockAbfsClient() {\n+    // Mock failure for client.read()\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\n+        \"test\",\n+        this.getAccountName(),\n+        this.getConfiguration());\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+\n+    return client;\n+  }\n+\n+  private AbfsInputStream getAbfsInputStream(AbfsClient mockAbfsClient, String fileName) {\n+    // Create AbfsInputStream with the client instance\n+    AbfsInputStream inputStream = new AbfsInputStream(\n+        mockAbfsClient,\n+        null,\n+        FORWARD_SLASH + fileName,\n+        3 * KILOBYTE,\n+        1 * KILOBYTE, // Setting read ahead buffer size of 1 KB\n+        this.getConfiguration().getReadAheadQueueDepth(),\n+        this.getConfiguration().getTolerateOobAppends(),\n+        \"eTag\");\n+\n+    return inputStream;\n+  }\n+\n+  private void queueReadAheads(AbfsInputStream inputStream) {\n+    // Mimic AbfsInputStream readAhead queue requests\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 0, 1 * KILOBYTE);\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 1 * KILOBYTE, 1 * KILOBYTE);\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 2 * KILOBYTE, 1 * KILOBYTE);\n+  }\n+\n+  private void verifyReadCallCount(AbfsClient client, int count) throws\n+      AzureBlobFileSystemException, InterruptedException {\n+    // ReadAhead threads are triggered asynchronously.\n+    // Wait a second before verifying the number of total calls.\n+    Thread.sleep(1000);\n+    verify(client, times(count)).read(any(String.class), any(Long.class),\n+        any(byte[].class), any(Integer.class), any(Integer.class),\n+        any(String.class));\n+  }\n+\n+  private void checkEvictedStatus(AbfsInputStream inputStream, int position, boolean expectedToThrowException)\n+      throws Exception {\n+    // Sleep for the eviction threshold time\n+    Thread.sleep(ReadBufferManager.getBufferManager().getThresholdAgeMilliseconds() + 1000);\n+\n+    // Eviction is done only when AbfsInputStream tries to queue new items.\n+    // 1 tryEvict will remove 1 eligible item. To ensure that the current test buffer\n+    // will get evicted (considering there could be other tests running in parallel),\n+    // call tryEvict for the number of items that are there in completedReadList.\n+    int numOfCompletedReadListItems = ReadBufferManager.getBufferManager().getCompletedReadListSize();\n+    while (numOfCompletedReadListItems > 0) {\n+      ReadBufferManager.getBufferManager().callTryEvict();\n+      numOfCompletedReadListItems--;\n+    }\n+\n+    if (expectedToThrowException) {\n+      intercept(IOException.class,\n+          () -> inputStream.read(position, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE));\n+    } else {\n+      inputStream.read(position, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE);\n+    }\n+  }\n+\n+  public TestAbfsInputStream() throws Exception {\n+    super();\n+  }\n+\n+  /**\n+   * This test expects AbfsInputStream to throw the exception that readAhead\n+   * thread received on read. The readAhead thread must be initiated from the\n+   * active read request itself.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testFailedReadAhead() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // Actual read request fails with the failure in readahead thread\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error RAH-Thread-Z\"))\n+        .doReturn(successOp) // Any extra calls to read, pass it.\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testFailedReadAhead.txt\");\n+\n+    // Scenario: ReadAhead triggered from current active read call failed\n+    // Before the change to return exception from readahead buffer,\n+    // AbfsInputStream would have triggered an extra readremote on noticing\n+    // data absent in readahead buffers\n+    // In this test, a read should trigger 3 client.read() calls as file is 3 KB\n+    // and readahead buffer size set in AbfsInputStream is 1 KB\n+    // There should only be a total of 3 client.read() in this test.\n+    intercept(IOException.class,\n+        () -> inputStream.read(new byte[1 * KILOBYTE]));\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Stub returns success for the 4th read request, if ReadBuffers still\n+    // persisted, ReadAheadManager getBlock would have returned exception.\n+    checkEvictedStatus(inputStream, 0, false);\n+  }\n+\n+  /**\n+   * The test expects AbfsInputStream to initiate a remote read request for\n+   * the request offset and length when previous read ahead on the offset had failed.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testOlderReadAheadFailure() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // First Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // A second read request will see that readahead had failed for data in\n+    // the requested offset range and also that its is an older readahead request.\n+    // So attempt a new read only for the requested range.\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Z\"))\n+        .doReturn(successOp) // pass the read for second read request\n+        .doReturn(successOp) // pass success for post eviction test\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testOlderReadAheadFailure.txt\");\n+\n+    // First read request that fails as the readahead triggered from this request failed.\n+    intercept(IOException.class,\n+        () -> inputStream.read(new byte[1 * KILOBYTE]));\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Sleep for 30 sec so that the read ahead buffer qualifies for being old.\n+    Thread.sleep(ReadBufferManager.getBufferManager().getThresholdAgeMilliseconds());\n+\n+    // Second read request should retry the read (and not issue any new readaheads)\n+    inputStream.read(1 * KILOBYTE, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE);\n+\n+    // Once created, mock will remember all interactions. So total number of read\n+    // calls will be one more from earlier (there is a reset mock which will reset the\n+    // count, but the mock stub is erased as well which needs AbsInputStream to be recreated,\n+    // which beats the purpose)\n+    verifyReadCallCount(client, 4);\n+\n+    // Stub returns success for the 5th read request, if ReadBuffers still\n+    // persisted request would have failed for position 0.\n+    checkEvictedStatus(inputStream, 0, false);\n+  }\n+\n+  /**\n+   * The test expects AbfsInputStream to utilize any data read ahead for\n+   * requested offset and length.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testSuccessfulReadAhead() throws Exception {\n+    // Mock failure for client.read()\n+    AbfsClient client = getMockAbfsClient();\n+\n+    // Success operation mock\n+    AbfsRestOperation op = getMockRestOp();\n+\n+    // Stub :\n+    // Pass all readAheads and fail the post eviction request to\n+    // prove ReadAhead buffer is used\n+    // for post eviction check, fail all read aheads\n+    doReturn(op)\n+        .doReturn(op)\n+        .doReturn(op)\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Z\"))\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testSuccessfulReadAhead.txt\");\n+\n+    // First read request that triggers readAheads.\n+    inputStream.read(new byte[1 * KILOBYTE]);\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Another read request whose requested data is already read ahead.\n+    inputStream.read(1 * KILOBYTE, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE);\n+\n+    // Once created, mock will remember all interactions.\n+    // As the above read should not have triggered any server calls, total\n+    // number of read calls made at this point will be same as last.\n+    verifyReadCallCount(client, 3);\n+\n+    // Stub will throw exception for client.read() for 4th and later calls\n+    // if not using the read-ahead buffer exception will be thrown on read\n+    checkEvictedStatus(inputStream, 0, true);\n+  }\n+\n+  /**\n+   * This test expects ReadAheadManager to throw exception if the read ahead\n+   * thread had failed within the last 30 sec.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadAheadManagerForFailedReadAhead() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // Actual read request fails with the failure in readahead thread\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error RAH-Thread-Z\"))\n+        .doReturn(successOp) // Any extra calls to read, pass it.\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testReadAheadManagerForFailedReadAhead.txt\");\n+\n+    queueReadAheads(inputStream);\n+\n+    // AbfsInputStream Read would have waited for the read-ahead for the requested offset\n+    // as we are testing from ReadAheadManager directly, sleep for a sec to\n+    // get the read ahead threads to complete\n+    Thread.sleep(1000);\n+\n+    // if readAhead failed for specific offset, getBlock should\n+    // throw exception from the ReadBuffer that failed within last 30 sec\n+    intercept(IOException.class,\n+        () -> ReadBufferManager.getBufferManager().getBlock(\n+            inputStream,\n+            0,\n+            1 * KILOBYTE,\n+            new byte[1 * KILOBYTE]));\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Stub returns success for the 4th read request, if ReadBuffers still\n+    // persisted, ReadAheadManager getBlock would have returned exception.\n+    checkEvictedStatus(inputStream, 0, false);\n+  }\n+\n+  /**\n+   * The test expects ReadAheadManager to return 0 receivedBytes when previous\n+   * read ahead on the offset had failed and not throw exception received then.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadAheadManagerForOlderReadAheadFailure() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // First Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // A second read request will see that readahead had failed for data in\n+    // the requested offset range but also that its is an older readahead request.\n+    // System issue could have resolved by now, so attempt a new read only for the requested range.\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doReturn(successOp) // pass the read for second read request\n+        .doReturn(successOp) // pass success for post eviction test\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testReadAheadManagerForOlderReadAheadFailure.txt\");\n+\n+    queueReadAheads(inputStream);\n+\n+    // AbfsInputStream Read would have waited for the read-ahead for the requested offset\n+    // as we are testing from ReadAheadManager directly, sleep for 30 secs so that\n+    // read buffer qualifies for to be an old buffer\n+    Thread.sleep(ReadBufferManager.getBufferManager().getThresholdAgeMilliseconds());\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // getBlock from a new read request should return 0 if there is a failure\n+    // 30 sec before in read ahead buffer for respective offset.\n+    int bytesRead = ReadBufferManager.getBufferManager().getBlock(\n+        inputStream,\n+        1 * KILOBYTE,\n+        1 * KILOBYTE,\n+        new byte[1 * KILOBYTE]);\n+    Assert.assertTrue(\"bytesRead should be zero when previously read \"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAyODk2Ng=="}, "originalCommit": {"oid": "52fba323269e6272849b89824dc372398ead4894"}, "originalPosition": 368}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMzM1ODkxOnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxODo0MDo0OVrOGCRH2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMjo1MTo1MlrOGXe6Ww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAzMDg3NQ==", "bodyText": "I'd go for replacing 1 * KILOBYTE with just KILOBYTE, or having constants _1KB, _2KB etc, which is what I've done elsewhere", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r405030875", "createdAt": "2020-04-07T18:40:49Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "diffHunk": "@@ -0,0 +1,433 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.io.IOException;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.TimeoutException;\n+\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.Mockito.doReturn;\n+import static org.mockito.Mockito.doThrow;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+import static org.apache.hadoop.test.LambdaTestUtils.intercept;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.FORWARD_SLASH;\n+\n+/**\n+ * Unit test AbfsInputStream.\n+ */\n+public class TestAbfsInputStream extends\n+    AbstractAbfsIntegrationTest {\n+\n+  private static final int KILOBYTE = 1024;\n+\n+  private AbfsRestOperation getMockRestOp() {\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsHttpOperation httpOp = mock(AbfsHttpOperation.class);\n+    when(httpOp.getBytesReceived()).thenReturn(1024L);\n+    when(op.getResult()).thenReturn(httpOp);\n+    return op;\n+  }\n+\n+  private AbfsClient getMockAbfsClient() {\n+    // Mock failure for client.read()\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\n+        \"test\",\n+        this.getAccountName(),\n+        this.getConfiguration());\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+\n+    return client;\n+  }\n+\n+  private AbfsInputStream getAbfsInputStream(AbfsClient mockAbfsClient, String fileName) {\n+    // Create AbfsInputStream with the client instance\n+    AbfsInputStream inputStream = new AbfsInputStream(\n+        mockAbfsClient,\n+        null,\n+        FORWARD_SLASH + fileName,\n+        3 * KILOBYTE,\n+        1 * KILOBYTE, // Setting read ahead buffer size of 1 KB\n+        this.getConfiguration().getReadAheadQueueDepth(),\n+        this.getConfiguration().getTolerateOobAppends(),\n+        \"eTag\");\n+\n+    return inputStream;\n+  }\n+\n+  private void queueReadAheads(AbfsInputStream inputStream) {\n+    // Mimic AbfsInputStream readAhead queue requests\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 0, 1 * KILOBYTE);\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 1 * KILOBYTE, 1 * KILOBYTE);\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 2 * KILOBYTE, 1 * KILOBYTE);\n+  }\n+\n+  private void verifyReadCallCount(AbfsClient client, int count) throws\n+      AzureBlobFileSystemException, InterruptedException {\n+    // ReadAhead threads are triggered asynchronously.\n+    // Wait a second before verifying the number of total calls.\n+    Thread.sleep(1000);\n+    verify(client, times(count)).read(any(String.class), any(Long.class),\n+        any(byte[].class), any(Integer.class), any(Integer.class),\n+        any(String.class));\n+  }\n+\n+  private void checkEvictedStatus(AbfsInputStream inputStream, int position, boolean expectedToThrowException)\n+      throws Exception {\n+    // Sleep for the eviction threshold time\n+    Thread.sleep(ReadBufferManager.getBufferManager().getThresholdAgeMilliseconds() + 1000);\n+\n+    // Eviction is done only when AbfsInputStream tries to queue new items.\n+    // 1 tryEvict will remove 1 eligible item. To ensure that the current test buffer\n+    // will get evicted (considering there could be other tests running in parallel),\n+    // call tryEvict for the number of items that are there in completedReadList.\n+    int numOfCompletedReadListItems = ReadBufferManager.getBufferManager().getCompletedReadListSize();\n+    while (numOfCompletedReadListItems > 0) {\n+      ReadBufferManager.getBufferManager().callTryEvict();\n+      numOfCompletedReadListItems--;\n+    }\n+\n+    if (expectedToThrowException) {\n+      intercept(IOException.class,\n+          () -> inputStream.read(position, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE));\n+    } else {\n+      inputStream.read(position, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE);\n+    }\n+  }\n+\n+  public TestAbfsInputStream() throws Exception {\n+    super();\n+  }\n+\n+  /**\n+   * This test expects AbfsInputStream to throw the exception that readAhead\n+   * thread received on read. The readAhead thread must be initiated from the\n+   * active read request itself.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testFailedReadAhead() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // Actual read request fails with the failure in readahead thread\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error RAH-Thread-Z\"))\n+        .doReturn(successOp) // Any extra calls to read, pass it.\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testFailedReadAhead.txt\");\n+\n+    // Scenario: ReadAhead triggered from current active read call failed\n+    // Before the change to return exception from readahead buffer,\n+    // AbfsInputStream would have triggered an extra readremote on noticing\n+    // data absent in readahead buffers\n+    // In this test, a read should trigger 3 client.read() calls as file is 3 KB\n+    // and readahead buffer size set in AbfsInputStream is 1 KB\n+    // There should only be a total of 3 client.read() in this test.\n+    intercept(IOException.class,\n+        () -> inputStream.read(new byte[1 * KILOBYTE]));\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Stub returns success for the 4th read request, if ReadBuffers still\n+    // persisted, ReadAheadManager getBlock would have returned exception.\n+    checkEvictedStatus(inputStream, 0, false);\n+  }\n+\n+  /**\n+   * The test expects AbfsInputStream to initiate a remote read request for\n+   * the request offset and length when previous read ahead on the offset had failed.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testOlderReadAheadFailure() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // First Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // A second read request will see that readahead had failed for data in\n+    // the requested offset range and also that its is an older readahead request.\n+    // So attempt a new read only for the requested range.\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Z\"))\n+        .doReturn(successOp) // pass the read for second read request\n+        .doReturn(successOp) // pass success for post eviction test\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testOlderReadAheadFailure.txt\");\n+\n+    // First read request that fails as the readahead triggered from this request failed.\n+    intercept(IOException.class,\n+        () -> inputStream.read(new byte[1 * KILOBYTE]));\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Sleep for 30 sec so that the read ahead buffer qualifies for being old.\n+    Thread.sleep(ReadBufferManager.getBufferManager().getThresholdAgeMilliseconds());\n+\n+    // Second read request should retry the read (and not issue any new readaheads)\n+    inputStream.read(1 * KILOBYTE, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE);\n+\n+    // Once created, mock will remember all interactions. So total number of read\n+    // calls will be one more from earlier (there is a reset mock which will reset the\n+    // count, but the mock stub is erased as well which needs AbsInputStream to be recreated,\n+    // which beats the purpose)\n+    verifyReadCallCount(client, 4);\n+\n+    // Stub returns success for the 5th read request, if ReadBuffers still\n+    // persisted request would have failed for position 0.\n+    checkEvictedStatus(inputStream, 0, false);\n+  }\n+\n+  /**\n+   * The test expects AbfsInputStream to utilize any data read ahead for\n+   * requested offset and length.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testSuccessfulReadAhead() throws Exception {\n+    // Mock failure for client.read()\n+    AbfsClient client = getMockAbfsClient();\n+\n+    // Success operation mock\n+    AbfsRestOperation op = getMockRestOp();\n+\n+    // Stub :\n+    // Pass all readAheads and fail the post eviction request to\n+    // prove ReadAhead buffer is used\n+    // for post eviction check, fail all read aheads\n+    doReturn(op)\n+        .doReturn(op)\n+        .doReturn(op)\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Z\"))\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testSuccessfulReadAhead.txt\");\n+\n+    // First read request that triggers readAheads.\n+    inputStream.read(new byte[1 * KILOBYTE]);\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Another read request whose requested data is already read ahead.\n+    inputStream.read(1 * KILOBYTE, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52fba323269e6272849b89824dc372398ead4894"}, "originalPosition": 262}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI3Njg5MQ==", "bodyText": "Done", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r427276891", "createdAt": "2020-05-19T12:51:52Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "diffHunk": "@@ -0,0 +1,433 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.io.IOException;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.TimeoutException;\n+\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.Mockito.doReturn;\n+import static org.mockito.Mockito.doThrow;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+import static org.apache.hadoop.test.LambdaTestUtils.intercept;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.FORWARD_SLASH;\n+\n+/**\n+ * Unit test AbfsInputStream.\n+ */\n+public class TestAbfsInputStream extends\n+    AbstractAbfsIntegrationTest {\n+\n+  private static final int KILOBYTE = 1024;\n+\n+  private AbfsRestOperation getMockRestOp() {\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsHttpOperation httpOp = mock(AbfsHttpOperation.class);\n+    when(httpOp.getBytesReceived()).thenReturn(1024L);\n+    when(op.getResult()).thenReturn(httpOp);\n+    return op;\n+  }\n+\n+  private AbfsClient getMockAbfsClient() {\n+    // Mock failure for client.read()\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\n+        \"test\",\n+        this.getAccountName(),\n+        this.getConfiguration());\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+\n+    return client;\n+  }\n+\n+  private AbfsInputStream getAbfsInputStream(AbfsClient mockAbfsClient, String fileName) {\n+    // Create AbfsInputStream with the client instance\n+    AbfsInputStream inputStream = new AbfsInputStream(\n+        mockAbfsClient,\n+        null,\n+        FORWARD_SLASH + fileName,\n+        3 * KILOBYTE,\n+        1 * KILOBYTE, // Setting read ahead buffer size of 1 KB\n+        this.getConfiguration().getReadAheadQueueDepth(),\n+        this.getConfiguration().getTolerateOobAppends(),\n+        \"eTag\");\n+\n+    return inputStream;\n+  }\n+\n+  private void queueReadAheads(AbfsInputStream inputStream) {\n+    // Mimic AbfsInputStream readAhead queue requests\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 0, 1 * KILOBYTE);\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 1 * KILOBYTE, 1 * KILOBYTE);\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 2 * KILOBYTE, 1 * KILOBYTE);\n+  }\n+\n+  private void verifyReadCallCount(AbfsClient client, int count) throws\n+      AzureBlobFileSystemException, InterruptedException {\n+    // ReadAhead threads are triggered asynchronously.\n+    // Wait a second before verifying the number of total calls.\n+    Thread.sleep(1000);\n+    verify(client, times(count)).read(any(String.class), any(Long.class),\n+        any(byte[].class), any(Integer.class), any(Integer.class),\n+        any(String.class));\n+  }\n+\n+  private void checkEvictedStatus(AbfsInputStream inputStream, int position, boolean expectedToThrowException)\n+      throws Exception {\n+    // Sleep for the eviction threshold time\n+    Thread.sleep(ReadBufferManager.getBufferManager().getThresholdAgeMilliseconds() + 1000);\n+\n+    // Eviction is done only when AbfsInputStream tries to queue new items.\n+    // 1 tryEvict will remove 1 eligible item. To ensure that the current test buffer\n+    // will get evicted (considering there could be other tests running in parallel),\n+    // call tryEvict for the number of items that are there in completedReadList.\n+    int numOfCompletedReadListItems = ReadBufferManager.getBufferManager().getCompletedReadListSize();\n+    while (numOfCompletedReadListItems > 0) {\n+      ReadBufferManager.getBufferManager().callTryEvict();\n+      numOfCompletedReadListItems--;\n+    }\n+\n+    if (expectedToThrowException) {\n+      intercept(IOException.class,\n+          () -> inputStream.read(position, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE));\n+    } else {\n+      inputStream.read(position, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE);\n+    }\n+  }\n+\n+  public TestAbfsInputStream() throws Exception {\n+    super();\n+  }\n+\n+  /**\n+   * This test expects AbfsInputStream to throw the exception that readAhead\n+   * thread received on read. The readAhead thread must be initiated from the\n+   * active read request itself.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testFailedReadAhead() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // Actual read request fails with the failure in readahead thread\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error RAH-Thread-Z\"))\n+        .doReturn(successOp) // Any extra calls to read, pass it.\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testFailedReadAhead.txt\");\n+\n+    // Scenario: ReadAhead triggered from current active read call failed\n+    // Before the change to return exception from readahead buffer,\n+    // AbfsInputStream would have triggered an extra readremote on noticing\n+    // data absent in readahead buffers\n+    // In this test, a read should trigger 3 client.read() calls as file is 3 KB\n+    // and readahead buffer size set in AbfsInputStream is 1 KB\n+    // There should only be a total of 3 client.read() in this test.\n+    intercept(IOException.class,\n+        () -> inputStream.read(new byte[1 * KILOBYTE]));\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Stub returns success for the 4th read request, if ReadBuffers still\n+    // persisted, ReadAheadManager getBlock would have returned exception.\n+    checkEvictedStatus(inputStream, 0, false);\n+  }\n+\n+  /**\n+   * The test expects AbfsInputStream to initiate a remote read request for\n+   * the request offset and length when previous read ahead on the offset had failed.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testOlderReadAheadFailure() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // First Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // A second read request will see that readahead had failed for data in\n+    // the requested offset range and also that its is an older readahead request.\n+    // So attempt a new read only for the requested range.\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Z\"))\n+        .doReturn(successOp) // pass the read for second read request\n+        .doReturn(successOp) // pass success for post eviction test\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testOlderReadAheadFailure.txt\");\n+\n+    // First read request that fails as the readahead triggered from this request failed.\n+    intercept(IOException.class,\n+        () -> inputStream.read(new byte[1 * KILOBYTE]));\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Sleep for 30 sec so that the read ahead buffer qualifies for being old.\n+    Thread.sleep(ReadBufferManager.getBufferManager().getThresholdAgeMilliseconds());\n+\n+    // Second read request should retry the read (and not issue any new readaheads)\n+    inputStream.read(1 * KILOBYTE, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE);\n+\n+    // Once created, mock will remember all interactions. So total number of read\n+    // calls will be one more from earlier (there is a reset mock which will reset the\n+    // count, but the mock stub is erased as well which needs AbsInputStream to be recreated,\n+    // which beats the purpose)\n+    verifyReadCallCount(client, 4);\n+\n+    // Stub returns success for the 5th read request, if ReadBuffers still\n+    // persisted request would have failed for position 0.\n+    checkEvictedStatus(inputStream, 0, false);\n+  }\n+\n+  /**\n+   * The test expects AbfsInputStream to utilize any data read ahead for\n+   * requested offset and length.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testSuccessfulReadAhead() throws Exception {\n+    // Mock failure for client.read()\n+    AbfsClient client = getMockAbfsClient();\n+\n+    // Success operation mock\n+    AbfsRestOperation op = getMockRestOp();\n+\n+    // Stub :\n+    // Pass all readAheads and fail the post eviction request to\n+    // prove ReadAhead buffer is used\n+    // for post eviction check, fail all read aheads\n+    doReturn(op)\n+        .doReturn(op)\n+        .doReturn(op)\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Z\"))\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testSuccessfulReadAhead.txt\");\n+\n+    // First read request that triggers readAheads.\n+    inputStream.read(new byte[1 * KILOBYTE]);\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Another read request whose requested data is already read ahead.\n+    inputStream.read(1 * KILOBYTE, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAzMDg3NQ=="}, "originalCommit": {"oid": "52fba323269e6272849b89824dc372398ead4894"}, "originalPosition": 262}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMzM2MzY2OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxODo0MTo1N1rOGCRKxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMjo1MTozOFrOGXe5yw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAzMTYyMw==", "bodyText": "add trailing .", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r405031623", "createdAt": "2020-04-07T18:41:57Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -289,6 +298,27 @@ private ReadBuffer getFromList(final Collection<ReadBuffer> list, final AbfsInpu\n     return null;\n   }\n \n+  /**\n+   * Returns buffers that failed or passed from completed queue", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52fba323269e6272849b89824dc372398ead4894"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI3Njc0Nw==", "bodyText": "Done", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r427276747", "createdAt": "2020-05-19T12:51:38Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -289,6 +298,27 @@ private ReadBuffer getFromList(final Collection<ReadBuffer> list, final AbfsInpu\n     return null;\n   }\n \n+  /**\n+   * Returns buffers that failed or passed from completed queue", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAzMTYyMw=="}, "originalCommit": {"oid": "52fba323269e6272849b89824dc372398ead4894"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMzM3MTM5OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxODo0Mzo1N1rOGCRPlQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxODo0Mzo1N1rOGCRPlQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAzMjg1Mw==", "bodyText": "Should always be for requested length? That is: we don't care about the total length of the buffer, only the amount which was requested from the far end -which may be less?", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r405032853", "createdAt": "2020-04-07T18:43:57Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -289,6 +298,27 @@ private ReadBuffer getFromList(final Collection<ReadBuffer> list, final AbfsInpu\n     return null;\n   }\n \n+  /**\n+   * Returns buffers that failed or passed from completed queue\n+   * @param stream\n+   * @param requestedOffset\n+   * @return\n+   */\n+  private ReadBuffer getBufferFromCompletedQueue(final AbfsInputStream stream, final long requestedOffset) {\n+    for (ReadBuffer buffer : completedReadList) {\n+      // Buffer is returned if the requestedOffset is at or above buffer's\n+      // offset but less than buffer's length or the actual requestedLength\n+      if ((buffer.getStream() == stream)\n+          && (requestedOffset >= buffer.getOffset())\n+          && ((requestedOffset < buffer.getOffset() + buffer.getLength())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52fba323269e6272849b89824dc372398ead4894"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU1NjU2MzI1OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQxNjoyMzo1OFrOGIc57Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQxNjoyMzo1OFrOGIc57Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTUxNTM3Mw==", "bodyText": "new line", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r411515373", "createdAt": "2020-04-20T16:23:58Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "diffHunk": "@@ -0,0 +1,433 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.io.IOException;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.TimeoutException;\n+\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.Mockito.doReturn;\n+import static org.mockito.Mockito.doThrow;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+import static org.apache.hadoop.test.LambdaTestUtils.intercept;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.FORWARD_SLASH;\n+\n+/**\n+ * Unit test AbfsInputStream.\n+ */\n+public class TestAbfsInputStream extends\n+    AbstractAbfsIntegrationTest {\n+\n+  private static final int KILOBYTE = 1024;\n+\n+  private AbfsRestOperation getMockRestOp() {\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsHttpOperation httpOp = mock(AbfsHttpOperation.class);\n+    when(httpOp.getBytesReceived()).thenReturn(1024L);\n+    when(op.getResult()).thenReturn(httpOp);\n+    return op;\n+  }\n+\n+  private AbfsClient getMockAbfsClient() {\n+    // Mock failure for client.read()\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\n+        \"test\",\n+        this.getAccountName(),\n+        this.getConfiguration());\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+\n+    return client;\n+  }\n+\n+  private AbfsInputStream getAbfsInputStream(AbfsClient mockAbfsClient, String fileName) {\n+    // Create AbfsInputStream with the client instance\n+    AbfsInputStream inputStream = new AbfsInputStream(\n+        mockAbfsClient,\n+        null,\n+        FORWARD_SLASH + fileName,\n+        3 * KILOBYTE,\n+        1 * KILOBYTE, // Setting read ahead buffer size of 1 KB\n+        this.getConfiguration().getReadAheadQueueDepth(),\n+        this.getConfiguration().getTolerateOobAppends(),\n+        \"eTag\");\n+\n+    return inputStream;\n+  }\n+\n+  private void queueReadAheads(AbfsInputStream inputStream) {\n+    // Mimic AbfsInputStream readAhead queue requests\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 0, 1 * KILOBYTE);\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 1 * KILOBYTE, 1 * KILOBYTE);\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 2 * KILOBYTE, 1 * KILOBYTE);\n+  }\n+\n+  private void verifyReadCallCount(AbfsClient client, int count) throws\n+      AzureBlobFileSystemException, InterruptedException {\n+    // ReadAhead threads are triggered asynchronously.\n+    // Wait a second before verifying the number of total calls.\n+    Thread.sleep(1000);\n+    verify(client, times(count)).read(any(String.class), any(Long.class),\n+        any(byte[].class), any(Integer.class), any(Integer.class),\n+        any(String.class));\n+  }\n+\n+  private void checkEvictedStatus(AbfsInputStream inputStream, int position, boolean expectedToThrowException)\n+      throws Exception {\n+    // Sleep for the eviction threshold time\n+    Thread.sleep(ReadBufferManager.getBufferManager().getThresholdAgeMilliseconds() + 1000);\n+\n+    // Eviction is done only when AbfsInputStream tries to queue new items.\n+    // 1 tryEvict will remove 1 eligible item. To ensure that the current test buffer\n+    // will get evicted (considering there could be other tests running in parallel),\n+    // call tryEvict for the number of items that are there in completedReadList.\n+    int numOfCompletedReadListItems = ReadBufferManager.getBufferManager().getCompletedReadListSize();\n+    while (numOfCompletedReadListItems > 0) {\n+      ReadBufferManager.getBufferManager().callTryEvict();\n+      numOfCompletedReadListItems--;\n+    }\n+\n+    if (expectedToThrowException) {\n+      intercept(IOException.class,\n+          () -> inputStream.read(position, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE));\n+    } else {\n+      inputStream.read(position, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE);\n+    }\n+  }\n+\n+  public TestAbfsInputStream() throws Exception {\n+    super();\n+  }\n+\n+  /**\n+   * This test expects AbfsInputStream to throw the exception that readAhead\n+   * thread received on read. The readAhead thread must be initiated from the\n+   * active read request itself.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testFailedReadAhead() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // Actual read request fails with the failure in readahead thread\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error RAH-Thread-Z\"))\n+        .doReturn(successOp) // Any extra calls to read, pass it.\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testFailedReadAhead.txt\");\n+\n+    // Scenario: ReadAhead triggered from current active read call failed\n+    // Before the change to return exception from readahead buffer,\n+    // AbfsInputStream would have triggered an extra readremote on noticing\n+    // data absent in readahead buffers\n+    // In this test, a read should trigger 3 client.read() calls as file is 3 KB\n+    // and readahead buffer size set in AbfsInputStream is 1 KB\n+    // There should only be a total of 3 client.read() in this test.\n+    intercept(IOException.class,\n+        () -> inputStream.read(new byte[1 * KILOBYTE]));\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Stub returns success for the 4th read request, if ReadBuffers still\n+    // persisted, ReadAheadManager getBlock would have returned exception.\n+    checkEvictedStatus(inputStream, 0, false);\n+  }\n+\n+  /**\n+   * The test expects AbfsInputStream to initiate a remote read request for\n+   * the request offset and length when previous read ahead on the offset had failed.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testOlderReadAheadFailure() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // First Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // A second read request will see that readahead had failed for data in\n+    // the requested offset range and also that its is an older readahead request.\n+    // So attempt a new read only for the requested range.\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Z\"))\n+        .doReturn(successOp) // pass the read for second read request\n+        .doReturn(successOp) // pass success for post eviction test\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testOlderReadAheadFailure.txt\");\n+\n+    // First read request that fails as the readahead triggered from this request failed.\n+    intercept(IOException.class,\n+        () -> inputStream.read(new byte[1 * KILOBYTE]));\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Sleep for 30 sec so that the read ahead buffer qualifies for being old.\n+    Thread.sleep(ReadBufferManager.getBufferManager().getThresholdAgeMilliseconds());\n+\n+    // Second read request should retry the read (and not issue any new readaheads)\n+    inputStream.read(1 * KILOBYTE, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE);\n+\n+    // Once created, mock will remember all interactions. So total number of read\n+    // calls will be one more from earlier (there is a reset mock which will reset the\n+    // count, but the mock stub is erased as well which needs AbsInputStream to be recreated,\n+    // which beats the purpose)\n+    verifyReadCallCount(client, 4);\n+\n+    // Stub returns success for the 5th read request, if ReadBuffers still\n+    // persisted request would have failed for position 0.\n+    checkEvictedStatus(inputStream, 0, false);\n+  }\n+\n+  /**\n+   * The test expects AbfsInputStream to utilize any data read ahead for\n+   * requested offset and length.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testSuccessfulReadAhead() throws Exception {\n+    // Mock failure for client.read()\n+    AbfsClient client = getMockAbfsClient();\n+\n+    // Success operation mock\n+    AbfsRestOperation op = getMockRestOp();\n+\n+    // Stub :\n+    // Pass all readAheads and fail the post eviction request to\n+    // prove ReadAhead buffer is used\n+    // for post eviction check, fail all read aheads\n+    doReturn(op)\n+        .doReturn(op)\n+        .doReturn(op)\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Z\"))\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testSuccessfulReadAhead.txt\");\n+\n+    // First read request that triggers readAheads.\n+    inputStream.read(new byte[1 * KILOBYTE]);\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Another read request whose requested data is already read ahead.\n+    inputStream.read(1 * KILOBYTE, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE);\n+\n+    // Once created, mock will remember all interactions.\n+    // As the above read should not have triggered any server calls, total\n+    // number of read calls made at this point will be same as last.\n+    verifyReadCallCount(client, 3);\n+\n+    // Stub will throw exception for client.read() for 4th and later calls\n+    // if not using the read-ahead buffer exception will be thrown on read\n+    checkEvictedStatus(inputStream, 0, true);\n+  }\n+\n+  /**\n+   * This test expects ReadAheadManager to throw exception if the read ahead\n+   * thread had failed within the last 30 sec.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadAheadManagerForFailedReadAhead() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // Actual read request fails with the failure in readahead thread\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error RAH-Thread-Z\"))\n+        .doReturn(successOp) // Any extra calls to read, pass it.\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testReadAheadManagerForFailedReadAhead.txt\");\n+\n+    queueReadAheads(inputStream);\n+\n+    // AbfsInputStream Read would have waited for the read-ahead for the requested offset\n+    // as we are testing from ReadAheadManager directly, sleep for a sec to\n+    // get the read ahead threads to complete\n+    Thread.sleep(1000);\n+\n+    // if readAhead failed for specific offset, getBlock should\n+    // throw exception from the ReadBuffer that failed within last 30 sec\n+    intercept(IOException.class,\n+        () -> ReadBufferManager.getBufferManager().getBlock(\n+            inputStream,\n+            0,\n+            1 * KILOBYTE,\n+            new byte[1 * KILOBYTE]));\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Stub returns success for the 4th read request, if ReadBuffers still\n+    // persisted, ReadAheadManager getBlock would have returned exception.\n+    checkEvictedStatus(inputStream, 0, false);\n+  }\n+\n+  /**\n+   * The test expects ReadAheadManager to return 0 receivedBytes when previous\n+   * read ahead on the offset had failed and not throw exception received then.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadAheadManagerForOlderReadAheadFailure() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // First Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // A second read request will see that readahead had failed for data in\n+    // the requested offset range but also that its is an older readahead request.\n+    // System issue could have resolved by now, so attempt a new read only for the requested range.\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doReturn(successOp) // pass the read for second read request\n+        .doReturn(successOp) // pass success for post eviction test\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testReadAheadManagerForOlderReadAheadFailure.txt\");\n+\n+    queueReadAheads(inputStream);\n+\n+    // AbfsInputStream Read would have waited for the read-ahead for the requested offset\n+    // as we are testing from ReadAheadManager directly, sleep for 30 secs so that\n+    // read buffer qualifies for to be an old buffer\n+    Thread.sleep(ReadBufferManager.getBufferManager().getThresholdAgeMilliseconds());\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // getBlock from a new read request should return 0 if there is a failure\n+    // 30 sec before in read ahead buffer for respective offset.\n+    int bytesRead = ReadBufferManager.getBufferManager().getBlock(\n+        inputStream,\n+        1 * KILOBYTE,\n+        1 * KILOBYTE,\n+        new byte[1 * KILOBYTE]);\n+    Assert.assertTrue(\"bytesRead should be zero when previously read \"\n+        + \"ahead buffer had failed\", bytesRead == 0);\n+\n+    // Stub returns success for the 5th read request, if ReadBuffers still\n+    // persisted request would have failed for position 0.\n+    checkEvictedStatus(inputStream, 0, false);\n+  }\n+\n+  /**\n+   * The test expects ReadAheadManager to return data from previously read\n+   * ahead data of same offset.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadAheadManagerForSuccessfulReadAhead() throws Exception {\n+    // Mock failure for client.read()\n+    AbfsClient client = getMockAbfsClient();\n+\n+    // Success operation mock\n+    AbfsRestOperation op = getMockRestOp();\n+\n+    // Stub :\n+    // Pass all readAheads and fail the post eviction request to\n+    // prove ReadAhead buffer is used\n+    doReturn(op)\n+        .doReturn(op)\n+        .doReturn(op)\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-X\")) // for post eviction request\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Z\"))\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testSuccessfulReadAhead.txt\");\n+\n+    queueReadAheads(inputStream);\n+\n+    // AbfsInputStream Read would have waited for the read-ahead for the requested offset\n+    // as we are testing from ReadAheadManager directly, sleep for a sec to\n+    // get the read ahead threads to complete\n+    Thread.sleep(1000);\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // getBlock for a new read should return the buffer read-ahead\n+    int bytesRead = ReadBufferManager.getBufferManager().getBlock(\n+        inputStream,\n+        1 * KILOBYTE,\n+        1 * KILOBYTE,\n+        new byte[1 * KILOBYTE]);\n+\n+    Assert.assertTrue(\"bytesRead should be non-zero from the \"\n+        + \"buffer that was read-ahead\", bytesRead > 0);\n+\n+    // Once created, mock will remember all interactions.\n+    // As the above read should not have triggered any server calls, total\n+    // number of read calls made at this point will be same as last.\n+    verifyReadCallCount(client, 3);\n+\n+    // Stub will throw exception for client.read() for 4th and later calls\n+    // if not using the read-ahead buffer exception will be thrown on read\n+    checkEvictedStatus(inputStream, 0, true);\n+  }\n+}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52fba323269e6272849b89824dc372398ead4894"}, "originalPosition": 433}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkNDM2OTk0OTQ4OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wOC0xMVQwNzoyNjo0N1rOKOw2ZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wOC0xMVQxMTozNjozNVrOKO7ofQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY4NjU2OTA2MQ==", "bodyText": "Hi @snvijaya\nI am unable to understand the significance of this change. I couldn't find in code anywhere where bufferIndex is set to -1 in case of read failure apart from the default value in the class. But when the buffers initialised, they are always set to value from 0 to 15.\nTrying to understand this for #3285. So please review that as well. Thanks.", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r686569061", "createdAt": "2021-08-11T07:26:47Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -253,7 +258,12 @@ private synchronized boolean tryEvict() {\n   }\n \n   private boolean evict(final ReadBuffer buf) {\n-    freeList.push(buf.getBufferindex());\n+    // As failed ReadBuffers (bufferIndx = -1) are saved in completedReadList,\n+    // avoid adding it to freeList.\n+    if (buf.getBufferindex() != -1) {\n+      freeList.push(buf.getBufferindex());\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "669f3c18d26b0c29c1c50dcc8e11a039d2fe4a72"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY4NjU4MTM4NQ==", "bodyText": "Its set to -1 when read fails. You will find the diff for this in ReadBuffer.java line 110.\nThere is an issue with this commit though, for which a hotfix was made. Incase its relevant to your change -> https://issues.apache.org/jira/browse/HADOOP-17301\nWill check on your PR by EOW.", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r686581385", "createdAt": "2021-08-11T07:44:16Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -253,7 +258,12 @@ private synchronized boolean tryEvict() {\n   }\n \n   private boolean evict(final ReadBuffer buf) {\n-    freeList.push(buf.getBufferindex());\n+    // As failed ReadBuffers (bufferIndx = -1) are saved in completedReadList,\n+    // avoid adding it to freeList.\n+    if (buf.getBufferindex() != -1) {\n+      freeList.push(buf.getBufferindex());\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY4NjU2OTA2MQ=="}, "originalCommit": {"oid": "669f3c18d26b0c29c1c50dcc8e11a039d2fe4a72"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY4Njc0NTcyNQ==", "bodyText": "Thanks @snvijaya", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r686745725", "createdAt": "2021-08-11T11:36:35Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -253,7 +258,12 @@ private synchronized boolean tryEvict() {\n   }\n \n   private boolean evict(final ReadBuffer buf) {\n-    freeList.push(buf.getBufferindex());\n+    // As failed ReadBuffers (bufferIndx = -1) are saved in completedReadList,\n+    // avoid adding it to freeList.\n+    if (buf.getBufferindex() != -1) {\n+      freeList.push(buf.getBufferindex());\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY4NjU2OTA2MQ=="}, "originalCommit": {"oid": "669f3c18d26b0c29c1c50dcc8e11a039d2fe4a72"}, "originalPosition": 55}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3628, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}