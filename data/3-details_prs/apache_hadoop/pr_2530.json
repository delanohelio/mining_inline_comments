{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTMzOTI3Mzgx", "number": 2530, "title": "HADOOP-17414. Magic committer files don't have the count of bytes written collected by spark", "bodyText": "Add getXAttr and listXAttr API calls to S3AFileSystem\nReturns all S3 object headers as XAttr attributes prefixed \"header.\" That's custom and standard (e.g header.Content-Length).\n\nThe setXAttr call isn't implemented, so for correctness the FS doesn't\ndeclare its support for the API in hasPathCapability().\nThe magic commit file write sets the custom header\nset the length of the data final data in the header\nx-hadoop-s3a-magic-data-length in the marker file.\nA matching patch in Spark will look for the XAttr\n\"header.x-hadoop-s3a-magic-data-length\" when the file\nbeing probed for output data is zero byte long.\nAs a result, the job tracking statistics will report the\nbytes written but yet to be manifest.\nExample from the debug log of a run\n2020-12-10 14:56:16,760 [Executor task launch worker for task 3] INFO  magic.MagicCommitTracker (MagicCommitTracker.java:aboutToComplete(134)) - Uncommitted data pending to file s3a://stevel-london/cloud-integration/DELAY_LISTING_ME/S3ACommitDataframeSuite/dataframe-committer/committer-magic-parquet/parquet/__magic/job-f718ab6a-27df-40fe-8b25-a8d83cd25dca/tasks/attempt_202012101456156213593579150946569_0005_m_000000_3/__base/year=2017/month=1/part-00000-fe9c22b3-8639-451e-9843-7498f2ea5f41.c000.snappy.parquet; commit metadata for 1 parts in cloud-integration/DELAY_LISTING_ME/S3ACommitDataframeSuite/dataframe-committer/committer-magic-parquet/parquet/__magic/job-f718ab6a-27df-40fe-8b25-a8d83cd25dca/tasks/attempt_202012101456156213593579150946569_0005_m_000000_3/__base/year=2017/month=1/part-00000-fe9c22b3-8639-451e-9843-7498f2ea5f41.c000.snappy.parquet.pending. sixe: 6286 byte(s)\n2020-12-10 14:56:17,513 [Executor task launch worker for task 3] INFO  s3a.S3ABlockOutputStream (S3ABlockOutputStream.java:close(388)) - File cloud-integration/DELAY_LISTING_ME/S3ACommitDataframeSuite/dataframe-committer/committer-magic-parquet/parquet/year=2017/month=1/part-00000-fe9c22b3-8639-451e-9843-7498f2ea5f41.c000.snappy.parquet will be visible when the job is committed\n2020-12-10 14:56:17,705 [Executor task launch worker for task 3] DEBUG datasources.BasicWriteTaskStatsTracker (Logging.scala:logDebug(61)) - File Length statistics for s3a://stevel-london/cloud-integration/DELAY_LISTING_ME/S3ACommitDataframeSuite/dataframe-committer/committer-magic-parquet/parquet/__magic/job-f718ab6a-27df-40fe-8b25-a8d83cd25dca/tasks/attempt_202012101456156213593579150946569_0005_m_000000_3/__base/year=2017/month=1/part-00000-fe9c22b3-8639-451e-9843-7498f2ea5f41.c000.snappy.parquet retrieved from XAttr: 6286\n\nThe key is the comment \"retrieved from XAttr: 6286\" -this means that the file length of 6286 was retrieved/returned.\nThe matching spark PR is apache/spark#30714", "createdAt": "2020-12-07T20:26:02Z", "url": "https://github.com/apache/hadoop/pull/2530", "merged": true, "mergeCommit": {"oid": "80c7404b519da8d7d69be4c01eb84dd2c08d80a5"}, "closed": true, "closedAt": "2021-01-26T19:30:52Z", "author": {"login": "steveloughran"}, "timelineItems": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdkxy7WgBqjQwOTQ2NTYxODA=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdz3BXzgFqTU3NjEwNTUxMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c3dfd4dfac457dc99fcf9a595e61fbe67d30b199", "author": {"user": {"login": "steveloughran", "name": "Steve Loughran"}}, "url": "https://github.com/apache/hadoop/commit/c3dfd4dfac457dc99fcf9a595e61fbe67d30b199", "committedDate": "2020-12-08T19:01:57Z", "message": "HADOOP-17414. All AWS headers returned as xattrs\n\n* New HeaderProcessing store operation to isolate this\n* ContextAccessors adds call to getObjectMetadata\n* the various FS getXattr* calls all delegate to HeaderProcessing\n* the returned header list contains all user headers\n* and the standard HTTP ones.\n\nTests for all of this\n* unit tests for HeaderProcessing\n* etag propagation in ITestMiscOperations\n* And something to query/log that getXAttr(/) is sensible\n* commit tests verify the attribute is set\n\nThis patch doesn't play games with getFileStatus and provides information\n(access to headers) which is more broadly useful.\n\nThe setXAttr calls aren't supported: they are immutable.\n\nChange-Id: I58f1ac1db88da945e93292f3738f3e71f89d8dd6"}, "afterCommit": {"oid": "b31a847325a51f5a6bfc878bce4b0f7f70b0e6ff", "author": {"user": {"login": "steveloughran", "name": "Steve Loughran"}}, "url": "https://github.com/apache/hadoop/commit/b31a847325a51f5a6bfc878bce4b0f7f70b0e6ff", "committedDate": "2020-12-10T09:21:06Z", "message": "HADOOP-17417 ongoing dev\n\nChange-Id: I06fe9ad3ff4e90bf152251e2111946a3ab14e4e8"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "b31a847325a51f5a6bfc878bce4b0f7f70b0e6ff", "author": {"user": {"login": "steveloughran", "name": "Steve Loughran"}}, "url": "https://github.com/apache/hadoop/commit/b31a847325a51f5a6bfc878bce4b0f7f70b0e6ff", "committedDate": "2020-12-10T09:21:06Z", "message": "HADOOP-17417 ongoing dev\n\nChange-Id: I06fe9ad3ff4e90bf152251e2111946a3ab14e4e8"}, "afterCommit": {"oid": "d2fd7e3b06bd83712b22fe89b459ffe323d7b85e", "author": {"user": {"login": "steveloughran", "name": "Steve Loughran"}}, "url": "https://github.com/apache/hadoop/commit/d2fd7e3b06bd83712b22fe89b459ffe323d7b85e", "committedDate": "2020-12-10T20:57:12Z", "message": "HADOOP-17414. Checkstyle\n\nChange-Id: I60276cf0f7e0e95583d877ecd2e77ff338b9f207"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d3b49e1d726bdd6a0f99b3afb679be6f9ec11262", "author": {"user": {"login": "steveloughran", "name": "Steve Loughran"}}, "url": "https://github.com/apache/hadoop/commit/d3b49e1d726bdd6a0f99b3afb679be6f9ec11262", "committedDate": "2020-12-14T13:28:42Z", "message": "HADOOP-17414. Checkstyle and whitespace\n\nChange-Id: Ibceaae5c9cabe15957a8aa50ba429e82854acd01"}, "afterCommit": {"oid": "3cf9d2be27bd6a65f14e71ebf433f8a1d6c37423", "author": {"user": {"login": "steveloughran", "name": "Steve Loughran"}}, "url": "https://github.com/apache/hadoop/commit/3cf9d2be27bd6a65f14e71ebf433f8a1d6c37423", "committedDate": "2021-01-04T13:45:25Z", "message": "HADOOP-17414. Checkstyle and whitespace\n\nChange-Id: Ibceaae5c9cabe15957a8aa50ba429e82854acd01"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3cf9d2be27bd6a65f14e71ebf433f8a1d6c37423", "author": {"user": {"login": "steveloughran", "name": "Steve Loughran"}}, "url": "https://github.com/apache/hadoop/commit/3cf9d2be27bd6a65f14e71ebf433f8a1d6c37423", "committedDate": "2021-01-04T13:45:25Z", "message": "HADOOP-17414. Checkstyle and whitespace\n\nChange-Id: Ibceaae5c9cabe15957a8aa50ba429e82854acd01"}, "afterCommit": {"oid": "732cceb01a8c2c5db8ed669bb4cf5086dd101b03", "author": {"user": {"login": "steveloughran", "name": "Steve Loughran"}}, "url": "https://github.com/apache/hadoop/commit/732cceb01a8c2c5db8ed669bb4cf5086dd101b03", "committedDate": "2021-01-07T11:40:07Z", "message": "HADOOP-17414. Magic committer files don't have the count of bytes written collected by spark\n\nThis is a PoC which, having implemented, I don't think is viable.\n\nYes, we can fix up getFileStatus so it reads the header. It even knows\nto always bypass S3Guard (no inconsistencies to worry about any more).\n\nBut: the blast radius of the change is too big. I'm worried about\ndistcp or any other code which goes\nlen =getFileStatus(path).getLen()\nopen(path).readFully(0, len, dest)\n\nYou'll get an EOF here. Find the file through a listing and you'll be OK\nprovided S3Guard isn't updated with that GetFileStatus result, which I\nhave seen.\n\nThe ordering of probes in ITestMagicCommitProtocol.validateTaskAttemptPathAfterWrite\nneed to be list before getFileStatus, so the S3Guard table is updated from\nthe list.\n\noverall: danger. Even without S3Guard there's risk.\n\nAnyway, shown it can be done. And I think there's a merit in a leaner patch\nwhich attaches the marker but doesn't do any fixup. This would let us add\nan API call \"getObjectHeaders(path) -> Future<Map<String, String>> and\nthen use that to do the lookup. We can implement the probe for\nABFS and S3, add a hasPathCapabilities for it as well as an interface\nthe FS can implement (which passthrough filesystems would need to do).\n\nChange-Id: If56213c0c5d8ab696d2d89b48ad52874960b0920"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY0MTY0NzUw", "url": "https://github.com/apache/hadoop/pull/2530#pullrequestreview-564164750", "createdAt": "2021-01-08T10:43:17Z", "commit": {"oid": "732cceb01a8c2c5db8ed669bb4cf5086dd101b03"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wOFQxMDo0MzoxN1rOIQNljQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wOFQxMDo0MzoxN1rOIQNljQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mzg3MDczMw==", "bodyText": "with this change we will enable magic committer for all the components. Maybe point that out in the title of the issue.", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r553870733", "createdAt": "2021-01-08T10:43:17Z", "author": {"login": "bgaborg"}, "path": "hadoop-common-project/hadoop-common/src/main/resources/core-default.xml", "diffHunk": "@@ -1873,11 +1873,9 @@\n \n <property>\n   <name>fs.s3a.committer.magic.enabled</name>\n-  <value>false</value>\n+  <value>true</value>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "732cceb01a8c2c5db8ed669bb4cf5086dd101b03"}, "originalPosition": 5}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY0NjY5NDYy", "url": "https://github.com/apache/hadoop/pull/2530#pullrequestreview-564669462", "createdAt": "2021-01-09T01:20:13Z", "commit": {"oid": "732cceb01a8c2c5db8ed669bb4cf5086dd101b03"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wOVQwMToyMDoxM1rOIQl59w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wOVQwMToyNjo1OFrOIQl-Dw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDI2OTE3NQ==", "bodyText": "nit: I wonder if some kind of caching will be useful here. We are calling getObjectMetadata for every getXAttr call.", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r554269175", "createdAt": "2021-01-09T01:20:13Z", "author": {"login": "sunchao"}, "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/HeaderProcessing.java", "diffHunk": "@@ -0,0 +1,300 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3a.impl;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.TreeMap;\n+\n+import com.amazonaws.services.s3.Headers;\n+import com.amazonaws.services.s3.model.ObjectMetadata;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.apache.hadoop.fs.s3a.Constants.HEADER_PREFIX;\n+import static org.apache.hadoop.fs.s3a.commit.CommitConstants.X_HEADER_MAGIC_MARKER;\n+\n+/**\n+ * Part of the S3A FS where object headers are\n+ * processed.\n+ * Implements all the various XAttr read operations.\n+ * Those APIs all expect byte arrays back.\n+ * Metadata cloning is also implemented here, so as\n+ * to stay in sync with custom header logic.\n+ */\n+public class HeaderProcessing extends AbstractStoreOperation {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      HeaderProcessing.class);\n+\n+  private static final byte[] EMPTY = new byte[0];\n+\n+  /**\n+   * Length XAttr.\n+   */\n+  public static final String XA_CONTENT_LENGTH =\n+      HEADER_PREFIX + Headers.CONTENT_LENGTH;\n+\n+  /**\n+   * last modified XAttr.\n+   */\n+  public static final String XA_LAST_MODIFIED =\n+      HEADER_PREFIX + Headers.LAST_MODIFIED;\n+\n+  public static final String XA_CONTENT_DISPOSITION =\n+      HEADER_PREFIX + Headers.CONTENT_DISPOSITION;\n+\n+  public static final String XA_CONTENT_ENCODING =\n+      HEADER_PREFIX + Headers.CONTENT_ENCODING;\n+\n+  public static final String XA_CONTENT_LANGUAGE =\n+      HEADER_PREFIX + Headers.CONTENT_LANGUAGE;\n+\n+  public static final String XA_CONTENT_MD5 =\n+      HEADER_PREFIX + Headers.CONTENT_MD5;\n+\n+  public static final String XA_CONTENT_RANGE =\n+      HEADER_PREFIX + Headers.CONTENT_RANGE;\n+\n+  public static final String XA_CONTENT_TYPE =\n+      HEADER_PREFIX + Headers.CONTENT_TYPE;\n+\n+  public static final String XA_ETAG = HEADER_PREFIX + Headers.ETAG;\n+\n+  public HeaderProcessing(final StoreContext storeContext) {\n+    super(storeContext);\n+  }\n+\n+  /**\n+   * Query the store, get all the headers into a map. Each Header\n+   * has the \"header.\" prefix.\n+   * Caller must have read access.\n+   * The value of each header is the string value of the object\n+   * UTF-8 encoded.\n+   * @param path path of object.\n+   * @return the headers\n+   * @throws IOException failure, including file not found.\n+   */\n+  private Map<String, byte[]> retrieveHeaders(Path path) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "732cceb01a8c2c5db8ed669bb4cf5086dd101b03"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDI2OTIzNQ==", "bodyText": "nit: to be consistent with the above, should we add comments for the following constants?", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r554269235", "createdAt": "2021-01-09T01:20:47Z", "author": {"login": "sunchao"}, "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/HeaderProcessing.java", "diffHunk": "@@ -0,0 +1,300 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3a.impl;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.TreeMap;\n+\n+import com.amazonaws.services.s3.Headers;\n+import com.amazonaws.services.s3.model.ObjectMetadata;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.apache.hadoop.fs.s3a.Constants.HEADER_PREFIX;\n+import static org.apache.hadoop.fs.s3a.commit.CommitConstants.X_HEADER_MAGIC_MARKER;\n+\n+/**\n+ * Part of the S3A FS where object headers are\n+ * processed.\n+ * Implements all the various XAttr read operations.\n+ * Those APIs all expect byte arrays back.\n+ * Metadata cloning is also implemented here, so as\n+ * to stay in sync with custom header logic.\n+ */\n+public class HeaderProcessing extends AbstractStoreOperation {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      HeaderProcessing.class);\n+\n+  private static final byte[] EMPTY = new byte[0];\n+\n+  /**\n+   * Length XAttr.\n+   */\n+  public static final String XA_CONTENT_LENGTH =\n+      HEADER_PREFIX + Headers.CONTENT_LENGTH;\n+\n+  /**\n+   * last modified XAttr.\n+   */\n+  public static final String XA_LAST_MODIFIED =\n+      HEADER_PREFIX + Headers.LAST_MODIFIED;\n+\n+  public static final String XA_CONTENT_DISPOSITION =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "732cceb01a8c2c5db8ed669bb4cf5086dd101b03"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDI2OTUyMg==", "bodyText": "hmm why we need a return value if ret is already changed in place?", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r554269522", "createdAt": "2021-01-09T01:22:57Z", "author": {"login": "sunchao"}, "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/HeaderProcessing.java", "diffHunk": "@@ -0,0 +1,300 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3a.impl;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.TreeMap;\n+\n+import com.amazonaws.services.s3.Headers;\n+import com.amazonaws.services.s3.model.ObjectMetadata;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.apache.hadoop.fs.s3a.Constants.HEADER_PREFIX;\n+import static org.apache.hadoop.fs.s3a.commit.CommitConstants.X_HEADER_MAGIC_MARKER;\n+\n+/**\n+ * Part of the S3A FS where object headers are\n+ * processed.\n+ * Implements all the various XAttr read operations.\n+ * Those APIs all expect byte arrays back.\n+ * Metadata cloning is also implemented here, so as\n+ * to stay in sync with custom header logic.\n+ */\n+public class HeaderProcessing extends AbstractStoreOperation {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      HeaderProcessing.class);\n+\n+  private static final byte[] EMPTY = new byte[0];\n+\n+  /**\n+   * Length XAttr.\n+   */\n+  public static final String XA_CONTENT_LENGTH =\n+      HEADER_PREFIX + Headers.CONTENT_LENGTH;\n+\n+  /**\n+   * last modified XAttr.\n+   */\n+  public static final String XA_LAST_MODIFIED =\n+      HEADER_PREFIX + Headers.LAST_MODIFIED;\n+\n+  public static final String XA_CONTENT_DISPOSITION =\n+      HEADER_PREFIX + Headers.CONTENT_DISPOSITION;\n+\n+  public static final String XA_CONTENT_ENCODING =\n+      HEADER_PREFIX + Headers.CONTENT_ENCODING;\n+\n+  public static final String XA_CONTENT_LANGUAGE =\n+      HEADER_PREFIX + Headers.CONTENT_LANGUAGE;\n+\n+  public static final String XA_CONTENT_MD5 =\n+      HEADER_PREFIX + Headers.CONTENT_MD5;\n+\n+  public static final String XA_CONTENT_RANGE =\n+      HEADER_PREFIX + Headers.CONTENT_RANGE;\n+\n+  public static final String XA_CONTENT_TYPE =\n+      HEADER_PREFIX + Headers.CONTENT_TYPE;\n+\n+  public static final String XA_ETAG = HEADER_PREFIX + Headers.ETAG;\n+\n+  public HeaderProcessing(final StoreContext storeContext) {\n+    super(storeContext);\n+  }\n+\n+  /**\n+   * Query the store, get all the headers into a map. Each Header\n+   * has the \"header.\" prefix.\n+   * Caller must have read access.\n+   * The value of each header is the string value of the object\n+   * UTF-8 encoded.\n+   * @param path path of object.\n+   * @return the headers\n+   * @throws IOException failure, including file not found.\n+   */\n+  private Map<String, byte[]> retrieveHeaders(Path path) throws IOException {\n+    StoreContext context = getStoreContext();\n+    ObjectMetadata md = context.getContextAccessors()\n+        .getObjectMetadata(path);\n+    Map<String, String> rawHeaders = md.getUserMetadata();\n+    Map<String, byte[]> headers = new TreeMap<>();\n+    rawHeaders.forEach((key, value) ->\n+        headers.put(HEADER_PREFIX + key, encodeBytes(value)));\n+    // and add the usual content length &c, if set\n+    headers.put(XA_CONTENT_DISPOSITION,\n+        encodeBytes(md.getContentDisposition()));\n+    headers.put(XA_CONTENT_ENCODING,\n+        encodeBytes(md.getContentEncoding()));\n+    headers.put(XA_CONTENT_LANGUAGE,\n+        encodeBytes(md.getContentLanguage()));\n+    headers.put(XA_CONTENT_LENGTH,\n+        encodeBytes(md.getContentLength()));\n+    headers.put(\n+        XA_CONTENT_MD5,\n+        encodeBytes(md.getContentMD5()));\n+    headers.put(XA_CONTENT_RANGE,\n+        encodeBytes(md.getContentRange()));\n+    headers.put(XA_CONTENT_TYPE,\n+        encodeBytes(md.getContentType()));\n+    headers.put(XA_ETAG,\n+        encodeBytes(md.getETag()));\n+    headers.put(XA_LAST_MODIFIED,\n+        encodeBytes(md.getLastModified()));\n+    return headers;\n+  }\n+\n+  /**\n+   * Stringify an object and return its bytes in UTF-8 encoding.\n+   * @param s source\n+   * @return encoded object or null\n+   */\n+  public static byte[] encodeBytes(Object s) {\n+    return s == null\n+        ? EMPTY\n+        : s.toString().getBytes(StandardCharsets.UTF_8);\n+  }\n+\n+  /**\n+   * Get the string value from the bytes.\n+   * if null : return null, otherwise the UTF-8 decoded\n+   * bytes.\n+   * @param bytes source bytes\n+   * @return decoded value\n+   */\n+  public static String decodeBytes(byte[] bytes) {\n+    return bytes == null\n+        ? null\n+        : new String(bytes, StandardCharsets.UTF_8);\n+  }\n+\n+  /**\n+   * Get an XAttr name and value for a file or directory.\n+   * @param path Path to get extended attribute\n+   * @param name XAttr name.\n+   * @return byte[] XAttr value or null\n+   * @throws IOException IO failure\n+   * @throws UnsupportedOperationException if the operation is unsupported\n+   *         (default outcome).\n+   */\n+  public byte[] getXAttr(Path path, String name) throws IOException {\n+    return retrieveHeaders(path).get(name);\n+  }\n+\n+  /**\n+   * See {@code FileSystem.getXAttrs(path}.\n+   *\n+   * @param path Path to get extended attributes\n+   * @return Map describing the XAttrs of the file or directory\n+   * @throws IOException IO failure\n+   * @throws UnsupportedOperationException if the operation is unsupported\n+   *         (default outcome).\n+   */\n+  public Map<String, byte[]> getXAttrs(Path path) throws IOException {\n+    return retrieveHeaders(path);\n+  }\n+\n+  /**\n+   * See {@code FileSystem.listXAttrs(path)}.\n+   * @param path Path to get extended attributes\n+   * @return List of supported XAttrs\n+   * @throws IOException IO failure\n+   */\n+  public List<String> listXAttrs(final Path path) throws IOException {\n+    return new ArrayList<>(retrieveHeaders(path).keySet());\n+  }\n+\n+  /**\n+   * See {@code FileSystem.getXAttrs(path, names}.\n+   * @param path Path to get extended attributes\n+   * @param names XAttr names.\n+   * @return Map describing the XAttrs of the file or directory\n+   * @throws IOException IO failure\n+   */\n+  public Map<String, byte[]> getXAttrs(Path path, List<String> names)\n+      throws IOException {\n+    Map<String, byte[]> headers = retrieveHeaders(path);\n+    Map<String, byte[]> result = new TreeMap<>();\n+    headers.entrySet().stream()\n+        .filter(entry -> names.contains(entry.getKey()))\n+        .forEach(entry -> result.put(entry.getKey(), entry.getValue()));\n+    return result;\n+  }\n+\n+  /**\n+   * Convert an XAttr byte array to a long.\n+   * testability.\n+   * @param data data to parse\n+   * @return either a length or none\n+   */\n+  public static Optional<Long> extractXAttrLongValue(byte[] data) {\n+    String xAttr;\n+    xAttr = HeaderProcessing.decodeBytes(data);\n+    if (StringUtils.isNotEmpty(xAttr)) {\n+      try {\n+        long l = Long.parseLong(xAttr);\n+        if (l >= 0) {\n+          return Optional.of(l);\n+        }\n+      } catch (NumberFormatException ex) {\n+        LOG.warn(\"Not a number: {}\", xAttr);\n+      }\n+    }\n+    // missing/empty header or parse failure.\n+    return Optional.empty();\n+  }\n+\n+  /**\n+   * Creates a copy of the passed {@link ObjectMetadata}.\n+   * Does so without using the {@link ObjectMetadata#clone()} method,\n+   * to avoid copying unnecessary headers.\n+   * This operation does not copy the {@code X_HEADER_MAGIC_MARKER}\n+   * header to avoid confusion. If a marker file is renamed,\n+   * it loses information about any remapped file.\n+   * @param source the {@link ObjectMetadata} to copy\n+   * @param ret the metadata to update; this is the return value.\n+   * @return a copy of {@link ObjectMetadata} with only relevant attributes", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "732cceb01a8c2c5db8ed669bb4cf5086dd101b03"}, "originalPosition": 241}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDI3MDA0MQ==", "bodyText": "I also wonder if this (along with the documentation change) is required for this PR.", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r554270041", "createdAt": "2021-01-09T01:26:01Z", "author": {"login": "sunchao"}, "path": "hadoop-common-project/hadoop-common/src/main/resources/core-default.xml", "diffHunk": "@@ -1873,11 +1873,9 @@\n \n <property>\n   <name>fs.s3a.committer.magic.enabled</name>\n-  <value>false</value>\n+  <value>true</value>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mzg3MDczMw=="}, "originalCommit": {"oid": "732cceb01a8c2c5db8ed669bb4cf5086dd101b03"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDI3MDIyMw==", "bodyText": "nit: is \"Created on demand\" accurate? it is created in initialize.", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r554270223", "createdAt": "2021-01-09T01:26:58Z", "author": {"login": "sunchao"}, "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java", "diffHunk": "@@ -330,6 +331,11 @@\n    */\n   private DirectoryPolicy directoryPolicy;\n \n+  /**\n+   * Header processing for XAttr. Created on demand.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "732cceb01a8c2c5db8ed669bb4cf5086dd101b03"}, "originalPosition": 13}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY1MDg3MjY1", "url": "https://github.com/apache/hadoop/pull/2530#pullrequestreview-565087265", "createdAt": "2021-01-11T06:14:25Z", "commit": {"oid": "732cceb01a8c2c5db8ed669bb4cf5086dd101b03"}, "state": "COMMENTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMVQwNjoxNDoyNVrOIRE9YA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMVQwNzoxMzozOFrOIRHwhw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDc3Nzk1Mg==", "bodyText": "I'm +1 on enabling this by default as it does not depend on S3Guard any more. I totally agree we should update the JIRA release notes or commit message to indicate this default config change. Or ideally a separate PR.", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r554777952", "createdAt": "2021-01-11T06:14:25Z", "author": {"login": "liuml07"}, "path": "hadoop-common-project/hadoop-common/src/main/resources/core-default.xml", "diffHunk": "@@ -1873,11 +1873,9 @@\n \n <property>\n   <name>fs.s3a.committer.magic.enabled</name>\n-  <value>false</value>\n+  <value>true</value>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mzg3MDczMw=="}, "originalCommit": {"oid": "732cceb01a8c2c5db8ed669bb4cf5086dd101b03"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDc3OTU3NQ==", "bodyText": "nit: is XA_HEADER_PREFIX a bit clearer name?", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r554779575", "createdAt": "2021-01-11T06:16:34Z", "author": {"login": "liuml07"}, "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Constants.java", "diffHunk": "@@ -1048,4 +1048,10 @@ private Constants() {\n   public static final String STORE_CAPABILITY_DIRECTORY_MARKER_ACTION_DELETE\n       = \"fs.s3a.capability.directory.marker.action.delete\";\n \n+  /**\n+   * To comply with the XAttr rules, all headers of the object retrieved\n+   * through the getXAttr APIs have the prefix: {@value}.\n+   */\n+  public static final String HEADER_PREFIX = \"header.\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "732cceb01a8c2c5db8ed669bb4cf5086dd101b03"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDc4NjkxMg==", "bodyText": "nit: seems better if we move those comments above into the implementation method HeaderProcessing#cloneObjectMetadata\n// This approach may be too brittle, especially if\n// in future there are new attributes added to ObjectMetadata\n// that we do not explicitly call to set here", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r554786912", "createdAt": "2021-01-11T06:26:19Z", "author": {"login": "liuml07"}, "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java", "diffHunk": "@@ -4103,56 +4111,7 @@ private ObjectMetadata cloneObjectMetadata(ObjectMetadata source) {\n     // in future there are new attributes added to ObjectMetadata\n     // that we do not explicitly call to set here\n     ObjectMetadata ret = newObjectMetadata(source.getContentLength());\n-\n-    // Possibly null attributes\n-    // Allowing nulls to pass breaks it during later use\n-    if (source.getCacheControl() != null) {\n-      ret.setCacheControl(source.getCacheControl());\n-    }\n-    if (source.getContentDisposition() != null) {\n-      ret.setContentDisposition(source.getContentDisposition());\n-    }\n-    if (source.getContentEncoding() != null) {\n-      ret.setContentEncoding(source.getContentEncoding());\n-    }\n-    if (source.getContentMD5() != null) {\n-      ret.setContentMD5(source.getContentMD5());\n-    }\n-    if (source.getContentType() != null) {\n-      ret.setContentType(source.getContentType());\n-    }\n-    if (source.getExpirationTime() != null) {\n-      ret.setExpirationTime(source.getExpirationTime());\n-    }\n-    if (source.getExpirationTimeRuleId() != null) {\n-      ret.setExpirationTimeRuleId(source.getExpirationTimeRuleId());\n-    }\n-    if (source.getHttpExpiresDate() != null) {\n-      ret.setHttpExpiresDate(source.getHttpExpiresDate());\n-    }\n-    if (source.getLastModified() != null) {\n-      ret.setLastModified(source.getLastModified());\n-    }\n-    if (source.getOngoingRestore() != null) {\n-      ret.setOngoingRestore(source.getOngoingRestore());\n-    }\n-    if (source.getRestoreExpirationTime() != null) {\n-      ret.setRestoreExpirationTime(source.getRestoreExpirationTime());\n-    }\n-    if (source.getSSEAlgorithm() != null) {\n-      ret.setSSEAlgorithm(source.getSSEAlgorithm());\n-    }\n-    if (source.getSSECustomerAlgorithm() != null) {\n-      ret.setSSECustomerAlgorithm(source.getSSECustomerAlgorithm());\n-    }\n-    if (source.getSSECustomerKeyMd5() != null) {\n-      ret.setSSECustomerKeyMd5(source.getSSECustomerKeyMd5());\n-    }\n-\n-    for (Map.Entry<String, String> e : source.getUserMetadata().entrySet()) {\n-      ret.addUserMetadata(e.getKey(), e.getValue());\n-    }\n-    return ret;\n+    return getHeaderProcessing().cloneObjectMetadata(source, ret);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "732cceb01a8c2c5db8ed669bb4cf5086dd101b03"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDc4ODk0OA==", "bodyText": "nit: replace headerProcessing with private getHeaderProcessing() as the getXattrs method?", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r554788948", "createdAt": "2021-01-11T06:28:43Z", "author": {"login": "liuml07"}, "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java", "diffHunk": "@@ -4382,6 +4341,37 @@ public EtagChecksum getFileChecksum(Path f, final long length)\n     }\n   }\n \n+  /**\n+   * Get header processing support.\n+   * @return the header processing of this instance.\n+   */\n+  private HeaderProcessing getHeaderProcessing() {\n+    return headerProcessing;\n+  }\n+\n+  @Override\n+  public byte[] getXAttr(final Path path, final String name)\n+      throws IOException {\n+    return getHeaderProcessing().getXAttr(path, name);\n+  }\n+\n+  @Override\n+  public Map<String, byte[]> getXAttrs(final Path path) throws IOException {\n+    return getHeaderProcessing().getXAttrs(path);\n+  }\n+\n+  @Override\n+  public Map<String, byte[]> getXAttrs(final Path path,\n+      final List<String> names)\n+      throws IOException {\n+    return getHeaderProcessing().getXAttrs(path, names);\n+  }\n+\n+  @Override\n+  public List<String> listXAttrs(final Path path) throws IOException {\n+    return headerProcessing.listXAttrs(path);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "732cceb01a8c2c5db8ed669bb4cf5086dd101b03"}, "originalPosition": 136}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDc5ODQ4OQ==", "bodyText": "will never return null, but could return empty byte array?", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r554798489", "createdAt": "2021-01-11T06:41:13Z", "author": {"login": "liuml07"}, "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/HeaderProcessing.java", "diffHunk": "@@ -0,0 +1,300 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3a.impl;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.TreeMap;\n+\n+import com.amazonaws.services.s3.Headers;\n+import com.amazonaws.services.s3.model.ObjectMetadata;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.apache.hadoop.fs.s3a.Constants.HEADER_PREFIX;\n+import static org.apache.hadoop.fs.s3a.commit.CommitConstants.X_HEADER_MAGIC_MARKER;\n+\n+/**\n+ * Part of the S3A FS where object headers are\n+ * processed.\n+ * Implements all the various XAttr read operations.\n+ * Those APIs all expect byte arrays back.\n+ * Metadata cloning is also implemented here, so as\n+ * to stay in sync with custom header logic.\n+ */\n+public class HeaderProcessing extends AbstractStoreOperation {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      HeaderProcessing.class);\n+\n+  private static final byte[] EMPTY = new byte[0];\n+\n+  /**\n+   * Length XAttr.\n+   */\n+  public static final String XA_CONTENT_LENGTH =\n+      HEADER_PREFIX + Headers.CONTENT_LENGTH;\n+\n+  /**\n+   * last modified XAttr.\n+   */\n+  public static final String XA_LAST_MODIFIED =\n+      HEADER_PREFIX + Headers.LAST_MODIFIED;\n+\n+  public static final String XA_CONTENT_DISPOSITION =\n+      HEADER_PREFIX + Headers.CONTENT_DISPOSITION;\n+\n+  public static final String XA_CONTENT_ENCODING =\n+      HEADER_PREFIX + Headers.CONTENT_ENCODING;\n+\n+  public static final String XA_CONTENT_LANGUAGE =\n+      HEADER_PREFIX + Headers.CONTENT_LANGUAGE;\n+\n+  public static final String XA_CONTENT_MD5 =\n+      HEADER_PREFIX + Headers.CONTENT_MD5;\n+\n+  public static final String XA_CONTENT_RANGE =\n+      HEADER_PREFIX + Headers.CONTENT_RANGE;\n+\n+  public static final String XA_CONTENT_TYPE =\n+      HEADER_PREFIX + Headers.CONTENT_TYPE;\n+\n+  public static final String XA_ETAG = HEADER_PREFIX + Headers.ETAG;\n+\n+  public HeaderProcessing(final StoreContext storeContext) {\n+    super(storeContext);\n+  }\n+\n+  /**\n+   * Query the store, get all the headers into a map. Each Header\n+   * has the \"header.\" prefix.\n+   * Caller must have read access.\n+   * The value of each header is the string value of the object\n+   * UTF-8 encoded.\n+   * @param path path of object.\n+   * @return the headers\n+   * @throws IOException failure, including file not found.\n+   */\n+  private Map<String, byte[]> retrieveHeaders(Path path) throws IOException {\n+    StoreContext context = getStoreContext();\n+    ObjectMetadata md = context.getContextAccessors()\n+        .getObjectMetadata(path);\n+    Map<String, String> rawHeaders = md.getUserMetadata();\n+    Map<String, byte[]> headers = new TreeMap<>();\n+    rawHeaders.forEach((key, value) ->\n+        headers.put(HEADER_PREFIX + key, encodeBytes(value)));\n+    // and add the usual content length &c, if set\n+    headers.put(XA_CONTENT_DISPOSITION,\n+        encodeBytes(md.getContentDisposition()));\n+    headers.put(XA_CONTENT_ENCODING,\n+        encodeBytes(md.getContentEncoding()));\n+    headers.put(XA_CONTENT_LANGUAGE,\n+        encodeBytes(md.getContentLanguage()));\n+    headers.put(XA_CONTENT_LENGTH,\n+        encodeBytes(md.getContentLength()));\n+    headers.put(\n+        XA_CONTENT_MD5,\n+        encodeBytes(md.getContentMD5()));\n+    headers.put(XA_CONTENT_RANGE,\n+        encodeBytes(md.getContentRange()));\n+    headers.put(XA_CONTENT_TYPE,\n+        encodeBytes(md.getContentType()));\n+    headers.put(XA_ETAG,\n+        encodeBytes(md.getETag()));\n+    headers.put(XA_LAST_MODIFIED,\n+        encodeBytes(md.getLastModified()));\n+    return headers;\n+  }\n+\n+  /**\n+   * Stringify an object and return its bytes in UTF-8 encoding.\n+   * @param s source\n+   * @return encoded object or null", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "732cceb01a8c2c5db8ed669bb4cf5086dd101b03"}, "originalPosition": 135}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDgxNjk5Ng==", "bodyText": "Is it useful to print ex? Or along with the line LOG.debug(\"Fail to parse {} to long with exception\", xAttr, ex)?", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r554816996", "createdAt": "2021-01-11T07:04:45Z", "author": {"login": "liuml07"}, "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/HeaderProcessing.java", "diffHunk": "@@ -0,0 +1,300 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3a.impl;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.TreeMap;\n+\n+import com.amazonaws.services.s3.Headers;\n+import com.amazonaws.services.s3.model.ObjectMetadata;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.apache.hadoop.fs.s3a.Constants.HEADER_PREFIX;\n+import static org.apache.hadoop.fs.s3a.commit.CommitConstants.X_HEADER_MAGIC_MARKER;\n+\n+/**\n+ * Part of the S3A FS where object headers are\n+ * processed.\n+ * Implements all the various XAttr read operations.\n+ * Those APIs all expect byte arrays back.\n+ * Metadata cloning is also implemented here, so as\n+ * to stay in sync with custom header logic.\n+ */\n+public class HeaderProcessing extends AbstractStoreOperation {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      HeaderProcessing.class);\n+\n+  private static final byte[] EMPTY = new byte[0];\n+\n+  /**\n+   * Length XAttr.\n+   */\n+  public static final String XA_CONTENT_LENGTH =\n+      HEADER_PREFIX + Headers.CONTENT_LENGTH;\n+\n+  /**\n+   * last modified XAttr.\n+   */\n+  public static final String XA_LAST_MODIFIED =\n+      HEADER_PREFIX + Headers.LAST_MODIFIED;\n+\n+  public static final String XA_CONTENT_DISPOSITION =\n+      HEADER_PREFIX + Headers.CONTENT_DISPOSITION;\n+\n+  public static final String XA_CONTENT_ENCODING =\n+      HEADER_PREFIX + Headers.CONTENT_ENCODING;\n+\n+  public static final String XA_CONTENT_LANGUAGE =\n+      HEADER_PREFIX + Headers.CONTENT_LANGUAGE;\n+\n+  public static final String XA_CONTENT_MD5 =\n+      HEADER_PREFIX + Headers.CONTENT_MD5;\n+\n+  public static final String XA_CONTENT_RANGE =\n+      HEADER_PREFIX + Headers.CONTENT_RANGE;\n+\n+  public static final String XA_CONTENT_TYPE =\n+      HEADER_PREFIX + Headers.CONTENT_TYPE;\n+\n+  public static final String XA_ETAG = HEADER_PREFIX + Headers.ETAG;\n+\n+  public HeaderProcessing(final StoreContext storeContext) {\n+    super(storeContext);\n+  }\n+\n+  /**\n+   * Query the store, get all the headers into a map. Each Header\n+   * has the \"header.\" prefix.\n+   * Caller must have read access.\n+   * The value of each header is the string value of the object\n+   * UTF-8 encoded.\n+   * @param path path of object.\n+   * @return the headers\n+   * @throws IOException failure, including file not found.\n+   */\n+  private Map<String, byte[]> retrieveHeaders(Path path) throws IOException {\n+    StoreContext context = getStoreContext();\n+    ObjectMetadata md = context.getContextAccessors()\n+        .getObjectMetadata(path);\n+    Map<String, String> rawHeaders = md.getUserMetadata();\n+    Map<String, byte[]> headers = new TreeMap<>();\n+    rawHeaders.forEach((key, value) ->\n+        headers.put(HEADER_PREFIX + key, encodeBytes(value)));\n+    // and add the usual content length &c, if set\n+    headers.put(XA_CONTENT_DISPOSITION,\n+        encodeBytes(md.getContentDisposition()));\n+    headers.put(XA_CONTENT_ENCODING,\n+        encodeBytes(md.getContentEncoding()));\n+    headers.put(XA_CONTENT_LANGUAGE,\n+        encodeBytes(md.getContentLanguage()));\n+    headers.put(XA_CONTENT_LENGTH,\n+        encodeBytes(md.getContentLength()));\n+    headers.put(\n+        XA_CONTENT_MD5,\n+        encodeBytes(md.getContentMD5()));\n+    headers.put(XA_CONTENT_RANGE,\n+        encodeBytes(md.getContentRange()));\n+    headers.put(XA_CONTENT_TYPE,\n+        encodeBytes(md.getContentType()));\n+    headers.put(XA_ETAG,\n+        encodeBytes(md.getETag()));\n+    headers.put(XA_LAST_MODIFIED,\n+        encodeBytes(md.getLastModified()));\n+    return headers;\n+  }\n+\n+  /**\n+   * Stringify an object and return its bytes in UTF-8 encoding.\n+   * @param s source\n+   * @return encoded object or null\n+   */\n+  public static byte[] encodeBytes(Object s) {\n+    return s == null\n+        ? EMPTY\n+        : s.toString().getBytes(StandardCharsets.UTF_8);\n+  }\n+\n+  /**\n+   * Get the string value from the bytes.\n+   * if null : return null, otherwise the UTF-8 decoded\n+   * bytes.\n+   * @param bytes source bytes\n+   * @return decoded value\n+   */\n+  public static String decodeBytes(byte[] bytes) {\n+    return bytes == null\n+        ? null\n+        : new String(bytes, StandardCharsets.UTF_8);\n+  }\n+\n+  /**\n+   * Get an XAttr name and value for a file or directory.\n+   * @param path Path to get extended attribute\n+   * @param name XAttr name.\n+   * @return byte[] XAttr value or null\n+   * @throws IOException IO failure\n+   * @throws UnsupportedOperationException if the operation is unsupported\n+   *         (default outcome).\n+   */\n+  public byte[] getXAttr(Path path, String name) throws IOException {\n+    return retrieveHeaders(path).get(name);\n+  }\n+\n+  /**\n+   * See {@code FileSystem.getXAttrs(path}.\n+   *\n+   * @param path Path to get extended attributes\n+   * @return Map describing the XAttrs of the file or directory\n+   * @throws IOException IO failure\n+   * @throws UnsupportedOperationException if the operation is unsupported\n+   *         (default outcome).\n+   */\n+  public Map<String, byte[]> getXAttrs(Path path) throws IOException {\n+    return retrieveHeaders(path);\n+  }\n+\n+  /**\n+   * See {@code FileSystem.listXAttrs(path)}.\n+   * @param path Path to get extended attributes\n+   * @return List of supported XAttrs\n+   * @throws IOException IO failure\n+   */\n+  public List<String> listXAttrs(final Path path) throws IOException {\n+    return new ArrayList<>(retrieveHeaders(path).keySet());\n+  }\n+\n+  /**\n+   * See {@code FileSystem.getXAttrs(path, names}.\n+   * @param path Path to get extended attributes\n+   * @param names XAttr names.\n+   * @return Map describing the XAttrs of the file or directory\n+   * @throws IOException IO failure\n+   */\n+  public Map<String, byte[]> getXAttrs(Path path, List<String> names)\n+      throws IOException {\n+    Map<String, byte[]> headers = retrieveHeaders(path);\n+    Map<String, byte[]> result = new TreeMap<>();\n+    headers.entrySet().stream()\n+        .filter(entry -> names.contains(entry.getKey()))\n+        .forEach(entry -> result.put(entry.getKey(), entry.getValue()));\n+    return result;\n+  }\n+\n+  /**\n+   * Convert an XAttr byte array to a long.\n+   * testability.\n+   * @param data data to parse\n+   * @return either a length or none\n+   */\n+  public static Optional<Long> extractXAttrLongValue(byte[] data) {\n+    String xAttr;\n+    xAttr = HeaderProcessing.decodeBytes(data);\n+    if (StringUtils.isNotEmpty(xAttr)) {\n+      try {\n+        long l = Long.parseLong(xAttr);\n+        if (l >= 0) {\n+          return Optional.of(l);\n+        }\n+      } catch (NumberFormatException ex) {\n+        LOG.warn(\"Not a number: {}\", xAttr);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "732cceb01a8c2c5db8ed669bb4cf5086dd101b03"}, "originalPosition": 225}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDgxNzg2Ng==", "bodyText": "nit: I know this is based on existing code...but Java 8 stream?", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r554817866", "createdAt": "2021-01-11T07:05:52Z", "author": {"login": "liuml07"}, "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/HeaderProcessing.java", "diffHunk": "@@ -0,0 +1,300 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3a.impl;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.TreeMap;\n+\n+import com.amazonaws.services.s3.Headers;\n+import com.amazonaws.services.s3.model.ObjectMetadata;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.apache.hadoop.fs.s3a.Constants.HEADER_PREFIX;\n+import static org.apache.hadoop.fs.s3a.commit.CommitConstants.X_HEADER_MAGIC_MARKER;\n+\n+/**\n+ * Part of the S3A FS where object headers are\n+ * processed.\n+ * Implements all the various XAttr read operations.\n+ * Those APIs all expect byte arrays back.\n+ * Metadata cloning is also implemented here, so as\n+ * to stay in sync with custom header logic.\n+ */\n+public class HeaderProcessing extends AbstractStoreOperation {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      HeaderProcessing.class);\n+\n+  private static final byte[] EMPTY = new byte[0];\n+\n+  /**\n+   * Length XAttr.\n+   */\n+  public static final String XA_CONTENT_LENGTH =\n+      HEADER_PREFIX + Headers.CONTENT_LENGTH;\n+\n+  /**\n+   * last modified XAttr.\n+   */\n+  public static final String XA_LAST_MODIFIED =\n+      HEADER_PREFIX + Headers.LAST_MODIFIED;\n+\n+  public static final String XA_CONTENT_DISPOSITION =\n+      HEADER_PREFIX + Headers.CONTENT_DISPOSITION;\n+\n+  public static final String XA_CONTENT_ENCODING =\n+      HEADER_PREFIX + Headers.CONTENT_ENCODING;\n+\n+  public static final String XA_CONTENT_LANGUAGE =\n+      HEADER_PREFIX + Headers.CONTENT_LANGUAGE;\n+\n+  public static final String XA_CONTENT_MD5 =\n+      HEADER_PREFIX + Headers.CONTENT_MD5;\n+\n+  public static final String XA_CONTENT_RANGE =\n+      HEADER_PREFIX + Headers.CONTENT_RANGE;\n+\n+  public static final String XA_CONTENT_TYPE =\n+      HEADER_PREFIX + Headers.CONTENT_TYPE;\n+\n+  public static final String XA_ETAG = HEADER_PREFIX + Headers.ETAG;\n+\n+  public HeaderProcessing(final StoreContext storeContext) {\n+    super(storeContext);\n+  }\n+\n+  /**\n+   * Query the store, get all the headers into a map. Each Header\n+   * has the \"header.\" prefix.\n+   * Caller must have read access.\n+   * The value of each header is the string value of the object\n+   * UTF-8 encoded.\n+   * @param path path of object.\n+   * @return the headers\n+   * @throws IOException failure, including file not found.\n+   */\n+  private Map<String, byte[]> retrieveHeaders(Path path) throws IOException {\n+    StoreContext context = getStoreContext();\n+    ObjectMetadata md = context.getContextAccessors()\n+        .getObjectMetadata(path);\n+    Map<String, String> rawHeaders = md.getUserMetadata();\n+    Map<String, byte[]> headers = new TreeMap<>();\n+    rawHeaders.forEach((key, value) ->\n+        headers.put(HEADER_PREFIX + key, encodeBytes(value)));\n+    // and add the usual content length &c, if set\n+    headers.put(XA_CONTENT_DISPOSITION,\n+        encodeBytes(md.getContentDisposition()));\n+    headers.put(XA_CONTENT_ENCODING,\n+        encodeBytes(md.getContentEncoding()));\n+    headers.put(XA_CONTENT_LANGUAGE,\n+        encodeBytes(md.getContentLanguage()));\n+    headers.put(XA_CONTENT_LENGTH,\n+        encodeBytes(md.getContentLength()));\n+    headers.put(\n+        XA_CONTENT_MD5,\n+        encodeBytes(md.getContentMD5()));\n+    headers.put(XA_CONTENT_RANGE,\n+        encodeBytes(md.getContentRange()));\n+    headers.put(XA_CONTENT_TYPE,\n+        encodeBytes(md.getContentType()));\n+    headers.put(XA_ETAG,\n+        encodeBytes(md.getETag()));\n+    headers.put(XA_LAST_MODIFIED,\n+        encodeBytes(md.getLastModified()));\n+    return headers;\n+  }\n+\n+  /**\n+   * Stringify an object and return its bytes in UTF-8 encoding.\n+   * @param s source\n+   * @return encoded object or null\n+   */\n+  public static byte[] encodeBytes(Object s) {\n+    return s == null\n+        ? EMPTY\n+        : s.toString().getBytes(StandardCharsets.UTF_8);\n+  }\n+\n+  /**\n+   * Get the string value from the bytes.\n+   * if null : return null, otherwise the UTF-8 decoded\n+   * bytes.\n+   * @param bytes source bytes\n+   * @return decoded value\n+   */\n+  public static String decodeBytes(byte[] bytes) {\n+    return bytes == null\n+        ? null\n+        : new String(bytes, StandardCharsets.UTF_8);\n+  }\n+\n+  /**\n+   * Get an XAttr name and value for a file or directory.\n+   * @param path Path to get extended attribute\n+   * @param name XAttr name.\n+   * @return byte[] XAttr value or null\n+   * @throws IOException IO failure\n+   * @throws UnsupportedOperationException if the operation is unsupported\n+   *         (default outcome).\n+   */\n+  public byte[] getXAttr(Path path, String name) throws IOException {\n+    return retrieveHeaders(path).get(name);\n+  }\n+\n+  /**\n+   * See {@code FileSystem.getXAttrs(path}.\n+   *\n+   * @param path Path to get extended attributes\n+   * @return Map describing the XAttrs of the file or directory\n+   * @throws IOException IO failure\n+   * @throws UnsupportedOperationException if the operation is unsupported\n+   *         (default outcome).\n+   */\n+  public Map<String, byte[]> getXAttrs(Path path) throws IOException {\n+    return retrieveHeaders(path);\n+  }\n+\n+  /**\n+   * See {@code FileSystem.listXAttrs(path)}.\n+   * @param path Path to get extended attributes\n+   * @return List of supported XAttrs\n+   * @throws IOException IO failure\n+   */\n+  public List<String> listXAttrs(final Path path) throws IOException {\n+    return new ArrayList<>(retrieveHeaders(path).keySet());\n+  }\n+\n+  /**\n+   * See {@code FileSystem.getXAttrs(path, names}.\n+   * @param path Path to get extended attributes\n+   * @param names XAttr names.\n+   * @return Map describing the XAttrs of the file or directory\n+   * @throws IOException IO failure\n+   */\n+  public Map<String, byte[]> getXAttrs(Path path, List<String> names)\n+      throws IOException {\n+    Map<String, byte[]> headers = retrieveHeaders(path);\n+    Map<String, byte[]> result = new TreeMap<>();\n+    headers.entrySet().stream()\n+        .filter(entry -> names.contains(entry.getKey()))\n+        .forEach(entry -> result.put(entry.getKey(), entry.getValue()));\n+    return result;\n+  }\n+\n+  /**\n+   * Convert an XAttr byte array to a long.\n+   * testability.\n+   * @param data data to parse\n+   * @return either a length or none\n+   */\n+  public static Optional<Long> extractXAttrLongValue(byte[] data) {\n+    String xAttr;\n+    xAttr = HeaderProcessing.decodeBytes(data);\n+    if (StringUtils.isNotEmpty(xAttr)) {\n+      try {\n+        long l = Long.parseLong(xAttr);\n+        if (l >= 0) {\n+          return Optional.of(l);\n+        }\n+      } catch (NumberFormatException ex) {\n+        LOG.warn(\"Not a number: {}\", xAttr);\n+      }\n+    }\n+    // missing/empty header or parse failure.\n+    return Optional.empty();\n+  }\n+\n+  /**\n+   * Creates a copy of the passed {@link ObjectMetadata}.\n+   * Does so without using the {@link ObjectMetadata#clone()} method,\n+   * to avoid copying unnecessary headers.\n+   * This operation does not copy the {@code X_HEADER_MAGIC_MARKER}\n+   * header to avoid confusion. If a marker file is renamed,\n+   * it loses information about any remapped file.\n+   * @param source the {@link ObjectMetadata} to copy\n+   * @param ret the metadata to update; this is the return value.\n+   * @return a copy of {@link ObjectMetadata} with only relevant attributes\n+   */\n+  public ObjectMetadata cloneObjectMetadata(ObjectMetadata source,\n+      ObjectMetadata ret) {\n+\n+    // Possibly null attributes\n+    // Allowing nulls to pass breaks it during later use\n+    if (source.getCacheControl() != null) {\n+      ret.setCacheControl(source.getCacheControl());\n+    }\n+    if (source.getContentDisposition() != null) {\n+      ret.setContentDisposition(source.getContentDisposition());\n+    }\n+    if (source.getContentEncoding() != null) {\n+      ret.setContentEncoding(source.getContentEncoding());\n+    }\n+    if (source.getContentMD5() != null) {\n+      ret.setContentMD5(source.getContentMD5());\n+    }\n+    if (source.getContentType() != null) {\n+      ret.setContentType(source.getContentType());\n+    }\n+    if (source.getExpirationTime() != null) {\n+      ret.setExpirationTime(source.getExpirationTime());\n+    }\n+    if (source.getExpirationTimeRuleId() != null) {\n+      ret.setExpirationTimeRuleId(source.getExpirationTimeRuleId());\n+    }\n+    if (source.getHttpExpiresDate() != null) {\n+      ret.setHttpExpiresDate(source.getHttpExpiresDate());\n+    }\n+    if (source.getLastModified() != null) {\n+      ret.setLastModified(source.getLastModified());\n+    }\n+    if (source.getOngoingRestore() != null) {\n+      ret.setOngoingRestore(source.getOngoingRestore());\n+    }\n+    if (source.getRestoreExpirationTime() != null) {\n+      ret.setRestoreExpirationTime(source.getRestoreExpirationTime());\n+    }\n+    if (source.getSSEAlgorithm() != null) {\n+      ret.setSSEAlgorithm(source.getSSEAlgorithm());\n+    }\n+    if (source.getSSECustomerAlgorithm() != null) {\n+      ret.setSSECustomerAlgorithm(source.getSSECustomerAlgorithm());\n+    }\n+    if (source.getSSECustomerKeyMd5() != null) {\n+      ret.setSSECustomerKeyMd5(source.getSSECustomerKeyMd5());\n+    }\n+\n+    // copy user metadata except the magic marker header.\n+    for (Map.Entry<String, String> e : source.getUserMetadata().entrySet()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "732cceb01a8c2c5db8ed669bb4cf5086dd101b03"}, "originalPosition": 292}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDgyMzgxNQ==", "bodyText": "you mean x-hadoop-s3a-magic-data-length here?", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r554823815", "createdAt": "2021-01-11T07:13:38Z", "author": {"login": "liuml07"}, "path": "hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/committer_architecture.md", "diffHunk": "@@ -1312,6 +1312,16 @@ On `close()`, summary data would be written to the file\n `/results/latest/__magic/job400_1/task_01_01/latest.orc.lzo.pending`.\n This would contain the upload ID and all the parts and etags of uploaded data.\n \n+A marker file is also created, so that code which verifies that a newly created file\n+exists does not fail.\n+1. These marker files are zero bytes long.\n+1. They declare the full length of the final file in the HTTP header\n+   `x-hadoop-s3a-magic-marker`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "732cceb01a8c2c5db8ed669bb4cf5086dd101b03"}, "originalPosition": 8}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c45a7a927d24b46bd988bbe68e4c8369c8d7f1e8", "author": {"user": {"login": "steveloughran", "name": "Steve Loughran"}}, "url": "https://github.com/apache/hadoop/commit/c45a7a927d24b46bd988bbe68e4c8369c8d7f1e8", "committedDate": "2021-01-12T15:28:13Z", "message": "HADOOP-17414. Minimising headers returned as XAttrs; directory support\n\n* Only headers returned are served up.\n* Assertions on value of content type for file and /\n* test to verify failure mode when called on missing objects\n* test to call XAttr on a dir -showed that more work was needed\n* cleaned up S3A entry points in porocess of that work\n\nChange-Id: Ib4911240325a56a3853632b411d84c3c99897a64"}, "afterCommit": {"oid": "f4f0fb77806eaafcb9388ea31cf19ff136ded02e", "author": {"user": {"login": "steveloughran", "name": "Steve Loughran"}}, "url": "https://github.com/apache/hadoop/commit/f4f0fb77806eaafcb9388ea31cf19ff136ded02e", "committedDate": "2021-01-13T16:14:14Z", "message": "HADOOP-17414. Minimising headers returned as XAttrs; directory support\n\n* Only headers returned are served up.\n* Assertions on value of content type for file and /\n* test to verify failure mode when called on missing objects\n* test to call XAttr on a dir -showed that more work was needed\n* cleaned up S3A entry points in porocess of that work\n\nChange-Id: Ib4911240325a56a3853632b411d84c3c99897a64"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTc1Mzg4OTA4", "url": "https://github.com/apache/hadoop/pull/2530#pullrequestreview-575388908", "createdAt": "2021-01-25T13:22:03Z", "commit": {"oid": "df719281e19140096206f3281b6b63e4fd7c6562"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yNVQxMzoyMjowM1rOIZmnbA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yNVQxMzoyMjowM1rOIZmnbA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MzcxNzk5Ng==", "bodyText": "I hope removal of this is not a typo.", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r563717996", "createdAt": "2021-01-25T13:22:03Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java", "diffHunk": "@@ -1800,31 +1809,16 @@ public ObjectMetadata getObjectMetadata(Path path) throws IOException {\n    * @return metadata\n    * @throws IOException IO and object access problems.\n    */\n-  @VisibleForTesting\n   @Retries.RetryTranslated\n-  public ObjectMetadata getObjectMetadata(Path path,\n+  private ObjectMetadata getObjectMetadata(Path path,\n       ChangeTracker changeTracker, Invoker changeInvoker, String operation)\n       throws IOException {\n-    checkNotClosed();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "df719281e19140096206f3281b6b63e4fd7c6562"}, "originalPosition": 57}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5e6674829a53d5c5448e7d12500b6470eae3c8a0", "author": {"user": {"login": "steveloughran", "name": "Steve Loughran"}}, "url": "https://github.com/apache/hadoop/commit/5e6674829a53d5c5448e7d12500b6470eae3c8a0", "committedDate": "2021-01-25T15:20:19Z", "message": "HADOOP-17414. Magic committer files don't have the count of bytes written collected by spark\n\nSquashed commit.\n\nHADOOP-17414: review feedback, iostatistics and expanded headers\nHADOOP-17414. NPEs on test teardown.\nHADOOP-17414. Minimising headers returned as XAttrs; directory support\nHADOOP-17414. checkstyle and EOF\nHADOOP-17414. include stack trace when number parse fails\nHADOOP-17414. magic file size: revert setting fs.s3a.magic.enabled to true.\n\nReverted changing the default value of fs.s3a.magic.enabled as I've a bigger PR ripping it out entirely\n\nChange-Id: Id79a3c334ca875aab205195285182106d6a6e9e2"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "1cd21482cec5de64ac67a185941d896076f473db", "author": {"user": {"login": "steveloughran", "name": "Steve Loughran"}}, "url": "https://github.com/apache/hadoop/commit/1cd21482cec5de64ac67a185941d896076f473db", "committedDate": "2021-01-25T13:02:06Z", "message": "HADOOP-17414. magic file size: revert setting fs.s3a.magic.enabled to true.\n\nReverted changing the default value of fs.s3a.magic.enabled as I've a bigger PR ripping it out entirely\n\nChange-Id: Id79a3c334ca875aab205195285182106d6a6e9e2"}, "afterCommit": {"oid": "5e6674829a53d5c5448e7d12500b6470eae3c8a0", "author": {"user": {"login": "steveloughran", "name": "Steve Loughran"}}, "url": "https://github.com/apache/hadoop/commit/5e6674829a53d5c5448e7d12500b6470eae3c8a0", "committedDate": "2021-01-25T15:20:19Z", "message": "HADOOP-17414. Magic committer files don't have the count of bytes written collected by spark\n\nSquashed commit.\n\nHADOOP-17414: review feedback, iostatistics and expanded headers\nHADOOP-17414. NPEs on test teardown.\nHADOOP-17414. Minimising headers returned as XAttrs; directory support\nHADOOP-17414. checkstyle and EOF\nHADOOP-17414. include stack trace when number parse fails\nHADOOP-17414. magic file size: revert setting fs.s3a.magic.enabled to true.\n\nReverted changing the default value of fs.s3a.magic.enabled as I've a bigger PR ripping it out entirely\n\nChange-Id: Id79a3c334ca875aab205195285182106d6a6e9e2"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e3b8ee303618a774c6b2a85b576220dba4f79141", "author": {"user": {"login": "steveloughran", "name": "Steve Loughran"}}, "url": "https://github.com/apache/hadoop/commit/e3b8ee303618a774c6b2a85b576220dba4f79141", "committedDate": "2021-01-25T15:29:27Z", "message": "HADOOP-17414. Restore check on FS being open.\n\nMukund's review comments\n\nChange-Id: I7e31d5dcd336e15a9baf0640d4fa16b0999fb5a1"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTc2MTA1NTEz", "url": "https://github.com/apache/hadoop/pull/2530#pullrequestreview-576105513", "createdAt": "2021-01-26T08:13:39Z", "commit": {"oid": "e3b8ee303618a774c6b2a85b576220dba4f79141"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3402, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}