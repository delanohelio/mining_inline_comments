{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzU4OTM1NTQ1", "number": 1790, "title": "[HADOOP-16818] ABFS: Combine append+flush calls for blockblob & appendblob", "bodyText": "\u2026d support for appendblob\nNOTICE\nPlease create an issue in ASF JIRA before opening a pull request,\nand you need to set the title of the pull request which starts with\nthe corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.)\nFor more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute", "createdAt": "2020-01-03T10:09:54Z", "url": "https://github.com/apache/hadoop/pull/1790", "merged": true, "mergeCommit": {"oid": "3612317038196ee0cb6d7204056d54b7a7ed8bf7"}, "closed": true, "closedAt": "2020-03-20T10:27:41Z", "author": {"login": "ishaniahuja"}, "timelineItems": {"totalCount": 71, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABb3kDIbAFqTMzODQ1OTg5MQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcOjIMdgFqTM3NjA1NzE1NQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzM4NDU5ODkx", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-338459891", "createdAt": "2020-01-06T04:00:46Z", "commit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwNDowMDo0NlrOFaVAnA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwNDowMDo0NlrOFaVAnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE1MTUxNg==", "bodyText": "This config value is passed until AbfsOutputStream.java -> writeCurrentBufferToService() as the first argument. but no specific action is seen inside writeCurrentBufferToService() if the value is true or false.\nWhat is intended of this config ? is it required ?", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r363151516", "createdAt": "2020-01-06T04:00:46Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java", "diffHunk": "@@ -146,6 +150,10 @@\n       DefaultValue = DEFAULT_ENABLE_FLUSH)\n   private boolean enableFlush;\n \n+  @BooleanConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_ENABLE_APPEND_WITH_FLUSH,\n+      DefaultValue = DEFAULT_ENABLE_APPEND_WITH_FLUSH)\n+  private boolean enableAppendWithFlush;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "originalPosition": 17}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzM4NDYwMDQx", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-338460041", "createdAt": "2020-01-06T04:01:45Z", "commit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwNDowMTo0NVrOFaVBJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwNDowMTo0NVrOFaVBJQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE1MTY1Mw==", "bodyText": "New configs need documentation. For reference, recent documentation done for flush related config : https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-azure/src/site/markdown/abfs.md#-flush-options", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r363151653", "createdAt": "2020-01-06T04:01:45Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java", "diffHunk": "@@ -130,6 +130,10 @@\n       DefaultValue = DEFAULT_FS_AZURE_ATOMIC_RENAME_DIRECTORIES)\n   private String azureAtomicDirs;\n \n+  @StringConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_APPEND_BLOB_KEY,\n+      DefaultValue = DEFAULT_FS_AZURE_APPEND_BLOB_DIRECTORIES)\n+  private String azureAppendBlobDirs;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "originalPosition": 6}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzM4NDYwMTQ0", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-338460144", "createdAt": "2020-01-06T04:02:23Z", "commit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwNDowMjoyNFrOFaVBdw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwNDowMjoyNFrOFaVBdw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE1MTczNQ==", "bodyText": "Why not return a HashSet as all callers are going to need it so.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r363151735", "createdAt": "2020-01-06T04:02:24Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java", "diffHunk": "@@ -411,6 +419,10 @@ public String getAzureAtomicRenameDirs() {\n     return this.azureAtomicDirs;\n   }\n \n+  public String getAppendBlobDirs() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "originalPosition": 26}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzM4NDYwNDAy", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-338460402", "createdAt": "2020-01-06T04:04:03Z", "commit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwNDowNDowM1rOFaVCSg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwNDowNDowM1rOFaVCSg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE1MTk0Ng==", "bodyText": "Is this version present on all the production clusters already ? If not, this change should go in only after the support is available.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r363151946", "createdAt": "2020-01-06T04:04:03Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java", "diffHunk": "@@ -55,7 +55,7 @@\n   public static final Logger LOG = LoggerFactory.getLogger(AbfsClient.class);\n   private final URL baseUrl;\n   private final SharedKeyCredentials sharedKeyCredentials;\n-  private final String xMsVersion = \"2018-11-09\";\n+  private final String xMsVersion = \"2019-12-12\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "originalPosition": 5}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzM4NDYwNTg1", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-338460585", "createdAt": "2020-01-06T04:05:35Z", "commit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwNDowNTozNVrOFaVC-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwNDowNTozNVrOFaVC-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE1MjEyMA==", "bodyText": "As mentioned in the comments for AbfsConfiguration.java, dont see flush in use inside the method. Also applies to the new isClose argument.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r363152120", "createdAt": "2020-01-06T04:05:35Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -257,17 +265,16 @@ public synchronized void close() throws IOException {\n \n   private synchronized void flushInternal(boolean isClose) throws IOException {\n     maybeThrowLastError();\n-    writeCurrentBufferToService();\n-    flushWrittenBytesToService(isClose);\n+    writeAndFlushWrittenBytesToService(isClose);\n   }\n \n   private synchronized void flushInternalAsync() throws IOException {\n     maybeThrowLastError();\n-    writeCurrentBufferToService();\n+    writeCurrentBufferToService(true, false);\n     flushWrittenBytesToServiceAsync();\n   }\n \n-  private synchronized void writeCurrentBufferToService() throws IOException {\n+  private synchronized void writeCurrentBufferToService(final boolean flush, final boolean isClose) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "originalPosition": 64}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzM4NDYxNzQx", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-338461741", "createdAt": "2020-01-06T04:13:41Z", "commit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwNDoxMzo0MVrOFaVHNg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwNDoxMzo0MVrOFaVHNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE1MzIwNg==", "bodyText": "AbfsOutputStream.close() call flow is:\nclose() -> flushInternal() -> flushWrittenBytesToService() -> flushWrittenBytesToServiceInternal()\nWithin flushWrittenBytesToServiceInternal() is where service \"Flush\" gets called.\nAbove change removes call to flushWrittenBytesToServiceInternal() and replaces it with shrinkWriteOperationQueue().  This is going to prevent flush for AbfsOutputStream.close() calls.\nThis might be ok for AppendBlob, but BlockBlob behaviour needs to be retained where AbfsClient->Flush() will get triggered.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r363153206", "createdAt": "2020-01-06T04:13:41Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -309,15 +316,23 @@ private synchronized void flushWrittenBytesToService(boolean isClose) throws IOE\n             throw new FileNotFoundException(ex.getMessage());\n           }\n         }\n-\n         if (ex.getCause() instanceof AzureBlobFileSystemException) {\n           ex = (AzureBlobFileSystemException) ex.getCause();\n         }\n         lastError = new IOException(ex);\n         throw lastError;\n       }\n     }\n-    flushWrittenBytesToServiceInternal(position, false, isClose);\n+    shrinkWriteOperationQueue();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "originalPosition": 81}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzM4NDYyODg0", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-338462884", "createdAt": "2020-01-06T04:21:40Z", "commit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwNDoyMTo0MFrOFaVK5w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwNDoyMTo0MFrOFaVK5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE1NDE1MQ==", "bodyText": "This method is supposed to be called from AbfsOutputStream.java -> writeCurrentBufferToService(), but there is no change for call to this API within that method. How did the compilation pass.\nAnd, (probably a repeat question of config query posted above, still) can you please explain why flush and close needs to be added to Append method.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r363154151", "createdAt": "2020-01-06T04:21:40Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java", "diffHunk": "@@ -278,7 +282,8 @@ public AbfsRestOperation renamePath(final String source, final String destinatio\n   }\n \n   public AbfsRestOperation append(final String path, final long position, final byte[] buffer, final int offset,\n-                                  final int length) throws AzureBlobFileSystemException {\n+                                  final int length, boolean flush, boolean isClose)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "originalPosition": 34}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzM4NDYzMDcy", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-338463072", "createdAt": "2020-01-06T04:22:47Z", "commit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzM4NTQ2OTA3", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-338546907", "createdAt": "2020-01-06T09:41:13Z", "commit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwOTo0MToxM1rOFaZLyg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwOTo0MToxM1rOFaZLyg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzIxOTkxNA==", "bodyText": "We specify dependency versions in the parent pom", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r363219914", "createdAt": "2020-01-06T09:41:13Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/pom.xml", "diffHunk": "@@ -143,6 +143,14 @@\n \n   <!-- see hadoop-project/pom.xml for version number declarations -->\n   <dependencies>\n+\t\t<!-- https://mvnrepository.com/artifact/org.mockito/mockito-all -->\n+\t\t<dependency>\n+\t\t\t\t<groupId>org.mockito</groupId>\n+\t\t\t\t<artifactId>mockito-all</artifactId>\n+\t\t\t\t<version>1.8.4</version>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "originalPosition": 8}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzM4NTUxODI1", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-338551825", "createdAt": "2020-01-06T09:51:18Z", "commit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwOTo1MToxOFrOFaZagg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwOTo1MToxOFrOFaZagg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzIyMzY4Mg==", "bodyText": "can the lines 171-185 be made a separate method and reuse in the test methods", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r363223682", "createdAt": "2020-01-06T09:51:18Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,268 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.mockito.ArgumentCaptor;\n+import static org.mockito.Mockito.*;\n+/**\n+ * Test useragent of abfs client.\n+ *\n+ */\n+public final class TestAbfsOutputStream {\n+\n+  private static int bufferSize = 4096;\n+  private static int writeSize = 1000;\n+  private static String path = \"~/testpath\";\n+\n+  @Test\n+  public void verifyShortWriteRequest() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, true, false);\n+    final byte[] b = new byte[writeSize];\n+    new Random().nextBytes(b);\n+    out.write(b);\n+    out.hsync();\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    final byte[] b1 = new byte[2*writeSize];\n+    new Random().nextBytes(b1);\n+    out.write(b1);\n+    out.flush();\n+    out.hflush();\n+\n+    out.hsync();\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(0), Long.valueOf(writeSize)), acLong.getAllValues());\n+    //flush=true, close=false, flush=true, close=false\n+    Assert.assertEquals(Arrays.asList(true, false, true, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0,writeSize, 0, 2*writeSize), acInt.getAllValues());\n+\n+    verifyNoMoreInteractions(client);\n+\n+  }\n+\n+  @Test\n+  public void verifyWriteRequest() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, true, false);\n+    final byte[] b = new byte[writeSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 5; i++) {\n+      out.write(b);\n+    }\n+    out.close();\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize)), acLong.getAllValues());\n+    //flush=false,close=false, flush=true,close=true\n+    Assert.assertEquals(Arrays.asList(false, false, true, true), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, 5*writeSize-bufferSize), acInt.getAllValues());\n+\n+    verifyNoMoreInteractions(client);\n+  }\n+\n+  @Test\n+  public void verifyWriteRequestOfBufferSizeAndClose() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, true, false);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    out.close();\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(new HashSet<Long>(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize))), new HashSet<Long>(acLong.getAllValues()));\n+    //flush=false, close=false, flush=false, close=false\n+    Assert.assertEquals(Arrays.asList(false, false, false, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, bufferSize), acInt.getAllValues());\n+\n+    ArgumentCaptor<String> acFlushString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acFlushLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Boolean> acFlushBool = ArgumentCaptor.forClass(Boolean.class);\n+\n+    verify(client, times(1)).flush(acFlushString.capture(), acFlushLong.capture(), acFlushBool.capture(), acFlushBool.capture());\n+    Assert.assertEquals(Arrays.asList(path) , acFlushString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(2*bufferSize)), acFlushLong.getAllValues());\n+    Assert.assertEquals(Arrays.asList(false, true), acFlushBool.getAllValues());\n+\n+    verifyNoMoreInteractions(client);\n+  }\n+\n+  @Test\n+  public void verifyWriteRequestOfBufferSize() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, true, false);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    Thread.sleep(1000);\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(new HashSet<Long>(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize))), new HashSet<Long>(acLong.getAllValues()));\n+    //flush=false, close=false, flush=false, close=false\n+    Assert.assertEquals(Arrays.asList(false, false, false, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, bufferSize), acInt.getAllValues());\n+\n+    verifyNoMoreInteractions(client);\n+  }\n+\n+  @Test\n+  public void verifyWriteRequestOfBufferSizeWithAppendBlob() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, true, true);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    Thread.sleep(1000);\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "originalPosition": 185}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzM4NTUzNjQ5", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-338553649", "createdAt": "2020-01-06T09:55:05Z", "commit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwOTo1NTowNVrOFaZgRA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwOTo1NTowNVrOFaZgRA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzIyNTE1Ng==", "bodyText": "Shall we have these operations as a separate private method and call that from this constructor. This constructor is becoming lengthy.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r363225156", "createdAt": "2020-01-06T09:55:05Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "diffHunk": "@@ -164,6 +169,23 @@ public AzureBlobFileSystemStore(URI uri, boolean isSecureScheme, Configuration c\n     boolean useHttps = (usingOauth || abfsConfiguration.isHttpsAlwaysUsed()) ? true : isSecureScheme;\n     initializeClient(uri, fileSystemName, accountName, useHttps);\n     this.identityTransformer = new IdentityTransformer(abfsConfiguration.getRawConfiguration());\n+\n+    // Extract the directories that should contain page blobs", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "originalPosition": 17}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQxMDc4MTM0", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-341078134", "createdAt": "2020-01-10T10:33:36Z", "commit": {"oid": "0c74b1ec54bbbea58fdf75f015d491d4bff221fb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQxMDozMzozNlrOFcQOgQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQxMDozMzozNlrOFcQOgQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTE3MDMwNQ==", "bodyText": "Wildfly jar version is picked from the parent project. Retain the original config.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r365170305", "createdAt": "2020-01-10T10:33:36Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/pom.xml", "diffHunk": "@@ -194,7 +203,8 @@\n     <dependency>\n       <groupId>org.wildfly.openssl</groupId>\n       <artifactId>wildfly-openssl</artifactId>\n-      <scope>runtime</scope>\n+      <version>1.0.9.Final</version>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c74b1ec54bbbea58fdf75f015d491d4bff221fb"}, "originalPosition": 21}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQxMDc4Mzc3", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-341078377", "createdAt": "2020-01-10T10:34:07Z", "commit": {"oid": "0c74b1ec54bbbea58fdf75f015d491d4bff221fb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQxMDozNDowOFrOFcQPOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQxMDozNDowOFrOFcQPOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTE3MDQ4OQ==", "bodyText": "Undo unintended config changes", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r365170489", "createdAt": "2020-01-10T10:34:08Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/pom.xml", "diffHunk": "@@ -291,6 +301,7 @@\n       </activation>\n       <build>\n         <plugins>\n+<!--", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c74b1ec54bbbea58fdf75f015d491d4bff221fb"}, "originalPosition": 30}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQxMDgzNzQz", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-341083743", "createdAt": "2020-01-10T10:44:02Z", "commit": {"oid": "0c74b1ec54bbbea58fdf75f015d491d4bff221fb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQxMDo0NDowM1rOFcQfgA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQxMDo0NDowM1rOFcQfgA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTE3NDY1Ng==", "bodyText": "Is support for AppendBlob over queryparam \"blobtype\" also enabled with new Dec-2019 version ? If yes, add a config control for AppendBlob as well and have the config off, which can be turned on when the API version is upgraded in driver.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r365174656", "createdAt": "2020-01-10T10:44:03Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "diffHunk": "@@ -344,40 +366,53 @@ public void deleteFilesystem() throws AzureBlobFileSystemException {\n \n   public OutputStream createFile(final Path path, final boolean overwrite, final FsPermission permission,\n                                  final FsPermission umask) throws AzureBlobFileSystemException {\n-    boolean isNamespaceEnabled = getIsNamespaceEnabled();\n-    LOG.debug(\"createFile filesystem: {} path: {} overwrite: {} permission: {} umask: {} isNamespaceEnabled: {}\",\n-            client.getFileSystem(),\n-            path,\n-            overwrite,\n-            permission.toString(),\n-            umask.toString(),\n-            isNamespaceEnabled);\n-\n-    client.createPath(AbfsHttpConstants.FORWARD_SLASH + getRelativePath(path), true, overwrite,\n-        isNamespaceEnabled ? getOctalNotation(permission) : null,\n-        isNamespaceEnabled ? getOctalNotation(umask) : null);\n+    try (AbfsPerfInfo perfInfo = startTracking(\"createFile\", \"createPath\")) {\n+      boolean isNamespaceEnabled = getIsNamespaceEnabled();\n+      LOG.debug(\"createFile filesystem: {} path: {} overwrite: {} permission: {} umask: {} isNamespaceEnabled: {}\",\n+              client.getFileSystem(),\n+              path,\n+              overwrite,\n+              permission.toString(),\n+              umask.toString(),\n+              isNamespaceEnabled);\n+\n+        boolean appendBlob = false;\n+        if (isAppendBlobKey(path.toString())) {\n+          appendBlob = true;\n+        }\n \n-    return new AbfsOutputStream(\n-        client,\n-        AbfsHttpConstants.FORWARD_SLASH + getRelativePath(path),\n-        0,\n-        abfsConfiguration.getWriteBufferSize(),\n-        abfsConfiguration.isFlushEnabled());\n+      client.createPath(AbfsHttpConstants.FORWARD_SLASH + getRelativePath(path), true, overwrite,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c74b1ec54bbbea58fdf75f015d491d4bff221fb"}, "originalPosition": 73}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQxMDkwMDA1", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-341090005", "createdAt": "2020-01-10T10:56:03Z", "commit": {"oid": "0c74b1ec54bbbea58fdf75f015d491d4bff221fb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQxMDo1NjowNFrOFcQyNQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQxMDo1NjowNFrOFcQyNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTE3OTQ0NQ==", "bodyText": "Many changes from trunk merge is showing up as new updates even though not added as part of this PR.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r365179445", "createdAt": "2020-01-10T10:56:04Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java", "diffHunk": "@@ -427,6 +443,14 @@ public boolean isFlushEnabled() {\n     return this.enableFlush;\n   }\n \n+  public boolean isAppendWithFlushEnabled() {\n+    return this.enableAppendWithFlush;\n+  }\n+\n+  public boolean isOutputStreamFlushDisabled() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c74b1ec54bbbea58fdf75f015d491d4bff221fb"}, "originalPosition": 45}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQxMTEyMTEw", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-341112110", "createdAt": "2020-01-10T11:42:49Z", "commit": {"oid": "0c74b1ec54bbbea58fdf75f015d491d4bff221fb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQxMTo0Mjo0OVrOFcR0uw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQxMTo0Mjo0OVrOFcR0uw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTE5NjQ3NQ==", "bodyText": "In the trunk, client calls have AbfsPerfTracker tracking all the calls. Not sure why the PR view is failing to show it as a delta from trunk. But looks like there is some issue here with merge from trunk. Please do a file to file compare from PR branch to trunk.\nexample: from trunk:\n      public Void call() throws Exception { AbfsPerfTracker tracker = client.getAbfsPerfTracker(); try (AbfsPerfInfo perfInfo = new AbfsPerfInfo(tracker, \"writeCurrentBufferToService\", \"append\")) { AbfsRestOperation op = client.append(path, offset, bytes, 0, bytesLength); perfInfo.registerResult(op.getResult()); byteBufferPool.putBuffer(ByteBuffer.wrap(bytes)); perfInfo.registerSuccess(true); return null; }\n(this is not the only delta)", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r365196475", "createdAt": "2020-01-10T11:42:49Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -287,13 +297,13 @@ private synchronized void writeCurrentBufferToService() throws IOException {\n       @Override\n       public Void call() throws Exception {\n         client.append(path, offset, bytes, 0,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c74b1ec54bbbea58fdf75f015d491d4bff221fb"}, "originalPosition": 74}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQxMTE0MTc0", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-341114174", "createdAt": "2020-01-10T11:47:45Z", "commit": {"oid": "0c74b1ec54bbbea58fdf75f015d491d4bff221fb"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ3NzUwNjc1", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-347750675", "createdAt": "2020-01-24T04:54:29Z", "commit": {"oid": "0c74b1ec54bbbea58fdf75f015d491d4bff221fb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yNFQwNDo1NDoyOVrOFhT6Hw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yNFQwNDo1NDoyOVrOFhT6Hw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDQ3MzUwMw==", "bodyText": "removed this commented section if not used", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r370473503", "createdAt": "2020-01-24T04:54:29Z", "author": {"login": "DadanielZ"}, "path": "hadoop-tools/hadoop-azure/pom.xml", "diffHunk": "@@ -143,6 +143,15 @@\n \n   <!-- see hadoop-project/pom.xml for version number declarations -->\n   <dependencies>\n+<!--\n+\t\t<dependency>\n+\t\t\t\t<groupId>org.mockito</groupId>\n+\t\t\t\t<artifactId>mockito-all</artifactId>\n+\t\t\t\t<version>1.8.4</version>\n+\t\t\t\t<scope>test</scope>\n+\t\t</dependency>\n+-->", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c74b1ec54bbbea58fdf75f015d491d4bff221fb"}, "originalPosition": 11}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ3NzUxNDgx", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-347751481", "createdAt": "2020-01-24T04:59:34Z", "commit": {"oid": "0c74b1ec54bbbea58fdf75f015d491d4bff221fb"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yNFQwNDo1OTozNFrOFhT9Yw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yNFQwNToyODozMFrOFhUOUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDQ3NDMzOQ==", "bodyText": "we are using 1.0.7.final: \n  \n    \n      hadoop/hadoop-project/pom.xml\n    \n    \n         Line 199\n      in\n      978c487\n    \n    \n    \n    \n\n        \n          \n           <openssl-wildfly.version>1.0.7.Final</openssl-wildfly.version> \n        \n    \n  \n\n\nany special reason to upgrade?", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r370474339", "createdAt": "2020-01-24T04:59:34Z", "author": {"login": "DadanielZ"}, "path": "hadoop-tools/hadoop-azure/pom.xml", "diffHunk": "@@ -194,7 +203,8 @@\n     <dependency>\n       <groupId>org.wildfly.openssl</groupId>\n       <artifactId>wildfly-openssl</artifactId>\n-      <scope>runtime</scope>\n+      <version>1.0.9.Final</version>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTE3MDMwNQ=="}, "originalCommit": {"oid": "0c74b1ec54bbbea58fdf75f015d491d4bff221fb"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDQ3NDQ1MQ==", "bodyText": "why commented?", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r370474451", "createdAt": "2020-01-24T05:00:28Z", "author": {"login": "DadanielZ"}, "path": "hadoop-tools/hadoop-azure/pom.xml", "diffHunk": "@@ -194,7 +203,8 @@\n     <dependency>\n       <groupId>org.wildfly.openssl</groupId>\n       <artifactId>wildfly-openssl</artifactId>\n-      <scope>runtime</scope>\n+      <version>1.0.9.Final</version>\n+      <!--<scope>runtime</scope>-->", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c74b1ec54bbbea58fdf75f015d491d4bff221fb"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDQ3NjgzMQ==", "bodyText": "remove the *, import the actual class", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r370476831", "createdAt": "2020-01-24T05:16:55Z", "author": {"login": "DadanielZ"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,268 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.mockito.ArgumentCaptor;\n+import static org.mockito.Mockito.*;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c74b1ec54bbbea58fdf75f015d491d4bff221fb"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDQ3ODI2MA==", "bodyText": "Looks like there are some issue in this PR, this try logic is already in:\n\n  \n    \n      hadoop/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java\n    \n    \n         Line 423\n      in\n      978c487\n    \n    \n    \n    \n\n        \n          \n           try (AbfsPerfInfo perfInfo = startTracking(\"createDirectory\", \"createPath\")) {", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r370478260", "createdAt": "2020-01-24T05:26:02Z", "author": {"login": "DadanielZ"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "diffHunk": "@@ -344,40 +366,53 @@ public void deleteFilesystem() throws AzureBlobFileSystemException {\n \n   public OutputStream createFile(final Path path, final boolean overwrite, final FsPermission permission,\n                                  final FsPermission umask) throws AzureBlobFileSystemException {\n-    boolean isNamespaceEnabled = getIsNamespaceEnabled();\n-    LOG.debug(\"createFile filesystem: {} path: {} overwrite: {} permission: {} umask: {} isNamespaceEnabled: {}\",\n-            client.getFileSystem(),\n-            path,\n-            overwrite,\n-            permission.toString(),\n-            umask.toString(),\n-            isNamespaceEnabled);\n-\n-    client.createPath(AbfsHttpConstants.FORWARD_SLASH + getRelativePath(path), true, overwrite,\n-        isNamespaceEnabled ? getOctalNotation(permission) : null,\n-        isNamespaceEnabled ? getOctalNotation(umask) : null);\n+    try (AbfsPerfInfo perfInfo = startTracking(\"createFile\", \"createPath\")) {\n+      boolean isNamespaceEnabled = getIsNamespaceEnabled();\n+      LOG.debug(\"createFile filesystem: {} path: {} overwrite: {} permission: {} umask: {} isNamespaceEnabled: {}\",\n+              client.getFileSystem(),\n+              path,\n+              overwrite,\n+              permission.toString(),\n+              umask.toString(),\n+              isNamespaceEnabled);\n+\n+        boolean appendBlob = false;\n+        if (isAppendBlobKey(path.toString())) {\n+          appendBlob = true;\n+        }\n \n-    return new AbfsOutputStream(\n-        client,\n-        AbfsHttpConstants.FORWARD_SLASH + getRelativePath(path),\n-        0,\n-        abfsConfiguration.getWriteBufferSize(),\n-        abfsConfiguration.isFlushEnabled());\n+      client.createPath(AbfsHttpConstants.FORWARD_SLASH + getRelativePath(path), true, overwrite,\n+          isNamespaceEnabled ? getOctalNotation(permission) : null,\n+          isNamespaceEnabled ? getOctalNotation(umask) : null,\n+          appendBlob);\n+\n+      return new AbfsOutputStream(\n+          client,\n+          AbfsHttpConstants.FORWARD_SLASH + getRelativePath(path),\n+          0,\n+          abfsConfiguration.getWriteBufferSize(),\n+          abfsConfiguration.isFlushEnabled(),\n+          abfsConfiguration.isAppendWithFlushEnabled(),\n+          appendBlob);\n+    }\n   }\n \n   public void createDirectory(final Path path, final FsPermission permission, final FsPermission umask)\n       throws AzureBlobFileSystemException {\n-    boolean isNamespaceEnabled = getIsNamespaceEnabled();\n-    LOG.debug(\"createDirectory filesystem: {} path: {} permission: {} umask: {} isNamespaceEnabled: {}\",\n-            client.getFileSystem(),\n-            path,\n-            permission,\n-            umask,\n-            isNamespaceEnabled);\n-\n-    client.createPath(AbfsHttpConstants.FORWARD_SLASH + getRelativePath(path), false, true,\n-        isNamespaceEnabled ? getOctalNotation(permission) : null,\n-        isNamespaceEnabled ? getOctalNotation(umask) : null);\n+    try (AbfsPerfInfo perfInfo = startTracking(\"createDirectory\", \"createPath\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c74b1ec54bbbea58fdf75f015d491d4bff221fb"}, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDQ3ODY3Mg==", "bodyText": "could you elaborate here a little bit more why for appendblob the max concurrent requests is 1?", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r370478672", "createdAt": "2020-01-24T05:28:30Z", "author": {"login": "DadanielZ"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -80,20 +82,29 @@ public AbfsOutputStream(\n       final String path,\n       final long position,\n       final int bufferSize,\n-      final boolean supportFlush) {\n+      final boolean supportFlush,\n+      final boolean disableOutputStreamFlush,\n+      final boolean supportAppendWithFlush,\n+      final boolean appendBlob) {\n     this.client = client;\n     this.path = path;\n     this.position = position;\n     this.closed = false;\n+    this.disableOutputStreamFlush = disableOutputStreamFlush;\n     this.supportFlush = supportFlush;\n+    this.supportAppendWithFlush = supportAppendWithFlush;\n     this.lastError = null;\n     this.lastFlushOffset = 0;\n     this.bufferSize = bufferSize;\n     this.buffer = byteBufferPool.getBuffer(false, bufferSize).array();\n     this.bufferIndex = 0;\n     this.writeOperations = new ConcurrentLinkedDeque<>();\n \n-    this.maxConcurrentRequestCount = 4 * Runtime.getRuntime().availableProcessors();\n+    if (appendBlob) {\n+      this.maxConcurrentRequestCount = 1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c74b1ec54bbbea58fdf75f015d491d4bff221fb"}, "originalPosition": 34}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "2caf6a731c58ae97e12c9427deb12cb4f1deb509", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/2caf6a731c58ae97e12c9427deb12cb4f1deb509", "committedDate": "2020-01-27T10:20:07Z", "message": "Merge branch 'personal/isahuja/shortwrite' of github.com:ishaniahuja/hadoop into personal/isahuja/shortwrite"}, "afterCommit": {"oid": "6cc320349434176cfbf0653a4e04d5ebb7238f2a", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/6cc320349434176cfbf0653a4e04d5ebb7238f2a", "committedDate": "2020-01-27T09:25:28Z", "message": "resolving conflicts\n+ some changes were not checked in with the previous commit and are checked in here."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU2OTUyNzY2", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-356952766", "createdAt": "2020-02-11T19:59:05Z", "commit": {"oid": "f0e5da3819a6ad0b5abbafcae07225b19566be31"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU3NjMwMzAw", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-357630300", "createdAt": "2020-02-12T17:07:04Z", "commit": {"oid": "8590f79e9ed00f055b2fd822c6c964943dddbb91"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQxNzowNzowNVrOFo3D1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQxNzoyNjoxN1rOFo3uvg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODM4OTQ2MQ==", "bodyText": "Are the trunk merge issues resolved ? This is an existing code in trunk but shown as new change here.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r378389461", "createdAt": "2020-02-12T17:07:05Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java", "diffHunk": "@@ -158,6 +162,14 @@\n       DefaultValue = DEFAULT_DISABLE_OUTPUTSTREAM_FLUSH)\n   private boolean disableOutputStreamFlush;\n \n+  @BooleanConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_ENABLE_APPEND_WITH_FLUSH,\n+      DefaultValue = DEFAULT_ENABLE_APPEND_WITH_FLUSH)\n+  private boolean enableAppendWithFlush;\n+\n+  @BooleanConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_DISABLE_OUTPUTSTREAM_FLUSH,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8590f79e9ed00f055b2fd822c6c964943dddbb91"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODM5NDM0Nw==", "bodyText": "Test the change against an account in prod tenant which still doesnt have Dec-12 bits .", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r378394347", "createdAt": "2020-02-12T17:15:41Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java", "diffHunk": "@@ -254,6 +273,9 @@ public AbfsRestOperation createPath(final String path, final boolean isFile, fin\n \n     final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\n     abfsUriQueryBuilder.addQuery(QUERY_PARAM_RESOURCE, isFile ? FILE : DIRECTORY);\n+    if (appendBlob) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8590f79e9ed00f055b2fd822c6c964943dddbb91"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODQwMDQ0Ng==", "bodyText": "Could you explain the logic here. Please add to code comments.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r378400446", "createdAt": "2020-02-12T17:26:17Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -307,10 +318,18 @@ public Void call() throws Exception {\n           perfInfo.registerSuccess(true);\n           return null;\n         }\n+        if (flush) {\n+          while(lastTotalAppendOffset <  lastFlushOffset);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8590f79e9ed00f055b2fd822c6c964943dddbb91"}, "originalPosition": 85}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY1MTYzNTIx", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-365163521", "createdAt": "2020-02-26T19:05:46Z", "commit": {"oid": "0840c8aa5b2371a79705e8183fe4381aba0d0e52"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f3db6b4f0f18cc16ea5cf2544ac42cc598e88b11", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/f3db6b4f0f18cc16ea5cf2544ac42cc598e88b11", "committedDate": "2020-03-02T09:15:22Z", "message": "changes for combined append+flush calls for blockblob(traditionan) and support for appendblob"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3a5a0f7e49aa84c1270062504ce1d62a578455b4", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/3a5a0f7e49aa84c1270062504ce1d62a578455b4", "committedDate": "2020-03-02T09:15:22Z", "message": "resolving conflicts\n+ some changes were not checked in with the previous commit and are checked in here."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ebfb85f73019032e18a3e1b64c24e75e8fd99f56", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/ebfb85f73019032e18a3e1b64c24e75e8fd99f56", "committedDate": "2020-03-02T09:15:22Z", "message": "added feedbacks"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "27dd4a5188955da05357b5e3ed0d7c59a0b76360", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/27dd4a5188955da05357b5e3ed0d7c59a0b76360", "committedDate": "2020-03-02T09:15:23Z", "message": "adding fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6b2fc80189e38c46b51bccb22346834a18af044b", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/6b2fc80189e38c46b51bccb22346834a18af044b", "committedDate": "2020-03-02T09:24:16Z", "message": "made rest version based on config parameter"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a23da93e97ab12df37073445d3692c4e2bd5b623", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/a23da93e97ab12df37073445d3692c4e2bd5b623", "committedDate": "2020-03-02T09:24:17Z", "message": "incorporated feedbacks/comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3226c04b2e6354b3767558ad14d2d6394f1322ae", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/3226c04b2e6354b3767558ad14d2d6394f1322ae", "committedDate": "2020-03-02T09:24:17Z", "message": "added appropriate comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c233aaee40c4165d9bf7cb888a2899cd71532f98", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/c233aaee40c4165d9bf7cb888a2899cd71532f98", "committedDate": "2020-03-02T09:25:00Z", "message": "compilation error fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c87e041e9f24d39381fadcc9690fc80f1a71da1f", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/c87e041e9f24d39381fadcc9690fc80f1a71da1f", "committedDate": "2020-03-02T09:25:01Z", "message": "fixed UTs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "673369fa7198fa218460f1eec97d01e7f504043e", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/673369fa7198fa218460f1eec97d01e7f504043e", "committedDate": "2020-03-02T09:25:01Z", "message": "fixing mockito errors"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cc053f4303b2092270c4ac0a267d737e24d0a0f0", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/cc053f4303b2092270c4ac0a267d737e24d0a0f0", "committedDate": "2020-03-02T09:25:01Z", "message": "adding right version of mockito"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e88e6ba098bede125637694dc8a0aa8b8a0c2e49", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/e88e6ba098bede125637694dc8a0aa8b8a0c2e49", "committedDate": "2020-03-02T09:25:01Z", "message": "fixed errors"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c03425291599dae085f5368d69d31fb659f68f5a", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/c03425291599dae085f5368d69d31fb659f68f5a", "committedDate": "2020-03-02T09:25:01Z", "message": "fixes in AbfsClient.java"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY3MzEzNzU3", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-367313757", "createdAt": "2020-03-02T15:57:49Z", "commit": {"oid": "0840c8aa5b2371a79705e8183fe4381aba0d0e52"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wMlQxNTo1Nzo1MVrOFwk9sQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wMlQxNjowNzo0N1rOFwlXVQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjQ4MTU4NQ==", "bodyText": "javadocs should be above the new option", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r386481585", "createdAt": "2020-03-02T15:57:51Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java", "diffHunk": "@@ -60,6 +61,12 @@\n    *  documentation does not have such expectations of data being persisted.\n    *  Default value of this config is true. **/\n   public static final String FS_AZURE_DISABLE_OUTPUTSTREAM_FLUSH = \"fs.azure.disable.outputstream.flush\";\n+  public static final String FS_AZURE_ENABLE_APPEND_WITH_FLUSH = \"fs.azure.enable.appendwithflush\";\n+  /** Provides a config control to disable or enable OutputStream Flush API", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0840c8aa5b2371a79705e8183fe4381aba0d0e52"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjQ4MzM3Mw==", "bodyText": "should be nested (i.e. ####); duplicate name will confuse link generation. Just cut the \"a name\" tag from the second line", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r386483373", "createdAt": "2020-03-02T16:00:29Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/site/markdown/abfs.md", "diffHunk": "@@ -643,6 +643,10 @@ Consult the javadocs for `org.apache.hadoop.fs.azurebfs.constants.ConfigurationK\n `org.apache.hadoop.fs.azurebfs.AbfsConfiguration` for the full list\n of configuration options and their default values.\n \n+### <a name=\"appendblobkeyconfigoptions\"></a> Append Blob Directories Options\n+### <a name=\"appendblobkeyconfigoptions\"></a> Config `fs.azure.appendblob.key` provides", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0840c8aa5b2371a79705e8183fe4381aba0d0e52"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjQ4NTM1Mw==", "bodyText": "org.apache imports need to go into their own block just above any static imports, ordering imports\njava., javax.\n--\nother\n---\norg.apache\n---\nstatic\n\nThis is to try and keep cherry-picking under control.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r386485353", "createdAt": "2020-03-02T16:03:27Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,349 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0840c8aa5b2371a79705e8183fe4381aba0d0e52"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjQ4NTc3Mg==", "bodyText": "ideally, yes, though we are a bit more relaxed about static imports...", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r386485772", "createdAt": "2020-03-02T16:04:05Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,268 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.mockito.ArgumentCaptor;\n+import static org.mockito.Mockito.*;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDQ3NjgzMQ=="}, "originalCommit": {"oid": "0c74b1ec54bbbea58fdf75f015d491d4bff221fb"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjQ4NjY4OQ==", "bodyText": "are these used", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r386486689", "createdAt": "2020-03-02T16:05:32Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,349 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.conf.Configuration;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.mockito.ArgumentCaptor;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.verifyNoMoreInteractions;\n+import static org.mockito.Mockito.*;\n+/**\n+ * Test useragent of abfs client.\n+ *\n+ */\n+public final class TestAbfsOutputStream {\n+\n+  private static int bufferSize = 4096;\n+  private static int writeSize = 1000;\n+  private static String path = \"~/testpath\";\n+  private final String globalKey = \"fs.azure.configuration\";\n+  private final String accountName1 = \"account1\";\n+  private final String accountKey1 = globalKey + \".\" + accountName1;\n+  private final String accountValue1 = \"one\";\n+\n+  @Test\n+  public void verifyShortWriteRequest() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[writeSize];\n+    new Random().nextBytes(b);\n+    out.write(b);\n+    out.hsync();\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    final byte[] b1 = new byte[2*writeSize];\n+    new Random().nextBytes(b1);\n+    out.write(b1);\n+    out.flush();\n+    out.hflush();\n+\n+    out.hsync();\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(0), Long.valueOf(writeSize)), acLong.getAllValues());\n+    //flush=true, close=false, flush=true, close=false\n+    Assert.assertEquals(Arrays.asList(true, false, true, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0,writeSize, 0, 2*writeSize), acInt.getAllValues());\n+\n+    //verifyNoMoreInteractions(client);\n+\n+  }\n+\n+  @Test\n+  public void verifyWriteRequest() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[writeSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 5; i++) {\n+      out.write(b);\n+    }\n+    out.close();\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize)), acLong.getAllValues());\n+    //flush=false,close=false, flush=true,close=true\n+    Assert.assertEquals(Arrays.asList(false, false, true, true), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, 5*writeSize-bufferSize), acInt.getAllValues());\n+\n+    //verifyNoMoreInteractions(client);\n+  }\n+\n+  @Test\n+  public void verifyWriteRequestOfBufferSizeAndClose() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+    when(client.flush(anyString(), anyLong(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    out.close();\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(new HashSet<Long>(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize))), new HashSet<Long>(acLong.getAllValues()));\n+    //flush=false, close=false, flush=false, close=false\n+    Assert.assertEquals(Arrays.asList(false, false, false, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, bufferSize), acInt.getAllValues());\n+\n+    ArgumentCaptor<String> acFlushString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acFlushLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Boolean> acFlushBool = ArgumentCaptor.forClass(Boolean.class);\n+\n+    verify(client, times(1)).flush(acFlushString.capture(), acFlushLong.capture(), acFlushBool.capture(), acFlushBool.capture());\n+    Assert.assertEquals(Arrays.asList(path) , acFlushString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(2*bufferSize)), acFlushLong.getAllValues());\n+    Assert.assertEquals(Arrays.asList(false, true), acFlushBool.getAllValues());\n+\n+//    verifyNoMoreInteractions(client);\n+  }\n+\n+  @Test\n+  public void verifyWriteRequestOfBufferSize() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    Thread.sleep(1000);\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(new HashSet<Long>(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize))), new HashSet<Long>(acLong.getAllValues()));\n+    //flush=false, close=false, flush=false, close=false\n+    Assert.assertEquals(Arrays.asList(false, false, false, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, bufferSize), acInt.getAllValues());\n+\n+ //   verifyNoMoreInteractions(client);\n+  }\n+\n+  @Test\n+  public void verifyWriteRequestOfBufferSizeWithAppendBlob() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    Thread.sleep(1000);\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize)), acLong.getAllValues());\n+    //flush=false, close=false, flush=false, close=false\n+    Assert.assertEquals(Arrays.asList(false, false, false, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, bufferSize), acInt.getAllValues());\n+\n+    //verifyNoMoreInteractions(client);\n+  }\n+\n+  @Test\n+  public void verifyWriteRequestOfBufferSizeAndHFlush() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+    when(client.flush(anyString(), anyLong(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    out.hflush();\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(new HashSet<Long>(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize))), new HashSet<Long>(acLong.getAllValues()));\n+    //flush=false, close=false, flush=false, close=false\n+    Assert.assertEquals(Arrays.asList(false, false, false, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, bufferSize), acInt.getAllValues());\n+\n+    ArgumentCaptor<String> acFlushString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acFlushLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Boolean> acFlushBool = ArgumentCaptor.forClass(Boolean.class);\n+\n+    verify(client, times(1)).flush(acFlushString.capture(), acFlushLong.capture(), acFlushBool.capture(), acFlushBool.capture());\n+    Assert.assertEquals(Arrays.asList(path) , acFlushString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(2*bufferSize)), acFlushLong.getAllValues());\n+    Assert.assertEquals(Arrays.asList(false, false), acFlushBool.getAllValues());\n+\n+    //verifyNoMoreInteractions(client);\n+  }\n+\n+  @Test\n+  public void verifyWriteRequestOfBufferSizeAndFlush() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    out.flush();\n+    Thread.sleep(1000);\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(new HashSet<Long>(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize))), new HashSet<Long>(acLong.getAllValues()));\n+    //flush=false, close=false, flush=false, close=false\n+    Assert.assertEquals(Arrays.asList(false, false, false, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, bufferSize), acInt.getAllValues());\n+\n+    ArgumentCaptor<String> acFlushString = ArgumentCaptor.forClass(String.class);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0840c8aa5b2371a79705e8183fe4381aba0d0e52"}, "originalPosition": 343}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjQ4ODA1OA==", "bodyText": "Mockito tests are always a maintenance pain because they are so brittle and sho hard to understand what is going on -for example, here I couldn't really understand any of the tests. Could you add some more detail as a comment for each test -at least to level of what each test case is looking for.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r386488058", "createdAt": "2020-03-02T16:07:38Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,349 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.conf.Configuration;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.mockito.ArgumentCaptor;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.verifyNoMoreInteractions;\n+import static org.mockito.Mockito.*;\n+/**\n+ * Test useragent of abfs client.\n+ *\n+ */\n+public final class TestAbfsOutputStream {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0840c8aa5b2371a79705e8183fe4381aba0d0e52"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjQ4ODE0OQ==", "bodyText": "check this", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r386488149", "createdAt": "2020-03-02T16:07:47Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,349 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.conf.Configuration;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.mockito.ArgumentCaptor;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.verifyNoMoreInteractions;\n+import static org.mockito.Mockito.*;\n+/**\n+ * Test useragent of abfs client.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0840c8aa5b2371a79705e8183fe4381aba0d0e52"}, "originalPosition": 36}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "371e5549e30913d25c3d86be64eb074744cddbd8", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/371e5549e30913d25c3d86be64eb074744cddbd8", "committedDate": "2020-03-04T08:39:07Z", "message": "modification to test due to the new call changes in the driver"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f955eb7fc835e0681eb1723cb29797982b8ffb7a", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/f955eb7fc835e0681eb1723cb29797982b8ffb7a", "committedDate": "2020-03-04T09:02:55Z", "message": "Merge branch 'personal/isahuja/shortwrite' of github.com:ishaniahuja/hadoop into personal/isahuja/shortwrite"}, "afterCommit": {"oid": "371e5549e30913d25c3d86be64eb074744cddbd8", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/371e5549e30913d25c3d86be64eb074744cddbd8", "committedDate": "2020-03-04T08:39:07Z", "message": "modification to test due to the new call changes in the driver"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e5cb3a8214edd5b2326ffd0b48403c01f90e8e94", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/e5cb3a8214edd5b2326ffd0b48403c01f90e8e94", "committedDate": "2020-03-04T12:24:13Z", "message": "whitespace fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "520f504efa052e72645c36c9f1fde76574dc2bf7", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/520f504efa052e72645c36c9f1fde76574dc2bf7", "committedDate": "2020-03-04T14:25:52Z", "message": "whitespace fix"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY4ODU2MjA4", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-368856208", "createdAt": "2020-03-04T15:07:26Z", "commit": {"oid": "520f504efa052e72645c36c9f1fde76574dc2bf7"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQxNTowNzoyNlrOFxw87g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQxNTowNzoyNlrOFxw87g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzcyNjU3NA==", "bodyText": "nit: why not just 0?", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r387726574", "createdAt": "2020-03-04T15:07:26Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "diffHunk": "@@ -404,18 +425,25 @@ public OutputStream createFile(final Path path, final boolean overwrite, final F\n               umask.toString(),\n               isNamespaceEnabled);\n \n-      final AbfsRestOperation op = client.createPath(AbfsHttpConstants.FORWARD_SLASH + getRelativePath(path), true, overwrite,\n-              isNamespaceEnabled ? getOctalNotation(permission) : null,\n-              isNamespaceEnabled ? getOctalNotation(umask) : null);\n-      perfInfo.registerResult(op.getResult()).registerSuccess(true);\n+        boolean appendBlob = false;\n+        if (isAppendBlobKey(path.toString())) {\n+          appendBlob = true;\n+        }\n+\n+      client.createPath(AbfsHttpConstants.FORWARD_SLASH + getRelativePath(path), true, overwrite,\n+          isNamespaceEnabled ? getOctalNotation(permission) : null,\n+          isNamespaceEnabled ? getOctalNotation(umask) : null,\n+          appendBlob);\n \n       return new AbfsOutputStream(\n-              client,\n-              AbfsHttpConstants.FORWARD_SLASH + getRelativePath(path),\n-              0,\n-              abfsConfiguration.getWriteBufferSize(),\n-              abfsConfiguration.isFlushEnabled(),\n-              abfsConfiguration.isOutputStreamFlushDisabled());\n+          client,\n+          AbfsHttpConstants.FORWARD_SLASH + getRelativePath(path),\n+          Long.valueOf(0),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "520f504efa052e72645c36c9f1fde76574dc2bf7"}, "originalPosition": 62}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY5MzA0MzYw", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-369304360", "createdAt": "2020-03-05T05:33:00Z", "commit": {"oid": "520f504efa052e72645c36c9f1fde76574dc2bf7"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNVQwNTozMzowMFrOFyG4JA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNVQwNTozMzowMFrOFyG4JA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODA4NTc5Ng==", "bodyText": "Could you please point me to the check in close() call that prevents server call ?\nAlso have the tests been run on these different configurations:\n\nWith appenblob feature enabled\n\n\nWith and Without Namespace enabled account : fs.azure.enable.appendwithflush true and false\n\n\nWith appenblob feature disabled\n\n\nWith and Without Namespace enabled account : fs.azure.enable.appendwithflush true and false", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r388085796", "createdAt": "2020-03-05T05:33:00Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemE2E.java", "diffHunk": "@@ -209,9 +209,10 @@ public void testFlushWithFileNotFoundException() throws Exception {\n \n     fs.delete(testFilePath, true);\n     assertFalse(fs.exists(testFilePath));\n+    AbfsConfiguration configuration = this.getConfiguration();\n \n-    intercept(FileNotFoundException.class,\n-            () -> stream.close());\n+    //With the new code, it would not trigger a call to the backend", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "520f504efa052e72645c36c9f1fde76574dc2bf7"}, "originalPosition": 8}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bebc0be1f55e354fe113674c838d79cd40826c81", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/bebc0be1f55e354fe113674c838d79cd40826c81", "committedDate": "2020-03-09T06:46:59Z", "message": "fixes for appendblob changes, + added comment to change in the test cases. tested with both namespace enabled and disabled accounts"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a5d7e0a49937cc8b0cc33560c87de89d137499f9", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/a5d7e0a49937cc8b0cc33560c87de89d137499f9", "committedDate": "2020-03-09T09:39:18Z", "message": "incorporated feedbacks/comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcxMDQ0Njg1", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-371044685", "createdAt": "2020-03-09T10:13:34Z", "commit": {"oid": "a5d7e0a49937cc8b0cc33560c87de89d137499f9"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyMjAzNzkw", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-372203790", "createdAt": "2020-03-10T18:12:14Z", "commit": {"oid": "a5d7e0a49937cc8b0cc33560c87de89d137499f9"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQxODoxMjoxNVrOF0bIBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQxODoxMjoxNVrOF0bIBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDUxNDY5Mw==", "bodyText": "version number should not be defined here, please do not add this change.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r390514693", "createdAt": "2020-03-10T18:12:15Z", "author": {"login": "DadanielZ"}, "path": "hadoop-tools/hadoop-azure/pom.xml", "diffHunk": "@@ -260,6 +260,7 @@\n     <dependency>\n       <groupId>org.mockito</groupId>\n       <artifactId>mockito-core</artifactId>\n+      <version>3.3.0</version>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a5d7e0a49937cc8b0cc33560c87de89d137499f9"}, "originalPosition": 4}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "41bb42fc64078462050fbda881deb7a6a74013eb", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/41bb42fc64078462050fbda881deb7a6a74013eb", "committedDate": "2020-03-11T06:14:04Z", "message": "incoroporated feedbacks in the TestAbfsOutputStream"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/be48931febe59c03becc9c8b8505bb41a65d5a68", "committedDate": "2020-03-11T06:15:05Z", "message": "removed mockito version from the pom file"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzczOTI5NDMy", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-373929432", "createdAt": "2020-03-12T21:56:59Z", "commit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc0NzgyMjEy", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-374782212", "createdAt": "2020-03-15T09:06:36Z", "commit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNVQwOTowNjozNlrOF2dvZA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNVQwOTowNjozNlrOF2dvZA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjY1NDY5Mg==", "bodyText": "This comment looks misplaced", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r392654692", "createdAt": "2020-03-15T09:06:36Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java", "diffHunk": "@@ -60,6 +61,15 @@\n    *  documentation does not have such expectations of data being persisted.\n    *  Default value of this config is true. **/\n   public static final String FS_AZURE_DISABLE_OUTPUTSTREAM_FLUSH = \"fs.azure.disable.outputstream.flush\";\n+  /** Provides a config control to enable OutputStream AppendWithFlush API\n+   *  operations in AbfsOutputStream.\n+   *  Default value of this config is true. **/\n+  public static final String FS_AZURE_ENABLE_APPEND_WITH_FLUSH = \"fs.azure.enable.appendwithflush\";\n+  /** Provides a config control to disable or enable OutputStream Flush API", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "originalPosition": 16}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc0NzgyNDI0", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-374782424", "createdAt": "2020-03-15T09:10:15Z", "commit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNVQwOToxMDoxNlrOF2dwNg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNVQwOToxMDoxNlrOF2dwNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjY1NDkwMg==", "bodyText": "Shouldn't boolean be named with the prefix \"is\"", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r392654902", "createdAt": "2020-03-15T09:10:16Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -55,6 +55,8 @@\n   private boolean closed;\n   private boolean supportFlush;\n   private boolean disableOutputStreamFlush;\n+  private boolean supportAppendWithFlush;\n+  private boolean appendBlob;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "originalPosition": 5}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc0NzgyNjUx", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-374782651", "createdAt": "2020-03-15T09:14:04Z", "commit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNVQwOToxNDowNVrOF2dxWw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNVQwOToxNDowNVrOF2dxWw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjY1NTE5NQ==", "bodyText": "Have the comment above the @test annotation", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r392655195", "createdAt": "2020-03-15T09:14:05Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,355 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.mockito.ArgumentCaptor;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.conf.Configuration;\n+\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.verifyNoMoreInteractions;\n+import static org.mockito.Mockito.*;\n+\n+public final class TestAbfsOutputStream {\n+\n+  private static int bufferSize = 4096;\n+  private static int writeSize = 1000;\n+  private static String path = \"~/testpath\";\n+  private final String globalKey = \"fs.azure.configuration\";\n+  private final String accountName1 = \"account1\";\n+  private final String accountKey1 = globalKey + \".\" + accountName1;\n+  private final String accountValue1 = \"one\";\n+\n+  @Test\n+  /**\n+   * The test verifies OutputStream shortwrite case(2000bytes write followed by flush, hflush, hsync) is making correct HTTP calls to the server\n+   */\n+  public void verifyShortWriteRequest() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[writeSize];\n+    new Random().nextBytes(b);\n+    out.write(b);\n+    out.hsync();\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    final byte[] b1 = new byte[2*writeSize];\n+    new Random().nextBytes(b1);\n+    out.write(b1);\n+    out.flush();\n+    out.hflush();\n+\n+    out.hsync();\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(0), Long.valueOf(writeSize)), acLong.getAllValues());\n+    //flush=true, close=false, flush=true, close=false\n+    Assert.assertEquals(Arrays.asList(true, false, true, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0,writeSize, 0, 2*writeSize), acInt.getAllValues());\n+\n+  }\n+\n+  @Test\n+  /**\n+   * The test verifies OutputStream Write of writeSize(1000 bytes) followed by a close is making correct HTTP calls to the server\n+   */\n+  public void verifyWriteRequest() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[writeSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 5; i++) {\n+      out.write(b);\n+    }\n+    out.close();\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize)), acLong.getAllValues());\n+    //flush=false,close=false, flush=true,close=true\n+    Assert.assertEquals(Arrays.asList(false, false, true, true), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, 5*writeSize-bufferSize), acInt.getAllValues());\n+\n+  }\n+\n+  @Test\n+  /**\n+   * The test verifies OutputStream Write of bufferSize(4KB) followed by a close is making correct HTTP calls to the server\n+   */\n+  public void verifyWriteRequestOfBufferSizeAndClose() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+    when(client.flush(anyString(), anyLong(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    out.close();\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(new HashSet<Long>(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize))), new HashSet<Long>(acLong.getAllValues()));\n+    //flush=false, close=false, flush=false, close=false\n+    Assert.assertEquals(Arrays.asList(false, false, false, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, bufferSize), acInt.getAllValues());\n+\n+    ArgumentCaptor<String> acFlushString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acFlushLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Boolean> acFlushBool = ArgumentCaptor.forClass(Boolean.class);\n+\n+    verify(client, times(1)).flush(acFlushString.capture(), acFlushLong.capture(), acFlushBool.capture(), acFlushBool.capture());\n+    Assert.assertEquals(Arrays.asList(path) , acFlushString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(2*bufferSize)), acFlushLong.getAllValues());\n+    Assert.assertEquals(Arrays.asList(false, true), acFlushBool.getAllValues());\n+\n+  }\n+\n+  @Test\n+  /**\n+   * The test verifies OutputStream Write of bufferSize(4KB) is making correct HTTP calls to the server\n+   */\n+  public void verifyWriteRequestOfBufferSize() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    Thread.sleep(1000);\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(new HashSet<Long>(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize))), new HashSet<Long>(acLong.getAllValues()));\n+    //flush=false, close=false, flush=false, close=false\n+    Assert.assertEquals(Arrays.asList(false, false, false, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, bufferSize), acInt.getAllValues());\n+\n+  }\n+\n+  @Test\n+  /**\n+   * The test verifies OutputStream Write of bufferSize(4KB) on a AppendBlob based stream is making correct HTTP calls to the server\n+   */\n+  public void verifyWriteRequestOfBufferSizeWithAppendBlob() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    Thread.sleep(1000);\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize)), acLong.getAllValues());\n+    //flush=false, close=false, flush=false, close=false\n+    Assert.assertEquals(Arrays.asList(false, false, false, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, bufferSize), acInt.getAllValues());\n+\n+  }\n+\n+  @Test\n+  /**\n+   * The test verifies OutputStream Write of bufferSize(4KB)  followed by a hflush call is making correct HTTP calls to the server\n+   */\n+  public void verifyWriteRequestOfBufferSizeAndHFlush() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+    when(client.flush(anyString(), anyLong(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    out.hflush();\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(new HashSet<Long>(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize))), new HashSet<Long>(acLong.getAllValues()));\n+    //flush=false, close=false, flush=false, close=false\n+    Assert.assertEquals(Arrays.asList(false, false, false, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, bufferSize), acInt.getAllValues());\n+\n+    ArgumentCaptor<String> acFlushString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acFlushLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Boolean> acFlushBool = ArgumentCaptor.forClass(Boolean.class);\n+\n+    verify(client, times(1)).flush(acFlushString.capture(), acFlushLong.capture(), acFlushBool.capture(), acFlushBool.capture());\n+    Assert.assertEquals(Arrays.asList(path) , acFlushString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(2*bufferSize)), acFlushLong.getAllValues());\n+    Assert.assertEquals(Arrays.asList(false, false), acFlushBool.getAllValues());\n+\n+  }\n+\n+  @Test\n+  /**", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "originalPosition": 316}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc0NzgzMjAz", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-374783203", "createdAt": "2020-03-15T09:23:20Z", "commit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNVQwOToyMzoyMFrOF2dz7A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNVQwOToyMzoyMFrOF2dz7A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjY1NTg1Mg==", "bodyText": "Pls add new line before the first import", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r392655852", "createdAt": "2020-03-15T09:23:20Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,355 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import java.util.Arrays;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "originalPosition": 20}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc0NzgzMzUx", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-374783351", "createdAt": "2020-03-15T09:25:32Z", "commit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNVQwOToyNTozMlrOF2d0eQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNVQwOToyNTozMlrOF2d0eQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjY1NTk5Mw==", "bodyText": "Order and group the non-static imports in the order:\njava*\nany non org.apache imports\norg.apache imports", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r392655993", "createdAt": "2020-03-15T09:25:32Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,355 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "originalPosition": 23}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc0NzgzMzYz", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-374783363", "createdAt": "2020-03-15T09:25:49Z", "commit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNVQwOToyNTo0OVrOF2d0iw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNVQwOToyNTo0OVrOF2d0iw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjY1NjAxMQ==", "bodyText": "Same for static imports", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r392656011", "createdAt": "2020-03-15T09:25:49Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,355 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.mockito.ArgumentCaptor;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.conf.Configuration;\n+\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "originalPosition": 30}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc0NzgzMzkx", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-374783391", "createdAt": "2020-03-15T09:26:11Z", "commit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNVQwOToyNjoxMVrOF2d0rA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNVQwOToyNjoxMVrOF2d0rA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjY1NjA0NA==", "bodyText": "Keep the comment above the @test annotation for all the tests", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r392656044", "createdAt": "2020-03-15T09:26:11Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,355 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.mockito.ArgumentCaptor;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.conf.Configuration;\n+\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.verifyNoMoreInteractions;\n+import static org.mockito.Mockito.*;\n+\n+public final class TestAbfsOutputStream {\n+\n+  private static int bufferSize = 4096;\n+  private static int writeSize = 1000;\n+  private static String path = \"~/testpath\";\n+  private final String globalKey = \"fs.azure.configuration\";\n+  private final String accountName1 = \"account1\";\n+  private final String accountKey1 = globalKey + \".\" + accountName1;\n+  private final String accountValue1 = \"one\";\n+\n+  @Test\n+  /**\n+   * The test verifies OutputStream shortwrite case(2000bytes write followed by flush, hflush, hsync) is making correct HTTP calls to the server", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "originalPosition": 48}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc0NzgzNDc4", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-374783478", "createdAt": "2020-03-15T09:27:35Z", "commit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc1MTAzOTMx", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-375103931", "createdAt": "2020-03-16T10:57:01Z", "commit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxMDo1NzowMVrOF2u6nQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxMDo1NzowMVrOF2u6nQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNjA5Mw==", "bodyText": "Nit: Remove unused import", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r392936093", "createdAt": "2020-03-16T10:57:01Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,355 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.mockito.ArgumentCaptor;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.conf.Configuration;\n+\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.verifyNoMoreInteractions;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "originalPosition": 33}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc1MTA0MzM5", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-375104339", "createdAt": "2020-03-16T10:57:41Z", "commit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxMDo1Nzo0MVrOF2u78g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxMDo1Nzo0MVrOF2u78g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNjQzNA==", "bodyText": "Nit: Using the '.*' form of import should be avoided", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r392936434", "createdAt": "2020-03-16T10:57:41Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,355 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.mockito.ArgumentCaptor;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.conf.Configuration;\n+\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.verifyNoMoreInteractions;\n+import static org.mockito.Mockito.*;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "originalPosition": 34}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc1MTA1NTg3", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-375105587", "createdAt": "2020-03-16T10:59:32Z", "commit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxMDo1OTozM1rOF2u_-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxMDo1OTozM1rOF2u_-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNzQ2Nw==", "bodyText": "Nit: These 2 can also be made constants", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r392937467", "createdAt": "2020-03-16T10:59:33Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,355 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.mockito.ArgumentCaptor;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.conf.Configuration;\n+\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.verifyNoMoreInteractions;\n+import static org.mockito.Mockito.*;\n+\n+public final class TestAbfsOutputStream {\n+\n+  private static int bufferSize = 4096;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "originalPosition": 38}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc1MTA2MDc2", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-375106076", "createdAt": "2020-03-16T11:00:19Z", "commit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxMTowMDoxOVrOF2vBkg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxMTowMDoxOVrOF2vBkg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNzg3NA==", "bodyText": "Nit: Line is longer than 160 characters\nTake care of the same in the entire file", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r392937874", "createdAt": "2020-03-16T11:00:19Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,355 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.mockito.ArgumentCaptor;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.conf.Configuration;\n+\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.verifyNoMoreInteractions;\n+import static org.mockito.Mockito.*;\n+\n+public final class TestAbfsOutputStream {\n+\n+  private static int bufferSize = 4096;\n+  private static int writeSize = 1000;\n+  private static String path = \"~/testpath\";\n+  private final String globalKey = \"fs.azure.configuration\";\n+  private final String accountName1 = \"account1\";\n+  private final String accountKey1 = globalKey + \".\" + accountName1;\n+  private final String accountValue1 = \"one\";\n+\n+  @Test\n+  /**\n+   * The test verifies OutputStream shortwrite case(2000bytes write followed by flush, hflush, hsync) is making correct HTTP calls to the server\n+   */\n+  public void verifyShortWriteRequest() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[writeSize];\n+    new Random().nextBytes(b);\n+    out.write(b);\n+    out.hsync();\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    final byte[] b1 = new byte[2*writeSize];\n+    new Random().nextBytes(b1);\n+    out.write(b1);\n+    out.flush();\n+    out.hflush();\n+\n+    out.hsync();\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "originalPosition": 81}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc1MTA2ODMy", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-375106832", "createdAt": "2020-03-16T11:01:30Z", "commit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxMTowMTozMFrOF2vEEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxMTowMTozMFrOF2vEEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzODUxNQ==", "bodyText": "Nit: ',' is preceded with whitespace\nTake care of the same in the entire file", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r392938515", "createdAt": "2020-03-16T11:01:30Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,355 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.mockito.ArgumentCaptor;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.conf.Configuration;\n+\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.verifyNoMoreInteractions;\n+import static org.mockito.Mockito.*;\n+\n+public final class TestAbfsOutputStream {\n+\n+  private static int bufferSize = 4096;\n+  private static int writeSize = 1000;\n+  private static String path = \"~/testpath\";\n+  private final String globalKey = \"fs.azure.configuration\";\n+  private final String accountName1 = \"account1\";\n+  private final String accountKey1 = globalKey + \".\" + accountName1;\n+  private final String accountValue1 = \"one\";\n+\n+  @Test\n+  /**\n+   * The test verifies OutputStream shortwrite case(2000bytes write followed by flush, hflush, hsync) is making correct HTTP calls to the server\n+   */\n+  public void verifyShortWriteRequest() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[writeSize];\n+    new Random().nextBytes(b);\n+    out.write(b);\n+    out.hsync();\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    final byte[] b1 = new byte[2*writeSize];\n+    new Random().nextBytes(b1);\n+    out.write(b1);\n+    out.flush();\n+    out.hflush();\n+\n+    out.hsync();\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "originalPosition": 82}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc1MTA3MzYw", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-375107360", "createdAt": "2020-03-16T11:02:18Z", "commit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxMTowMjoxOFrOF2vF9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxMTowMjoxOFrOF2vF9A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzODk5Ng==", "bodyText": "Nit: ',' is not followed by whitespace. space before and after *\nTake care of the same in the entire file", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r392938996", "createdAt": "2020-03-16T11:02:18Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,355 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.mockito.ArgumentCaptor;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.conf.Configuration;\n+\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.verifyNoMoreInteractions;\n+import static org.mockito.Mockito.*;\n+\n+public final class TestAbfsOutputStream {\n+\n+  private static int bufferSize = 4096;\n+  private static int writeSize = 1000;\n+  private static String path = \"~/testpath\";\n+  private final String globalKey = \"fs.azure.configuration\";\n+  private final String accountName1 = \"account1\";\n+  private final String accountKey1 = globalKey + \".\" + accountName1;\n+  private final String accountValue1 = \"one\";\n+\n+  @Test\n+  /**\n+   * The test verifies OutputStream shortwrite case(2000bytes write followed by flush, hflush, hsync) is making correct HTTP calls to the server\n+   */\n+  public void verifyShortWriteRequest() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[writeSize];\n+    new Random().nextBytes(b);\n+    out.write(b);\n+    out.hsync();\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    final byte[] b1 = new byte[2*writeSize];\n+    new Random().nextBytes(b1);\n+    out.write(b1);\n+    out.flush();\n+    out.hflush();\n+\n+    out.hsync();\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(0), Long.valueOf(writeSize)), acLong.getAllValues());\n+    //flush=true, close=false, flush=true, close=false\n+    Assert.assertEquals(Arrays.asList(true, false, true, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0,writeSize, 0, 2*writeSize), acInt.getAllValues());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "originalPosition": 86}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc1MTA4NjM0", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-375108634", "createdAt": "2020-03-16T11:04:19Z", "commit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc1NTczMTcw", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-375573170", "createdAt": "2020-03-16T20:50:37Z", "commit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQyMDo1MDozN1rOF3FSgQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQyMDo1MDozN1rOF3FSgQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzMwMjY1Nw==", "bodyText": "Can you look at moving to assertJ here; these declarations are complex enough they need one\nand add error strings via .describedAs()", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r393302657", "createdAt": "2020-03-16T20:50:37Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,355 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.mockito.ArgumentCaptor;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.conf.Configuration;\n+\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.verifyNoMoreInteractions;\n+import static org.mockito.Mockito.*;\n+\n+public final class TestAbfsOutputStream {\n+\n+  private static int bufferSize = 4096;\n+  private static int writeSize = 1000;\n+  private static String path = \"~/testpath\";\n+  private final String globalKey = \"fs.azure.configuration\";\n+  private final String accountName1 = \"account1\";\n+  private final String accountKey1 = globalKey + \".\" + accountName1;\n+  private final String accountValue1 = \"one\";\n+\n+  @Test\n+  /**\n+   * The test verifies OutputStream shortwrite case(2000bytes write followed by flush, hflush, hsync) is making correct HTTP calls to the server\n+   */\n+  public void verifyShortWriteRequest() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[writeSize];\n+    new Random().nextBytes(b);\n+    out.write(b);\n+    out.hsync();\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    final byte[] b1 = new byte[2*writeSize];\n+    new Random().nextBytes(b1);\n+    out.write(b1);\n+    out.flush();\n+    out.hflush();\n+\n+    out.hsync();\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(0), Long.valueOf(writeSize)), acLong.getAllValues());\n+    //flush=true, close=false, flush=true, close=false\n+    Assert.assertEquals(Arrays.asList(true, false, true, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0,writeSize, 0, 2*writeSize), acInt.getAllValues());\n+\n+  }\n+\n+  @Test\n+  /**\n+   * The test verifies OutputStream Write of writeSize(1000 bytes) followed by a close is making correct HTTP calls to the server\n+   */\n+  public void verifyWriteRequest() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[writeSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 5; i++) {\n+      out.write(b);\n+    }\n+    out.close();\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize)), acLong.getAllValues());\n+    //flush=false,close=false, flush=true,close=true\n+    Assert.assertEquals(Arrays.asList(false, false, true, true), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, 5*writeSize-bufferSize), acInt.getAllValues());\n+\n+  }\n+\n+  @Test\n+  /**\n+   * The test verifies OutputStream Write of bufferSize(4KB) followed by a close is making correct HTTP calls to the server\n+   */\n+  public void verifyWriteRequestOfBufferSizeAndClose() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+    when(client.flush(anyString(), anyLong(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    out.close();\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(new HashSet<Long>(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize))), new HashSet<Long>(acLong.getAllValues()));\n+    //flush=false, close=false, flush=false, close=false\n+    Assert.assertEquals(Arrays.asList(false, false, false, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, bufferSize), acInt.getAllValues());\n+\n+    ArgumentCaptor<String> acFlushString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acFlushLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Boolean> acFlushBool = ArgumentCaptor.forClass(Boolean.class);\n+\n+    verify(client, times(1)).flush(acFlushString.capture(), acFlushLong.capture(), acFlushBool.capture(), acFlushBool.capture());\n+    Assert.assertEquals(Arrays.asList(path) , acFlushString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(2*bufferSize)), acFlushLong.getAllValues());\n+    Assert.assertEquals(Arrays.asList(false, true), acFlushBool.getAllValues());\n+\n+  }\n+\n+  @Test\n+  /**\n+   * The test verifies OutputStream Write of bufferSize(4KB) is making correct HTTP calls to the server\n+   */\n+  public void verifyWriteRequestOfBufferSize() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    Thread.sleep(1000);\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(new HashSet<Long>(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize))), new HashSet<Long>(acLong.getAllValues()));\n+    //flush=false, close=false, flush=false, close=false\n+    Assert.assertEquals(Arrays.asList(false, false, false, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, bufferSize), acInt.getAllValues());\n+\n+  }\n+\n+  @Test\n+  /**\n+   * The test verifies OutputStream Write of bufferSize(4KB) on a AppendBlob based stream is making correct HTTP calls to the server\n+   */\n+  public void verifyWriteRequestOfBufferSizeWithAppendBlob() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    Thread.sleep(1000);\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize)), acLong.getAllValues());\n+    //flush=false, close=false, flush=false, close=false\n+    Assert.assertEquals(Arrays.asList(false, false, false, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, bufferSize), acInt.getAllValues());\n+\n+  }\n+\n+  @Test\n+  /**\n+   * The test verifies OutputStream Write of bufferSize(4KB)  followed by a hflush call is making correct HTTP calls to the server\n+   */\n+  public void verifyWriteRequestOfBufferSizeAndHFlush() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+    when(client.flush(anyString(), anyLong(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    out.hflush();\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(new HashSet<Long>(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize))), new HashSet<Long>(acLong.getAllValues()));\n+    //flush=false, close=false, flush=false, close=false\n+    Assert.assertEquals(Arrays.asList(false, false, false, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, bufferSize), acInt.getAllValues());\n+\n+    ArgumentCaptor<String> acFlushString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acFlushLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Boolean> acFlushBool = ArgumentCaptor.forClass(Boolean.class);\n+\n+    verify(client, times(1)).flush(acFlushString.capture(), acFlushLong.capture(), acFlushBool.capture(), acFlushBool.capture());\n+    Assert.assertEquals(Arrays.asList(path) , acFlushString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(2*bufferSize)), acFlushLong.getAllValues());\n+    Assert.assertEquals(Arrays.asList(false, false), acFlushBool.getAllValues());\n+\n+  }\n+\n+  @Test\n+  /**\n+   * The test verifies OutputStream Write of bufferSize(4KB)  followed by a flush call is making correct HTTP calls to the server\n+   */\n+  public void verifyWriteRequestOfBufferSizeAndFlush() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    out.flush();\n+    Thread.sleep(1000);\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "originalPosition": 348}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "017f1232bb4aca1abb6e733bb9a751d5c41344cf", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/017f1232bb4aca1abb6e733bb9a751d5c41344cf", "committedDate": "2020-03-17T06:36:53Z", "message": "incorporated feedbacks"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc1ODExNTEy", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-375811512", "createdAt": "2020-03-17T08:08:33Z", "commit": {"oid": "017f1232bb4aca1abb6e733bb9a751d5c41344cf"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7388a10e54fd8fdff5e9aca3618f244451002b07", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/7388a10e54fd8fdff5e9aca3618f244451002b07", "committedDate": "2020-03-17T12:00:57Z", "message": "incorporated feedbacks"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f9a80c49469e5e55a32eeb8f9786fe14c39b8538", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/f9a80c49469e5e55a32eeb8f9786fe14c39b8538", "committedDate": "2020-03-17T12:09:55Z", "message": "incorporated feedbacks"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc2MDU3MTU1", "url": "https://github.com/apache/hadoop/pull/1790#pullrequestreview-376057155", "createdAt": "2020-03-17T13:56:39Z", "commit": {"oid": "f9a80c49469e5e55a32eeb8f9786fe14c39b8538"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4613, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}