{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM2NzQ5NjEw", "number": 2084, "title": "HDFS-15418. ViewFileSystemOverloadScheme should represent mount links\u2026", "bodyText": "https://issues.apache.org/jira/browse/HDFS-15418", "createdAt": "2020-06-18T20:35:59Z", "url": "https://github.com/apache/hadoop/pull/2084", "merged": true, "mergeCommit": {"oid": "b27810aa6015253866ccc0ccc7247ad7024c0730"}, "closed": true, "closedAt": "2020-06-20T07:32:03Z", "author": {"login": "umamaheswararao"}, "timelineItems": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcskeu3AH2gAyNDM2NzQ5NjEwOjJkMTU2YmNiZGRhMjAxNGNjMGQwOTgyMmFhNjMxYWIyYzFjODAwNWU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABctB5xggFqTQzNDQwODAzMQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "2d156bcbdda2014cc0d09822aa631ab2c1c8005e", "author": {"user": {"login": "umamaheswararao", "name": "Uma Maheswara Rao G"}}, "url": "https://github.com/apache/hadoop/commit/2d156bcbdda2014cc0d09822aa631ab2c1c8005e", "committedDate": "2020-06-18T20:28:54Z", "message": "HDFS-15418. ViewFileSystemOverloadScheme should represent mount links as non symlinks"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2c55c9a5cf7f88f3e435a23203504072050d5e43", "author": {"user": {"login": "umamaheswararao", "name": "Uma Maheswara Rao G"}}, "url": "https://github.com/apache/hadoop/commit/2c55c9a5cf7f88f3e435a23203504072050d5e43", "committedDate": "2020-06-19T02:41:10Z", "message": "HDFS-15418. CHeckstyle and test fixes"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMzODYyMDQz", "url": "https://github.com/apache/hadoop/pull/2084#pullrequestreview-433862043", "createdAt": "2020-06-19T07:31:03Z", "commit": {"oid": "2c55c9a5cf7f88f3e435a23203504072050d5e43"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQwNzozMTowM1rOGmK_jw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQwNzo1ODoyNVrOGmLyUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY3OTE4Mw==", "bodyText": "Should we just fallback to showing link permissions if the directory isn't found? as it was before.\nLs command throwing FNF for a child seems little weird. If the child isn't there the Ls need not to list it. FNF from Ls should be only I guess when the directory on which we are calling getListing() isn't available, For a child if it isn't present in the destination we can just log it and revert back to previous behavior and we can document that too as well, if the actual destination is present then only it shows destination permissions.", "url": "https://github.com/apache/hadoop/pull/2084#discussion_r442679183", "createdAt": "2020-06-19T07:31:03Z", "author": {"login": "ayushtkn"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java", "diffHunk": "@@ -1216,37 +1222,50 @@ public FileStatus getFileStatus(Path f) throws IOException {\n       for (Entry<String, INode<FileSystem>> iEntry :\n           theInternalDir.getChildren().entrySet()) {\n         INode<FileSystem> inode = iEntry.getValue();\n+        Path path = new Path(inode.fullPath).makeQualified(myUri, null);\n         if (inode.isLink()) {\n           INodeLink<FileSystem> link = (INodeLink<FileSystem>) inode;\n+\n+          if (showMountLinksAsSymlinks) {\n+            // To maintain backward compatibility, with default option(showing\n+            // mount links as symlinks), we will represent target link as\n+            // symlink and rest other properties are belongs to mount link only.\n+            result[i++] =\n+                new FileStatus(0, false, 0, 0, creationTime, creationTime,\n+                    PERMISSION_555, ugi.getShortUserName(),\n+                    ugi.getPrimaryGroupName(), link.getTargetLink(),\n+                    path);\n+            continue;\n+          }\n+\n+          //  We will represent as non-symlinks. Here it will show target\n+          //  directory/file properties like permissions, isDirectory etc on\n+          //  mount path. The path will be a mount link path and isDirectory is\n+          //  true if target is dir, otherwise false.\n+          String linkedPath = link.getTargetFileSystem().getUri().getPath();\n+          if (\"\".equals(linkedPath)) {\n+            linkedPath = \"/\";\n+          }\n           try {\n-            String linkedPath = link.getTargetFileSystem().getUri().getPath();\n-            if(\"\".equals(linkedPath)) {\n-              linkedPath = \"/\";\n-            }\n             FileStatus status =\n                 ((ChRootedFileSystem)link.getTargetFileSystem())\n                 .getMyFs().getFileStatus(new Path(linkedPath));\n-            result[i++] = new FileStatus(status.getLen(), false,\n-              status.getReplication(), status.getBlockSize(),\n-              status.getModificationTime(), status.getAccessTime(),\n-              status.getPermission(), status.getOwner(), status.getGroup(),\n-              link.getTargetLink(),\n-              new Path(inode.fullPath).makeQualified(\n-                  myUri, null));\n+            result[i++] = new FileStatus(status.getLen(), status.isDirectory(),\n+                status.getReplication(), status.getBlockSize(),\n+                status.getModificationTime(), status.getAccessTime(),\n+                status.getPermission(), status.getOwner(), status.getGroup(),\n+                null, path);\n           } catch (FileNotFoundException ex) {\n-            result[i++] = new FileStatus(0, false, 0, 0,\n-              creationTime, creationTime, PERMISSION_555,\n-              ugi.getShortUserName(), ugi.getPrimaryGroupName(),\n-              link.getTargetLink(),\n-              new Path(inode.fullPath).makeQualified(\n-                  myUri, null));\n+            LOG.warn(\"Cannot get one of the children's(\" + path\n+                + \")  target path(\" + link.getTargetFileSystem().getUri()\n+                + \") file status.\", ex);\n+            throw ex;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c55c9a5cf7f88f3e435a23203504072050d5e43"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY3OTUzMw==", "bodyText": "assertFalse() ?", "url": "https://github.com/apache/hadoop/pull/2084#discussion_r442679533", "createdAt": "2020-06-19T07:31:52Z", "author": {"login": "ayushtkn"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewfsFileStatus.java", "diffHunk": "@@ -148,9 +148,11 @@ public void testListStatusACL() throws IOException {\n         if (status.getPath().getName().equals(\"file\")) {\n           assertEquals(FsPermission.valueOf(\"-rwxr--r--\"),\n               status.getPermission());\n+          assertEquals(false, status.isDirectory());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c55c9a5cf7f88f3e435a23203504072050d5e43"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY3OTYyNQ==", "bodyText": "assertTrue() ?", "url": "https://github.com/apache/hadoop/pull/2084#discussion_r442679625", "createdAt": "2020-06-19T07:32:04Z", "author": {"login": "ayushtkn"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewfsFileStatus.java", "diffHunk": "@@ -148,9 +148,11 @@ public void testListStatusACL() throws IOException {\n         if (status.getPath().getName().equals(\"file\")) {\n           assertEquals(FsPermission.valueOf(\"-rwxr--r--\"),\n               status.getPermission());\n+          assertEquals(false, status.isDirectory());\n         } else {\n           assertEquals(FsPermission.valueOf(\"-r--rwxr--\"),\n               status.getPermission());\n+          assertEquals(true, status.isDirectory());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c55c9a5cf7f88f3e435a23203504072050d5e43"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY4MTUyNQ==", "bodyText": "A line break after @before should look better.", "url": "https://github.com/apache/hadoop/pull/2084#discussion_r442681525", "createdAt": "2020-06-19T07:36:24Z", "author": {"login": "ayushtkn"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeListStatus.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.*;\n+\n+/**\n+ * ViewFsOverloadScheme ListStatus.\n+ */\n+public class TestViewFsOverloadSchemeListStatus {\n+\n+  private static final File TEST_DIR =\n+      GenericTestUtils.getTestDir(TestViewfsFileStatus.class.getSimpleName());\n+\n+  @Before public void setUp() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c55c9a5cf7f88f3e435a23203504072050d5e43"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY4NzY3Mg==", "bodyText": "Line Break.", "url": "https://github.com/apache/hadoop/pull/2084#discussion_r442687672", "createdAt": "2020-06-19T07:49:12Z", "author": {"login": "ayushtkn"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeListStatus.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.*;\n+\n+/**\n+ * ViewFsOverloadScheme ListStatus.\n+ */\n+public class TestViewFsOverloadSchemeListStatus {\n+\n+  private static final File TEST_DIR =\n+      GenericTestUtils.getTestDir(TestViewfsFileStatus.class.getSimpleName());\n+\n+  @Before public void setUp() {\n+    FileUtil.fullyDelete(TEST_DIR);\n+    assertTrue(TEST_DIR.mkdirs());\n+  }\n+\n+  @After public void tearDown() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c55c9a5cf7f88f3e435a23203504072050d5e43"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY4ODk1OA==", "bodyText": "assertFalse()", "url": "https://github.com/apache/hadoop/pull/2084#discussion_r442688958", "createdAt": "2020-06-19T07:51:52Z", "author": {"login": "ayushtkn"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeListStatus.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.*;\n+\n+/**\n+ * ViewFsOverloadScheme ListStatus.\n+ */\n+public class TestViewFsOverloadSchemeListStatus {\n+\n+  private static final File TEST_DIR =\n+      GenericTestUtils.getTestDir(TestViewfsFileStatus.class.getSimpleName());\n+\n+  @Before public void setUp() {\n+    FileUtil.fullyDelete(TEST_DIR);\n+    assertTrue(TEST_DIR.mkdirs());\n+  }\n+\n+  @After public void tearDown() throws IOException {\n+    FileUtil.fullyDelete(TEST_DIR);\n+  }\n+\n+  /**\n+   * Tests the ACL and isDirectory returned from listStatus for directories and\n+   * files.\n+   */\n+  @Test\n+  public void testListStatusACL() throws IOException, URISyntaxException {\n+    String testfilename = \"testFileACL\";\n+    String childDirectoryName = \"testDirectoryACL\";\n+    TEST_DIR.mkdirs();\n+    File infile = new File(TEST_DIR, testfilename);\n+    final byte[] content = \"dingos\".getBytes();\n+\n+    try (FileOutputStream fos = new FileOutputStream(infile)) {\n+      fos.write(content);\n+    }\n+    assertEquals(content.length, infile.length());\n+    File childDir = new File(TEST_DIR, childDirectoryName);\n+    childDir.mkdirs();\n+\n+    Configuration conf = new Configuration();\n+    ConfigUtil.addLink(conf, \"/file\", infile.toURI());\n+    ConfigUtil.addLink(conf, \"/dir\", childDir.toURI());\n+    String fileScheme = \"file\";\n+    conf.set(String.format(\"fs.%s.impl\", fileScheme),\n+        ViewFileSystemOverloadScheme.class.getName());\n+    conf.set(String\n+        .format(FsConstants.FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN,\n+            fileScheme), LocalFileSystem.class.getName());\n+    String fileUriStr = \"file:///\";\n+    try (FileSystem vfs = FileSystem.get(new URI(fileUriStr), conf)) {\n+      assertEquals(ViewFileSystemOverloadScheme.class, vfs.getClass());\n+      FileStatus[] statuses = vfs.listStatus(new Path(\"/\"));\n+\n+      FileSystem localFs = ((ViewFileSystemOverloadScheme) vfs)\n+          .getRawFileSystem(new Path(fileUriStr), conf);\n+      FileStatus fileStat = localFs.getFileStatus(new Path(infile.getPath()));\n+      FileStatus dirStat = localFs.getFileStatus(new Path(childDir.getPath()));\n+\n+      for (FileStatus status : statuses) {\n+        if (status.getPath().getName().equals(fileScheme)) {\n+          assertEquals(fileStat.getPermission(), status.getPermission());\n+        } else {\n+          assertEquals(dirStat.getPermission(), status.getPermission());\n+        }\n+      }\n+\n+      localFs.setPermission(new Path(infile.getPath()),\n+          FsPermission.valueOf(\"-rwxr--r--\"));\n+      localFs.setPermission(new Path(childDir.getPath()),\n+          FsPermission.valueOf(\"-r--rwxr--\"));\n+\n+      statuses = vfs.listStatus(new Path(\"/\"));\n+      for (FileStatus status : statuses) {\n+        if (status.getPath().getName().equals(fileScheme)) {\n+          assertEquals(FsPermission.valueOf(\"-rwxr--r--\"),\n+              status.getPermission());\n+          assertEquals(false, status.isDirectory());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c55c9a5cf7f88f3e435a23203504072050d5e43"}, "originalPosition": 115}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY4OTA3OQ==", "bodyText": "assertTrue()?", "url": "https://github.com/apache/hadoop/pull/2084#discussion_r442689079", "createdAt": "2020-06-19T07:52:07Z", "author": {"login": "ayushtkn"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeListStatus.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.*;\n+\n+/**\n+ * ViewFsOverloadScheme ListStatus.\n+ */\n+public class TestViewFsOverloadSchemeListStatus {\n+\n+  private static final File TEST_DIR =\n+      GenericTestUtils.getTestDir(TestViewfsFileStatus.class.getSimpleName());\n+\n+  @Before public void setUp() {\n+    FileUtil.fullyDelete(TEST_DIR);\n+    assertTrue(TEST_DIR.mkdirs());\n+  }\n+\n+  @After public void tearDown() throws IOException {\n+    FileUtil.fullyDelete(TEST_DIR);\n+  }\n+\n+  /**\n+   * Tests the ACL and isDirectory returned from listStatus for directories and\n+   * files.\n+   */\n+  @Test\n+  public void testListStatusACL() throws IOException, URISyntaxException {\n+    String testfilename = \"testFileACL\";\n+    String childDirectoryName = \"testDirectoryACL\";\n+    TEST_DIR.mkdirs();\n+    File infile = new File(TEST_DIR, testfilename);\n+    final byte[] content = \"dingos\".getBytes();\n+\n+    try (FileOutputStream fos = new FileOutputStream(infile)) {\n+      fos.write(content);\n+    }\n+    assertEquals(content.length, infile.length());\n+    File childDir = new File(TEST_DIR, childDirectoryName);\n+    childDir.mkdirs();\n+\n+    Configuration conf = new Configuration();\n+    ConfigUtil.addLink(conf, \"/file\", infile.toURI());\n+    ConfigUtil.addLink(conf, \"/dir\", childDir.toURI());\n+    String fileScheme = \"file\";\n+    conf.set(String.format(\"fs.%s.impl\", fileScheme),\n+        ViewFileSystemOverloadScheme.class.getName());\n+    conf.set(String\n+        .format(FsConstants.FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN,\n+            fileScheme), LocalFileSystem.class.getName());\n+    String fileUriStr = \"file:///\";\n+    try (FileSystem vfs = FileSystem.get(new URI(fileUriStr), conf)) {\n+      assertEquals(ViewFileSystemOverloadScheme.class, vfs.getClass());\n+      FileStatus[] statuses = vfs.listStatus(new Path(\"/\"));\n+\n+      FileSystem localFs = ((ViewFileSystemOverloadScheme) vfs)\n+          .getRawFileSystem(new Path(fileUriStr), conf);\n+      FileStatus fileStat = localFs.getFileStatus(new Path(infile.getPath()));\n+      FileStatus dirStat = localFs.getFileStatus(new Path(childDir.getPath()));\n+\n+      for (FileStatus status : statuses) {\n+        if (status.getPath().getName().equals(fileScheme)) {\n+          assertEquals(fileStat.getPermission(), status.getPermission());\n+        } else {\n+          assertEquals(dirStat.getPermission(), status.getPermission());\n+        }\n+      }\n+\n+      localFs.setPermission(new Path(infile.getPath()),\n+          FsPermission.valueOf(\"-rwxr--r--\"));\n+      localFs.setPermission(new Path(childDir.getPath()),\n+          FsPermission.valueOf(\"-r--rwxr--\"));\n+\n+      statuses = vfs.listStatus(new Path(\"/\"));\n+      for (FileStatus status : statuses) {\n+        if (status.getPath().getName().equals(fileScheme)) {\n+          assertEquals(FsPermission.valueOf(\"-rwxr--r--\"),\n+              status.getPermission());\n+          assertEquals(false, status.isDirectory());\n+        } else {\n+          assertEquals(FsPermission.valueOf(\"-r--rwxr--\"),\n+              status.getPermission());\n+          assertEquals(true, status.isDirectory());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c55c9a5cf7f88f3e435a23203504072050d5e43"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY5MTU1NA==", "bodyText": "Rather than doing this below and re iterating and calling calling listStatus. Can't we add permissions too above and verify in one go like :\n`      FileStatus dirStat = localFs.getFileStatus(new Path(childDir.getPath()));\n  localFs.setPermission(new Path(infile.getPath()),\n      FsPermission.valueOf(\"-rwxr--r--\"));\n  localFs.setPermission(new Path(childDir.getPath()),\n      FsPermission.valueOf(\"-r--rwxr--\"));\n\n  FileStatus[] statuses = vfs.listStatus(new Path(\"/\"));\n  for (FileStatus status : statuses) {\n    if (status.getPath().getName().equals(fileScheme)) {\n      assertEquals(fileStat.getPermission(), status.getPermission());\n      assertEquals(FsPermission.valueOf(\"-rwxr--r--\"),\n          status.getPermission());\n      assertEquals(false, status.isDirectory());\n    } else {\n      assertEquals(dirStat.getPermission(), status.getPermission());\n      assertEquals(FsPermission.valueOf(\"-r--rwxr--\"),\n          status.getPermission());\n      assertEquals(true, status.isDirectory());\n    }\n  }`", "url": "https://github.com/apache/hadoop/pull/2084#discussion_r442691554", "createdAt": "2020-06-19T07:57:08Z", "author": {"login": "ayushtkn"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeListStatus.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.*;\n+\n+/**\n+ * ViewFsOverloadScheme ListStatus.\n+ */\n+public class TestViewFsOverloadSchemeListStatus {\n+\n+  private static final File TEST_DIR =\n+      GenericTestUtils.getTestDir(TestViewfsFileStatus.class.getSimpleName());\n+\n+  @Before public void setUp() {\n+    FileUtil.fullyDelete(TEST_DIR);\n+    assertTrue(TEST_DIR.mkdirs());\n+  }\n+\n+  @After public void tearDown() throws IOException {\n+    FileUtil.fullyDelete(TEST_DIR);\n+  }\n+\n+  /**\n+   * Tests the ACL and isDirectory returned from listStatus for directories and\n+   * files.\n+   */\n+  @Test\n+  public void testListStatusACL() throws IOException, URISyntaxException {\n+    String testfilename = \"testFileACL\";\n+    String childDirectoryName = \"testDirectoryACL\";\n+    TEST_DIR.mkdirs();\n+    File infile = new File(TEST_DIR, testfilename);\n+    final byte[] content = \"dingos\".getBytes();\n+\n+    try (FileOutputStream fos = new FileOutputStream(infile)) {\n+      fos.write(content);\n+    }\n+    assertEquals(content.length, infile.length());\n+    File childDir = new File(TEST_DIR, childDirectoryName);\n+    childDir.mkdirs();\n+\n+    Configuration conf = new Configuration();\n+    ConfigUtil.addLink(conf, \"/file\", infile.toURI());\n+    ConfigUtil.addLink(conf, \"/dir\", childDir.toURI());\n+    String fileScheme = \"file\";\n+    conf.set(String.format(\"fs.%s.impl\", fileScheme),\n+        ViewFileSystemOverloadScheme.class.getName());\n+    conf.set(String\n+        .format(FsConstants.FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN,\n+            fileScheme), LocalFileSystem.class.getName());\n+    String fileUriStr = \"file:///\";\n+    try (FileSystem vfs = FileSystem.get(new URI(fileUriStr), conf)) {\n+      assertEquals(ViewFileSystemOverloadScheme.class, vfs.getClass());\n+      FileStatus[] statuses = vfs.listStatus(new Path(\"/\"));\n+\n+      FileSystem localFs = ((ViewFileSystemOverloadScheme) vfs)\n+          .getRawFileSystem(new Path(fileUriStr), conf);\n+      FileStatus fileStat = localFs.getFileStatus(new Path(infile.getPath()));\n+      FileStatus dirStat = localFs.getFileStatus(new Path(childDir.getPath()));\n+\n+      for (FileStatus status : statuses) {\n+        if (status.getPath().getName().equals(fileScheme)) {\n+          assertEquals(fileStat.getPermission(), status.getPermission());\n+        } else {\n+          assertEquals(dirStat.getPermission(), status.getPermission());\n+        }\n+      }\n+\n+      localFs.setPermission(new Path(infile.getPath()),\n+          FsPermission.valueOf(\"-rwxr--r--\"));\n+      localFs.setPermission(new Path(childDir.getPath()),\n+          FsPermission.valueOf(\"-r--rwxr--\"));\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c55c9a5cf7f88f3e435a23203504072050d5e43"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY5MjE3Ng==", "bodyText": "Ditto. Can be a fallback to original", "url": "https://github.com/apache/hadoop/pull/2084#discussion_r442692176", "createdAt": "2020-06-19T07:58:25Z", "author": {"login": "ayushtkn"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFs.java", "diffHunk": "@@ -999,39 +1007,53 @@ public int getUriDefaultPort() {\n      * will be listed in the returned result.\n      */\n     @Override\n-    public FileStatus[] listStatus(final Path f) throws AccessControlException,\n-        IOException {\n+    public FileStatus[] listStatus(final Path f) throws IOException {\n       checkPathIsSlash(f);\n       FileStatus[] fallbackStatuses = listStatusForFallbackLink();\n       FileStatus[] result = new FileStatus[theInternalDir.getChildren().size()];\n       int i = 0;\n       for (Entry<String, INode<AbstractFileSystem>> iEntry :\n           theInternalDir.getChildren().entrySet()) {\n         INode<AbstractFileSystem> inode = iEntry.getValue();\n-\n-        \n+        Path path = new Path(inode.fullPath).makeQualified(myUri, null);\n         if (inode.isLink()) {\n           INodeLink<AbstractFileSystem> link = \n             (INodeLink<AbstractFileSystem>) inode;\n \n+          if (showMountLinksAsSymlinks) {\n+            // To maintain backward compatibility, with default option(showing\n+            // mount links as symlinks), we will represent target link as\n+            // symlink and rest other properties are belongs to mount link only.\n+            result[i++] =\n+                new FileStatus(0, false, 0, 0, creationTime, creationTime,\n+                    PERMISSION_555, ugi.getShortUserName(),\n+                    ugi.getPrimaryGroupName(), link.getTargetLink(),\n+                    path);\n+            continue;\n+          }\n+\n+          //  We will represent as non-symlinks. Here it will show target\n+          //  directory/file properties like permissions, isDirectory etc on\n+          //  mount path. The path will be a mount link path and isDirectory is\n+          //  true if target is dir, otherwise false.\n+          String linkedPath = link.getTargetFileSystem().getUri().getPath();\n+          if (\"\".equals(linkedPath)) {\n+            linkedPath = \"/\";\n+          }\n           try {\n-            String linkedPath = link.getTargetFileSystem().getUri().getPath();\n-            FileStatus status = ((ChRootedFs)link.getTargetFileSystem())\n-                .getMyFs().getFileStatus(new Path(linkedPath));\n-            result[i++] = new FileStatus(status.getLen(), false,\n-              status.getReplication(), status.getBlockSize(),\n-              status.getModificationTime(), status.getAccessTime(),\n-              status.getPermission(), status.getOwner(), status.getGroup(),\n-              link.getTargetLink(),\n-              new Path(inode.fullPath).makeQualified(\n-                  myUri, null));\n+            FileStatus status =\n+                ((ChRootedFs) link.getTargetFileSystem()).getMyFs()\n+                    .getFileStatus(new Path(linkedPath));\n+            result[i++] = new FileStatus(status.getLen(), status.isDirectory(),\n+                status.getReplication(), status.getBlockSize(),\n+                status.getModificationTime(), status.getAccessTime(),\n+                status.getPermission(), status.getOwner(), status.getGroup(),\n+                null, path);\n           } catch (FileNotFoundException ex) {\n-            result[i++] = new FileStatus(0, false, 0, 0,\n-              creationTime, creationTime, PERMISSION_555,\n-              ugi.getShortUserName(), ugi.getPrimaryGroupName(),\n-              link.getTargetLink(),\n-              new Path(inode.fullPath).makeQualified(\n-                  myUri, null));\n+            LOG.warn(\"Cannot get one of the children's(\" + path\n+                + \")  target path(\" + link.getTargetFileSystem().getUri()\n+                + \") file status.\", ex);\n+            throw ex;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c55c9a5cf7f88f3e435a23203504072050d5e43"}, "originalPosition": 115}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2d2ec00ca3c7d4f5d5706153b3eab162262fddfc", "author": {"user": {"login": "umamaheswararao", "name": "Uma Maheswara Rao G"}}, "url": "https://github.com/apache/hadoop/commit/2d2ec00ca3c7d4f5d5706153b3eab162262fddfc", "committedDate": "2020-06-19T17:55:31Z", "message": "Fixed a checkstyle and few review comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM0MzQ3ODI0", "url": "https://github.com/apache/hadoop/pull/2084#pullrequestreview-434347824", "createdAt": "2020-06-19T21:23:30Z", "commit": {"oid": "2d2ec00ca3c7d4f5d5706153b3eab162262fddfc"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQyMToyMzozMFrOGmhkrQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQyMToyMzozMFrOGmhkrQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA0OTEzMw==", "bodyText": "is link not required after cluster?\nfs.viewfs.mountable.cluster.link./user", "url": "https://github.com/apache/hadoop/pull/2084#discussion_r443049133", "createdAt": "2020-06-19T21:23:30Z", "author": {"login": "ayushtkn"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/site/markdown/ViewFsOverloadScheme.md", "diffHunk": "@@ -55,44 +55,44 @@ Here `<scheme>` should be same as the uri-scheme configured in fs.defautFS. For\n \n **Example 1:**\n \n-If users want some of their existing cluster (`hdfs://mycluster`) data to mount with hdfs(`hdfs://mycluster`) and other object store clusters(`o3fs://bucket1.volume1.omhost/`, `s3a://bucket1/`), the following example configurations can show how to add mount links.\n+If users want some of their existing cluster (`hdfs://cluster`) data to mount with hdfs(`hdfs://cluster`) and other object store clusters(`o3fs://bucket1.volume1.omhost/`, `s3a://bucket1/`), the following example configurations can show how to add mount links.\n \n \n ```xml\n <property>\n-  <name>fs.viewfs.mounttable.Cluster./user</name>\n-  <value>hdfs://mycluster/user</value>\n+  <name>fs.viewfs.mounttable.cluster./user</name>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d2ec00ca3c7d4f5d5706153b3eab162262fddfc"}, "originalPosition": 12}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "14577639dcc74a2b0ec0ff67191b9739d8b3a2d1", "author": {"user": {"login": "umamaheswararao", "name": "Uma Maheswara Rao G"}}, "url": "https://github.com/apache/hadoop/commit/14577639dcc74a2b0ec0ff67191b9739d8b3a2d1", "committedDate": "2020-06-20T00:27:45Z", "message": "Fixed few doc issues"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM0NDA4MDMx", "url": "https://github.com/apache/hadoop/pull/2084#pullrequestreview-434408031", "createdAt": "2020-06-20T06:45:41Z", "commit": {"oid": "14577639dcc74a2b0ec0ff67191b9739d8b3a2d1"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4052, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}