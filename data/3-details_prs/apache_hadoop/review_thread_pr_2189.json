{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDYyNTA0MjY4", "number": 2189, "reviewThreads": {"totalCount": 15, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQwNjo0MDoyOFrOEbbQXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMFQwNzozOToxN1rOElZvXA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk3MTkzNTY1OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockStoragePolicySuite.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQwNjo0MDoyOFrOHFWdfg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQxMjo0MDoyOFrOHHKI5A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTM3MjkyNg==", "bodyText": "nit: I see other variables are using lower case, could we change this name to allnvdimmId", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r475372926", "createdAt": "2020-08-24T06:40:28Z", "author": {"login": "liuml07"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockStoragePolicySuite.java", "diffHunk": "@@ -63,6 +63,12 @@ public static BlockStoragePolicySuite createDefaultSuite(\n         new StorageType[]{StorageType.DISK},\n         new StorageType[]{StorageType.DISK},\n         true);    // Cannot be changed on regular files, but inherited.\n+    final byte allNVDIMMId = HdfsConstants.StoragePolicy.ALL_NVDIMM.value();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bb02bd8718d81805aeba6b758772c326037bd77a"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjEwOTI3Mw==", "bodyText": "\u201callnvdimmId\u201d is more applicable in the situation.", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r476109273", "createdAt": "2020-08-25T03:20:05Z", "author": {"login": "YaYun-Wang"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockStoragePolicySuite.java", "diffHunk": "@@ -63,6 +63,12 @@ public static BlockStoragePolicySuite createDefaultSuite(\n         new StorageType[]{StorageType.DISK},\n         new StorageType[]{StorageType.DISK},\n         true);    // Cannot be changed on regular files, but inherited.\n+    final byte allNVDIMMId = HdfsConstants.StoragePolicy.ALL_NVDIMM.value();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTM3MjkyNg=="}, "originalCommit": {"oid": "bb02bd8718d81805aeba6b758772c326037bd77a"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjE0NDk2NQ==", "bodyText": "The overall change looks good to me, thanks! I will finish the review of testing in 1/2 days and provide more input.\nI also will check which isTransient() will need to be replaced with isRAM(). It seems case by case for all usages. One simple question is for this NVDIMM storage type, we save the checksum file right?\n\nThe calculation of checksum in hadoop is the responsibility of clients and datanodes, each storage media including NVDIMM participates in the checksum of data.", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r476144965", "createdAt": "2020-08-25T04:12:16Z", "author": {"login": "YaYun-Wang"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockStoragePolicySuite.java", "diffHunk": "@@ -63,6 +63,12 @@ public static BlockStoragePolicySuite createDefaultSuite(\n         new StorageType[]{StorageType.DISK},\n         new StorageType[]{StorageType.DISK},\n         true);    // Cannot be changed on regular files, but inherited.\n+    final byte allNVDIMMId = HdfsConstants.StoragePolicy.ALL_NVDIMM.value();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTM3MjkyNg=="}, "originalCommit": {"oid": "bb02bd8718d81805aeba6b758772c326037bd77a"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjIxNDk3MA==", "bodyText": "Yes I agree the checksum calculation and read is built-in hadoop. I'm thinking of more about: do we need checksum for this storage type? For example, the RAM_DISK does not need checksum as far as I remember. This is RAM, but this also survives the service restarts, so checksum makes sense for data integrity. The code so far looks good regarding this.", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r476214970", "createdAt": "2020-08-25T06:48:43Z", "author": {"login": "liuml07"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockStoragePolicySuite.java", "diffHunk": "@@ -63,6 +63,12 @@ public static BlockStoragePolicySuite createDefaultSuite(\n         new StorageType[]{StorageType.DISK},\n         new StorageType[]{StorageType.DISK},\n         true);    // Cannot be changed on regular files, but inherited.\n+    final byte allNVDIMMId = HdfsConstants.StoragePolicy.ALL_NVDIMM.value();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTM3MjkyNg=="}, "originalCommit": {"oid": "bb02bd8718d81805aeba6b758772c326037bd77a"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzI2ODE5Ng==", "bodyText": "Yes I agree the checksum calculation and read is built-in hadoop. I'm thinking of more about: do we need checksum for this storage type? For example, the RAM_DISK does not need checksum as far as I remember. This is RAM, but this also survives the service restarts, so checksum makes sense for data integrity. The code so far looks good regarding this.\n\nYes, NVDIMM, like ordinary persistent storage type, such as DISK, SSD, etc. requires  checksum too.", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r477268196", "createdAt": "2020-08-26T12:40:28Z", "author": {"login": "YaYun-Wang"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockStoragePolicySuite.java", "diffHunk": "@@ -63,6 +63,12 @@ public static BlockStoragePolicySuite createDefaultSuite(\n         new StorageType[]{StorageType.DISK},\n         new StorageType[]{StorageType.DISK},\n         true);    // Cannot be changed on regular files, but inherited.\n+    final byte allNVDIMMId = HdfsConstants.StoragePolicy.ALL_NVDIMM.value();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTM3MjkyNg=="}, "originalCommit": {"oid": "bb02bd8718d81805aeba6b758772c326037bd77a"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk3MTk4ODk2OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/site/markdown/ArchivalStorage.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQwNzowMToxNlrOHFW9tg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQwMzo0Mzo1OVrOHGEZ7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTM4MTE3NA==", "bodyText": "is added for supporting archival storage. What is the main purpose of this use case? I don't believe this is for \"archival storage\".\nAlso, this sentence is a bit verbose to me. Do you think we can make it concise, something like this?\n\nFrom Hadoop 3.4, a new storage type *NVDIMM* is added for supporting writing replica files in non-volatile \nmemory that has the capability to hold saved data even if the power is turned off.\n\nIf you instead prefer keeping the current phrase, correct the syntax error by s/retains/retain/ and s/datastored/data stored.", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r475381174", "createdAt": "2020-08-24T07:01:16Z", "author": {"login": "liuml07"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/site/markdown/ArchivalStorage.md", "diffHunk": "@@ -27,15 +27,17 @@ The frameworks provided by Heterogeneous Storage and Archival Storage generalize\n Storage Types and Storage Policies\n ----------------------------------\n \n-### Storage Types: ARCHIVE, DISK, SSD and RAM\\_DISK\n+### Storage Types: ARCHIVE, DISK, SSD, NVDIMM and RAM\\_DISK\n \n The first phase of [Heterogeneous Storage (HDFS-2832)](https://issues.apache.org/jira/browse/HDFS-2832) changed datanode storage model from a single storage, which may correspond to multiple physical storage medias, to a collection of storages with each storage corresponding to a physical storage media. It also added the notion of storage types, DISK and SSD, where DISK is the default storage type.\n \n A new storage type *ARCHIVE*, which has high storage density (petabyte of storage) but little compute power, is added for supporting archival storage.\n \n Another new storage type *RAM\\_DISK* is added for supporting writing single replica files in memory.\n \n-### Storage Policies: Hot, Warm, Cold, All\\_SSD, One\\_SSD, Lazy\\_Persist and Provided\n+And a new storage type *NVDIMM*, which is a non-volatile memory that will retains the datastored on the memory when the computer is powered down or system crashes and restore the data when the machine powered on, is added for supporting archival storage.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bb02bd8718d81805aeba6b758772c326037bd77a"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjEyNTY3OQ==", "bodyText": "The original description is indeed inaccurate , and  has been modified as suggested.", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r476125679", "createdAt": "2020-08-25T03:43:59Z", "author": {"login": "YaYun-Wang"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/site/markdown/ArchivalStorage.md", "diffHunk": "@@ -27,15 +27,17 @@ The frameworks provided by Heterogeneous Storage and Archival Storage generalize\n Storage Types and Storage Policies\n ----------------------------------\n \n-### Storage Types: ARCHIVE, DISK, SSD and RAM\\_DISK\n+### Storage Types: ARCHIVE, DISK, SSD, NVDIMM and RAM\\_DISK\n \n The first phase of [Heterogeneous Storage (HDFS-2832)](https://issues.apache.org/jira/browse/HDFS-2832) changed datanode storage model from a single storage, which may correspond to multiple physical storage medias, to a collection of storages with each storage corresponding to a physical storage media. It also added the notion of storage types, DISK and SSD, where DISK is the default storage type.\n \n A new storage type *ARCHIVE*, which has high storage density (petabyte of storage) but little compute power, is added for supporting archival storage.\n \n Another new storage type *RAM\\_DISK* is added for supporting writing single replica files in memory.\n \n-### Storage Policies: Hot, Warm, Cold, All\\_SSD, One\\_SSD, Lazy\\_Persist and Provided\n+And a new storage type *NVDIMM*, which is a non-volatile memory that will retains the datastored on the memory when the computer is powered down or system crashes and restore the data when the machine powered on, is added for supporting archival storage.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTM4MTE3NA=="}, "originalCommit": {"oid": "bb02bd8718d81805aeba6b758772c326037bd77a"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk3OTkyMzIxOnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StorageType.java", "isResolved": true, "comments": {"totalCount": 15, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQxODoxMzoxNFrOHGkF7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwNDo1MzoyN1rOHXJq9w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NDg0Ng==", "bodyText": "will RAM_DISK and NVDIMM co-exist..? if co-exist's, why can't we name NVDIM itself..?", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r476644846", "createdAt": "2020-08-25T18:13:14Z", "author": {"login": "brahmareddybattula"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StorageType.java", "diffHunk": "@@ -34,28 +34,35 @@\n @InterfaceStability.Unstable\n public enum StorageType {\n   // sorted by the speed of the storage types, from fast to slow\n-  RAM_DISK(true),\n-  SSD(false),\n-  DISK(false),\n-  ARCHIVE(false),\n-  PROVIDED(false);\n+  RAM_DISK(true, true),\n+  NVDIMM(false, true),\n+  SSD(false, false),\n+  DISK(false, false),\n+  ARCHIVE(false, false),\n+  PROVIDED(false, false);\n \n   private final boolean isTransient;\n+  private final boolean isRAM;\n \n   public static final StorageType DEFAULT = DISK;\n \n   public static final StorageType[] EMPTY_ARRAY = {};\n \n   private static final StorageType[] VALUES = values();\n \n-  StorageType(boolean isTransient) {\n+  StorageType(boolean isTransient, boolean isRAM) {\n     this.isTransient = isTransient;\n+    this.isRAM = isRAM;\n   }\n \n   public boolean isTransient() {\n     return isTransient;\n   }\n \n+  public boolean isRAM() {\n+    return isRAM;\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84c30785e64d6419d702c468a0a6b22b6c855bc6"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY3OTk3Nw==", "bodyText": "why can't we name NVDIM itself..?\n\nI don't think I get the question totally. Yes they can co-exist. You can have some folders being RAM_DISK storage and some folders being NVDIMM storage. RAM_DISK will get data lost after process restart, so it is transient. NVDIMM is not transient.", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r476679977", "createdAt": "2020-08-25T19:16:25Z", "author": {"login": "liuml07"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StorageType.java", "diffHunk": "@@ -34,28 +34,35 @@\n @InterfaceStability.Unstable\n public enum StorageType {\n   // sorted by the speed of the storage types, from fast to slow\n-  RAM_DISK(true),\n-  SSD(false),\n-  DISK(false),\n-  ARCHIVE(false),\n-  PROVIDED(false);\n+  RAM_DISK(true, true),\n+  NVDIMM(false, true),\n+  SSD(false, false),\n+  DISK(false, false),\n+  ARCHIVE(false, false),\n+  PROVIDED(false, false);\n \n   private final boolean isTransient;\n+  private final boolean isRAM;\n \n   public static final StorageType DEFAULT = DISK;\n \n   public static final StorageType[] EMPTY_ARRAY = {};\n \n   private static final StorageType[] VALUES = values();\n \n-  StorageType(boolean isTransient) {\n+  StorageType(boolean isTransient, boolean isRAM) {\n     this.isTransient = isTransient;\n+    this.isRAM = isRAM;\n   }\n \n   public boolean isTransient() {\n     return isTransient;\n   }\n \n+  public boolean isRAM() {\n+    return isRAM;\n+  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NDg0Ng=="}, "originalCommit": {"oid": "84c30785e64d6419d702c468a0a6b22b6c855bc6"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjI0OTYyMw==", "bodyText": "As this will be co-exists. I was suggesting to change the method name to something like \"isNvdimm\" which was revlant here", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r482249623", "createdAt": "2020-09-02T17:38:25Z", "author": {"login": "brahmareddybattula"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StorageType.java", "diffHunk": "@@ -34,28 +34,35 @@\n @InterfaceStability.Unstable\n public enum StorageType {\n   // sorted by the speed of the storage types, from fast to slow\n-  RAM_DISK(true),\n-  SSD(false),\n-  DISK(false),\n-  ARCHIVE(false),\n-  PROVIDED(false);\n+  RAM_DISK(true, true),\n+  NVDIMM(false, true),\n+  SSD(false, false),\n+  DISK(false, false),\n+  ARCHIVE(false, false),\n+  PROVIDED(false, false);\n \n   private final boolean isTransient;\n+  private final boolean isRAM;\n \n   public static final StorageType DEFAULT = DISK;\n \n   public static final StorageType[] EMPTY_ARRAY = {};\n \n   private static final StorageType[] VALUES = values();\n \n-  StorageType(boolean isTransient) {\n+  StorageType(boolean isTransient, boolean isRAM) {\n     this.isTransient = isTransient;\n+    this.isRAM = isRAM;\n   }\n \n   public boolean isTransient() {\n     return isTransient;\n   }\n \n+  public boolean isRAM() {\n+    return isRAM;\n+  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NDg0Ng=="}, "originalCommit": {"oid": "84c30785e64d6419d702c468a0a6b22b6c855bc6"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjM0MTIyOA==", "bodyText": "No. Both RAM_DISK and NVDIMM will return true here.", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r482341228", "createdAt": "2020-09-02T19:30:23Z", "author": {"login": "liuml07"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StorageType.java", "diffHunk": "@@ -34,28 +34,35 @@\n @InterfaceStability.Unstable\n public enum StorageType {\n   // sorted by the speed of the storage types, from fast to slow\n-  RAM_DISK(true),\n-  SSD(false),\n-  DISK(false),\n-  ARCHIVE(false),\n-  PROVIDED(false);\n+  RAM_DISK(true, true),\n+  NVDIMM(false, true),\n+  SSD(false, false),\n+  DISK(false, false),\n+  ARCHIVE(false, false),\n+  PROVIDED(false, false);\n \n   private final boolean isTransient;\n+  private final boolean isRAM;\n \n   public static final StorageType DEFAULT = DISK;\n \n   public static final StorageType[] EMPTY_ARRAY = {};\n \n   private static final StorageType[] VALUES = values();\n \n-  StorageType(boolean isTransient) {\n+  StorageType(boolean isTransient, boolean isRAM) {\n     this.isTransient = isTransient;\n+    this.isRAM = isRAM;\n   }\n \n   public boolean isTransient() {\n     return isTransient;\n   }\n \n+  public boolean isRAM() {\n+    return isRAM;\n+  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NDg0Ng=="}, "originalCommit": {"oid": "84c30785e64d6419d702c468a0a6b22b6c855bc6"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzEyNTYyMw==", "bodyText": "Balancer and mover will not move the blocks based on the isTransient ( they call getMovableTypes(..))..The blocks which are in NVDIMM shouldn't moved I feel(as this also exists in RAM and no need to move),but as per this change it will move.", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r483125623", "createdAt": "2020-09-03T16:59:01Z", "author": {"login": "brahmareddybattula"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StorageType.java", "diffHunk": "@@ -34,28 +34,35 @@\n @InterfaceStability.Unstable\n public enum StorageType {\n   // sorted by the speed of the storage types, from fast to slow\n-  RAM_DISK(true),\n-  SSD(false),\n-  DISK(false),\n-  ARCHIVE(false),\n-  PROVIDED(false);\n+  RAM_DISK(true, true),\n+  NVDIMM(false, true),\n+  SSD(false, false),\n+  DISK(false, false),\n+  ARCHIVE(false, false),\n+  PROVIDED(false, false);\n \n   private final boolean isTransient;\n+  private final boolean isRAM;\n \n   public static final StorageType DEFAULT = DISK;\n \n   public static final StorageType[] EMPTY_ARRAY = {};\n \n   private static final StorageType[] VALUES = values();\n \n-  StorageType(boolean isTransient) {\n+  StorageType(boolean isTransient, boolean isRAM) {\n     this.isTransient = isTransient;\n+    this.isRAM = isRAM;\n   }\n \n   public boolean isTransient() {\n     return isTransient;\n   }\n \n+  public boolean isRAM() {\n+    return isRAM;\n+  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NDg0Ng=="}, "originalCommit": {"oid": "84c30785e64d6419d702c468a0a6b22b6c855bc6"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzE5NDEyNQ==", "bodyText": "Oh, I was thinking that allowing Balancer to move the NVDIMM data is by design since they are not volatile. But if that is not case, then we can update Balancer code by replacing isTransient() call with isRAM() call. Not sure if this makes more sense?", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r483194125", "createdAt": "2020-09-03T19:03:28Z", "author": {"login": "liuml07"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StorageType.java", "diffHunk": "@@ -34,28 +34,35 @@\n @InterfaceStability.Unstable\n public enum StorageType {\n   // sorted by the speed of the storage types, from fast to slow\n-  RAM_DISK(true),\n-  SSD(false),\n-  DISK(false),\n-  ARCHIVE(false),\n-  PROVIDED(false);\n+  RAM_DISK(true, true),\n+  NVDIMM(false, true),\n+  SSD(false, false),\n+  DISK(false, false),\n+  ARCHIVE(false, false),\n+  PROVIDED(false, false);\n \n   private final boolean isTransient;\n+  private final boolean isRAM;\n \n   public static final StorageType DEFAULT = DISK;\n \n   public static final StorageType[] EMPTY_ARRAY = {};\n \n   private static final StorageType[] VALUES = values();\n \n-  StorageType(boolean isTransient) {\n+  StorageType(boolean isTransient, boolean isRAM) {\n     this.isTransient = isTransient;\n+    this.isRAM = isRAM;\n   }\n \n   public boolean isTransient() {\n     return isTransient;\n   }\n \n+  public boolean isRAM() {\n+    return isRAM;\n+  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NDg0Ng=="}, "originalCommit": {"oid": "84c30785e64d6419d702c468a0a6b22b6c855bc6"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzQxMDE4Mw==", "bodyText": "NVDIMM is special RAM, the data above can be stored persistently. It can be regarded as a general hardware device. We don't have to consider what storage type it is, balancer and mover can be applied on NVDIMM, therefore, I think it is better to use isRAM to determine whether to use mover . In addition, neither RAM nor nvdimm need FsDatasetCache, and isTransient() used to determine whether FsDatasetCache is needed", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r483410183", "createdAt": "2020-09-04T06:19:16Z", "author": {"login": "YaYun-Wang"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StorageType.java", "diffHunk": "@@ -34,28 +34,35 @@\n @InterfaceStability.Unstable\n public enum StorageType {\n   // sorted by the speed of the storage types, from fast to slow\n-  RAM_DISK(true),\n-  SSD(false),\n-  DISK(false),\n-  ARCHIVE(false),\n-  PROVIDED(false);\n+  RAM_DISK(true, true),\n+  NVDIMM(false, true),\n+  SSD(false, false),\n+  DISK(false, false),\n+  ARCHIVE(false, false),\n+  PROVIDED(false, false);\n \n   private final boolean isTransient;\n+  private final boolean isRAM;\n \n   public static final StorageType DEFAULT = DISK;\n \n   public static final StorageType[] EMPTY_ARRAY = {};\n \n   private static final StorageType[] VALUES = values();\n \n-  StorageType(boolean isTransient) {\n+  StorageType(boolean isTransient, boolean isRAM) {\n     this.isTransient = isTransient;\n+    this.isRAM = isRAM;\n   }\n \n   public boolean isTransient() {\n     return isTransient;\n   }\n \n+  public boolean isRAM() {\n+    return isRAM;\n+  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NDg0Ng=="}, "originalCommit": {"oid": "84c30785e64d6419d702c468a0a6b22b6c855bc6"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzQxNDc1NQ==", "bodyText": "neither RAM nor nvdimm need FsDatasetCache, and isTransient() used to determine whether FsDatasetCache is needed\n\nI think we have agreed on this.\n\nI think it is better to use isRAM to determine whether to use mover\n\nSo just to be clear, if we use isRAM() to determine, both RAM_DISK and NVDIMM will return true and thus will simply disable Balancer and Mover. I was proposing that only when we do not want to move those NVDIMM data replicas around (disk/node) ever. Is that the design here? I'm fine with that but I think this contradicts with the statement that \"It can be regarded as a general hardware device. \"", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r483414755", "createdAt": "2020-09-04T06:31:47Z", "author": {"login": "liuml07"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StorageType.java", "diffHunk": "@@ -34,28 +34,35 @@\n @InterfaceStability.Unstable\n public enum StorageType {\n   // sorted by the speed of the storage types, from fast to slow\n-  RAM_DISK(true),\n-  SSD(false),\n-  DISK(false),\n-  ARCHIVE(false),\n-  PROVIDED(false);\n+  RAM_DISK(true, true),\n+  NVDIMM(false, true),\n+  SSD(false, false),\n+  DISK(false, false),\n+  ARCHIVE(false, false),\n+  PROVIDED(false, false);\n \n   private final boolean isTransient;\n+  private final boolean isRAM;\n \n   public static final StorageType DEFAULT = DISK;\n \n   public static final StorageType[] EMPTY_ARRAY = {};\n \n   private static final StorageType[] VALUES = values();\n \n-  StorageType(boolean isTransient) {\n+  StorageType(boolean isTransient, boolean isRAM) {\n     this.isTransient = isTransient;\n+    this.isRAM = isRAM;\n   }\n \n   public boolean isTransient() {\n     return isTransient;\n   }\n \n+  public boolean isRAM() {\n+    return isRAM;\n+  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NDg0Ng=="}, "originalCommit": {"oid": "84c30785e64d6419d702c468a0a6b22b6c855bc6"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzUxNTUyMQ==", "bodyText": "Sorry, I made a big mistake in the above reply. Our design idea is: NVDIMM supports mover and balancer, and isTransient()  applied in the case. isRAM() is only used to \u201cFsDatasetCache\u201d judgment. So the current code is reasonable, i think it\u2018s not necessary to modify the code.", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r483515521", "createdAt": "2020-09-04T09:54:42Z", "author": {"login": "YaYun-Wang"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StorageType.java", "diffHunk": "@@ -34,28 +34,35 @@\n @InterfaceStability.Unstable\n public enum StorageType {\n   // sorted by the speed of the storage types, from fast to slow\n-  RAM_DISK(true),\n-  SSD(false),\n-  DISK(false),\n-  ARCHIVE(false),\n-  PROVIDED(false);\n+  RAM_DISK(true, true),\n+  NVDIMM(false, true),\n+  SSD(false, false),\n+  DISK(false, false),\n+  ARCHIVE(false, false),\n+  PROVIDED(false, false);\n \n   private final boolean isTransient;\n+  private final boolean isRAM;\n \n   public static final StorageType DEFAULT = DISK;\n \n   public static final StorageType[] EMPTY_ARRAY = {};\n \n   private static final StorageType[] VALUES = values();\n \n-  StorageType(boolean isTransient) {\n+  StorageType(boolean isTransient, boolean isRAM) {\n     this.isTransient = isTransient;\n+    this.isRAM = isRAM;\n   }\n \n   public boolean isTransient() {\n     return isTransient;\n   }\n \n+  public boolean isRAM() {\n+    return isRAM;\n+  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NDg0Ng=="}, "originalCommit": {"oid": "84c30785e64d6419d702c468a0a6b22b6c855bc6"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mzc4MDc1OA==", "bodyText": "Thanks for clarification, @YaYun-Wang I think now we both are on the same page. @brahmareddybattula Does this make sense to you? Thanks", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r483780758", "createdAt": "2020-09-04T18:16:06Z", "author": {"login": "liuml07"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StorageType.java", "diffHunk": "@@ -34,28 +34,35 @@\n @InterfaceStability.Unstable\n public enum StorageType {\n   // sorted by the speed of the storage types, from fast to slow\n-  RAM_DISK(true),\n-  SSD(false),\n-  DISK(false),\n-  ARCHIVE(false),\n-  PROVIDED(false);\n+  RAM_DISK(true, true),\n+  NVDIMM(false, true),\n+  SSD(false, false),\n+  DISK(false, false),\n+  ARCHIVE(false, false),\n+  PROVIDED(false, false);\n \n   private final boolean isTransient;\n+  private final boolean isRAM;\n \n   public static final StorageType DEFAULT = DISK;\n \n   public static final StorageType[] EMPTY_ARRAY = {};\n \n   private static final StorageType[] VALUES = values();\n \n-  StorageType(boolean isTransient) {\n+  StorageType(boolean isTransient, boolean isRAM) {\n     this.isTransient = isTransient;\n+    this.isRAM = isRAM;\n   }\n \n   public boolean isTransient() {\n     return isTransient;\n   }\n \n+  public boolean isRAM() {\n+    return isRAM;\n+  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NDg0Ng=="}, "originalCommit": {"oid": "84c30785e64d6419d702c468a0a6b22b6c855bc6"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTQ2MzQ0MQ==", "bodyText": "@brahmareddybattula  would you please have a look for this? Thanks.", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r485463441", "createdAt": "2020-09-09T09:14:43Z", "author": {"login": "huangtianhua"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StorageType.java", "diffHunk": "@@ -34,28 +34,35 @@\n @InterfaceStability.Unstable\n public enum StorageType {\n   // sorted by the speed of the storage types, from fast to slow\n-  RAM_DISK(true),\n-  SSD(false),\n-  DISK(false),\n-  ARCHIVE(false),\n-  PROVIDED(false);\n+  RAM_DISK(true, true),\n+  NVDIMM(false, true),\n+  SSD(false, false),\n+  DISK(false, false),\n+  ARCHIVE(false, false),\n+  PROVIDED(false, false);\n \n   private final boolean isTransient;\n+  private final boolean isRAM;\n \n   public static final StorageType DEFAULT = DISK;\n \n   public static final StorageType[] EMPTY_ARRAY = {};\n \n   private static final StorageType[] VALUES = values();\n \n-  StorageType(boolean isTransient) {\n+  StorageType(boolean isTransient, boolean isRAM) {\n     this.isTransient = isTransient;\n+    this.isRAM = isRAM;\n   }\n \n   public boolean isTransient() {\n     return isTransient;\n   }\n \n+  public boolean isRAM() {\n+    return isRAM;\n+  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NDg0Ng=="}, "originalCommit": {"oid": "84c30785e64d6419d702c468a0a6b22b6c855bc6"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjUzNTYzOQ==", "bodyText": "ok, By design if you dn't want to move.", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r486535639", "createdAt": "2020-09-10T18:06:45Z", "author": {"login": "brahmareddybattula"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StorageType.java", "diffHunk": "@@ -34,28 +34,35 @@\n @InterfaceStability.Unstable\n public enum StorageType {\n   // sorted by the speed of the storage types, from fast to slow\n-  RAM_DISK(true),\n-  SSD(false),\n-  DISK(false),\n-  ARCHIVE(false),\n-  PROVIDED(false);\n+  RAM_DISK(true, true),\n+  NVDIMM(false, true),\n+  SSD(false, false),\n+  DISK(false, false),\n+  ARCHIVE(false, false),\n+  PROVIDED(false, false);\n \n   private final boolean isTransient;\n+  private final boolean isRAM;\n \n   public static final StorageType DEFAULT = DISK;\n \n   public static final StorageType[] EMPTY_ARRAY = {};\n \n   private static final StorageType[] VALUES = values();\n \n-  StorageType(boolean isTransient) {\n+  StorageType(boolean isTransient, boolean isRAM) {\n     this.isTransient = isTransient;\n+    this.isRAM = isRAM;\n   }\n \n   public boolean isTransient() {\n     return isTransient;\n   }\n \n+  public boolean isRAM() {\n+    return isRAM;\n+  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NDg0Ng=="}, "originalCommit": {"oid": "84c30785e64d6419d702c468a0a6b22b6c855bc6"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU0OTY4Mg==", "bodyText": "My final query then, why can't have one NVDIMM like one SSD as this also movable and peristent..?", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r486549682", "createdAt": "2020-09-10T18:27:08Z", "author": {"login": "brahmareddybattula"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StorageType.java", "diffHunk": "@@ -34,28 +34,35 @@\n @InterfaceStability.Unstable\n public enum StorageType {\n   // sorted by the speed of the storage types, from fast to slow\n-  RAM_DISK(true),\n-  SSD(false),\n-  DISK(false),\n-  ARCHIVE(false),\n-  PROVIDED(false);\n+  RAM_DISK(true, true),\n+  NVDIMM(false, true),\n+  SSD(false, false),\n+  DISK(false, false),\n+  ARCHIVE(false, false),\n+  PROVIDED(false, false);\n \n   private final boolean isTransient;\n+  private final boolean isRAM;\n \n   public static final StorageType DEFAULT = DISK;\n \n   public static final StorageType[] EMPTY_ARRAY = {};\n \n   private static final StorageType[] VALUES = values();\n \n-  StorageType(boolean isTransient) {\n+  StorageType(boolean isTransient, boolean isRAM) {\n     this.isTransient = isTransient;\n+    this.isRAM = isRAM;\n   }\n \n   public boolean isTransient() {\n     return isTransient;\n   }\n \n+  public boolean isRAM() {\n+    return isRAM;\n+  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NDg0Ng=="}, "originalCommit": {"oid": "84c30785e64d6419d702c468a0a6b22b6c855bc6"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjcxNjQ3MQ==", "bodyText": "My final query then, why can't have one NVDIMM like one SSD as this also movable and peristent..?\n\nConsidering NVDIMM is faster, so NVDIMM does not  use FsDatasetCache() which SSD needs in the design.", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r486716471", "createdAt": "2020-09-11T01:12:40Z", "author": {"login": "YaYun-Wang"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StorageType.java", "diffHunk": "@@ -34,28 +34,35 @@\n @InterfaceStability.Unstable\n public enum StorageType {\n   // sorted by the speed of the storage types, from fast to slow\n-  RAM_DISK(true),\n-  SSD(false),\n-  DISK(false),\n-  ARCHIVE(false),\n-  PROVIDED(false);\n+  RAM_DISK(true, true),\n+  NVDIMM(false, true),\n+  SSD(false, false),\n+  DISK(false, false),\n+  ARCHIVE(false, false),\n+  PROVIDED(false, false);\n \n   private final boolean isTransient;\n+  private final boolean isRAM;\n \n   public static final StorageType DEFAULT = DISK;\n \n   public static final StorageType[] EMPTY_ARRAY = {};\n \n   private static final StorageType[] VALUES = values();\n \n-  StorageType(boolean isTransient) {\n+  StorageType(boolean isTransient, boolean isRAM) {\n     this.isTransient = isTransient;\n+    this.isRAM = isRAM;\n   }\n \n   public boolean isTransient() {\n     return isTransient;\n   }\n \n+  public boolean isRAM() {\n+    return isRAM;\n+  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NDg0Ng=="}, "originalCommit": {"oid": "84c30785e64d6419d702c468a0a6b22b6c855bc6"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDAzNzc1MQ==", "bodyText": "ok", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r494037751", "createdAt": "2020-09-24T04:53:27Z", "author": {"login": "brahmareddybattula"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StorageType.java", "diffHunk": "@@ -34,28 +34,35 @@\n @InterfaceStability.Unstable\n public enum StorageType {\n   // sorted by the speed of the storage types, from fast to slow\n-  RAM_DISK(true),\n-  SSD(false),\n-  DISK(false),\n-  ARCHIVE(false),\n-  PROVIDED(false);\n+  RAM_DISK(true, true),\n+  NVDIMM(false, true),\n+  SSD(false, false),\n+  DISK(false, false),\n+  ARCHIVE(false, false),\n+  PROVIDED(false, false);\n \n   private final boolean isTransient;\n+  private final boolean isRAM;\n \n   public static final StorageType DEFAULT = DISK;\n \n   public static final StorageType[] EMPTY_ARRAY = {};\n \n   private static final StorageType[] VALUES = values();\n \n-  StorageType(boolean isTransient) {\n+  StorageType(boolean isTransient, boolean isRAM) {\n     this.isTransient = isTransient;\n+    this.isRAM = isRAM;\n   }\n \n   public boolean isTransient() {\n     return isTransient;\n   }\n \n+  public boolean isRAM() {\n+    return isRAM;\n+  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NDg0Ng=="}, "originalCommit": {"oid": "84c30785e64d6419d702c468a0a6b22b6c855bc6"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk3OTkyNDY1OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StorageType.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQxODoxMzozOVrOHGkG4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxNzozNjowNlrOHL6Gig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NTA4OQ==", "bodyText": "Let's add only this line", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r476645089", "createdAt": "2020-08-25T18:13:39Z", "author": {"login": "brahmareddybattula"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StorageType.java", "diffHunk": "@@ -34,28 +34,35 @@\n @InterfaceStability.Unstable\n public enum StorageType {\n   // sorted by the speed of the storage types, from fast to slow\n-  RAM_DISK(true),\n-  SSD(false),\n-  DISK(false),\n-  ARCHIVE(false),\n-  PROVIDED(false);\n+  RAM_DISK(true, true),\n+  NVDIMM(false, true),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84c30785e64d6419d702c468a0a6b22b6c855bc6"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE2NjU0Ng==", "bodyText": "Sorry, I don't understand.", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r477166546", "createdAt": "2020-08-26T09:30:21Z", "author": {"login": "huangtianhua"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StorageType.java", "diffHunk": "@@ -34,28 +34,35 @@\n @InterfaceStability.Unstable\n public enum StorageType {\n   // sorted by the speed of the storage types, from fast to slow\n-  RAM_DISK(true),\n-  SSD(false),\n-  DISK(false),\n-  ARCHIVE(false),\n-  PROVIDED(false);\n+  RAM_DISK(true, true),\n+  NVDIMM(false, true),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NTA4OQ=="}, "originalCommit": {"oid": "84c30785e64d6419d702c468a0a6b22b6c855bc6"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjI0ODMzMA==", "bodyText": "As we are adding only \"NVDIMM\",So I expect one line change here but I missed that you added one more param to enum.", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r482248330", "createdAt": "2020-09-02T17:36:06Z", "author": {"login": "brahmareddybattula"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StorageType.java", "diffHunk": "@@ -34,28 +34,35 @@\n @InterfaceStability.Unstable\n public enum StorageType {\n   // sorted by the speed of the storage types, from fast to slow\n-  RAM_DISK(true),\n-  SSD(false),\n-  DISK(false),\n-  ARCHIVE(false),\n-  PROVIDED(false);\n+  RAM_DISK(true, true),\n+  NVDIMM(false, true),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NTA4OQ=="}, "originalCommit": {"oid": "84c30785e64d6419d702c468a0a6b22b6c855bc6"}, "originalPosition": 10}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk3OTkyNzgyOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsVolumeSpi.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQxODoxNDo0NVrOHGkJAg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMVQwMToxNDowNVrOHQK1ig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NTYzNA==", "bodyText": "is this will not valid anymore.?", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r476645634", "createdAt": "2020-08-25T18:14:45Z", "author": {"login": "brahmareddybattula"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsVolumeSpi.java", "diffHunk": "@@ -77,6 +77,9 @@\n   /** Returns true if the volume is NOT backed by persistent storage. */\n   boolean isTransientStorage();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84c30785e64d6419d702c468a0a6b22b6c855bc6"}, "originalPosition": 2}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzMwNDQ3OA==", "bodyText": "The \"isTransientStorage\" method is still available.\nIn the original code, isTransient() and isTransientStorage methods are used to determine whether to support FsDatasetCache, Persistent, Quota, and Movable.\nFsDatasetCache will be used When the storage type is persistent. NVDIMM is RAM to some extent, which is fast. However, NVDIMM is a persistent storage type.  Then, isTransient() and isTransientStorage()  used to determine whether to support FsDatasetCache can't meet the requirements. Therefore, we add  isRAM() and isRAMStorage()  methods to decide whether cache is supported or not. And the other functions, such as, Persistent,  Quota, and Movable judged byisTransient() and isTransientStorage()  methods.", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r477304478", "createdAt": "2020-08-26T13:34:51Z", "author": {"login": "YaYun-Wang"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsVolumeSpi.java", "diffHunk": "@@ -77,6 +77,9 @@\n   /** Returns true if the volume is NOT backed by persistent storage. */\n   boolean isTransientStorage();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NTYzNA=="}, "originalCommit": {"oid": "84c30785e64d6419d702c468a0a6b22b6c855bc6"}, "originalPosition": 2}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU0NTExMQ==", "bodyText": "Ok. Got it.", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r486545111", "createdAt": "2020-09-10T18:20:56Z", "author": {"login": "brahmareddybattula"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsVolumeSpi.java", "diffHunk": "@@ -77,6 +77,9 @@\n   /** Returns true if the volume is NOT backed by persistent storage. */\n   boolean isTransientStorage();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NTYzNA=="}, "originalCommit": {"oid": "84c30785e64d6419d702c468a0a6b22b6c855bc6"}, "originalPosition": 2}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU0ODA1Mw==", "bodyText": "So, NVDIMM is peristent storage and RAM.", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r486548053", "createdAt": "2020-09-10T18:24:54Z", "author": {"login": "brahmareddybattula"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsVolumeSpi.java", "diffHunk": "@@ -77,6 +77,9 @@\n   /** Returns true if the volume is NOT backed by persistent storage. */\n   boolean isTransientStorage();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NTYzNA=="}, "originalCommit": {"oid": "84c30785e64d6419d702c468a0a6b22b6c855bc6"}, "originalPosition": 2}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjcxNjgxMA==", "bodyText": "So, NVDIMM is peristent storage and RAM.\n\nyes, that\u2019s right.", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r486716810", "createdAt": "2020-09-11T01:14:05Z", "author": {"login": "YaYun-Wang"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsVolumeSpi.java", "diffHunk": "@@ -77,6 +77,9 @@\n   /** Returns true if the volume is NOT backed by persistent storage. */\n   boolean isTransientStorage();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NTYzNA=="}, "originalCommit": {"oid": "84c30785e64d6419d702c468a0a6b22b6c855bc6"}, "originalPosition": 2}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk3OTkzMjQ5OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/site/markdown/ArchivalStorage.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQxODoxNjowNVrOHGkL6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwOTowNzoyMVrOHHC94g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NjM3Ng==", "bodyText": "can we maintain same order here also..?", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r476646376", "createdAt": "2020-08-25T18:16:05Z", "author": {"login": "brahmareddybattula"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/site/markdown/ArchivalStorage.md", "diffHunk": "@@ -88,13 +92,14 @@ The effective storage policy can be retrieved by the \"[`storagepolicies -getStor\n ### Configuration\n \n * **dfs.storage.policy.enabled** - for enabling/disabling the storage policy feature. The default value is `true`.\n-* **dfs.storage.default.policy** - Set the default storage policy with the policy name. The default value is `HOT`.  All possible policies are defined in enum StoragePolicy, including `LAZY_PERSIST` `ALL_SSD` `ONE_SSD` `HOT` `WARM` `COLD` and `PROVIDED`.\n+* **dfs.storage.default.policy** - Set the default storage policy with the policy name. The default value is `HOT`.  All possible policies are defined in enum StoragePolicy, including `LAZY_PERSIST` `ALL_SSD` `ONE_SSD` `HOT` `WARM` `COLD` `ALL_NVDIMM` and `PROVIDED`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84c30785e64d6419d702c468a0a6b22b6c855bc6"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE1MDY5MA==", "bodyText": "sure", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r477150690", "createdAt": "2020-08-26T09:07:21Z", "author": {"login": "huangtianhua"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/site/markdown/ArchivalStorage.md", "diffHunk": "@@ -88,13 +92,14 @@ The effective storage policy can be retrieved by the \"[`storagepolicies -getStor\n ### Configuration\n \n * **dfs.storage.policy.enabled** - for enabling/disabling the storage policy feature. The default value is `true`.\n-* **dfs.storage.default.policy** - Set the default storage policy with the policy name. The default value is `HOT`.  All possible policies are defined in enum StoragePolicy, including `LAZY_PERSIST` `ALL_SSD` `ONE_SSD` `HOT` `WARM` `COLD` and `PROVIDED`.\n+* **dfs.storage.default.policy** - Set the default storage policy with the policy name. The default value is `HOT`.  All possible policies are defined in enum StoragePolicy, including `LAZY_PERSIST` `ALL_SSD` `ONE_SSD` `HOT` `WARM` `COLD` `ALL_NVDIMM` and `PROVIDED`.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NjM3Ng=="}, "originalCommit": {"oid": "84c30785e64d6419d702c468a0a6b22b6c855bc6"}, "originalPosition": 50}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0ODg0NzI4OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMlQwODo1NzoxNVrOHQzyIw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMlQwODo1NzoxNVrOHQzyIw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzM4NzY4Mw==", "bodyText": "nit: we can make it clear what the volume is.\n LOG.warn(\"Caching not supported on block with id {} since the volume \"\n    + \"is backed by {} which is RAM.\", blockId, volume);", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r487387683", "createdAt": "2020-09-12T08:57:15Z", "author": {"login": "liuml07"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java", "diffHunk": "@@ -2298,9 +2298,9 @@ private void cacheBlock(String bpid, long blockId) {\n               \": volume was not an instance of FsVolumeImpl.\");\n           return;\n         }\n-        if (volume.isTransientStorage()) {\n+        if (volume.isRAMStorage()) {\n           LOG.warn(\"Caching not supported on block with id \" + blockId +\n-              \" since the volume is backed by RAM.\");\n+              \" since the volume is backed by RAM_DISK or NVDIMM.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dea6c63a91a7d0815dbdb405f8cf1c219365aeca"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA3NjUyMTUzOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMFQwNzowMzowOFrOHU4obg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMFQwNzowMzowOFrOHU4obg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY2MTQyMg==", "bodyText": "nit: Add a blank line before every new rack, aka\n  \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r1\",\n  \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r2\",\n  \"/l3/d5/r1\", \"/l3/d5/r1\", \"/l3/d5/r2\"};\n\ncan be\n  \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r1\",\n  \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r2\",\n  \n  \"/l3/d5/r1\", \"/l3/d5/r1\", \"/l3/d5/r2\"};\n\nSame to the hosts, please  make the last three hosts in a new line so that racks, hosts, and types can be easily read with eyeballs.", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r491661422", "createdAt": "2020-09-20T07:03:08Z", "author": {"login": "liuml07"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java", "diffHunk": "@@ -64,37 +64,42 @@ public void setupDatanodes() {\n     final String[] racks = {\n         \"/l1/d1/r1\", \"/l1/d1/r1\", \"/l1/d1/r2\", \"/l1/d1/r2\", \"/l1/d1/r2\",\n \n-        \"/l1/d2/r3\", \"/l1/d2/r3\", \"/l1/d2/r3\",\n+        \"/l1/d2/r3\", \"/l1/d2/r3\", \"/l1/d2/r3\", \"/l1/d2/r3\",\n \n         \"/l2/d3/r1\", \"/l2/d3/r2\", \"/l2/d3/r3\", \"/l2/d3/r4\", \"/l2/d3/r5\",\n \n         \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r1\",\n-        \"/l2/d4/r1\", \"/l2/d4/r1\"};\n+        \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r2\",\n+        \"/l3/d5/r1\", \"/l3/d5/r1\", \"/l3/d5/r2\"};", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dea6c63a91a7d0815dbdb405f8cf1c219365aeca"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA3NjUyMjg5OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMFQwNzowNToxN1rOHU4pBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMFQwNzowNToxN1rOHU4pBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY2MTU3Mg==", "bodyText": "nit: replace with assertEquals(4, d2info.get(\"r3\").size());", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r491661572", "createdAt": "2020-09-20T07:05:17Z", "author": {"login": "liuml07"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java", "diffHunk": "@@ -120,10 +125,11 @@ public void testGetStorageTypeInfo() throws Exception {\n     HashMap<String, EnumMap<StorageType, Integer>> d2info =\n         d2.getChildrenStorageInfo();\n     assertEquals(1, d2info.keySet().size());\n-    assertTrue(d2info.get(\"r3\").size() == 3);\n+    assertTrue(d2info.get(\"r3\").size() == 4);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dea6c63a91a7d0815dbdb405f8cf1c219365aeca"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA3NjUzNzE5OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMFQwNzoyNzoyN1rOHU4vjQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMFQwNzoyNzoyN1rOHU4vjQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY2MzI0NQ==", "bodyText": "nit: why not use 34.34.34.34?", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r491663245", "createdAt": "2020-09-20T07:27:27Z", "author": {"login": "liuml07"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java", "diffHunk": "@@ -182,15 +212,16 @@ public void testGetStorageTypeInfo() throws Exception {\n    */\n   @Test\n   public void testAddAndRemoveTopology() throws Exception {\n-    String[] newRack = {\"/l1/d1/r1\", \"/l1/d1/r3\", \"/l1/d3/r3\", \"/l1/d3/r3\"};\n-    String[] newHost = {\"nhost1\", \"nhost2\", \"nhost3\", \"nhost4\"};\n+    String[] newRack = {\"/l1/d1/r1\", \"/l1/d1/r3\", \"/l1/d3/r3\", \"/l1/d3/r3\",\n+        \"/l1/d3/r4\"};\n+    String[] newHost = {\"nhost1\", \"nhost2\", \"nhost3\", \"nhost4\", \"nhost5\"};\n     String[] newips = {\"30.30.30.30\", \"31.31.31.31\", \"32.32.32.32\",\n-        \"33.33.33.33\"};\n+        \"33.33.33.33\", \"33.33.33.34\"};", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dea6c63a91a7d0815dbdb405f8cf1c219365aeca"}, "originalPosition": 142}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA3NjUzNzUxOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMFQwNzoyNzo1N1rOHU4vsA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMFQwNzoyNzo1N1rOHU4vsA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY2MzI4MA==", "bodyText": "nit: let's have space before and after < operator", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r491663280", "createdAt": "2020-09-20T07:27:57Z", "author": {"login": "liuml07"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java", "diffHunk": "@@ -182,15 +212,16 @@ public void testGetStorageTypeInfo() throws Exception {\n    */\n   @Test\n   public void testAddAndRemoveTopology() throws Exception {\n-    String[] newRack = {\"/l1/d1/r1\", \"/l1/d1/r3\", \"/l1/d3/r3\", \"/l1/d3/r3\"};\n-    String[] newHost = {\"nhost1\", \"nhost2\", \"nhost3\", \"nhost4\"};\n+    String[] newRack = {\"/l1/d1/r1\", \"/l1/d1/r3\", \"/l1/d3/r3\", \"/l1/d3/r3\",\n+        \"/l1/d3/r4\"};\n+    String[] newHost = {\"nhost1\", \"nhost2\", \"nhost3\", \"nhost4\", \"nhost5\"};\n     String[] newips = {\"30.30.30.30\", \"31.31.31.31\", \"32.32.32.32\",\n-        \"33.33.33.33\"};\n+        \"33.33.33.33\", \"33.33.33.34\"};\n     StorageType[] newTypes = {StorageType.DISK, StorageType.SSD,\n-        StorageType.SSD, StorageType.SSD};\n-    DatanodeDescriptor[] newDD = new DatanodeDescriptor[4];\n+        StorageType.SSD, StorageType.SSD, StorageType.NVDIMM};\n+    DatanodeDescriptor[] newDD = new DatanodeDescriptor[5];\n \n-    for (int i = 0; i<4; i++) {\n+    for (int i = 0; i<5; i++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dea6c63a91a7d0815dbdb405f8cf1c219365aeca"}, "originalPosition": 150}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA3NjUzOTgxOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockStatsMXBean.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMFQwNzozMToyMVrOHU4wvQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMVQwODoxMToxMVrOHVEvGw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY2MzU0OQ==", "bodyText": "As this if-else if-else if-else getting longer, let's use switch case?", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r491663549", "createdAt": "2020-09-20T07:31:21Z", "author": {"login": "liuml07"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockStatsMXBean.java", "diffHunk": "@@ -145,9 +150,11 @@ public void testStorageTypeStatsJMX() throws Exception {\n       Map<String,Object> storageTypeStats = (Map<String,Object>)entry.get(\"value\");\n       typesPresent.add(storageType);\n       if (storageType.equals(\"ARCHIVE\") || storageType.equals(\"DISK\") ) {\n-        assertEquals(3l, storageTypeStats.get(\"nodesInService\"));\n+        assertEquals(3L, storageTypeStats.get(\"nodesInService\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dea6c63a91a7d0815dbdb405f8cf1c219365aeca"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTg1MjQ3MQ==", "bodyText": "storageType is a parameter of \"java.lang.String\" , and switch()  does not support \"java.lang.String\" before java 1.7. So, will if-else  be more appropriate here?", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r491852471", "createdAt": "2020-09-21T07:56:39Z", "author": {"login": "YaYun-Wang"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockStatsMXBean.java", "diffHunk": "@@ -145,9 +150,11 @@ public void testStorageTypeStatsJMX() throws Exception {\n       Map<String,Object> storageTypeStats = (Map<String,Object>)entry.get(\"value\");\n       typesPresent.add(storageType);\n       if (storageType.equals(\"ARCHIVE\") || storageType.equals(\"DISK\") ) {\n-        assertEquals(3l, storageTypeStats.get(\"nodesInService\"));\n+        assertEquals(3L, storageTypeStats.get(\"nodesInService\"));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY2MzU0OQ=="}, "originalCommit": {"oid": "dea6c63a91a7d0815dbdb405f8cf1c219365aeca"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTg1Nzc5Nw==", "bodyText": "I have not used Java 7 for a while, but I remember vaguely this is actually supported?\nhttps://docs.oracle.com/javase/specs/jls/se7/html/jls-14.html", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r491857797", "createdAt": "2020-09-21T08:07:29Z", "author": {"login": "liuml07"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockStatsMXBean.java", "diffHunk": "@@ -145,9 +150,11 @@ public void testStorageTypeStatsJMX() throws Exception {\n       Map<String,Object> storageTypeStats = (Map<String,Object>)entry.get(\"value\");\n       typesPresent.add(storageType);\n       if (storageType.equals(\"ARCHIVE\") || storageType.equals(\"DISK\") ) {\n-        assertEquals(3l, storageTypeStats.get(\"nodesInService\"));\n+        assertEquals(3L, storageTypeStats.get(\"nodesInService\"));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY2MzU0OQ=="}, "originalCommit": {"oid": "dea6c63a91a7d0815dbdb405f8cf1c219365aeca"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTg1OTczOQ==", "bodyText": "Hadoop releases before 2.10 are all end of life (EoL). Hadoop 2.10 is the only version using Java 7. We do not need any support, compile or runtime, for Java versions before Java 7.\nHadoop 3.x are all using Java 8+. We do not need any Java 7 support in Hadoop 3.", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r491859739", "createdAt": "2020-09-21T08:11:11Z", "author": {"login": "liuml07"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockStatsMXBean.java", "diffHunk": "@@ -145,9 +150,11 @@ public void testStorageTypeStatsJMX() throws Exception {\n       Map<String,Object> storageTypeStats = (Map<String,Object>)entry.get(\"value\");\n       typesPresent.add(storageType);\n       if (storageType.equals(\"ARCHIVE\") || storageType.equals(\"DISK\") ) {\n-        assertEquals(3l, storageTypeStats.get(\"nodesInService\"));\n+        assertEquals(3L, storageTypeStats.get(\"nodesInService\"));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY2MzU0OQ=="}, "originalCommit": {"oid": "dea6c63a91a7d0815dbdb405f8cf1c219365aeca"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA3NjU0MjE4OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataDirs.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMFQwNzozNDo1NVrOHU4xww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMFQwNzozNDo1NVrOHU4xww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY2MzgxMQ==", "bodyText": "The invalid 8th URI has ending space deliberately for testing. Let's keep it, aka\n\"[ram_disk]/dir4,[disk]/dir5, [disk] /dir6, [disk] , [nvdimm]/dir7\";", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r491663811", "createdAt": "2020-09-20T07:34:55Z", "author": {"login": "liuml07"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataDirs.java", "diffHunk": "@@ -43,14 +43,15 @@ public void testDataDirParsing() throws Throwable {\n \n     File dir5 = new File(\"/dir5\");\n     File dir6 = new File(\"/dir6\");\n+    File dir7 = new File(\"/dir7\");\n     // Verify that a valid string is correctly parsed, and that storage\n     // type is not case-sensitive and we are able to handle white-space between\n     // storage type and URI.\n     String locations1 = \"[disk]/dir0,[DISK]/dir1,[sSd]/dir2,[disK]/dir3,\" +\n-            \"[ram_disk]/dir4,[disk]/dir5, [disk] /dir6, [disk] \";\n+            \"[ram_disk]/dir4,[disk]/dir5, [disk] /dir6, [disk], [nvdimm]/dir7\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dea6c63a91a7d0815dbdb405f8cf1c219365aeca"}, "originalPosition": 10}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA3NjU0MzA1OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsVolumeList.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMFQwNzozNTo1OVrOHU4yJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQxNjoxNzoxNFrOHWCK2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY2MzkxMA==", "bodyText": "nit: Let's use better assertion statement assertEquals(3L, volume5.getReserved());", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r491663910", "createdAt": "2020-09-20T07:35:59Z", "author": {"login": "liuml07"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsVolumeList.java", "diffHunk": "@@ -202,6 +205,14 @@ public void testDfsReservedForDifferentStorageTypes() throws IOException {\n         .setConf(conf)\n         .build();\n     assertEquals(\"\", 100L, volume4.getReserved());\n+    FsVolumeImpl volume5 = new FsVolumeImplBuilder().setDataset(dataset)\n+        .setStorageDirectory(\n+            new StorageDirectory(\n+                StorageLocation.parse(\"[NVDIMM]\"+volDir.getPath())))\n+        .setStorageID(\"storage-id\")\n+        .setConf(conf)\n+        .build();\n+    assertEquals(\"\", 3L, volume5.getReserved());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dea6c63a91a7d0815dbdb405f8cf1c219365aeca"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU5ODgwNQ==", "bodyText": "In order to be consistent with the original code, the assertEquals()  here has three parameters, such as, lines  196 and 204 of the original code.", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r492598805", "createdAt": "2020-09-22T09:32:30Z", "author": {"login": "YaYun-Wang"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsVolumeList.java", "diffHunk": "@@ -202,6 +205,14 @@ public void testDfsReservedForDifferentStorageTypes() throws IOException {\n         .setConf(conf)\n         .build();\n     assertEquals(\"\", 100L, volume4.getReserved());\n+    FsVolumeImpl volume5 = new FsVolumeImplBuilder().setDataset(dataset)\n+        .setStorageDirectory(\n+            new StorageDirectory(\n+                StorageLocation.parse(\"[NVDIMM]\"+volDir.getPath())))\n+        .setStorageID(\"storage-id\")\n+        .setConf(conf)\n+        .build();\n+    assertEquals(\"\", 3L, volume5.getReserved());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY2MzkxMA=="}, "originalCommit": {"oid": "dea6c63a91a7d0815dbdb405f8cf1c219365aeca"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mjg2NjI2Ng==", "bodyText": "Usually we can, but following original code style here is bad. When it fails, the original code gives up empty string. My code shows your expected value and actual value so you can debug. Please change it.\nWe can also file a JIRA to update all such cases where assertEquals can be improved.", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r492866266", "createdAt": "2020-09-22T16:17:14Z", "author": {"login": "liuml07"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsVolumeList.java", "diffHunk": "@@ -202,6 +205,14 @@ public void testDfsReservedForDifferentStorageTypes() throws IOException {\n         .setConf(conf)\n         .build();\n     assertEquals(\"\", 100L, volume4.getReserved());\n+    FsVolumeImpl volume5 = new FsVolumeImplBuilder().setDataset(dataset)\n+        .setStorageDirectory(\n+            new StorageDirectory(\n+                StorageLocation.parse(\"[NVDIMM]\"+volDir.getPath())))\n+        .setStorageID(\"storage-id\")\n+        .setConf(conf)\n+        .build();\n+    assertEquals(\"\", 3L, volume5.getReserved());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY2MzkxMA=="}, "originalCommit": {"oid": "dea6c63a91a7d0815dbdb405f8cf1c219365aeca"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA3NjU0NDkyOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHDFS.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMFQwNzozOToxN1rOHU4y_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMFQwNzozOToxN1rOHU4y_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY2NDEyNg==", "bodyText": "Also add a few happy test?", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r491664126", "createdAt": "2020-09-20T07:39:17Z", "author": {"login": "liuml07"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHDFS.java", "diffHunk": "@@ -1103,6 +1103,8 @@ public void testSetQuota() throws Exception {\n         () -> webHdfs.setQuotaByStorageType(path, StorageType.SSD, -100));\n     LambdaTestUtils.intercept(IllegalArgumentException.class,\n         () -> webHdfs.setQuotaByStorageType(path, StorageType.RAM_DISK, 100));\n+    LambdaTestUtils.intercept(IllegalArgumentException.class,\n+        () -> webHdfs.setQuotaByStorageType(path, StorageType.NVDIMM, -100));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dea6c63a91a7d0815dbdb405f8cf1c219365aeca"}, "originalPosition": 5}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3362, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}