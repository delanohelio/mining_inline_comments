{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE3MzIyNzUz", "number": 2019, "title": "HADOOP-17029. Return correct permission and owner for listing on internal directories in ViewFs", "bodyText": "HADOOP-17029. ViewFS does not return correct user/group and ACL", "createdAt": "2020-05-13T12:17:17Z", "url": "https://github.com/apache/hadoop/pull/2019", "merged": true, "mergeCommit": {"oid": "e7dd02768b658b2a1f216fbedc65938d9b6ca6e9"}, "closed": true, "closedAt": "2020-06-05T21:56:52Z", "author": {"login": "abhishekdas99"}, "timelineItems": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABci4VqRgFqTQxNDY4NzI2Mg==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcoQyRvgH2gAyNDE3MzIyNzUzOjdkNDNiMjkzMmQzODQ1ODA0MWIyZmMwMGM2OGIyN2I4ZWMwZWUyMDk=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE0Njg3MjYy", "url": "https://github.com/apache/hadoop/pull/2019#pullrequestreview-414687262", "createdAt": "2020-05-19T17:52:01Z", "commit": {"oid": "291e5ad8a2cf48d44f42ba83434e95ce297be8a9"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNzo1MjowMVrOGXr7mg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNzo1NzoyNFrOGXsJIQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ5MDIwMg==", "bodyText": "two comments:\n\nseems to be some duplicate code, the \"else\" branch is pretty much the same, can we refactor here?\nNot sure if this is the best way when dealing with FileNotFoundException. If I understand this correctly, it is possible that some mounts does not have this path, so it can hit FileNotFoundException?\n\nIf this is the case, I wonder if it makes more sense to just skip this mount, by not adding a FileStatus for mount at all. So that clients do not get confused by an actually non-existing FileStatus, among other existing ones. But one issue here would be that result array is strictly the size of the # of mounts. Creating result as a list, append, and then return as a array may resolve this.", "url": "https://github.com/apache/hadoop/pull/2019#discussion_r427490202", "createdAt": "2020-05-19T17:52:01Z", "author": {"login": "chliang71"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java", "diffHunk": "@@ -1211,13 +1211,29 @@ public FileStatus getFileStatus(Path f) throws IOException {\n         INode<FileSystem> inode = iEntry.getValue();\n         if (inode.isLink()) {\n           INodeLink<FileSystem> link = (INodeLink<FileSystem>) inode;\n-\n-          result[i++] = new FileStatus(0, false, 0, 0,\n-            creationTime, creationTime, PERMISSION_555,\n-            ugi.getShortUserName(), ugi.getPrimaryGroupName(),\n-            link.getTargetLink(),\n-            new Path(inode.fullPath).makeQualified(\n-                myUri, null));\n+          // For MERGE or NFLY links, the first target link is considered\n+          // for fetching the FileStatus with an assumption that the permission\n+          // and the owner will be the same for all the target directories.\n+          Path linkedPath = new Path(link.targetDirLinkList[0].toString());\n+          ChRootedFileSystem linkedFs = (ChRootedFileSystem)\n+              link.getTargetFileSystem();\n+          try {\n+            FileStatus status = linkedFs.getMyFs().getFileStatus(linkedPath);\n+            result[i++] = new FileStatus(status.getLen(), false,\n+              status.getReplication(), status.getBlockSize(),\n+              status.getModificationTime(), status.getAccessTime(),\n+              status.getPermission(), status.getOwner(), status.getGroup(),\n+              link.getTargetLink(),\n+              new Path(inode.fullPath).makeQualified(\n+                  myUri, null));\n+          } catch (FileNotFoundException ex) {\n+            result[i++] = new FileStatus(0, false, 0, 0,\n+              creationTime, creationTime, PERMISSION_555,\n+              ugi.getShortUserName(), ugi.getPrimaryGroupName(),\n+              link.getTargetLink(),\n+              new Path(inode.fullPath).makeQualified(\n+                  myUri, null));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "291e5ad8a2cf48d44f42ba83434e95ce297be8a9"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ5Mjk1MA==", "bodyText": "similar comment regarding FileNotFoundException. I think in general, it's better to match behavior of non-federated client. If a path does not exist, just throw back FileNotFoundException.", "url": "https://github.com/apache/hadoop/pull/2019#discussion_r427492950", "createdAt": "2020-05-19T17:56:16Z", "author": {"login": "chliang71"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFs.java", "diffHunk": "@@ -915,11 +915,24 @@ public FileStatus getFileLinkStatus(final Path f)\n       if (inode.isLink()) {\n         INodeLink<AbstractFileSystem> inodelink = \n           (INodeLink<AbstractFileSystem>) inode;\n-        result = new FileStatus(0, false, 0, 0, creationTime, creationTime,\n+        Path linkedPath = new Path(inodelink.targetDirLinkList[0].toString());\n+        ChRootedFs linkedFs = (ChRootedFs) inodelink.getTargetFileSystem();\n+        try {\n+          FileStatus status = linkedFs.getMyFs().getFileStatus(linkedPath);\n+          result = new FileStatus(status.getLen(), false,\n+            status.getReplication(), status.getBlockSize(),\n+            status.getModificationTime(), status.getAccessTime(),\n+            status.getPermission(), status.getOwner(), status.getGroup(),\n+            inodelink.getTargetLink(),\n+            new Path(inode.fullPath).makeQualified(\n+                myUri, null));\n+        } catch (FileNotFoundException ex) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "291e5ad8a2cf48d44f42ba83434e95ce297be8a9"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ5MzEzNQ==", "bodyText": "same comment about FileNotFoundException  here", "url": "https://github.com/apache/hadoop/pull/2019#discussion_r427493135", "createdAt": "2020-05-19T17:56:34Z", "author": {"login": "chliang71"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFs.java", "diffHunk": "@@ -965,12 +978,25 @@ public int getUriDefaultPort() {\n           INodeLink<AbstractFileSystem> link = \n             (INodeLink<AbstractFileSystem>) inode;\n \n-          result[i++] = new FileStatus(0, false, 0, 0,\n-            creationTime, creationTime,\n-            PERMISSION_555, ugi.getShortUserName(), ugi.getPrimaryGroupName(),\n-            link.getTargetLink(),\n-            new Path(inode.fullPath).makeQualified(\n-                myUri, null));\n+          Path linkedPath = new Path(link.targetDirLinkList[0].toString());\n+          ChRootedFs linkedFs = (ChRootedFs) link.getTargetFileSystem();\n+          try {\n+            FileStatus status = linkedFs.getMyFs().getFileStatus(linkedPath);\n+            result[i++] = new FileStatus(status.getLen(), false,\n+              status.getReplication(), status.getBlockSize(),\n+              status.getModificationTime(), status.getAccessTime(),\n+              status.getPermission(), status.getOwner(), status.getGroup(),\n+              link.getTargetLink(),\n+              new Path(inode.fullPath).makeQualified(\n+                  myUri, null));\n+          } catch (FileNotFoundException ex) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "291e5ad8a2cf48d44f42ba83434e95ce297be8a9"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ5MzY2NQ==", "bodyText": "some javadoc, and comments in the code could be helpful", "url": "https://github.com/apache/hadoop/pull/2019#discussion_r427493665", "createdAt": "2020-05-19T17:57:24Z", "author": {"login": "chliang71"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewfsFileStatus.java", "diffHunk": "@@ -56,38 +69,71 @@ public void testFileStatusSerialziation()\n     File infile = new File(TEST_DIR, testfilename);\n     final byte[] content = \"dingos\".getBytes();\n \n-    FileOutputStream fos = null;\n-    try {\n-      fos = new FileOutputStream(infile);\n+    try (FileOutputStream fos =  new FileOutputStream(infile)) {\n       fos.write(content);\n-    } finally {\n-      if (fos != null) {\n-        fos.close();\n-      }\n     }\n     assertEquals((long)content.length, infile.length());\n \n     Configuration conf = new Configuration();\n     ConfigUtil.addLink(conf, \"/foo/bar/baz\", TEST_DIR.toURI());\n-    FileSystem vfs = FileSystem.get(FsConstants.VIEWFS_URI, conf);\n-    assertEquals(ViewFileSystem.class, vfs.getClass());\n-    Path path = new Path(\"/foo/bar/baz\", testfilename);\n-    FileStatus stat = vfs.getFileStatus(path);\n-    assertEquals(content.length, stat.getLen());\n-    ContractTestUtils.assertNotErasureCoded(vfs, path);\n-    assertTrue(path + \" should have erasure coding unset in \" +\n-            \"FileStatus#toString(): \" + stat,\n-        stat.toString().contains(\"isErasureCoded=false\"));\n-\n-    // check serialization/deserialization\n-    DataOutputBuffer dob = new DataOutputBuffer();\n-    stat.write(dob);\n-    DataInputBuffer dib = new DataInputBuffer();\n-    dib.reset(dob.getData(), 0, dob.getLength());\n-    FileStatus deSer = new FileStatus();\n-    deSer.readFields(dib);\n-    assertEquals(content.length, deSer.getLen());\n-    assertFalse(deSer.isErasureCoded());\n+    try (FileSystem vfs = FileSystem.get(FsConstants.VIEWFS_URI, conf)) {\n+      assertEquals(ViewFileSystem.class, vfs.getClass());\n+      Path path = new Path(\"/foo/bar/baz\", testfilename);\n+      FileStatus stat = vfs.getFileStatus(path);\n+      assertEquals(content.length, stat.getLen());\n+      ContractTestUtils.assertNotErasureCoded(vfs, path);\n+      assertTrue(path + \" should have erasure coding unset in \" +\n+          \"FileStatus#toString(): \" + stat,\n+          stat.toString().contains(\"isErasureCoded=false\"));\n+\n+      // check serialization/deserialization\n+      DataOutputBuffer dob = new DataOutputBuffer();\n+      stat.write(dob);\n+      DataInputBuffer dib = new DataInputBuffer();\n+      dib.reset(dob.getData(), 0, dob.getLength());\n+      FileStatus deSer = new FileStatus();\n+      deSer.readFields(dib);\n+      assertEquals(content.length, deSer.getLen());\n+      assertFalse(deSer.isErasureCoded());\n+    }\n+  }\n+\n+  @Test\n+  public void testListStatusACL()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "291e5ad8a2cf48d44f42ba83434e95ce297be8a9"}, "originalPosition": 88}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4NTM2NzAw", "url": "https://github.com/apache/hadoop/pull/2019#pullrequestreview-418536700", "createdAt": "2020-05-26T17:50:07Z", "commit": {"oid": "291e5ad8a2cf48d44f42ba83434e95ce297be8a9"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQxNzo1MDowN1rOGapjOw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQxODozMzo0MVrOGarK9Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU5NjkyMw==", "bodyText": "instead of link.targetDirLinkList[0], link.getTargetFileSystem().getUri() should work?\nThis might work for nfly also I think. Because nfly has its own GetFileStatus impl, probably we should use that impl only instead of getting one targetDirLinkList[0]", "url": "https://github.com/apache/hadoop/pull/2019#discussion_r430596923", "createdAt": "2020-05-26T17:50:07Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java", "diffHunk": "@@ -1211,13 +1211,29 @@ public FileStatus getFileStatus(Path f) throws IOException {\n         INode<FileSystem> inode = iEntry.getValue();\n         if (inode.isLink()) {\n           INodeLink<FileSystem> link = (INodeLink<FileSystem>) inode;\n-\n-          result[i++] = new FileStatus(0, false, 0, 0,\n-            creationTime, creationTime, PERMISSION_555,\n-            ugi.getShortUserName(), ugi.getPrimaryGroupName(),\n-            link.getTargetLink(),\n-            new Path(inode.fullPath).makeQualified(\n-                myUri, null));\n+          // For MERGE or NFLY links, the first target link is considered\n+          // for fetching the FileStatus with an assumption that the permission\n+          // and the owner will be the same for all the target directories.\n+          Path linkedPath = new Path(link.targetDirLinkList[0].toString());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "291e5ad8a2cf48d44f42ba83434e95ce297be8a9"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYxMTM5Ng==", "bodyText": "similar comment regarding FileNotFoundException. I think in general, it's better to match behavior of non-federated client. If a path does not exist, just throw back FileNotFoundException.\n\nI just verified symlinks. When target deleted,  ls on symlink does not throw FNFE.  Instead it is converted to file link. I tested dir->dir link.\nIt seems this behavior is correct when compared with other fs. I tested on my MAC.\nShould this be fixed in federated clusters is necessary ? Could you please validate this?\nIf we attempt to open that non existent link file, then we can throw out exception. But ls seems to simply pass.\nWork % mkdir linkTarget \nWork % ln -s linkTarget linkSrc\nWork % ls -l\nlrwxr-xr-x   1 umagangumalla  xxxx     10 May 26 11:08 linkSrc -> linkTarget\nWork % rm -rf linkTarget \nWork % ls -l            \nlrwxr-xr-x   1 umagangumalla  xxxx     10 May 26 11:08 linkSrc -> linkTarget\nWork % cd linkSrc\ncd: no such file or directory: linkSrc", "url": "https://github.com/apache/hadoop/pull/2019#discussion_r430611396", "createdAt": "2020-05-26T18:12:49Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFs.java", "diffHunk": "@@ -915,11 +915,24 @@ public FileStatus getFileLinkStatus(final Path f)\n       if (inode.isLink()) {\n         INodeLink<AbstractFileSystem> inodelink = \n           (INodeLink<AbstractFileSystem>) inode;\n-        result = new FileStatus(0, false, 0, 0, creationTime, creationTime,\n+        Path linkedPath = new Path(inodelink.targetDirLinkList[0].toString());\n+        ChRootedFs linkedFs = (ChRootedFs) inodelink.getTargetFileSystem();\n+        try {\n+          FileStatus status = linkedFs.getMyFs().getFileStatus(linkedPath);\n+          result = new FileStatus(status.getLen(), false,\n+            status.getReplication(), status.getBlockSize(),\n+            status.getModificationTime(), status.getAccessTime(),\n+            status.getPermission(), status.getOwner(), status.getGroup(),\n+            inodelink.getTargetLink(),\n+            new Path(inode.fullPath).makeQualified(\n+                myUri, null));\n+        } catch (FileNotFoundException ex) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ5Mjk1MA=="}, "originalCommit": {"oid": "291e5ad8a2cf48d44f42ba83434e95ce297be8a9"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYxNTQyNQ==", "bodyText": "Can we change permission on target and assert whether its getting changed permissions or simply link permissions?", "url": "https://github.com/apache/hadoop/pull/2019#discussion_r430615425", "createdAt": "2020-05-26T18:20:03Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewfsFileStatus.java", "diffHunk": "@@ -56,38 +69,71 @@ public void testFileStatusSerialziation()\n     File infile = new File(TEST_DIR, testfilename);\n     final byte[] content = \"dingos\".getBytes();\n \n-    FileOutputStream fos = null;\n-    try {\n-      fos = new FileOutputStream(infile);\n+    try (FileOutputStream fos =  new FileOutputStream(infile)) {\n       fos.write(content);\n-    } finally {\n-      if (fos != null) {\n-        fos.close();\n-      }\n     }\n     assertEquals((long)content.length, infile.length());\n \n     Configuration conf = new Configuration();\n     ConfigUtil.addLink(conf, \"/foo/bar/baz\", TEST_DIR.toURI());\n-    FileSystem vfs = FileSystem.get(FsConstants.VIEWFS_URI, conf);\n-    assertEquals(ViewFileSystem.class, vfs.getClass());\n-    Path path = new Path(\"/foo/bar/baz\", testfilename);\n-    FileStatus stat = vfs.getFileStatus(path);\n-    assertEquals(content.length, stat.getLen());\n-    ContractTestUtils.assertNotErasureCoded(vfs, path);\n-    assertTrue(path + \" should have erasure coding unset in \" +\n-            \"FileStatus#toString(): \" + stat,\n-        stat.toString().contains(\"isErasureCoded=false\"));\n-\n-    // check serialization/deserialization\n-    DataOutputBuffer dob = new DataOutputBuffer();\n-    stat.write(dob);\n-    DataInputBuffer dib = new DataInputBuffer();\n-    dib.reset(dob.getData(), 0, dob.getLength());\n-    FileStatus deSer = new FileStatus();\n-    deSer.readFields(dib);\n-    assertEquals(content.length, deSer.getLen());\n-    assertFalse(deSer.isErasureCoded());\n+    try (FileSystem vfs = FileSystem.get(FsConstants.VIEWFS_URI, conf)) {\n+      assertEquals(ViewFileSystem.class, vfs.getClass());\n+      Path path = new Path(\"/foo/bar/baz\", testfilename);\n+      FileStatus stat = vfs.getFileStatus(path);\n+      assertEquals(content.length, stat.getLen());\n+      ContractTestUtils.assertNotErasureCoded(vfs, path);\n+      assertTrue(path + \" should have erasure coding unset in \" +\n+          \"FileStatus#toString(): \" + stat,\n+          stat.toString().contains(\"isErasureCoded=false\"));\n+\n+      // check serialization/deserialization\n+      DataOutputBuffer dob = new DataOutputBuffer();\n+      stat.write(dob);\n+      DataInputBuffer dib = new DataInputBuffer();\n+      dib.reset(dob.getData(), 0, dob.getLength());\n+      FileStatus deSer = new FileStatus();\n+      deSer.readFields(dib);\n+      assertEquals(content.length, deSer.getLen());\n+      assertFalse(deSer.isErasureCoded());\n+    }\n+  }\n+\n+  @Test\n+  public void testListStatusACL()\n+      throws IOException, URISyntaxException {\n+    String testfilename = \"testFileACL\";\n+    String childDirectoryName = \"testDirectoryACL\";\n+    TEST_DIR.mkdirs();\n+    File infile = new File(TEST_DIR, testfilename);\n+    final byte[] content = \"dingos\".getBytes();\n+\n+    try (FileOutputStream fos =  new FileOutputStream(infile)) {\n+      fos.write(content);\n+    }\n+    assertEquals((long)content.length, infile.length());\n+    File childDir = new File(TEST_DIR, childDirectoryName);\n+    childDir.mkdirs();\n+\n+    Configuration conf = new Configuration();\n+    ConfigUtil.addLink(conf, \"/file\", infile.toURI());\n+    ConfigUtil.addLink(conf, \"/dir\", childDir.toURI());\n+\n+    try (FileSystem vfs = FileSystem.get(FsConstants.VIEWFS_URI, conf)) {\n+      assertEquals(ViewFileSystem.class, vfs.getClass());\n+      FileStatus[] statuses = vfs.listStatus(new Path(\"/\"));\n+\n+      FileSystem localFs = FileSystem.getLocal(conf);\n+      FileStatus fileStat = localFs.getFileStatus(new Path(infile.getPath()));\n+      FileStatus dirStat = localFs.getFileStatus(new Path(childDir.getPath()));\n+\n+      for (FileStatus status : statuses) {\n+        if (status.getPath().getName().equals(\"file\")) {\n+          assertEquals(fileStat.getPermission(), status.getPermission());\n+        } else {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "291e5ad8a2cf48d44f42ba83434e95ce297be8a9"}, "originalPosition": 118}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYyMzQ3Nw==", "bodyText": "For # 2: The problem here I think we may not be able reset mount automatically, so until some one checks file exists or do op on target, we will not know whether the file exists or not. This will continue until user updates mount points accordingly.\nThis can be possible when some one deletes the target directory directly but not updated the mount tables accordingly. Please check one of my comment on behavior of ls in MAC. Also we have other issue:  isDir is inconsistent.", "url": "https://github.com/apache/hadoop/pull/2019#discussion_r430623477", "createdAt": "2020-05-26T18:33:41Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java", "diffHunk": "@@ -1211,13 +1211,29 @@ public FileStatus getFileStatus(Path f) throws IOException {\n         INode<FileSystem> inode = iEntry.getValue();\n         if (inode.isLink()) {\n           INodeLink<FileSystem> link = (INodeLink<FileSystem>) inode;\n-\n-          result[i++] = new FileStatus(0, false, 0, 0,\n-            creationTime, creationTime, PERMISSION_555,\n-            ugi.getShortUserName(), ugi.getPrimaryGroupName(),\n-            link.getTargetLink(),\n-            new Path(inode.fullPath).makeQualified(\n-                myUri, null));\n+          // For MERGE or NFLY links, the first target link is considered\n+          // for fetching the FileStatus with an assumption that the permission\n+          // and the owner will be the same for all the target directories.\n+          Path linkedPath = new Path(link.targetDirLinkList[0].toString());\n+          ChRootedFileSystem linkedFs = (ChRootedFileSystem)\n+              link.getTargetFileSystem();\n+          try {\n+            FileStatus status = linkedFs.getMyFs().getFileStatus(linkedPath);\n+            result[i++] = new FileStatus(status.getLen(), false,\n+              status.getReplication(), status.getBlockSize(),\n+              status.getModificationTime(), status.getAccessTime(),\n+              status.getPermission(), status.getOwner(), status.getGroup(),\n+              link.getTargetLink(),\n+              new Path(inode.fullPath).makeQualified(\n+                  myUri, null));\n+          } catch (FileNotFoundException ex) {\n+            result[i++] = new FileStatus(0, false, 0, 0,\n+              creationTime, creationTime, PERMISSION_555,\n+              ugi.getShortUserName(), ugi.getPrimaryGroupName(),\n+              link.getTargetLink(),\n+              new Path(inode.fullPath).makeQualified(\n+                  myUri, null));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ5MDIwMg=="}, "originalCommit": {"oid": "291e5ad8a2cf48d44f42ba83434e95ce297be8a9"}, "originalPosition": 32}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "35cfe4e6eddfae9a10207b839d92040b01434162", "author": {"user": {"login": "abhishekdas99", "name": "Abhishek Das"}}, "url": "https://github.com/apache/hadoop/commit/35cfe4e6eddfae9a10207b839d92040b01434162", "committedDate": "2020-06-03T06:24:13Z", "message": "HADOOP-17029. Return correct permission and owner for listing on internal directories in ViewFs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0243a701637c6296f77eb3c7ad0e9f8ccb63c8c4", "author": {"user": {"login": "abhishekdas99", "name": "Abhishek Das"}}, "url": "https://github.com/apache/hadoop/commit/0243a701637c6296f77eb3c7ad0e9f8ccb63c8c4", "committedDate": "2020-06-03T06:24:13Z", "message": "HADOOP-17029. Change in ViewFs to cover FileContext based fs impl."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a7a7875038ca29f87d6f050547c35d081d9401e1", "author": {"user": {"login": "abhishekdas99", "name": "Abhishek Das"}}, "url": "https://github.com/apache/hadoop/commit/a7a7875038ca29f87d6f050547c35d081d9401e1", "committedDate": "2020-06-03T06:24:13Z", "message": "HADOOP-17029. Fix for dangling links"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "017e3603ff44bcecd2c81b298ed924278ed18869", "author": {"user": {"login": "abhishekdas99", "name": "Abhishek Das"}}, "url": "https://github.com/apache/hadoop/commit/017e3603ff44bcecd2c81b298ed924278ed18869", "committedDate": "2020-06-03T06:17:30Z", "message": "Merge branch 'HADOOP-17029' of https://github.com/abhishekdas99/hadoop into HADOOP-17029"}, "afterCommit": {"oid": "cf990e0b8e43c3a7c855ac103b8cedf450985204", "author": {"user": {"login": "abhishekdas99", "name": "Abhishek Das"}}, "url": "https://github.com/apache/hadoop/commit/cf990e0b8e43c3a7c855ac103b8cedf450985204", "committedDate": "2020-06-03T06:24:13Z", "message": "HADOOP-17029. Addressed code review comments."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "cf990e0b8e43c3a7c855ac103b8cedf450985204", "author": {"user": {"login": "abhishekdas99", "name": "Abhishek Das"}}, "url": "https://github.com/apache/hadoop/commit/cf990e0b8e43c3a7c855ac103b8cedf450985204", "committedDate": "2020-06-03T06:24:13Z", "message": "HADOOP-17029. Addressed code review comments."}, "afterCommit": {"oid": "49071eb5c26be346b2a9b90cfa02d4273c91b97c", "author": {"user": {"login": "abhishekdas99", "name": "Abhishek Das"}}, "url": "https://github.com/apache/hadoop/commit/49071eb5c26be346b2a9b90cfa02d4273c91b97c", "committedDate": "2020-06-03T06:38:22Z", "message": "HADOOP-17029. Addressed code review comments."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "da1702af608f5f27d62d31dca4705c7872f2e6c3", "author": {"user": {"login": "abhishekdas99", "name": "Abhishek Das"}}, "url": "https://github.com/apache/hadoop/commit/da1702af608f5f27d62d31dca4705c7872f2e6c3", "committedDate": "2020-06-03T08:40:29Z", "message": "HADOOP-17029. Addressed code review comments."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "49071eb5c26be346b2a9b90cfa02d4273c91b97c", "author": {"user": {"login": "abhishekdas99", "name": "Abhishek Das"}}, "url": "https://github.com/apache/hadoop/commit/49071eb5c26be346b2a9b90cfa02d4273c91b97c", "committedDate": "2020-06-03T06:38:22Z", "message": "HADOOP-17029. Addressed code review comments."}, "afterCommit": {"oid": "da1702af608f5f27d62d31dca4705c7872f2e6c3", "author": {"user": {"login": "abhishekdas99", "name": "Abhishek Das"}}, "url": "https://github.com/apache/hadoop/commit/da1702af608f5f27d62d31dca4705c7872f2e6c3", "committedDate": "2020-06-03T08:40:29Z", "message": "HADOOP-17029. Addressed code review comments."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI0ODg1NjYw", "url": "https://github.com/apache/hadoop/pull/2019#pullrequestreview-424885660", "createdAt": "2020-06-04T22:46:06Z", "commit": {"oid": "da1702af608f5f27d62d31dca4705c7872f2e6c3"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQyMjo0NjowN1rOGfaf_Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQyMjo1MzoyM1rOGfapuw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU5MzIxMw==", "bodyText": "IMO, FileNotFoundException is fine. Please see my comment below https://github.com/apache/hadoop/pull/2019/files#r430611396\n@chliang71 , What do you say?", "url": "https://github.com/apache/hadoop/pull/2019#discussion_r435593213", "createdAt": "2020-06-04T22:46:07Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java", "diffHunk": "@@ -1211,13 +1211,29 @@ public FileStatus getFileStatus(Path f) throws IOException {\n         INode<FileSystem> inode = iEntry.getValue();\n         if (inode.isLink()) {\n           INodeLink<FileSystem> link = (INodeLink<FileSystem>) inode;\n-\n-          result[i++] = new FileStatus(0, false, 0, 0,\n-            creationTime, creationTime, PERMISSION_555,\n-            ugi.getShortUserName(), ugi.getPrimaryGroupName(),\n-            link.getTargetLink(),\n-            new Path(inode.fullPath).makeQualified(\n-                myUri, null));\n+          // For MERGE or NFLY links, the first target link is considered\n+          // for fetching the FileStatus with an assumption that the permission\n+          // and the owner will be the same for all the target directories.\n+          Path linkedPath = new Path(link.targetDirLinkList[0].toString());\n+          ChRootedFileSystem linkedFs = (ChRootedFileSystem)\n+              link.getTargetFileSystem();\n+          try {\n+            FileStatus status = linkedFs.getMyFs().getFileStatus(linkedPath);\n+            result[i++] = new FileStatus(status.getLen(), false,\n+              status.getReplication(), status.getBlockSize(),\n+              status.getModificationTime(), status.getAccessTime(),\n+              status.getPermission(), status.getOwner(), status.getGroup(),\n+              link.getTargetLink(),\n+              new Path(inode.fullPath).makeQualified(\n+                  myUri, null));\n+          } catch (FileNotFoundException ex) {\n+            result[i++] = new FileStatus(0, false, 0, 0,\n+              creationTime, creationTime, PERMISSION_555,\n+              ugi.getShortUserName(), ugi.getPrimaryGroupName(),\n+              link.getTargetLink(),\n+              new Path(inode.fullPath).makeQualified(\n+                  myUri, null));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ5MDIwMg=="}, "originalCommit": {"oid": "291e5ad8a2cf48d44f42ba83434e95ce297be8a9"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU5NTcwNw==", "bodyText": "Why are we getting status on \"/\"? In practical scenario it should work. However what if the target uri is a file?\nShould we simply use link.getTargetFileSystem().getUri() ?\nCould you please check scenario? If this works, I have no other changes.\nThanks for update.", "url": "https://github.com/apache/hadoop/pull/2019#discussion_r435595707", "createdAt": "2020-06-04T22:53:23Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java", "diffHunk": "@@ -1200,13 +1200,24 @@ public FileStatus getFileStatus(Path f) throws IOException {\n         INode<FileSystem> inode = iEntry.getValue();\n         if (inode.isLink()) {\n           INodeLink<FileSystem> link = (INodeLink<FileSystem>) inode;\n-\n-          result[i++] = new FileStatus(0, false, 0, 0,\n-            creationTime, creationTime, PERMISSION_555,\n-            ugi.getShortUserName(), ugi.getPrimaryGroupName(),\n-            link.getTargetLink(),\n-            new Path(inode.fullPath).makeQualified(\n-                myUri, null));\n+          try {\n+            FileStatus status = link.getTargetFileSystem()\n+                .getFileStatus(new Path(\"/\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da1702af608f5f27d62d31dca4705c7872f2e6c3"}, "originalPosition": 13}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7d43b2932d38458041b2fc00c68b27b8ec0ee209", "author": {"user": {"login": "abhishekdas99", "name": "Abhishek Das"}}, "url": "https://github.com/apache/hadoop/commit/7d43b2932d38458041b2fc00c68b27b8ec0ee209", "committedDate": "2020-06-05T11:16:27Z", "message": "HADOOP-17029. Used link.getTargetFileSystem().getUri() for path"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4312, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}