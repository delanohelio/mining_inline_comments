{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDUzNTc5ODMz", "number": 2154, "title": "HADOOP-17113. Adding ReadAhead Counters in ABFS", "bodyText": "tested by: mvn -T 1C -Dparallel-tests=abfs clean verify\nRegion: East US, West US\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time:  16:11 min (Wall Clock)\n[INFO] Finished at: 2020-07-20T20:39:21+05:30", "createdAt": "2020-07-20T15:46:41Z", "url": "https://github.com/apache/hadoop/pull/2154", "merged": true, "mergeCommit": {"oid": "48a7c5b6baf3cbf5ef85433c348753842eb8ec7d"}, "closed": true, "closedAt": "2020-07-22T17:22:31Z", "author": {"login": "mehakmeet"}, "timelineItems": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc2zkf4gH2gAyNDUzNTc5ODMzOmJhMjg2OWEwODY2ZjQwZDkxOGU4MTdiN2U2MTE5YzJmNjVjNzFhY2I=", "endCursor": "Y3Vyc29yOnYyOpPPAAABc3EcFogH2gAyNDUzNTc5ODMzOjg0YjI4MmMyY2ViMjYwMjU5MGM5MzhhMjhkYjdlMmY5OTZjODQ5NjM=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "ba2869a0866f40d918e817b7e6119c2f65c71acb", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/ba2869a0866f40d918e817b7e6119c2f65c71acb", "committedDate": "2020-07-20T15:43:01Z", "message": "HADOOP-17113. Adding ReadAhead Counters in ABFS"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUxOTEzNDI2", "url": "https://github.com/apache/hadoop/pull/2154#pullrequestreview-451913426", "createdAt": "2020-07-20T19:54:52Z", "commit": {"oid": "ba2869a0866f40d918e817b7e6119c2f65c71acb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQxOTo1NDo1MlrOG0dD4w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQxOTo1NDo1MlrOG0dD4w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY1NTI2Nw==", "bodyText": "nice explanation", "url": "https://github.com/apache/hadoop/pull/2154#discussion_r457655267", "createdAt": "2020-07-20T19:54:52Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsInputStreamStatistics.java", "diffHunk": "@@ -285,6 +291,98 @@ public void testWithNullStreamStatistics() throws IOException {\n     }\n   }\n \n+  /**\n+   * Testing readAhead counters in AbfsInputStream with 30 seconds timeout.\n+   */\n+  @Test(timeout = TIMEOUT_30_SECONDS)\n+  public void testReadAheadCounters() throws IOException {\n+    describe(\"Test to check correct values for readAhead counters in \"\n+        + \"AbfsInputStream\");\n+\n+    AzureBlobFileSystem fs = getFileSystem();\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    Path readAheadCountersPath = path(getMethodName());\n+\n+    /*\n+     * Setting the block size for readAhead as 4KB.\n+     */\n+    abfss.getAbfsConfiguration().setReadBufferSize(CUSTOM_BLOCK_BUFFER_SIZE);\n+\n+    AbfsOutputStream out = null;\n+    AbfsInputStream in = null;\n+\n+    try {\n+\n+      /*\n+       * Creating a file of 1MB size.\n+       */\n+      out = createAbfsOutputStreamWithFlushEnabled(fs, readAheadCountersPath);\n+      out.write(defBuffer);\n+      out.close();\n+\n+      in = abfss.openFileForRead(readAheadCountersPath, fs.getFsStatistics());\n+\n+      /*\n+       * Reading 1KB after each i * KB positions. Hence the reads are from 0\n+       * to 1KB, 1KB to 2KB, and so on.. for 5 operations.\n+       */\n+      for (int i = 0; i < 5; i++) {\n+        in.seek(ONE_KB * i);\n+        in.read(defBuffer, ONE_KB * i, ONE_KB);\n+      }\n+      AbfsInputStreamStatisticsImpl stats =\n+          (AbfsInputStreamStatisticsImpl) in.getStreamStatistics();\n+\n+      /*\n+       * Since, readAhead is done in background threads. Sometimes, the\n+       * threads aren't finished in the background and could result in\n+       * inaccurate results. So, we wait till we have the accurate values\n+       * with a limit of 30 seconds as that's when the test times out.\n+       *\n+       */\n+      while (stats.getRemoteBytesRead() < CUSTOM_READ_AHEAD_BUFFER_SIZE\n+          || stats.getReadAheadBytesRead() < CUSTOM_BLOCK_BUFFER_SIZE) {\n+        Thread.sleep(THREAD_SLEEP_10_SECONDS);\n+      }\n+\n+      /*\n+       * Verifying the counter values of readAheadBytesRead and remoteBytesRead.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ba2869a0866f40d918e817b7e6119c2f65c71acb"}, "originalPosition": 79}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUxOTEzODkx", "url": "https://github.com/apache/hadoop/pull/2154#pullrequestreview-451913891", "createdAt": "2020-07-20T19:55:36Z", "commit": {"oid": "ba2869a0866f40d918e817b7e6119c2f65c71acb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQxOTo1NTozNlrOG0dFWg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQxOTo1NTozNlrOG0dFWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY1NTY0Mg==", "bodyText": "Try using AssertJ.assertThat here, it lets you declare the specific \"isGreaterThan\" assertion; it's describedAs() does the string formatting too.", "url": "https://github.com/apache/hadoop/pull/2154#discussion_r457655642", "createdAt": "2020-07-20T19:55:36Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsInputStreamStatistics.java", "diffHunk": "@@ -285,6 +291,98 @@ public void testWithNullStreamStatistics() throws IOException {\n     }\n   }\n \n+  /**\n+   * Testing readAhead counters in AbfsInputStream with 30 seconds timeout.\n+   */\n+  @Test(timeout = TIMEOUT_30_SECONDS)\n+  public void testReadAheadCounters() throws IOException {\n+    describe(\"Test to check correct values for readAhead counters in \"\n+        + \"AbfsInputStream\");\n+\n+    AzureBlobFileSystem fs = getFileSystem();\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    Path readAheadCountersPath = path(getMethodName());\n+\n+    /*\n+     * Setting the block size for readAhead as 4KB.\n+     */\n+    abfss.getAbfsConfiguration().setReadBufferSize(CUSTOM_BLOCK_BUFFER_SIZE);\n+\n+    AbfsOutputStream out = null;\n+    AbfsInputStream in = null;\n+\n+    try {\n+\n+      /*\n+       * Creating a file of 1MB size.\n+       */\n+      out = createAbfsOutputStreamWithFlushEnabled(fs, readAheadCountersPath);\n+      out.write(defBuffer);\n+      out.close();\n+\n+      in = abfss.openFileForRead(readAheadCountersPath, fs.getFsStatistics());\n+\n+      /*\n+       * Reading 1KB after each i * KB positions. Hence the reads are from 0\n+       * to 1KB, 1KB to 2KB, and so on.. for 5 operations.\n+       */\n+      for (int i = 0; i < 5; i++) {\n+        in.seek(ONE_KB * i);\n+        in.read(defBuffer, ONE_KB * i, ONE_KB);\n+      }\n+      AbfsInputStreamStatisticsImpl stats =\n+          (AbfsInputStreamStatisticsImpl) in.getStreamStatistics();\n+\n+      /*\n+       * Since, readAhead is done in background threads. Sometimes, the\n+       * threads aren't finished in the background and could result in\n+       * inaccurate results. So, we wait till we have the accurate values\n+       * with a limit of 30 seconds as that's when the test times out.\n+       *\n+       */\n+      while (stats.getRemoteBytesRead() < CUSTOM_READ_AHEAD_BUFFER_SIZE\n+          || stats.getReadAheadBytesRead() < CUSTOM_BLOCK_BUFFER_SIZE) {\n+        Thread.sleep(THREAD_SLEEP_10_SECONDS);\n+      }\n+\n+      /*\n+       * Verifying the counter values of readAheadBytesRead and remoteBytesRead.\n+       *\n+       * readAheadBytesRead : Since, we read 1KBs 5 times, that means we go\n+       * from 0 to 5KB in the file. The bufferSize is set to 4KB, and since\n+       * we have 8 blocks of readAhead buffer. We would have 8 blocks of 4KB\n+       * buffer. Our read is till 5KB, hence readAhead would ideally read 2\n+       * blocks of 4KB which is equal to 8KB. But, sometimes to get more than\n+       * one block from readAhead buffer we might have to wait for background\n+       * threads to fill the buffer and hence we might do remote read which\n+       * would be faster. Therefore, readAheadBytesRead would be equal to or\n+       * greater than 4KB.\n+       *\n+       * remoteBytesRead : Since, the bufferSize is set to 4KB and the number\n+       * of blocks or readAheadQueueDepth is equal to 8. We would read 8 * 4\n+       * KB buffer on the first read, which is equal to 32KB. But, if we are not\n+       * able to read some bytes that were in the buffer after doing\n+       * readAhead, we might use remote read again. Thus, the bytes read\n+       * remotely could also be greater than 32Kb.\n+       *\n+       */\n+      assertTrue(String.format(\"actual value of %d is not greater than or \"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ba2869a0866f40d918e817b7e6119c2f65c71acb"}, "originalPosition": 99}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2f3763d5fb11897063b1571cb546734d7b6d8c85", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/2f3763d5fb11897063b1571cb546734d7b6d8c85", "committedDate": "2020-07-21T05:14:04Z", "message": "HADOOP-17113. Adding assertThat"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUyMzIwNTI4", "url": "https://github.com/apache/hadoop/pull/2154#pullrequestreview-452320528", "createdAt": "2020-07-21T10:19:03Z", "commit": {"oid": "2f3763d5fb11897063b1571cb546734d7b6d8c85"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxMDoxOTowM1rOG0xjxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxMDoxOTowM1rOG0xjxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzk5MTEwOA==", "bodyText": "can't we just throw this? If not, at least use LOG", "url": "https://github.com/apache/hadoop/pull/2154#discussion_r457991108", "createdAt": "2020-07-21T10:19:03Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsInputStreamStatistics.java", "diffHunk": "@@ -285,6 +292,96 @@ public void testWithNullStreamStatistics() throws IOException {\n     }\n   }\n \n+  /**\n+   * Testing readAhead counters in AbfsInputStream with 30 seconds timeout.\n+   */\n+  @Test(timeout = TIMEOUT_30_SECONDS)\n+  public void testReadAheadCounters() throws IOException {\n+    describe(\"Test to check correct values for readAhead counters in \"\n+        + \"AbfsInputStream\");\n+\n+    AzureBlobFileSystem fs = getFileSystem();\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    Path readAheadCountersPath = path(getMethodName());\n+\n+    /*\n+     * Setting the block size for readAhead as 4KB.\n+     */\n+    abfss.getAbfsConfiguration().setReadBufferSize(CUSTOM_BLOCK_BUFFER_SIZE);\n+\n+    AbfsOutputStream out = null;\n+    AbfsInputStream in = null;\n+\n+    try {\n+\n+      /*\n+       * Creating a file of 1MB size.\n+       */\n+      out = createAbfsOutputStreamWithFlushEnabled(fs, readAheadCountersPath);\n+      out.write(defBuffer);\n+      out.close();\n+\n+      in = abfss.openFileForRead(readAheadCountersPath, fs.getFsStatistics());\n+\n+      /*\n+       * Reading 1KB after each i * KB positions. Hence the reads are from 0\n+       * to 1KB, 1KB to 2KB, and so on.. for 5 operations.\n+       */\n+      for (int i = 0; i < 5; i++) {\n+        in.seek(ONE_KB * i);\n+        in.read(defBuffer, ONE_KB * i, ONE_KB);\n+      }\n+      AbfsInputStreamStatisticsImpl stats =\n+          (AbfsInputStreamStatisticsImpl) in.getStreamStatistics();\n+\n+      /*\n+       * Since, readAhead is done in background threads. Sometimes, the\n+       * threads aren't finished in the background and could result in\n+       * inaccurate results. So, we wait till we have the accurate values\n+       * with a limit of 30 seconds as that's when the test times out.\n+       *\n+       */\n+      while (stats.getRemoteBytesRead() < CUSTOM_READ_AHEAD_BUFFER_SIZE\n+          || stats.getReadAheadBytesRead() < CUSTOM_BLOCK_BUFFER_SIZE) {\n+        Thread.sleep(THREAD_SLEEP_10_SECONDS);\n+      }\n+\n+      /*\n+       * Verifying the counter values of readAheadBytesRead and remoteBytesRead.\n+       *\n+       * readAheadBytesRead : Since, we read 1KBs 5 times, that means we go\n+       * from 0 to 5KB in the file. The bufferSize is set to 4KB, and since\n+       * we have 8 blocks of readAhead buffer. We would have 8 blocks of 4KB\n+       * buffer. Our read is till 5KB, hence readAhead would ideally read 2\n+       * blocks of 4KB which is equal to 8KB. But, sometimes to get more than\n+       * one block from readAhead buffer we might have to wait for background\n+       * threads to fill the buffer and hence we might do remote read which\n+       * would be faster. Therefore, readAheadBytesRead would be equal to or\n+       * greater than 4KB.\n+       *\n+       * remoteBytesRead : Since, the bufferSize is set to 4KB and the number\n+       * of blocks or readAheadQueueDepth is equal to 8. We would read 8 * 4\n+       * KB buffer on the first read, which is equal to 32KB. But, if we are not\n+       * able to read some bytes that were in the buffer after doing\n+       * readAhead, we might use remote read again. Thus, the bytes read\n+       * remotely could also be greater than 32Kb.\n+       *\n+       */\n+      Assertions.assertThat(stats.getReadAheadBytesRead()).describedAs(\n+          \"Mismatch in readAheadBytesRead counter value\")\n+          .isGreaterThanOrEqualTo(CUSTOM_BLOCK_BUFFER_SIZE);\n+\n+      Assertions.assertThat(stats.getRemoteBytesRead()).describedAs(\n+          \"Mismatch in remoteBytesRead counter value\")\n+          .isGreaterThanOrEqualTo(CUSTOM_READ_AHEAD_BUFFER_SIZE);\n+\n+    } catch (InterruptedException e) {\n+      e.printStackTrace();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2f3763d5fb11897063b1571cb546734d7b6d8c85"}, "originalPosition": 116}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "84b282c2ceb2602590c938a28db7e2f996c84963", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/84b282c2ceb2602590c938a28db7e2f996c84963", "committedDate": "2020-07-21T11:22:13Z", "message": "HADOOP-17113. adding exception to method"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4183, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}