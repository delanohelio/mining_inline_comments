{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU5NDI0OTkw", "number": 2179, "title": "HADOOP-17166. ABFS: making max concurrent requests and max requests that can be que\u2026", "bodyText": "Making the AbfsOutputStream maxConcurrentRequests and the maximum size to which the threadpool queue can grow up to.\nDriver test results using accounts in Central India\nmvn -T 1C -Dparallel-tests=abfs -Dscale -DtestsThreadCount=8 clean verify\n**Client credentials\nAccount with HNS Support**\n[INFO] Tests run: 87, Failures: 0, Errors: 0, Skipped: 0\n[ERROR] Errors:\n[ERROR]   ITestAbfsInputStreamStatistics.testReadAheadCounters:346 \u00bb TestTimedOut test t...\n[INFO]\n[ERROR] Tests run: 451, Failures: 0, Errors: 1, Skipped: 75\n[WARNING] Tests run: 207, Failures: 0, Errors: 0, Skipped: 24\nAccount without HNS support\n[INFO] Tests run: 87, Failures: 0, Errors: 0, Skipped: 0\n[ERROR] Errors:\n[ERROR]   ITestAbfsInputStreamStatistics.testReadAheadCounters:346 \u00bb TestTimedOut test t...\n[INFO]\n[ERROR] Tests run: 451, Failures: 0, Errors: 1, Skipped: 248\n[WARNING] Tests run: 207, Failures: 0, Errors: 0, Skipped: 24\n**Accesskey\nAccount with HNS Support**\n[INFO] Tests run: 87, Failures: 0, Errors: 0, Skipped: 0\n[ERROR] Errors:\n[ERROR]   ITestAbfsInputStreamStatistics.testReadAheadCounters:346 \u00bb TestTimedOut test t...\n[ERROR]   ITestGetNameSpaceEnabled.testFailedRequestWhenCredentialsNotCorrect:160->AbstractAbfsIntegrationTest.getFileSystem:254 \u00bb KeyProvider\n[INFO]\n[ERROR] Tests run: 451, Failures: 0, Errors: 2, Skipped: 42\nWARNING] Tests run: 207, Failures: 0, Errors: 0, Skipped: 16\nAccount without HNS support\n[INFO] Tests run: 87, Failures: 0, Errors: 0, Skipped: 0\n[ERROR] Errors:\n[ERROR]   ITestAbfsInputStreamStatistics.testReadAheadCounters:346 \u00bb TestTimedOut test t...\n[INFO]\n[ERROR] Tests run: 451, Failures: 0, Errors: 1, Skipped: 245\n[WARNING] Tests run: 207, Failures: 0, Errors: 0, Skipped: 16", "createdAt": "2020-07-30T17:25:53Z", "url": "https://github.com/apache/hadoop/pull/2179", "merged": true, "mergeCommit": {"oid": "85119267be75d7960e2880d251ccaf3bda4a87d9"}, "closed": true, "closedAt": "2020-09-09T15:41:37Z", "author": {"login": "bilaharith"}, "timelineItems": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc5_jvCgH2gAyNDU5NDI0OTkwOmI3MWU5NGI3ZTkxMGMzMTE3MTJlNzEyZDNlMTIyYWUwOGE1MDhlYjg=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdFH6R9gH2gAyNDU5NDI0OTkwOjg4YzQzNTA4ZmUzODNhYWUwNzA5Y2E2M2VjNTk1YmNmMDA0MmQ3OWU=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "b71e94b7e910c311712e712d3e122ae08a508eb8", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/b71e94b7e910c311712e712d3e122ae08a508eb8", "committedDate": "2020-07-30T13:22:49Z", "message": "ABFS: making max concurrent requests and max requests that can be queued configurable for AbfsOutputStream"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9dd0d286c1afac1ecd1b10f7fb136e78fbf8effa", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/9dd0d286c1afac1ecd1b10f7fb136e78fbf8effa", "committedDate": "2020-08-17T15:57:14Z", "message": "Improved the documentation"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f58893c9e0ddf9be7f384514b0bb62c24fc833fa", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/f58893c9e0ddf9be7f384514b0bb62c24fc833fa", "committedDate": "2020-08-18T02:26:28Z", "message": "Added testcases"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b26a83afd8193b51bc78ed7b197ef901d6c44ce3", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/b26a83afd8193b51bc78ed7b197ef901d6c44ce3", "committedDate": "2020-08-18T08:55:48Z", "message": "Removing whitespace"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6358dd32bceb23a657689e66f385585fbe05b70d", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/6358dd32bceb23a657689e66f385585fbe05b70d", "committedDate": "2020-08-19T08:27:13Z", "message": "Merge branch 'trunk' into HADOOP-17166-opsconfigs"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDcwNDI3NDM2", "url": "https://github.com/apache/hadoop/pull/2179#pullrequestreview-470427436", "createdAt": "2020-08-19T12:55:58Z", "commit": {"oid": "6358dd32bceb23a657689e66f385585fbe05b70d"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ef072e07600fcba97a18271eba92b3934648d4e9", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/ef072e07600fcba97a18271eba92b3934648d4e9", "committedDate": "2020-08-28T19:26:31Z", "message": "Additional deocumentation"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4MTkwODUz", "url": "https://github.com/apache/hadoop/pull/2179#pullrequestreview-478190853", "createdAt": "2020-08-30T10:29:11Z", "commit": {"oid": "ef072e07600fcba97a18271eba92b3934648d4e9"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMFQxMDoyOToxMVrOHJhueQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMFQxMDozMDo0MVrOHJhvBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTc1MTgwMQ==", "bodyText": "use experimental in name to show they are exactly that", "url": "https://github.com/apache/hadoop/pull/2179#discussion_r479751801", "createdAt": "2020-08-30T10:29:11Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java", "diffHunk": "@@ -52,6 +52,8 @@\n   public static final String AZURE_OAUTH_TOKEN_FETCH_RETRY_DELTA_BACKOFF = \"fs.azure.oauth.token.fetch.retry.delta.backoff\";\n \n   // Read and write buffer sizes defined by the user\n+  public static final String AZURE_WRITE_MAX_CONCURRENT_REQUESTS = \"fs.azure.write.max.concurrent.requests\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef072e07600fcba97a18271eba92b3934648d4e9"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTc1MTg5NQ==", "bodyText": "make clear: or when doing many writes in same process (bulk uploads, hive LLAP/spark with many workers)", "url": "https://github.com/apache/hadoop/pull/2179#discussion_r479751895", "createdAt": "2020-08-30T10:30:04Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/site/markdown/abfs.md", "diffHunk": "@@ -796,6 +796,18 @@ will be -1. To disable readaheads, set this value to 0. If your workload is\n  doing only random reads (non-sequential) or you are seeing throttling, you\n   may try setting this value to 0.\n \n+To run under limited memory situations configure the following.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef072e07600fcba97a18271eba92b3934648d4e9"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTc1MTk0Mg==", "bodyText": "try with resources, always", "url": "https://github.com/apache/hadoop/pull/2179#discussion_r479751942", "createdAt": "2020-08-30T10:30:41Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,77 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.assertj.core.api.Assertions;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+\n+/**\n+ * Test create operation.\n+ */\n+public class ITestAbfsOutputStream extends AbstractAbfsIntegrationTest {\n+  private static final Path TEST_FILE_PATH = new Path(\"testfile\");\n+\n+  public ITestAbfsOutputStream() throws Exception {\n+    super();\n+  }\n+\n+  @Test\n+  public void testMaxRequestsAndQueueCapacityDefaults() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    final AzureBlobFileSystem fs = getFileSystem(conf);\n+    FSDataOutputStream out = fs.create(TEST_FILE_PATH);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef072e07600fcba97a18271eba92b3934648d4e9"}, "originalPosition": 45}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "88c43508fe383aae0709ca63ec595bcf0042d79e", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/88c43508fe383aae0709ca63ec595bcf0042d79e", "committedDate": "2020-09-03T03:19:51Z", "message": "Addressing review comments"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3821, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}