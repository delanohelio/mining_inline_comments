{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM2MTE1NzY1", "number": 2080, "title": "HDFS-15417. RBF: Get the datanode report from cache for federation WebHDFS operations", "bodyText": "https://issues.apache.org/jira/browse/HDFS-15417\nNOTICE\nPlease create an issue in ASF JIRA before opening a pull request,\nand you need to set the title of the pull request which starts with\nthe corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.)\nFor more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute", "createdAt": "2020-06-17T21:53:20Z", "url": "https://github.com/apache/hadoop/pull/2080", "merged": true, "mergeCommit": {"oid": "e820baa6e6f7e850ba62cbf150d760bd0ea6d0e0"}, "closed": true, "closedAt": "2020-07-06T23:17:10Z", "author": {"login": "NickyYe"}, "timelineItems": {"totalCount": 18, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcsQ-GwAH2gAyNDM2MTE1NzY1OmExYWQ4OGU2ZDQ1OGExNGJlMjY3MzYxODM4OTRlZDBlZDJhMzEwMTc=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcyTfQfAFqTQ0MzIxMTc4MQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "a1ad88e6d458a14be26736183894ed0ed2a31017", "author": {"user": {"login": "NickyYe", "name": "Ye Ni"}}, "url": "https://github.com/apache/hadoop/commit/a1ad88e6d458a14be26736183894ed0ed2a31017", "committedDate": "2020-06-17T21:45:04Z", "message": "Update RouterWebHdfsMethods.java"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3369ba7b5eea4355487d8961a1f89fee15650407", "author": {"user": {"login": "NickyYe", "name": "Ye Ni"}}, "url": "https://github.com/apache/hadoop/commit/3369ba7b5eea4355487d8961a1f89fee15650407", "committedDate": "2020-06-17T21:48:54Z", "message": "Update RouterWebHdfsMethods.java"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMzNDkzMzk3", "url": "https://github.com/apache/hadoop/pull/2080#pullrequestreview-433493397", "createdAt": "2020-06-18T17:20:54Z", "commit": {"oid": "3369ba7b5eea4355487d8961a1f89fee15650407"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxNzoyMDo1NVrOGl4_kw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxNzoyODo0M1rOGl5Rrw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM4NDI3NQ==", "bodyText": "nit: this is always true.", "url": "https://github.com/apache/hadoop/pull/2080#discussion_r442384275", "createdAt": "2020-06-18T17:20:55Z", "author": {"login": "sunchao"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterWebHdfsMethods.java", "diffHunk": "@@ -502,12 +479,27 @@ private DatanodeInfo chooseDatanode(final Router router,\n         final LocatedBlocks locations = cp.getBlockLocations(path, offset, 1);\n         final int count = locations.locatedBlockCount();\n         if (count > 0) {\n+          if (excludeDatanodes != null) {\n+            Collection<String> collection =\n+                getTrimmedStringCollection(excludeDatanodes);\n+            dns = getDatanodeReport(router);\n+            for (DatanodeInfo dn : dns) {\n+              if (collection.contains(dn.getName())) {\n+                excludes.add(dn);\n+              }\n+            }\n+          }\n+          \n           LocatedBlock location0 = locations.get(0);\n           return bestNode(location0.getLocations(), excludes);\n         }\n       }\n     }\n \n+    if (dns == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3369ba7b5eea4355487d8961a1f89fee15650407"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM4NTEyOQ==", "bodyText": "In this case, excludes is always empty. We need to compute it using the dns obtained.", "url": "https://github.com/apache/hadoop/pull/2080#discussion_r442385129", "createdAt": "2020-06-18T17:22:21Z", "author": {"login": "sunchao"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterWebHdfsMethods.java", "diffHunk": "@@ -502,12 +479,27 @@ private DatanodeInfo chooseDatanode(final Router router,\n         final LocatedBlocks locations = cp.getBlockLocations(path, offset, 1);\n         final int count = locations.locatedBlockCount();\n         if (count > 0) {\n+          if (excludeDatanodes != null) {\n+            Collection<String> collection =\n+                getTrimmedStringCollection(excludeDatanodes);\n+            dns = getDatanodeReport(router);\n+            for (DatanodeInfo dn : dns) {\n+              if (collection.contains(dn.getName())) {\n+                excludes.add(dn);\n+              }\n+            }\n+          }\n+          \n           LocatedBlock location0 = locations.get(0);\n           return bestNode(location0.getLocations(), excludes);\n         }\n       }\n     }\n \n+    if (dns == null) {\n+      dns = getDatanodeReport(router);\n+    }\n+    \n     return getRandomDatanode(dns, excludes);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3369ba7b5eea4355487d8961a1f89fee15650407"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM4NTMxOQ==", "bodyText": "nit: leave a blank line before the @param line.", "url": "https://github.com/apache/hadoop/pull/2080#discussion_r442385319", "createdAt": "2020-06-18T17:22:38Z", "author": {"login": "sunchao"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterWebHdfsMethods.java", "diffHunk": "@@ -564,4 +556,28 @@ public Credentials createCredentials(\n         renewer != null? renewer: ugi.getShortUserName());\n     return c;\n   }\n+  \n+    /**\n+   * Get the datanode report from all namespaces that are registered\n+   * and active in the federation.\n+   * @param router Router from which to get the report.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3369ba7b5eea4355487d8961a1f89fee15650407"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM4ODkxMQ==", "bodyText": "should we wrap these in the try .. catch block? and what is the return value if rpcServer.getDatanodeReport fail? null?", "url": "https://github.com/apache/hadoop/pull/2080#discussion_r442388911", "createdAt": "2020-06-18T17:28:43Z", "author": {"login": "sunchao"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterWebHdfsMethods.java", "diffHunk": "@@ -564,4 +556,28 @@ public Credentials createCredentials(\n         renewer != null? renewer: ugi.getShortUserName());\n     return c;\n   }\n+  \n+    /**\n+   * Get the datanode report from all namespaces that are registered\n+   * and active in the federation.\n+   * @param router Router from which to get the report.\n+   * @return List of datanodes.\n+   * @throws IOException If it cannot get the RPC Server.\n+   */\n+  private static DatanodeInfo[] getDatanodeReport(\n+      final Router router) throws IOException {\n+    // We need to get the DNs as a privileged user\n+    final RouterRpcServer rpcServer = getRPCServer(router);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3369ba7b5eea4355487d8961a1f89fee15650407"}, "originalPosition": 76}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "712835aa8d579f6b9d2c3b4d4798b393d2941657", "author": {"user": {"login": "NickyYe", "name": "Ye Ni"}}, "url": "https://github.com/apache/hadoop/commit/712835aa8d579f6b9d2c3b4d4798b393d2941657", "committedDate": "2020-06-19T20:15:50Z", "message": "Update RouterWebHdfsMethods.java"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "53723474f41259315f1f9919b8c989b73a8b20a3", "author": {"user": {"login": "NickyYe", "name": "Ye Ni"}}, "url": "https://github.com/apache/hadoop/commit/53723474f41259315f1f9919b8c989b73a8b20a3", "committedDate": "2020-06-19T20:21:58Z", "message": "Update RouterRpcServer.java"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "66b2b33ab275a6819ca1494ec3b272392134d5e1", "author": {"user": {"login": "NickyYe", "name": "Ye Ni"}}, "url": "https://github.com/apache/hadoop/commit/66b2b33ab275a6819ca1494ec3b272392134d5e1", "committedDate": "2020-06-19T20:45:50Z", "message": "Update RouterRpcServer.java"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM0MzgxMTE1", "url": "https://github.com/apache/hadoop/pull/2080#pullrequestreview-434381115", "createdAt": "2020-06-19T23:40:33Z", "commit": {"oid": "66b2b33ab275a6819ca1494ec3b272392134d5e1"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQyMzo0MDozM1rOGmjXgA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQyMzo0ODo0NFrOGmjbsA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3ODUyOA==", "bodyText": "This will cause compilation error. Also I'm wondering whether it makes sense to move the DN cache logic from  NamenodeBeanMetrics to here and have the former to depend on this. This way we don't have to keep two copies of cache.", "url": "https://github.com/apache/hadoop/pull/2080#discussion_r443078528", "createdAt": "2020-06-19T23:40:33Z", "author": {"login": "sunchao"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java", "diffHunk": "@@ -18,6 +18,8 @@\n package org.apache.hadoop.hdfs.server.federation.router;\n \n import static org.apache.hadoop.fs.CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHORIZATION;\n+import static org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics.DN_REPORT_CACHE_EXPIRE;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "66b2b33ab275a6819ca1494ec3b272392134d5e1"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3OTYwMA==", "bodyText": "Question from me and not quite related to this JIRA: seems this is getting DNs for all the clusters while in the CREATE case we should only choose DN from a particular sub-cluster. Where is this logic implemented?", "url": "https://github.com/apache/hadoop/pull/2080#discussion_r443079600", "createdAt": "2020-06-19T23:48:44Z", "author": {"login": "sunchao"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterWebHdfsMethods.java", "diffHunk": "@@ -454,19 +454,12 @@ private URI redirectURI(final Router router, final UserGroupInformation ugi,\n   private DatanodeInfo chooseDatanode(final Router router,\n       final String path, final HttpOpParam.Op op, final long openOffset,\n       final String excludeDatanodes) throws IOException {\n-    // We need to get the DNs as a privileged user\n     final RouterRpcServer rpcServer = getRPCServer(router);\n-    UserGroupInformation loginUser = UserGroupInformation.getLoginUser();\n-    RouterRpcServer.setCurrentUser(loginUser);\n-\n     DatanodeInfo[] dns = null;\n     try {\n-      dns = rpcServer.getDatanodeReport(DatanodeReportType.LIVE);\n+      dns = rpcServer.getCachedDatanodeReport(DatanodeReportType.LIVE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "66b2b33ab275a6819ca1494ec3b272392134d5e1"}, "originalPosition": 12}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "801216bc6b2ab05b0be9f4fc82bb6714c8120200", "author": {"user": {"login": "NickyYe", "name": "Ye Ni"}}, "url": "https://github.com/apache/hadoop/commit/801216bc6b2ab05b0be9f4fc82bb6714c8120200", "committedDate": "2020-06-22T19:01:57Z", "message": "Update RouterRpcServer.java"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6bbbf46eb96ab315daf6d00d87c716e9543d04b8", "author": {"user": {"login": "NickyYe", "name": "Ye Ni"}}, "url": "https://github.com/apache/hadoop/commit/6bbbf46eb96ab315daf6d00d87c716e9543d04b8", "committedDate": "2020-06-22T20:47:08Z", "message": "Remove whitespace-eol"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM2MzQ4NTU4", "url": "https://github.com/apache/hadoop/pull/2080#pullrequestreview-436348558", "createdAt": "2020-06-24T06:08:04Z", "commit": {"oid": "6bbbf46eb96ab315daf6d00d87c716e9543d04b8"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwNjowODowNVrOGoEKmA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwNjoxMToxNlrOGoEOsQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY2NDQ3Mg==", "bodyText": "Hmm, have you considered using\nthis.dnCache = CacheBuilder.newBuilder()\n         .refreshAfterWrite(dnCacheExpire, TimeUnit.MILLISECONDS)\n         .build(new DatanodeReportCacheLoader());\nThis will also automatically refresh the caches. Also it only refreshes a key iff 1) it becomes stale, and 2) there is a request on it. So this will save some calls for those infrequent DN report types.", "url": "https://github.com/apache/hadoop/pull/2080#discussion_r444664472", "createdAt": "2020-06-24T06:08:05Z", "author": {"login": "sunchao"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java", "diffHunk": "@@ -361,6 +380,23 @@ public RouterRpcServer(Configuration configuration, Router router,\n     this.nnProto = new RouterNamenodeProtocol(this);\n     this.clientProto = new RouterClientProtocol(conf, this);\n     this.routerProto = new RouterUserProtocol(this);\n+\n+    long dnCacheExpire = conf.getTimeDuration(\n+        DN_REPORT_CACHE_EXPIRE,\n+        DN_REPORT_CACHE_EXPIRE_MS_DEFAULT, TimeUnit.MILLISECONDS);\n+    this.dnCache = CacheBuilder.newBuilder()\n+        .build(new DatanodeReportCacheLoader());\n+\n+    // Actively refresh the dn cache in a configured interval\n+    Executors", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6bbbf46eb96ab315daf6d00d87c716e9543d04b8"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY2NDU5NA==", "bodyText": "nit: this can be package-private?", "url": "https://github.com/apache/hadoop/pull/2080#discussion_r444664594", "createdAt": "2020-06-24T06:08:29Z", "author": {"login": "sunchao"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java", "diffHunk": "@@ -868,6 +904,50 @@ public HdfsLocatedFileStatus getLocatedFileInfo(String src,\n     return clientProto.getDatanodeReport(type);\n   }\n \n+  /**\n+   * Get the datanode report from cache.\n+   *\n+   * @param type Type of the datanode.\n+   * @return List of datanodes.\n+   * @throws IOException If it cannot get the report.\n+   */\n+  public DatanodeInfo[] getCachedDatanodeReport(DatanodeReportType type)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6bbbf46eb96ab315daf6d00d87c716e9543d04b8"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY2NDY2NQ==", "bodyText": "nit: space after {", "url": "https://github.com/apache/hadoop/pull/2080#discussion_r444664665", "createdAt": "2020-06-24T06:08:44Z", "author": {"login": "sunchao"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java", "diffHunk": "@@ -868,6 +904,50 @@ public HdfsLocatedFileStatus getLocatedFileInfo(String src,\n     return clientProto.getDatanodeReport(type);\n   }\n \n+  /**\n+   * Get the datanode report from cache.\n+   *\n+   * @param type Type of the datanode.\n+   * @return List of datanodes.\n+   * @throws IOException If it cannot get the report.\n+   */\n+  public DatanodeInfo[] getCachedDatanodeReport(DatanodeReportType type)\n+      throws IOException {\n+    try {\n+      DatanodeInfo[] dns = this.dnCache.get(type);\n+      if (dns == null) {\n+        LOG.debug(\"Get null DN report from cache\");\n+        dns = getCachedDatanodeReportImpl(type);\n+        this.dnCache.put(type, dns);\n+      }\n+      return dns;\n+    } catch (ExecutionException e) {\n+      LOG.error(\"Cannot get the DN report for {}\", type, e);\n+      Throwable cause = e.getCause();\n+      if (cause instanceof IOException) {\n+        throw (IOException) cause;\n+      } else {\n+        throw new IOException(cause);\n+      }\n+    }\n+  }\n+\n+  private DatanodeInfo[] getCachedDatanodeReportImpl\n+      (final DatanodeReportType type) throws IOException{", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6bbbf46eb96ab315daf6d00d87c716e9543d04b8"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY2NTE1NQ==", "bodyText": "hmm can we just use:\nexecutorService = MoreExecutors.listeningDecorator(\n    Executors.newSingleThreadExecutor());\n?", "url": "https://github.com/apache/hadoop/pull/2080#discussion_r444665155", "createdAt": "2020-06-24T06:10:10Z", "author": {"login": "sunchao"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java", "diffHunk": "@@ -1748,4 +1828,58 @@ public void refreshSuperUserGroupsConfiguration() throws IOException {\n   public String[] getGroupsForUser(String user) throws IOException {\n     return routerProto.getGroupsForUser(user);\n   }\n-}\n\\ No newline at end of file\n+\n+  /**\n+   * Deals with loading datanode report into the cache and refresh.\n+   */\n+  private class DatanodeReportCacheLoader\n+      extends CacheLoader<DatanodeReportType, DatanodeInfo[]> {\n+\n+    private ListeningExecutorService executorService;\n+\n+    DatanodeReportCacheLoader() {\n+      ThreadFactory threadFactory = new ThreadFactoryBuilder()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6bbbf46eb96ab315daf6d00d87c716e9543d04b8"}, "originalPosition": 133}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY2NTUyMQ==", "bodyText": "nit: variable listenableFuture is redundant - you can just return from submit call.", "url": "https://github.com/apache/hadoop/pull/2080#discussion_r444665521", "createdAt": "2020-06-24T06:11:16Z", "author": {"login": "sunchao"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java", "diffHunk": "@@ -1748,4 +1828,58 @@ public void refreshSuperUserGroupsConfiguration() throws IOException {\n   public String[] getGroupsForUser(String user) throws IOException {\n     return routerProto.getGroupsForUser(user);\n   }\n-}\n\\ No newline at end of file\n+\n+  /**\n+   * Deals with loading datanode report into the cache and refresh.\n+   */\n+  private class DatanodeReportCacheLoader\n+      extends CacheLoader<DatanodeReportType, DatanodeInfo[]> {\n+\n+    private ListeningExecutorService executorService;\n+\n+    DatanodeReportCacheLoader() {\n+      ThreadFactory threadFactory = new ThreadFactoryBuilder()\n+          .setNameFormat(\"DatanodeReport-Cache-Reload\")\n+          .setDaemon(true)\n+          .build();\n+\n+      // Only use 1 thread to refresh cache.\n+      // With coreThreadCount == maxThreadCount we effectively\n+      // create a fixed size thread pool. As allowCoreThreadTimeOut\n+      // has been set, all threads will die after 60 seconds of non use.\n+      ThreadPoolExecutor parentExecutor = new ThreadPoolExecutor(\n+          1,\n+          1,\n+          60,\n+          TimeUnit.SECONDS,\n+          new LinkedBlockingQueue<Runnable>(),\n+          threadFactory);\n+      parentExecutor.allowCoreThreadTimeOut(true);\n+      executorService = MoreExecutors.listeningDecorator(parentExecutor);\n+    }\n+\n+    @Override\n+    public DatanodeInfo[] load(DatanodeReportType type) throws Exception {\n+      return getCachedDatanodeReportImpl(type);\n+    }\n+\n+    /**\n+     * Override the reload method to provide an asynchronous implementation,\n+     * so that the query will not be slowed down by the cache refresh. It\n+     * will return the old cache value and schedule a background refresh.\n+     */\n+    @Override\n+    public ListenableFuture<DatanodeInfo[]> reload(\n+        final DatanodeReportType type, DatanodeInfo[] oldValue)\n+        throws Exception {\n+      ListenableFuture<DatanodeInfo[]> listenableFuture =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6bbbf46eb96ab315daf6d00d87c716e9543d04b8"}, "originalPosition": 167}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "33f104b75cdb61cd453c07cc356116e4231edb42", "author": {"user": {"login": "NickyYe", "name": "Ye Ni"}}, "url": "https://github.com/apache/hadoop/commit/33f104b75cdb61cd453c07cc356116e4231edb42", "committedDate": "2020-06-24T07:01:23Z", "message": "Address comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM2Mzc5MDUx", "url": "https://github.com/apache/hadoop/pull/2080#pullrequestreview-436379051", "createdAt": "2020-06-24T07:08:44Z", "commit": {"oid": "33f104b75cdb61cd453c07cc356116e4231edb42"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8970a0ed39d645fe47c2ec31ce5a39a490f7989e", "author": {"user": {"login": "NickyYe", "name": "Ye Ni"}}, "url": "https://github.com/apache/hadoop/commit/8970a0ed39d645fe47c2ec31ce5a39a490f7989e", "committedDate": "2020-06-24T18:00:20Z", "message": "Fix checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7eeb2b527dbce451e3006b5f3b0698132ad8b2ad", "author": {"user": {"login": "NickyYe", "name": "Ye Ni"}}, "url": "https://github.com/apache/hadoop/commit/7eeb2b527dbce451e3006b5f3b0698132ad8b2ad", "committedDate": "2020-07-05T08:19:15Z", "message": "Add testGetCachedDatanodeReport"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyNjk4NTMy", "url": "https://github.com/apache/hadoop/pull/2080#pullrequestreview-442698532", "createdAt": "2020-07-05T18:48:49Z", "commit": {"oid": "7eeb2b527dbce451e3006b5f3b0698132ad8b2ad"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNVQxODo0ODo0OVrOGtEJ3Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNVQxODo0ODo0OVrOGtEJ3Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkwNzE2NQ==", "bodyText": "I think we need to clear state after the test case so that it won't affect others like testNamenodeMetrics.\nAlso long line.", "url": "https://github.com/apache/hadoop/pull/2080#discussion_r449907165", "createdAt": "2020-07-05T18:48:49Z", "author": {"login": "sunchao"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterRpc.java", "diffHunk": "@@ -1777,6 +1777,43 @@ public void testgetGroupsForUser() throws IOException {\n     assertArrayEquals(group, result);\n   }\n \n+  @Test\n+  public void testGetCachedDatanodeReport() throws Exception {\n+    final DatanodeInfo[] datanodeReport =\n+        routerProtocol.getDatanodeReport(DatanodeReportType.ALL);\n+\n+    // We should have 12 nodes in total\n+    assertEquals(12, datanodeReport.length);\n+\n+    // We should be caching this information\n+    DatanodeInfo[] datanodeReport1 =\n+        routerProtocol.getDatanodeReport(DatanodeReportType.ALL);\n+    assertArrayEquals(datanodeReport1, datanodeReport);\n+\n+    // Add one datanode\n+    getCluster().getCluster().startDataNodes(getCluster().getCluster().getConfiguration(0),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7eeb2b527dbce451e3006b5f3b0698132ad8b2ad"}, "originalPosition": 18}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f4d4b9ac945f5983c7d7bb9f5295594b39959980", "author": {"user": {"login": "NickyYe", "name": "Ye Ni"}}, "url": "https://github.com/apache/hadoop/commit/f4d4b9ac945f5983c7d7bb9f5295594b39959980", "committedDate": "2020-07-06T01:38:54Z", "message": "Update TestRouterRpc.java"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dfb68119968e1601c7e0dbc0e7701f49d37c88a9", "author": {"user": {"login": "NickyYe", "name": "Ye Ni"}}, "url": "https://github.com/apache/hadoop/commit/dfb68119968e1601c7e0dbc0e7701f49d37c88a9", "committedDate": "2020-07-06T05:54:51Z", "message": "Update TestRouterRpc.java"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQzMjExNzgx", "url": "https://github.com/apache/hadoop/pull/2080#pullrequestreview-443211781", "createdAt": "2020-07-06T16:04:38Z", "commit": {"oid": "dfb68119968e1601c7e0dbc0e7701f49d37c88a9"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4042, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}