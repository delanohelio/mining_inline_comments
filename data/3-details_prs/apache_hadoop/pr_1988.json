{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDExMTk3ODYx", "number": 1988, "title": "HDFS-15305. Extend ViewFS and provide ViewFSOverloadScheme implementation with scheme configurable.", "bodyText": "https://issues.apache.org/jira/browse/HDFS-15305", "createdAt": "2020-04-30T07:03:37Z", "url": "https://github.com/apache/hadoop/pull/1988", "merged": true, "mergeCommit": {"oid": "9c8236d04dfc3d4cefe7a00b63625f60ee232cfe"}, "closed": true, "closedAt": "2020-05-05T00:55:41Z", "author": {"login": "umamaheswararao"}, "timelineItems": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABccneeogH2gAyNDExMTk3ODYxOjQyMWVmYmQyNGMyNjUzMjczY2RhODk1YWNhMzNjOGExNDZjZWNiNjI=", "endCursor": "Y3Vyc29yOnYyOpPPAAABceIu56gFqTQwNTQyOTY2Ng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "421efbd24c2653273cda895aca33c8a146cecb62", "author": {"user": {"login": "umamaheswararao", "name": "Uma Maheswara Rao G"}}, "url": "https://github.com/apache/hadoop/commit/421efbd24c2653273cda895aca33c8a146cecb62", "committedDate": "2020-04-30T06:55:33Z", "message": "HDFS-15305. Extend ViewFS and provide ViewFsOverloadScheme implementation with scheme configurable."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAzOTc4MDMy", "url": "https://github.com/apache/hadoop/pull/1988#pullrequestreview-403978032", "createdAt": "2020-05-01T01:23:52Z", "commit": {"oid": "421efbd24c2653273cda895aca33c8a146cecb62"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwMToyMzo1MlrOGO_nDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwMjowNDozNVrOGPAK5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM3NTQzNw==", "bodyText": "Why add this here? This is just used in tests right?", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418375437", "createdAt": "2020-05-01T01:23:52Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FsConstants.java", "diffHunk": "@@ -42,4 +42,11 @@\n    */\n   public static final URI VIEWFS_URI = URI.create(\"viewfs:///\");\n   public static final String VIEWFS_SCHEME = \"viewfs\";\n+\n+  public static final String VIEWFS_OVERLOAD_SCHEME_KEY =\n+      \"fs.viewfs.overload.scheme\";\n+  public static final String VIEWFS_OVERLOAD_SCHEME_DEFAULT = \"hdfs\";\n+  public static final String FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN_KEY =\n+      \"fs.viewfs.overload.scheme.target.%s.impl\";\n+  public static final String FS_IMPL_PATTERN_KEY = \"fs.%s.impl\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "421efbd24c2653273cda895aca33c8a146cecb62"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM3NjA0NA==", "bodyText": "javadoc for this method? This seems a bit hacky but I understand the need. I think initializeSuperFs is a slightly  better name but don't have a strong opinion here.", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418376044", "createdAt": "2020-05-01T01:26:26Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java", "diffHunk": "@@ -302,6 +320,11 @@ protected FileSystem getTargetFileSystem(final String settings,\n     }\n   }\n \n+  protected void superFSInit(final URI theUri, final Configuration conf)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "421efbd24c2653273cda895aca33c8a146cecb62"}, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM3NjI3OQ==", "bodyText": "nits:\n\"object is\" -> \"objective here is to handle\"\n\"a multiple\" -> multiple.", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418376279", "createdAt": "2020-05-01T01:27:23Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+\n+/******************************************************************************\n+ * This class is extended from the ViewFileSystem for the overloaded scheme \n+ * file system. This object is the way end-user code interacts with a multiple", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "421efbd24c2653273cda895aca33c8a146cecb62"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM3Njc2Ng==", "bodyText": "The explanation here is a little confusing. I think it's easier to provide an example on how it works rather than try to write about it.", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418376766", "createdAt": "2020-05-01T01:29:34Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+\n+/******************************************************************************", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "421efbd24c2653273cda895aca33c8a146cecb62"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM3OTU5NA==", "bodyText": "What if this is not set? No check for this currently.", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418379594", "createdAt": "2020-05-01T01:42:28Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+\n+/******************************************************************************\n+ * This class is extended from the ViewFileSystem for the overloaded scheme \n+ * file system. This object is the way end-user code interacts with a multiple\n+ * mounted file systems transparently. Overloaded scheme based uri can be\n+ * continued to use as end user interactive uri and mount links can be\n+ * configured to any Hadoop compatible file system. This class maintains all \n+ * the target file system instances and delegates the calls to respective \n+ * target file system based on mount link mapping. Mount link configuration\n+ * format and behavior is same as ViewFileSystem.\n+ *****************************************************************************/\n+@InterfaceAudience.LimitedPrivate({ \"MapReduce\", \"HBase\", \"Hive\" })\n+@InterfaceStability.Evolving\n+public class ViewFsOverloadScheme extends ViewFileSystem {\n+\n+  public ViewFsOverloadScheme() throws IOException {\n+    super();\n+  }\n+\n+  private FsCreator fsCreator;\n+  private String myScheme;\n+\n+  @Override\n+  public String getScheme() {\n+    return myScheme;\n+  }\n+\n+  @Override\n+  public void initialize(final URI theUri, final Configuration conf)\n+      throws IOException {\n+    superFSInit(theUri, conf);\n+    setConf(conf);\n+    config = conf;\n+    myScheme = config.get(FsConstants.VIEWFS_OVERLOAD_SCHEME_KEY);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "421efbd24c2653273cda895aca33c8a146cecb62"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM4MDE2Ng==", "bodyText": "Is there a reason why the scheme of this FS needs to be set using VIEWFS_OVERLOAD_SCHEME_KEY? I would have assumed that we use \"view://\" similar to ViewFileSystem. Will we have valid use cases where fs.getScheme() needs to match \"hdfs\"?", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418380166", "createdAt": "2020-05-01T01:44:55Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+\n+/******************************************************************************\n+ * This class is extended from the ViewFileSystem for the overloaded scheme \n+ * file system. This object is the way end-user code interacts with a multiple\n+ * mounted file systems transparently. Overloaded scheme based uri can be\n+ * continued to use as end user interactive uri and mount links can be\n+ * configured to any Hadoop compatible file system. This class maintains all \n+ * the target file system instances and delegates the calls to respective \n+ * target file system based on mount link mapping. Mount link configuration\n+ * format and behavior is same as ViewFileSystem.\n+ *****************************************************************************/\n+@InterfaceAudience.LimitedPrivate({ \"MapReduce\", \"HBase\", \"Hive\" })\n+@InterfaceStability.Evolving\n+public class ViewFsOverloadScheme extends ViewFileSystem {\n+\n+  public ViewFsOverloadScheme() throws IOException {\n+    super();\n+  }\n+\n+  private FsCreator fsCreator;\n+  private String myScheme;\n+\n+  @Override\n+  public String getScheme() {\n+    return myScheme;\n+  }\n+\n+  @Override\n+  public void initialize(final URI theUri, final Configuration conf)\n+      throws IOException {\n+    superFSInit(theUri, conf);\n+    setConf(conf);\n+    config = conf;\n+    myScheme = config.get(FsConstants.VIEWFS_OVERLOAD_SCHEME_KEY);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "421efbd24c2653273cda895aca33c8a146cecb62"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM4MDk4OA==", "bodyText": "Currently, the override only works for one scheme. This alone can prevent the circular dependency. However, should we consider calling createFileSystem for all schemes that have FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN_KEY set?", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418380988", "createdAt": "2020-05-01T01:48:35Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+\n+/******************************************************************************\n+ * This class is extended from the ViewFileSystem for the overloaded scheme \n+ * file system. This object is the way end-user code interacts with a multiple\n+ * mounted file systems transparently. Overloaded scheme based uri can be\n+ * continued to use as end user interactive uri and mount links can be\n+ * configured to any Hadoop compatible file system. This class maintains all \n+ * the target file system instances and delegates the calls to respective \n+ * target file system based on mount link mapping. Mount link configuration\n+ * format and behavior is same as ViewFileSystem.\n+ *****************************************************************************/\n+@InterfaceAudience.LimitedPrivate({ \"MapReduce\", \"HBase\", \"Hive\" })\n+@InterfaceStability.Evolving\n+public class ViewFsOverloadScheme extends ViewFileSystem {\n+\n+  public ViewFsOverloadScheme() throws IOException {\n+    super();\n+  }\n+\n+  private FsCreator fsCreator;\n+  private String myScheme;\n+\n+  @Override\n+  public String getScheme() {\n+    return myScheme;\n+  }\n+\n+  @Override\n+  public void initialize(final URI theUri, final Configuration conf)\n+      throws IOException {\n+    superFSInit(theUri, conf);\n+    setConf(conf);\n+    config = conf;\n+    myScheme = config.get(FsConstants.VIEWFS_OVERLOAD_SCHEME_KEY);\n+    fsCreator = new FsCreator() {\n+\n+      /**\n+       * This method is overridden because in ViewFsOverloadScheme if\n+       * overloaded scheme matches with mounted target fs scheme, file system\n+       * should be created without going into fs.<scheme>.impl based \n+       * resolution. Otherwise it will end up into loop as target will be \n+       * resolved again to ViewFsOverloadScheme as fs.<scheme>.impl points to\n+       * ViewFsOverloadScheme. So, below method will initialize the\n+       * fs.viewfs.overload.scheme.target.<scheme>.impl. Other schemes can\n+       * follow fs.newInstance\n+       */\n+      @Override\n+      public FileSystem createFs(URI uri, Configuration conf)\n+          throws IOException {\n+        if (uri.getScheme().equals(myScheme)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "421efbd24c2653273cda895aca33c8a146cecb62"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM4MjIxNg==", "bodyText": "nit: \"with mounted target fs scheme\" -> \"the scheme of the target fs\"\n\"file system should be created\" -> \"the target file system ..\"", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418382216", "createdAt": "2020-05-01T01:53:50Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+\n+/******************************************************************************\n+ * This class is extended from the ViewFileSystem for the overloaded scheme \n+ * file system. This object is the way end-user code interacts with a multiple\n+ * mounted file systems transparently. Overloaded scheme based uri can be\n+ * continued to use as end user interactive uri and mount links can be\n+ * configured to any Hadoop compatible file system. This class maintains all \n+ * the target file system instances and delegates the calls to respective \n+ * target file system based on mount link mapping. Mount link configuration\n+ * format and behavior is same as ViewFileSystem.\n+ *****************************************************************************/\n+@InterfaceAudience.LimitedPrivate({ \"MapReduce\", \"HBase\", \"Hive\" })\n+@InterfaceStability.Evolving\n+public class ViewFsOverloadScheme extends ViewFileSystem {\n+\n+  public ViewFsOverloadScheme() throws IOException {\n+    super();\n+  }\n+\n+  private FsCreator fsCreator;\n+  private String myScheme;\n+\n+  @Override\n+  public String getScheme() {\n+    return myScheme;\n+  }\n+\n+  @Override\n+  public void initialize(final URI theUri, final Configuration conf)\n+      throws IOException {\n+    superFSInit(theUri, conf);\n+    setConf(conf);\n+    config = conf;\n+    myScheme = config.get(FsConstants.VIEWFS_OVERLOAD_SCHEME_KEY);\n+    fsCreator = new FsCreator() {\n+\n+      /**\n+       * This method is overridden because in ViewFsOverloadScheme if\n+       * overloaded scheme matches with mounted target fs scheme, file system", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "421efbd24c2653273cda895aca33c8a146cecb62"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM4MjUyNw==", "bodyText": "nit: \"into loop\" -> \"in an infinite loop as the target\"", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418382527", "createdAt": "2020-05-01T01:55:04Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+\n+/******************************************************************************\n+ * This class is extended from the ViewFileSystem for the overloaded scheme \n+ * file system. This object is the way end-user code interacts with a multiple\n+ * mounted file systems transparently. Overloaded scheme based uri can be\n+ * continued to use as end user interactive uri and mount links can be\n+ * configured to any Hadoop compatible file system. This class maintains all \n+ * the target file system instances and delegates the calls to respective \n+ * target file system based on mount link mapping. Mount link configuration\n+ * format and behavior is same as ViewFileSystem.\n+ *****************************************************************************/\n+@InterfaceAudience.LimitedPrivate({ \"MapReduce\", \"HBase\", \"Hive\" })\n+@InterfaceStability.Evolving\n+public class ViewFsOverloadScheme extends ViewFileSystem {\n+\n+  public ViewFsOverloadScheme() throws IOException {\n+    super();\n+  }\n+\n+  private FsCreator fsCreator;\n+  private String myScheme;\n+\n+  @Override\n+  public String getScheme() {\n+    return myScheme;\n+  }\n+\n+  @Override\n+  public void initialize(final URI theUri, final Configuration conf)\n+      throws IOException {\n+    superFSInit(theUri, conf);\n+    setConf(conf);\n+    config = conf;\n+    myScheme = config.get(FsConstants.VIEWFS_OVERLOAD_SCHEME_KEY);\n+    fsCreator = new FsCreator() {\n+\n+      /**\n+       * This method is overridden because in ViewFsOverloadScheme if\n+       * overloaded scheme matches with mounted target fs scheme, file system\n+       * should be created without going into fs.<scheme>.impl based \n+       * resolution. Otherwise it will end up into loop as target will be ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "421efbd24c2653273cda895aca33c8a146cecb62"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM4NDYxNQ==", "bodyText": "If we add a protected method getFsCreator(String scheme) in ViewFileSystem and override it this class,I think initialize can be made much simpler. it just needs to conf.setBoolean(CONFIG_VIEWFS_ENABLE_INNER_CACHE, true) and call  super.initialize(theUri, conf). Is that correct?", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418384615", "createdAt": "2020-05-01T02:04:35Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+\n+/******************************************************************************\n+ * This class is extended from the ViewFileSystem for the overloaded scheme \n+ * file system. This object is the way end-user code interacts with a multiple\n+ * mounted file systems transparently. Overloaded scheme based uri can be\n+ * continued to use as end user interactive uri and mount links can be\n+ * configured to any Hadoop compatible file system. This class maintains all \n+ * the target file system instances and delegates the calls to respective \n+ * target file system based on mount link mapping. Mount link configuration\n+ * format and behavior is same as ViewFileSystem.\n+ *****************************************************************************/\n+@InterfaceAudience.LimitedPrivate({ \"MapReduce\", \"HBase\", \"Hive\" })\n+@InterfaceStability.Evolving\n+public class ViewFsOverloadScheme extends ViewFileSystem {\n+\n+  public ViewFsOverloadScheme() throws IOException {\n+    super();\n+  }\n+\n+  private FsCreator fsCreator;\n+  private String myScheme;\n+\n+  @Override\n+  public String getScheme() {\n+    return myScheme;\n+  }\n+\n+  @Override\n+  public void initialize(final URI theUri, final Configuration conf)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "421efbd24c2653273cda895aca33c8a146cecb62"}, "originalPosition": 60}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8010048dc27959c10c7a982774b4312f4c64924c", "author": {"user": {"login": "umamaheswararao", "name": "Uma Maheswara Rao G"}}, "url": "https://github.com/apache/hadoop/commit/8010048dc27959c10c7a982774b4312f4c64924c", "committedDate": "2020-05-01T08:13:23Z", "message": "HDFS-15305. Update: Simplified initialization part and added a test case along with comment fixups."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA0MzkzOTU2", "url": "https://github.com/apache/hadoop/pull/1988#pullrequestreview-404393956", "createdAt": "2020-05-01T20:50:29Z", "commit": {"oid": "8010048dc27959c10c7a982774b4312f4c64924c"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQyMDo1MDoyOVrOGPVYfg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQyMTowMzo1NFrOGPVr0Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODczMjE1OA==", "bodyText": "why do need this method while we can just call new FsGetter() directly?", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418732158", "createdAt": "2020-05-01T20:50:29Z", "author": {"login": "chliang71"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java", "diffHunk": "@@ -96,16 +96,49 @@ static AccessControlException readOnlyMountTable(final String operation,\n     return readOnlyMountTable(operation, p.toString());\n   }\n \n+  /**\n+   * File system instance getter.\n+   */\n+  static class FsGetter {\n+\n+    /**\n+     * Gets new file system instance of given uri.\n+     */\n+    public FileSystem getNewInstance(URI uri, Configuration conf)\n+        throws IOException {\n+      return FileSystem.newInstance(uri, conf);\n+    }\n+\n+    /**\n+     * Gets file system instance of given uri.\n+     */\n+    public FileSystem get(URI uri, Configuration conf) throws IOException {\n+      return FileSystem.get(uri, conf);\n+    }\n+  }\n+\n+  /**\n+   * Gets file system creator instance.\n+   */\n+  protected FsGetter fsGetter() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8010048dc27959c10c7a982774b4312f4c64924c"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODczMzIyMA==", "bodyText": "a general comment is it would be good to have some logging in this class (DEBUG level)", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418733220", "createdAt": "2020-05-01T20:53:10Z", "author": {"login": "chliang71"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "diffHunk": "@@ -0,0 +1,170 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n+import java.net.URI;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+\n+/******************************************************************************\n+ * This class is extended from the ViewFileSystem for the overloaded scheme file\n+ * system. The objective here is to handle multiple mounted file systems\n+ * transparently. Mount link configurations and in-memory mount table\n+ * building behaviors are inherited from ViewFileSystem. Unlike ViewFileSystem\n+ * scheme (viewfs://), the users would be able to use any scheme.\n+ *\n+ * Example 1:\n+ * If users want some of their existing cluster (hdfs://Cluster)\n+ * data to mount with other hdfs and object store clusters(hdfs://NN1,\n+ * o3fs://bucket1.volume1/, s3a://bucket1/)\n+ *\n+ * fs.viewfs.mounttable.Cluster./user = hdfs://NN1/user\n+ * fs.viewfs.mounttable.Cluster./data = o3fs://bucket1.volume1/data\n+ * fs.viewfs.mounttable.Cluster./backup = s3a://bucket1/backup/\n+ *\n+ * Op1: Create file hdfs://Cluster/user/fileA will go to hdfs://NN1/user/fileA\n+ * Op2: Create file hdfs://Cluster/data/datafile will go to\n+ *      o3fs://bucket1.volume1/data/datafile\n+ * Op3: Create file hdfs://Cluster/backup/data.zip will go to\n+ *      s3a://bucket1/backup/data.zip\n+ *\n+ * Example 2:\n+ * If users want some of their existing cluster (s3a://bucketA/)\n+ * data to mount with other hdfs and object store clusters\n+ * (hdfs://NN1, o3fs://bucket1.volume1/)\n+ *\n+ * fs.viewfs.mounttable.bucketA./user = hdfs://NN1/user\n+ * fs.viewfs.mounttable.bucketA./data = o3fs://bucket1.volume1/data\n+ * fs.viewfs.mounttable.bucketA./salesDB = s3a://bucketA/salesDB/\n+ *\n+ * Op1: Create file s3a://bucketA/user/fileA will go to hdfs://NN1/user/fileA\n+ * Op2: Create file s3a://bucketA/data/datafile will go to\n+ *      o3fs://bucket1.volume1/data/datafile\n+ * Op3: Create file s3a://bucketA/salesDB/dbfile will go to\n+ *      s3a://bucketA/salesDB/dbfile\n+ *****************************************************************************/\n+@InterfaceAudience.LimitedPrivate({ \"MapReduce\", \"HBase\", \"Hive\" })\n+@InterfaceStability.Evolving\n+public class ViewFsOverloadScheme extends ViewFileSystem {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8010048dc27959c10c7a982774b4312f4c64924c"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODczNzEwNQ==", "bodyText": "shouldn't this message be testLocalFsLinkSlashMerge? similar for the other test below", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418737105", "createdAt": "2020-05-01T21:03:54Z", "author": {"login": "chliang71"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeLocalFileSystem.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+/**\n+ *\n+ * Test the TestViewFsOverloadSchemeLocalFS using a file with authority:\n+ * file://mountTableName/ i.e, the authority is used to load a mount table.\n+ */\n+public class TestViewFsOverloadSchemeLocalFileSystem {\n+  private static final String FILE = \"file\";\n+  private static final Log LOG =\n+      LogFactory.getLog(TestViewFsOverloadSchemeLocalFileSystem.class);\n+  private FileSystem fsTarget;\n+  private Configuration conf;\n+  private Path targetTestRoot;\n+  private FileSystemTestHelper fileSystemTestHelper;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    conf.set(String.format(\"fs.%s.impl\",\n+        FILE),\n+        ViewFsOverloadScheme.class.getName());\n+    conf.set(String.format(\n+        FsConstants.FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN,\n+        FILE),\n+        LocalFileSystem.class.getName());\n+    fsTarget = new LocalFileSystem();\n+    fsTarget.initialize(new URI(\"file:///\"), conf);\n+    fileSystemTestHelper = new FileSystemTestHelper();\n+    // create the test root on local_fs\n+    targetTestRoot = fileSystemTestHelper.getAbsoluteTestRootPath(fsTarget);\n+    fsTarget.delete(targetTestRoot, true);\n+    fsTarget.mkdirs(targetTestRoot);\n+  }\n+\n+  /**\n+   * Tests write file and read file with ViewFSOverloadScheme.\n+   */\n+  @Test\n+  public void testLocalTargetLinkWriteSimple() throws IOException {\n+    LOG.info(\"Starting testLocalTargetLinkWriteSimple\");\n+    final String testString = \"Hello Local!...\";\n+    final Path lfsRoot = new Path(\"/lfsRoot\");\n+    ConfigUtil.addLink(conf, lfsRoot.toString(),\n+        URI.create(targetTestRoot + \"/local\"));\n+    final FileSystem lViewFs = FileSystem.get(URI.create(\"file:///\"), conf);\n+\n+    final Path testPath = new Path(lfsRoot, \"test.txt\");\n+    final FSDataOutputStream fsDos = lViewFs.create(testPath);\n+    try {\n+      fsDos.writeUTF(testString);\n+    } finally {\n+      fsDos.close();\n+    }\n+\n+    FSDataInputStream lViewIs = lViewFs.open(testPath);\n+    try {\n+      Assert.assertEquals(testString, lViewIs.readUTF());\n+    } finally {\n+      lViewIs.close();\n+    }\n+  }\n+\n+  /**\n+   * Tests create file and delete file with ViewFSOverloadScheme.\n+   */\n+  @Test\n+  public void testLocalFsCreateAndDelete() throws Exception {\n+    LOG.info(\"Starting testLocalFsCreateAndDelete\");\n+    ConfigUtil.addLink(conf, \"mt\", \"/lfsroot\",\n+        URI.create(targetTestRoot + \"/wd2\"));\n+    final URI mountURI = URI.create(\"file://mt/\");\n+    final FileSystem lViewFS = FileSystem.get(mountURI, conf);\n+    try {\n+      Path testPath = new Path(mountURI.toString() + \"/lfsroot/test\");\n+      lViewFS.create(testPath);\n+      Assert.assertTrue(lViewFS.exists(testPath));\n+      lViewFS.delete(testPath, true);\n+      Assert.assertFalse(lViewFS.exists(testPath));\n+    } finally {\n+      lViewFS.close();\n+    }\n+  }\n+\n+  /**\n+   * Tests root level file with linkMergeSlash with ViewFSOverloadScheme.\n+   */\n+  @Test\n+  public void testLocalFsLinkSlashMerge() throws Exception {\n+    LOG.info(\"Starting testLocalFSCreateAndDelete\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8010048dc27959c10c7a982774b4312f4c64924c"}, "originalPosition": 125}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c1525f808f02ed40ca7451d4d71b7fa12708e106", "author": {"user": {"login": "umamaheswararao", "name": "Uma Maheswara Rao G"}}, "url": "https://github.com/apache/hadoop/commit/c1525f808f02ed40ca7451d4d71b7fa12708e106", "committedDate": "2020-05-01T22:28:05Z", "message": "HDFS-15305. Update: Fixed few review comments.."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA0NTc4NDA2", "url": "https://github.com/apache/hadoop/pull/1988#pullrequestreview-404578406", "createdAt": "2020-05-03T03:31:28Z", "commit": {"oid": "c1525f808f02ed40ca7451d4d71b7fa12708e106"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QwMzozMToyOFrOGPoO0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QwNDoyNToxMVrOGPofBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0MDk3Nw==", "bodyText": "Thanks for adding these Uma. A couple of comments:\n(a) \"The objective here is to handle multiple mounted file systems transparently.\", \"Unlike ViewFileSystem\nscheme (viewfs://), the users would be able to use any scheme.\"   --> These are not functions of this class. ViewFileSystem already does this.\n(b) Configuring fs.SCHEME.impl = ViewFsOverloadScheme is not explained in these examples. That should also be brought up.", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419040977", "createdAt": "2020-05-03T03:31:28Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+\n+/******************************************************************************", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM3Njc2Ng=="}, "originalCommit": {"oid": "421efbd24c2653273cda895aca33c8a146cecb62"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0MjEyNQ==", "bodyText": "indentation of the conf.set lines is confusing. can you make this better?", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419042125", "createdAt": "2020-05-03T03:46:45Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeLocalFileSystem.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+/**\n+ *\n+ * Test the TestViewFsOverloadSchemeLocalFS using a file with authority:\n+ * file://mountTableName/ i.e, the authority is used to load a mount table.\n+ */\n+public class TestViewFsOverloadSchemeLocalFileSystem {\n+  private static final String FILE = \"file\";\n+  private static final Log LOG =\n+      LogFactory.getLog(TestViewFsOverloadSchemeLocalFileSystem.class);\n+  private FileSystem fsTarget;\n+  private Configuration conf;\n+  private Path targetTestRoot;\n+  private FileSystemTestHelper fileSystemTestHelper;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    conf.set(String.format(\"fs.%s.impl\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c1525f808f02ed40ca7451d4d71b7fa12708e106"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0MjM1NA==", "bodyText": "try (FSDataOutputStream fsDos = lViewFs.create(testPath)) { }", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419042354", "createdAt": "2020-05-03T03:49:17Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeLocalFileSystem.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+/**\n+ *\n+ * Test the TestViewFsOverloadSchemeLocalFS using a file with authority:\n+ * file://mountTableName/ i.e, the authority is used to load a mount table.\n+ */\n+public class TestViewFsOverloadSchemeLocalFileSystem {\n+  private static final String FILE = \"file\";\n+  private static final Log LOG =\n+      LogFactory.getLog(TestViewFsOverloadSchemeLocalFileSystem.class);\n+  private FileSystem fsTarget;\n+  private Configuration conf;\n+  private Path targetTestRoot;\n+  private FileSystemTestHelper fileSystemTestHelper;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    conf.set(String.format(\"fs.%s.impl\",\n+        FILE),\n+        ViewFsOverloadScheme.class.getName());\n+    conf.set(String.format(\n+        FsConstants.FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN,\n+        FILE),\n+        LocalFileSystem.class.getName());\n+    fsTarget = new LocalFileSystem();\n+    fsTarget.initialize(new URI(\"file:///\"), conf);\n+    fileSystemTestHelper = new FileSystemTestHelper();\n+    // create the test root on local_fs\n+    targetTestRoot = fileSystemTestHelper.getAbsoluteTestRootPath(fsTarget);\n+    fsTarget.delete(targetTestRoot, true);\n+    fsTarget.mkdirs(targetTestRoot);\n+  }\n+\n+  /**\n+   * Tests write file and read file with ViewFSOverloadScheme.\n+   */\n+  @Test\n+  public void testLocalTargetLinkWriteSimple() throws IOException {\n+    LOG.info(\"Starting testLocalTargetLinkWriteSimple\");\n+    final String testString = \"Hello Local!...\";\n+    final Path lfsRoot = new Path(\"/lfsRoot\");\n+    ConfigUtil.addLink(conf, lfsRoot.toString(),\n+        URI.create(targetTestRoot + \"/local\"));\n+    final FileSystem lViewFs = FileSystem.get(URI.create(\"file:///\"), conf);\n+\n+    final Path testPath = new Path(lfsRoot, \"test.txt\");\n+    final FSDataOutputStream fsDos = lViewFs.create(testPath);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c1525f808f02ed40ca7451d4d71b7fa12708e106"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0MjM5OA==", "bodyText": "try(FSDataInputStream lViewIs = lViewFs.open(testPath)) {}", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419042398", "createdAt": "2020-05-03T03:49:54Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeLocalFileSystem.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+/**\n+ *\n+ * Test the TestViewFsOverloadSchemeLocalFS using a file with authority:\n+ * file://mountTableName/ i.e, the authority is used to load a mount table.\n+ */\n+public class TestViewFsOverloadSchemeLocalFileSystem {\n+  private static final String FILE = \"file\";\n+  private static final Log LOG =\n+      LogFactory.getLog(TestViewFsOverloadSchemeLocalFileSystem.class);\n+  private FileSystem fsTarget;\n+  private Configuration conf;\n+  private Path targetTestRoot;\n+  private FileSystemTestHelper fileSystemTestHelper;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    conf.set(String.format(\"fs.%s.impl\",\n+        FILE),\n+        ViewFsOverloadScheme.class.getName());\n+    conf.set(String.format(\n+        FsConstants.FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN,\n+        FILE),\n+        LocalFileSystem.class.getName());\n+    fsTarget = new LocalFileSystem();\n+    fsTarget.initialize(new URI(\"file:///\"), conf);\n+    fileSystemTestHelper = new FileSystemTestHelper();\n+    // create the test root on local_fs\n+    targetTestRoot = fileSystemTestHelper.getAbsoluteTestRootPath(fsTarget);\n+    fsTarget.delete(targetTestRoot, true);\n+    fsTarget.mkdirs(targetTestRoot);\n+  }\n+\n+  /**\n+   * Tests write file and read file with ViewFSOverloadScheme.\n+   */\n+  @Test\n+  public void testLocalTargetLinkWriteSimple() throws IOException {\n+    LOG.info(\"Starting testLocalTargetLinkWriteSimple\");\n+    final String testString = \"Hello Local!...\";\n+    final Path lfsRoot = new Path(\"/lfsRoot\");\n+    ConfigUtil.addLink(conf, lfsRoot.toString(),\n+        URI.create(targetTestRoot + \"/local\"));\n+    final FileSystem lViewFs = FileSystem.get(URI.create(\"file:///\"), conf);\n+\n+    final Path testPath = new Path(lfsRoot, \"test.txt\");\n+    final FSDataOutputStream fsDos = lViewFs.create(testPath);\n+    try {\n+      fsDos.writeUTF(testString);\n+    } finally {\n+      fsDos.close();\n+    }\n+\n+    FSDataInputStream lViewIs = lViewFs.open(testPath);\n+    try {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c1525f808f02ed40ca7451d4d71b7fa12708e106"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0MjcxNA==", "bodyText": "stream is not closed. may be use createNewFile?", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419042714", "createdAt": "2020-05-03T03:54:13Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeLocalFileSystem.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+/**\n+ *\n+ * Test the TestViewFsOverloadSchemeLocalFS using a file with authority:\n+ * file://mountTableName/ i.e, the authority is used to load a mount table.\n+ */\n+public class TestViewFsOverloadSchemeLocalFileSystem {\n+  private static final String FILE = \"file\";\n+  private static final Log LOG =\n+      LogFactory.getLog(TestViewFsOverloadSchemeLocalFileSystem.class);\n+  private FileSystem fsTarget;\n+  private Configuration conf;\n+  private Path targetTestRoot;\n+  private FileSystemTestHelper fileSystemTestHelper;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    conf.set(String.format(\"fs.%s.impl\",\n+        FILE),\n+        ViewFsOverloadScheme.class.getName());\n+    conf.set(String.format(\n+        FsConstants.FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN,\n+        FILE),\n+        LocalFileSystem.class.getName());\n+    fsTarget = new LocalFileSystem();\n+    fsTarget.initialize(new URI(\"file:///\"), conf);\n+    fileSystemTestHelper = new FileSystemTestHelper();\n+    // create the test root on local_fs\n+    targetTestRoot = fileSystemTestHelper.getAbsoluteTestRootPath(fsTarget);\n+    fsTarget.delete(targetTestRoot, true);\n+    fsTarget.mkdirs(targetTestRoot);\n+  }\n+\n+  /**\n+   * Tests write file and read file with ViewFSOverloadScheme.\n+   */\n+  @Test\n+  public void testLocalTargetLinkWriteSimple() throws IOException {\n+    LOG.info(\"Starting testLocalTargetLinkWriteSimple\");\n+    final String testString = \"Hello Local!...\";\n+    final Path lfsRoot = new Path(\"/lfsRoot\");\n+    ConfigUtil.addLink(conf, lfsRoot.toString(),\n+        URI.create(targetTestRoot + \"/local\"));\n+    final FileSystem lViewFs = FileSystem.get(URI.create(\"file:///\"), conf);\n+\n+    final Path testPath = new Path(lfsRoot, \"test.txt\");\n+    final FSDataOutputStream fsDos = lViewFs.create(testPath);\n+    try {\n+      fsDos.writeUTF(testString);\n+    } finally {\n+      fsDos.close();\n+    }\n+\n+    FSDataInputStream lViewIs = lViewFs.open(testPath);\n+    try {\n+      Assert.assertEquals(testString, lViewIs.readUTF());\n+    } finally {\n+      lViewIs.close();\n+    }\n+  }\n+\n+  /**\n+   * Tests create file and delete file with ViewFSOverloadScheme.\n+   */\n+  @Test\n+  public void testLocalFsCreateAndDelete() throws Exception {\n+    LOG.info(\"Starting testLocalFsCreateAndDelete\");\n+    ConfigUtil.addLink(conf, \"mt\", \"/lfsroot\",\n+        URI.create(targetTestRoot + \"/wd2\"));\n+    final URI mountURI = URI.create(\"file://mt/\");\n+    final FileSystem lViewFS = FileSystem.get(mountURI, conf);\n+    try {\n+      Path testPath = new Path(mountURI.toString() + \"/lfsroot/test\");\n+      lViewFS.create(testPath);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c1525f808f02ed40ca7451d4d71b7fa12708e106"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0Mjc3MA==", "bodyText": "same comment here -- use createNewFile?", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419042770", "createdAt": "2020-05-03T03:54:48Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeLocalFileSystem.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+/**\n+ *\n+ * Test the TestViewFsOverloadSchemeLocalFS using a file with authority:\n+ * file://mountTableName/ i.e, the authority is used to load a mount table.\n+ */\n+public class TestViewFsOverloadSchemeLocalFileSystem {\n+  private static final String FILE = \"file\";\n+  private static final Log LOG =\n+      LogFactory.getLog(TestViewFsOverloadSchemeLocalFileSystem.class);\n+  private FileSystem fsTarget;\n+  private Configuration conf;\n+  private Path targetTestRoot;\n+  private FileSystemTestHelper fileSystemTestHelper;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    conf.set(String.format(\"fs.%s.impl\",\n+        FILE),\n+        ViewFsOverloadScheme.class.getName());\n+    conf.set(String.format(\n+        FsConstants.FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN,\n+        FILE),\n+        LocalFileSystem.class.getName());\n+    fsTarget = new LocalFileSystem();\n+    fsTarget.initialize(new URI(\"file:///\"), conf);\n+    fileSystemTestHelper = new FileSystemTestHelper();\n+    // create the test root on local_fs\n+    targetTestRoot = fileSystemTestHelper.getAbsoluteTestRootPath(fsTarget);\n+    fsTarget.delete(targetTestRoot, true);\n+    fsTarget.mkdirs(targetTestRoot);\n+  }\n+\n+  /**\n+   * Tests write file and read file with ViewFSOverloadScheme.\n+   */\n+  @Test\n+  public void testLocalTargetLinkWriteSimple() throws IOException {\n+    LOG.info(\"Starting testLocalTargetLinkWriteSimple\");\n+    final String testString = \"Hello Local!...\";\n+    final Path lfsRoot = new Path(\"/lfsRoot\");\n+    ConfigUtil.addLink(conf, lfsRoot.toString(),\n+        URI.create(targetTestRoot + \"/local\"));\n+    final FileSystem lViewFs = FileSystem.get(URI.create(\"file:///\"), conf);\n+\n+    final Path testPath = new Path(lfsRoot, \"test.txt\");\n+    final FSDataOutputStream fsDos = lViewFs.create(testPath);\n+    try {\n+      fsDos.writeUTF(testString);\n+    } finally {\n+      fsDos.close();\n+    }\n+\n+    FSDataInputStream lViewIs = lViewFs.open(testPath);\n+    try {\n+      Assert.assertEquals(testString, lViewIs.readUTF());\n+    } finally {\n+      lViewIs.close();\n+    }\n+  }\n+\n+  /**\n+   * Tests create file and delete file with ViewFSOverloadScheme.\n+   */\n+  @Test\n+  public void testLocalFsCreateAndDelete() throws Exception {\n+    LOG.info(\"Starting testLocalFsCreateAndDelete\");\n+    ConfigUtil.addLink(conf, \"mt\", \"/lfsroot\",\n+        URI.create(targetTestRoot + \"/wd2\"));\n+    final URI mountURI = URI.create(\"file://mt/\");\n+    final FileSystem lViewFS = FileSystem.get(mountURI, conf);\n+    try {\n+      Path testPath = new Path(mountURI.toString() + \"/lfsroot/test\");\n+      lViewFS.create(testPath);\n+      Assert.assertTrue(lViewFS.exists(testPath));\n+      lViewFS.delete(testPath, true);\n+      Assert.assertFalse(lViewFS.exists(testPath));\n+    } finally {\n+      lViewFS.close();\n+    }\n+  }\n+\n+  /**\n+   * Tests root level file with linkMergeSlash with ViewFSOverloadScheme.\n+   */\n+  @Test\n+  public void testLocalFsLinkSlashMerge() throws Exception {\n+    LOG.info(\"Starting testLocalFsLinkSlashMerge\");\n+    ConfigUtil.addLinkMergeSlash(conf, \"mt\",\n+        URI.create(targetTestRoot + \"/wd2\"));\n+    final URI mountURI = URI.create(\"file://mt/\");\n+    final FileSystem lViewFS = FileSystem.get(mountURI, conf);\n+    try {\n+      Path fileOnRoot = new Path(mountURI.toString() + \"/NewFile\");\n+      lViewFS.create(fileOnRoot);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c1525f808f02ed40ca7451d4d71b7fa12708e106"}, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0MzEwMg==", "bodyText": "fsTarget.close() as well?", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419043102", "createdAt": "2020-05-03T03:59:08Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeLocalFileSystem.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+/**\n+ *\n+ * Test the TestViewFsOverloadSchemeLocalFS using a file with authority:\n+ * file://mountTableName/ i.e, the authority is used to load a mount table.\n+ */\n+public class TestViewFsOverloadSchemeLocalFileSystem {\n+  private static final String FILE = \"file\";\n+  private static final Log LOG =\n+      LogFactory.getLog(TestViewFsOverloadSchemeLocalFileSystem.class);\n+  private FileSystem fsTarget;\n+  private Configuration conf;\n+  private Path targetTestRoot;\n+  private FileSystemTestHelper fileSystemTestHelper;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    conf.set(String.format(\"fs.%s.impl\",\n+        FILE),\n+        ViewFsOverloadScheme.class.getName());\n+    conf.set(String.format(\n+        FsConstants.FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN,\n+        FILE),\n+        LocalFileSystem.class.getName());\n+    fsTarget = new LocalFileSystem();\n+    fsTarget.initialize(new URI(\"file:///\"), conf);\n+    fileSystemTestHelper = new FileSystemTestHelper();\n+    // create the test root on local_fs\n+    targetTestRoot = fileSystemTestHelper.getAbsoluteTestRootPath(fsTarget);\n+    fsTarget.delete(targetTestRoot, true);\n+    fsTarget.mkdirs(targetTestRoot);\n+  }\n+\n+  /**\n+   * Tests write file and read file with ViewFSOverloadScheme.\n+   */\n+  @Test\n+  public void testLocalTargetLinkWriteSimple() throws IOException {\n+    LOG.info(\"Starting testLocalTargetLinkWriteSimple\");\n+    final String testString = \"Hello Local!...\";\n+    final Path lfsRoot = new Path(\"/lfsRoot\");\n+    ConfigUtil.addLink(conf, lfsRoot.toString(),\n+        URI.create(targetTestRoot + \"/local\"));\n+    final FileSystem lViewFs = FileSystem.get(URI.create(\"file:///\"), conf);\n+\n+    final Path testPath = new Path(lfsRoot, \"test.txt\");\n+    final FSDataOutputStream fsDos = lViewFs.create(testPath);\n+    try {\n+      fsDos.writeUTF(testString);\n+    } finally {\n+      fsDos.close();\n+    }\n+\n+    FSDataInputStream lViewIs = lViewFs.open(testPath);\n+    try {\n+      Assert.assertEquals(testString, lViewIs.readUTF());\n+    } finally {\n+      lViewIs.close();\n+    }\n+  }\n+\n+  /**\n+   * Tests create file and delete file with ViewFSOverloadScheme.\n+   */\n+  @Test\n+  public void testLocalFsCreateAndDelete() throws Exception {\n+    LOG.info(\"Starting testLocalFsCreateAndDelete\");\n+    ConfigUtil.addLink(conf, \"mt\", \"/lfsroot\",\n+        URI.create(targetTestRoot + \"/wd2\"));\n+    final URI mountURI = URI.create(\"file://mt/\");\n+    final FileSystem lViewFS = FileSystem.get(mountURI, conf);\n+    try {\n+      Path testPath = new Path(mountURI.toString() + \"/lfsroot/test\");\n+      lViewFS.create(testPath);\n+      Assert.assertTrue(lViewFS.exists(testPath));\n+      lViewFS.delete(testPath, true);\n+      Assert.assertFalse(lViewFS.exists(testPath));\n+    } finally {\n+      lViewFS.close();\n+    }\n+  }\n+\n+  /**\n+   * Tests root level file with linkMergeSlash with ViewFSOverloadScheme.\n+   */\n+  @Test\n+  public void testLocalFsLinkSlashMerge() throws Exception {\n+    LOG.info(\"Starting testLocalFsLinkSlashMerge\");\n+    ConfigUtil.addLinkMergeSlash(conf, \"mt\",\n+        URI.create(targetTestRoot + \"/wd2\"));\n+    final URI mountURI = URI.create(\"file://mt/\");\n+    final FileSystem lViewFS = FileSystem.get(mountURI, conf);\n+    try {\n+      Path fileOnRoot = new Path(mountURI.toString() + \"/NewFile\");\n+      lViewFS.create(fileOnRoot);\n+      Assert.assertTrue(lViewFS.exists(fileOnRoot));\n+    } finally {\n+      lViewFS.close();\n+    }\n+  }\n+\n+  /**\n+   * Tests with linkMergeSlash and other mounts in ViewFSOverloadScheme.\n+   */\n+  @Test(expected = IOException.class)\n+  public void testLocalFsLinkSlashMergeWithOtherMountLinks() throws Exception {\n+    LOG.info(\"Starting testLocalFsLinkSlashMergeWithOtherMountLinks\");\n+    ConfigUtil.addLink(conf, \"mt\", \"/lfsroot\",\n+        URI.create(targetTestRoot + \"/wd2\"));\n+    ConfigUtil.addLinkMergeSlash(conf, \"mt\",\n+        URI.create(targetTestRoot + \"/wd2\"));\n+    final URI mountURI = URI.create(\"file://mt/\");\n+    FileSystem.get(mountURI, conf);\n+    Assert.fail(\"A merge slash cannot be configured with other mount links.\");\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    fsTarget.delete(fileSystemTestHelper.getTestRootPath(fsTarget), true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c1525f808f02ed40ca7451d4d71b7fa12708e106"}, "originalPosition": 156}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0MzY1OQ==", "bodyText": "cast not required.", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419043659", "createdAt": "2020-05-03T04:06:11Z", "author": {"login": "virajith"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeHdfsFileSystemContract.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import static org.junit.Assume.assumeTrue;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.CommonConfigurationKeys;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemContractBaseTest;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.AppendTestUtil;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.HdfsConfiguration;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.TestHDFSFileSystemContract;\n+import org.apache.hadoop.security.AccessControlException;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Ignore;\n+import org.junit.Test;\n+\n+/**\n+ * Tests ViewFsOverloadScheme with file system contract tests.\n+ */\n+public class TestViewFsOverloadSchemeHdfsFileSystemContract\n+    extends TestHDFSFileSystemContract {\n+\n+  private MiniDFSCluster cluster;\n+  private String defaultWorkingDirectory;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    final Configuration conf = new HdfsConfiguration();\n+    conf.set(CommonConfigurationKeys.FS_PERMISSIONS_UMASK_KEY,\n+        FileSystemContractBaseTest.TEST_UMASK);\n+    final File basedir = GenericTestUtils.getRandomizedTestDir();\n+    cluster = new MiniDFSCluster.Builder(conf, basedir)\n+        .numDataNodes(2)\n+        .build();\n+    defaultWorkingDirectory =\n+        \"/user/\" + UserGroupInformation.getCurrentUser().getShortUserName();\n+    conf.set(String.format(\"fs.%s.impl\", \"hdfs\"),\n+        ViewFsOverloadScheme.class.getName());\n+    conf.set(String.format(\n+        FsConstants.FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN,\n+        \"hdfs\"),\n+        DistributedFileSystem.class.getName());\n+    URI defaultFSURI =\n+        URI.create(conf.get(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY));\n+    ConfigUtil.addLink(conf, defaultFSURI.getAuthority(), \"/user\",\n+        defaultFSURI);\n+    ConfigUtil.addLink(conf, defaultFSURI.getAuthority(), \"/append\",\n+        defaultFSURI);\n+    ConfigUtil.addLink(conf, defaultFSURI.getAuthority(),\n+        \"/FileSystemContractBaseTest/\",\n+        new URI(defaultFSURI.toString() + \"/FileSystemContractBaseTest/\"));\n+    fs = (ViewFsOverloadScheme) FileSystem.get(conf);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c1525f808f02ed40ca7451d4d71b7fa12708e106"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0MzkwOA==", "bodyText": "Except for these two, can the rest be done @BeforeClass?", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419043908", "createdAt": "2020-05-03T04:09:01Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeLocalFileSystem.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+/**\n+ *\n+ * Test the TestViewFsOverloadSchemeLocalFS using a file with authority:\n+ * file://mountTableName/ i.e, the authority is used to load a mount table.\n+ */\n+public class TestViewFsOverloadSchemeLocalFileSystem {\n+  private static final String FILE = \"file\";\n+  private static final Log LOG =\n+      LogFactory.getLog(TestViewFsOverloadSchemeLocalFileSystem.class);\n+  private FileSystem fsTarget;\n+  private Configuration conf;\n+  private Path targetTestRoot;\n+  private FileSystemTestHelper fileSystemTestHelper;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    conf.set(String.format(\"fs.%s.impl\",\n+        FILE),\n+        ViewFsOverloadScheme.class.getName());\n+    conf.set(String.format(\n+        FsConstants.FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN,\n+        FILE),\n+        LocalFileSystem.class.getName());\n+    fsTarget = new LocalFileSystem();\n+    fsTarget.initialize(new URI(\"file:///\"), conf);\n+    fileSystemTestHelper = new FileSystemTestHelper();\n+    // create the test root on local_fs\n+    targetTestRoot = fileSystemTestHelper.getAbsoluteTestRootPath(fsTarget);\n+    fsTarget.delete(targetTestRoot, true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c1525f808f02ed40ca7451d4d71b7fa12708e106"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0NDAzNw==", "bodyText": "do we need the cluster created for every test? seems heavy :|\nAlso, can we set FS_DEFAULT_NAME_KEY explicitly here?", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419044037", "createdAt": "2020-05-03T04:11:05Z", "author": {"login": "virajith"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeHdfsFileSystemContract.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import static org.junit.Assume.assumeTrue;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.CommonConfigurationKeys;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemContractBaseTest;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.AppendTestUtil;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.HdfsConfiguration;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.TestHDFSFileSystemContract;\n+import org.apache.hadoop.security.AccessControlException;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Ignore;\n+import org.junit.Test;\n+\n+/**\n+ * Tests ViewFsOverloadScheme with file system contract tests.\n+ */\n+public class TestViewFsOverloadSchemeHdfsFileSystemContract\n+    extends TestHDFSFileSystemContract {\n+\n+  private MiniDFSCluster cluster;\n+  private String defaultWorkingDirectory;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    final Configuration conf = new HdfsConfiguration();\n+    conf.set(CommonConfigurationKeys.FS_PERMISSIONS_UMASK_KEY,\n+        FileSystemContractBaseTest.TEST_UMASK);\n+    final File basedir = GenericTestUtils.getRandomizedTestDir();\n+    cluster = new MiniDFSCluster.Builder(conf, basedir)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c1525f808f02ed40ca7451d4d71b7fa12708e106"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0NDI4Mw==", "bodyText": "I think ViewOverloadSchemeFilesystem is a better name. Also do we need an implementation extending ViewFs class (which extends AbstractFileSystem)?", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419044283", "createdAt": "2020-05-03T04:14:15Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "diffHunk": "@@ -0,0 +1,187 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n+import java.net.URI;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+\n+/******************************************************************************\n+ * This class is extended from the ViewFileSystem for the overloaded scheme file\n+ * system. The objective here is to handle multiple mounted file systems\n+ * transparently. Mount link configurations and in-memory mount table\n+ * building behaviors are inherited from ViewFileSystem. Unlike ViewFileSystem\n+ * scheme (viewfs://), the users would be able to use any scheme.\n+ *\n+ * Example 1:\n+ * If users want some of their existing cluster (hdfs://Cluster)\n+ * data to mount with other hdfs and object store clusters(hdfs://NN1,\n+ * o3fs://bucket1.volume1/, s3a://bucket1/)\n+ *\n+ * fs.viewfs.mounttable.Cluster./user = hdfs://NN1/user\n+ * fs.viewfs.mounttable.Cluster./data = o3fs://bucket1.volume1/data\n+ * fs.viewfs.mounttable.Cluster./backup = s3a://bucket1/backup/\n+ *\n+ * Op1: Create file hdfs://Cluster/user/fileA will go to hdfs://NN1/user/fileA\n+ * Op2: Create file hdfs://Cluster/data/datafile will go to\n+ *      o3fs://bucket1.volume1/data/datafile\n+ * Op3: Create file hdfs://Cluster/backup/data.zip will go to\n+ *      s3a://bucket1/backup/data.zip\n+ *\n+ * Example 2:\n+ * If users want some of their existing cluster (s3a://bucketA/)\n+ * data to mount with other hdfs and object store clusters\n+ * (hdfs://NN1, o3fs://bucket1.volume1/)\n+ *\n+ * fs.viewfs.mounttable.bucketA./user = hdfs://NN1/user\n+ * fs.viewfs.mounttable.bucketA./data = o3fs://bucket1.volume1/data\n+ * fs.viewfs.mounttable.bucketA./salesDB = s3a://bucketA/salesDB/\n+ *\n+ * Op1: Create file s3a://bucketA/user/fileA will go to hdfs://NN1/user/fileA\n+ * Op2: Create file s3a://bucketA/data/datafile will go to\n+ *      o3fs://bucket1.volume1/data/datafile\n+ * Op3: Create file s3a://bucketA/salesDB/dbfile will go to\n+ *      s3a://bucketA/salesDB/dbfile\n+ *****************************************************************************/\n+@InterfaceAudience.LimitedPrivate({ \"MapReduce\", \"HBase\", \"Hive\" })\n+@InterfaceStability.Evolving\n+public class ViewFsOverloadScheme extends ViewFileSystem {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c1525f808f02ed40ca7451d4d71b7fa12708e106"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0NTA5Mw==", "bodyText": "calling createLinks isn't even needed here right?", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419045093", "createdAt": "2020-05-03T04:24:39Z", "author": {"login": "virajith"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeWithHdfsScheme.java", "diffHunk": "@@ -0,0 +1,351 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.CommonConfigurationKeys;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.RawLocalFileSystem;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.security.AccessControlException;\n+import org.apache.hadoop.test.PathUtils;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+/**\n+ * Tests ViewFsOverloadScheme with configured mount links.\n+ */\n+public class TestViewFsOverloadSchemeWithHdfsScheme {\n+  private static final String FS_IMPL_PATTERN_KEY = \"fs.%s.impl\";\n+  private static final String HDFS_SCHEME = \"hdfs\";\n+  private Configuration conf = null;\n+  private MiniDFSCluster cluster = null;\n+  private URI defaultFSURI;\n+  private File localTargetDir;\n+  private static final String TEST_ROOT_DIR =\n+      PathUtils.getTestDirName(TestViewFsOverloadSchemeWithHdfsScheme.class);\n+  private static final String HDFS_USER_FOLDER = \"/HDFSUser\";\n+  private static final String LOCAL_FOLDER = \"/local\";\n+\n+  @Before\n+  public void startCluster() throws IOException {\n+    conf = new Configuration();\n+    conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_DELEGATION_TOKEN_ALWAYS_USE_KEY,\n+        true);\n+    conf.set(String.format(FS_IMPL_PATTERN_KEY, HDFS_SCHEME),\n+        ViewFsOverloadScheme.class.getName());\n+    conf.set(String.format(\n+        FsConstants.FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN,\n+        HDFS_SCHEME), DistributedFileSystem.class.getName());\n+    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(2).build();\n+    cluster.waitClusterUp();\n+    defaultFSURI =\n+        URI.create(conf.get(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY));\n+    localTargetDir = new File(TEST_ROOT_DIR, \"/root/\");\n+    Assert.assertEquals(HDFS_SCHEME, defaultFSURI.getScheme()); // hdfs scheme.\n+  }\n+\n+  @After\n+  public void tearDown() throws IOException {\n+    if (cluster != null) {\n+      FileSystem.closeAll();\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  private void createLinks(boolean needFalbackLink, Path hdfsTargetPath,\n+      Path localTragetPath) {\n+    ConfigUtil.addLink(conf, defaultFSURI.getAuthority(), HDFS_USER_FOLDER,\n+        hdfsTargetPath.toUri());\n+    ConfigUtil.addLink(conf, defaultFSURI.getAuthority(), LOCAL_FOLDER,\n+        localTragetPath.toUri());\n+    if (needFalbackLink) {\n+      ConfigUtil.addLinkFallback(conf, defaultFSURI.getAuthority(),\n+          hdfsTargetPath.toUri());\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows.\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   *\n+   * create file /HDFSUser/testfile should create in hdfs\n+   * create file /local/test should create directory in local fs\n+   */\n+  @Test(timeout = 30000)\n+  public void testMountLinkWithLocalAndHDFS() throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(false, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    Assert.assertEquals(2, fs.getMountPoints().length);\n+\n+    // /HDFSUser/testfile\n+    Path hdfsFile = new Path(HDFS_USER_FOLDER + \"/testfile\");\n+    // /local/test\n+    Path localDir = new Path(LOCAL_FOLDER + \"/test\");\n+\n+    fs.create(hdfsFile); // /HDFSUser/testfile\n+    fs.mkdirs(localDir); // /local/test\n+\n+    // Initialize HDFS and test files exist in ls or not\n+    DistributedFileSystem dfs = new DistributedFileSystem();\n+    dfs.initialize(defaultFSURI, conf);\n+    try {\n+      Assert.assertTrue(dfs.exists(\n+          new Path(Path.getPathWithoutSchemeAndAuthority(hdfsTargetPath),\n+              hdfsFile.getName()))); // should be in hdfs.\n+      Assert.assertFalse(dfs.exists(\n+          new Path(Path.getPathWithoutSchemeAndAuthority(localTragetPath),\n+              localDir.getName()))); // should not be in local fs.\n+    } finally {\n+      dfs.close();\n+    }\n+\n+    RawLocalFileSystem lfs = new RawLocalFileSystem();\n+    lfs.initialize(localTragetPath.toUri(), conf);\n+    try {\n+      Assert.assertFalse(lfs.exists(\n+          new Path(Path.getPathWithoutSchemeAndAuthority(hdfsTargetPath),\n+              hdfsFile.getName()))); // should not be in hdfs.\n+      Assert.assertTrue(lfs.exists(\n+          new Path(Path.getPathWithoutSchemeAndAuthority(localTragetPath),\n+              localDir.getName()))); // should be in local fs.\n+    } finally {\n+      lfs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows.\n+   * hdfs://localhost:xxx/HDFSUser --> nonexistent://NonExistent/User/\n+   * It should fail to add non existent fs link.\n+   */\n+  @Test(expected = IOException.class, timeout = 30000)\n+  public void testMountLinkWithNonExistentLink() throws Exception {\n+    final String userFolder = \"/User\";\n+    final Path nonExistTargetPath =\n+        new Path(\"nonexistent://NonExistent\" + userFolder);\n+\n+    /**\n+     * Below addLink will create following mount points\n+     * hdfs://localhost:xxx/User --> nonexistent://NonExistent/User/\n+     */\n+    ConfigUtil.addLink(conf, defaultFSURI.getAuthority(), userFolder,\n+        nonExistTargetPath.toUri());\n+    FileSystem.get(conf);\n+    Assert.fail(\"Expected to fail with non existent link\");\n+  }\n+\n+  /**\n+   * Create mount links as follows.\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   * ListStatus on / should list the mount links.\n+   */\n+  @Test(timeout = 30000)\n+  public void testListStatusOnRootShouldListAllMountLinks() throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(false, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      FileStatus[] ls = fs.listStatus(new Path(\"/\"));\n+      Assert.assertEquals(2, ls.length);\n+      String lsPath1 =\n+          Path.getPathWithoutSchemeAndAuthority(ls[0].getPath()).toString();\n+      String lsPath2 =\n+          Path.getPathWithoutSchemeAndAuthority(ls[1].getPath()).toString();\n+      Assert.assertTrue(\n+          HDFS_USER_FOLDER.equals(lsPath1) || LOCAL_FOLDER.equals(lsPath1));\n+      Assert.assertTrue(\n+          HDFS_USER_FOLDER.equals(lsPath2) || LOCAL_FOLDER.equals(lsPath2));\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   * ListStatus non mount directory should fail.\n+   */\n+  @Test(expected = IOException.class, timeout = 30000)\n+  public void testListStatusOnNonMountedPath() throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(false, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      fs.listStatus(new Path(\"/nonMount\"));\n+      Assert.fail(\"It should fail as no mount link with /nonMount\");\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows hdfs://localhost:xxx/HDFSUser -->\n+   * hdfs://localhost:xxx/HDFSUser/ hdfs://localhost:xxx/local -->\n+   * file://TEST_ROOT_DIR/root/ fallback --> hdfs://localhost:xxx/HDFSUser/\n+   * Creating file or directory at non root level should succeed with fallback\n+   * links.\n+   */\n+  @Test(timeout = 30000)\n+  public void testWithLinkFallBack() throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(true, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      fs.create(new Path(\"/nonMount/myfile\"));\n+      FileStatus[] ls = fs.listStatus(new Path(\"/nonMount\"));\n+      Assert.assertEquals(1, ls.length);\n+      Assert.assertEquals(\n+          Path.getPathWithoutSchemeAndAuthority(ls[0].getPath()).getName(),\n+          \"myfile\");\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   *\n+   * It can not find any mount link. ViewFS expects a mount point from root.\n+   */\n+  @Test(expected = NotInMountpointException.class, timeout = 30000)\n+  public void testCreateOnRootShouldFailWhenMountLinkConfigured()\n+      throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(false, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      fs.create(new Path(\"/newFileOnRoot\"));\n+      Assert.fail(\"It should fail as root is read only in viewFS.\");\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   * fallback --> hdfs://localhost:xxx/HDFSUser/\n+   *\n+   * It will find fallback link, but root is not accessible and read only.\n+   */\n+  @Test(expected = AccessControlException.class, timeout = 30000)\n+  public void testCreateOnRootShouldFailEvenFallBackMountLinkConfigured()\n+      throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(true, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      fs.create(new Path(\"/onRootWhenFallBack\"));\n+      Assert.fail(\n+          \"It should fail as root is read only in viewFS, even when configured\"\n+              + \" with fallback.\");\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   * fallback --> hdfs://localhost:xxx/HDFSUser/\n+   *\n+   * It will find fallback link, but root is not accessible and read only.\n+   */\n+  @Test(expected = UnsupportedFileSystemException.class, timeout = 30000)\n+  public void testInvalidOverloadSchemeTargetFS() throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+    conf = new Configuration();\n+    createLinks(true, hdfsTargetPath, localTragetPath);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c1525f808f02ed40ca7451d4d71b7fa12708e106"}, "originalPosition": 313}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0NTEyNQ==", "bodyText": "Can we add a test that cache actually works?", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419045125", "createdAt": "2020-05-03T04:25:11Z", "author": {"login": "virajith"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeWithHdfsScheme.java", "diffHunk": "@@ -0,0 +1,351 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.CommonConfigurationKeys;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.RawLocalFileSystem;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.security.AccessControlException;\n+import org.apache.hadoop.test.PathUtils;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+/**\n+ * Tests ViewFsOverloadScheme with configured mount links.\n+ */\n+public class TestViewFsOverloadSchemeWithHdfsScheme {\n+  private static final String FS_IMPL_PATTERN_KEY = \"fs.%s.impl\";\n+  private static final String HDFS_SCHEME = \"hdfs\";\n+  private Configuration conf = null;\n+  private MiniDFSCluster cluster = null;\n+  private URI defaultFSURI;\n+  private File localTargetDir;\n+  private static final String TEST_ROOT_DIR =\n+      PathUtils.getTestDirName(TestViewFsOverloadSchemeWithHdfsScheme.class);\n+  private static final String HDFS_USER_FOLDER = \"/HDFSUser\";\n+  private static final String LOCAL_FOLDER = \"/local\";\n+\n+  @Before\n+  public void startCluster() throws IOException {\n+    conf = new Configuration();\n+    conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_DELEGATION_TOKEN_ALWAYS_USE_KEY,\n+        true);\n+    conf.set(String.format(FS_IMPL_PATTERN_KEY, HDFS_SCHEME),\n+        ViewFsOverloadScheme.class.getName());\n+    conf.set(String.format(\n+        FsConstants.FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN,\n+        HDFS_SCHEME), DistributedFileSystem.class.getName());\n+    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(2).build();\n+    cluster.waitClusterUp();\n+    defaultFSURI =\n+        URI.create(conf.get(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY));\n+    localTargetDir = new File(TEST_ROOT_DIR, \"/root/\");\n+    Assert.assertEquals(HDFS_SCHEME, defaultFSURI.getScheme()); // hdfs scheme.\n+  }\n+\n+  @After\n+  public void tearDown() throws IOException {\n+    if (cluster != null) {\n+      FileSystem.closeAll();\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  private void createLinks(boolean needFalbackLink, Path hdfsTargetPath,\n+      Path localTragetPath) {\n+    ConfigUtil.addLink(conf, defaultFSURI.getAuthority(), HDFS_USER_FOLDER,\n+        hdfsTargetPath.toUri());\n+    ConfigUtil.addLink(conf, defaultFSURI.getAuthority(), LOCAL_FOLDER,\n+        localTragetPath.toUri());\n+    if (needFalbackLink) {\n+      ConfigUtil.addLinkFallback(conf, defaultFSURI.getAuthority(),\n+          hdfsTargetPath.toUri());\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows.\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   *\n+   * create file /HDFSUser/testfile should create in hdfs\n+   * create file /local/test should create directory in local fs\n+   */\n+  @Test(timeout = 30000)\n+  public void testMountLinkWithLocalAndHDFS() throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(false, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    Assert.assertEquals(2, fs.getMountPoints().length);\n+\n+    // /HDFSUser/testfile\n+    Path hdfsFile = new Path(HDFS_USER_FOLDER + \"/testfile\");\n+    // /local/test\n+    Path localDir = new Path(LOCAL_FOLDER + \"/test\");\n+\n+    fs.create(hdfsFile); // /HDFSUser/testfile\n+    fs.mkdirs(localDir); // /local/test\n+\n+    // Initialize HDFS and test files exist in ls or not\n+    DistributedFileSystem dfs = new DistributedFileSystem();\n+    dfs.initialize(defaultFSURI, conf);\n+    try {\n+      Assert.assertTrue(dfs.exists(\n+          new Path(Path.getPathWithoutSchemeAndAuthority(hdfsTargetPath),\n+              hdfsFile.getName()))); // should be in hdfs.\n+      Assert.assertFalse(dfs.exists(\n+          new Path(Path.getPathWithoutSchemeAndAuthority(localTragetPath),\n+              localDir.getName()))); // should not be in local fs.\n+    } finally {\n+      dfs.close();\n+    }\n+\n+    RawLocalFileSystem lfs = new RawLocalFileSystem();\n+    lfs.initialize(localTragetPath.toUri(), conf);\n+    try {\n+      Assert.assertFalse(lfs.exists(\n+          new Path(Path.getPathWithoutSchemeAndAuthority(hdfsTargetPath),\n+              hdfsFile.getName()))); // should not be in hdfs.\n+      Assert.assertTrue(lfs.exists(\n+          new Path(Path.getPathWithoutSchemeAndAuthority(localTragetPath),\n+              localDir.getName()))); // should be in local fs.\n+    } finally {\n+      lfs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows.\n+   * hdfs://localhost:xxx/HDFSUser --> nonexistent://NonExistent/User/\n+   * It should fail to add non existent fs link.\n+   */\n+  @Test(expected = IOException.class, timeout = 30000)\n+  public void testMountLinkWithNonExistentLink() throws Exception {\n+    final String userFolder = \"/User\";\n+    final Path nonExistTargetPath =\n+        new Path(\"nonexistent://NonExistent\" + userFolder);\n+\n+    /**\n+     * Below addLink will create following mount points\n+     * hdfs://localhost:xxx/User --> nonexistent://NonExistent/User/\n+     */\n+    ConfigUtil.addLink(conf, defaultFSURI.getAuthority(), userFolder,\n+        nonExistTargetPath.toUri());\n+    FileSystem.get(conf);\n+    Assert.fail(\"Expected to fail with non existent link\");\n+  }\n+\n+  /**\n+   * Create mount links as follows.\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   * ListStatus on / should list the mount links.\n+   */\n+  @Test(timeout = 30000)\n+  public void testListStatusOnRootShouldListAllMountLinks() throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(false, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      FileStatus[] ls = fs.listStatus(new Path(\"/\"));\n+      Assert.assertEquals(2, ls.length);\n+      String lsPath1 =\n+          Path.getPathWithoutSchemeAndAuthority(ls[0].getPath()).toString();\n+      String lsPath2 =\n+          Path.getPathWithoutSchemeAndAuthority(ls[1].getPath()).toString();\n+      Assert.assertTrue(\n+          HDFS_USER_FOLDER.equals(lsPath1) || LOCAL_FOLDER.equals(lsPath1));\n+      Assert.assertTrue(\n+          HDFS_USER_FOLDER.equals(lsPath2) || LOCAL_FOLDER.equals(lsPath2));\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   * ListStatus non mount directory should fail.\n+   */\n+  @Test(expected = IOException.class, timeout = 30000)\n+  public void testListStatusOnNonMountedPath() throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(false, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      fs.listStatus(new Path(\"/nonMount\"));\n+      Assert.fail(\"It should fail as no mount link with /nonMount\");\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows hdfs://localhost:xxx/HDFSUser -->\n+   * hdfs://localhost:xxx/HDFSUser/ hdfs://localhost:xxx/local -->\n+   * file://TEST_ROOT_DIR/root/ fallback --> hdfs://localhost:xxx/HDFSUser/\n+   * Creating file or directory at non root level should succeed with fallback\n+   * links.\n+   */\n+  @Test(timeout = 30000)\n+  public void testWithLinkFallBack() throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(true, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      fs.create(new Path(\"/nonMount/myfile\"));\n+      FileStatus[] ls = fs.listStatus(new Path(\"/nonMount\"));\n+      Assert.assertEquals(1, ls.length);\n+      Assert.assertEquals(\n+          Path.getPathWithoutSchemeAndAuthority(ls[0].getPath()).getName(),\n+          \"myfile\");\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   *\n+   * It can not find any mount link. ViewFS expects a mount point from root.\n+   */\n+  @Test(expected = NotInMountpointException.class, timeout = 30000)\n+  public void testCreateOnRootShouldFailWhenMountLinkConfigured()\n+      throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(false, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      fs.create(new Path(\"/newFileOnRoot\"));\n+      Assert.fail(\"It should fail as root is read only in viewFS.\");\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   * fallback --> hdfs://localhost:xxx/HDFSUser/\n+   *\n+   * It will find fallback link, but root is not accessible and read only.\n+   */\n+  @Test(expected = AccessControlException.class, timeout = 30000)\n+  public void testCreateOnRootShouldFailEvenFallBackMountLinkConfigured()\n+      throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(true, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      fs.create(new Path(\"/onRootWhenFallBack\"));\n+      Assert.fail(\n+          \"It should fail as root is read only in viewFS, even when configured\"\n+              + \" with fallback.\");\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   * fallback --> hdfs://localhost:xxx/HDFSUser/\n+   *\n+   * It will find fallback link, but root is not accessible and read only.\n+   */\n+  @Test(expected = UnsupportedFileSystemException.class, timeout = 30000)\n+  public void testInvalidOverloadSchemeTargetFS() throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+    conf = new Configuration();\n+    createLinks(true, hdfsTargetPath, localTragetPath);\n+    conf.set(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY,\n+        defaultFSURI.toString());\n+    conf.set(String.format(FS_IMPL_PATTERN_KEY, HDFS_SCHEME),\n+        ViewFsOverloadScheme.class.getName());\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      fs.create(new Path(\"/onRootWhenFallBack\"));\n+      Assert.fail(\"OverloadScheme target fs should be valid.\");\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   *\n+   * It should be able to create file using ViewFsOverloadScheme.\n+   */\n+  @Test(timeout = 30000)\n+  public void testViewFsOverloadSchemeWhenInnerCacheDisabled()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c1525f808f02ed40ca7451d4d71b7fa12708e106"}, "originalPosition": 336}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c3129e386369381d61694ad685ef6ade775e5f46", "author": {"user": {"login": "umamaheswararao", "name": "Uma Maheswara Rao G"}}, "url": "https://github.com/apache/hadoop/commit/c3129e386369381d61694ad685ef6ade775e5f46", "committedDate": "2020-05-03T10:44:35Z", "message": "HDFS-15305. Update: Fixed few review comments."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bb8c3c3ad79e2aa2dfc943e30deb2b81c1c165fb", "author": {"user": {"login": "umamaheswararao", "name": "Uma Maheswara Rao G"}}, "url": "https://github.com/apache/hadoop/commit/bb8c3c3ad79e2aa2dfc943e30deb2b81c1c165fb", "committedDate": "2020-05-03T20:48:11Z", "message": "HDFS-15305. Update: Fixed checkstyle comments."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2513bc5ca25e3e754bf0b2d31a2780f1332c4fe5", "author": {"user": {"login": "umamaheswararao", "name": "Uma Maheswara Rao G"}}, "url": "https://github.com/apache/hadoop/commit/2513bc5ca25e3e754bf0b2d31a2780f1332c4fe5", "committedDate": "2020-05-04T19:34:42Z", "message": "HDFS-15305. Improvement: name FS->FileSystem."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA1NDI5NjY2", "url": "https://github.com/apache/hadoop/pull/1988#pullrequestreview-405429666", "createdAt": "2020-05-05T00:14:17Z", "commit": {"oid": "2513bc5ca25e3e754bf0b2d31a2780f1332c4fe5"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4244, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}