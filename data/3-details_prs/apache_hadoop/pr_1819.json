{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzY3MzI2NDk3", "number": 1819, "title": "HDFS-14989. Add a 'swapBlockList' operation to Namenode.", "bodyText": "This is the first patch HDFS-14989 for the In Place Erasure Coding Feature HDFS-14978.\nWork Done\nThe swapBlockList takes two parameters, a source file and a destination file. This operation swaps the blocks belonging to the source and the destination atomically.\nThe namespace metadata of interest is the INodeFile class. A file (INodeFile) contains a header composed of PREFERRED_BLOCK_SIZE, BLOCK_LAYOUT_AND_REDUNDANCY and STORAGE_POLICY_ID. In addition, an INodeFile contains a list of blocks (BlockInfo[]). The operation will swap BLOCK_LAYOUT_AND_REDUNDANCY header bits and the block lists. But it will not touch other fields. To avoid complication, this operation will abort if either file is open (isUnderConstruction() == true)\nNote : This patch is intentionally not exposing the client side API to use this operation. That will be handled in a separate JIRA where the client side API will involve using this operation to carry out EC conversion.\nAdded unit tests for the operation as well.", "createdAt": "2020-01-27T06:07:02Z", "url": "https://github.com/apache/hadoop/pull/1819", "merged": true, "mergeCommit": {"oid": "aad533d85b3dc8c48e07a92beeb215c783fa20ff"}, "closed": true, "closedAt": "2020-02-04T19:20:28Z", "author": {"login": "avijayanhwx"}, "timelineItems": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABb-WYQDAH2gAyMzY3MzI2NDk3OmM3MjhhNGRkMGFkOTRhM2MxZTY0YTdhNTFlZTk3YTQ3YWM0MmEzMTA=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcAD8_egFqTM1MTg4NDkxNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "c728a4dd0ad94a3c1e64a7a51ee97a47ac42a310", "author": {"user": {"login": "avijayanhwx", "name": null}}, "url": "https://github.com/apache/hadoop/commit/c728a4dd0ad94a3c1e64a7a51ee97a47ac42a310", "committedDate": "2020-01-27T06:02:38Z", "message": "HDFS-14989. Add a 'swapBlockList' operation to Namenode."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fb2fda8c97b24b88ab679ada6144a6a939e7aa61", "author": {"user": {"login": "avijayanhwx", "name": null}}, "url": "https://github.com/apache/hadoop/commit/fb2fda8c97b24b88ab679ada6144a6a939e7aa61", "committedDate": "2020-01-27T06:02:46Z", "message": "HDFS-14989. Add a 'swapBlockList' operation to Namenode. (Fix checkstyle issues)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ef331c8120bd898a0ab5a454effc72a867b4f6f8", "author": {"user": {"login": "avijayanhwx", "name": null}}, "url": "https://github.com/apache/hadoop/commit/ef331c8120bd898a0ab5a454effc72a867b4f6f8", "committedDate": "2020-01-27T06:02:55Z", "message": "HDFS-14989. Address review comments."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2ca150ea42fd75ebe11bffbcbac73c94fc1a61e1", "author": {"user": {"login": "avijayanhwx", "name": null}}, "url": "https://github.com/apache/hadoop/commit/2ca150ea42fd75ebe11bffbcbac73c94fc1a61e1", "committedDate": "2020-01-27T06:03:03Z", "message": "HDFS-14989. Swap storage policy ID, check destination file genStamp."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "eaee621cd681730b1e4cf1c37c261d5e7292e3fc", "author": {"user": {"login": "avijayanhwx", "name": null}}, "url": "https://github.com/apache/hadoop/commit/eaee621cd681730b1e4cf1c37c261d5e7292e3fc", "committedDate": "2020-01-27T06:03:10Z", "message": "HDFS-14989. Remove unused import."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ4NzcwMDM5", "url": "https://github.com/apache/hadoop/pull/1819#pullrequestreview-348770039", "createdAt": "2020-01-27T15:52:41Z", "commit": {"oid": "eaee621cd681730b1e4cf1c37c261d5e7292e3fc"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yN1QxNTo1Mjo0MVrOFiHswg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yN1QxNjo0NTo0MlrOFiJr2A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTMyMjA1MA==", "bodyText": "This isn't required, This would be redundant check, You are having the same check in FSNamesystem. Its required only twice, once before taking lock and once after...", "url": "https://github.com/apache/hadoop/pull/1819#discussion_r371322050", "createdAt": "2020-01-27T15:52:41Z", "author": {"login": "ayushtkn"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java", "diffHunk": "@@ -2667,4 +2667,15 @@ public Long getNextSPSPath() throws IOException {\n     }\n     return namesystem.getBlockManager().getSPSManager().getNextPathId();\n   }\n+\n+  public boolean swapBlockList(String src, String dst, long maxTimestamp)\n+      throws IOException {\n+    checkNNStartup();\n+    if (stateChangeLog.isDebugEnabled()) {\n+      stateChangeLog.debug(\"*DIR* NameNode.swapBlockList: {} and {}\", src, dst);\n+    }\n+    namesystem.checkOperation(OperationCategory.WRITE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eaee621cd681730b1e4cf1c37c261d5e7292e3fc"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTMyMzc1Nw==", "bodyText": "Can use FsDirectory.resolveLastINode()", "url": "https://github.com/apache/hadoop/pull/1819#discussion_r371323757", "createdAt": "2020-01-27T15:55:24Z", "author": {"login": "ayushtkn"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SwapBlockListOp.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdfs.server.namenode;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo;\n+import org.apache.hadoop.hdfs.server.namenode.FSDirectory.DirOp;\n+import org.apache.hadoop.hdfs.server.namenode.INodeFile.HeaderFormat;\n+import org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot;\n+import org.apache.hadoop.util.Time;\n+\n+/**\n+ * Class to carry out the operation of swapping blocks from one file to another.\n+ * Along with swapping blocks, we can also optionally swap the block layout\n+ * of a file header, which is useful for client operations like converting\n+ * replicated to EC file.\n+ */\n+public final class SwapBlockListOp {\n+\n+  private SwapBlockListOp() {\n+  }\n+\n+  static SwapBlockListResult swapBlocks(FSDirectory fsd, FSPermissionChecker pc,\n+                          String src, String dst, long genTimestamp)\n+      throws IOException {\n+\n+    final INodesInPath srcIIP = fsd.resolvePath(pc, src, DirOp.WRITE);\n+    final INodesInPath dstIIP = fsd.resolvePath(pc, dst, DirOp.WRITE);\n+    if (fsd.isPermissionEnabled()) {\n+      fsd.checkAncestorAccess(pc, srcIIP, FsAction.WRITE);\n+      fsd.checkAncestorAccess(pc, dstIIP, FsAction.WRITE);\n+    }\n+    if (NameNode.stateChangeLog.isDebugEnabled()) {\n+      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.swapBlockList: \"\n+          + srcIIP.getPath() + \" and \" + dstIIP.getPath());\n+    }\n+    SwapBlockListResult result = null;\n+    fsd.writeLock();\n+    try {\n+      result = swapBlockList(fsd, srcIIP, dstIIP, genTimestamp);\n+    } finally {\n+      fsd.writeUnlock();\n+    }\n+    return result;\n+  }\n+\n+  private static SwapBlockListResult swapBlockList(FSDirectory fsd,\n+                                    final INodesInPath srcIIP,\n+                                    final INodesInPath dstIIP,\n+                                    long genTimestamp)\n+      throws IOException {\n+\n+    assert fsd.hasWriteLock();\n+    validateInode(srcIIP);\n+    validateInode(dstIIP);\n+    fsd.ezManager.checkMoveValidity(srcIIP, dstIIP);\n+\n+    final String src = srcIIP.getPath();\n+    final String dst = dstIIP.getPath();\n+    if (dst.equals(src)) {\n+      throw new FileAlreadyExistsException(\"The source \" + src +\n+          \" and destination \" + dst + \" are the same\");\n+    }\n+\n+    INodeFile srcINodeFile = (INodeFile) srcIIP.getLastINode();\n+    INodeFile dstINodeFile = (INodeFile) dstIIP.getLastINode();\n+\n+    String errorPrefix = \"DIR* FSDirectory.swapBlockList: \";\n+    String error = \"Swap Block List destination file \";\n+    BlockInfo lastBlock = dstINodeFile.getLastBlock();\n+    if (lastBlock != null && lastBlock.getGenerationStamp() != genTimestamp) {\n+      error  += dstIIP.getPath() +\n+          \" has last block with different gen timestamp.\";\n+      NameNode.stateChangeLog.warn(errorPrefix + error);\n+      throw new IOException(error);\n+    }\n+\n+    long mtime = Time.now();\n+    BlockInfo[] dstINodeFileBlocks = dstINodeFile.getBlocks();\n+    dstINodeFile.replaceBlocks(srcINodeFile.getBlocks());\n+    srcINodeFile.replaceBlocks(dstINodeFileBlocks);\n+\n+    long srcHeader = srcINodeFile.getHeaderLong();\n+    long dstHeader = dstINodeFile.getHeaderLong();\n+\n+    byte dstBlockLayoutPolicy =\n+        HeaderFormat.getBlockLayoutPolicy(dstHeader);\n+    byte srcBlockLayoutPolicy =\n+        HeaderFormat.getBlockLayoutPolicy(srcHeader);\n+\n+    byte dstStoragePolicyID = HeaderFormat.getStoragePolicyID(dstHeader);\n+    byte srcStoragePolicyID = HeaderFormat.getStoragePolicyID(srcHeader);\n+\n+    dstINodeFile.updateHeaderWithNewPolicy(srcBlockLayoutPolicy,\n+        srcStoragePolicyID);\n+    dstINodeFile.setModificationTime(mtime);\n+\n+    srcINodeFile.updateHeaderWithNewPolicy(dstBlockLayoutPolicy,\n+        dstStoragePolicyID);\n+    srcINodeFile.setModificationTime(mtime);\n+\n+    return new SwapBlockListResult(true,\n+        fsd.getAuditFileInfo(srcIIP),\n+        fsd.getAuditFileInfo(dstIIP));\n+  }\n+\n+  private static void validateInode(INodesInPath srcIIP)\n+      throws IOException {\n+\n+    String errorPrefix = \"DIR* FSDirectory.swapBlockList: \";\n+    String error = \"Swap Block List input \";\n+    final INode srcInode = srcIIP.getLastINode();\n+\n+    // Check if INode is null.\n+    if (srcInode == null) {\n+      error  += srcIIP.getPath() + \" is not found.\";\n+      NameNode.stateChangeLog.warn(errorPrefix + error);\n+      throw new FileNotFoundException(error);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eaee621cd681730b1e4cf1c37c261d5e7292e3fc"}, "originalPosition": 140}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTMyNDM4NQ==", "bodyText": "nit. My IDE complains that the null assignment is redundant. No need to assign null, it is already null.", "url": "https://github.com/apache/hadoop/pull/1819#discussion_r371324385", "createdAt": "2020-01-27T15:56:20Z", "author": {"login": "ayushtkn"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SwapBlockListOp.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdfs.server.namenode;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo;\n+import org.apache.hadoop.hdfs.server.namenode.FSDirectory.DirOp;\n+import org.apache.hadoop.hdfs.server.namenode.INodeFile.HeaderFormat;\n+import org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot;\n+import org.apache.hadoop.util.Time;\n+\n+/**\n+ * Class to carry out the operation of swapping blocks from one file to another.\n+ * Along with swapping blocks, we can also optionally swap the block layout\n+ * of a file header, which is useful for client operations like converting\n+ * replicated to EC file.\n+ */\n+public final class SwapBlockListOp {\n+\n+  private SwapBlockListOp() {\n+  }\n+\n+  static SwapBlockListResult swapBlocks(FSDirectory fsd, FSPermissionChecker pc,\n+                          String src, String dst, long genTimestamp)\n+      throws IOException {\n+\n+    final INodesInPath srcIIP = fsd.resolvePath(pc, src, DirOp.WRITE);\n+    final INodesInPath dstIIP = fsd.resolvePath(pc, dst, DirOp.WRITE);\n+    if (fsd.isPermissionEnabled()) {\n+      fsd.checkAncestorAccess(pc, srcIIP, FsAction.WRITE);\n+      fsd.checkAncestorAccess(pc, dstIIP, FsAction.WRITE);\n+    }\n+    if (NameNode.stateChangeLog.isDebugEnabled()) {\n+      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.swapBlockList: \"\n+          + srcIIP.getPath() + \" and \" + dstIIP.getPath());\n+    }\n+    SwapBlockListResult result = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eaee621cd681730b1e4cf1c37c261d5e7292e3fc"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTMzMTY2MQ==", "bodyText": "Can use srcIIP.getLastINode().asFile(). Looks better. :)", "url": "https://github.com/apache/hadoop/pull/1819#discussion_r371331661", "createdAt": "2020-01-27T16:07:46Z", "author": {"login": "ayushtkn"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SwapBlockListOp.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdfs.server.namenode;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo;\n+import org.apache.hadoop.hdfs.server.namenode.FSDirectory.DirOp;\n+import org.apache.hadoop.hdfs.server.namenode.INodeFile.HeaderFormat;\n+import org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot;\n+import org.apache.hadoop.util.Time;\n+\n+/**\n+ * Class to carry out the operation of swapping blocks from one file to another.\n+ * Along with swapping blocks, we can also optionally swap the block layout\n+ * of a file header, which is useful for client operations like converting\n+ * replicated to EC file.\n+ */\n+public final class SwapBlockListOp {\n+\n+  private SwapBlockListOp() {\n+  }\n+\n+  static SwapBlockListResult swapBlocks(FSDirectory fsd, FSPermissionChecker pc,\n+                          String src, String dst, long genTimestamp)\n+      throws IOException {\n+\n+    final INodesInPath srcIIP = fsd.resolvePath(pc, src, DirOp.WRITE);\n+    final INodesInPath dstIIP = fsd.resolvePath(pc, dst, DirOp.WRITE);\n+    if (fsd.isPermissionEnabled()) {\n+      fsd.checkAncestorAccess(pc, srcIIP, FsAction.WRITE);\n+      fsd.checkAncestorAccess(pc, dstIIP, FsAction.WRITE);\n+    }\n+    if (NameNode.stateChangeLog.isDebugEnabled()) {\n+      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.swapBlockList: \"\n+          + srcIIP.getPath() + \" and \" + dstIIP.getPath());\n+    }\n+    SwapBlockListResult result = null;\n+    fsd.writeLock();\n+    try {\n+      result = swapBlockList(fsd, srcIIP, dstIIP, genTimestamp);\n+    } finally {\n+      fsd.writeUnlock();\n+    }\n+    return result;\n+  }\n+\n+  private static SwapBlockListResult swapBlockList(FSDirectory fsd,\n+                                    final INodesInPath srcIIP,\n+                                    final INodesInPath dstIIP,\n+                                    long genTimestamp)\n+      throws IOException {\n+\n+    assert fsd.hasWriteLock();\n+    validateInode(srcIIP);\n+    validateInode(dstIIP);\n+    fsd.ezManager.checkMoveValidity(srcIIP, dstIIP);\n+\n+    final String src = srcIIP.getPath();\n+    final String dst = dstIIP.getPath();\n+    if (dst.equals(src)) {\n+      throw new FileAlreadyExistsException(\"The source \" + src +\n+          \" and destination \" + dst + \" are the same\");\n+    }\n+\n+    INodeFile srcINodeFile = (INodeFile) srcIIP.getLastINode();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eaee621cd681730b1e4cf1c37c261d5e7292e3fc"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTMzNDIzNw==", "bodyText": "nit : Refactored dstInodeFIleBlocks and used only once, but didn't do for srcInodeFile. May be should keep same for both.", "url": "https://github.com/apache/hadoop/pull/1819#discussion_r371334237", "createdAt": "2020-01-27T16:11:51Z", "author": {"login": "ayushtkn"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SwapBlockListOp.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdfs.server.namenode;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo;\n+import org.apache.hadoop.hdfs.server.namenode.FSDirectory.DirOp;\n+import org.apache.hadoop.hdfs.server.namenode.INodeFile.HeaderFormat;\n+import org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot;\n+import org.apache.hadoop.util.Time;\n+\n+/**\n+ * Class to carry out the operation of swapping blocks from one file to another.\n+ * Along with swapping blocks, we can also optionally swap the block layout\n+ * of a file header, which is useful for client operations like converting\n+ * replicated to EC file.\n+ */\n+public final class SwapBlockListOp {\n+\n+  private SwapBlockListOp() {\n+  }\n+\n+  static SwapBlockListResult swapBlocks(FSDirectory fsd, FSPermissionChecker pc,\n+                          String src, String dst, long genTimestamp)\n+      throws IOException {\n+\n+    final INodesInPath srcIIP = fsd.resolvePath(pc, src, DirOp.WRITE);\n+    final INodesInPath dstIIP = fsd.resolvePath(pc, dst, DirOp.WRITE);\n+    if (fsd.isPermissionEnabled()) {\n+      fsd.checkAncestorAccess(pc, srcIIP, FsAction.WRITE);\n+      fsd.checkAncestorAccess(pc, dstIIP, FsAction.WRITE);\n+    }\n+    if (NameNode.stateChangeLog.isDebugEnabled()) {\n+      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.swapBlockList: \"\n+          + srcIIP.getPath() + \" and \" + dstIIP.getPath());\n+    }\n+    SwapBlockListResult result = null;\n+    fsd.writeLock();\n+    try {\n+      result = swapBlockList(fsd, srcIIP, dstIIP, genTimestamp);\n+    } finally {\n+      fsd.writeUnlock();\n+    }\n+    return result;\n+  }\n+\n+  private static SwapBlockListResult swapBlockList(FSDirectory fsd,\n+                                    final INodesInPath srcIIP,\n+                                    final INodesInPath dstIIP,\n+                                    long genTimestamp)\n+      throws IOException {\n+\n+    assert fsd.hasWriteLock();\n+    validateInode(srcIIP);\n+    validateInode(dstIIP);\n+    fsd.ezManager.checkMoveValidity(srcIIP, dstIIP);\n+\n+    final String src = srcIIP.getPath();\n+    final String dst = dstIIP.getPath();\n+    if (dst.equals(src)) {\n+      throw new FileAlreadyExistsException(\"The source \" + src +\n+          \" and destination \" + dst + \" are the same\");\n+    }\n+\n+    INodeFile srcINodeFile = (INodeFile) srcIIP.getLastINode();\n+    INodeFile dstINodeFile = (INodeFile) dstIIP.getLastINode();\n+\n+    String errorPrefix = \"DIR* FSDirectory.swapBlockList: \";\n+    String error = \"Swap Block List destination file \";\n+    BlockInfo lastBlock = dstINodeFile.getLastBlock();\n+    if (lastBlock != null && lastBlock.getGenerationStamp() != genTimestamp) {\n+      error  += dstIIP.getPath() +\n+          \" has last block with different gen timestamp.\";\n+      NameNode.stateChangeLog.warn(errorPrefix + error);\n+      throw new IOException(error);\n+    }\n+\n+    long mtime = Time.now();\n+    BlockInfo[] dstINodeFileBlocks = dstINodeFile.getBlocks();\n+    dstINodeFile.replaceBlocks(srcINodeFile.getBlocks());\n+    srcINodeFile.replaceBlocks(dstINodeFileBlocks);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eaee621cd681730b1e4cf1c37c261d5e7292e3fc"}, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTM1NDU4NA==", "bodyText": "For the validations can use LambdaTestUtils, Like this\n LambdaTestUtils.intercept(FileNotFoundException.class, \"/TestSwapBlockList/dir1/fileXYZ\", () -> fsn .swapBlockList(\"/TestSwapBlockList/dir1/fileXYZ\", \"/TestSwapBlockList/dir1/dir11/file3\", 0L));", "url": "https://github.com/apache/hadoop/pull/1819#discussion_r371354584", "createdAt": "2020-01-27T16:45:42Z", "author": {"login": "ayushtkn"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestSwapBlockList.java", "diffHunk": "@@ -0,0 +1,257 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdfs.server.namenode;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.DFSTestUtil;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo;\n+import org.apache.hadoop.hdfs.server.namenode.INodeFile.HeaderFormat;\n+import org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotTestHelper;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+/**\n+ * Test SwapBlockListOp working.\n+ */\n+public class TestSwapBlockList {\n+\n+  private static final short REPLICATION = 3;\n+\n+  private static final long SEED = 0;\n+  private final Path rootDir = new Path(\"/\" + getClass().getSimpleName());\n+\n+  private final Path subDir1 = new Path(rootDir, \"dir1\");\n+  private final Path file1 = new Path(subDir1, \"file1\");\n+  private final Path file2 = new Path(subDir1, \"file2\");\n+\n+  private final Path subDir11 = new Path(subDir1, \"dir11\");\n+  private final Path file3 = new Path(subDir11, \"file3\");\n+\n+  private final Path subDir2 = new Path(rootDir, \"dir2\");\n+  private final Path file4 = new Path(subDir2, \"file4\");\n+\n+  private Configuration conf;\n+  private MiniDFSCluster cluster;\n+  private FSNamesystem fsn;\n+  private FSDirectory fsdir;\n+\n+  private DistributedFileSystem hdfs;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    conf.setInt(DFSConfigKeys.DFS_NAMENODE_MAX_XATTRS_PER_INODE_KEY, 2);\n+    cluster = new MiniDFSCluster.Builder(conf)\n+        .numDataNodes(REPLICATION)\n+        .build();\n+    cluster.waitActive();\n+\n+    fsn = cluster.getNamesystem();\n+    fsdir = fsn.getFSDirectory();\n+\n+    hdfs = cluster.getFileSystem();\n+\n+    hdfs.mkdirs(subDir2);\n+\n+    DFSTestUtil.createFile(hdfs, file1, 1024, REPLICATION, SEED);\n+    DFSTestUtil.createFile(hdfs, file2, 1024, REPLICATION, SEED);\n+    DFSTestUtil.createFile(hdfs, file3, 1024, REPLICATION, SEED);\n+    DFSTestUtil.createFile(hdfs, file4, 1024, REPLICATION, SEED);\n+\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  @Test\n+  public void testInputValidation() throws Exception {\n+\n+    // Source file not found.\n+    try {\n+      fsn.swapBlockList(\"/TestSwapBlockList/dir1/fileXYZ\",\n+          \"/TestSwapBlockList/dir1/dir11/file3\", 0L);\n+      Assert.fail();\n+    } catch (IOException ioEx) {\n+      Assert.assertTrue(ioEx instanceof FileNotFoundException);\n+      Assert.assertTrue(\n+          ioEx.getMessage().contains(\"/TestSwapBlockList/dir1/fileXYZ\"));\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eaee621cd681730b1e4cf1c37c261d5e7292e3fc"}, "originalPosition": 109}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5a1e25dcbbc5f45d90d24a53b5daa6c6cdc028de", "author": {"user": {"login": "avijayanhwx", "name": null}}, "url": "https://github.com/apache/hadoop/commit/5a1e25dcbbc5f45d90d24a53b5daa6c6cdc028de", "committedDate": "2020-01-30T22:49:49Z", "message": "HDFS-14989. Address review comment."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUxMjY5NTQz", "url": "https://github.com/apache/hadoop/pull/1819#pullrequestreview-351269543", "createdAt": "2020-01-31T02:54:57Z", "commit": {"oid": "5a1e25dcbbc5f45d90d24a53b5daa6c6cdc028de"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQwMjo1NDo1N1rOFkANKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQwMjo1NDo1N1rOFkANKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzI5NjQyNw==", "bodyText": "Any reason for not assigning FSDirectory.resolveLastINode(srcIIP); to srcInode?", "url": "https://github.com/apache/hadoop/pull/1819#discussion_r373296427", "createdAt": "2020-01-31T02:54:57Z", "author": {"login": "ayushtkn"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SwapBlockListOp.java", "diffHunk": "@@ -130,15 +129,10 @@ private static void validateInode(INodesInPath srcIIP)\n \n     String errorPrefix = \"DIR* FSDirectory.swapBlockList: \";\n     String error = \"Swap Block List input \";\n-    final INode srcInode = srcIIP.getLastINode();\n \n-    // Check if INode is null.\n-    if (srcInode == null) {\n-      error  += srcIIP.getPath() + \" is not found.\";\n-      NameNode.stateChangeLog.warn(errorPrefix + error);\n-      throw new FileNotFoundException(error);\n-    }\n+    FSDirectory.resolveLastINode(srcIIP);\n \n+    final INode srcInode = srcIIP.getLastINode();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5a1e25dcbbc5f45d90d24a53b5daa6c6cdc028de"}, "originalPosition": 42}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b5885b3c7ff0b42dad955133405e4189ad1b3dd3", "author": {"user": {"login": "avijayanhwx", "name": null}}, "url": "https://github.com/apache/hadoop/commit/b5885b3c7ff0b42dad955133405e4189ad1b3dd3", "committedDate": "2020-01-31T21:05:15Z", "message": "HDFS-14989. Use FSDirectory.resolveLastINode."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUxODg0OTE2", "url": "https://github.com/apache/hadoop/pull/1819#pullrequestreview-351884916", "createdAt": "2020-02-01T13:42:25Z", "commit": {"oid": "b5885b3c7ff0b42dad955133405e4189ad1b3dd3"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4667, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}