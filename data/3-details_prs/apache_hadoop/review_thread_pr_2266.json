{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDc3MzU2OTYz", "number": 2266, "reviewThreads": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQyMjowNTo0NlrOEfGMWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQyMDowNzoyMVrOEh1zaw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAxMDQyNzc3OnYy", "diffSide": "LEFT", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterAdmin.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQyMjowNTo0NlrOHLJ43Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQyMjowNTo0NlrOHLJ43Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ1ODM5Nw==", "bodyText": "Avoid", "url": "https://github.com/apache/hadoop/pull/2266#discussion_r481458397", "createdAt": "2020-09-01T22:05:46Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterAdmin.java", "diffHunk": "@@ -128,7 +166,6 @@ public void testAddMountTable() throws IOException {\n     MountTable newEntry = MountTable.newInstance(\n         \"/testpath\", Collections.singletonMap(\"ns0\", \"/testdir\"),\n         Time.now(), Time.now());\n-", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4ddfb34bdaec13f609d421391108edb021638f26"}, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAxMDQyODg3OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterAdmin.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQyMjowNjoxMVrOHLJ5gg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQyMjowNjoxMVrOHLJ5gg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ1ODU2Mg==", "bodyText": "Add a comment highlighting what this part does.", "url": "https://github.com/apache/hadoop/pull/2266#discussion_r481458562", "createdAt": "2020-09-01T22:06:11Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterAdmin.java", "diffHunk": "@@ -103,11 +113,39 @@ public static void globalSetUp() throws Exception {\n         createNamenodeReport(\"ns1\", \"nn1\", HAServiceState.ACTIVE));\n     stateStore.refreshCaches(true);\n \n+    setUpMocks();\n+  }\n+\n+  private static void setUpMocks() throws IOException {\n     RouterRpcServer spyRpcServer =\n         Mockito.spy(routerContext.getRouter().createRpcServer());\n     Whitebox\n         .setInternalState(routerContext.getRouter(), \"rpcServer\", spyRpcServer);\n     Mockito.doReturn(null).when(spyRpcServer).getFileInfo(Mockito.anyString());\n+\n+    mockRpcClient = Mockito.spy(spyRpcServer.getRPCClient());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4ddfb34bdaec13f609d421391108edb021638f26"}, "originalPosition": 50}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAxMDQyOTMwOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterAdmin.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQyMjowNjoyMFrOHLJ5ww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQyMjowNjoyMFrOHLJ5ww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ1ODYyNw==", "bodyText": "Add javadoc", "url": "https://github.com/apache/hadoop/pull/2266#discussion_r481458627", "createdAt": "2020-09-01T22:06:20Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterAdmin.java", "diffHunk": "@@ -103,11 +113,39 @@ public static void globalSetUp() throws Exception {\n         createNamenodeReport(\"ns1\", \"nn1\", HAServiceState.ACTIVE));\n     stateStore.refreshCaches(true);\n \n+    setUpMocks();\n+  }\n+\n+  private static void setUpMocks() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4ddfb34bdaec13f609d421391108edb021638f26"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAxMDQzMTIxOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterAdmin.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQyMjowNjo1NlrOHLJ6wQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQyMjowNjo1NlrOHLJ6wQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ1ODg4MQ==", "bodyText": "I think whitebox needed to be deprecated.", "url": "https://github.com/apache/hadoop/pull/2266#discussion_r481458881", "createdAt": "2020-09-01T22:06:56Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterAdmin.java", "diffHunk": "@@ -103,11 +113,39 @@ public static void globalSetUp() throws Exception {\n         createNamenodeReport(\"ns1\", \"nn1\", HAServiceState.ACTIVE));\n     stateStore.refreshCaches(true);\n \n+    setUpMocks();\n+  }\n+\n+  private static void setUpMocks() throws IOException {\n     RouterRpcServer spyRpcServer =\n         Mockito.spy(routerContext.getRouter().createRpcServer());\n     Whitebox\n         .setInternalState(routerContext.getRouter(), \"rpcServer\", spyRpcServer);\n     Mockito.doReturn(null).when(spyRpcServer).getFileInfo(Mockito.anyString());\n+\n+    mockRpcClient = Mockito.spy(spyRpcServer.getRPCClient());\n+    Whitebox", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4ddfb34bdaec13f609d421391108edb021638f26"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAxMDQzMjUyOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterAdmin.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQyMjowNzoyOFrOHLJ7hA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQwMToxOToyNlrOHLNzNA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ1OTA3Ng==", "bodyText": "What are we doing with this? Is there a test actually using it?", "url": "https://github.com/apache/hadoop/pull/2266#discussion_r481459076", "createdAt": "2020-09-01T22:07:28Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterAdmin.java", "diffHunk": "@@ -103,11 +113,39 @@ public static void globalSetUp() throws Exception {\n         createNamenodeReport(\"ns1\", \"nn1\", HAServiceState.ACTIVE));\n     stateStore.refreshCaches(true);\n \n+    setUpMocks();\n+  }\n+\n+  private static void setUpMocks() throws IOException {\n     RouterRpcServer spyRpcServer =\n         Mockito.spy(routerContext.getRouter().createRpcServer());\n     Whitebox\n         .setInternalState(routerContext.getRouter(), \"rpcServer\", spyRpcServer);\n     Mockito.doReturn(null).when(spyRpcServer).getFileInfo(Mockito.anyString());\n+\n+    mockRpcClient = Mockito.spy(spyRpcServer.getRPCClient());\n+    Whitebox\n+        .setInternalState(spyRpcServer, \"rpcClient\", mockRpcClient);\n+    RemoteLocation remoteLocation0 = new RemoteLocation(\"ns0\", \"/testdir\", null);\n+    RemoteLocation remoteLocation1 = new RemoteLocation(\"ns1\", \"/\", null);\n+    mockResponse0.put(remoteLocation0,\n+        new HdfsFileStatus.Builder().build());\n+    Mockito.doReturn(mockResponse0).when(mockRpcClient).invokeConcurrent(\n+        Mockito.eq(Lists.newArrayList(remoteLocation0)),\n+        Mockito.any(RemoteMethod.class),\n+        Mockito.eq(false),\n+        Mockito.eq(false),\n+        Mockito.eq(HdfsFileStatus.class)\n+    );\n+    mockResponse1.put(remoteLocation1,\n+        new HdfsFileStatus.Builder().build());\n+    Mockito.doReturn(mockResponse1).when(mockRpcClient).invokeConcurrent(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4ddfb34bdaec13f609d421391108edb021638f26"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTUyMjQ4NA==", "bodyText": "After adding the new logic in addMount and updateMount, the current tests would fail since they don't have the file created in destination ns. The mock is to avoid the failure for existing tests.\nI can add a negative test.", "url": "https://github.com/apache/hadoop/pull/2266#discussion_r481522484", "createdAt": "2020-09-02T01:19:26Z", "author": {"login": "fengnanli"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterAdmin.java", "diffHunk": "@@ -103,11 +113,39 @@ public static void globalSetUp() throws Exception {\n         createNamenodeReport(\"ns1\", \"nn1\", HAServiceState.ACTIVE));\n     stateStore.refreshCaches(true);\n \n+    setUpMocks();\n+  }\n+\n+  private static void setUpMocks() throws IOException {\n     RouterRpcServer spyRpcServer =\n         Mockito.spy(routerContext.getRouter().createRpcServer());\n     Whitebox\n         .setInternalState(routerContext.getRouter(), \"rpcServer\", spyRpcServer);\n     Mockito.doReturn(null).when(spyRpcServer).getFileInfo(Mockito.anyString());\n+\n+    mockRpcClient = Mockito.spy(spyRpcServer.getRPCClient());\n+    Whitebox\n+        .setInternalState(spyRpcServer, \"rpcClient\", mockRpcClient);\n+    RemoteLocation remoteLocation0 = new RemoteLocation(\"ns0\", \"/testdir\", null);\n+    RemoteLocation remoteLocation1 = new RemoteLocation(\"ns1\", \"/\", null);\n+    mockResponse0.put(remoteLocation0,\n+        new HdfsFileStatus.Builder().build());\n+    Mockito.doReturn(mockResponse0).when(mockRpcClient).invokeConcurrent(\n+        Mockito.eq(Lists.newArrayList(remoteLocation0)),\n+        Mockito.any(RemoteMethod.class),\n+        Mockito.eq(false),\n+        Mockito.eq(false),\n+        Mockito.eq(HdfsFileStatus.class)\n+    );\n+    mockResponse1.put(remoteLocation1,\n+        new HdfsFileStatus.Builder().build());\n+    Mockito.doReturn(mockResponse1).when(mockRpcClient).invokeConcurrent(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ1OTA3Ng=="}, "originalCommit": {"oid": "4ddfb34bdaec13f609d421391108edb021638f26"}, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAxMDQzMzcwOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterAdminServer.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQyMjowNzo1OFrOHLJ8OQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQyMjowNzo1OFrOHLJ8OQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ1OTI1Nw==", "bodyText": "Add exception reasons", "url": "https://github.com/apache/hadoop/pull/2266#discussion_r481459257", "createdAt": "2020-09-01T22:07:58Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterAdminServer.java", "diffHunk": "@@ -562,11 +595,35 @@ public GetDestinationResponse getDestination(\n       LOG.error(\"Cannot get location for {}: {}\",\n           src, ioe.getMessage());\n     }\n-    if (nsIds.isEmpty() && !locations.isEmpty()) {\n-      String nsId = locations.get(0).getNameserviceId();\n-      nsIds.add(nsId);\n+    return nsIds;\n+  }\n+\n+  /**\n+   * Verify the file exists in destination nameservices to avoid dangling\n+   * mount points.\n+   *\n+   * @param entry the new mount points added, could be from add or update.\n+   * @return destination nameservices where the file doesn't exist.\n+   * @throws IOException", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4ddfb34bdaec13f609d421391108edb021638f26"}, "originalPosition": 89}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAxMTM1NzgyOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterAdminServer.java", "isResolved": false, "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQwMzoyOTo0MlrOHLS8mA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QyMToyMzo0NFrOHOHxHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYwNjgwOA==", "bodyText": "I would unit tests just for this function if possible.", "url": "https://github.com/apache/hadoop/pull/2266#discussion_r481606808", "createdAt": "2020-09-02T03:29:42Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterAdminServer.java", "diffHunk": "@@ -562,11 +595,35 @@ public GetDestinationResponse getDestination(\n       LOG.error(\"Cannot get location for {}: {}\",\n           src, ioe.getMessage());\n     }\n-    if (nsIds.isEmpty() && !locations.isEmpty()) {\n-      String nsId = locations.get(0).getNameserviceId();\n-      nsIds.add(nsId);\n+    return nsIds;\n+  }\n+\n+  /**\n+   * Verify the file exists in destination nameservices to avoid dangling\n+   * mount points.\n+   *\n+   * @param entry the new mount points added, could be from add or update.\n+   * @return destination nameservices where the file doesn't exist.\n+   * @throws IOException\n+   */\n+  private List<String> verifyFileInDestinations(MountTable entry)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4ddfb34bdaec13f609d421391108edb021638f26"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTY5MTI1MQ==", "bodyText": "Yeah, I can add that.\nThis logic change will break a lot of current tests listed by yetus above. What is a good practice here? I have two ideas in mind:\n\nmake the logic based on a command option when adding/updating mount tables.\nadd mock (if possible) like what I did for TestRouterAdmin.java to fake out the dirs in namenodes.\nI like 2 since I think this should be the default check, what's your opinion?", "url": "https://github.com/apache/hadoop/pull/2266#discussion_r481691251", "createdAt": "2020-09-02T04:56:11Z", "author": {"login": "fengnanli"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterAdminServer.java", "diffHunk": "@@ -562,11 +595,35 @@ public GetDestinationResponse getDestination(\n       LOG.error(\"Cannot get location for {}: {}\",\n           src, ioe.getMessage());\n     }\n-    if (nsIds.isEmpty() && !locations.isEmpty()) {\n-      String nsId = locations.get(0).getNameserviceId();\n-      nsIds.add(nsId);\n+    return nsIds;\n+  }\n+\n+  /**\n+   * Verify the file exists in destination nameservices to avoid dangling\n+   * mount points.\n+   *\n+   * @param entry the new mount points added, could be from add or update.\n+   * @return destination nameservices where the file doesn't exist.\n+   * @throws IOException\n+   */\n+  private List<String> verifyFileInDestinations(MountTable entry)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYwNjgwOA=="}, "originalCommit": {"oid": "4ddfb34bdaec13f609d421391108edb021638f26"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjI0NTgyOQ==", "bodyText": "I would fix those tests to have those paths existing yes.", "url": "https://github.com/apache/hadoop/pull/2266#discussion_r482245829", "createdAt": "2020-09-02T17:31:43Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterAdminServer.java", "diffHunk": "@@ -562,11 +595,35 @@ public GetDestinationResponse getDestination(\n       LOG.error(\"Cannot get location for {}: {}\",\n           src, ioe.getMessage());\n     }\n-    if (nsIds.isEmpty() && !locations.isEmpty()) {\n-      String nsId = locations.get(0).getNameserviceId();\n-      nsIds.add(nsId);\n+    return nsIds;\n+  }\n+\n+  /**\n+   * Verify the file exists in destination nameservices to avoid dangling\n+   * mount points.\n+   *\n+   * @param entry the new mount points added, could be from add or update.\n+   * @return destination nameservices where the file doesn't exist.\n+   * @throws IOException\n+   */\n+  private List<String> verifyFileInDestinations(MountTable entry)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYwNjgwOA=="}, "originalCommit": {"oid": "4ddfb34bdaec13f609d421391108edb021638f26"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzEyNjE3NA==", "bodyText": "Thanks for the suggestion. I want to involve more people as well since when I started to fix the tests, I found there are quite a few tests targeting/testing cases for dangling mount points.\n@aajisaka Can you share your thoughts as well?", "url": "https://github.com/apache/hadoop/pull/2266#discussion_r483126174", "createdAt": "2020-09-03T16:59:54Z", "author": {"login": "fengnanli"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterAdminServer.java", "diffHunk": "@@ -562,11 +595,35 @@ public GetDestinationResponse getDestination(\n       LOG.error(\"Cannot get location for {}: {}\",\n           src, ioe.getMessage());\n     }\n-    if (nsIds.isEmpty() && !locations.isEmpty()) {\n-      String nsId = locations.get(0).getNameserviceId();\n-      nsIds.add(nsId);\n+    return nsIds;\n+  }\n+\n+  /**\n+   * Verify the file exists in destination nameservices to avoid dangling\n+   * mount points.\n+   *\n+   * @param entry the new mount points added, could be from add or update.\n+   * @return destination nameservices where the file doesn't exist.\n+   * @throws IOException\n+   */\n+  private List<String> verifyFileInDestinations(MountTable entry)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYwNjgwOA=="}, "originalCommit": {"oid": "4ddfb34bdaec13f609d421391108edb021638f26"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzM5OTM3Ng==", "bodyText": "@goiri Uploaded an early version of trying to fix all tests. This is pretty tedious work so before I spend more time on this, let me know your thoughts.\nThere are mainly two types of tests when dealing with mount table:\n\nUse mock RouterRpcServer and so on, this way no downstream namenode calls are made. I put the mock as well, see the change for TestRouterAdmin.java\nUse real downstream namenode interaction, see TestRouterMountTable.java. I created the paths before calling mount points change.\n\nI kept thinking a much easier way is to add a Router server side config to turn this on and the default is on. In the tests I can just turn the config off explicitly and this way I don't need to deal with individual tests.", "url": "https://github.com/apache/hadoop/pull/2266#discussion_r483399376", "createdAt": "2020-09-04T05:44:12Z", "author": {"login": "fengnanli"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterAdminServer.java", "diffHunk": "@@ -562,11 +595,35 @@ public GetDestinationResponse getDestination(\n       LOG.error(\"Cannot get location for {}: {}\",\n           src, ioe.getMessage());\n     }\n-    if (nsIds.isEmpty() && !locations.isEmpty()) {\n-      String nsId = locations.get(0).getNameserviceId();\n-      nsIds.add(nsId);\n+    return nsIds;\n+  }\n+\n+  /**\n+   * Verify the file exists in destination nameservices to avoid dangling\n+   * mount points.\n+   *\n+   * @param entry the new mount points added, could be from add or update.\n+   * @return destination nameservices where the file doesn't exist.\n+   * @throws IOException\n+   */\n+  private List<String> verifyFileInDestinations(MountTable entry)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYwNjgwOA=="}, "originalCommit": {"oid": "4ddfb34bdaec13f609d421391108edb021638f26"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mzc2MDk4Mg==", "bodyText": "Eventually we may want to make all the tests be correct, but for now I'm fine setting up the config for just the new tests.", "url": "https://github.com/apache/hadoop/pull/2266#discussion_r483760982", "createdAt": "2020-09-04T17:29:50Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterAdminServer.java", "diffHunk": "@@ -562,11 +595,35 @@ public GetDestinationResponse getDestination(\n       LOG.error(\"Cannot get location for {}: {}\",\n           src, ioe.getMessage());\n     }\n-    if (nsIds.isEmpty() && !locations.isEmpty()) {\n-      String nsId = locations.get(0).getNameserviceId();\n-      nsIds.add(nsId);\n+    return nsIds;\n+  }\n+\n+  /**\n+   * Verify the file exists in destination nameservices to avoid dangling\n+   * mount points.\n+   *\n+   * @param entry the new mount points added, could be from add or update.\n+   * @return destination nameservices where the file doesn't exist.\n+   * @throws IOException\n+   */\n+  private List<String> verifyFileInDestinations(MountTable entry)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYwNjgwOA=="}, "originalCommit": {"oid": "4ddfb34bdaec13f609d421391108edb021638f26"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mzc2Nzc3Mg==", "bodyText": "If that's the case, I will try to fix all in one batch.", "url": "https://github.com/apache/hadoop/pull/2266#discussion_r483767772", "createdAt": "2020-09-04T17:45:35Z", "author": {"login": "fengnanli"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterAdminServer.java", "diffHunk": "@@ -562,11 +595,35 @@ public GetDestinationResponse getDestination(\n       LOG.error(\"Cannot get location for {}: {}\",\n           src, ioe.getMessage());\n     }\n-    if (nsIds.isEmpty() && !locations.isEmpty()) {\n-      String nsId = locations.get(0).getNameserviceId();\n-      nsIds.add(nsId);\n+    return nsIds;\n+  }\n+\n+  /**\n+   * Verify the file exists in destination nameservices to avoid dangling\n+   * mount points.\n+   *\n+   * @param entry the new mount points added, could be from add or update.\n+   * @return destination nameservices where the file doesn't exist.\n+   * @throws IOException\n+   */\n+  private List<String> verifyFileInDestinations(MountTable entry)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYwNjgwOA=="}, "originalCommit": {"oid": "4ddfb34bdaec13f609d421391108edb021638f26"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDU2OTM3Mw==", "bodyText": "Had a discussion with @ayushtkn in the jira ticket and he suggested just making the config to preserve the current workflow and enable this feature optionally. I updated the diff as that and it is not necessary to fix a lot of tests in that case.", "url": "https://github.com/apache/hadoop/pull/2266#discussion_r484569373", "createdAt": "2020-09-07T21:23:44Z", "author": {"login": "fengnanli"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterAdminServer.java", "diffHunk": "@@ -562,11 +595,35 @@ public GetDestinationResponse getDestination(\n       LOG.error(\"Cannot get location for {}: {}\",\n           src, ioe.getMessage());\n     }\n-    if (nsIds.isEmpty() && !locations.isEmpty()) {\n-      String nsId = locations.get(0).getNameserviceId();\n-      nsIds.add(nsId);\n+    return nsIds;\n+  }\n+\n+  /**\n+   * Verify the file exists in destination nameservices to avoid dangling\n+   * mount points.\n+   *\n+   * @param entry the new mount points added, could be from add or update.\n+   * @return destination nameservices where the file doesn't exist.\n+   * @throws IOException\n+   */\n+  private List<String> verifyFileInDestinations(MountTable entry)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYwNjgwOA=="}, "originalCommit": {"oid": "4ddfb34bdaec13f609d421391108edb021638f26"}, "originalPosition": 91}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzOTE5NTMxOnYy", "diffSide": "LEFT", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterAdmin.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQyMDowNTo1OVrOHPYc2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQyMDowNTo1OVrOHPYc2Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTg5MTI4OQ==", "bodyText": "Avoid the empty change", "url": "https://github.com/apache/hadoop/pull/2266#discussion_r485891289", "createdAt": "2020-09-09T20:05:59Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterAdmin.java", "diffHunk": "@@ -128,7 +175,6 @@ public void testAddMountTable() throws IOException {\n     MountTable newEntry = MountTable.newInstance(\n         \"/testpath\", Collections.singletonMap(\"ns0\", \"/testdir\"),\n         Time.now(), Time.now());\n-", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9274efe2c77bcddf5968491791cb74044b269d44"}, "originalPosition": 107}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzOTE5OTc5OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterAdmin.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQyMDowNzoyMVrOHPYfuQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQyMDowNzoyMVrOHPYfuQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTg5MjAyNQ==", "bodyText": "checkstyle is complaining", "url": "https://github.com/apache/hadoop/pull/2266#discussion_r485892025", "createdAt": "2020-09-09T20:07:21Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterAdmin.java", "diffHunk": "@@ -78,6 +83,11 @@\n       \"Hadoop:service=Router,name=FederationRPC\";\n   private static List<MountTable> mockMountTable;\n   private static StateStoreService stateStore;\n+  private static RouterRpcClient mockRpcClient;\n+  private static final Map<RemoteLocation, HdfsFileStatus> mockResponse0 =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9274efe2c77bcddf5968491791cb74044b269d44"}, "originalPosition": 38}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3263, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}