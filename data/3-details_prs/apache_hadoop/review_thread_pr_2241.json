{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDcyNjY5MTk3", "number": 2241, "reviewThreads": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQwMzo0NzowNVrOEb3__g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QxMjo0NzowN1rOEdNLiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk3NjY0NTEwOnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetUtils.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQwMzo0NzowNVrOHGEiKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQxMTo1NjoxOFrOHGUnrw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjEyNzc4NQ==", "bodyText": "Thanks @1996fanrui for your proposal here.\nA. Not sure if this improvement could impact some other modules because {{NetUtil}} is very common util for different modules rather than DFSClient only.\nB. It is better to make the scope to DFSClient only if we could based on the original description HADOOP-17222.\nC. add evict policy for {{URI_CACHE}}?", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r476127785", "createdAt": "2020-08-25T03:47:05Z", "author": {"login": "Hexiaoqiao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetUtils.java", "diffHunk": "@@ -218,6 +211,28 @@ public static InetSocketAddress createSocketAddr(String target,\n     return createSocketAddrForHost(host, port);\n   }\n \n+  private static final Map<String, URI> URI_CACHE = new ConcurrentHashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f08b5120268a57b5a439d452ddc7cd87c0bbba8f"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjE0MDM3OA==", "bodyText": "Hi @Hexiaoqiao , thank you very much for your suggestions.\nAB. Now Cache is just URI, URI object does not contain ip, only some fixed information. The same target corresponds to the same URI. If the URI contains some variable attributes, I think the current Cache may impact other modules, and even the hdfs client will also be affected. According to my understanding, cache URI should be safe.\nC. Is evict policy considering that the URI will change? Or consider it may take up more memory?", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r476140378", "createdAt": "2020-08-25T04:05:43Z", "author": {"login": "1996fanrui"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetUtils.java", "diffHunk": "@@ -218,6 +211,28 @@ public static InetSocketAddress createSocketAddr(String target,\n     return createSocketAddrForHost(host, port);\n   }\n \n+  private static final Map<String, URI> URI_CACHE = new ConcurrentHashMap<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjEyNzc4NQ=="}, "originalCommit": {"oid": "f08b5120268a57b5a439d452ddc7cd87c0bbba8f"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjE4MDUwNg==", "bodyText": "Thanks @1996fanrui for your proposal here.\nA. Not sure if this improvement could impact some other modules because {{NetUtil}} is very common util for different modules rather than DFSClient only.\nB. It is better to make the scope to DFSClient only if we could based on the original description HADOOP-17222.\nC. add evict policy for {{URI_CACHE}}?\n\nAnother idea: add a parameter isUseURICache to the createSocketAddr() method.\nisUseURICache default is false.", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r476180506", "createdAt": "2020-08-25T05:08:16Z", "author": {"login": "1996fanrui"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetUtils.java", "diffHunk": "@@ -218,6 +211,28 @@ public static InetSocketAddress createSocketAddr(String target,\n     return createSocketAddrForHost(host, port);\n   }\n \n+  private static final Map<String, URI> URI_CACHE = new ConcurrentHashMap<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjEyNzc4NQ=="}, "originalCommit": {"oid": "f08b5120268a57b5a439d452ddc7cd87c0bbba8f"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjM5MTM0Mw==", "bodyText": "Hi @Hexiaoqiao\nDFSUtilClient.isLocalAddress also uses Cache for acceleration. It is basically similar to the maximum data size of the current patch Cache, that is, the number of DNs. The Cache of DFSUtilClient.isLocalAddress does not use the evict policy, so the current patch should not worry about taking up too much memory.\nOf course, if the evict policy is added, the code will be more robust.\nIf necessary, I can use guava Cache instead of Map. What do you think?\n\n  \n    \n      hadoop/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSUtilClient.java\n    \n    \n         Line 634\n      in\n      82a7505\n    \n    \n    \n    \n\n        \n          \n           private static final Map<String, Boolean> localAddrMap = Collections", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r476391343", "createdAt": "2020-08-25T11:56:18Z", "author": {"login": "1996fanrui"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetUtils.java", "diffHunk": "@@ -218,6 +211,28 @@ public static InetSocketAddress createSocketAddr(String target,\n     return createSocketAddrForHost(host, port);\n   }\n \n+  private static final Map<String, URI> URI_CACHE = new ConcurrentHashMap<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjEyNzc4NQ=="}, "originalCommit": {"oid": "f08b5120268a57b5a439d452ddc7cd87c0bbba8f"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk3OTcxOTk4OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/resources/hdfs-default.xml", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQxNzoyMzoxNFrOHGiLdw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QwNTozMzoxNVrOHH_a2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjYxMzQ5NQ==", "bodyText": "nit: let's remove this sentence. The default value is clear in 5 lines above. Otherwise if we update the default value in future, we may miss this one which will cause confusion.", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r476613495", "createdAt": "2020-08-25T17:23:14Z", "author": {"login": "liuml07"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/resources/hdfs-default.xml", "diffHunk": "@@ -4185,6 +4185,16 @@\n   </description>\n </property>\n \n+<property>\n+  <name>dfs.client.read.uri.cache.enable</name>\n+  <value>false</value>\n+  <description>\n+    If true, dfs client will use cache when creating URI based on host:port\n+    to reduce the frequency of URI object creation.\n+    Default is false.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2221aa0ecd5ab37032ec53021117f3af32425129"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODE0MTE0Ng==", "bodyText": "Ok, very much agree with you", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r478141146", "createdAt": "2020-08-27T05:33:15Z", "author": {"login": "1996fanrui"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/resources/hdfs-default.xml", "diffHunk": "@@ -4185,6 +4185,16 @@\n   </description>\n </property>\n \n+<property>\n+  <name>dfs.client.read.uri.cache.enable</name>\n+  <value>false</value>\n+  <description>\n+    If true, dfs client will use cache when creating URI based on host:port\n+    to reduce the frequency of URI object creation.\n+    Default is false.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjYxMzQ5NQ=="}, "originalCommit": {"oid": "2221aa0ecd5ab37032ec53021117f3af32425129"}, "originalPosition": 10}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk3OTc2MTQ4OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQxNzoyNzoyMVrOHGifxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QwNTo0NTo1MFrOHH_5GA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjYxODY5Mg==", "bodyText": "nit: name this ...uri.cache.enabled by replacing enable with enabled?\nAlso change other variables related to uriCacheEnable", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r476618692", "createdAt": "2020-08-25T17:27:21Z", "author": {"login": "liuml07"}, "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.java", "diffHunk": "@@ -414,6 +414,9 @@\n \n     String  PREFETCH_SIZE_KEY = PREFIX + \"prefetch.size\";\n \n+    String URI_CACHE_KEY = PREFIX + \"uri.cache.enable\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2221aa0ecd5ab37032ec53021117f3af32425129"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODE0ODg4OA==", "bodyText": "ok, thanks for your review", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r478148888", "createdAt": "2020-08-27T05:45:50Z", "author": {"login": "1996fanrui"}, "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.java", "diffHunk": "@@ -414,6 +414,9 @@\n \n     String  PREFETCH_SIZE_KEY = PREFIX + \"prefetch.size\";\n \n+    String URI_CACHE_KEY = PREFIX + \"uri.cache.enable\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjYxODY5Mg=="}, "originalCommit": {"oid": "2221aa0ecd5ab37032ec53021117f3af32425129"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk4ODg1MDU0OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QwNDo1MTo1NVrOHH9z-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QwNjoxMTo1OFrOHIA9lg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODExNDgxMA==", "bodyText": "I think it's better not to have this config as I suggested in the JIRA. It's for two reasons:\n\nThis seems not critical parameter and Hadoop already has too many configurations. I think the default value 12hours is good enough. It's not meant to be tuned by per client since the cache is static and shared. So removing this config seems preferred to me.\nThe URI cache is initialized only once. So passing the uriCacheExpireMs parameter every time does not really carry useful information. And it may be confusing if people mistakenly interpret this as a per-URI setting when they call NetUtils.createSocketAddr() which is not.\n\nThoughts?", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r478114810", "createdAt": "2020-08-27T04:51:55Z", "author": {"login": "liuml07"}, "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.java", "diffHunk": "@@ -414,6 +415,11 @@\n \n     String  PREFETCH_SIZE_KEY = PREFIX + \"prefetch.size\";\n \n+    String URI_CACHE_KEY = PREFIX + \"uri.cache.enable\";\n+    boolean URI_CACHE_DEFAULT = false;\n+    String URI_CACHE_EXPIRE_MS_KEY = PREFIX + \"uri.cache.expire.ms\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7a46d4987a9a2fed7f447aef10aae1e5c12c1f5"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODE0ODE5MA==", "bodyText": "I'm so sorry, I misunderstood the meaning of jira. I thought the two parameters cache size and expire time seem a bit overkill, so the cache size was removed and the expire time was retained.\nNow I will remove expire time and use 12 hours as the default value.", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r478148190", "createdAt": "2020-08-27T05:44:51Z", "author": {"login": "1996fanrui"}, "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.java", "diffHunk": "@@ -414,6 +415,11 @@\n \n     String  PREFETCH_SIZE_KEY = PREFIX + \"prefetch.size\";\n \n+    String URI_CACHE_KEY = PREFIX + \"uri.cache.enable\";\n+    boolean URI_CACHE_DEFAULT = false;\n+    String URI_CACHE_EXPIRE_MS_KEY = PREFIX + \"uri.cache.expire.ms\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODExNDgxMA=="}, "originalCommit": {"oid": "f7a46d4987a9a2fed7f447aef10aae1e5c12c1f5"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODE2NDU5NA==", "bodyText": "Thanks, it's my bad. I could have made it clearer.", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r478164594", "createdAt": "2020-08-27T06:09:21Z", "author": {"login": "liuml07"}, "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.java", "diffHunk": "@@ -414,6 +415,11 @@\n \n     String  PREFETCH_SIZE_KEY = PREFIX + \"prefetch.size\";\n \n+    String URI_CACHE_KEY = PREFIX + \"uri.cache.enable\";\n+    boolean URI_CACHE_DEFAULT = false;\n+    String URI_CACHE_EXPIRE_MS_KEY = PREFIX + \"uri.cache.expire.ms\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODExNDgxMA=="}, "originalCommit": {"oid": "f7a46d4987a9a2fed7f447aef10aae1e5c12c1f5"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODE2NjQyMg==", "bodyText": "Thanks for your review, I have fixed all the comments.", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r478166422", "createdAt": "2020-08-27T06:11:58Z", "author": {"login": "1996fanrui"}, "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.java", "diffHunk": "@@ -414,6 +415,11 @@\n \n     String  PREFETCH_SIZE_KEY = PREFIX + \"prefetch.size\";\n \n+    String URI_CACHE_KEY = PREFIX + \"uri.cache.enable\";\n+    boolean URI_CACHE_DEFAULT = false;\n+    String URI_CACHE_EXPIRE_MS_KEY = PREFIX + \"uri.cache.expire.ms\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODExNDgxMA=="}, "originalCommit": {"oid": "f7a46d4987a9a2fed7f447aef10aae1e5c12c1f5"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk5MDU3NTYxOnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/net/TestNetUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QxMjo0MDo1OFrOHIOdmw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QxMjo1NzowNVrOHIPFow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODM4NzYxMQ==", "bodyText": "nit: delete no meaningless annotation?", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r478387611", "createdAt": "2020-08-27T12:40:58Z", "author": {"login": "Hexiaoqiao"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/net/TestNetUtils.java", "diffHunk": "@@ -360,6 +360,49 @@ public void testCreateSocketAddress() throws Throwable {\n     }\n   }\n \n+  @Test\n+  public void testCreateSocketAddressWithURICache() throws Throwable {\n+    InetSocketAddress addr = NetUtils.createSocketAddr(\n+        \"127.0.0.1:12345\", 1000, \"myconfig\", true);\n+    assertEquals(\"127.0.0.1\", addr.getAddress().getHostAddress());\n+    assertEquals(12345, addr.getPort());\n+\n+    addr = NetUtils.createSocketAddr(\n+        \"127.0.0.1:12345\", 1000, \"myconfig\", true);\n+    assertEquals(\"127.0.0.1\", addr.getAddress().getHostAddress());\n+    assertEquals(12345, addr.getPort());\n+\n+    // ----------------------------------------------------", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e0a0103b7887d612217800c4f7d7647fe4d29dd9"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODM5Nzg1OQ==", "bodyText": "@Hexiaoqiao , thanks for your review.\nTo test the Cache, the same case is executed twice. There are a total of three cases. In order to divide the three cases, these comments are added.", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r478397859", "createdAt": "2020-08-27T12:57:05Z", "author": {"login": "1996fanrui"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/net/TestNetUtils.java", "diffHunk": "@@ -360,6 +360,49 @@ public void testCreateSocketAddress() throws Throwable {\n     }\n   }\n \n+  @Test\n+  public void testCreateSocketAddressWithURICache() throws Throwable {\n+    InetSocketAddress addr = NetUtils.createSocketAddr(\n+        \"127.0.0.1:12345\", 1000, \"myconfig\", true);\n+    assertEquals(\"127.0.0.1\", addr.getAddress().getHostAddress());\n+    assertEquals(12345, addr.getPort());\n+\n+    addr = NetUtils.createSocketAddr(\n+        \"127.0.0.1:12345\", 1000, \"myconfig\", true);\n+    assertEquals(\"127.0.0.1\", addr.getAddress().getHostAddress());\n+    assertEquals(12345, addr.getPort());\n+\n+    // ----------------------------------------------------", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODM4NzYxMQ=="}, "originalCommit": {"oid": "e0a0103b7887d612217800c4f7d7647fe4d29dd9"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk5MDU4MDMwOnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/net/TestNetUtils.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QxMjo0MjowM1rOHIOgWg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QxNDozMDo0NFrOHITG9Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODM4ODMxNA==", "bodyText": "nit: addr seems useless? no need to assign to addr anymore?", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r478388314", "createdAt": "2020-08-27T12:42:03Z", "author": {"login": "Hexiaoqiao"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/net/TestNetUtils.java", "diffHunk": "@@ -360,6 +360,49 @@ public void testCreateSocketAddress() throws Throwable {\n     }\n   }\n \n+  @Test\n+  public void testCreateSocketAddressWithURICache() throws Throwable {\n+    InetSocketAddress addr = NetUtils.createSocketAddr(\n+        \"127.0.0.1:12345\", 1000, \"myconfig\", true);\n+    assertEquals(\"127.0.0.1\", addr.getAddress().getHostAddress());\n+    assertEquals(12345, addr.getPort());\n+\n+    addr = NetUtils.createSocketAddr(\n+        \"127.0.0.1:12345\", 1000, \"myconfig\", true);\n+    assertEquals(\"127.0.0.1\", addr.getAddress().getHostAddress());\n+    assertEquals(12345, addr.getPort());\n+\n+    // ----------------------------------------------------\n+\n+    addr = NetUtils.createSocketAddr(\n+        \"127.0.0.1\", 1000, \"myconfig\", true);\n+    assertEquals(\"127.0.0.1\", addr.getAddress().getHostAddress());\n+    assertEquals(1000, addr.getPort());\n+\n+    addr = NetUtils.createSocketAddr(\n+        \"127.0.0.1\", 1000, \"myconfig\", true);\n+    assertEquals(\"127.0.0.1\", addr.getAddress().getHostAddress());\n+    assertEquals(1000, addr.getPort());\n+\n+    // ----------------------------------------------------\n+\n+    try {\n+      addr = NetUtils.createSocketAddr(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e0a0103b7887d612217800c4f7d7647fe4d29dd9"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODQyMjAwNQ==", "bodyText": "ok, I will do it", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r478422005", "createdAt": "2020-08-27T13:32:58Z", "author": {"login": "1996fanrui"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/net/TestNetUtils.java", "diffHunk": "@@ -360,6 +360,49 @@ public void testCreateSocketAddress() throws Throwable {\n     }\n   }\n \n+  @Test\n+  public void testCreateSocketAddressWithURICache() throws Throwable {\n+    InetSocketAddress addr = NetUtils.createSocketAddr(\n+        \"127.0.0.1:12345\", 1000, \"myconfig\", true);\n+    assertEquals(\"127.0.0.1\", addr.getAddress().getHostAddress());\n+    assertEquals(12345, addr.getPort());\n+\n+    addr = NetUtils.createSocketAddr(\n+        \"127.0.0.1:12345\", 1000, \"myconfig\", true);\n+    assertEquals(\"127.0.0.1\", addr.getAddress().getHostAddress());\n+    assertEquals(12345, addr.getPort());\n+\n+    // ----------------------------------------------------\n+\n+    addr = NetUtils.createSocketAddr(\n+        \"127.0.0.1\", 1000, \"myconfig\", true);\n+    assertEquals(\"127.0.0.1\", addr.getAddress().getHostAddress());\n+    assertEquals(1000, addr.getPort());\n+\n+    addr = NetUtils.createSocketAddr(\n+        \"127.0.0.1\", 1000, \"myconfig\", true);\n+    assertEquals(\"127.0.0.1\", addr.getAddress().getHostAddress());\n+    assertEquals(1000, addr.getPort());\n+\n+    // ----------------------------------------------------\n+\n+    try {\n+      addr = NetUtils.createSocketAddr(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODM4ODMxNA=="}, "originalCommit": {"oid": "e0a0103b7887d612217800c4f7d7647fe4d29dd9"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODQ2MzczMw==", "bodyText": "@Hexiaoqiao I have done it", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r478463733", "createdAt": "2020-08-27T14:30:44Z", "author": {"login": "1996fanrui"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/net/TestNetUtils.java", "diffHunk": "@@ -360,6 +360,49 @@ public void testCreateSocketAddress() throws Throwable {\n     }\n   }\n \n+  @Test\n+  public void testCreateSocketAddressWithURICache() throws Throwable {\n+    InetSocketAddress addr = NetUtils.createSocketAddr(\n+        \"127.0.0.1:12345\", 1000, \"myconfig\", true);\n+    assertEquals(\"127.0.0.1\", addr.getAddress().getHostAddress());\n+    assertEquals(12345, addr.getPort());\n+\n+    addr = NetUtils.createSocketAddr(\n+        \"127.0.0.1:12345\", 1000, \"myconfig\", true);\n+    assertEquals(\"127.0.0.1\", addr.getAddress().getHostAddress());\n+    assertEquals(12345, addr.getPort());\n+\n+    // ----------------------------------------------------\n+\n+    addr = NetUtils.createSocketAddr(\n+        \"127.0.0.1\", 1000, \"myconfig\", true);\n+    assertEquals(\"127.0.0.1\", addr.getAddress().getHostAddress());\n+    assertEquals(1000, addr.getPort());\n+\n+    addr = NetUtils.createSocketAddr(\n+        \"127.0.0.1\", 1000, \"myconfig\", true);\n+    assertEquals(\"127.0.0.1\", addr.getAddress().getHostAddress());\n+    assertEquals(1000, addr.getPort());\n+\n+    // ----------------------------------------------------\n+\n+    try {\n+      addr = NetUtils.createSocketAddr(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODM4ODMxNA=="}, "originalCommit": {"oid": "e0a0103b7887d612217800c4f7d7647fe4d29dd9"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk5MDYwMTA1OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/DfsClientConf.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QxMjo0NzowN1rOHIOtXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QxMzowMzo0MlrOHIPXAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODM5MTY0NQ==", "bodyText": "nit: the changes seems no related with this ticket,  Should we update it in another single one?", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r478391645", "createdAt": "2020-08-27T12:47:07Z", "author": {"login": "Hexiaoqiao"}, "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/DfsClientConf.java", "diffHunk": "@@ -211,24 +212,7 @@ public DfsClientConf(Configuration conf) {\n         Write.MAX_PACKETS_IN_FLIGHT_KEY,\n         Write.MAX_PACKETS_IN_FLIGHT_DEFAULT);\n \n-    final boolean byteArrayManagerEnabled = conf.getBoolean(\n-        Write.ByteArrayManager.ENABLED_KEY,\n-        Write.ByteArrayManager.ENABLED_DEFAULT);\n-    if (!byteArrayManagerEnabled) {\n-      writeByteArrayManagerConf = null;\n-    } else {\n-      final int countThreshold = conf.getInt(\n-          Write.ByteArrayManager.COUNT_THRESHOLD_KEY,\n-          Write.ByteArrayManager.COUNT_THRESHOLD_DEFAULT);\n-      final int countLimit = conf.getInt(\n-          Write.ByteArrayManager.COUNT_LIMIT_KEY,\n-          Write.ByteArrayManager.COUNT_LIMIT_DEFAULT);\n-      final long countResetTimePeriodMs = conf.getLong(\n-          Write.ByteArrayManager.COUNT_RESET_TIME_PERIOD_MS_KEY,\n-          Write.ByteArrayManager.COUNT_RESET_TIME_PERIOD_MS_DEFAULT);\n-      writeByteArrayManagerConf = new ByteArrayManager.Conf(\n-          countThreshold, countLimit, countResetTimePeriodMs);\n-    }\n+    writeByteArrayManagerConf = loadWriteByteArrayManagerConf(conf);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e0a0103b7887d612217800c4f7d7647fe4d29dd9"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODM5OTk4OQ==", "bodyText": "The constructor of DfsClientConf exceeds 150 lines, and the checkstyle is not passed. So a simple refactoring was made.", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r478399989", "createdAt": "2020-08-27T13:00:07Z", "author": {"login": "1996fanrui"}, "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/DfsClientConf.java", "diffHunk": "@@ -211,24 +212,7 @@ public DfsClientConf(Configuration conf) {\n         Write.MAX_PACKETS_IN_FLIGHT_KEY,\n         Write.MAX_PACKETS_IN_FLIGHT_DEFAULT);\n \n-    final boolean byteArrayManagerEnabled = conf.getBoolean(\n-        Write.ByteArrayManager.ENABLED_KEY,\n-        Write.ByteArrayManager.ENABLED_DEFAULT);\n-    if (!byteArrayManagerEnabled) {\n-      writeByteArrayManagerConf = null;\n-    } else {\n-      final int countThreshold = conf.getInt(\n-          Write.ByteArrayManager.COUNT_THRESHOLD_KEY,\n-          Write.ByteArrayManager.COUNT_THRESHOLD_DEFAULT);\n-      final int countLimit = conf.getInt(\n-          Write.ByteArrayManager.COUNT_LIMIT_KEY,\n-          Write.ByteArrayManager.COUNT_LIMIT_DEFAULT);\n-      final long countResetTimePeriodMs = conf.getLong(\n-          Write.ByteArrayManager.COUNT_RESET_TIME_PERIOD_MS_KEY,\n-          Write.ByteArrayManager.COUNT_RESET_TIME_PERIOD_MS_DEFAULT);\n-      writeByteArrayManagerConf = new ByteArrayManager.Conf(\n-          countThreshold, countLimit, countResetTimePeriodMs);\n-    }\n+    writeByteArrayManagerConf = loadWriteByteArrayManagerConf(conf);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODM5MTY0NQ=="}, "originalCommit": {"oid": "e0a0103b7887d612217800c4f7d7647fe4d29dd9"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODQwMjMwNw==", "bodyText": "Thanks for your quick response, this is reasonable.", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r478402307", "createdAt": "2020-08-27T13:03:42Z", "author": {"login": "Hexiaoqiao"}, "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/DfsClientConf.java", "diffHunk": "@@ -211,24 +212,7 @@ public DfsClientConf(Configuration conf) {\n         Write.MAX_PACKETS_IN_FLIGHT_KEY,\n         Write.MAX_PACKETS_IN_FLIGHT_DEFAULT);\n \n-    final boolean byteArrayManagerEnabled = conf.getBoolean(\n-        Write.ByteArrayManager.ENABLED_KEY,\n-        Write.ByteArrayManager.ENABLED_DEFAULT);\n-    if (!byteArrayManagerEnabled) {\n-      writeByteArrayManagerConf = null;\n-    } else {\n-      final int countThreshold = conf.getInt(\n-          Write.ByteArrayManager.COUNT_THRESHOLD_KEY,\n-          Write.ByteArrayManager.COUNT_THRESHOLD_DEFAULT);\n-      final int countLimit = conf.getInt(\n-          Write.ByteArrayManager.COUNT_LIMIT_KEY,\n-          Write.ByteArrayManager.COUNT_LIMIT_DEFAULT);\n-      final long countResetTimePeriodMs = conf.getLong(\n-          Write.ByteArrayManager.COUNT_RESET_TIME_PERIOD_MS_KEY,\n-          Write.ByteArrayManager.COUNT_RESET_TIME_PERIOD_MS_DEFAULT);\n-      writeByteArrayManagerConf = new ByteArrayManager.Conf(\n-          countThreshold, countLimit, countResetTimePeriodMs);\n-    }\n+    writeByteArrayManagerConf = loadWriteByteArrayManagerConf(conf);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODM5MTY0NQ=="}, "originalCommit": {"oid": "e0a0103b7887d612217800c4f7d7647fe4d29dd9"}, "originalPosition": 30}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3399, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}