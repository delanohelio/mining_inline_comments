{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDg0Mjc1Mzk4", "number": 2297, "reviewThreads": {"totalCount": 21, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQyMzoxOToxMlrOEiUxaA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxODo0NToyOFrOEosngg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0NDI3MzY4OnYy", "diffSide": "LEFT", "path": "hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/io/compress/snappy/SnappyCompressor.c", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQyMzoxOToxMlrOHQI6OQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQyMzozMDozNlrOHQJHvg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjY4NTI0MQ==", "bodyText": "Per #2201 (comment) Are those native code used in hadoop-mapreduce-client-nativetask? If so, we probably need to keep it now.", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r486685241", "createdAt": "2020-09-10T23:19:12Z", "author": {"login": "dbtsai"}, "path": "hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/io/compress/snappy/SnappyCompressor.c", "diffHunk": "@@ -1,166 +0,0 @@\n-/*", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f52dd205707259064247b23f7443f5840bace62f"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjY4NjQ4Mw==", "bodyText": "Hmm, because we remove native method in java files, I think we don't generate .h file needed for compilation: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-2297/1/artifact/out/patch-unit-hadoop-common-project_hadoop-common.txt\n[WARNING] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-2297/src/hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.c:32:10: fatal error: org_apache_hadoop_io_compress_snappy_SnappyDecompressor.h: No such file or directory\n[WARNING]  #include \"org_apache_hadoop_io_compress_snappy_SnappyDecompressor.h\"\n[WARNING]           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n[WARNING] compilation terminated.", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r486686483", "createdAt": "2020-09-10T23:23:18Z", "author": {"login": "viirya"}, "path": "hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/io/compress/snappy/SnappyCompressor.c", "diffHunk": "@@ -1,166 +0,0 @@\n-/*", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjY4NTI0MQ=="}, "originalCommit": {"oid": "f52dd205707259064247b23f7443f5840bace62f"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjY4ODcwMg==", "bodyText": "Btw, I don't see they are used in hadoop-mapreduce-client-nativetask if I don't miss it. Let's wait the build and test.", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r486688702", "createdAt": "2020-09-10T23:30:36Z", "author": {"login": "viirya"}, "path": "hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/io/compress/snappy/SnappyCompressor.c", "diffHunk": "@@ -1,166 +0,0 @@\n-/*", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjY4NTI0MQ=="}, "originalCommit": {"oid": "f52dd205707259064247b23f7443f5840bace62f"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA2MjMxOTA5OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/pom.xml", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQxMzowNjowM1rOHSv4vA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQxODo1MDoxMlrOHS-rEA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTQyMDk4OA==", "bodyText": "provided", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r489420988", "createdAt": "2020-09-16T13:06:03Z", "author": {"login": "steveloughran"}, "path": "hadoop-common-project/hadoop-common/pom.xml", "diffHunk": "@@ -363,6 +363,10 @@\n       <artifactId>wildfly-openssl-java</artifactId>\n       <scope>provided</scope>\n     </dependency>\n+    <dependency>\n+      <groupId>org.xerial.snappy</groupId>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "666a37bc56d6512fe953e438ad4224e935ea6cd2"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTY2MzI0OA==", "bodyText": "We can make it provided, and once we create a hadoop-compression module, we can add back the jar. @viirya since the jar will be provided, we need to check if the class exists so we can log it with right message.", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r489663248", "createdAt": "2020-09-16T18:50:12Z", "author": {"login": "dbtsai"}, "path": "hadoop-common-project/hadoop-common/pom.xml", "diffHunk": "@@ -363,6 +363,10 @@\n       <artifactId>wildfly-openssl-java</artifactId>\n       <scope>provided</scope>\n     </dependency>\n+    <dependency>\n+      <groupId>org.xerial.snappy</groupId>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTQyMDk4OA=="}, "originalCommit": {"oid": "666a37bc56d6512fe953e438ad4224e935ea6cd2"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA2MjMyNzg2OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQxMzowODowM1rOHSv98w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xN1QwMTozNjoxMlrOHTKtbA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTQyMjMyMw==", "bodyText": "use name of config option which users can tun", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r489422323", "createdAt": "2020-09-16T13:08:03Z", "author": {"login": "steveloughran"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java", "diffHunk": "@@ -276,13 +258,29 @@ public void end() {\n     // do nothing\n   }\n \n-  private native static void initIDs();\n+  private int decompressBytesDirect() throws IOException {\n+    if (compressedDirectBufLen == 0) {\n+      return 0;\n+    } else {\n+      // Set the position and limit of `compressedDirectBuf` for reading\n+      compressedDirectBuf.limit(compressedDirectBufLen).position(0);\n+      // There is compressed input, decompress it now.\n+      int size = Snappy.uncompressedLength((ByteBuffer) compressedDirectBuf);\n+      if (size > uncompressedDirectBuf.remaining()) {\n+        throw new IOException(\"Could not decompress data. \" +\n+          \"uncompressedDirectBuf length is too small.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "666a37bc56d6512fe953e438ad4224e935ea6cd2"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTg2MDQ2MA==", "bodyText": "I found this check is not needed. Removed.", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r489860460", "createdAt": "2020-09-17T01:36:12Z", "author": {"login": "viirya"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java", "diffHunk": "@@ -276,13 +258,29 @@ public void end() {\n     // do nothing\n   }\n \n-  private native static void initIDs();\n+  private int decompressBytesDirect() throws IOException {\n+    if (compressedDirectBufLen == 0) {\n+      return 0;\n+    } else {\n+      // Set the position and limit of `compressedDirectBuf` for reading\n+      compressedDirectBuf.limit(compressedDirectBufLen).position(0);\n+      // There is compressed input, decompress it now.\n+      int size = Snappy.uncompressedLength((ByteBuffer) compressedDirectBuf);\n+      if (size > uncompressedDirectBuf.remaining()) {\n+        throw new IOException(\"Could not decompress data. \" +\n+          \"uncompressedDirectBuf length is too small.\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTQyMjMyMw=="}, "originalCommit": {"oid": "666a37bc56d6512fe953e438ad4224e935ea6cd2"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA2MjMyOTk5OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQxMzowODozM1rOHSv_RQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xN1QwMTozMjoyNlrOHTKkKQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTQyMjY2MQ==", "bodyText": "please stop the IDE removing trailing whitespace on lines which haven't been edited; complicates life", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r489422661", "createdAt": "2020-09-16T13:08:33Z", "author": {"login": "steveloughran"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java", "diffHunk": "@@ -276,13 +258,29 @@ public void end() {\n     // do nothing\n   }\n \n-  private native static void initIDs();\n+  private int decompressBytesDirect() throws IOException {\n+    if (compressedDirectBufLen == 0) {\n+      return 0;\n+    } else {\n+      // Set the position and limit of `compressedDirectBuf` for reading\n+      compressedDirectBuf.limit(compressedDirectBufLen).position(0);\n+      // There is compressed input, decompress it now.\n+      int size = Snappy.uncompressedLength((ByteBuffer) compressedDirectBuf);\n+      if (size > uncompressedDirectBuf.remaining()) {\n+        throw new IOException(\"Could not decompress data. \" +\n+          \"uncompressedDirectBuf length is too small.\");\n+      }\n+      size = Snappy.uncompress((ByteBuffer) compressedDirectBuf,\n+              (ByteBuffer) uncompressedDirectBuf);\n+      compressedDirectBufLen = 0;\n+      compressedDirectBuf.limit(compressedDirectBuf.capacity()).position(0);\n+      return size;\n+    }\n+  }\n \n-  private native int decompressBytesDirect();\n-  \n   int decompressDirect(ByteBuffer src, ByteBuffer dst) throws IOException {\n     assert (this instanceof SnappyDirectDecompressor);\n-    \n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "666a37bc56d6512fe953e438ad4224e935ea6cd2"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTg1ODA4OQ==", "bodyText": "ok, reverted tailing whitespace.", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r489858089", "createdAt": "2020-09-17T01:32:26Z", "author": {"login": "viirya"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java", "diffHunk": "@@ -276,13 +258,29 @@ public void end() {\n     // do nothing\n   }\n \n-  private native static void initIDs();\n+  private int decompressBytesDirect() throws IOException {\n+    if (compressedDirectBufLen == 0) {\n+      return 0;\n+    } else {\n+      // Set the position and limit of `compressedDirectBuf` for reading\n+      compressedDirectBuf.limit(compressedDirectBufLen).position(0);\n+      // There is compressed input, decompress it now.\n+      int size = Snappy.uncompressedLength((ByteBuffer) compressedDirectBuf);\n+      if (size > uncompressedDirectBuf.remaining()) {\n+        throw new IOException(\"Could not decompress data. \" +\n+          \"uncompressedDirectBuf length is too small.\");\n+      }\n+      size = Snappy.uncompress((ByteBuffer) compressedDirectBuf,\n+              (ByteBuffer) uncompressedDirectBuf);\n+      compressedDirectBufLen = 0;\n+      compressedDirectBuf.limit(compressedDirectBuf.capacity()).position(0);\n+      return size;\n+    }\n+  }\n \n-  private native int decompressBytesDirect();\n-  \n   int decompressDirect(ByteBuffer src, ByteBuffer dst) throws IOException {\n     assert (this instanceof SnappyDirectDecompressor);\n-    \n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTQyMjY2MQ=="}, "originalCommit": {"oid": "666a37bc56d6512fe953e438ad4224e935ea6cd2"}, "originalPosition": 75}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA2MjMzNzIyOnYy", "diffSide": "RIGHT", "path": "hadoop-project-dist/pom.xml", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQxMzoxMDoxMlrOHSwDww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMjozNToyOVrOHaEkJA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTQyMzgxMQ==", "bodyText": "what about the others snappylibs?", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r489423811", "createdAt": "2020-09-16T13:10:12Z", "author": {"login": "steveloughran"}, "path": "hadoop-project-dist/pom.xml", "diffHunk": "@@ -341,7 +340,6 @@\n                     <argument>--openssllib=${openssl.lib}</argument>\n                     <argument>--opensslbinbundle=${bundle.openssl.in.bin}</argument>\n                     <argument>--openssllibbundle=${bundle.openssl}</argument>\n-                    <argument>--snappybinbundle=${bundle.snappy.in.bin}</argument>\n                     <argument>--snappylib=${snappy.lib}</argument>\n                     <argument>--snappylibbundle=${bundle.snappy}</argument>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "666a37bc56d6512fe953e438ad4224e935ea6cd2"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTgyOTQxMQ==", "bodyText": "we don't touch hadoop-mapreduce-client-nativetask that needs snappy lib still, per #2201 (comment)", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r489829411", "createdAt": "2020-09-17T00:43:49Z", "author": {"login": "viirya"}, "path": "hadoop-project-dist/pom.xml", "diffHunk": "@@ -341,7 +340,6 @@\n                     <argument>--openssllib=${openssl.lib}</argument>\n                     <argument>--opensslbinbundle=${bundle.openssl.in.bin}</argument>\n                     <argument>--openssllibbundle=${bundle.openssl}</argument>\n-                    <argument>--snappybinbundle=${bundle.snappy.in.bin}</argument>\n                     <argument>--snappylib=${snappy.lib}</argument>\n                     <argument>--snappylibbundle=${bundle.snappy}</argument>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTQyMzgxMQ=="}, "originalCommit": {"oid": "666a37bc56d6512fe953e438ad4224e935ea6cd2"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njk3NzM2NA==", "bodyText": "does this mean we don't need the option in dev-support/bin/dist-copynativelibs for snappy?", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r496977364", "createdAt": "2020-09-29T19:13:54Z", "author": {"login": "sunchao"}, "path": "hadoop-project-dist/pom.xml", "diffHunk": "@@ -341,7 +340,6 @@\n                     <argument>--openssllib=${openssl.lib}</argument>\n                     <argument>--opensslbinbundle=${bundle.openssl.in.bin}</argument>\n                     <argument>--openssllibbundle=${bundle.openssl}</argument>\n-                    <argument>--snappybinbundle=${bundle.snappy.in.bin}</argument>\n                     <argument>--snappylib=${snappy.lib}</argument>\n                     <argument>--snappylibbundle=${bundle.snappy}</argument>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTQyMzgxMQ=="}, "originalCommit": {"oid": "666a37bc56d6512fe953e438ad4224e935ea6cd2"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA5MzY4Mw==", "bodyText": "Hmm, I am not sure about this now. I think it is safer to revert this back? I am not sure if hadoop-mapreduce-client-nativetask uses this.", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r497093683", "createdAt": "2020-09-29T22:19:24Z", "author": {"login": "viirya"}, "path": "hadoop-project-dist/pom.xml", "diffHunk": "@@ -341,7 +340,6 @@\n                     <argument>--openssllib=${openssl.lib}</argument>\n                     <argument>--opensslbinbundle=${bundle.openssl.in.bin}</argument>\n                     <argument>--openssllibbundle=${bundle.openssl}</argument>\n-                    <argument>--snappybinbundle=${bundle.snappy.in.bin}</argument>\n                     <argument>--snappylib=${snappy.lib}</argument>\n                     <argument>--snappylibbundle=${bundle.snappy}</argument>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTQyMzgxMQ=="}, "originalCommit": {"oid": "666a37bc56d6512fe953e438ad4224e935ea6cd2"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA5OTgxMg==", "bodyText": "Re-checked. hadoop-mapreduce-client-nativetask doesn't use bundle.snappy.in.bin. Only native-win profile of hadoop-common and native-win profile in hadoop-project use bundle.snappy.in.bin. They are added by HADOOP-9802 to support SnappyCodec on Windows. So looks like it is safe to remove.", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r497099812", "createdAt": "2020-09-29T22:35:29Z", "author": {"login": "viirya"}, "path": "hadoop-project-dist/pom.xml", "diffHunk": "@@ -341,7 +340,6 @@\n                     <argument>--openssllib=${openssl.lib}</argument>\n                     <argument>--opensslbinbundle=${bundle.openssl.in.bin}</argument>\n                     <argument>--openssllibbundle=${bundle.openssl}</argument>\n-                    <argument>--snappybinbundle=${bundle.snappy.in.bin}</argument>\n                     <argument>--snappylib=${snappy.lib}</argument>\n                     <argument>--snappylibbundle=${bundle.snappy}</argument>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTQyMzgxMQ=="}, "originalCommit": {"oid": "666a37bc56d6512fe953e438ad4224e935ea6cd2"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA2NTM5MDI4OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xN1QwMzoxNTowMlrOHTOdZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xN1QyMjoyODo1MVrOHT3qBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTkyMTg5Mw==", "bodyText": "In the original code, we throw a runtime exception if the native snappy is not found. Should we follow?\n      throw new RuntimeException(\"native snappy library not available: \" +\n          \"SnappyCompressor has not been loaded.\");", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r489921893", "createdAt": "2020-09-17T03:15:02Z", "author": {"login": "dbtsai"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java", "diffHunk": "@@ -45,30 +46,19 @@\n   private int userBufOff = 0, userBufLen = 0;\n   private boolean finished;\n \n-  private static boolean nativeSnappyLoaded = false;\n-\n-  static {\n-    if (NativeCodeLoader.isNativeCodeLoaded() &&\n-        NativeCodeLoader.buildSupportsSnappy()) {\n-      try {\n-        initIDs();\n-        nativeSnappyLoaded = true;\n-      } catch (Throwable t) {\n-        LOG.error(\"failed to load SnappyDecompressor\", t);\n-      }\n-    }\n-  }\n-  \n-  public static boolean isNativeCodeLoaded() {\n-    return nativeSnappyLoaded;\n-  }\n-  \n   /**\n    * Creates a new compressor.\n    *\n    * @param directBufferSize size of the direct buffer to be used.\n    */\n   public SnappyDecompressor(int directBufferSize) {\n+    // `snappy-java` is provided scope. We need to check if its availability.\n+    try {\n+      SnappyLoader.getVersion();\n+    } catch (Throwable t) {\n+      LOG.warn(\"Error loading snappy libraries: \" + t);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ed518d238adfee81c821b74434785afb5684710"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU5Njg2OA==", "bodyText": "ok, changed to throw RuntimeException.", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r490596868", "createdAt": "2020-09-17T22:28:51Z", "author": {"login": "viirya"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java", "diffHunk": "@@ -45,30 +46,19 @@\n   private int userBufOff = 0, userBufLen = 0;\n   private boolean finished;\n \n-  private static boolean nativeSnappyLoaded = false;\n-\n-  static {\n-    if (NativeCodeLoader.isNativeCodeLoaded() &&\n-        NativeCodeLoader.buildSupportsSnappy()) {\n-      try {\n-        initIDs();\n-        nativeSnappyLoaded = true;\n-      } catch (Throwable t) {\n-        LOG.error(\"failed to load SnappyDecompressor\", t);\n-      }\n-    }\n-  }\n-  \n-  public static boolean isNativeCodeLoaded() {\n-    return nativeSnappyLoaded;\n-  }\n-  \n   /**\n    * Creates a new compressor.\n    *\n    * @param directBufferSize size of the direct buffer to be used.\n    */\n   public SnappyDecompressor(int directBufferSize) {\n+    // `snappy-java` is provided scope. We need to check if its availability.\n+    try {\n+      SnappyLoader.getVersion();\n+    } catch (Throwable t) {\n+      LOG.warn(\"Error loading snappy libraries: \" + t);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTkyMTg5Mw=="}, "originalCommit": {"oid": "0ed518d238adfee81c821b74434785afb5684710"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA2NTM5Njg1OnYy", "diffSide": "LEFT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xN1QwMzoxNjo1MFrOHTOhpQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xN1QyMjoyODozM1rOHT3pmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTkyMjk4MQ==", "bodyText": "we need to check if the snappy class is available for SnappyCompressor too.", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r489922981", "createdAt": "2020-09-17T03:16:50Z", "author": {"login": "dbtsai"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java", "diffHunk": "@@ -48,24 +48,6 @@\n   private long bytesRead = 0L;\n   private long bytesWritten = 0L;\n \n-  private static boolean nativeSnappyLoaded = false;\n-  \n-  static {\n-    if (NativeCodeLoader.isNativeCodeLoaded() &&\n-        NativeCodeLoader.buildSupportsSnappy()) {\n-      try {\n-        initIDs();\n-        nativeSnappyLoaded = true;\n-      } catch (Throwable t) {\n-        LOG.error(\"failed to load SnappyCompressor\", t);\n-      }\n-    }\n-  }\n-  \n-  public static boolean isNativeCodeLoaded() {\n-    return nativeSnappyLoaded;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ed518d238adfee81c821b74434785afb5684710"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU5Njc2MQ==", "bodyText": "added.", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r490596761", "createdAt": "2020-09-17T22:28:33Z", "author": {"login": "viirya"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java", "diffHunk": "@@ -48,24 +48,6 @@\n   private long bytesRead = 0L;\n   private long bytesWritten = 0L;\n \n-  private static boolean nativeSnappyLoaded = false;\n-  \n-  static {\n-    if (NativeCodeLoader.isNativeCodeLoaded() &&\n-        NativeCodeLoader.buildSupportsSnappy()) {\n-      try {\n-        initIDs();\n-        nativeSnappyLoaded = true;\n-      } catch (Throwable t) {\n-        LOG.error(\"failed to load SnappyCompressor\", t);\n-      }\n-    }\n-  }\n-  \n-  public static boolean isNativeCodeLoaded() {\n-    return nativeSnappyLoaded;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTkyMjk4MQ=="}, "originalCommit": {"oid": "0ed518d238adfee81c821b74434785afb5684710"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MjA3ODk1OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwNTo1MTozOFrOHXKqoA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwNjozNzoxN1rOHXLsKg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA1NDA0OA==", "bodyText": "Fix this last sentence if you make a new PR", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494054048", "createdAt": "2020-09-24T05:51:38Z", "author": {"login": "saintstack"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java", "diffHunk": "@@ -48,30 +49,20 @@\n   private long bytesRead = 0L;\n   private long bytesWritten = 0L;\n \n-  private static boolean nativeSnappyLoaded = false;\n-  \n-  static {\n-    if (NativeCodeLoader.isNativeCodeLoaded() &&\n-        NativeCodeLoader.buildSupportsSnappy()) {\n-      try {\n-        initIDs();\n-        nativeSnappyLoaded = true;\n-      } catch (Throwable t) {\n-        LOG.error(\"failed to load SnappyCompressor\", t);\n-      }\n-    }\n-  }\n-  \n-  public static boolean isNativeCodeLoaded() {\n-    return nativeSnappyLoaded;\n-  }\n-  \n   /**\n    * Creates a new compressor.\n    *\n    * @param directBufferSize size of the direct buffer to be used.\n    */\n   public SnappyCompressor(int directBufferSize) {\n+    // `snappy-java` is provided scope. We need to check if its availability.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "712749c041c012bb8eef41f826d1abc8da937a36"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA3MDgyNg==", "bodyText": "Oops, thanks.", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494070826", "createdAt": "2020-09-24T06:37:17Z", "author": {"login": "viirya"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java", "diffHunk": "@@ -48,30 +49,20 @@\n   private long bytesRead = 0L;\n   private long bytesWritten = 0L;\n \n-  private static boolean nativeSnappyLoaded = false;\n-  \n-  static {\n-    if (NativeCodeLoader.isNativeCodeLoaded() &&\n-        NativeCodeLoader.buildSupportsSnappy()) {\n-      try {\n-        initIDs();\n-        nativeSnappyLoaded = true;\n-      } catch (Throwable t) {\n-        LOG.error(\"failed to load SnappyCompressor\", t);\n-      }\n-    }\n-  }\n-  \n-  public static boolean isNativeCodeLoaded() {\n-    return nativeSnappyLoaded;\n-  }\n-  \n   /**\n    * Creates a new compressor.\n    *\n    * @param directBufferSize size of the direct buffer to be used.\n    */\n   public SnappyCompressor(int directBufferSize) {\n+    // `snappy-java` is provided scope. We need to check if its availability.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA1NDA0OA=="}, "originalCommit": {"oid": "712749c041c012bb8eef41f826d1abc8da937a36"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MjA4MTI1OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwNTo1Mjo0NFrOHXKr9Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwNjozNjoyNFrOHXLqqg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA1NDM4OQ==", "bodyText": "Is it the 'native snappy library' that is missing or the java-snappy jar?", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494054389", "createdAt": "2020-09-24T05:52:44Z", "author": {"login": "saintstack"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java", "diffHunk": "@@ -48,30 +49,20 @@\n   private long bytesRead = 0L;\n   private long bytesWritten = 0L;\n \n-  private static boolean nativeSnappyLoaded = false;\n-  \n-  static {\n-    if (NativeCodeLoader.isNativeCodeLoaded() &&\n-        NativeCodeLoader.buildSupportsSnappy()) {\n-      try {\n-        initIDs();\n-        nativeSnappyLoaded = true;\n-      } catch (Throwable t) {\n-        LOG.error(\"failed to load SnappyCompressor\", t);\n-      }\n-    }\n-  }\n-  \n-  public static boolean isNativeCodeLoaded() {\n-    return nativeSnappyLoaded;\n-  }\n-  \n   /**\n    * Creates a new compressor.\n    *\n    * @param directBufferSize size of the direct buffer to be used.\n    */\n   public SnappyCompressor(int directBufferSize) {\n+    // `snappy-java` is provided scope. We need to check if its availability.\n+    try {\n+      SnappyLoader.getVersion();\n+    } catch (Throwable t) {\n+      throw new RuntimeException(\"native snappy library not available: \" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "712749c041c012bb8eef41f826d1abc8da937a36"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA3MDQ0Mg==", "bodyText": "It is java-snappy jar, yeah, I will revise the message.", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494070442", "createdAt": "2020-09-24T06:36:24Z", "author": {"login": "viirya"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java", "diffHunk": "@@ -48,30 +49,20 @@\n   private long bytesRead = 0L;\n   private long bytesWritten = 0L;\n \n-  private static boolean nativeSnappyLoaded = false;\n-  \n-  static {\n-    if (NativeCodeLoader.isNativeCodeLoaded() &&\n-        NativeCodeLoader.buildSupportsSnappy()) {\n-      try {\n-        initIDs();\n-        nativeSnappyLoaded = true;\n-      } catch (Throwable t) {\n-        LOG.error(\"failed to load SnappyCompressor\", t);\n-      }\n-    }\n-  }\n-  \n-  public static boolean isNativeCodeLoaded() {\n-    return nativeSnappyLoaded;\n-  }\n-  \n   /**\n    * Creates a new compressor.\n    *\n    * @param directBufferSize size of the direct buffer to be used.\n    */\n   public SnappyCompressor(int directBufferSize) {\n+    // `snappy-java` is provided scope. We need to check if its availability.\n+    try {\n+      SnappyLoader.getVersion();\n+    } catch (Throwable t) {\n+      throw new RuntimeException(\"native snappy library not available: \" +", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA1NDM4OQ=="}, "originalCommit": {"oid": "712749c041c012bb8eef41f826d1abc8da937a36"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MjA4NzY2OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwNTo1NTo0MVrOHXKvsA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwNjozNTo0OFrOHXLprQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA1NTM0NA==", "bodyText": "s/compressBytesDirect/compressBytesDirectBuf/ ? Or.. why the Bytes... I see none referenced in the method so compressDirectBuf?", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494055344", "createdAt": "2020-09-24T05:55:41Z", "author": {"login": "saintstack"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java", "diffHunk": "@@ -291,9 +282,17 @@ public long getBytesWritten() {\n   public void end() {\n   }\n \n-  private native static void initIDs();\n-\n-  private native int compressBytesDirect();\n-\n-  public native static String getLibraryName();\n+  private int compressBytesDirect() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "712749c041c012bb8eef41f826d1abc8da937a36"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA3MDE4OQ==", "bodyText": "This compressBytesDirect and decompressBytesDirect basically are copied from original method names. compressDirectBuf and decompressDirectBuf looks good to me.", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494070189", "createdAt": "2020-09-24T06:35:48Z", "author": {"login": "viirya"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java", "diffHunk": "@@ -291,9 +282,17 @@ public long getBytesWritten() {\n   public void end() {\n   }\n \n-  private native static void initIDs();\n-\n-  private native int compressBytesDirect();\n-\n-  public native static String getLibraryName();\n+  private int compressBytesDirect() throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA1NTM0NA=="}, "originalCommit": {"oid": "712749c041c012bb8eef41f826d1abc8da937a36"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MjA5MTM5OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwNTo1NzoxNVrOHXKx3Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwNTo1NzoxNVrOHXKx3Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA1NTkwMQ==", "bodyText": "ditto... could this message be more informative: i.e. \"hey, operator... you need to add the snappy-java.jar to your CLASSPATH... its not packaged up for you..\"", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494055901", "createdAt": "2020-09-24T05:57:15Z", "author": {"login": "saintstack"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java", "diffHunk": "@@ -45,30 +46,20 @@\n   private int userBufOff = 0, userBufLen = 0;\n   private boolean finished;\n \n-  private static boolean nativeSnappyLoaded = false;\n-\n-  static {\n-    if (NativeCodeLoader.isNativeCodeLoaded() &&\n-        NativeCodeLoader.buildSupportsSnappy()) {\n-      try {\n-        initIDs();\n-        nativeSnappyLoaded = true;\n-      } catch (Throwable t) {\n-        LOG.error(\"failed to load SnappyDecompressor\", t);\n-      }\n-    }\n-  }\n-  \n-  public static boolean isNativeCodeLoaded() {\n-    return nativeSnappyLoaded;\n-  }\n-  \n   /**\n    * Creates a new compressor.\n    *\n    * @param directBufferSize size of the direct buffer to be used.\n    */\n   public SnappyDecompressor(int directBufferSize) {\n+    // `snappy-java` is provided scope. We need to check if its availability.\n+    try {\n+      SnappyLoader.getVersion();\n+    } catch (Throwable t) {\n+      throw new RuntimeException(\"native snappy library not available: \" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "712749c041c012bb8eef41f826d1abc8da937a36"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MjA5NDg1OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwNTo1OTowNFrOHXKz7Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwNTo1OTowNFrOHXKz7Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA1NjQyOQ==", "bodyText": "ditto", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494056429", "createdAt": "2020-09-24T05:59:04Z", "author": {"login": "saintstack"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java", "diffHunk": "@@ -276,10 +267,20 @@ public void end() {\n     // do nothing\n   }\n \n-  private native static void initIDs();\n+  private int decompressBytesDirect() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "712749c041c012bb8eef41f826d1abc8da937a36"}, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MjEwMjEwOnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/snappy/TestSnappyCompressorDecompressor.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwNjowMjoxOFrOHXK4CA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxODozNjozOVrOHXn2gg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA1NzQ4MA==", "bodyText": "hmm... this is a little anemic. Have you considered adding a data file that is a little more interesting than this?", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494057480", "createdAt": "2020-09-24T06:02:18Z", "author": {"login": "saintstack"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/snappy/TestSnappyCompressorDecompressor.java", "diffHunk": "@@ -446,4 +442,43 @@ public void doWork() throws Exception {\n \n     ctx.waitFor(60000);\n   }\n+\n+  @Test\n+  public void testSnappyCompatibility() throws Exception {\n+    // HADOOP-17125. Using snappy-java in SnappyCodec. These strings are raw data and compressed data\n+    // using previous native Snappy codec. We use updated Snappy codec to decode it and check if it\n+    // matches.\n+    String rawData = \"010a06030a040a0c0109020c0a010204020d02000b010701080605080b090902060a080502060a0d06070908080a0c0105030904090d05090800040c090c0d0d0804000d00040b0b0d010d060907020a030a0c0900040905080107040d0c01060a0b09070a04000b01040b09000e0e00020b06050b060e030e0a07050d06050d\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "712749c041c012bb8eef41f826d1abc8da937a36"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA2OTU4Ng==", "bodyText": "String is to make the test as simple as possible. Maybe further shorten the string?", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494069586", "createdAt": "2020-09-24T06:34:20Z", "author": {"login": "viirya"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/snappy/TestSnappyCompressorDecompressor.java", "diffHunk": "@@ -446,4 +442,43 @@ public void doWork() throws Exception {\n \n     ctx.waitFor(60000);\n   }\n+\n+  @Test\n+  public void testSnappyCompatibility() throws Exception {\n+    // HADOOP-17125. Using snappy-java in SnappyCodec. These strings are raw data and compressed data\n+    // using previous native Snappy codec. We use updated Snappy codec to decode it and check if it\n+    // matches.\n+    String rawData = \"010a06030a040a0c0109020c0a010204020d02000b010701080605080b090902060a080502060a0d06070908080a0c0105030904090d05090800040c090c0d0d0804000d00040b0b0d010d060907020a030a0c0900040905080107040d0c01060a0b09070a04000b01040b09000e0e00020b06050b060e030e0a07050d06050d\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA1NzQ4MA=="}, "originalCommit": {"oid": "712749c041c012bb8eef41f826d1abc8da937a36"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDI1Njg2Ng==", "bodyText": "should be split across lines, but otherwise fine inline -simpler for tests", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494256866", "createdAt": "2020-09-24T12:00:01Z", "author": {"login": "steveloughran"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/snappy/TestSnappyCompressorDecompressor.java", "diffHunk": "@@ -446,4 +442,43 @@ public void doWork() throws Exception {\n \n     ctx.waitFor(60000);\n   }\n+\n+  @Test\n+  public void testSnappyCompatibility() throws Exception {\n+    // HADOOP-17125. Using snappy-java in SnappyCodec. These strings are raw data and compressed data\n+    // using previous native Snappy codec. We use updated Snappy codec to decode it and check if it\n+    // matches.\n+    String rawData = \"010a06030a040a0c0109020c0a010204020d02000b010701080605080b090902060a080502060a0d06070908080a0c0105030904090d05090800040c090c0d0d0804000d00040b0b0d010d060907020a030a0c0900040905080107040d0c01060a0b09070a04000b01040b09000e0e00020b06050b060e030e0a07050d06050d\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA1NzQ4MA=="}, "originalCommit": {"oid": "712749c041c012bb8eef41f826d1abc8da937a36"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDUzMjIyNg==", "bodyText": "Ok, I split the long string. Thanks.", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494532226", "createdAt": "2020-09-24T18:36:39Z", "author": {"login": "viirya"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/snappy/TestSnappyCompressorDecompressor.java", "diffHunk": "@@ -446,4 +442,43 @@ public void doWork() throws Exception {\n \n     ctx.waitFor(60000);\n   }\n+\n+  @Test\n+  public void testSnappyCompatibility() throws Exception {\n+    // HADOOP-17125. Using snappy-java in SnappyCodec. These strings are raw data and compressed data\n+    // using previous native Snappy codec. We use updated Snappy codec to decode it and check if it\n+    // matches.\n+    String rawData = \"010a06030a040a0c0109020c0a010204020d02000b010701080605080b090902060a080502060a0d06070908080a0c0105030904090d05090800040c090c0d0d0804000d00040b0b0d010d060907020a030a0c0900040905080107040d0c01060a0b09070a04000b01040b09000e0e00020b06050b060e030e0a07050d06050d\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA1NzQ4MA=="}, "originalCommit": {"oid": "712749c041c012bb8eef41f826d1abc8da937a36"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5NTEzMjIyOnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxODo0NjowNlrOHXoLfQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxOToxMDowMlrOHXo_GQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDUzNzU5Nw==", "bodyText": "nit, uncompressedDirectBuf.limit(uncompressedDirectBuf.capacity()).position(0); for safety.", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494537597", "createdAt": "2020-09-24T18:46:06Z", "author": {"login": "dbtsai"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java", "diffHunk": "@@ -291,9 +283,17 @@ public long getBytesWritten() {\n   public void end() {\n   }\n \n-  private native static void initIDs();\n-\n-  private native int compressBytesDirect();\n-\n-  public native static String getLibraryName();\n+  private int compressDirectBuf() throws IOException {\n+    if (uncompressedDirectBufLen == 0) {\n+      return 0;\n+    } else {\n+      // Set the position and limit of `uncompressedDirectBuf` for reading\n+      uncompressedDirectBuf.limit(uncompressedDirectBufLen).position(0);\n+      int size = Snappy.compress((ByteBuffer) uncompressedDirectBuf,\n+              (ByteBuffer) compressedDirectBuf);\n+      uncompressedDirectBufLen = 0;\n+      uncompressedDirectBuf.limit(directBufferSize).position(0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2b5a2122d597c06373d242b0cbc3eed8d3fb7aa4"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDU1MDgwOQ==", "bodyText": "done. thanks.", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494550809", "createdAt": "2020-09-24T19:10:02Z", "author": {"login": "viirya"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java", "diffHunk": "@@ -291,9 +283,17 @@ public long getBytesWritten() {\n   public void end() {\n   }\n \n-  private native static void initIDs();\n-\n-  private native int compressBytesDirect();\n-\n-  public native static String getLibraryName();\n+  private int compressDirectBuf() throws IOException {\n+    if (uncompressedDirectBufLen == 0) {\n+      return 0;\n+    } else {\n+      // Set the position and limit of `uncompressedDirectBuf` for reading\n+      uncompressedDirectBuf.limit(uncompressedDirectBufLen).position(0);\n+      int size = Snappy.compress((ByteBuffer) uncompressedDirectBuf,\n+              (ByteBuffer) compressedDirectBuf);\n+      uncompressedDirectBufLen = 0;\n+      uncompressedDirectBuf.limit(directBufferSize).position(0);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDUzNzU5Nw=="}, "originalCommit": {"oid": "2b5a2122d597c06373d242b0cbc3eed8d3fb7aa4"}, "originalPosition": 79}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5NTE0MTMwOnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/CompressDecompressTester.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxODo0ODozOFrOHXoRBg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxNjo0ODoyMFrOHcle5g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDUzOTAxNA==", "bodyText": "Why is this change needed?", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494539014", "createdAt": "2020-09-24T18:48:38Z", "author": {"login": "dbtsai"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/CompressDecompressTester.java", "diffHunk": "@@ -432,7 +412,11 @@ public void assertCompression(String name, Compressor compressor,\n               joiner.join(name, \"byte arrays not equals error !!!\"),\n               originalRawData, decompressOut.toByteArray());\n         } catch (Exception ex) {\n-          fail(joiner.join(name, ex.getMessage()));\n+          if (ex.getMessage() != null) {\n+            fail(joiner.join(name, ex.getMessage()));\n+          } else {\n+            fail(joiner.join(name, ExceptionUtils.getStackTrace(ex)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2b5a2122d597c06373d242b0cbc3eed8d3fb7aa4"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDU0MjI0OQ==", "bodyText": "When I first took over this change, the test failed with NPE without any details. It is because the exception thrown returns null from getMessage(). joiner.join(name, null) causes the NPE, so I changed it to print stack trace once getMessage() returns null. It's better for debugging.", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494542249", "createdAt": "2020-09-24T18:54:24Z", "author": {"login": "viirya"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/CompressDecompressTester.java", "diffHunk": "@@ -432,7 +412,11 @@ public void assertCompression(String name, Compressor compressor,\n               joiner.join(name, \"byte arrays not equals error !!!\"),\n               originalRawData, decompressOut.toByteArray());\n         } catch (Exception ex) {\n-          fail(joiner.join(name, ex.getMessage()));\n+          if (ex.getMessage() != null) {\n+            fail(joiner.join(name, ex.getMessage()));\n+          } else {\n+            fail(joiner.join(name, ExceptionUtils.getStackTrace(ex)));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDUzOTAxNA=="}, "originalCommit": {"oid": "2b5a2122d597c06373d242b0cbc3eed8d3fb7aa4"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTczNjI5NA==", "bodyText": "NPE is why toString() is what new code should do.\nWhy don't we just throw new AssertionError(name +ex, ex). That way, the stack trace doesn't get lost, which is something we never want to have happen,", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r499736294", "createdAt": "2020-10-05T16:48:20Z", "author": {"login": "steveloughran"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/CompressDecompressTester.java", "diffHunk": "@@ -432,7 +412,11 @@ public void assertCompression(String name, Compressor compressor,\n               joiner.join(name, \"byte arrays not equals error !!!\"),\n               originalRawData, decompressOut.toByteArray());\n         } catch (Exception ex) {\n-          fail(joiner.join(name, ex.getMessage()));\n+          if (ex.getMessage() != null) {\n+            fail(joiner.join(name, ex.getMessage()));\n+          } else {\n+            fail(joiner.join(name, ExceptionUtils.getStackTrace(ex)));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDUzOTAxNA=="}, "originalCommit": {"oid": "2b5a2122d597c06373d242b0cbc3eed8d3fb7aa4"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5OTE2MTI0OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxODoyMjowN1rOHYOEWg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxOToxMzo1N1rOHYPlVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE1ODM2Mg==", "bodyText": "nit: this seems unnecessary as clear is called shortly after at the call site?", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r495158362", "createdAt": "2020-09-25T18:22:07Z", "author": {"login": "sunchao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java", "diffHunk": "@@ -291,9 +283,17 @@ public long getBytesWritten() {\n   public void end() {\n   }\n \n-  private native static void initIDs();\n-\n-  private native int compressBytesDirect();\n-\n-  public native static String getLibraryName();\n+  private int compressDirectBuf() throws IOException {\n+    if (uncompressedDirectBufLen == 0) {\n+      return 0;\n+    } else {\n+      // Set the position and limit of `uncompressedDirectBuf` for reading\n+      uncompressedDirectBuf.limit(uncompressedDirectBufLen).position(0);\n+      int size = Snappy.compress((ByteBuffer) uncompressedDirectBuf,\n+              (ByteBuffer) compressedDirectBuf);\n+      uncompressedDirectBufLen = 0;\n+      uncompressedDirectBuf.limit(uncompressedDirectBuf.capacity()).position(0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1cb398bbbaf702501f558ce32cda07d1ca7917ca"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE4MzE5MQ==", "bodyText": "Seems so, I remember I added this to fix test failure. It might be SnappyDecompressor, I think, then I copied to SnappyCompressor. Deleted this and see what Jenkins tells.", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r495183191", "createdAt": "2020-09-25T19:13:57Z", "author": {"login": "viirya"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java", "diffHunk": "@@ -291,9 +283,17 @@ public long getBytesWritten() {\n   public void end() {\n   }\n \n-  private native static void initIDs();\n-\n-  private native int compressBytesDirect();\n-\n-  public native static String getLibraryName();\n+  private int compressDirectBuf() throws IOException {\n+    if (uncompressedDirectBufLen == 0) {\n+      return 0;\n+    } else {\n+      // Set the position and limit of `uncompressedDirectBuf` for reading\n+      uncompressedDirectBuf.limit(uncompressedDirectBufLen).position(0);\n+      int size = Snappy.compress((ByteBuffer) uncompressedDirectBuf,\n+              (ByteBuffer) compressedDirectBuf);\n+      uncompressedDirectBufLen = 0;\n+      uncompressedDirectBuf.limit(uncompressedDirectBuf.capacity()).position(0);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE1ODM2Mg=="}, "originalCommit": {"oid": "1cb398bbbaf702501f558ce32cda07d1ca7917ca"}, "originalPosition": 79}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5OTE2MzEzOnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxODoyMjo0MVrOHYOFfw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxOToxMTo1NVrOHYPh_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE1ODY1NQ==", "bodyText": "nit: SnappyLoader is marked as \"internal use-only\" though so not sure if there is better alternative here.", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r495158655", "createdAt": "2020-09-25T18:22:41Z", "author": {"login": "sunchao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java", "diffHunk": "@@ -45,30 +46,21 @@\n   private int userBufOff = 0, userBufLen = 0;\n   private boolean finished;\n \n-  private static boolean nativeSnappyLoaded = false;\n-\n-  static {\n-    if (NativeCodeLoader.isNativeCodeLoaded() &&\n-        NativeCodeLoader.buildSupportsSnappy()) {\n-      try {\n-        initIDs();\n-        nativeSnappyLoaded = true;\n-      } catch (Throwable t) {\n-        LOG.error(\"failed to load SnappyDecompressor\", t);\n-      }\n-    }\n-  }\n-  \n-  public static boolean isNativeCodeLoaded() {\n-    return nativeSnappyLoaded;\n-  }\n-  \n   /**\n    * Creates a new compressor.\n    *\n    * @param directBufferSize size of the direct buffer to be used.\n    */\n   public SnappyDecompressor(int directBufferSize) {\n+    // `snappy-java` is provided scope. We need to check if it is available.\n+    try {\n+      SnappyLoader.getVersion();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1cb398bbbaf702501f558ce32cda07d1ca7917ca"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE4MjMzMg==", "bodyText": "The \"internal user-only\" of SnappyLoader, based on its comment, seems more related to native library loading stuff.\ngetVersion is static method and it doesn't involve loading of native library described in SnappyLoader, so I guess it is fine? Otherwise, I don't find other proper one to check.", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r495182332", "createdAt": "2020-09-25T19:11:55Z", "author": {"login": "viirya"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java", "diffHunk": "@@ -45,30 +46,21 @@\n   private int userBufOff = 0, userBufLen = 0;\n   private boolean finished;\n \n-  private static boolean nativeSnappyLoaded = false;\n-\n-  static {\n-    if (NativeCodeLoader.isNativeCodeLoaded() &&\n-        NativeCodeLoader.buildSupportsSnappy()) {\n-      try {\n-        initIDs();\n-        nativeSnappyLoaded = true;\n-      } catch (Throwable t) {\n-        LOG.error(\"failed to load SnappyDecompressor\", t);\n-      }\n-    }\n-  }\n-  \n-  public static boolean isNativeCodeLoaded() {\n-    return nativeSnappyLoaded;\n-  }\n-  \n   /**\n    * Creates a new compressor.\n    *\n    * @param directBufferSize size of the direct buffer to be used.\n    */\n   public SnappyDecompressor(int directBufferSize) {\n+    // `snappy-java` is provided scope. We need to check if it is available.\n+    try {\n+      SnappyLoader.getVersion();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE1ODY1NQ=="}, "originalCommit": {"oid": "1cb398bbbaf702501f558ce32cda07d1ca7917ca"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5OTE5NjAyOnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxODozMzowNFrOHYOZyg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxOToxNDoxMVrOHYPlug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE2Mzg1MA==", "bodyText": "nit: can we just call compressedDirectBuf.clear()?", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r495163850", "createdAt": "2020-09-25T18:33:04Z", "author": {"login": "sunchao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java", "diffHunk": "@@ -276,10 +268,20 @@ public void end() {\n     // do nothing\n   }\n \n-  private native static void initIDs();\n+  private int decompressDirectBuf() throws IOException {\n+    if (compressedDirectBufLen == 0) {\n+      return 0;\n+    } else {\n+      // Set the position and limit of `compressedDirectBuf` for reading\n+      compressedDirectBuf.limit(compressedDirectBufLen).position(0);\n+      int size = Snappy.uncompress((ByteBuffer) compressedDirectBuf,\n+              (ByteBuffer) uncompressedDirectBuf);\n+      compressedDirectBufLen = 0;\n+      compressedDirectBuf.limit(compressedDirectBuf.capacity()).position(0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1cb398bbbaf702501f558ce32cda07d1ca7917ca"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE4MzI5MA==", "bodyText": "yap", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r495183290", "createdAt": "2020-09-25T19:14:11Z", "author": {"login": "viirya"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java", "diffHunk": "@@ -276,10 +268,20 @@ public void end() {\n     // do nothing\n   }\n \n-  private native static void initIDs();\n+  private int decompressDirectBuf() throws IOException {\n+    if (compressedDirectBufLen == 0) {\n+      return 0;\n+    } else {\n+      // Set the position and limit of `compressedDirectBuf` for reading\n+      compressedDirectBuf.limit(compressedDirectBufLen).position(0);\n+      int size = Snappy.uncompress((ByteBuffer) compressedDirectBuf,\n+              (ByteBuffer) uncompressedDirectBuf);\n+      compressedDirectBufLen = 0;\n+      compressedDirectBuf.limit(compressedDirectBuf.capacity()).position(0);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE2Mzg1MA=="}, "originalCommit": {"oid": "1cb398bbbaf702501f558ce32cda07d1ca7917ca"}, "originalPosition": 84}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5OTIwNTY3OnYy", "diffSide": "LEFT", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/CompressDecompressTester.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxODozNjoxM1rOHYOf1g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxOTozNjo0M1rOHYQM-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE2NTM5OA==", "bodyText": "nit: unrelated changes :)", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r495165398", "createdAt": "2020-09-25T18:36:13Z", "author": {"login": "sunchao"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/CompressDecompressTester.java", "diffHunk": "@@ -495,19 +479,16 @@ public String getName() {\n     Compressor compressor = pair.compressor;\n \n     if (compressor.getClass().isAssignableFrom(Lz4Compressor.class)\n-            && (NativeCodeLoader.isNativeCodeLoaded()))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1cb398bbbaf702501f558ce32cda07d1ca7917ca"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE3NTkwOA==", "bodyText": "Oh, this is from @dbtsai's original change. I think adding curly brackets is better? I can revert this if you think it is necessary.", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r495175908", "createdAt": "2020-09-25T18:58:13Z", "author": {"login": "viirya"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/CompressDecompressTester.java", "diffHunk": "@@ -495,19 +479,16 @@ public String getName() {\n     Compressor compressor = pair.compressor;\n \n     if (compressor.getClass().isAssignableFrom(Lz4Compressor.class)\n-            && (NativeCodeLoader.isNativeCodeLoaded()))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE2NTM5OA=="}, "originalCommit": {"oid": "1cb398bbbaf702501f558ce32cda07d1ca7917ca"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE3NjkwOA==", "bodyText": "Yeah usually it's not recommended to include unrelated changes in Hadoop patch, we may add another refactoring PR later if this is absolutely necessary.", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r495176908", "createdAt": "2020-09-25T19:00:23Z", "author": {"login": "sunchao"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/CompressDecompressTester.java", "diffHunk": "@@ -495,19 +479,16 @@ public String getName() {\n     Compressor compressor = pair.compressor;\n \n     if (compressor.getClass().isAssignableFrom(Lz4Compressor.class)\n-            && (NativeCodeLoader.isNativeCodeLoaded()))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE2NTM5OA=="}, "originalCommit": {"oid": "1cb398bbbaf702501f558ce32cda07d1ca7917ca"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE5MzMzOA==", "bodyText": "Ok. Reverted the change.", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r495193338", "createdAt": "2020-09-25T19:36:43Z", "author": {"login": "viirya"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/CompressDecompressTester.java", "diffHunk": "@@ -495,19 +479,16 @@ public String getName() {\n     Compressor compressor = pair.compressor;\n \n     if (compressor.getClass().isAssignableFrom(Lz4Compressor.class)\n-            && (NativeCodeLoader.isNativeCodeLoaded()))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE2NTM5OA=="}, "originalCommit": {"oid": "1cb398bbbaf702501f558ce32cda07d1ca7917ca"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5OTIwOTAxOnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/snappy/TestSnappyCompressorDecompressor.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxODozNzoxNlrOHYOh3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxODozNzoxNlrOHYOh3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE2NTkxOQ==", "bodyText": "nit: long lines (80 chars).", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r495165919", "createdAt": "2020-09-25T18:37:16Z", "author": {"login": "sunchao"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/snappy/TestSnappyCompressorDecompressor.java", "diffHunk": "@@ -446,4 +442,49 @@ public void doWork() throws Exception {\n \n     ctx.waitFor(60000);\n   }\n+\n+  @Test\n+  public void testSnappyCompatibility() throws Exception {\n+    // HADOOP-17125. Using snappy-java in SnappyCodec. These strings are raw data and compressed data\n+    // using previous native Snappy codec. We use updated Snappy codec to decode it and check if it\n+    // matches.\n+    String rawData = \"010a06030a040a0c0109020c0a010204020d02000b010701080605080b090902060a08050206\" +\n+            \"0a0d06070908080a0c0105030904090d05090800040c090c0d0d0804000d00040b0b0d010d060907020a0\" +\n+            \"30a0c0900040905080107040d0c01060a0b09070a04000b01040b09000e0e00020b06050b060e030e0a07\" +\n+            \"050d06050d\";\n+    String compressed = \"8001f07f010a06030a040a0c0109020c0a010204020d02000b010701080605080b0909020\" +\n+            \"60a080502060a0d06070908080a0c0105030904090d05090800040c090c0d0d0804000d00040b0b0d010d\" +\n+            \"060907020a030a0c0900040905080107040d0c01060a0b09070a04000b01040b09000e0e00020b06050b0\" +\n+            \"60e030e0a07050d06050d\";\n+\n+    byte[] rawDataBytes = Hex.decodeHex(rawData);\n+    byte[] compressedBytes = Hex.decodeHex(compressed);\n+\n+    ByteBuffer inBuf = ByteBuffer.allocateDirect(compressedBytes.length);\n+    inBuf.put(compressedBytes, 0, compressedBytes.length);\n+    inBuf.flip();\n+\n+    ByteBuffer outBuf = ByteBuffer.allocateDirect(rawDataBytes.length);\n+    ByteBuffer expected = ByteBuffer.wrap(rawDataBytes);\n+\n+    SnappyDecompressor.SnappyDirectDecompressor decompressor = new SnappyDecompressor.SnappyDirectDecompressor();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1cb398bbbaf702501f558ce32cda07d1ca7917ca"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExMTA5NTA2OnYy", "diffSide": "LEFT", "path": "hadoop-common-project/hadoop-common/src/main/native/native.vcxproj", "isResolved": false, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxODo0NToyOFrOHZ8EMw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMTo0NDowMFrOHaDPVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njk2MDU2Mw==", "bodyText": "We shouldn't remove this", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r496960563", "createdAt": "2020-09-29T18:45:28Z", "author": {"login": "sunchao"}, "path": "hadoop-common-project/hadoop-common/src/main/native/native.vcxproj", "diffHunk": "@@ -68,30 +68,13 @@\n     <IntDir>..\\..\\..\\target\\native\\$(Configuration)\\</IntDir>\n     <TargetName>hadoop</TargetName>\n   </PropertyGroup>\n-  <PropertyGroup>\n-    <SnappyLib Condition=\"Exists('$(CustomSnappyPrefix)\\snappy.dll')\">$(CustomSnappyPrefix)</SnappyLib>\n-    <SnappyLib Condition=\"Exists('$(CustomSnappyPrefix)\\lib\\snappy.dll') And '$(SnappyLib)' == ''\">$(CustomSnappyPrefix)\\lib</SnappyLib>\n-    <SnappyLib Condition=\"Exists('$(CustomSnappyPrefix)\\bin\\snappy.dll') And '$(SnappyLib)' == ''\">$(CustomSnappyPrefix)\\bin</SnappyLib>\n-    <SnappyLib Condition=\"Exists('$(CustomSnappyLib)') And '$(SnappyLib)' == ''\">$(CustomSnappyLib)</SnappyLib>\n-    <SnappyInclude Condition=\"Exists('$(CustomSnappyPrefix)\\snappy.h')\">$(CustomSnappyPrefix)</SnappyInclude>\n-    <SnappyInclude Condition=\"Exists('$(CustomSnappyPrefix)\\include\\snappy.h') And '$(SnappyInclude)' == ''\">$(CustomSnappyPrefix)\\include</SnappyInclude>\n-    <SnappyInclude Condition=\"Exists('$(CustomSnappyInclude)') And '$(SnappyInclude)' == ''\">$(CustomSnappyInclude)</SnappyInclude>\n-    <SnappyEnabled Condition=\"'$(SnappyLib)' != '' And '$(SnappyInclude)' != ''\">true</SnappyEnabled>\n-    <IncludePath Condition=\"'$(SnappyEnabled)' == 'true'\">$(SnappyInclude);$(IncludePath)</IncludePath>\n-    <IncludePath Condition=\"Exists('$(ZLIB_HOME)')\">$(ZLIB_HOME);$(IncludePath)</IncludePath>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fc525faf79b68662f452fc0cd8233194ef9f4555"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njk5Nzc3NA==", "bodyText": "Since we remove snappy native code, why do we need to keep this on windows?", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r496997774", "createdAt": "2020-09-29T19:39:01Z", "author": {"login": "viirya"}, "path": "hadoop-common-project/hadoop-common/src/main/native/native.vcxproj", "diffHunk": "@@ -68,30 +68,13 @@\n     <IntDir>..\\..\\..\\target\\native\\$(Configuration)\\</IntDir>\n     <TargetName>hadoop</TargetName>\n   </PropertyGroup>\n-  <PropertyGroup>\n-    <SnappyLib Condition=\"Exists('$(CustomSnappyPrefix)\\snappy.dll')\">$(CustomSnappyPrefix)</SnappyLib>\n-    <SnappyLib Condition=\"Exists('$(CustomSnappyPrefix)\\lib\\snappy.dll') And '$(SnappyLib)' == ''\">$(CustomSnappyPrefix)\\lib</SnappyLib>\n-    <SnappyLib Condition=\"Exists('$(CustomSnappyPrefix)\\bin\\snappy.dll') And '$(SnappyLib)' == ''\">$(CustomSnappyPrefix)\\bin</SnappyLib>\n-    <SnappyLib Condition=\"Exists('$(CustomSnappyLib)') And '$(SnappyLib)' == ''\">$(CustomSnappyLib)</SnappyLib>\n-    <SnappyInclude Condition=\"Exists('$(CustomSnappyPrefix)\\snappy.h')\">$(CustomSnappyPrefix)</SnappyInclude>\n-    <SnappyInclude Condition=\"Exists('$(CustomSnappyPrefix)\\include\\snappy.h') And '$(SnappyInclude)' == ''\">$(CustomSnappyPrefix)\\include</SnappyInclude>\n-    <SnappyInclude Condition=\"Exists('$(CustomSnappyInclude)') And '$(SnappyInclude)' == ''\">$(CustomSnappyInclude)</SnappyInclude>\n-    <SnappyEnabled Condition=\"'$(SnappyLib)' != '' And '$(SnappyInclude)' != ''\">true</SnappyEnabled>\n-    <IncludePath Condition=\"'$(SnappyEnabled)' == 'true'\">$(SnappyInclude);$(IncludePath)</IncludePath>\n-    <IncludePath Condition=\"Exists('$(ZLIB_HOME)')\">$(ZLIB_HOME);$(IncludePath)</IncludePath>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njk2MDU2Mw=="}, "originalCommit": {"oid": "fc525faf79b68662f452fc0cd8233194ef9f4555"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAwOTAyNw==", "bodyText": "I mean the last line, which is about ZLIB.", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r497009027", "createdAt": "2020-09-29T20:00:04Z", "author": {"login": "sunchao"}, "path": "hadoop-common-project/hadoop-common/src/main/native/native.vcxproj", "diffHunk": "@@ -68,30 +68,13 @@\n     <IntDir>..\\..\\..\\target\\native\\$(Configuration)\\</IntDir>\n     <TargetName>hadoop</TargetName>\n   </PropertyGroup>\n-  <PropertyGroup>\n-    <SnappyLib Condition=\"Exists('$(CustomSnappyPrefix)\\snappy.dll')\">$(CustomSnappyPrefix)</SnappyLib>\n-    <SnappyLib Condition=\"Exists('$(CustomSnappyPrefix)\\lib\\snappy.dll') And '$(SnappyLib)' == ''\">$(CustomSnappyPrefix)\\lib</SnappyLib>\n-    <SnappyLib Condition=\"Exists('$(CustomSnappyPrefix)\\bin\\snappy.dll') And '$(SnappyLib)' == ''\">$(CustomSnappyPrefix)\\bin</SnappyLib>\n-    <SnappyLib Condition=\"Exists('$(CustomSnappyLib)') And '$(SnappyLib)' == ''\">$(CustomSnappyLib)</SnappyLib>\n-    <SnappyInclude Condition=\"Exists('$(CustomSnappyPrefix)\\snappy.h')\">$(CustomSnappyPrefix)</SnappyInclude>\n-    <SnappyInclude Condition=\"Exists('$(CustomSnappyPrefix)\\include\\snappy.h') And '$(SnappyInclude)' == ''\">$(CustomSnappyPrefix)\\include</SnappyInclude>\n-    <SnappyInclude Condition=\"Exists('$(CustomSnappyInclude)') And '$(SnappyInclude)' == ''\">$(CustomSnappyInclude)</SnappyInclude>\n-    <SnappyEnabled Condition=\"'$(SnappyLib)' != '' And '$(SnappyInclude)' != ''\">true</SnappyEnabled>\n-    <IncludePath Condition=\"'$(SnappyEnabled)' == 'true'\">$(SnappyInclude);$(IncludePath)</IncludePath>\n-    <IncludePath Condition=\"Exists('$(ZLIB_HOME)')\">$(ZLIB_HOME);$(IncludePath)</IncludePath>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njk2MDU2Mw=="}, "originalCommit": {"oid": "fc525faf79b68662f452fc0cd8233194ef9f4555"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA2ODMyNg==", "bodyText": "I think it is together with snappy stuffs here, no? They are in same PropertyGroup. I think it is used to add zlib home into include paths in the property group.", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r497068326", "createdAt": "2020-09-29T21:23:52Z", "author": {"login": "viirya"}, "path": "hadoop-common-project/hadoop-common/src/main/native/native.vcxproj", "diffHunk": "@@ -68,30 +68,13 @@\n     <IntDir>..\\..\\..\\target\\native\\$(Configuration)\\</IntDir>\n     <TargetName>hadoop</TargetName>\n   </PropertyGroup>\n-  <PropertyGroup>\n-    <SnappyLib Condition=\"Exists('$(CustomSnappyPrefix)\\snappy.dll')\">$(CustomSnappyPrefix)</SnappyLib>\n-    <SnappyLib Condition=\"Exists('$(CustomSnappyPrefix)\\lib\\snappy.dll') And '$(SnappyLib)' == ''\">$(CustomSnappyPrefix)\\lib</SnappyLib>\n-    <SnappyLib Condition=\"Exists('$(CustomSnappyPrefix)\\bin\\snappy.dll') And '$(SnappyLib)' == ''\">$(CustomSnappyPrefix)\\bin</SnappyLib>\n-    <SnappyLib Condition=\"Exists('$(CustomSnappyLib)') And '$(SnappyLib)' == ''\">$(CustomSnappyLib)</SnappyLib>\n-    <SnappyInclude Condition=\"Exists('$(CustomSnappyPrefix)\\snappy.h')\">$(CustomSnappyPrefix)</SnappyInclude>\n-    <SnappyInclude Condition=\"Exists('$(CustomSnappyPrefix)\\include\\snappy.h') And '$(SnappyInclude)' == ''\">$(CustomSnappyPrefix)\\include</SnappyInclude>\n-    <SnappyInclude Condition=\"Exists('$(CustomSnappyInclude)') And '$(SnappyInclude)' == ''\">$(CustomSnappyInclude)</SnappyInclude>\n-    <SnappyEnabled Condition=\"'$(SnappyLib)' != '' And '$(SnappyInclude)' != ''\">true</SnappyEnabled>\n-    <IncludePath Condition=\"'$(SnappyEnabled)' == 'true'\">$(SnappyInclude);$(IncludePath)</IncludePath>\n-    <IncludePath Condition=\"Exists('$(ZLIB_HOME)')\">$(ZLIB_HOME);$(IncludePath)</IncludePath>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njk2MDU2Mw=="}, "originalCommit": {"oid": "fc525faf79b68662f452fc0cd8233194ef9f4555"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA3MzI3OA==", "bodyText": "I think this is for Zlib compressor (see https://issues.apache.org/jira/browse/HADOOP-10450). Yeah it is a bit confusing that it's defined in the same group.", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r497073278", "createdAt": "2020-09-29T21:33:27Z", "author": {"login": "sunchao"}, "path": "hadoop-common-project/hadoop-common/src/main/native/native.vcxproj", "diffHunk": "@@ -68,30 +68,13 @@\n     <IntDir>..\\..\\..\\target\\native\\$(Configuration)\\</IntDir>\n     <TargetName>hadoop</TargetName>\n   </PropertyGroup>\n-  <PropertyGroup>\n-    <SnappyLib Condition=\"Exists('$(CustomSnappyPrefix)\\snappy.dll')\">$(CustomSnappyPrefix)</SnappyLib>\n-    <SnappyLib Condition=\"Exists('$(CustomSnappyPrefix)\\lib\\snappy.dll') And '$(SnappyLib)' == ''\">$(CustomSnappyPrefix)\\lib</SnappyLib>\n-    <SnappyLib Condition=\"Exists('$(CustomSnappyPrefix)\\bin\\snappy.dll') And '$(SnappyLib)' == ''\">$(CustomSnappyPrefix)\\bin</SnappyLib>\n-    <SnappyLib Condition=\"Exists('$(CustomSnappyLib)') And '$(SnappyLib)' == ''\">$(CustomSnappyLib)</SnappyLib>\n-    <SnappyInclude Condition=\"Exists('$(CustomSnappyPrefix)\\snappy.h')\">$(CustomSnappyPrefix)</SnappyInclude>\n-    <SnappyInclude Condition=\"Exists('$(CustomSnappyPrefix)\\include\\snappy.h') And '$(SnappyInclude)' == ''\">$(CustomSnappyPrefix)\\include</SnappyInclude>\n-    <SnappyInclude Condition=\"Exists('$(CustomSnappyInclude)') And '$(SnappyInclude)' == ''\">$(CustomSnappyInclude)</SnappyInclude>\n-    <SnappyEnabled Condition=\"'$(SnappyLib)' != '' And '$(SnappyInclude)' != ''\">true</SnappyEnabled>\n-    <IncludePath Condition=\"'$(SnappyEnabled)' == 'true'\">$(SnappyInclude);$(IncludePath)</IncludePath>\n-    <IncludePath Condition=\"Exists('$(ZLIB_HOME)')\">$(ZLIB_HOME);$(IncludePath)</IncludePath>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njk2MDU2Mw=="}, "originalCommit": {"oid": "fc525faf79b68662f452fc0cd8233194ef9f4555"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA3ODEwMw==", "bodyText": "Oh, yeah, will add it back.", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r497078103", "createdAt": "2020-09-29T21:44:00Z", "author": {"login": "viirya"}, "path": "hadoop-common-project/hadoop-common/src/main/native/native.vcxproj", "diffHunk": "@@ -68,30 +68,13 @@\n     <IntDir>..\\..\\..\\target\\native\\$(Configuration)\\</IntDir>\n     <TargetName>hadoop</TargetName>\n   </PropertyGroup>\n-  <PropertyGroup>\n-    <SnappyLib Condition=\"Exists('$(CustomSnappyPrefix)\\snappy.dll')\">$(CustomSnappyPrefix)</SnappyLib>\n-    <SnappyLib Condition=\"Exists('$(CustomSnappyPrefix)\\lib\\snappy.dll') And '$(SnappyLib)' == ''\">$(CustomSnappyPrefix)\\lib</SnappyLib>\n-    <SnappyLib Condition=\"Exists('$(CustomSnappyPrefix)\\bin\\snappy.dll') And '$(SnappyLib)' == ''\">$(CustomSnappyPrefix)\\bin</SnappyLib>\n-    <SnappyLib Condition=\"Exists('$(CustomSnappyLib)') And '$(SnappyLib)' == ''\">$(CustomSnappyLib)</SnappyLib>\n-    <SnappyInclude Condition=\"Exists('$(CustomSnappyPrefix)\\snappy.h')\">$(CustomSnappyPrefix)</SnappyInclude>\n-    <SnappyInclude Condition=\"Exists('$(CustomSnappyPrefix)\\include\\snappy.h') And '$(SnappyInclude)' == ''\">$(CustomSnappyPrefix)\\include</SnappyInclude>\n-    <SnappyInclude Condition=\"Exists('$(CustomSnappyInclude)') And '$(SnappyInclude)' == ''\">$(CustomSnappyInclude)</SnappyInclude>\n-    <SnappyEnabled Condition=\"'$(SnappyLib)' != '' And '$(SnappyInclude)' != ''\">true</SnappyEnabled>\n-    <IncludePath Condition=\"'$(SnappyEnabled)' == 'true'\">$(SnappyInclude);$(IncludePath)</IncludePath>\n-    <IncludePath Condition=\"Exists('$(ZLIB_HOME)')\">$(ZLIB_HOME);$(IncludePath)</IncludePath>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njk2MDU2Mw=="}, "originalCommit": {"oid": "fc525faf79b68662f452fc0cd8233194ef9f4555"}, "originalPosition": 23}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3284, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}