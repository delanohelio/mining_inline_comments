{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDMzNzk5NzI3", "number": 2072, "reviewThreads": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQxNjoyNzowNlrOEFVSjw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQwNzozNzo1N1rOEIpXiw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0MDI3MTUxOnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQxNjoyNzowNlrOGjeFCQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxOTowOToxM1rOGn24jg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0NjE1Mw==", "bodyText": "nit: the 2 if here can be combined with &&", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r439846153", "createdAt": "2020-06-14T16:27:06Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "diffHunk": "@@ -1314,7 +1352,30 @@ private String convertXmsPropertiesToCommaSeparatedString(final Hashtable<String\n     return properties;\n   }\n \n+  private boolean isKeyForPrefixSet(String key, Set<String> dirSet) {\n+\n+    for (String dir : dirSet) {\n+      if (dir.isEmpty() || key.startsWith(dir)) {\n+        return true;\n+      }\n+\n+      try {\n+        URI uri = new URI(dir);\n+        if (null == uri.getAuthority()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d1f1fbf0ef8588d0aebc8441e2a4784508042f12"}, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ0Njg2Mg==", "bodyText": "code is removed.", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r444446862", "createdAt": "2020-06-23T19:09:13Z", "author": {"login": "ishaniahuja"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "diffHunk": "@@ -1314,7 +1352,30 @@ private String convertXmsPropertiesToCommaSeparatedString(final Hashtable<String\n     return properties;\n   }\n \n+  private boolean isKeyForPrefixSet(String key, Set<String> dirSet) {\n+\n+    for (String dir : dirSet) {\n+      if (dir.isEmpty() || key.startsWith(dir)) {\n+        return true;\n+      }\n+\n+      try {\n+        URI uri = new URI(dir);\n+        if (null == uri.getAuthority()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0NjE1Mw=="}, "originalCommit": {"oid": "d1f1fbf0ef8588d0aebc8441e2a4784508042f12"}, "originalPosition": 127}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0MDI3MjAzOnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQxNjoyNzo0M1rOGjeFRQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQxNjoyNzo0M1rOGjeFRQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0NjIxMw==", "bodyText": "nit: Use the constant for forward slash", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r439846213", "createdAt": "2020-06-14T16:27:43Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "diffHunk": "@@ -1314,7 +1352,30 @@ private String convertXmsPropertiesToCommaSeparatedString(final Hashtable<String\n     return properties;\n   }\n \n+  private boolean isKeyForPrefixSet(String key, Set<String> dirSet) {\n+\n+    for (String dir : dirSet) {\n+      if (dir.isEmpty() || key.startsWith(dir)) {\n+        return true;\n+      }\n+\n+      try {\n+        URI uri = new URI(dir);\n+        if (null == uri.getAuthority()) {\n+          if (key.startsWith(dir + \"/\")){", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d1f1fbf0ef8588d0aebc8441e2a4784508042f12"}, "originalPosition": 128}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0MDI4MTg0OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQxNjo0NDowOFrOGjeKWw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQxMDo1OTo1N1rOGjtUnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0NzUxNQ==", "bodyText": "How about throwing an AzureBlobFileSystemException from here. So that the customer will get to know that the config is not correct.", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r439847515", "createdAt": "2020-06-14T16:44:08Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "diffHunk": "@@ -1314,7 +1352,30 @@ private String convertXmsPropertiesToCommaSeparatedString(final Hashtable<String\n     return properties;\n   }\n \n+  private boolean isKeyForPrefixSet(String key, Set<String> dirSet) {\n+\n+    for (String dir : dirSet) {\n+      if (dir.isEmpty() || key.startsWith(dir)) {\n+        return true;\n+      }\n+\n+      try {\n+        URI uri = new URI(dir);\n+        if (null == uri.getAuthority()) {\n+          if (key.startsWith(dir + \"/\")){\n+            return true;\n+          }\n+        }\n+      } catch (URISyntaxException e) {\n+        LOG.info(\"URI syntax error creating URI for {}\", dir);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d1f1fbf0ef8588d0aebc8441e2a4784508042f12"}, "originalPosition": 133}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDAwMTIwMQ==", "bodyText": "this is used for every file being created, returning true or false. Raising an exception can be a problem.", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r440001201", "createdAt": "2020-06-15T08:10:22Z", "author": {"login": "ishaniahuja"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "diffHunk": "@@ -1314,7 +1352,30 @@ private String convertXmsPropertiesToCommaSeparatedString(final Hashtable<String\n     return properties;\n   }\n \n+  private boolean isKeyForPrefixSet(String key, Set<String> dirSet) {\n+\n+    for (String dir : dirSet) {\n+      if (dir.isEmpty() || key.startsWith(dir)) {\n+        return true;\n+      }\n+\n+      try {\n+        URI uri = new URI(dir);\n+        if (null == uri.getAuthority()) {\n+          if (key.startsWith(dir + \"/\")){\n+            return true;\n+          }\n+        }\n+      } catch (URISyntaxException e) {\n+        LOG.info(\"URI syntax error creating URI for {}\", dir);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0NzUxNQ=="}, "originalCommit": {"oid": "d1f1fbf0ef8588d0aebc8441e2a4784508042f12"}, "originalPosition": 133}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDA5NTkwMA==", "bodyText": "but what if only one comma separated value is incorrect, we would be throwing an exception adn crashing the app/jvm?", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r440095900", "createdAt": "2020-06-15T10:59:57Z", "author": {"login": "ishaniahuja"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "diffHunk": "@@ -1314,7 +1352,30 @@ private String convertXmsPropertiesToCommaSeparatedString(final Hashtable<String\n     return properties;\n   }\n \n+  private boolean isKeyForPrefixSet(String key, Set<String> dirSet) {\n+\n+    for (String dir : dirSet) {\n+      if (dir.isEmpty() || key.startsWith(dir)) {\n+        return true;\n+      }\n+\n+      try {\n+        URI uri = new URI(dir);\n+        if (null == uri.getAuthority()) {\n+          if (key.startsWith(dir + \"/\")){\n+            return true;\n+          }\n+        }\n+      } catch (URISyntaxException e) {\n+        LOG.info(\"URI syntax error creating URI for {}\", dir);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0NzUxNQ=="}, "originalCommit": {"oid": "d1f1fbf0ef8588d0aebc8441e2a4784508042f12"}, "originalPosition": 133}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0MDI4MjkyOnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQxNjo0NTo1MVrOGjeK3Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxOTowOTozNFrOGn25Rw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0NzY0NQ==", "bodyText": "fs.azure.appendblob.key config name doesn't look good. Wouldsomething like fs.azure.appendblob.directories convey the meaning better? (The way it is named in the FileSystemConfiguarations)", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r439847645", "createdAt": "2020-06-14T16:45:51Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java", "diffHunk": "@@ -59,6 +59,9 @@\n   public static final String FS_AZURE_ENABLE_AUTOTHROTTLING = \"fs.azure.enable.autothrottling\";\n   public static final String FS_AZURE_ALWAYS_USE_HTTPS = \"fs.azure.always.use.https\";\n   public static final String FS_AZURE_ATOMIC_RENAME_KEY = \"fs.azure.atomic.rename.key\";\n+  /** Provides a config to provide comma separated path prefixes on which Appendblob based files are created\n+   *  Default is empty. **/\n+  public static final String FS_AZURE_APPEND_BLOB_KEY = \"fs.azure.appendblob.key\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d1f1fbf0ef8588d0aebc8441e2a4784508042f12"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDAwMjQ0Ng==", "bodyText": "I have made is similar to FS_AZURE_ATOMIC_RENAME_KEY which also provide a set of directories. Further please note for appendblob, this is actually a prefix for the path (and not necessarily the directories). This is done so that the test suite (which all runs on a container with a randon guid can run) can run on appendblob based files. Let me know ur comments/thoughts here.", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r440002446", "createdAt": "2020-06-15T08:12:41Z", "author": {"login": "ishaniahuja"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java", "diffHunk": "@@ -59,6 +59,9 @@\n   public static final String FS_AZURE_ENABLE_AUTOTHROTTLING = \"fs.azure.enable.autothrottling\";\n   public static final String FS_AZURE_ALWAYS_USE_HTTPS = \"fs.azure.always.use.https\";\n   public static final String FS_AZURE_ATOMIC_RENAME_KEY = \"fs.azure.atomic.rename.key\";\n+  /** Provides a config to provide comma separated path prefixes on which Appendblob based files are created\n+   *  Default is empty. **/\n+  public static final String FS_AZURE_APPEND_BLOB_KEY = \"fs.azure.appendblob.key\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0NzY0NQ=="}, "originalCommit": {"oid": "d1f1fbf0ef8588d0aebc8441e2a4784508042f12"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ0NzA0Nw==", "bodyText": "done", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r444447047", "createdAt": "2020-06-23T19:09:34Z", "author": {"login": "ishaniahuja"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java", "diffHunk": "@@ -59,6 +59,9 @@\n   public static final String FS_AZURE_ENABLE_AUTOTHROTTLING = \"fs.azure.enable.autothrottling\";\n   public static final String FS_AZURE_ALWAYS_USE_HTTPS = \"fs.azure.always.use.https\";\n   public static final String FS_AZURE_ATOMIC_RENAME_KEY = \"fs.azure.atomic.rename.key\";\n+  /** Provides a config to provide comma separated path prefixes on which Appendblob based files are created\n+   *  Default is empty. **/\n+  public static final String FS_AZURE_APPEND_BLOB_KEY = \"fs.azure.appendblob.key\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0NzY0NQ=="}, "originalCommit": {"oid": "d1f1fbf0ef8588d0aebc8441e2a4784508042f12"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0MDI4NjUyOnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQxNjo1MTowMlrOGjeMqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxOTowOTo1MVrOGn253g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0ODEwNQ==", "bodyText": "Could you move this append blob handling to a separate method and call the same from here.", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r439848105", "createdAt": "2020-06-14T16:51:02Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -323,6 +328,35 @@ private synchronized void writeCurrentBufferToService() throws IOException {\n     final long offset = position;\n     position += bytesLength;\n \n+    if (this.isAppendBlob) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d1f1fbf0ef8588d0aebc8441e2a4784508042f12"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ0NzE5OA==", "bodyText": "done", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r444447198", "createdAt": "2020-06-23T19:09:51Z", "author": {"login": "ishaniahuja"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -323,6 +328,35 @@ private synchronized void writeCurrentBufferToService() throws IOException {\n     final long offset = position;\n     position += bytesLength;\n \n+    if (this.isAppendBlob) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0ODEwNQ=="}, "originalCommit": {"oid": "d1f1fbf0ef8588d0aebc8441e2a4784508042f12"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0MDI4Nzc2OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQxNjo1Mjo0M1rOGjeNQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQwNzo0MzowOVrOGovEdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0ODI1Nw==", "bodyText": "nit: Should we have a debug log here?", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r439848257", "createdAt": "2020-06-14T16:52:43Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -389,6 +423,12 @@ private synchronized void flushWrittenBytesToServiceAsync() throws IOException {\n \n   private synchronized void flushWrittenBytesToServiceInternal(final long offset,\n       final boolean retainUncommitedData, final boolean isClose) throws IOException {\n+\n+    // flush is not called for appendblob as is not needed\n+    if (this.isAppendBlob) {\n+      return;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d1f1fbf0ef8588d0aebc8441e2a4784508042f12"}, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDA4MTQ0OA==", "bodyText": "this can lead to frequent log lines, every time a flush(), hflush(), hsync() is called. will that be ok?", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r440081448", "createdAt": "2020-06-15T10:30:26Z", "author": {"login": "ishaniahuja"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -389,6 +423,12 @@ private synchronized void flushWrittenBytesToServiceAsync() throws IOException {\n \n   private synchronized void flushWrittenBytesToServiceInternal(final long offset,\n       final boolean retainUncommitedData, final boolean isClose) throws IOException {\n+\n+    // flush is not called for appendblob as is not needed\n+    if (this.isAppendBlob) {\n+      return;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0ODI1Nw=="}, "originalCommit": {"oid": "d1f1fbf0ef8588d0aebc8441e2a4784508042f12"}, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM2NzQxMg==", "bodyText": "Resolved", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r445367412", "createdAt": "2020-06-25T07:43:09Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -389,6 +423,12 @@ private synchronized void flushWrittenBytesToServiceAsync() throws IOException {\n \n   private synchronized void flushWrittenBytesToServiceInternal(final long offset,\n       final boolean retainUncommitedData, final boolean isClose) throws IOException {\n+\n+    // flush is not called for appendblob as is not needed\n+    if (this.isAppendBlob) {\n+      return;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0ODI1Nw=="}, "originalCommit": {"oid": "d1f1fbf0ef8588d0aebc8441e2a4784508042f12"}, "originalPosition": 98}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3NDk0NjY1OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQwNzoxNjo1OFrOGouOsQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQwNzoxNjo1OFrOGouOsQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM1MzY0OQ==", "bodyText": "Minor. Fix comment.", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r445353649", "createdAt": "2020-06-25T07:16:58Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java", "diffHunk": "@@ -47,6 +47,7 @@\n \n   // Default upload and download buffer size\n   public static final int DEFAULT_WRITE_BUFFER_SIZE = 8 * ONE_MB;  // 8 MB\n+  public static final int APPENDBLOB_MAX_WRITE_BUFFER_SIZE = 4 * ONE_MB;  // 8 MB", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b11ea2d7754561bc5ffa635a3a00284cad58e13c"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3NDk0OTY4OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQwNzoxNzo0OVrOGouQiw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQwNzoxNzo0OVrOGouQiw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM1NDEyMw==", "bodyText": "Minor. boolean flag better named as isAppendBlob", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r445354123", "createdAt": "2020-06-25T07:17:49Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java", "diffHunk": "@@ -272,7 +272,8 @@ public AbfsRestOperation deleteFilesystem() throws AzureBlobFileSystemException\n   }\n \n   public AbfsRestOperation createPath(final String path, final boolean isFile, final boolean overwrite,\n-                                      final String permission, final String umask) throws AzureBlobFileSystemException {\n+                                      final String permission, final String umask,\n+                                      final boolean appendBlob) throws AzureBlobFileSystemException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b11ea2d7754561bc5ffa635a3a00284cad58e13c"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3NDk1MzEyOnYy", "diffSide": "LEFT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsHttpOperation.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQwNzoxODo1MFrOGouSlA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQwNzoxODo1MFrOGouSlA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM1NDY0NA==", "bodyText": "Undo. newline needed after a block.", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r445354644", "createdAt": "2020-06-25T07:18:50Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsHttpOperation.java", "diffHunk": "@@ -337,7 +344,6 @@ public void processResponse(final byte[] buffer, final int offset, final int len\n     if (this.isTraceEnabled) {\n       startTime = System.nanoTime();\n     }\n-", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b11ea2d7754561bc5ffa635a3a00284cad58e13c"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3NDk1NTczOnYy", "diffSide": "LEFT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQwNzoxOTozNFrOGouUJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQwNzoxOTozNFrOGouUJQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM1NTA0NQ==", "bodyText": "Undo. new line needed.", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r445355045", "createdAt": "2020-06-25T07:19:34Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -367,7 +418,6 @@ private synchronized void flushWrittenBytesToService(boolean isClose) throws IOE\n             throw new FileNotFoundException(ex.getMessage());\n           }\n         }\n-", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b11ea2d7754561bc5ffa635a3a00284cad58e13c"}, "originalPosition": 97}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3NDk1Njk1OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQwNzoxOTo1N1rOGouU2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQwNzoxOTo1N1rOGouU2Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM1NTIyNQ==", "bodyText": "unnecessary additional new line.", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r445355225", "createdAt": "2020-06-25T07:19:57Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -378,6 +428,7 @@ private synchronized void flushWrittenBytesToService(boolean isClose) throws IOE\n     flushWrittenBytesToServiceInternal(position, false, isClose);\n   }\n \n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b11ea2d7754561bc5ffa635a3a00284cad58e13c"}, "originalPosition": 105}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3NDk2MDQ0OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQwNzoyMDo0NlrOGouWug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQwNzoyMDo0NlrOGouWug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM1NTcwNg==", "bodyText": "remove newline", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r445355706", "createdAt": "2020-06-25T07:20:46Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -389,6 +440,12 @@ private synchronized void flushWrittenBytesToServiceAsync() throws IOException {\n \n   private synchronized void flushWrittenBytesToServiceInternal(final long offset,\n       final boolean retainUncommitedData, final boolean isClose) throws IOException {\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b11ea2d7754561bc5ffa635a3a00284cad58e13c"}, "originalPosition": 113}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3NDk2MTE0OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQwNzoyMTowMFrOGouXIw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQwNzoyMTowMFrOGouXIw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM1NTgxMQ==", "bodyText": "remove newline", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r445355811", "createdAt": "2020-06-25T07:21:00Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -430,10 +487,15 @@ private synchronized void shrinkWriteOperationQueue() throws IOException {\n   }\n \n   private void waitForTaskToComplete() throws IOException {\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b11ea2d7754561bc5ffa635a3a00284cad58e13c"}, "originalPosition": 126}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3NDk2NTQxOnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQwNzoyMjowN1rOGouZfQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQwNzoyMjowN1rOGouZfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM1NjQxMw==", "bodyText": "remove newline", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r445356413", "createdAt": "2020-06-25T07:22:07Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java", "diffHunk": "@@ -185,13 +189,18 @@ void execute() throws AzureBlobFileSystemException {\n       try {\n         LOG.debug(\"Retrying REST operation {}. RetryCount = {}\",\n             operationType, retryCount);\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b11ea2d7754561bc5ffa635a3a00284cad58e13c"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3NDk3MzExOnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQwNzoyNDoxN1rOGoud7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMDoxMTo0NFrOGpbeYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM1NzU1MA==", "bodyText": "Why is it that for Http Status code 400 and above, exception is suppressed ? Can you please add code comments for the reason.", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r445357550", "createdAt": "2020-06-25T07:24:17Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java", "diffHunk": "@@ -185,13 +189,18 @@ void execute() throws AzureBlobFileSystemException {\n       try {\n         LOG.debug(\"Retrying REST operation {}. RetryCount = {}\",\n             operationType, retryCount);\n+\n         Thread.sleep(client.getRetryPolicy().getRetryInterval(retryCount));\n       } catch (InterruptedException ex) {\n         Thread.currentThread().interrupt();\n       }\n     }\n \n     if (result.getStatusCode() >= HttpURLConnection.HTTP_BAD_REQUEST) {\n+      if (this.isAppendBlobAppend && retryCount > 0 && result.getStorageErrorCode().equals(\"InvalidQueryParameterValue\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b11ea2d7754561bc5ffa635a3a00284cad58e13c"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjA5NDk0NQ==", "bodyText": "This isnt the right place to handle a case specific to appendblob. HttpOperation returned to specific AbfsClient method can take the call on what to return. Similar is done for rename.\nYou can check:\nhttps://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java#L358", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r446094945", "createdAt": "2020-06-26T10:11:44Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java", "diffHunk": "@@ -185,13 +189,18 @@ void execute() throws AzureBlobFileSystemException {\n       try {\n         LOG.debug(\"Retrying REST operation {}. RetryCount = {}\",\n             operationType, retryCount);\n+\n         Thread.sleep(client.getRetryPolicy().getRetryInterval(retryCount));\n       } catch (InterruptedException ex) {\n         Thread.currentThread().interrupt();\n       }\n     }\n \n     if (result.getStatusCode() >= HttpURLConnection.HTTP_BAD_REQUEST) {\n+      if (this.isAppendBlobAppend && retryCount > 0 && result.getStorageErrorCode().equals(\"InvalidQueryParameterValue\")) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM1NzU1MA=="}, "originalCommit": {"oid": "b11ea2d7754561bc5ffa635a3a00284cad58e13c"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3NTAxODM1OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQwNzozNzo1N1rOGou5eA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxODoxMDoyNVrOGpGIiw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM2NDYwMA==", "bodyText": "Would be better to add a new line since the next line is a constructor.", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r445364600", "createdAt": "2020-06-25T07:37:57Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "diffHunk": "@@ -144,6 +145,10 @@\n   private final IdentityTransformerInterface identityTransformer;\n   private final AbfsPerfTracker abfsPerfTracker;\n \n+  /**\n+   * The set of directories where we should store files as append blobs.\n+   */\n+  private Set<String> appendBlobDirSet;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b11ea2d7754561bc5ffa635a3a00284cad58e13c"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTc0NTI5MQ==", "bodyText": "done", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r445745291", "createdAt": "2020-06-25T18:10:25Z", "author": {"login": "ishaniahuja"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "diffHunk": "@@ -144,6 +145,10 @@\n   private final IdentityTransformerInterface identityTransformer;\n   private final AbfsPerfTracker abfsPerfTracker;\n \n+  /**\n+   * The set of directories where we should store files as append blobs.\n+   */\n+  private Set<String> appendBlobDirSet;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM2NDYwMA=="}, "originalCommit": {"oid": "b11ea2d7754561bc5ffa635a3a00284cad58e13c"}, "originalPosition": 15}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3428, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}