{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzg5Nzg0MDA5", "number": 1899, "title": "HADOOP-16914 Adding Output Stream Counters in ABFS", "bodyText": "Change-Id: Ie976f23c6c3e2f5cf9167794357cfc669a232c80\nNOTICE\nPlease create an issue in ASF JIRA before opening a pull request,\nand you need to set the title of the pull request which starts with\nthe corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.)\nFor more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute", "createdAt": "2020-03-17T11:38:47Z", "url": "https://github.com/apache/hadoop/pull/1899", "merged": true, "mergeCommit": {"oid": "459eb2ad6d5bc6b21462e728fb334c6e30e14c39"}, "closed": true, "closedAt": "2020-04-23T12:35:40Z", "author": {"login": "mehakmeet"}, "timelineItems": {"totalCount": 54, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcOku6rgFqTM3NjE2Mzg3OQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcabeA6ABqjMyNjQ2OTI3MjA=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc2MTYzODc5", "url": "https://github.com/apache/hadoop/pull/1899#pullrequestreview-376163879", "createdAt": "2020-03-17T15:48:51Z", "commit": {"oid": "4271e7b361ba253fd13ee67923e332145c22cdee"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4271e7b361ba253fd13ee67923e332145c22cdee", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/4271e7b361ba253fd13ee67923e332145c22cdee", "committedDate": "2020-03-17T11:34:50Z", "message": "CDPD-8525 AbfsOutputStream Counter\n\nChange-Id: Ie976f23c6c3e2f5cf9167794357cfc669a232c80"}, "afterCommit": {"oid": "20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e", "committedDate": "2020-03-18T08:23:48Z", "message": "Fixing issues"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc2NzE0OTEy", "url": "https://github.com/apache/hadoop/pull/1899#pullrequestreview-376714912", "createdAt": "2020-03-18T10:00:23Z", "commit": {"oid": "20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQxMDowMDoyM1rOF39wtg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQxMDowMDoyM1rOF39wtg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDIyNzg5NA==", "bodyText": "Need help in simulating Bytes to fail to upload in this test to get some values for bytesUploadFailed counter.", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r394227894", "createdAt": "2020-03-18T10:00:23Z", "author": {"login": "mehakmeet"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsOutputStream.java", "diffHunk": "@@ -28,28 +26,38 @@ public ITestAbfsOutputStream() throws Exception {\n   public void testAbfsOutputStreamUploadingBytes() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e"}, "originalPosition": 15}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc2NzE1OTc1", "url": "https://github.com/apache/hadoop/pull/1899#pullrequestreview-376715975", "createdAt": "2020-03-18T10:01:47Z", "commit": {"oid": "20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQxMDowMTo0N1rOF390EA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQxMDowMTo0N1rOF390EA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDIyODc1Mg==", "bodyText": "any way around flush() to get queueShrink() calls after writing ?\nflush() is quite expensive as it takes some time even at 1000 calls to test.", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r394228752", "createdAt": "2020-03-18T10:01:47Z", "author": {"login": "mehakmeet"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsOutputStream.java", "diffHunk": "@@ -133,58 +133,60 @@ public void testAbfsOutputStreamTimeSpendOnWaitTask() throws IOException {\n   public void testAbfsOutputStreamQueueShrink() throws IOException {\n     describe(\"Testing Queue Shrink calls in AbfsOutputStream\");\n     final AzureBlobFileSystem fs = getFileSystem();\n-    Path TEST_PATH = new Path(\"AbfsOutputStreamStatsPath\");\n+    Path queueShrinkFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n     AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n     abfss.getAbfsConfiguration().setDisableOutputStreamFlush(false);\n     FileSystem.Statistics statistics = fs.getFsStatistics();\n     String testQueueShrink = \"testQueue\";\n \n-\n     AbfsOutputStream outForOneOp = null;\n \n     try {\n-      outForOneOp = (AbfsOutputStream) abfss.createFile(TEST_PATH, statistics,\n-        true,\n-          FsPermission.getDefault(), FsPermission.getUMask(fs.getConf()));\n+      outForOneOp =\n+          (AbfsOutputStream) abfss.createFile(queueShrinkFilePath, statistics,\n+              true,\n+              FsPermission.getDefault(), FsPermission.getUMask(fs.getConf()));\n \n       //Test for shrinking Queue zero time\n-      Assert.assertEquals(\"Mismatch in number of queueShrink() Calls\", 0,\n+      assertValues(\"number of queueShrink() Calls\", 0,\n           outForOneOp.getOutputStreamStatistics().queueShrink);\n \n       outForOneOp.write(testQueueShrink.getBytes());\n       // Queue is shrunk 2 times when outStream is flushed\n       outForOneOp.flush();\n \n       //Test for shrinking Queue 2 times\n-      Assert.assertEquals(\"Mismatch in number of queueShrink() Calls\", 2,\n+      assertValues(\"number of queueShrink() Calls\", 2,\n           outForOneOp.getOutputStreamStatistics().queueShrink);\n \n     } finally {\n-      if(outForOneOp != null){\n+      if (outForOneOp != null) {\n         outForOneOp.close();\n       }\n     }\n \n     AbfsOutputStream outForLargeOps = null;\n \n     try {\n-      outForLargeOps = (AbfsOutputStream) abfss.createFile(TEST_PATH,\n+      outForLargeOps = (AbfsOutputStream) abfss.createFile(queueShrinkFilePath,\n           statistics, true,\n           FsPermission.getDefault(), FsPermission.getUMask(fs.getConf()));\n \n+      int largeValue = 1000;\n       //QueueShrink is called 2 times in 1 flush(), hence 1000 flushes must\n       // give 2000 QueueShrink calls\n-      for (int i = 0; i < 1000; i++) {\n+      for (int i = 0; i < largeValue; i++) {\n         outForLargeOps.write(testQueueShrink.getBytes());\n         //Flush is quite expensive so 1000 calls only which takes 1 min+\n         outForLargeOps.flush();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e"}, "originalPosition": 205}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc2NzE3Nzg4", "url": "https://github.com/apache/hadoop/pull/1899#pullrequestreview-376717788", "createdAt": "2020-03-18T10:04:18Z", "commit": {"oid": "20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQxMDowNDoxOFrOF3955g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQxMDowNDoxOFrOF3955g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDIzMDI0Ng==", "bodyText": "Need help on how to write tests for this counter.", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r394230246", "createdAt": "2020-03-18T10:04:18Z", "author": {"login": "mehakmeet"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,278 @@\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+\n+/**\n+ * Test AbfsOutputStream statistics.\n+ */\n+public class ITestAbfsOutputStream extends AbstractAbfsIntegrationTest {\n+\n+  public ITestAbfsOutputStream() throws Exception {\n+  }\n+\n+  /**\n+   * Tests to check bytes Uploading in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamUploadingBytes() throws IOException {\n+    describe(\"Testing Bytes uploaded in AbfsOutputSteam\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path uploadBytesFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+    abfss.getAbfsConfiguration().setDisableOutputStreamFlush(false);\n+    String testBytesToUpload = \"bytes\";\n+\n+    AbfsOutputStream outForSomeBytes = null;\n+    try {\n+      outForSomeBytes = (AbfsOutputStream) abfss.createFile(uploadBytesFilePath,\n+          statistics,\n+          true,\n+          FsPermission.getDefault(), FsPermission.getUMask(fs.getConf()));\n+\n+      //Test for zero bytes To upload\n+      assertValues(\"bytes to upload\", 0,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload);\n+\n+      outForSomeBytes.write(testBytesToUpload.getBytes());\n+      outForSomeBytes.flush();\n+\n+      //Test for some bytes to upload\n+      assertValues(\"bytes to upload\", testBytesToUpload.getBytes().length,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload);\n+\n+      //Test for relation between bytesUploadSuccessful, bytesUploadFailed\n+      // and bytesToUpload\n+      assertValues(\"bytesUploadSuccessful equal to difference between \"\n+              + \"bytesToUpload and bytesUploadFailed\",\n+          outForSomeBytes.getOutputStreamStatistics().bytesUploadSuccessful,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload -\n+              outForSomeBytes.getOutputStreamStatistics().bytesUploadFailed);\n+\n+    } finally {\n+      if (outForSomeBytes != null) {\n+        outForSomeBytes.close();\n+      }\n+    }\n+\n+    AbfsOutputStream outForLargeBytes = null;\n+    try {\n+      outForLargeBytes =\n+          (AbfsOutputStream) abfss.createFile(uploadBytesFilePath,\n+              statistics\n+              , true, FsPermission.getDefault(),\n+              FsPermission.getUMask(fs.getConf()));\n+\n+      int largeValue = 100000;\n+      for (int i = 0; i < largeValue; i++) {\n+        outForLargeBytes.write(testBytesToUpload.getBytes());\n+      }\n+      outForLargeBytes.flush();\n+\n+      //Test for large bytes to upload\n+      assertValues(\"bytes to upload\",\n+          largeValue * (testBytesToUpload.getBytes().length),\n+          outForLargeBytes.getOutputStreamStatistics().bytesToUpload);\n+\n+      //Test for relation between bytesUploadSuccessful, bytesUploadFailed\n+      // and bytesToUpload\n+      assertValues(\"bytesUploadSuccessful equal to difference between \"\n+              + \"bytesToUpload and bytesUploadFailed\",\n+          outForSomeBytes.getOutputStreamStatistics().bytesUploadSuccessful,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload -\n+              outForSomeBytes.getOutputStreamStatistics().bytesUploadFailed);\n+\n+    } finally {\n+      if (outForLargeBytes != null) {\n+        outForLargeBytes.close();\n+      }\n+    }\n+\n+  }\n+\n+  /**\n+   * Tests to check time spend on waiting for tasks to be complete on a\n+   * blocking queue in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamTimeSpendOnWaitTask() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e"}, "originalPosition": 109}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc4NjIyNzkw", "url": "https://github.com/apache/hadoop/pull/1899#pullrequestreview-378622790", "createdAt": "2020-03-20T16:14:19Z", "commit": {"oid": "20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 28, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQxNjoxNDoyMFrOF5aRPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQxNjozNDozOFrOF5bC2A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc0MzU1MQ==", "bodyText": "this is input stream; presumably it's come in from somewhere else", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r395743551", "createdAt": "2020-03-20T16:14:20Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -101,6 +101,7 @@ public synchronized int read(final byte[] b, final int off, final int len) throw\n     int currentLen = len;\n     int lastReadBytes;\n     int totalReadBytes = 0;\n+    incrementReadOps();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc0MzkyMg==", "bodyText": "move down to under ElasticByteBufferPool", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r395743922", "createdAt": "2020-03-20T16:15:00Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -36,20 +36,25 @@\n import com.google.common.annotations.VisibleForTesting;\n import com.google.common.base.Preconditions;\n \n+import org.apache.hadoop.fs.FileSystem.Statistics;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc0NDQ3NA==", "bodyText": "add both new fields at the bottom of the other fields, e.g Line 85, and keep togeher.", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r395744474", "createdAt": "2020-03-20T16:15:55Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -36,20 +36,25 @@\n import com.google.common.annotations.VisibleForTesting;\n import com.google.common.base.Preconditions;\n \n+import org.apache.hadoop.fs.FileSystem.Statistics;\n import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AbfsRestOperationException;\n import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException;\n import org.apache.hadoop.io.ElasticByteBufferPool;\n import org.apache.hadoop.fs.FSExceptionMessages;\n import org.apache.hadoop.fs.StreamCapabilities;\n import org.apache.hadoop.fs.Syncable;\n \n+import static org.apache.hadoop.io.IOUtils.LOG;\n import static org.apache.hadoop.io.IOUtils.wrapException;\n \n /**\n  * The BlobFsOutputStream for Rest AbfsClient.\n  */\n public class AbfsOutputStream extends OutputStream implements Syncable, StreamCapabilities {\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc0NTI0OQ==", "bodyText": "prefer a more detailed description like uploadFailed(long). It's recording that an upload failed and the number of bytes", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r395745249", "createdAt": "2020-03-20T16:17:15Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStreamStatistics.java", "diffHunk": "@@ -0,0 +1,60 @@\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.classification.InterfaceStability;\n+\n+/**\n+ * Interface for {@link AbfsOutputStream} statistics.\n+ */\n+@InterfaceStability.Unstable\n+public interface AbfsOutputStreamStatistics {\n+\n+  /**\n+   * Number of bytes to be uploaded.\n+   *\n+   * @param bytes number of bytes to upload\n+   */\n+  void bytesToUpload(long bytes);\n+\n+  /**\n+   * Number of bytes uploaded Successfully.\n+   *\n+   * @param bytes number of bytes that were successfully uploaded\n+   */\n+  void bytesUploadedSuccessfully(long bytes);\n+\n+  /**\n+   * Number of bytes failed to upload.\n+   *\n+   * @param bytes number of bytes that failed to upload\n+   */\n+  void bytesFailed(long bytes);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc0NjAxMQ==", "bodyText": "MUST NOT use @link to private/package-private/protected methods. Javadoc will fail", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r395746011", "createdAt": "2020-03-20T16:18:30Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStreamStatisticsImpl.java", "diffHunk": "@@ -0,0 +1,102 @@\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+/**\n+ * OutputStream Statistics Implementation for Abfs.\n+ * timeSpendOnTaskWait - Time spend on waiting for tasks to be complete on\n+ * Blocking Queue in AbfsOutputStream.\n+ *\n+ * queueShrink - Number of times Blocking Queue was shrunk after writing\n+ * data.\n+ *\n+ * WriteCurrentBufferOperations - Number of times\n+ * {@codes writeCurrentBufferToService()} calls were made.\n+ */\n+public class AbfsOutputStreamStatisticsImpl\n+    implements AbfsOutputStreamStatistics {\n+  public volatile long bytesToUpload;\n+  public volatile long bytesUploadSuccessful;\n+  public volatile long bytesUploadFailed;\n+  public volatile long timeSpendOnTaskWait;\n+  public volatile long queueShrink;\n+  public volatile long writeCurrentBufferOperations;\n+\n+  /**\n+   * Number of bytes uploaded only when bytes passed are positive.\n+   *\n+   * @param bytes negative values are ignored\n+   */\n+  @Override\n+  public void bytesToUpload(long bytes) {\n+    if (bytes > 0) {\n+      bytesToUpload += bytes;\n+    }\n+  }\n+\n+  @Override\n+  public void bytesUploadedSuccessfully(long bytes) {\n+    if (bytes > 0) {\n+      bytesUploadSuccessful += bytes;\n+    }\n+  }\n+\n+  /**\n+   * Number of bytes that weren't uploaded.\n+   *\n+   * @param bytes negative values are ignored\n+   */\n+  @Override\n+  public void bytesFailed(long bytes) {\n+    if (bytes > 0) {\n+      bytesUploadFailed += bytes;\n+    }\n+  }\n+\n+  /**\n+   * Time spend for waiting a task to be completed.\n+   *\n+   * @param startTime on calling {@link AbfsOutputStream#waitForTaskToComplete()}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc0NjI1Mg==", "bodyText": "see above comment about javadocs", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r395746252", "createdAt": "2020-03-20T16:18:52Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStreamStatisticsImpl.java", "diffHunk": "@@ -0,0 +1,102 @@\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+/**\n+ * OutputStream Statistics Implementation for Abfs.\n+ * timeSpendOnTaskWait - Time spend on waiting for tasks to be complete on\n+ * Blocking Queue in AbfsOutputStream.\n+ *\n+ * queueShrink - Number of times Blocking Queue was shrunk after writing\n+ * data.\n+ *\n+ * WriteCurrentBufferOperations - Number of times\n+ * {@codes writeCurrentBufferToService()} calls were made.\n+ */\n+public class AbfsOutputStreamStatisticsImpl\n+    implements AbfsOutputStreamStatistics {\n+  public volatile long bytesToUpload;\n+  public volatile long bytesUploadSuccessful;\n+  public volatile long bytesUploadFailed;\n+  public volatile long timeSpendOnTaskWait;\n+  public volatile long queueShrink;\n+  public volatile long writeCurrentBufferOperations;\n+\n+  /**\n+   * Number of bytes uploaded only when bytes passed are positive.\n+   *\n+   * @param bytes negative values are ignored\n+   */\n+  @Override\n+  public void bytesToUpload(long bytes) {\n+    if (bytes > 0) {\n+      bytesToUpload += bytes;\n+    }\n+  }\n+\n+  @Override\n+  public void bytesUploadedSuccessfully(long bytes) {\n+    if (bytes > 0) {\n+      bytesUploadSuccessful += bytes;\n+    }\n+  }\n+\n+  /**\n+   * Number of bytes that weren't uploaded.\n+   *\n+   * @param bytes negative values are ignored\n+   */\n+  @Override\n+  public void bytesFailed(long bytes) {\n+    if (bytes > 0) {\n+      bytesUploadFailed += bytes;\n+    }\n+  }\n+\n+  /**\n+   * Time spend for waiting a task to be completed.\n+   *\n+   * @param startTime on calling {@link AbfsOutputStream#waitForTaskToComplete()}\n+   * @param endTime   on method completing\n+   */\n+  @Override\n+  public void timeSpendTaskWait(long startTime, long endTime) {\n+    timeSpendOnTaskWait += endTime - startTime;\n+  }\n+\n+  /**\n+   * Number of calls to {@link AbfsOutputStream#shrinkWriteOperationQueue()}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc0NjMwMA==", "bodyText": "see above comment about javadocs", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r395746300", "createdAt": "2020-03-20T16:18:57Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStreamStatisticsImpl.java", "diffHunk": "@@ -0,0 +1,102 @@\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+/**\n+ * OutputStream Statistics Implementation for Abfs.\n+ * timeSpendOnTaskWait - Time spend on waiting for tasks to be complete on\n+ * Blocking Queue in AbfsOutputStream.\n+ *\n+ * queueShrink - Number of times Blocking Queue was shrunk after writing\n+ * data.\n+ *\n+ * WriteCurrentBufferOperations - Number of times\n+ * {@codes writeCurrentBufferToService()} calls were made.\n+ */\n+public class AbfsOutputStreamStatisticsImpl\n+    implements AbfsOutputStreamStatistics {\n+  public volatile long bytesToUpload;\n+  public volatile long bytesUploadSuccessful;\n+  public volatile long bytesUploadFailed;\n+  public volatile long timeSpendOnTaskWait;\n+  public volatile long queueShrink;\n+  public volatile long writeCurrentBufferOperations;\n+\n+  /**\n+   * Number of bytes uploaded only when bytes passed are positive.\n+   *\n+   * @param bytes negative values are ignored\n+   */\n+  @Override\n+  public void bytesToUpload(long bytes) {\n+    if (bytes > 0) {\n+      bytesToUpload += bytes;\n+    }\n+  }\n+\n+  @Override\n+  public void bytesUploadedSuccessfully(long bytes) {\n+    if (bytes > 0) {\n+      bytesUploadSuccessful += bytes;\n+    }\n+  }\n+\n+  /**\n+   * Number of bytes that weren't uploaded.\n+   *\n+   * @param bytes negative values are ignored\n+   */\n+  @Override\n+  public void bytesFailed(long bytes) {\n+    if (bytes > 0) {\n+      bytesUploadFailed += bytes;\n+    }\n+  }\n+\n+  /**\n+   * Time spend for waiting a task to be completed.\n+   *\n+   * @param startTime on calling {@link AbfsOutputStream#waitForTaskToComplete()}\n+   * @param endTime   on method completing\n+   */\n+  @Override\n+  public void timeSpendTaskWait(long startTime, long endTime) {\n+    timeSpendOnTaskWait += endTime - startTime;\n+  }\n+\n+  /**\n+   * Number of calls to {@link AbfsOutputStream#shrinkWriteOperationQueue()}.\n+   */\n+  @Override\n+  public void queueShrinked() {\n+    queueShrink++;\n+  }\n+\n+  /**\n+   * Number of calls to {@link AbfsOutputStream#writeCurrentBufferToService()}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc0ODMwNw==", "bodyText": "MUST use { } in all if () clauses.\nIf there's a mismatch, use AssertEquals and include the pos where the problem occurred\n\nImagine: \"A remote test run failed -what information should be in the test report to begin debugging this?\"", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r395748307", "createdAt": "2020-03-20T16:22:08Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/AbstractAbfsTestWithTimeout.java", "diffHunk": "@@ -67,4 +77,46 @@ public void nameThread() {\n   protected int getTestTimeoutMillis() {\n     return TEST_TIMEOUT;\n   }\n+\n+  /**\n+   * Describe a test in the logs.\n+   *\n+   * @param text text to print\n+   * @param args arguments to format in the printing\n+   */\n+  protected void describe(String text, Object... args) {\n+    LOG.info(\"\\n\\n{}: {}\\n\",\n+        methodName.getMethodName(),\n+        String.format(text, args));\n+  }\n+\n+  /**\n+   * Validate Contents written on a file in Abfs.\n+   *\n+   * @param fs                AzureBlobFileSystem\n+   * @param path              Path of the file\n+   * @param originalByteArray original byte array\n+   * @return if content is validated true else, false\n+   * @throws IOException\n+   */\n+  protected boolean validateContent(AzureBlobFileSystem fs, Path path,\n+      byte[] originalByteArray)\n+      throws IOException {\n+    FSDataInputStream in = fs.open(path);\n+\n+    int pos = 0;\n+    int lenOfOriginalByteArray = originalByteArray.length;\n+    byte valueOfContentAtPos = (byte) in.read();\n+\n+    while (valueOfContentAtPos != -1 && pos < lenOfOriginalByteArray) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc0OTAyMw==", "bodyText": "IOUtils.closeQuietly(LOG, ...), or try-with-resources", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r395749023", "createdAt": "2020-03-20T16:23:14Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,278 @@\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+\n+/**\n+ * Test AbfsOutputStream statistics.\n+ */\n+public class ITestAbfsOutputStream extends AbstractAbfsIntegrationTest {\n+\n+  public ITestAbfsOutputStream() throws Exception {\n+  }\n+\n+  /**\n+   * Tests to check bytes Uploading in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamUploadingBytes() throws IOException {\n+    describe(\"Testing Bytes uploaded in AbfsOutputSteam\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path uploadBytesFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+    abfss.getAbfsConfiguration().setDisableOutputStreamFlush(false);\n+    String testBytesToUpload = \"bytes\";\n+\n+    AbfsOutputStream outForSomeBytes = null;\n+    try {\n+      outForSomeBytes = (AbfsOutputStream) abfss.createFile(uploadBytesFilePath,\n+          statistics,\n+          true,\n+          FsPermission.getDefault(), FsPermission.getUMask(fs.getConf()));\n+\n+      //Test for zero bytes To upload\n+      assertValues(\"bytes to upload\", 0,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload);\n+\n+      outForSomeBytes.write(testBytesToUpload.getBytes());\n+      outForSomeBytes.flush();\n+\n+      //Test for some bytes to upload\n+      assertValues(\"bytes to upload\", testBytesToUpload.getBytes().length,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload);\n+\n+      //Test for relation between bytesUploadSuccessful, bytesUploadFailed\n+      // and bytesToUpload\n+      assertValues(\"bytesUploadSuccessful equal to difference between \"\n+              + \"bytesToUpload and bytesUploadFailed\",\n+          outForSomeBytes.getOutputStreamStatistics().bytesUploadSuccessful,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload -\n+              outForSomeBytes.getOutputStreamStatistics().bytesUploadFailed);\n+\n+    } finally {\n+      if (outForSomeBytes != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc0OTIxMw==", "bodyText": "same", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r395749213", "createdAt": "2020-03-20T16:23:31Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,278 @@\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+\n+/**\n+ * Test AbfsOutputStream statistics.\n+ */\n+public class ITestAbfsOutputStream extends AbstractAbfsIntegrationTest {\n+\n+  public ITestAbfsOutputStream() throws Exception {\n+  }\n+\n+  /**\n+   * Tests to check bytes Uploading in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamUploadingBytes() throws IOException {\n+    describe(\"Testing Bytes uploaded in AbfsOutputSteam\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path uploadBytesFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+    abfss.getAbfsConfiguration().setDisableOutputStreamFlush(false);\n+    String testBytesToUpload = \"bytes\";\n+\n+    AbfsOutputStream outForSomeBytes = null;\n+    try {\n+      outForSomeBytes = (AbfsOutputStream) abfss.createFile(uploadBytesFilePath,\n+          statistics,\n+          true,\n+          FsPermission.getDefault(), FsPermission.getUMask(fs.getConf()));\n+\n+      //Test for zero bytes To upload\n+      assertValues(\"bytes to upload\", 0,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload);\n+\n+      outForSomeBytes.write(testBytesToUpload.getBytes());\n+      outForSomeBytes.flush();\n+\n+      //Test for some bytes to upload\n+      assertValues(\"bytes to upload\", testBytesToUpload.getBytes().length,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload);\n+\n+      //Test for relation between bytesUploadSuccessful, bytesUploadFailed\n+      // and bytesToUpload\n+      assertValues(\"bytesUploadSuccessful equal to difference between \"\n+              + \"bytesToUpload and bytesUploadFailed\",\n+          outForSomeBytes.getOutputStreamStatistics().bytesUploadSuccessful,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload -\n+              outForSomeBytes.getOutputStreamStatistics().bytesUploadFailed);\n+\n+    } finally {\n+      if (outForSomeBytes != null) {\n+        outForSomeBytes.close();\n+      }\n+    }\n+\n+    AbfsOutputStream outForLargeBytes = null;\n+    try {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc0OTYwMQ==", "bodyText": "I don't see any easy way except to assert that it is > 0", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r395749601", "createdAt": "2020-03-20T16:24:09Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,278 @@\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+\n+/**\n+ * Test AbfsOutputStream statistics.\n+ */\n+public class ITestAbfsOutputStream extends AbstractAbfsIntegrationTest {\n+\n+  public ITestAbfsOutputStream() throws Exception {\n+  }\n+\n+  /**\n+   * Tests to check bytes Uploading in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamUploadingBytes() throws IOException {\n+    describe(\"Testing Bytes uploaded in AbfsOutputSteam\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path uploadBytesFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+    abfss.getAbfsConfiguration().setDisableOutputStreamFlush(false);\n+    String testBytesToUpload = \"bytes\";\n+\n+    AbfsOutputStream outForSomeBytes = null;\n+    try {\n+      outForSomeBytes = (AbfsOutputStream) abfss.createFile(uploadBytesFilePath,\n+          statistics,\n+          true,\n+          FsPermission.getDefault(), FsPermission.getUMask(fs.getConf()));\n+\n+      //Test for zero bytes To upload\n+      assertValues(\"bytes to upload\", 0,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload);\n+\n+      outForSomeBytes.write(testBytesToUpload.getBytes());\n+      outForSomeBytes.flush();\n+\n+      //Test for some bytes to upload\n+      assertValues(\"bytes to upload\", testBytesToUpload.getBytes().length,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload);\n+\n+      //Test for relation between bytesUploadSuccessful, bytesUploadFailed\n+      // and bytesToUpload\n+      assertValues(\"bytesUploadSuccessful equal to difference between \"\n+              + \"bytesToUpload and bytesUploadFailed\",\n+          outForSomeBytes.getOutputStreamStatistics().bytesUploadSuccessful,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload -\n+              outForSomeBytes.getOutputStreamStatistics().bytesUploadFailed);\n+\n+    } finally {\n+      if (outForSomeBytes != null) {\n+        outForSomeBytes.close();\n+      }\n+    }\n+\n+    AbfsOutputStream outForLargeBytes = null;\n+    try {\n+      outForLargeBytes =\n+          (AbfsOutputStream) abfss.createFile(uploadBytesFilePath,\n+              statistics\n+              , true, FsPermission.getDefault(),\n+              FsPermission.getUMask(fs.getConf()));\n+\n+      int largeValue = 100000;\n+      for (int i = 0; i < largeValue; i++) {\n+        outForLargeBytes.write(testBytesToUpload.getBytes());\n+      }\n+      outForLargeBytes.flush();\n+\n+      //Test for large bytes to upload\n+      assertValues(\"bytes to upload\",\n+          largeValue * (testBytesToUpload.getBytes().length),\n+          outForLargeBytes.getOutputStreamStatistics().bytesToUpload);\n+\n+      //Test for relation between bytesUploadSuccessful, bytesUploadFailed\n+      // and bytesToUpload\n+      assertValues(\"bytesUploadSuccessful equal to difference between \"\n+              + \"bytesToUpload and bytesUploadFailed\",\n+          outForSomeBytes.getOutputStreamStatistics().bytesUploadSuccessful,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload -\n+              outForSomeBytes.getOutputStreamStatistics().bytesUploadFailed);\n+\n+    } finally {\n+      if (outForLargeBytes != null) {\n+        outForLargeBytes.close();\n+      }\n+    }\n+\n+  }\n+\n+  /**\n+   * Tests to check time spend on waiting for tasks to be complete on a\n+   * blocking queue in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamTimeSpendOnWaitTask() throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDIzMDI0Ng=="}, "originalCommit": {"oid": "20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc0OTc2Mw==", "bodyText": "Also, \"time spent\"", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r395749763", "createdAt": "2020-03-20T16:24:25Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,278 @@\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+\n+/**\n+ * Test AbfsOutputStream statistics.\n+ */\n+public class ITestAbfsOutputStream extends AbstractAbfsIntegrationTest {\n+\n+  public ITestAbfsOutputStream() throws Exception {\n+  }\n+\n+  /**\n+   * Tests to check bytes Uploading in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamUploadingBytes() throws IOException {\n+    describe(\"Testing Bytes uploaded in AbfsOutputSteam\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path uploadBytesFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+    abfss.getAbfsConfiguration().setDisableOutputStreamFlush(false);\n+    String testBytesToUpload = \"bytes\";\n+\n+    AbfsOutputStream outForSomeBytes = null;\n+    try {\n+      outForSomeBytes = (AbfsOutputStream) abfss.createFile(uploadBytesFilePath,\n+          statistics,\n+          true,\n+          FsPermission.getDefault(), FsPermission.getUMask(fs.getConf()));\n+\n+      //Test for zero bytes To upload\n+      assertValues(\"bytes to upload\", 0,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload);\n+\n+      outForSomeBytes.write(testBytesToUpload.getBytes());\n+      outForSomeBytes.flush();\n+\n+      //Test for some bytes to upload\n+      assertValues(\"bytes to upload\", testBytesToUpload.getBytes().length,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload);\n+\n+      //Test for relation between bytesUploadSuccessful, bytesUploadFailed\n+      // and bytesToUpload\n+      assertValues(\"bytesUploadSuccessful equal to difference between \"\n+              + \"bytesToUpload and bytesUploadFailed\",\n+          outForSomeBytes.getOutputStreamStatistics().bytesUploadSuccessful,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload -\n+              outForSomeBytes.getOutputStreamStatistics().bytesUploadFailed);\n+\n+    } finally {\n+      if (outForSomeBytes != null) {\n+        outForSomeBytes.close();\n+      }\n+    }\n+\n+    AbfsOutputStream outForLargeBytes = null;\n+    try {\n+      outForLargeBytes =\n+          (AbfsOutputStream) abfss.createFile(uploadBytesFilePath,\n+              statistics\n+              , true, FsPermission.getDefault(),\n+              FsPermission.getUMask(fs.getConf()));\n+\n+      int largeValue = 100000;\n+      for (int i = 0; i < largeValue; i++) {\n+        outForLargeBytes.write(testBytesToUpload.getBytes());\n+      }\n+      outForLargeBytes.flush();\n+\n+      //Test for large bytes to upload\n+      assertValues(\"bytes to upload\",\n+          largeValue * (testBytesToUpload.getBytes().length),\n+          outForLargeBytes.getOutputStreamStatistics().bytesToUpload);\n+\n+      //Test for relation between bytesUploadSuccessful, bytesUploadFailed\n+      // and bytesToUpload\n+      assertValues(\"bytesUploadSuccessful equal to difference between \"\n+              + \"bytesToUpload and bytesUploadFailed\",\n+          outForSomeBytes.getOutputStreamStatistics().bytesUploadSuccessful,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload -\n+              outForSomeBytes.getOutputStreamStatistics().bytesUploadFailed);\n+\n+    } finally {\n+      if (outForLargeBytes != null) {\n+        outForLargeBytes.close();\n+      }\n+    }\n+\n+  }\n+\n+  /**\n+   * Tests to check time spend on waiting for tasks to be complete on a\n+   * blocking queue in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamTimeSpendOnWaitTask() throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDIzMDI0Ng=="}, "originalCommit": {"oid": "20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc1MDAxMQ==", "bodyText": "will need to be closed", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r395750011", "createdAt": "2020-03-20T16:24:50Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,278 @@\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+\n+/**\n+ * Test AbfsOutputStream statistics.\n+ */\n+public class ITestAbfsOutputStream extends AbstractAbfsIntegrationTest {\n+\n+  public ITestAbfsOutputStream() throws Exception {\n+  }\n+\n+  /**\n+   * Tests to check bytes Uploading in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamUploadingBytes() throws IOException {\n+    describe(\"Testing Bytes uploaded in AbfsOutputSteam\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path uploadBytesFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+    abfss.getAbfsConfiguration().setDisableOutputStreamFlush(false);\n+    String testBytesToUpload = \"bytes\";\n+\n+    AbfsOutputStream outForSomeBytes = null;\n+    try {\n+      outForSomeBytes = (AbfsOutputStream) abfss.createFile(uploadBytesFilePath,\n+          statistics,\n+          true,\n+          FsPermission.getDefault(), FsPermission.getUMask(fs.getConf()));\n+\n+      //Test for zero bytes To upload\n+      assertValues(\"bytes to upload\", 0,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload);\n+\n+      outForSomeBytes.write(testBytesToUpload.getBytes());\n+      outForSomeBytes.flush();\n+\n+      //Test for some bytes to upload\n+      assertValues(\"bytes to upload\", testBytesToUpload.getBytes().length,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload);\n+\n+      //Test for relation between bytesUploadSuccessful, bytesUploadFailed\n+      // and bytesToUpload\n+      assertValues(\"bytesUploadSuccessful equal to difference between \"\n+              + \"bytesToUpload and bytesUploadFailed\",\n+          outForSomeBytes.getOutputStreamStatistics().bytesUploadSuccessful,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload -\n+              outForSomeBytes.getOutputStreamStatistics().bytesUploadFailed);\n+\n+    } finally {\n+      if (outForSomeBytes != null) {\n+        outForSomeBytes.close();\n+      }\n+    }\n+\n+    AbfsOutputStream outForLargeBytes = null;\n+    try {\n+      outForLargeBytes =\n+          (AbfsOutputStream) abfss.createFile(uploadBytesFilePath,\n+              statistics\n+              , true, FsPermission.getDefault(),\n+              FsPermission.getUMask(fs.getConf()));\n+\n+      int largeValue = 100000;\n+      for (int i = 0; i < largeValue; i++) {\n+        outForLargeBytes.write(testBytesToUpload.getBytes());\n+      }\n+      outForLargeBytes.flush();\n+\n+      //Test for large bytes to upload\n+      assertValues(\"bytes to upload\",\n+          largeValue * (testBytesToUpload.getBytes().length),\n+          outForLargeBytes.getOutputStreamStatistics().bytesToUpload);\n+\n+      //Test for relation between bytesUploadSuccessful, bytesUploadFailed\n+      // and bytesToUpload\n+      assertValues(\"bytesUploadSuccessful equal to difference between \"\n+              + \"bytesToUpload and bytesUploadFailed\",\n+          outForSomeBytes.getOutputStreamStatistics().bytesUploadSuccessful,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload -\n+              outForSomeBytes.getOutputStreamStatistics().bytesUploadFailed);\n+\n+    } finally {\n+      if (outForLargeBytes != null) {\n+        outForLargeBytes.close();\n+      }\n+    }\n+\n+  }\n+\n+  /**\n+   * Tests to check time spend on waiting for tasks to be complete on a\n+   * blocking queue in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamTimeSpendOnWaitTask() throws IOException {\n+    describe(\"Testing Time Spend on Waiting for Task to be complete\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path timeSpendFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+\n+    AbfsOutputStream out =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc1MDQwMA==", "bodyText": "try-with-resources or IOUtils.closeQuietly", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r395750400", "createdAt": "2020-03-20T16:25:28Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,278 @@\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+\n+/**\n+ * Test AbfsOutputStream statistics.\n+ */\n+public class ITestAbfsOutputStream extends AbstractAbfsIntegrationTest {\n+\n+  public ITestAbfsOutputStream() throws Exception {\n+  }\n+\n+  /**\n+   * Tests to check bytes Uploading in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamUploadingBytes() throws IOException {\n+    describe(\"Testing Bytes uploaded in AbfsOutputSteam\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path uploadBytesFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+    abfss.getAbfsConfiguration().setDisableOutputStreamFlush(false);\n+    String testBytesToUpload = \"bytes\";\n+\n+    AbfsOutputStream outForSomeBytes = null;\n+    try {\n+      outForSomeBytes = (AbfsOutputStream) abfss.createFile(uploadBytesFilePath,\n+          statistics,\n+          true,\n+          FsPermission.getDefault(), FsPermission.getUMask(fs.getConf()));\n+\n+      //Test for zero bytes To upload\n+      assertValues(\"bytes to upload\", 0,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload);\n+\n+      outForSomeBytes.write(testBytesToUpload.getBytes());\n+      outForSomeBytes.flush();\n+\n+      //Test for some bytes to upload\n+      assertValues(\"bytes to upload\", testBytesToUpload.getBytes().length,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload);\n+\n+      //Test for relation between bytesUploadSuccessful, bytesUploadFailed\n+      // and bytesToUpload\n+      assertValues(\"bytesUploadSuccessful equal to difference between \"\n+              + \"bytesToUpload and bytesUploadFailed\",\n+          outForSomeBytes.getOutputStreamStatistics().bytesUploadSuccessful,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload -\n+              outForSomeBytes.getOutputStreamStatistics().bytesUploadFailed);\n+\n+    } finally {\n+      if (outForSomeBytes != null) {\n+        outForSomeBytes.close();\n+      }\n+    }\n+\n+    AbfsOutputStream outForLargeBytes = null;\n+    try {\n+      outForLargeBytes =\n+          (AbfsOutputStream) abfss.createFile(uploadBytesFilePath,\n+              statistics\n+              , true, FsPermission.getDefault(),\n+              FsPermission.getUMask(fs.getConf()));\n+\n+      int largeValue = 100000;\n+      for (int i = 0; i < largeValue; i++) {\n+        outForLargeBytes.write(testBytesToUpload.getBytes());\n+      }\n+      outForLargeBytes.flush();\n+\n+      //Test for large bytes to upload\n+      assertValues(\"bytes to upload\",\n+          largeValue * (testBytesToUpload.getBytes().length),\n+          outForLargeBytes.getOutputStreamStatistics().bytesToUpload);\n+\n+      //Test for relation between bytesUploadSuccessful, bytesUploadFailed\n+      // and bytesToUpload\n+      assertValues(\"bytesUploadSuccessful equal to difference between \"\n+              + \"bytesToUpload and bytesUploadFailed\",\n+          outForSomeBytes.getOutputStreamStatistics().bytesUploadSuccessful,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload -\n+              outForSomeBytes.getOutputStreamStatistics().bytesUploadFailed);\n+\n+    } finally {\n+      if (outForLargeBytes != null) {\n+        outForLargeBytes.close();\n+      }\n+    }\n+\n+  }\n+\n+  /**\n+   * Tests to check time spend on waiting for tasks to be complete on a\n+   * blocking queue in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamTimeSpendOnWaitTask() throws IOException {\n+    describe(\"Testing Time Spend on Waiting for Task to be complete\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path timeSpendFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+\n+    AbfsOutputStream out =\n+        (AbfsOutputStream) abfss.createFile(timeSpendFilePath,\n+            statistics, true,\n+            FsPermission.getDefault(), FsPermission.getUMask(fs.getConf()));\n+\n+  }\n+\n+  /**\n+   * Tests to check number of {@codes shrinkWriteOperationQueue()}\n+   * calls.\n+   * After writing data, AbfsOutputStream doesn't upload the data until\n+   * Flushed. Hence, flush() method is called after write() to test Queue\n+   * shrink calls.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamQueueShrink() throws IOException {\n+    describe(\"Testing Queue Shrink calls in AbfsOutputStream\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path queueShrinkFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    abfss.getAbfsConfiguration().setDisableOutputStreamFlush(false);\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+    String testQueueShrink = \"testQueue\";\n+\n+    AbfsOutputStream outForOneOp = null;\n+\n+    try {\n+      outForOneOp =\n+          (AbfsOutputStream) abfss.createFile(queueShrinkFilePath, statistics,\n+              true,\n+              FsPermission.getDefault(), FsPermission.getUMask(fs.getConf()));\n+\n+      //Test for shrinking Queue zero time\n+      assertValues(\"number of queueShrink() Calls\", 0,\n+          outForOneOp.getOutputStreamStatistics().queueShrink);\n+\n+      outForOneOp.write(testQueueShrink.getBytes());\n+      // Queue is shrunk 2 times when outStream is flushed\n+      outForOneOp.flush();\n+\n+      //Test for shrinking Queue 2 times\n+      assertValues(\"number of queueShrink() Calls\", 2,\n+          outForOneOp.getOutputStreamStatistics().queueShrink);\n+\n+    } finally {\n+      if (outForOneOp != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e"}, "originalPosition": 163}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc1MDk2Nw==", "bodyText": "you are calling createFile() enough in these tests it makes sense to factor out into it own method", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r395750967", "createdAt": "2020-03-20T16:26:14Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,278 @@\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+\n+/**\n+ * Test AbfsOutputStream statistics.\n+ */\n+public class ITestAbfsOutputStream extends AbstractAbfsIntegrationTest {\n+\n+  public ITestAbfsOutputStream() throws Exception {\n+  }\n+\n+  /**\n+   * Tests to check bytes Uploading in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamUploadingBytes() throws IOException {\n+    describe(\"Testing Bytes uploaded in AbfsOutputSteam\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path uploadBytesFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+    abfss.getAbfsConfiguration().setDisableOutputStreamFlush(false);\n+    String testBytesToUpload = \"bytes\";\n+\n+    AbfsOutputStream outForSomeBytes = null;\n+    try {\n+      outForSomeBytes = (AbfsOutputStream) abfss.createFile(uploadBytesFilePath,\n+          statistics,\n+          true,\n+          FsPermission.getDefault(), FsPermission.getUMask(fs.getConf()));\n+\n+      //Test for zero bytes To upload\n+      assertValues(\"bytes to upload\", 0,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload);\n+\n+      outForSomeBytes.write(testBytesToUpload.getBytes());\n+      outForSomeBytes.flush();\n+\n+      //Test for some bytes to upload\n+      assertValues(\"bytes to upload\", testBytesToUpload.getBytes().length,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload);\n+\n+      //Test for relation between bytesUploadSuccessful, bytesUploadFailed\n+      // and bytesToUpload\n+      assertValues(\"bytesUploadSuccessful equal to difference between \"\n+              + \"bytesToUpload and bytesUploadFailed\",\n+          outForSomeBytes.getOutputStreamStatistics().bytesUploadSuccessful,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload -\n+              outForSomeBytes.getOutputStreamStatistics().bytesUploadFailed);\n+\n+    } finally {\n+      if (outForSomeBytes != null) {\n+        outForSomeBytes.close();\n+      }\n+    }\n+\n+    AbfsOutputStream outForLargeBytes = null;\n+    try {\n+      outForLargeBytes =\n+          (AbfsOutputStream) abfss.createFile(uploadBytesFilePath,\n+              statistics\n+              , true, FsPermission.getDefault(),\n+              FsPermission.getUMask(fs.getConf()));\n+\n+      int largeValue = 100000;\n+      for (int i = 0; i < largeValue; i++) {\n+        outForLargeBytes.write(testBytesToUpload.getBytes());\n+      }\n+      outForLargeBytes.flush();\n+\n+      //Test for large bytes to upload\n+      assertValues(\"bytes to upload\",\n+          largeValue * (testBytesToUpload.getBytes().length),\n+          outForLargeBytes.getOutputStreamStatistics().bytesToUpload);\n+\n+      //Test for relation between bytesUploadSuccessful, bytesUploadFailed\n+      // and bytesToUpload\n+      assertValues(\"bytesUploadSuccessful equal to difference between \"\n+              + \"bytesToUpload and bytesUploadFailed\",\n+          outForSomeBytes.getOutputStreamStatistics().bytesUploadSuccessful,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload -\n+              outForSomeBytes.getOutputStreamStatistics().bytesUploadFailed);\n+\n+    } finally {\n+      if (outForLargeBytes != null) {\n+        outForLargeBytes.close();\n+      }\n+    }\n+\n+  }\n+\n+  /**\n+   * Tests to check time spend on waiting for tasks to be complete on a\n+   * blocking queue in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamTimeSpendOnWaitTask() throws IOException {\n+    describe(\"Testing Time Spend on Waiting for Task to be complete\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path timeSpendFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+\n+    AbfsOutputStream out =\n+        (AbfsOutputStream) abfss.createFile(timeSpendFilePath,\n+            statistics, true,\n+            FsPermission.getDefault(), FsPermission.getUMask(fs.getConf()));\n+\n+  }\n+\n+  /**\n+   * Tests to check number of {@codes shrinkWriteOperationQueue()}\n+   * calls.\n+   * After writing data, AbfsOutputStream doesn't upload the data until\n+   * Flushed. Hence, flush() method is called after write() to test Queue\n+   * shrink calls.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamQueueShrink() throws IOException {\n+    describe(\"Testing Queue Shrink calls in AbfsOutputStream\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path queueShrinkFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    abfss.getAbfsConfiguration().setDisableOutputStreamFlush(false);\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+    String testQueueShrink = \"testQueue\";\n+\n+    AbfsOutputStream outForOneOp = null;\n+\n+    try {\n+      outForOneOp =\n+          (AbfsOutputStream) abfss.createFile(queueShrinkFilePath, statistics,\n+              true,\n+              FsPermission.getDefault(), FsPermission.getUMask(fs.getConf()));\n+\n+      //Test for shrinking Queue zero time\n+      assertValues(\"number of queueShrink() Calls\", 0,\n+          outForOneOp.getOutputStreamStatistics().queueShrink);\n+\n+      outForOneOp.write(testQueueShrink.getBytes());\n+      // Queue is shrunk 2 times when outStream is flushed\n+      outForOneOp.flush();\n+\n+      //Test for shrinking Queue 2 times\n+      assertValues(\"number of queueShrink() Calls\", 2,\n+          outForOneOp.getOutputStreamStatistics().queueShrink);\n+\n+    } finally {\n+      if (outForOneOp != null) {\n+        outForOneOp.close();\n+      }\n+    }\n+\n+    AbfsOutputStream outForLargeOps = null;\n+\n+    try {\n+      outForLargeOps = (AbfsOutputStream) abfss.createFile(queueShrinkFilePath,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e"}, "originalPosition": 171}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc1MTIwMg==", "bodyText": "do you have to call it so many times?", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r395751202", "createdAt": "2020-03-20T16:26:36Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsOutputStream.java", "diffHunk": "@@ -133,58 +133,60 @@ public void testAbfsOutputStreamTimeSpendOnWaitTask() throws IOException {\n   public void testAbfsOutputStreamQueueShrink() throws IOException {\n     describe(\"Testing Queue Shrink calls in AbfsOutputStream\");\n     final AzureBlobFileSystem fs = getFileSystem();\n-    Path TEST_PATH = new Path(\"AbfsOutputStreamStatsPath\");\n+    Path queueShrinkFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n     AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n     abfss.getAbfsConfiguration().setDisableOutputStreamFlush(false);\n     FileSystem.Statistics statistics = fs.getFsStatistics();\n     String testQueueShrink = \"testQueue\";\n \n-\n     AbfsOutputStream outForOneOp = null;\n \n     try {\n-      outForOneOp = (AbfsOutputStream) abfss.createFile(TEST_PATH, statistics,\n-        true,\n-          FsPermission.getDefault(), FsPermission.getUMask(fs.getConf()));\n+      outForOneOp =\n+          (AbfsOutputStream) abfss.createFile(queueShrinkFilePath, statistics,\n+              true,\n+              FsPermission.getDefault(), FsPermission.getUMask(fs.getConf()));\n \n       //Test for shrinking Queue zero time\n-      Assert.assertEquals(\"Mismatch in number of queueShrink() Calls\", 0,\n+      assertValues(\"number of queueShrink() Calls\", 0,\n           outForOneOp.getOutputStreamStatistics().queueShrink);\n \n       outForOneOp.write(testQueueShrink.getBytes());\n       // Queue is shrunk 2 times when outStream is flushed\n       outForOneOp.flush();\n \n       //Test for shrinking Queue 2 times\n-      Assert.assertEquals(\"Mismatch in number of queueShrink() Calls\", 2,\n+      assertValues(\"number of queueShrink() Calls\", 2,\n           outForOneOp.getOutputStreamStatistics().queueShrink);\n \n     } finally {\n-      if(outForOneOp != null){\n+      if (outForOneOp != null) {\n         outForOneOp.close();\n       }\n     }\n \n     AbfsOutputStream outForLargeOps = null;\n \n     try {\n-      outForLargeOps = (AbfsOutputStream) abfss.createFile(TEST_PATH,\n+      outForLargeOps = (AbfsOutputStream) abfss.createFile(queueShrinkFilePath,\n           statistics, true,\n           FsPermission.getDefault(), FsPermission.getUMask(fs.getConf()));\n \n+      int largeValue = 1000;\n       //QueueShrink is called 2 times in 1 flush(), hence 1000 flushes must\n       // give 2000 QueueShrink calls\n-      for (int i = 0; i < 1000; i++) {\n+      for (int i = 0; i < largeValue; i++) {\n         outForLargeOps.write(testQueueShrink.getBytes());\n         //Flush is quite expensive so 1000 calls only which takes 1 min+\n         outForLargeOps.flush();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDIyODc1Mg=="}, "originalCommit": {"oid": "20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e"}, "originalPosition": 205}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc1MTQ3MA==", "bodyText": "@code", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r395751470", "createdAt": "2020-03-20T16:27:00Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,278 @@\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+\n+/**\n+ * Test AbfsOutputStream statistics.\n+ */\n+public class ITestAbfsOutputStream extends AbstractAbfsIntegrationTest {\n+\n+  public ITestAbfsOutputStream() throws Exception {\n+  }\n+\n+  /**\n+   * Tests to check bytes Uploading in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamUploadingBytes() throws IOException {\n+    describe(\"Testing Bytes uploaded in AbfsOutputSteam\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path uploadBytesFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+    abfss.getAbfsConfiguration().setDisableOutputStreamFlush(false);\n+    String testBytesToUpload = \"bytes\";\n+\n+    AbfsOutputStream outForSomeBytes = null;\n+    try {\n+      outForSomeBytes = (AbfsOutputStream) abfss.createFile(uploadBytesFilePath,\n+          statistics,\n+          true,\n+          FsPermission.getDefault(), FsPermission.getUMask(fs.getConf()));\n+\n+      //Test for zero bytes To upload\n+      assertValues(\"bytes to upload\", 0,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload);\n+\n+      outForSomeBytes.write(testBytesToUpload.getBytes());\n+      outForSomeBytes.flush();\n+\n+      //Test for some bytes to upload\n+      assertValues(\"bytes to upload\", testBytesToUpload.getBytes().length,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload);\n+\n+      //Test for relation between bytesUploadSuccessful, bytesUploadFailed\n+      // and bytesToUpload\n+      assertValues(\"bytesUploadSuccessful equal to difference between \"\n+              + \"bytesToUpload and bytesUploadFailed\",\n+          outForSomeBytes.getOutputStreamStatistics().bytesUploadSuccessful,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload -\n+              outForSomeBytes.getOutputStreamStatistics().bytesUploadFailed);\n+\n+    } finally {\n+      if (outForSomeBytes != null) {\n+        outForSomeBytes.close();\n+      }\n+    }\n+\n+    AbfsOutputStream outForLargeBytes = null;\n+    try {\n+      outForLargeBytes =\n+          (AbfsOutputStream) abfss.createFile(uploadBytesFilePath,\n+              statistics\n+              , true, FsPermission.getDefault(),\n+              FsPermission.getUMask(fs.getConf()));\n+\n+      int largeValue = 100000;\n+      for (int i = 0; i < largeValue; i++) {\n+        outForLargeBytes.write(testBytesToUpload.getBytes());\n+      }\n+      outForLargeBytes.flush();\n+\n+      //Test for large bytes to upload\n+      assertValues(\"bytes to upload\",\n+          largeValue * (testBytesToUpload.getBytes().length),\n+          outForLargeBytes.getOutputStreamStatistics().bytesToUpload);\n+\n+      //Test for relation between bytesUploadSuccessful, bytesUploadFailed\n+      // and bytesToUpload\n+      assertValues(\"bytesUploadSuccessful equal to difference between \"\n+              + \"bytesToUpload and bytesUploadFailed\",\n+          outForSomeBytes.getOutputStreamStatistics().bytesUploadSuccessful,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload -\n+              outForSomeBytes.getOutputStreamStatistics().bytesUploadFailed);\n+\n+    } finally {\n+      if (outForLargeBytes != null) {\n+        outForLargeBytes.close();\n+      }\n+    }\n+\n+  }\n+\n+  /**\n+   * Tests to check time spend on waiting for tasks to be complete on a\n+   * blocking queue in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamTimeSpendOnWaitTask() throws IOException {\n+    describe(\"Testing Time Spend on Waiting for Task to be complete\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path timeSpendFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+\n+    AbfsOutputStream out =\n+        (AbfsOutputStream) abfss.createFile(timeSpendFilePath,\n+            statistics, true,\n+            FsPermission.getDefault(), FsPermission.getUMask(fs.getConf()));\n+\n+  }\n+\n+  /**\n+   * Tests to check number of {@codes shrinkWriteOperationQueue()}\n+   * calls.\n+   * After writing data, AbfsOutputStream doesn't upload the data until\n+   * Flushed. Hence, flush() method is called after write() to test Queue\n+   * shrink calls.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamQueueShrink() throws IOException {\n+    describe(\"Testing Queue Shrink calls in AbfsOutputStream\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path queueShrinkFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    abfss.getAbfsConfiguration().setDisableOutputStreamFlush(false);\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+    String testQueueShrink = \"testQueue\";\n+\n+    AbfsOutputStream outForOneOp = null;\n+\n+    try {\n+      outForOneOp =\n+          (AbfsOutputStream) abfss.createFile(queueShrinkFilePath, statistics,\n+              true,\n+              FsPermission.getDefault(), FsPermission.getUMask(fs.getConf()));\n+\n+      //Test for shrinking Queue zero time\n+      assertValues(\"number of queueShrink() Calls\", 0,\n+          outForOneOp.getOutputStreamStatistics().queueShrink);\n+\n+      outForOneOp.write(testQueueShrink.getBytes());\n+      // Queue is shrunk 2 times when outStream is flushed\n+      outForOneOp.flush();\n+\n+      //Test for shrinking Queue 2 times\n+      assertValues(\"number of queueShrink() Calls\", 2,\n+          outForOneOp.getOutputStreamStatistics().queueShrink);\n+\n+    } finally {\n+      if (outForOneOp != null) {\n+        outForOneOp.close();\n+      }\n+    }\n+\n+    AbfsOutputStream outForLargeOps = null;\n+\n+    try {\n+      outForLargeOps = (AbfsOutputStream) abfss.createFile(queueShrinkFilePath,\n+          statistics, true,\n+          FsPermission.getDefault(), FsPermission.getUMask(fs.getConf()));\n+\n+      int largeValue = 1000;\n+      //QueueShrink is called 2 times in 1 flush(), hence 1000 flushes must\n+      // give 2000 QueueShrink calls\n+      for (int i = 0; i < largeValue; i++) {\n+        outForLargeOps.write(testQueueShrink.getBytes());\n+        //Flush is quite expensive so 1000 calls only which takes 1 min+\n+        outForLargeOps.flush();\n+      }\n+\n+      //Test for 2000 queue shrink calls\n+      assertValues(\"number of queueShrink() Calls\",\n+          2 * largeValue,\n+          outForLargeOps.getOutputStreamStatistics().queueShrink);\n+    } finally {\n+      if (outForLargeOps != null) {\n+        outForLargeOps.close();\n+      }\n+    }\n+\n+  }\n+\n+  /**\n+   * Test to check number of {@codes writeCurrentBufferToService()}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e"}, "originalPosition": 197}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc1MTc1MA==", "bodyText": "try-with-resources or IOUtils.closeQuietly", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r395751750", "createdAt": "2020-03-20T16:27:22Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,278 @@\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+\n+/**\n+ * Test AbfsOutputStream statistics.\n+ */\n+public class ITestAbfsOutputStream extends AbstractAbfsIntegrationTest {\n+\n+  public ITestAbfsOutputStream() throws Exception {\n+  }\n+\n+  /**\n+   * Tests to check bytes Uploading in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamUploadingBytes() throws IOException {\n+    describe(\"Testing Bytes uploaded in AbfsOutputSteam\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path uploadBytesFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+    abfss.getAbfsConfiguration().setDisableOutputStreamFlush(false);\n+    String testBytesToUpload = \"bytes\";\n+\n+    AbfsOutputStream outForSomeBytes = null;\n+    try {\n+      outForSomeBytes = (AbfsOutputStream) abfss.createFile(uploadBytesFilePath,\n+          statistics,\n+          true,\n+          FsPermission.getDefault(), FsPermission.getUMask(fs.getConf()));\n+\n+      //Test for zero bytes To upload\n+      assertValues(\"bytes to upload\", 0,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload);\n+\n+      outForSomeBytes.write(testBytesToUpload.getBytes());\n+      outForSomeBytes.flush();\n+\n+      //Test for some bytes to upload\n+      assertValues(\"bytes to upload\", testBytesToUpload.getBytes().length,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload);\n+\n+      //Test for relation between bytesUploadSuccessful, bytesUploadFailed\n+      // and bytesToUpload\n+      assertValues(\"bytesUploadSuccessful equal to difference between \"\n+              + \"bytesToUpload and bytesUploadFailed\",\n+          outForSomeBytes.getOutputStreamStatistics().bytesUploadSuccessful,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload -\n+              outForSomeBytes.getOutputStreamStatistics().bytesUploadFailed);\n+\n+    } finally {\n+      if (outForSomeBytes != null) {\n+        outForSomeBytes.close();\n+      }\n+    }\n+\n+    AbfsOutputStream outForLargeBytes = null;\n+    try {\n+      outForLargeBytes =\n+          (AbfsOutputStream) abfss.createFile(uploadBytesFilePath,\n+              statistics\n+              , true, FsPermission.getDefault(),\n+              FsPermission.getUMask(fs.getConf()));\n+\n+      int largeValue = 100000;\n+      for (int i = 0; i < largeValue; i++) {\n+        outForLargeBytes.write(testBytesToUpload.getBytes());\n+      }\n+      outForLargeBytes.flush();\n+\n+      //Test for large bytes to upload\n+      assertValues(\"bytes to upload\",\n+          largeValue * (testBytesToUpload.getBytes().length),\n+          outForLargeBytes.getOutputStreamStatistics().bytesToUpload);\n+\n+      //Test for relation between bytesUploadSuccessful, bytesUploadFailed\n+      // and bytesToUpload\n+      assertValues(\"bytesUploadSuccessful equal to difference between \"\n+              + \"bytesToUpload and bytesUploadFailed\",\n+          outForSomeBytes.getOutputStreamStatistics().bytesUploadSuccessful,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload -\n+              outForSomeBytes.getOutputStreamStatistics().bytesUploadFailed);\n+\n+    } finally {\n+      if (outForLargeBytes != null) {\n+        outForLargeBytes.close();\n+      }\n+    }\n+\n+  }\n+\n+  /**\n+   * Tests to check time spend on waiting for tasks to be complete on a\n+   * blocking queue in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamTimeSpendOnWaitTask() throws IOException {\n+    describe(\"Testing Time Spend on Waiting for Task to be complete\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path timeSpendFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+\n+    AbfsOutputStream out =\n+        (AbfsOutputStream) abfss.createFile(timeSpendFilePath,\n+            statistics, true,\n+            FsPermission.getDefault(), FsPermission.getUMask(fs.getConf()));\n+\n+  }\n+\n+  /**\n+   * Tests to check number of {@codes shrinkWriteOperationQueue()}\n+   * calls.\n+   * After writing data, AbfsOutputStream doesn't upload the data until\n+   * Flushed. Hence, flush() method is called after write() to test Queue\n+   * shrink calls.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamQueueShrink() throws IOException {\n+    describe(\"Testing Queue Shrink calls in AbfsOutputStream\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path queueShrinkFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    abfss.getAbfsConfiguration().setDisableOutputStreamFlush(false);\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+    String testQueueShrink = \"testQueue\";\n+\n+    AbfsOutputStream outForOneOp = null;\n+\n+    try {\n+      outForOneOp =\n+          (AbfsOutputStream) abfss.createFile(queueShrinkFilePath, statistics,\n+              true,\n+              FsPermission.getDefault(), FsPermission.getUMask(fs.getConf()));\n+\n+      //Test for shrinking Queue zero time\n+      assertValues(\"number of queueShrink() Calls\", 0,\n+          outForOneOp.getOutputStreamStatistics().queueShrink);\n+\n+      outForOneOp.write(testQueueShrink.getBytes());\n+      // Queue is shrunk 2 times when outStream is flushed\n+      outForOneOp.flush();\n+\n+      //Test for shrinking Queue 2 times\n+      assertValues(\"number of queueShrink() Calls\", 2,\n+          outForOneOp.getOutputStreamStatistics().queueShrink);\n+\n+    } finally {\n+      if (outForOneOp != null) {\n+        outForOneOp.close();\n+      }\n+    }\n+\n+    AbfsOutputStream outForLargeOps = null;\n+\n+    try {\n+      outForLargeOps = (AbfsOutputStream) abfss.createFile(queueShrinkFilePath,\n+          statistics, true,\n+          FsPermission.getDefault(), FsPermission.getUMask(fs.getConf()));\n+\n+      int largeValue = 1000;\n+      //QueueShrink is called 2 times in 1 flush(), hence 1000 flushes must\n+      // give 2000 QueueShrink calls\n+      for (int i = 0; i < largeValue; i++) {\n+        outForLargeOps.write(testQueueShrink.getBytes());\n+        //Flush is quite expensive so 1000 calls only which takes 1 min+\n+        outForLargeOps.flush();\n+      }\n+\n+      //Test for 2000 queue shrink calls\n+      assertValues(\"number of queueShrink() Calls\",\n+          2 * largeValue,\n+          outForLargeOps.getOutputStreamStatistics().queueShrink);\n+    } finally {\n+      if (outForLargeOps != null) {\n+        outForLargeOps.close();\n+      }\n+    }\n+\n+  }\n+\n+  /**\n+   * Test to check number of {@codes writeCurrentBufferToService()}\n+   * calls.\n+   * After writing data, AbfsOutputStream doesn't upload data till flush() is\n+   * called. Hence, flush() calls were made after write() to simulate the\n+   * scenario.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamWriteBuffer() throws IOException {\n+    describe(\"Testing writeCurrentBufferToService() calls\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path writeBufferFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+    abfss.getAbfsConfiguration().setDisableOutputStreamFlush(false);\n+    String testWriteBuffer = \"Buffer\";\n+\n+    AbfsOutputStream outForOneOp = null;\n+\n+    try {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e"}, "originalPosition": 217}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc1MTg3MA==", "bodyText": "try-with-resources or IOUtils.closeQuietly", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r395751870", "createdAt": "2020-03-20T16:27:35Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,278 @@\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+\n+/**\n+ * Test AbfsOutputStream statistics.\n+ */\n+public class ITestAbfsOutputStream extends AbstractAbfsIntegrationTest {\n+\n+  public ITestAbfsOutputStream() throws Exception {\n+  }\n+\n+  /**\n+   * Tests to check bytes Uploading in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamUploadingBytes() throws IOException {\n+    describe(\"Testing Bytes uploaded in AbfsOutputSteam\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path uploadBytesFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+    abfss.getAbfsConfiguration().setDisableOutputStreamFlush(false);\n+    String testBytesToUpload = \"bytes\";\n+\n+    AbfsOutputStream outForSomeBytes = null;\n+    try {\n+      outForSomeBytes = (AbfsOutputStream) abfss.createFile(uploadBytesFilePath,\n+          statistics,\n+          true,\n+          FsPermission.getDefault(), FsPermission.getUMask(fs.getConf()));\n+\n+      //Test for zero bytes To upload\n+      assertValues(\"bytes to upload\", 0,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload);\n+\n+      outForSomeBytes.write(testBytesToUpload.getBytes());\n+      outForSomeBytes.flush();\n+\n+      //Test for some bytes to upload\n+      assertValues(\"bytes to upload\", testBytesToUpload.getBytes().length,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload);\n+\n+      //Test for relation between bytesUploadSuccessful, bytesUploadFailed\n+      // and bytesToUpload\n+      assertValues(\"bytesUploadSuccessful equal to difference between \"\n+              + \"bytesToUpload and bytesUploadFailed\",\n+          outForSomeBytes.getOutputStreamStatistics().bytesUploadSuccessful,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload -\n+              outForSomeBytes.getOutputStreamStatistics().bytesUploadFailed);\n+\n+    } finally {\n+      if (outForSomeBytes != null) {\n+        outForSomeBytes.close();\n+      }\n+    }\n+\n+    AbfsOutputStream outForLargeBytes = null;\n+    try {\n+      outForLargeBytes =\n+          (AbfsOutputStream) abfss.createFile(uploadBytesFilePath,\n+              statistics\n+              , true, FsPermission.getDefault(),\n+              FsPermission.getUMask(fs.getConf()));\n+\n+      int largeValue = 100000;\n+      for (int i = 0; i < largeValue; i++) {\n+        outForLargeBytes.write(testBytesToUpload.getBytes());\n+      }\n+      outForLargeBytes.flush();\n+\n+      //Test for large bytes to upload\n+      assertValues(\"bytes to upload\",\n+          largeValue * (testBytesToUpload.getBytes().length),\n+          outForLargeBytes.getOutputStreamStatistics().bytesToUpload);\n+\n+      //Test for relation between bytesUploadSuccessful, bytesUploadFailed\n+      // and bytesToUpload\n+      assertValues(\"bytesUploadSuccessful equal to difference between \"\n+              + \"bytesToUpload and bytesUploadFailed\",\n+          outForSomeBytes.getOutputStreamStatistics().bytesUploadSuccessful,\n+          outForSomeBytes.getOutputStreamStatistics().bytesToUpload -\n+              outForSomeBytes.getOutputStreamStatistics().bytesUploadFailed);\n+\n+    } finally {\n+      if (outForLargeBytes != null) {\n+        outForLargeBytes.close();\n+      }\n+    }\n+\n+  }\n+\n+  /**\n+   * Tests to check time spend on waiting for tasks to be complete on a\n+   * blocking queue in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamTimeSpendOnWaitTask() throws IOException {\n+    describe(\"Testing Time Spend on Waiting for Task to be complete\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path timeSpendFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+\n+    AbfsOutputStream out =\n+        (AbfsOutputStream) abfss.createFile(timeSpendFilePath,\n+            statistics, true,\n+            FsPermission.getDefault(), FsPermission.getUMask(fs.getConf()));\n+\n+  }\n+\n+  /**\n+   * Tests to check number of {@codes shrinkWriteOperationQueue()}\n+   * calls.\n+   * After writing data, AbfsOutputStream doesn't upload the data until\n+   * Flushed. Hence, flush() method is called after write() to test Queue\n+   * shrink calls.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamQueueShrink() throws IOException {\n+    describe(\"Testing Queue Shrink calls in AbfsOutputStream\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path queueShrinkFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    abfss.getAbfsConfiguration().setDisableOutputStreamFlush(false);\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+    String testQueueShrink = \"testQueue\";\n+\n+    AbfsOutputStream outForOneOp = null;\n+\n+    try {\n+      outForOneOp =\n+          (AbfsOutputStream) abfss.createFile(queueShrinkFilePath, statistics,\n+              true,\n+              FsPermission.getDefault(), FsPermission.getUMask(fs.getConf()));\n+\n+      //Test for shrinking Queue zero time\n+      assertValues(\"number of queueShrink() Calls\", 0,\n+          outForOneOp.getOutputStreamStatistics().queueShrink);\n+\n+      outForOneOp.write(testQueueShrink.getBytes());\n+      // Queue is shrunk 2 times when outStream is flushed\n+      outForOneOp.flush();\n+\n+      //Test for shrinking Queue 2 times\n+      assertValues(\"number of queueShrink() Calls\", 2,\n+          outForOneOp.getOutputStreamStatistics().queueShrink);\n+\n+    } finally {\n+      if (outForOneOp != null) {\n+        outForOneOp.close();\n+      }\n+    }\n+\n+    AbfsOutputStream outForLargeOps = null;\n+\n+    try {\n+      outForLargeOps = (AbfsOutputStream) abfss.createFile(queueShrinkFilePath,\n+          statistics, true,\n+          FsPermission.getDefault(), FsPermission.getUMask(fs.getConf()));\n+\n+      int largeValue = 1000;\n+      //QueueShrink is called 2 times in 1 flush(), hence 1000 flushes must\n+      // give 2000 QueueShrink calls\n+      for (int i = 0; i < largeValue; i++) {\n+        outForLargeOps.write(testQueueShrink.getBytes());\n+        //Flush is quite expensive so 1000 calls only which takes 1 min+\n+        outForLargeOps.flush();\n+      }\n+\n+      //Test for 2000 queue shrink calls\n+      assertValues(\"number of queueShrink() Calls\",\n+          2 * largeValue,\n+          outForLargeOps.getOutputStreamStatistics().queueShrink);\n+    } finally {\n+      if (outForLargeOps != null) {\n+        outForLargeOps.close();\n+      }\n+    }\n+\n+  }\n+\n+  /**\n+   * Test to check number of {@codes writeCurrentBufferToService()}\n+   * calls.\n+   * After writing data, AbfsOutputStream doesn't upload data till flush() is\n+   * called. Hence, flush() calls were made after write() to simulate the\n+   * scenario.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamWriteBuffer() throws IOException {\n+    describe(\"Testing writeCurrentBufferToService() calls\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path writeBufferFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+    abfss.getAbfsConfiguration().setDisableOutputStreamFlush(false);\n+    String testWriteBuffer = \"Buffer\";\n+\n+    AbfsOutputStream outForOneOp = null;\n+\n+    try {\n+      outForOneOp =\n+          (AbfsOutputStream) abfss.createFile(writeBufferFilePath, statistics,\n+              true,\n+              FsPermission.getDefault(), FsPermission.getUMask(fs.getConf()));\n+\n+      //Test for zero time writing Buffer to service\n+      assertValues(\"number writeCurrentBufferToService() calls\", 0,\n+          outForOneOp.getOutputStreamStatistics().writeCurrentBufferOperations);\n+\n+      outForOneOp.write(testWriteBuffer.getBytes());\n+      outForOneOp.flush();\n+\n+      //Test for one time writeCurrentBuffer() call\n+      assertValues(\"number writeCurrentBufferToService() calls\", 1,\n+          outForOneOp.getOutputStreamStatistics().writeCurrentBufferOperations);\n+    } finally {\n+      if (outForOneOp != null) {\n+        outForOneOp.close();\n+      }\n+    }\n+\n+    AbfsOutputStream outForLargeOps = null;\n+    try {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e"}, "originalPosition": 240}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc1MjYwOQ==", "bodyText": "This is sounds like a slow test.\n\nUse smaller values than 1000, e.g. \"10\"\nmake the value a constant used across all tests.", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r395752609", "createdAt": "2020-03-20T16:28:41Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,278 @@\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+\n+/**\n+ * Test AbfsOutputStream statistics.\n+ */\n+public class ITestAbfsOutputStream extends AbstractAbfsIntegrationTest {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc1Mjk0Nw==", "bodyText": "not needed", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r395752947", "createdAt": "2020-03-20T16:29:12Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsStreamStatistics.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+\n+/**\n+ * Test Abfs Stream.\n+ */\n+\n+public class ITestAbfsStreamStatistics extends AbstractAbfsIntegrationTest {\n+  public ITestAbfsStreamStatistics() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc1MzIwNQ==", "bodyText": "IOUtils.closeQuietly", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r395753205", "createdAt": "2020-03-20T16:29:38Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsStreamStatistics.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+\n+/**\n+ * Test Abfs Stream.\n+ */\n+\n+public class ITestAbfsStreamStatistics extends AbstractAbfsIntegrationTest {\n+  public ITestAbfsStreamStatistics() throws Exception {\n+  }\n+\n+  /***\n+   * Testing {@code incrementReadOps()} in class {@code AbfsInputStream} and\n+   * {@code incrementWriteOps()} in class {@code AbfsOutputStream}.\n+   *\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testAbfsStreamOps() throws Exception {\n+    describe(\"Test to see correct population of read and write operations in \"\n+        + \"Abfs\");\n+\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path smallOperationsFile = new Path(\"testOneReadWriteOps\");\n+    Path largeOperationsFile = new Path(\"testLargeReadWriteOps\");\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+    String testReadWriteOps = \"test this\";\n+    statistics.reset();\n+\n+    //Test for zero write operation\n+    assertReadWriteOps(\"write\", 0, statistics.getWriteOps());\n+\n+    //Test for zero read operation\n+    assertReadWriteOps(\"read\", 0, statistics.getReadOps());\n+\n+    FSDataOutputStream outForOneOperation = null;\n+    FSDataInputStream inForOneOperation = null;\n+    try {\n+      outForOneOperation = fs.create(smallOperationsFile);\n+      statistics.reset();\n+      outForOneOperation.write(testReadWriteOps.getBytes());\n+\n+      //Test for a single write operation\n+      assertReadWriteOps(\"write\", 1, statistics.getWriteOps());\n+\n+      inForOneOperation = fs.open(smallOperationsFile);\n+      inForOneOperation.read(testReadWriteOps.getBytes(), 0,\n+          testReadWriteOps.getBytes().length);\n+\n+      //Test for a single read operation\n+      assertReadWriteOps(\"read\", 1, statistics.getReadOps());\n+\n+    } finally {\n+      if (inForOneOperation != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc1MzQ0Mg==", "bodyText": "once validateContent raises exceptions, you don't need to wrap in an assert", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r395753442", "createdAt": "2020-03-20T16:29:59Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsStreamStatistics.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+\n+/**\n+ * Test Abfs Stream.\n+ */\n+\n+public class ITestAbfsStreamStatistics extends AbstractAbfsIntegrationTest {\n+  public ITestAbfsStreamStatistics() throws Exception {\n+  }\n+\n+  /***\n+   * Testing {@code incrementReadOps()} in class {@code AbfsInputStream} and\n+   * {@code incrementWriteOps()} in class {@code AbfsOutputStream}.\n+   *\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testAbfsStreamOps() throws Exception {\n+    describe(\"Test to see correct population of read and write operations in \"\n+        + \"Abfs\");\n+\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path smallOperationsFile = new Path(\"testOneReadWriteOps\");\n+    Path largeOperationsFile = new Path(\"testLargeReadWriteOps\");\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+    String testReadWriteOps = \"test this\";\n+    statistics.reset();\n+\n+    //Test for zero write operation\n+    assertReadWriteOps(\"write\", 0, statistics.getWriteOps());\n+\n+    //Test for zero read operation\n+    assertReadWriteOps(\"read\", 0, statistics.getReadOps());\n+\n+    FSDataOutputStream outForOneOperation = null;\n+    FSDataInputStream inForOneOperation = null;\n+    try {\n+      outForOneOperation = fs.create(smallOperationsFile);\n+      statistics.reset();\n+      outForOneOperation.write(testReadWriteOps.getBytes());\n+\n+      //Test for a single write operation\n+      assertReadWriteOps(\"write\", 1, statistics.getWriteOps());\n+\n+      inForOneOperation = fs.open(smallOperationsFile);\n+      inForOneOperation.read(testReadWriteOps.getBytes(), 0,\n+          testReadWriteOps.getBytes().length);\n+\n+      //Test for a single read operation\n+      assertReadWriteOps(\"read\", 1, statistics.getReadOps());\n+\n+    } finally {\n+      if (inForOneOperation != null) {\n+        inForOneOperation.close();\n+      }\n+      if (outForOneOperation != null) {\n+        outForOneOperation.close();\n+      }\n+    }\n+\n+    //Validating if content is being written in the smallOperationsFile\n+    Assert.assertTrue(\"Mismatch in content validation\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e"}, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc1MzgyMg==", "bodyText": "IOUtils.closeQuietly", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r395753822", "createdAt": "2020-03-20T16:30:33Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsStreamStatistics.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+\n+/**\n+ * Test Abfs Stream.\n+ */\n+\n+public class ITestAbfsStreamStatistics extends AbstractAbfsIntegrationTest {\n+  public ITestAbfsStreamStatistics() throws Exception {\n+  }\n+\n+  /***\n+   * Testing {@code incrementReadOps()} in class {@code AbfsInputStream} and\n+   * {@code incrementWriteOps()} in class {@code AbfsOutputStream}.\n+   *\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testAbfsStreamOps() throws Exception {\n+    describe(\"Test to see correct population of read and write operations in \"\n+        + \"Abfs\");\n+\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path smallOperationsFile = new Path(\"testOneReadWriteOps\");\n+    Path largeOperationsFile = new Path(\"testLargeReadWriteOps\");\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+    String testReadWriteOps = \"test this\";\n+    statistics.reset();\n+\n+    //Test for zero write operation\n+    assertReadWriteOps(\"write\", 0, statistics.getWriteOps());\n+\n+    //Test for zero read operation\n+    assertReadWriteOps(\"read\", 0, statistics.getReadOps());\n+\n+    FSDataOutputStream outForOneOperation = null;\n+    FSDataInputStream inForOneOperation = null;\n+    try {\n+      outForOneOperation = fs.create(smallOperationsFile);\n+      statistics.reset();\n+      outForOneOperation.write(testReadWriteOps.getBytes());\n+\n+      //Test for a single write operation\n+      assertReadWriteOps(\"write\", 1, statistics.getWriteOps());\n+\n+      inForOneOperation = fs.open(smallOperationsFile);\n+      inForOneOperation.read(testReadWriteOps.getBytes(), 0,\n+          testReadWriteOps.getBytes().length);\n+\n+      //Test for a single read operation\n+      assertReadWriteOps(\"read\", 1, statistics.getReadOps());\n+\n+    } finally {\n+      if (inForOneOperation != null) {\n+        inForOneOperation.close();\n+      }\n+      if (outForOneOperation != null) {\n+        outForOneOperation.close();\n+      }\n+    }\n+\n+    //Validating if content is being written in the smallOperationsFile\n+    Assert.assertTrue(\"Mismatch in content validation\",\n+        validateContent(fs, smallOperationsFile,\n+            testReadWriteOps.getBytes()));\n+\n+    FSDataOutputStream outForLargeOperations = null;\n+    FSDataInputStream inForLargeOperations = null;\n+    StringBuilder largeOperationsValidationString = new StringBuilder();\n+    try {\n+      outForLargeOperations = fs.create(largeOperationsFile);\n+      statistics.reset();\n+      int largeValue = 1000000;\n+      for (int i = 0; i < largeValue; i++) {\n+        outForLargeOperations.write(testReadWriteOps.getBytes());\n+\n+        //Creating the String for content Validation\n+        largeOperationsValidationString.append(testReadWriteOps);\n+      }\n+\n+      //Test for 1000000 write operations\n+      assertReadWriteOps(\"write\", largeValue, statistics.getWriteOps());\n+\n+      inForLargeOperations = fs.open(largeOperationsFile);\n+      for (int i = 0; i < largeValue; i++)\n+        inForLargeOperations\n+            .read(testReadWriteOps.getBytes(), 0,\n+                testReadWriteOps.getBytes().length);\n+\n+      //Test for 1000000 read operations\n+      assertReadWriteOps(\"read\", largeValue, statistics.getReadOps());\n+\n+    } finally {\n+      if (inForLargeOperations != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc1NDEwMg==", "bodyText": "again, superflous with validateContent raising exceptions", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r395754102", "createdAt": "2020-03-20T16:31:01Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsStreamStatistics.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+\n+/**\n+ * Test Abfs Stream.\n+ */\n+\n+public class ITestAbfsStreamStatistics extends AbstractAbfsIntegrationTest {\n+  public ITestAbfsStreamStatistics() throws Exception {\n+  }\n+\n+  /***\n+   * Testing {@code incrementReadOps()} in class {@code AbfsInputStream} and\n+   * {@code incrementWriteOps()} in class {@code AbfsOutputStream}.\n+   *\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testAbfsStreamOps() throws Exception {\n+    describe(\"Test to see correct population of read and write operations in \"\n+        + \"Abfs\");\n+\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path smallOperationsFile = new Path(\"testOneReadWriteOps\");\n+    Path largeOperationsFile = new Path(\"testLargeReadWriteOps\");\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+    String testReadWriteOps = \"test this\";\n+    statistics.reset();\n+\n+    //Test for zero write operation\n+    assertReadWriteOps(\"write\", 0, statistics.getWriteOps());\n+\n+    //Test for zero read operation\n+    assertReadWriteOps(\"read\", 0, statistics.getReadOps());\n+\n+    FSDataOutputStream outForOneOperation = null;\n+    FSDataInputStream inForOneOperation = null;\n+    try {\n+      outForOneOperation = fs.create(smallOperationsFile);\n+      statistics.reset();\n+      outForOneOperation.write(testReadWriteOps.getBytes());\n+\n+      //Test for a single write operation\n+      assertReadWriteOps(\"write\", 1, statistics.getWriteOps());\n+\n+      inForOneOperation = fs.open(smallOperationsFile);\n+      inForOneOperation.read(testReadWriteOps.getBytes(), 0,\n+          testReadWriteOps.getBytes().length);\n+\n+      //Test for a single read operation\n+      assertReadWriteOps(\"read\", 1, statistics.getReadOps());\n+\n+    } finally {\n+      if (inForOneOperation != null) {\n+        inForOneOperation.close();\n+      }\n+      if (outForOneOperation != null) {\n+        outForOneOperation.close();\n+      }\n+    }\n+\n+    //Validating if content is being written in the smallOperationsFile\n+    Assert.assertTrue(\"Mismatch in content validation\",\n+        validateContent(fs, smallOperationsFile,\n+            testReadWriteOps.getBytes()));\n+\n+    FSDataOutputStream outForLargeOperations = null;\n+    FSDataInputStream inForLargeOperations = null;\n+    StringBuilder largeOperationsValidationString = new StringBuilder();\n+    try {\n+      outForLargeOperations = fs.create(largeOperationsFile);\n+      statistics.reset();\n+      int largeValue = 1000000;\n+      for (int i = 0; i < largeValue; i++) {\n+        outForLargeOperations.write(testReadWriteOps.getBytes());\n+\n+        //Creating the String for content Validation\n+        largeOperationsValidationString.append(testReadWriteOps);\n+      }\n+\n+      //Test for 1000000 write operations\n+      assertReadWriteOps(\"write\", largeValue, statistics.getWriteOps());\n+\n+      inForLargeOperations = fs.open(largeOperationsFile);\n+      for (int i = 0; i < largeValue; i++)\n+        inForLargeOperations\n+            .read(testReadWriteOps.getBytes(), 0,\n+                testReadWriteOps.getBytes().length);\n+\n+      //Test for 1000000 read operations\n+      assertReadWriteOps(\"read\", largeValue, statistics.getReadOps());\n+\n+    } finally {\n+      if (inForLargeOperations != null) {\n+        inForLargeOperations.close();\n+      }\n+      if (outForLargeOperations != null) {\n+        outForLargeOperations.close();\n+      }\n+    }\n+\n+    //Validating if content is being written in largeOperationsFile\n+    Assert.assertTrue(\"Mismatch in content validation\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e"}, "originalPosition": 128}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc1NDUxOQ==", "bodyText": "add a .close(), even if the original code didn't. Always good to improve a test", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r395754519", "createdAt": "2020-03-20T16:31:42Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemOauth.java", "diffHunk": "@@ -143,7 +143,7 @@ public void testBlobDataReader() throws Exception {\n \n     // TEST WRITE FILE\n     try {\n-      abfsStore.openFileForWrite(EXISTED_FILE_PATH, true);\n+      abfsStore.openFileForWrite(EXISTED_FILE_PATH, fs.getFsStatistics(), true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc1NTM4MQ==", "bodyText": "let's make these private and have getters", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r395755381", "createdAt": "2020-03-20T16:33:05Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStreamStatisticsImpl.java", "diffHunk": "@@ -0,0 +1,102 @@\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+/**\n+ * OutputStream Statistics Implementation for Abfs.\n+ * timeSpendOnTaskWait - Time spend on waiting for tasks to be complete on\n+ * Blocking Queue in AbfsOutputStream.\n+ *\n+ * queueShrink - Number of times Blocking Queue was shrunk after writing\n+ * data.\n+ *\n+ * WriteCurrentBufferOperations - Number of times\n+ * {@codes writeCurrentBufferToService()} calls were made.\n+ */\n+public class AbfsOutputStreamStatisticsImpl\n+    implements AbfsOutputStreamStatistics {\n+  public volatile long bytesToUpload;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc1NjI0OA==", "bodyText": "I can't think of any. Maybe just have a unit test to take an AbfsOutputStreamsImpl and verify that when the method is called, the counter is updated.\n(Actually, mocking could simulate failure, ...)", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r395756248", "createdAt": "2020-03-20T16:34:38Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsOutputStream.java", "diffHunk": "@@ -28,28 +26,38 @@ public ITestAbfsOutputStream() throws Exception {\n   public void testAbfsOutputStreamUploadingBytes() throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDIyNzg5NA=="}, "originalCommit": {"oid": "20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e"}, "originalPosition": 15}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/20e1a7f80c6a0bb50889a7ab0d5c4a51ad8b134e", "committedDate": "2020-03-18T08:23:48Z", "message": "Fixing issues"}, "afterCommit": {"oid": "0e7e9b2c77b19c5704af07b102633a5fe573eb9e", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/0e7e9b2c77b19c5704af07b102633a5fe573eb9e", "committedDate": "2020-04-01T10:20:09Z", "message": "HADOOP-16914. Fixing review comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg2MTU2NDAy", "url": "https://github.com/apache/hadoop/pull/1899#pullrequestreview-386156402", "createdAt": "2020-04-02T07:07:41Z", "commit": {"oid": "0e7e9b2c77b19c5704af07b102633a5fe573eb9e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMlQwNzowNzo0MlrOF_d8SQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMlQwNzowNzo0MlrOF_d8SQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjA5NTE3Nw==", "bodyText": "explain more", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r402095177", "createdAt": "2020-04-02T07:07:42Z", "author": {"login": "mehakmeet"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStreamStatistics.java", "diffHunk": "@@ -0,0 +1,78 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.classification.InterfaceStability;\n+\n+/**\n+ * Interface for {@link AbfsOutputStream} statistics.\n+ */\n+@InterfaceStability.Unstable\n+public interface AbfsOutputStreamStatistics {\n+\n+  /**\n+   * Number of bytes to be uploaded.\n+   *\n+   * @param bytes number of bytes to upload\n+   */\n+  void bytesToUpload(long bytes);\n+\n+  /**\n+   * Records a successful upload and the number of bytes uploaded.\n+   *\n+   * @param bytes number of bytes that were successfully uploaded\n+   */\n+  void uploadSuccessful(long bytes);\n+\n+  /**\n+   * Records that upload is failed and the number of bytes.\n+   *\n+   * @param bytes number of bytes that failed to upload\n+   */\n+  void uploadFailed(long bytes);\n+\n+  /**\n+   * Time spent in waiting for tasks to be completed in the blocking Queue.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0e7e9b2c77b19c5704af07b102633a5fe573eb9e"}, "originalPosition": 51}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg2MTU2Njgx", "url": "https://github.com/apache/hadoop/pull/1899#pullrequestreview-386156681", "createdAt": "2020-04-02T07:08:12Z", "commit": {"oid": "0e7e9b2c77b19c5704af07b102633a5fe573eb9e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMlQwNzowODoxMlrOF_d9OQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMlQwNzowODoxMlrOF_d9OQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjA5NTQxNw==", "bodyText": "this java doc above variable name", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r402095417", "createdAt": "2020-04-02T07:08:12Z", "author": {"login": "mehakmeet"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStreamStatisticsImpl.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+/**\n+ * OutputStream Statistics Implementation for Abfs.\n+ * timeSpendOnTaskWait - Time spend on waiting for tasks to be complete on\n+ * Blocking Queue in AbfsOutputStream.\n+ *\n+ * queueShrink - Number of times Blocking Queue was shrunk after writing\n+ * data.\n+ *\n+ * WriteCurrentBufferOperations - Number of times\n+ * {@code writeCurrentBufferToService()} calls were made.\n+ */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0e7e9b2c77b19c5704af07b102633a5fe573eb9e"}, "originalPosition": 31}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg2MTI4NjY4", "url": "https://github.com/apache/hadoop/pull/1899#pullrequestreview-386128668", "createdAt": "2020-04-02T06:07:06Z", "commit": {"oid": "0e7e9b2c77b19c5704af07b102633a5fe573eb9e"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMlQwNjowNzowN1rOF_cdiA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMlQwNjoyMzo1OFrOF_c04w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjA3MDkyMA==", "bodyText": "We should be defining here using the interface not concrete class and initialising it in constructor with concrete class implementation. That whole point of using interface is we can change add the new implementation in future and change the the implementation to be used in constructor using some configuration. Right @steveloughran?", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r402070920", "createdAt": "2020-04-02T06:07:07Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -80,6 +82,7 @@\n           = new ElasticByteBufferPool();\n \n   private final Statistics statistics;\n+  private final AbfsOutputStreamStatisticsImpl outputStreamStatistics;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0e7e9b2c77b19c5704af07b102633a5fe573eb9e"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjA3MzE2Mw==", "bodyText": "Add trailing . in the end for all java docs.", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r402073163", "createdAt": "2020-04-02T06:13:22Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStreamStatistics.java", "diffHunk": "@@ -0,0 +1,78 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.classification.InterfaceStability;\n+\n+/**\n+ * Interface for {@link AbfsOutputStream} statistics.\n+ */\n+@InterfaceStability.Unstable\n+public interface AbfsOutputStreamStatistics {\n+\n+  /**\n+   * Number of bytes to be uploaded.\n+   *\n+   * @param bytes number of bytes to upload", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0e7e9b2c77b19c5704af07b102633a5fe573eb9e"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjA3NTc4NQ==", "bodyText": "Write full method in javadoc like AbfsOutputStream#waitForTaskToComplete() otherwise people will have to figure out where this method actually is.", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r402075785", "createdAt": "2020-04-02T06:21:04Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStreamStatisticsImpl.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+/**\n+ * OutputStream Statistics Implementation for Abfs.\n+ * timeSpendOnTaskWait - Time spend on waiting for tasks to be complete on\n+ * Blocking Queue in AbfsOutputStream.\n+ *\n+ * queueShrink - Number of times Blocking Queue was shrunk after writing\n+ * data.\n+ *\n+ * WriteCurrentBufferOperations - Number of times\n+ * {@code writeCurrentBufferToService()} calls were made.\n+ */\n+public class AbfsOutputStreamStatisticsImpl\n+    implements AbfsOutputStreamStatistics {\n+  private volatile long bytesToUpload;\n+  private volatile long bytesUploadSuccessful;\n+  private volatile long bytesUploadFailed;\n+  private volatile long timeSpendOnTaskWait;\n+  private volatile long queueShrink;\n+  private volatile long writeCurrentBufferOperations;\n+\n+  /**\n+   * Number of bytes uploaded.\n+   *\n+   * @param bytes negative values are ignored\n+   */\n+  @Override\n+  public void bytesToUpload(long bytes) {\n+    if (bytes > 0) {\n+      bytesToUpload += bytes;\n+    }\n+  }\n+\n+  /**\n+   * Upload successful with the number of bytes.\n+   * @param bytes number of bytes that were successfully uploaded\n+   */\n+  @Override\n+  public void uploadSuccessful(long bytes) {\n+    if (bytes > 0) {\n+      bytesUploadSuccessful += bytes;\n+    }\n+  }\n+\n+  /**\n+   * Upload failed and the number of bytes.\n+   *\n+   * @param bytes negative values are ignored\n+   */\n+  @Override\n+  public void uploadFailed(long bytes) {\n+    if (bytes > 0) {\n+      bytesUploadFailed += bytes;\n+    }\n+  }\n+\n+  /**\n+   * Time spent for waiting a task to be completed.\n+   *\n+   * @param startTime on calling {@code waitForTaskToComplete()}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0e7e9b2c77b19c5704af07b102633a5fe573eb9e"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjA3NjU4NA==", "bodyText": "What is the task here? Please explain. This is for all javadocs. There is no harm in writing more lines :P", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r402076584", "createdAt": "2020-04-02T06:23:09Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStreamStatisticsImpl.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+/**\n+ * OutputStream Statistics Implementation for Abfs.\n+ * timeSpendOnTaskWait - Time spend on waiting for tasks to be complete on\n+ * Blocking Queue in AbfsOutputStream.\n+ *\n+ * queueShrink - Number of times Blocking Queue was shrunk after writing\n+ * data.\n+ *\n+ * WriteCurrentBufferOperations - Number of times\n+ * {@code writeCurrentBufferToService()} calls were made.\n+ */\n+public class AbfsOutputStreamStatisticsImpl\n+    implements AbfsOutputStreamStatistics {\n+  private volatile long bytesToUpload;\n+  private volatile long bytesUploadSuccessful;\n+  private volatile long bytesUploadFailed;\n+  private volatile long timeSpendOnTaskWait;\n+  private volatile long queueShrink;\n+  private volatile long writeCurrentBufferOperations;\n+\n+  /**\n+   * Number of bytes uploaded.\n+   *\n+   * @param bytes negative values are ignored\n+   */\n+  @Override\n+  public void bytesToUpload(long bytes) {\n+    if (bytes > 0) {\n+      bytesToUpload += bytes;\n+    }\n+  }\n+\n+  /**\n+   * Upload successful with the number of bytes.\n+   * @param bytes number of bytes that were successfully uploaded\n+   */\n+  @Override\n+  public void uploadSuccessful(long bytes) {\n+    if (bytes > 0) {\n+      bytesUploadSuccessful += bytes;\n+    }\n+  }\n+\n+  /**\n+   * Upload failed and the number of bytes.\n+   *\n+   * @param bytes negative values are ignored\n+   */\n+  @Override\n+  public void uploadFailed(long bytes) {\n+    if (bytes > 0) {\n+      bytesUploadFailed += bytes;\n+    }\n+  }\n+\n+  /**\n+   * Time spent for waiting a task to be completed.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0e7e9b2c77b19c5704af07b102633a5fe573eb9e"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjA3Njg5OQ==", "bodyText": "Once again which queue. How is this metrics important??", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r402076899", "createdAt": "2020-04-02T06:23:58Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStreamStatisticsImpl.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+/**\n+ * OutputStream Statistics Implementation for Abfs.\n+ * timeSpendOnTaskWait - Time spend on waiting for tasks to be complete on\n+ * Blocking Queue in AbfsOutputStream.\n+ *\n+ * queueShrink - Number of times Blocking Queue was shrunk after writing\n+ * data.\n+ *\n+ * WriteCurrentBufferOperations - Number of times\n+ * {@code writeCurrentBufferToService()} calls were made.\n+ */\n+public class AbfsOutputStreamStatisticsImpl\n+    implements AbfsOutputStreamStatistics {\n+  private volatile long bytesToUpload;\n+  private volatile long bytesUploadSuccessful;\n+  private volatile long bytesUploadFailed;\n+  private volatile long timeSpendOnTaskWait;\n+  private volatile long queueShrink;\n+  private volatile long writeCurrentBufferOperations;\n+\n+  /**\n+   * Number of bytes uploaded.\n+   *\n+   * @param bytes negative values are ignored\n+   */\n+  @Override\n+  public void bytesToUpload(long bytes) {\n+    if (bytes > 0) {\n+      bytesToUpload += bytes;\n+    }\n+  }\n+\n+  /**\n+   * Upload successful with the number of bytes.\n+   * @param bytes number of bytes that were successfully uploaded\n+   */\n+  @Override\n+  public void uploadSuccessful(long bytes) {\n+    if (bytes > 0) {\n+      bytesUploadSuccessful += bytes;\n+    }\n+  }\n+\n+  /**\n+   * Upload failed and the number of bytes.\n+   *\n+   * @param bytes negative values are ignored\n+   */\n+  @Override\n+  public void uploadFailed(long bytes) {\n+    if (bytes > 0) {\n+      bytesUploadFailed += bytes;\n+    }\n+  }\n+\n+  /**\n+   * Time spent for waiting a task to be completed.\n+   *\n+   * @param startTime on calling {@code waitForTaskToComplete()}\n+   * @param endTime   on method completing\n+   */\n+  @Override\n+  public void timeSpentTaskWait(long startTime, long endTime) {\n+    timeSpendOnTaskWait += endTime - startTime;\n+  }\n+\n+  /**\n+   * Number of calls to {@code shrinkWriteOperationQueue()}.\n+   */\n+  @Override\n+  public void queueShrinked() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0e7e9b2c77b19c5704af07b102633a5fe573eb9e"}, "originalPosition": 91}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "0e7e9b2c77b19c5704af07b102633a5fe573eb9e", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/0e7e9b2c77b19c5704af07b102633a5fe573eb9e", "committedDate": "2020-04-01T10:20:09Z", "message": "HADOOP-16914. Fixing review comments"}, "afterCommit": {"oid": "c8c660884cb509c835e6e4941acdddfabad2d2fe", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/c8c660884cb509c835e6e4941acdddfabad2d2fe", "committedDate": "2020-04-06T11:51:23Z", "message": "HADOOP-16914. Java Docs"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c8c660884cb509c835e6e4941acdddfabad2d2fe", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/c8c660884cb509c835e6e4941acdddfabad2d2fe", "committedDate": "2020-04-06T11:51:23Z", "message": "HADOOP-16914. Java Docs"}, "afterCommit": {"oid": "2bada7fb3bd6bf93de926ffa07fbf346fc3fbf61", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/2bada7fb3bd6bf93de926ffa07fbf346fc3fbf61", "committedDate": "2020-04-06T12:03:06Z", "message": "HADOOP-16914. Java Docs"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg4ODE2MTI4", "url": "https://github.com/apache/hadoop/pull/1899#pullrequestreview-388816128", "createdAt": "2020-04-07T06:26:41Z", "commit": {"oid": "2bada7fb3bd6bf93de926ffa07fbf346fc3fbf61"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwNjoyNjo0MVrOGB0nQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwNjoyNjo0MVrOGB0nQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDU2Mzc3OQ==", "bodyText": "Change name to ITestAbfsOutputStreamStatictics.", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r404563779", "createdAt": "2020-04-07T06:26:41Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+import java.util.Random;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStreamStatisticsImpl;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+\n+/**\n+ * Test AbfsOutputStream statistics.\n+ */\n+public class ITestAbfsOutputStream extends AbstractAbfsIntegrationTest {\n+  public ITestAbfsOutputStream() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2bada7fb3bd6bf93de926ffa07fbf346fc3fbf61"}, "originalPosition": 36}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg4ODQyNjE5", "url": "https://github.com/apache/hadoop/pull/1899#pullrequestreview-388842619", "createdAt": "2020-04-07T07:15:25Z", "commit": {"oid": "2bada7fb3bd6bf93de926ffa07fbf346fc3fbf61"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwNzoxNToyNVrOGB1-0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwNzoxNToyNVrOGB1-0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDU4NjE5NQ==", "bodyText": "use path(methodName.getMethodName());. The current code won't create your files under test directory so the cleanup might miss cleaning up them during teardown.", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r404586195", "createdAt": "2020-04-07T07:15:25Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+import java.util.Random;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStreamStatisticsImpl;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+\n+/**\n+ * Test AbfsOutputStream statistics.\n+ */\n+public class ITestAbfsOutputStream extends AbstractAbfsIntegrationTest {\n+  public ITestAbfsOutputStream() throws Exception {\n+  }\n+\n+  private static final int LARGE_OPERATIONS = 10;\n+  private static final int LOW_RANGE_FOR_RANDOM_VALUE = 49;\n+  private static final int HIGH_RANGE_FOR_RANDOM_VALUE = 9999;\n+\n+  /**\n+   * Tests to check bytes Uploading in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamUploadingBytes() throws IOException {\n+    describe(\"Testing Bytes uploaded in AbfsOutputSteam\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path uploadBytesFilePath = new Path(\"AbfsOutputStreamStatsPath\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2bada7fb3bd6bf93de926ffa07fbf346fc3fbf61"}, "originalPosition": 52}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg4ODQ1Mjcz", "url": "https://github.com/apache/hadoop/pull/1899#pullrequestreview-388845273", "createdAt": "2020-04-07T07:19:27Z", "commit": {"oid": "2bada7fb3bd6bf93de926ffa07fbf346fc3fbf61"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwNzoxOToyN1rOGB2HgQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwNzoxOToyN1rOGB2HgQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDU4ODQxNw==", "bodyText": "extract in variable utForSomeBytes.getOutputStreamStatistics(). like it is done at L65.", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r404588417", "createdAt": "2020-04-07T07:19:27Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+import java.util.Random;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStreamStatisticsImpl;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+\n+/**\n+ * Test AbfsOutputStream statistics.\n+ */\n+public class ITestAbfsOutputStream extends AbstractAbfsIntegrationTest {\n+  public ITestAbfsOutputStream() throws Exception {\n+  }\n+\n+  private static final int LARGE_OPERATIONS = 10;\n+  private static final int LOW_RANGE_FOR_RANDOM_VALUE = 49;\n+  private static final int HIGH_RANGE_FOR_RANDOM_VALUE = 9999;\n+\n+  /**\n+   * Tests to check bytes Uploading in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamUploadingBytes() throws IOException {\n+    describe(\"Testing Bytes uploaded in AbfsOutputSteam\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path uploadBytesFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n+    String testBytesToUpload = \"bytes\";\n+\n+    try (AbfsOutputStream outForSomeBytes = createAbfsOutputStream(fs,\n+        uploadBytesFilePath)\n+    ) {\n+\n+      //Test for zero bytes To upload\n+      assertValues(\"bytes to upload\", 0,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2bada7fb3bd6bf93de926ffa07fbf346fc3fbf61"}, "originalPosition": 60}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg4ODQ1OTk1", "url": "https://github.com/apache/hadoop/pull/1899#pullrequestreview-388845995", "createdAt": "2020-04-07T07:20:31Z", "commit": {"oid": "2bada7fb3bd6bf93de926ffa07fbf346fc3fbf61"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwNzoyMDozMVrOGB2J0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwNzoyMDozMVrOGB2J0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDU4OTAxMQ==", "bodyText": "Same method was present in earlier patch. Move to base class and reuse.", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r404589011", "createdAt": "2020-04-07T07:20:31Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+import java.util.Random;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStreamStatisticsImpl;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+\n+/**\n+ * Test AbfsOutputStream statistics.\n+ */\n+public class ITestAbfsOutputStream extends AbstractAbfsIntegrationTest {\n+  public ITestAbfsOutputStream() throws Exception {\n+  }\n+\n+  private static final int LARGE_OPERATIONS = 10;\n+  private static final int LOW_RANGE_FOR_RANDOM_VALUE = 49;\n+  private static final int HIGH_RANGE_FOR_RANDOM_VALUE = 9999;\n+\n+  /**\n+   * Tests to check bytes Uploading in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamUploadingBytes() throws IOException {\n+    describe(\"Testing Bytes uploaded in AbfsOutputSteam\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path uploadBytesFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n+    String testBytesToUpload = \"bytes\";\n+\n+    try (AbfsOutputStream outForSomeBytes = createAbfsOutputStream(fs,\n+        uploadBytesFilePath)\n+    ) {\n+\n+      //Test for zero bytes To upload\n+      assertValues(\"bytes to upload\", 0,\n+          outForSomeBytes.getOutputStreamStatistics().getBytesToUpload());\n+\n+      outForSomeBytes.write(testBytesToUpload.getBytes());\n+      outForSomeBytes.flush();\n+      AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics =\n+          outForSomeBytes.getOutputStreamStatistics();\n+\n+      //Test for bytes to upload\n+      assertValues(\"bytes to upload\", testBytesToUpload.getBytes().length,\n+          abfsOutputStreamStatistics.getBytesToUpload());\n+\n+      //Test for successful bytes uploaded\n+      assertValues(\"successful bytes uploaded\",\n+          testBytesToUpload.getBytes().length,\n+          abfsOutputStreamStatistics.getBytesUploadSuccessful());\n+\n+      //Populating random value for bytesFailed\n+      int randomBytesFailed = new Random().nextInt(LOW_RANGE_FOR_RANDOM_VALUE);\n+      abfsOutputStreamStatistics.uploadFailed(randomBytesFailed);\n+      //Test for bytes failed to upload\n+      assertValues(\"number fo bytes failed to upload\", randomBytesFailed,\n+          abfsOutputStreamStatistics.getBytesUploadFailed());\n+\n+    }\n+\n+    try (AbfsOutputStream outForLargeBytes = createAbfsOutputStream(fs,\n+        uploadBytesFilePath)) {\n+\n+      int largeValue = LARGE_OPERATIONS;\n+      for (int i = 0; i < largeValue; i++) {\n+        outForLargeBytes.write(testBytesToUpload.getBytes());\n+      }\n+      outForLargeBytes.flush();\n+      AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics =\n+          outForLargeBytes.getOutputStreamStatistics();\n+\n+      //Test for bytes to upload\n+      assertValues(\"bytes to upload\",\n+          largeValue * (testBytesToUpload.getBytes().length),\n+          abfsOutputStreamStatistics.getBytesToUpload());\n+\n+      //Test for successful bytes uploaded\n+      assertValues(\"successful bytes uploaded\",\n+          largeValue * (testBytesToUpload.getBytes().length),\n+          abfsOutputStreamStatistics.getBytesUploadSuccessful());\n+\n+      //Populating random values for bytesFailed\n+      int randomBytesFailed = new Random().nextInt(HIGH_RANGE_FOR_RANDOM_VALUE);\n+      abfsOutputStreamStatistics.uploadFailed(randomBytesFailed);\n+      //Test for bytes failed to upload\n+      assertValues(\"bytes failed to upload\", randomBytesFailed,\n+          abfsOutputStreamStatistics.getBytesUploadFailed());\n+    }\n+  }\n+\n+  /**\n+   * Tests to check time spent on waiting for tasks to be complete on a\n+   * blocking queue in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamTimeSpentOnWaitTask() throws IOException {\n+    describe(\"Testing Time Spend on Waiting for Task to be complete\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path timeSpendFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n+\n+    try (AbfsOutputStream out = createAbfsOutputStream(fs, timeSpendFilePath)) {\n+\n+      AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics =\n+          out.getOutputStreamStatistics();\n+\n+      //Test for initial value of timeSpentWaitTask\n+      assertValues(\"Time spent on waiting tasks\", 0,\n+          abfsOutputStreamStatistics.getTimeSpendOnTaskWait());\n+\n+      int smallRandomStartTime =\n+          new Random().nextInt(LOW_RANGE_FOR_RANDOM_VALUE);\n+      int smallRandomEndTime =\n+          new Random().nextInt(LOW_RANGE_FOR_RANDOM_VALUE)\n+              + smallRandomStartTime;\n+      int smallDiff = smallRandomEndTime - smallRandomStartTime;\n+      abfsOutputStreamStatistics\n+          .timeSpentTaskWait(smallRandomStartTime, smallRandomEndTime);\n+      //Test for small random value of timeSpentWaitTask\n+      assertValues(\"Time spent on waiting tasks\", smallDiff,\n+          abfsOutputStreamStatistics.getTimeSpendOnTaskWait());\n+\n+      int largeRandomStartTime =\n+          new Random().nextInt(HIGH_RANGE_FOR_RANDOM_VALUE);\n+      int largeRandomEndTime = new Random().nextInt(HIGH_RANGE_FOR_RANDOM_VALUE)\n+          + largeRandomStartTime;\n+      int randomDiff = largeRandomEndTime - largeRandomStartTime;\n+      abfsOutputStreamStatistics\n+          .timeSpentTaskWait(largeRandomStartTime, largeRandomEndTime);\n+      /*\n+      Test for large random value of timeSpentWaitTask plus the time spent\n+      in previous test\n+       */\n+      assertValues(\"Time spent on waiting tasks\", smallDiff + randomDiff,\n+          abfsOutputStreamStatistics.getTimeSpendOnTaskWait());\n+    }\n+\n+  }\n+\n+  /**\n+   * Tests to check number of {@code shrinkWriteOperationQueue()}\n+   * calls.\n+   * After writing data, AbfsOutputStream doesn't upload the data until\n+   * Flushed. Hence, flush() method is called after write() to test Queue\n+   * shrink calls.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamQueueShrink() throws IOException {\n+    describe(\"Testing Queue Shrink calls in AbfsOutputStream\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path queueShrinkFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n+    String testQueueShrink = \"testQueue\";\n+\n+    try (AbfsOutputStream outForOneOp = createAbfsOutputStream(fs,\n+        queueShrinkFilePath)) {\n+\n+      //Test for shrinking Queue zero time\n+      assertValues(\"number of queueShrink() Calls\", 0,\n+          outForOneOp.getOutputStreamStatistics().getQueueShrink());\n+\n+      outForOneOp.write(testQueueShrink.getBytes());\n+      // Queue is shrunk 2 times when outStream is flushed\n+      outForOneOp.flush();\n+\n+      //Test for shrinking Queue 2 times\n+      assertValues(\"number of queueShrink() Calls\", 2,\n+          outForOneOp.getOutputStreamStatistics().getQueueShrink());\n+\n+    }\n+\n+    try (AbfsOutputStream outForLargeOps = createAbfsOutputStream(fs,\n+        queueShrinkFilePath)) {\n+\n+      int largeValue = LARGE_OPERATIONS;\n+      for (int i = 0; i < largeValue; i++) {\n+        outForLargeOps.write(testQueueShrink.getBytes());\n+        outForLargeOps.flush();\n+      }\n+\n+      //Test for 20 queue shrink calls\n+      assertValues(\"number of queueShrink() Calls\",\n+          2 * largeValue,\n+          outForLargeOps.getOutputStreamStatistics().getQueueShrink());\n+    }\n+\n+  }\n+\n+  /**\n+   * Test to check number of {@code writeCurrentBufferToService()}\n+   * calls.\n+   * After writing data, AbfsOutputStream doesn't upload data till flush() is\n+   * called. Hence, flush() calls were made after write() to simulate the\n+   * scenario.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamWriteBuffer() throws IOException {\n+    describe(\"Testing writeCurrentBufferToService() calls\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path writeBufferFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n+    String testWriteBuffer = \"Buffer\";\n+\n+    try (AbfsOutputStream outForOneOp = createAbfsOutputStream(fs,\n+        writeBufferFilePath)) {\n+\n+      //Test for zero time writing Buffer to service\n+      assertValues(\"number writeCurrentBufferToService() calls\", 0,\n+          outForOneOp.getOutputStreamStatistics()\n+              .getWriteCurrentBufferOperations());\n+\n+      outForOneOp.write(testWriteBuffer.getBytes());\n+      outForOneOp.flush();\n+\n+      //Test for one time writeCurrentBuffer() call\n+      assertValues(\"number writeCurrentBufferToService() calls\", 1,\n+          outForOneOp.getOutputStreamStatistics()\n+              .getWriteCurrentBufferOperations());\n+    }\n+\n+    try (AbfsOutputStream outForLargeOps = createAbfsOutputStream(fs,\n+        writeBufferFilePath)) {\n+\n+      int largeValue = LARGE_OPERATIONS;\n+      for (int i = 0; i < largeValue; i++) {\n+        outForLargeOps.write(testWriteBuffer.getBytes());\n+        outForLargeOps.flush();\n+      }\n+      //Test for 10 writeBufferOperations\n+      assertValues(\"number of writeCurrentBufferToService() calls\", largeValue,\n+          outForLargeOps\n+              .getOutputStreamStatistics().getWriteCurrentBufferOperations());\n+    }\n+\n+  }\n+\n+  /**\n+   * Generic create File and setting OutputStreamFlush to false.\n+   *\n+   * @param fs   AzureBlobFileSystem that is initialised in the test\n+   * @param path Path of the file to be created\n+   * @return AbfsOutputStream for writing\n+   * @throws AzureBlobFileSystemException\n+   */\n+  private AbfsOutputStream createAbfsOutputStream(AzureBlobFileSystem fs,\n+      Path path) throws AzureBlobFileSystemException {\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    abfss.getAbfsConfiguration().setDisableOutputStreamFlush(false);\n+\n+    return (AbfsOutputStream) abfss.createFile(path, fs.getFsStatistics(),\n+        true, FsPermission.getDefault(), FsPermission.getUMask(fs.getConf()));\n+  }\n+\n+  /**\n+   * Generic assert method.\n+   *\n+   * @param operation     operation being asserted\n+   * @param expectedValue value that is expected\n+   * @param actualValue   value that is actual\n+   */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2bada7fb3bd6bf93de926ffa07fbf346fc3fbf61"}, "originalPosition": 288}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg4ODQ2OTgx", "url": "https://github.com/apache/hadoop/pull/1899#pullrequestreview-388846981", "createdAt": "2020-04-07T07:22:01Z", "commit": {"oid": "2bada7fb3bd6bf93de926ffa07fbf346fc3fbf61"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwNzoyMjowMVrOGB2M0g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwNzoyMjowMVrOGB2M0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDU4OTc3OA==", "bodyText": "Move the java doc to corresponding variables.", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r404589778", "createdAt": "2020-04-07T07:22:01Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStreamStatisticsImpl.java", "diffHunk": "@@ -0,0 +1,171 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+/**\n+ * OutputStream Statistics Implementation for Abfs.\n+ */\n+public class AbfsOutputStreamStatisticsImpl\n+    implements AbfsOutputStreamStatistics {\n+\n+  /**\n+   * timeSpentOnTaskWait - Time spent on waiting for tasks to be complete on\n+   * Blocking Queue in AbfsOutputStream.\n+   *\n+   * queueShrink - Number of times Blocking Queue was shrunk after writing\n+   * data.\n+   *\n+   * WriteCurrentBufferOperations - Number of times the current buffer which\n+   * was written has been forwarded to the service.\n+   */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2bada7fb3bd6bf93de926ffa07fbf346fc3fbf61"}, "originalPosition": 36}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg4ODU5NTE1", "url": "https://github.com/apache/hadoop/pull/1899#pullrequestreview-388859515", "createdAt": "2020-04-07T07:40:15Z", "commit": {"oid": "2bada7fb3bd6bf93de926ffa07fbf346fc3fbf61"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwNzo0MDoxNVrOGB23EA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwNzo0MDoxNVrOGB23EA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDYwMDU5Mg==", "bodyText": "Should go down after if clause?", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r404600592", "createdAt": "2020-04-07T07:40:15Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -294,19 +299,23 @@ private synchronized void flushInternalAsync() throws IOException {\n   }\n \n   private synchronized void writeCurrentBufferToService() throws IOException {\n+    outputStreamStatistics.writeCurrentBuffer();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2bada7fb3bd6bf93de926ffa07fbf346fc3fbf61"}, "originalPosition": 43}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg4ODYwMTQ1", "url": "https://github.com/apache/hadoop/pull/1899#pullrequestreview-388860145", "createdAt": "2020-04-07T07:41:07Z", "commit": {"oid": "2bada7fb3bd6bf93de926ffa07fbf346fc3fbf61"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwNzo0MTowN1rOGB24_Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwNzo0MTowN1rOGB24_Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDYwMTA4NQ==", "bodyText": "I don't understand the benifits of capturing this metric?", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r404601085", "createdAt": "2020-04-07T07:41:07Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -384,6 +398,7 @@ private synchronized void flushWrittenBytesToServiceInternal(final long offset,\n    * operation FIFO queue.\n    */\n   private synchronized void shrinkWriteOperationQueue() throws IOException {\n+    outputStreamStatistics.queueShrinked();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2bada7fb3bd6bf93de926ffa07fbf346fc3fbf61"}, "originalPosition": 79}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg4ODY1MDM2", "url": "https://github.com/apache/hadoop/pull/1899#pullrequestreview-388865036", "createdAt": "2020-04-07T07:48:07Z", "commit": {"oid": "2bada7fb3bd6bf93de926ffa07fbf346fc3fbf61"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwNzo0ODowN1rOGB3IPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwNzo0ODowN1rOGB3IPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDYwNDk4OQ==", "bodyText": "Use LARGE_OPERATIONS directly. Why to create a new variable.", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r404604989", "createdAt": "2020-04-07T07:48:07Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+import java.util.Random;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStreamStatisticsImpl;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+\n+/**\n+ * Test AbfsOutputStream statistics.\n+ */\n+public class ITestAbfsOutputStream extends AbstractAbfsIntegrationTest {\n+  public ITestAbfsOutputStream() throws Exception {\n+  }\n+\n+  private static final int LARGE_OPERATIONS = 10;\n+  private static final int LOW_RANGE_FOR_RANDOM_VALUE = 49;\n+  private static final int HIGH_RANGE_FOR_RANDOM_VALUE = 9999;\n+\n+  /**\n+   * Tests to check bytes Uploading in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamUploadingBytes() throws IOException {\n+    describe(\"Testing Bytes uploaded in AbfsOutputSteam\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path uploadBytesFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n+    String testBytesToUpload = \"bytes\";\n+\n+    try (AbfsOutputStream outForSomeBytes = createAbfsOutputStream(fs,\n+        uploadBytesFilePath)\n+    ) {\n+\n+      //Test for zero bytes To upload\n+      assertValues(\"bytes to upload\", 0,\n+          outForSomeBytes.getOutputStreamStatistics().getBytesToUpload());\n+\n+      outForSomeBytes.write(testBytesToUpload.getBytes());\n+      outForSomeBytes.flush();\n+      AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics =\n+          outForSomeBytes.getOutputStreamStatistics();\n+\n+      //Test for bytes to upload\n+      assertValues(\"bytes to upload\", testBytesToUpload.getBytes().length,\n+          abfsOutputStreamStatistics.getBytesToUpload());\n+\n+      //Test for successful bytes uploaded\n+      assertValues(\"successful bytes uploaded\",\n+          testBytesToUpload.getBytes().length,\n+          abfsOutputStreamStatistics.getBytesUploadSuccessful());\n+\n+      //Populating random value for bytesFailed\n+      int randomBytesFailed = new Random().nextInt(LOW_RANGE_FOR_RANDOM_VALUE);\n+      abfsOutputStreamStatistics.uploadFailed(randomBytesFailed);\n+      //Test for bytes failed to upload\n+      assertValues(\"number fo bytes failed to upload\", randomBytesFailed,\n+          abfsOutputStreamStatistics.getBytesUploadFailed());\n+\n+    }\n+\n+    try (AbfsOutputStream outForLargeBytes = createAbfsOutputStream(fs,\n+        uploadBytesFilePath)) {\n+\n+      int largeValue = LARGE_OPERATIONS;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2bada7fb3bd6bf93de926ffa07fbf346fc3fbf61"}, "originalPosition": 89}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg4ODY1Nzkw", "url": "https://github.com/apache/hadoop/pull/1899#pullrequestreview-388865790", "createdAt": "2020-04-07T07:49:06Z", "commit": {"oid": "2bada7fb3bd6bf93de926ffa07fbf346fc3fbf61"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwNzo0OTowNlrOGB3KvA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwNzo0OTowNlrOGB3KvA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDYwNTYyOA==", "bodyText": "same use path(getMethodName())", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r404605628", "createdAt": "2020-04-07T07:49:06Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+import java.util.Random;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStreamStatisticsImpl;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+\n+/**\n+ * Test AbfsOutputStream statistics.\n+ */\n+public class ITestAbfsOutputStream extends AbstractAbfsIntegrationTest {\n+  public ITestAbfsOutputStream() throws Exception {\n+  }\n+\n+  private static final int LARGE_OPERATIONS = 10;\n+  private static final int LOW_RANGE_FOR_RANDOM_VALUE = 49;\n+  private static final int HIGH_RANGE_FOR_RANDOM_VALUE = 9999;\n+\n+  /**\n+   * Tests to check bytes Uploading in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamUploadingBytes() throws IOException {\n+    describe(\"Testing Bytes uploaded in AbfsOutputSteam\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path uploadBytesFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n+    String testBytesToUpload = \"bytes\";\n+\n+    try (AbfsOutputStream outForSomeBytes = createAbfsOutputStream(fs,\n+        uploadBytesFilePath)\n+    ) {\n+\n+      //Test for zero bytes To upload\n+      assertValues(\"bytes to upload\", 0,\n+          outForSomeBytes.getOutputStreamStatistics().getBytesToUpload());\n+\n+      outForSomeBytes.write(testBytesToUpload.getBytes());\n+      outForSomeBytes.flush();\n+      AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics =\n+          outForSomeBytes.getOutputStreamStatistics();\n+\n+      //Test for bytes to upload\n+      assertValues(\"bytes to upload\", testBytesToUpload.getBytes().length,\n+          abfsOutputStreamStatistics.getBytesToUpload());\n+\n+      //Test for successful bytes uploaded\n+      assertValues(\"successful bytes uploaded\",\n+          testBytesToUpload.getBytes().length,\n+          abfsOutputStreamStatistics.getBytesUploadSuccessful());\n+\n+      //Populating random value for bytesFailed\n+      int randomBytesFailed = new Random().nextInt(LOW_RANGE_FOR_RANDOM_VALUE);\n+      abfsOutputStreamStatistics.uploadFailed(randomBytesFailed);\n+      //Test for bytes failed to upload\n+      assertValues(\"number fo bytes failed to upload\", randomBytesFailed,\n+          abfsOutputStreamStatistics.getBytesUploadFailed());\n+\n+    }\n+\n+    try (AbfsOutputStream outForLargeBytes = createAbfsOutputStream(fs,\n+        uploadBytesFilePath)) {\n+\n+      int largeValue = LARGE_OPERATIONS;\n+      for (int i = 0; i < largeValue; i++) {\n+        outForLargeBytes.write(testBytesToUpload.getBytes());\n+      }\n+      outForLargeBytes.flush();\n+      AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics =\n+          outForLargeBytes.getOutputStreamStatistics();\n+\n+      //Test for bytes to upload\n+      assertValues(\"bytes to upload\",\n+          largeValue * (testBytesToUpload.getBytes().length),\n+          abfsOutputStreamStatistics.getBytesToUpload());\n+\n+      //Test for successful bytes uploaded\n+      assertValues(\"successful bytes uploaded\",\n+          largeValue * (testBytesToUpload.getBytes().length),\n+          abfsOutputStreamStatistics.getBytesUploadSuccessful());\n+\n+      //Populating random values for bytesFailed\n+      int randomBytesFailed = new Random().nextInt(HIGH_RANGE_FOR_RANDOM_VALUE);\n+      abfsOutputStreamStatistics.uploadFailed(randomBytesFailed);\n+      //Test for bytes failed to upload\n+      assertValues(\"bytes failed to upload\", randomBytesFailed,\n+          abfsOutputStreamStatistics.getBytesUploadFailed());\n+    }\n+  }\n+\n+  /**\n+   * Tests to check time spent on waiting for tasks to be complete on a\n+   * blocking queue in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamTimeSpentOnWaitTask() throws IOException {\n+    describe(\"Testing Time Spend on Waiting for Task to be complete\");\n+    final AzureBlobFileSystem fs = getFileSystem();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2bada7fb3bd6bf93de926ffa07fbf346fc3fbf61"}, "originalPosition": 125}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg4ODY5MTM2", "url": "https://github.com/apache/hadoop/pull/1899#pullrequestreview-388869136", "createdAt": "2020-04-07T07:53:34Z", "commit": {"oid": "2bada7fb3bd6bf93de926ffa07fbf346fc3fbf61"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwNzo1MzozNVrOGB3VKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwNzo1MzozNVrOGB3VKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDYwODI5OQ==", "bodyText": "Looks like this an UT. Should move under UT folder. Create a new class TestAbfsOutputStreamStatictics and write all UT's there. You don't even have to create a file there. What do you say @steveloughran?", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r404608299", "createdAt": "2020-04-07T07:53:35Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+import java.util.Random;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStreamStatisticsImpl;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+\n+/**\n+ * Test AbfsOutputStream statistics.\n+ */\n+public class ITestAbfsOutputStream extends AbstractAbfsIntegrationTest {\n+  public ITestAbfsOutputStream() throws Exception {\n+  }\n+\n+  private static final int LARGE_OPERATIONS = 10;\n+  private static final int LOW_RANGE_FOR_RANDOM_VALUE = 49;\n+  private static final int HIGH_RANGE_FOR_RANDOM_VALUE = 9999;\n+\n+  /**\n+   * Tests to check bytes Uploading in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamUploadingBytes() throws IOException {\n+    describe(\"Testing Bytes uploaded in AbfsOutputSteam\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path uploadBytesFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n+    String testBytesToUpload = \"bytes\";\n+\n+    try (AbfsOutputStream outForSomeBytes = createAbfsOutputStream(fs,\n+        uploadBytesFilePath)\n+    ) {\n+\n+      //Test for zero bytes To upload\n+      assertValues(\"bytes to upload\", 0,\n+          outForSomeBytes.getOutputStreamStatistics().getBytesToUpload());\n+\n+      outForSomeBytes.write(testBytesToUpload.getBytes());\n+      outForSomeBytes.flush();\n+      AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics =\n+          outForSomeBytes.getOutputStreamStatistics();\n+\n+      //Test for bytes to upload\n+      assertValues(\"bytes to upload\", testBytesToUpload.getBytes().length,\n+          abfsOutputStreamStatistics.getBytesToUpload());\n+\n+      //Test for successful bytes uploaded\n+      assertValues(\"successful bytes uploaded\",\n+          testBytesToUpload.getBytes().length,\n+          abfsOutputStreamStatistics.getBytesUploadSuccessful());\n+\n+      //Populating random value for bytesFailed\n+      int randomBytesFailed = new Random().nextInt(LOW_RANGE_FOR_RANDOM_VALUE);\n+      abfsOutputStreamStatistics.uploadFailed(randomBytesFailed);\n+      //Test for bytes failed to upload\n+      assertValues(\"number fo bytes failed to upload\", randomBytesFailed,\n+          abfsOutputStreamStatistics.getBytesUploadFailed());\n+\n+    }\n+\n+    try (AbfsOutputStream outForLargeBytes = createAbfsOutputStream(fs,\n+        uploadBytesFilePath)) {\n+\n+      int largeValue = LARGE_OPERATIONS;\n+      for (int i = 0; i < largeValue; i++) {\n+        outForLargeBytes.write(testBytesToUpload.getBytes());\n+      }\n+      outForLargeBytes.flush();\n+      AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics =\n+          outForLargeBytes.getOutputStreamStatistics();\n+\n+      //Test for bytes to upload\n+      assertValues(\"bytes to upload\",\n+          largeValue * (testBytesToUpload.getBytes().length),\n+          abfsOutputStreamStatistics.getBytesToUpload());\n+\n+      //Test for successful bytes uploaded\n+      assertValues(\"successful bytes uploaded\",\n+          largeValue * (testBytesToUpload.getBytes().length),\n+          abfsOutputStreamStatistics.getBytesUploadSuccessful());\n+\n+      //Populating random values for bytesFailed\n+      int randomBytesFailed = new Random().nextInt(HIGH_RANGE_FOR_RANDOM_VALUE);\n+      abfsOutputStreamStatistics.uploadFailed(randomBytesFailed);\n+      //Test for bytes failed to upload\n+      assertValues(\"bytes failed to upload\", randomBytesFailed,\n+          abfsOutputStreamStatistics.getBytesUploadFailed());\n+    }\n+  }\n+\n+  /**\n+   * Tests to check time spent on waiting for tasks to be complete on a\n+   * blocking queue in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamTimeSpentOnWaitTask() throws IOException {\n+    describe(\"Testing Time Spend on Waiting for Task to be complete\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path timeSpendFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n+\n+    try (AbfsOutputStream out = createAbfsOutputStream(fs, timeSpendFilePath)) {\n+\n+      AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics =\n+          out.getOutputStreamStatistics();\n+\n+      //Test for initial value of timeSpentWaitTask\n+      assertValues(\"Time spent on waiting tasks\", 0,\n+          abfsOutputStreamStatistics.getTimeSpendOnTaskWait());\n+\n+      int smallRandomStartTime =\n+          new Random().nextInt(LOW_RANGE_FOR_RANDOM_VALUE);\n+      int smallRandomEndTime =\n+          new Random().nextInt(LOW_RANGE_FOR_RANDOM_VALUE)\n+              + smallRandomStartTime;\n+      int smallDiff = smallRandomEndTime - smallRandomStartTime;\n+      abfsOutputStreamStatistics\n+          .timeSpentTaskWait(smallRandomStartTime, smallRandomEndTime);\n+      //Test for small random value of timeSpentWaitTask", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2bada7fb3bd6bf93de926ffa07fbf346fc3fbf61"}, "originalPosition": 145}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg4ODgyMDk5", "url": "https://github.com/apache/hadoop/pull/1899#pullrequestreview-388882099", "createdAt": "2020-04-07T08:10:56Z", "commit": {"oid": "2bada7fb3bd6bf93de926ffa07fbf346fc3fbf61"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwODoxMDo1NlrOGB39mg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwODoxMDo1NlrOGB39mg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDYxODY1MA==", "bodyText": "I can see you are disabling flush in createAbfsOutputStream(). Then why are you expecting double the actual values?", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r404618650", "createdAt": "2020-04-07T08:10:56Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+import java.util.Random;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStreamStatisticsImpl;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+\n+/**\n+ * Test AbfsOutputStream statistics.\n+ */\n+public class ITestAbfsOutputStream extends AbstractAbfsIntegrationTest {\n+  public ITestAbfsOutputStream() throws Exception {\n+  }\n+\n+  private static final int LARGE_OPERATIONS = 10;\n+  private static final int LOW_RANGE_FOR_RANDOM_VALUE = 49;\n+  private static final int HIGH_RANGE_FOR_RANDOM_VALUE = 9999;\n+\n+  /**\n+   * Tests to check bytes Uploading in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamUploadingBytes() throws IOException {\n+    describe(\"Testing Bytes uploaded in AbfsOutputSteam\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path uploadBytesFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n+    String testBytesToUpload = \"bytes\";\n+\n+    try (AbfsOutputStream outForSomeBytes = createAbfsOutputStream(fs,\n+        uploadBytesFilePath)\n+    ) {\n+\n+      //Test for zero bytes To upload\n+      assertValues(\"bytes to upload\", 0,\n+          outForSomeBytes.getOutputStreamStatistics().getBytesToUpload());\n+\n+      outForSomeBytes.write(testBytesToUpload.getBytes());\n+      outForSomeBytes.flush();\n+      AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics =\n+          outForSomeBytes.getOutputStreamStatistics();\n+\n+      //Test for bytes to upload\n+      assertValues(\"bytes to upload\", testBytesToUpload.getBytes().length,\n+          abfsOutputStreamStatistics.getBytesToUpload());\n+\n+      //Test for successful bytes uploaded\n+      assertValues(\"successful bytes uploaded\",\n+          testBytesToUpload.getBytes().length,\n+          abfsOutputStreamStatistics.getBytesUploadSuccessful());\n+\n+      //Populating random value for bytesFailed\n+      int randomBytesFailed = new Random().nextInt(LOW_RANGE_FOR_RANDOM_VALUE);\n+      abfsOutputStreamStatistics.uploadFailed(randomBytesFailed);\n+      //Test for bytes failed to upload\n+      assertValues(\"number fo bytes failed to upload\", randomBytesFailed,\n+          abfsOutputStreamStatistics.getBytesUploadFailed());\n+\n+    }\n+\n+    try (AbfsOutputStream outForLargeBytes = createAbfsOutputStream(fs,\n+        uploadBytesFilePath)) {\n+\n+      int largeValue = LARGE_OPERATIONS;\n+      for (int i = 0; i < largeValue; i++) {\n+        outForLargeBytes.write(testBytesToUpload.getBytes());\n+      }\n+      outForLargeBytes.flush();\n+      AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics =\n+          outForLargeBytes.getOutputStreamStatistics();\n+\n+      //Test for bytes to upload\n+      assertValues(\"bytes to upload\",\n+          largeValue * (testBytesToUpload.getBytes().length),\n+          abfsOutputStreamStatistics.getBytesToUpload());\n+\n+      //Test for successful bytes uploaded\n+      assertValues(\"successful bytes uploaded\",\n+          largeValue * (testBytesToUpload.getBytes().length),\n+          abfsOutputStreamStatistics.getBytesUploadSuccessful());\n+\n+      //Populating random values for bytesFailed\n+      int randomBytesFailed = new Random().nextInt(HIGH_RANGE_FOR_RANDOM_VALUE);\n+      abfsOutputStreamStatistics.uploadFailed(randomBytesFailed);\n+      //Test for bytes failed to upload\n+      assertValues(\"bytes failed to upload\", randomBytesFailed,\n+          abfsOutputStreamStatistics.getBytesUploadFailed());\n+    }\n+  }\n+\n+  /**\n+   * Tests to check time spent on waiting for tasks to be complete on a\n+   * blocking queue in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamTimeSpentOnWaitTask() throws IOException {\n+    describe(\"Testing Time Spend on Waiting for Task to be complete\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path timeSpendFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n+\n+    try (AbfsOutputStream out = createAbfsOutputStream(fs, timeSpendFilePath)) {\n+\n+      AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics =\n+          out.getOutputStreamStatistics();\n+\n+      //Test for initial value of timeSpentWaitTask\n+      assertValues(\"Time spent on waiting tasks\", 0,\n+          abfsOutputStreamStatistics.getTimeSpendOnTaskWait());\n+\n+      int smallRandomStartTime =\n+          new Random().nextInt(LOW_RANGE_FOR_RANDOM_VALUE);\n+      int smallRandomEndTime =\n+          new Random().nextInt(LOW_RANGE_FOR_RANDOM_VALUE)\n+              + smallRandomStartTime;\n+      int smallDiff = smallRandomEndTime - smallRandomStartTime;\n+      abfsOutputStreamStatistics\n+          .timeSpentTaskWait(smallRandomStartTime, smallRandomEndTime);\n+      //Test for small random value of timeSpentWaitTask\n+      assertValues(\"Time spent on waiting tasks\", smallDiff,\n+          abfsOutputStreamStatistics.getTimeSpendOnTaskWait());\n+\n+      int largeRandomStartTime =\n+          new Random().nextInt(HIGH_RANGE_FOR_RANDOM_VALUE);\n+      int largeRandomEndTime = new Random().nextInt(HIGH_RANGE_FOR_RANDOM_VALUE)\n+          + largeRandomStartTime;\n+      int randomDiff = largeRandomEndTime - largeRandomStartTime;\n+      abfsOutputStreamStatistics\n+          .timeSpentTaskWait(largeRandomStartTime, largeRandomEndTime);\n+      /*\n+      Test for large random value of timeSpentWaitTask plus the time spent\n+      in previous test\n+       */\n+      assertValues(\"Time spent on waiting tasks\", smallDiff + randomDiff,\n+          abfsOutputStreamStatistics.getTimeSpendOnTaskWait());\n+    }\n+\n+  }\n+\n+  /**\n+   * Tests to check number of {@code shrinkWriteOperationQueue()}\n+   * calls.\n+   * After writing data, AbfsOutputStream doesn't upload the data until\n+   * Flushed. Hence, flush() method is called after write() to test Queue\n+   * shrink calls.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamQueueShrink() throws IOException {\n+    describe(\"Testing Queue Shrink calls in AbfsOutputStream\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path queueShrinkFilePath = new Path(\"AbfsOutputStreamStatsPath\");\n+    String testQueueShrink = \"testQueue\";\n+\n+    try (AbfsOutputStream outForOneOp = createAbfsOutputStream(fs,\n+        queueShrinkFilePath)) {\n+\n+      //Test for shrinking Queue zero time\n+      assertValues(\"number of queueShrink() Calls\", 0,\n+          outForOneOp.getOutputStreamStatistics().getQueueShrink());\n+\n+      outForOneOp.write(testQueueShrink.getBytes());\n+      // Queue is shrunk 2 times when outStream is flushed\n+      outForOneOp.flush();\n+\n+      //Test for shrinking Queue 2 times\n+      assertValues(\"number of queueShrink() Calls\", 2,\n+          outForOneOp.getOutputStreamStatistics().getQueueShrink());\n+\n+    }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2bada7fb3bd6bf93de926ffa07fbf346fc3fbf61"}, "originalPosition": 198}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg4ODg0MDA1", "url": "https://github.com/apache/hadoop/pull/1899#pullrequestreview-388884005", "createdAt": "2020-04-07T08:13:28Z", "commit": {"oid": "2bada7fb3bd6bf93de926ffa07fbf346fc3fbf61"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5MzgwODM3", "url": "https://github.com/apache/hadoop/pull/1899#pullrequestreview-389380837", "createdAt": "2020-04-07T18:27:16Z", "commit": {"oid": "2bada7fb3bd6bf93de926ffa07fbf346fc3fbf61"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxODoyNzoxNlrOGCQnkg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxODoyNzoxNlrOGCQnkg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAyMjYxMA==", "bodyText": "let's guard this with a LOG.isDebugEnabled(), because that toString() operation is doing enough work. Or, use this as the argument and have SLF4J Call this.toString() only if it is printing the log entry", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r405022610", "createdAt": "2020-04-07T18:27:16Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -279,6 +283,7 @@ public synchronized void close() throws IOException {\n         threadExecutor.shutdownNow();\n       }\n     }\n+    LOG.debug(\"Closing AbfsOutputStream \", toString());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2bada7fb3bd6bf93de926ffa07fbf346fc3fbf61"}, "originalPosition": 35}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "2bada7fb3bd6bf93de926ffa07fbf346fc3fbf61", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/2bada7fb3bd6bf93de926ffa07fbf346fc3fbf61", "committedDate": "2020-04-06T12:03:06Z", "message": "HADOOP-16914. Java Docs"}, "afterCommit": {"oid": "7e58998ff063bc8a11b7fa3cea711f95a1e71bc3", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/7e58998ff063bc8a11b7fa3cea711f95a1e71bc3", "committedDate": "2020-04-07T20:47:46Z", "message": "HADOOP-16914. Fixing comments and tests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NDgyNTcw", "url": "https://github.com/apache/hadoop/pull/1899#pullrequestreview-389482570", "createdAt": "2020-04-07T20:54:21Z", "commit": {"oid": "7e58998ff063bc8a11b7fa3cea711f95a1e71bc3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1NDoyMVrOGCVvGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1NDoyMVrOGCVvGA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwNjQ1Ng==", "bodyText": "*disableOutputStreamFlush", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r405106456", "createdAt": "2020-04-07T20:54:21Z", "author": {"login": "mehakmeet"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsOutputStreamStatistics.java", "diffHunk": "@@ -0,0 +1,296 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+import java.util.Random;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStreamStatisticsImpl;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+\n+/**\n+ * Test AbfsOutputStream statistics.\n+ */\n+public class ITestAbfsOutputStreamStatistics\n+    extends AbstractAbfsIntegrationTest {\n+  private static final int LARGE_OPERATIONS = 10;\n+  private static final int LOW_RANGE_FOR_RANDOM_VALUE = 49;\n+  private static final int HIGH_RANGE_FOR_RANDOM_VALUE = 9999;\n+\n+  public ITestAbfsOutputStreamStatistics() throws Exception {\n+  }\n+\n+  /**\n+   * Tests to check bytes Uploading in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamUploadingBytes() throws IOException {\n+    describe(\"Testing Bytes uploaded in AbfsOutputSteam\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path uploadBytesFilePath = path(getMethodName());\n+    String testBytesToUpload = \"bytes\";\n+\n+    try (AbfsOutputStream outForSomeBytes = createAbfsOutputStream(fs,\n+        uploadBytesFilePath)\n+    ) {\n+\n+      AbfsOutputStreamStatisticsImpl abfsOutputStreamStatisticsForUploadBytes =\n+          outForSomeBytes.getOutputStreamStatistics();\n+\n+      //Test for zero bytes To upload.\n+      assertValues(\"bytes to upload\", 0,\n+          abfsOutputStreamStatisticsForUploadBytes.getBytesToUpload());\n+\n+      outForSomeBytes.write(testBytesToUpload.getBytes());\n+      outForSomeBytes.flush();\n+      abfsOutputStreamStatisticsForUploadBytes =\n+          outForSomeBytes.getOutputStreamStatistics();\n+\n+      //Test for bytes to upload.\n+      assertValues(\"bytes to upload\", testBytesToUpload.getBytes().length,\n+          abfsOutputStreamStatisticsForUploadBytes.getBytesToUpload());\n+\n+      //Test for successful bytes uploaded.\n+      assertValues(\"successful bytes uploaded\",\n+          testBytesToUpload.getBytes().length,\n+          abfsOutputStreamStatisticsForUploadBytes.getBytesUploadSuccessful());\n+\n+      //Populating random value for bytesFailed.\n+      int randomBytesFailed = new Random().nextInt(LOW_RANGE_FOR_RANDOM_VALUE);\n+      abfsOutputStreamStatisticsForUploadBytes.uploadFailed(randomBytesFailed);\n+      //Test for bytes failed to upload.\n+      assertValues(\"number fo bytes failed to upload\", randomBytesFailed,\n+          abfsOutputStreamStatisticsForUploadBytes.getBytesUploadFailed());\n+\n+    }\n+\n+    try (AbfsOutputStream outForLargeBytes = createAbfsOutputStream(fs,\n+        uploadBytesFilePath)) {\n+\n+      for (int i = 0; i < LARGE_OPERATIONS; i++) {\n+        outForLargeBytes.write(testBytesToUpload.getBytes());\n+      }\n+      outForLargeBytes.flush();\n+      AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics =\n+          outForLargeBytes.getOutputStreamStatistics();\n+\n+      //Test for bytes to upload.\n+      assertValues(\"bytes to upload\",\n+          LARGE_OPERATIONS * (testBytesToUpload.getBytes().length),\n+          abfsOutputStreamStatistics.getBytesToUpload());\n+\n+      //Test for successful bytes uploaded.\n+      assertValues(\"successful bytes uploaded\",\n+          LARGE_OPERATIONS * (testBytesToUpload.getBytes().length),\n+          abfsOutputStreamStatistics.getBytesUploadSuccessful());\n+\n+      //Populating random values for bytesFailed.\n+      int randomBytesFailed = new Random().nextInt(HIGH_RANGE_FOR_RANDOM_VALUE);\n+      abfsOutputStreamStatistics.uploadFailed(randomBytesFailed);\n+      //Test for bytes failed to upload.\n+      assertValues(\"bytes failed to upload\", randomBytesFailed,\n+          abfsOutputStreamStatistics.getBytesUploadFailed());\n+    }\n+  }\n+\n+  /**\n+   * Tests to check time spent on waiting for tasks to be complete on a\n+   * blocking queue in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamTimeSpentOnWaitTask() throws IOException {\n+    describe(\"Testing Time Spend on Waiting for Task to be complete\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path timeSpendFilePath = path(getMethodName());\n+\n+    try (AbfsOutputStream out = createAbfsOutputStream(fs, timeSpendFilePath)) {\n+\n+      AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics =\n+          out.getOutputStreamStatistics();\n+\n+      //Test for initial value of timeSpentWaitTask.\n+      assertValues(\"Time spent on waiting tasks\", 0,\n+          abfsOutputStreamStatistics.getTimeSpendOnTaskWait());\n+\n+      int smallRandomStartTime =\n+          new Random().nextInt(LOW_RANGE_FOR_RANDOM_VALUE);\n+      int smallRandomEndTime =\n+          new Random().nextInt(LOW_RANGE_FOR_RANDOM_VALUE)\n+              + smallRandomStartTime;\n+      int smallDiff = smallRandomEndTime - smallRandomStartTime;\n+      abfsOutputStreamStatistics\n+          .timeSpentTaskWait(smallRandomStartTime, smallRandomEndTime);\n+      //Test for small random value of timeSpentWaitTask.\n+      assertValues(\"Time spend on waiting for tasks to complete\", smallDiff,\n+          abfsOutputStreamStatistics.getTimeSpendOnTaskWait());\n+\n+      int largeRandomStartTime =\n+          new Random().nextInt(HIGH_RANGE_FOR_RANDOM_VALUE);\n+      int largeRandomEndTime = new Random().nextInt(HIGH_RANGE_FOR_RANDOM_VALUE)\n+          + largeRandomStartTime;\n+      int randomDiff = largeRandomEndTime - largeRandomStartTime;\n+      abfsOutputStreamStatistics\n+          .timeSpentTaskWait(largeRandomStartTime, largeRandomEndTime);\n+      /*\n+      Test for large random value of timeSpentWaitTask plus the time spent\n+      in previous test.\n+       */\n+      assertValues(\"Time spend on waiting for tasks to complete\",\n+          smallDiff + randomDiff,\n+          abfsOutputStreamStatistics.getTimeSpendOnTaskWait());\n+    }\n+\n+  }\n+\n+  /**\n+   * Tests to check number of {@code\n+   * AbfsOutputStream#shrinkWriteOperationQueue()} calls.\n+   * After writing data, AbfsOutputStream doesn't upload the data until\n+   * Flushed. Hence, flush() method is called after write() to test Queue\n+   * shrink calls.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamQueueShrink() throws IOException {\n+    describe(\"Testing Queue Shrink calls in AbfsOutputStream\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path queueShrinkFilePath = path(getMethodName());\n+    String testQueueShrink = \"testQueue\";\n+\n+    try (AbfsOutputStream outForOneOp = createAbfsOutputStream(fs,\n+        queueShrinkFilePath)) {\n+\n+      AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics =\n+          outForOneOp.getOutputStreamStatistics();\n+\n+      //Test for shrinking Queue zero time.\n+      assertValues(\"Queue shrunk operations\", 0,\n+          abfsOutputStreamStatistics.getQueueShrunkOps());\n+\n+      outForOneOp.write(testQueueShrink.getBytes());\n+      // Queue is shrunk 2 times when outputStream is flushed.\n+      outForOneOp.flush();\n+\n+      abfsOutputStreamStatistics = outForOneOp.getOutputStreamStatistics();\n+\n+      //Test for shrinking Queue 2 times.\n+      assertValues(\"Queue shrunk operations\", 2,\n+          abfsOutputStreamStatistics.getQueueShrunkOps());\n+\n+    }\n+\n+    try (AbfsOutputStream outForLargeOps = createAbfsOutputStream(fs,\n+        queueShrinkFilePath)) {\n+      for (int i = 0; i < LARGE_OPERATIONS; i++) {\n+        outForLargeOps.write(testQueueShrink.getBytes());\n+        outForLargeOps.flush();\n+      }\n+\n+      AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics =\n+          outForLargeOps.getOutputStreamStatistics();\n+\n+      //Test for 20 queue shrink operations.\n+      assertValues(\"Queue shrunk operations\",\n+          2 * LARGE_OPERATIONS,\n+          abfsOutputStreamStatistics.getQueueShrunkOps());\n+    }\n+\n+  }\n+\n+  /**\n+   * Test to check number of {@code\n+   * AbfsOutputStream#writeCurrentBufferToService()} calls.\n+   * After writing data, AbfsOutputStream doesn't upload data till flush() is\n+   * called. Hence, flush() calls were made after write() to simulate the\n+   * scenario.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamWriteBuffer() throws IOException {\n+    describe(\"Testing writeCurrentBufferToService() calls\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path writeBufferFilePath = path(getMethodName());\n+    String testWriteBuffer = \"Buffer\";\n+\n+    try (AbfsOutputStream outForOneOp = createAbfsOutputStream(fs,\n+        writeBufferFilePath)) {\n+\n+      AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics =\n+          outForOneOp.getOutputStreamStatistics();\n+\n+      //Test for zero time writing Buffer to service.\n+      assertValues(\"number writeCurrentBufferToService() calls\", 0,\n+          abfsOutputStreamStatistics.getWriteCurrentBufferOperations());\n+\n+      outForOneOp.write(testWriteBuffer.getBytes());\n+      outForOneOp.flush();\n+\n+      abfsOutputStreamStatistics = outForOneOp.getOutputStreamStatistics();\n+\n+      //Test for one time writeCurrentBuffer() call.\n+      assertValues(\"number writeCurrentBufferToService() calls\", 1,\n+          abfsOutputStreamStatistics.getWriteCurrentBufferOperations());\n+    }\n+\n+    try (AbfsOutputStream outForLargeOps = createAbfsOutputStream(fs,\n+        writeBufferFilePath)) {\n+\n+      for (int i = 0; i < LARGE_OPERATIONS; i++) {\n+        outForLargeOps.write(testWriteBuffer.getBytes());\n+        outForLargeOps.flush();\n+      }\n+      AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics =\n+          outForLargeOps.getOutputStreamStatistics();\n+      //Test for 10 writeBufferOperations.\n+      assertValues(\"number of writeCurrentBufferToService() calls\",\n+          LARGE_OPERATIONS,\n+          abfsOutputStreamStatistics.getWriteCurrentBufferOperations());\n+    }\n+\n+  }\n+\n+  /**\n+   * Generic create File and setting OutputStreamFlush to false.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7e58998ff063bc8a11b7fa3cea711f95a1e71bc3"}, "originalPosition": 280}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7e58998ff063bc8a11b7fa3cea711f95a1e71bc3", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/7e58998ff063bc8a11b7fa3cea711f95a1e71bc3", "committedDate": "2020-04-07T20:47:46Z", "message": "HADOOP-16914. Fixing comments and tests"}, "afterCommit": {"oid": "8d86c63f4053d4843323ff58341f2f2a78676625", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/8d86c63f4053d4843323ff58341f2f2a78676625", "committedDate": "2020-04-08T17:39:23Z", "message": "HADOOP-16914. Fixing comments and tests"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "8d86c63f4053d4843323ff58341f2f2a78676625", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/8d86c63f4053d4843323ff58341f2f2a78676625", "committedDate": "2020-04-08T17:39:23Z", "message": "HADOOP-16914. Fixing comments and tests"}, "afterCommit": {"oid": "09d0deab572c8ddf34bba64b25437949380356bc", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/09d0deab572c8ddf34bba64b25437949380356bc", "committedDate": "2020-04-09T06:43:22Z", "message": "HADOOP-16914 AbfsOutputStream Counter\n\nChange-Id: Ie976f23c6c3e2f5cf9167794357cfc669a232c80"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "09d0deab572c8ddf34bba64b25437949380356bc", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/09d0deab572c8ddf34bba64b25437949380356bc", "committedDate": "2020-04-09T06:43:22Z", "message": "HADOOP-16914 AbfsOutputStream Counter\n\nChange-Id: Ie976f23c6c3e2f5cf9167794357cfc669a232c80"}, "afterCommit": {"oid": "a667ab0820443fde451225be1f628f7f451005da", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/a667ab0820443fde451225be1f628f7f451005da", "committedDate": "2020-04-14T12:27:26Z", "message": "HADOOP-16914 AbfsOutputStream Counter\n\nChange-Id: Ie976f23c6c3e2f5cf9167794357cfc669a232c80"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkyOTUwMDY5", "url": "https://github.com/apache/hadoop/pull/1899#pullrequestreview-392950069", "createdAt": "2020-04-14T13:54:40Z", "commit": {"oid": "a667ab0820443fde451225be1f628f7f451005da"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxMzo1NDo0MVrOGFPyww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxMzo1NDo0MVrOGFPyww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE1NDgxOQ==", "bodyText": "typo.. Remove first word bytes.", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r408154819", "createdAt": "2020-04-14T13:54:41Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStreamStatistics.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.classification.InterfaceStability;\n+\n+/**\n+ * Interface for {@link AbfsOutputStream} statistics.\n+ */\n+@InterfaceStability.Unstable\n+public interface AbfsOutputStreamStatistics {\n+\n+  /**\n+   * Number of bytes to be uploaded.\n+   *\n+   * @param bytes number of bytes to upload.\n+   */\n+  void bytesToUpload(long bytes);\n+\n+  /**\n+   * Records a successful upload and the number of bytes uploaded.\n+   *\n+   * @param bytes number of bytes that were successfully uploaded.\n+   */\n+  void uploadSuccessful(long bytes);\n+\n+  /**\n+   * Records that upload is failed and the number of bytes.\n+   *\n+   * @param bytes number of bytes that failed to upload.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a667ab0820443fde451225be1f628f7f451005da"}, "originalPosition": 46}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkyOTUwMTg4", "url": "https://github.com/apache/hadoop/pull/1899#pullrequestreview-392950188", "createdAt": "2020-04-14T13:54:48Z", "commit": {"oid": "a667ab0820443fde451225be1f628f7f451005da"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxMzo1NDo0OFrOGFPzJw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxMzo1NDo0OFrOGFPzJw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE1NDkxOQ==", "bodyText": "typo.. Remove first word bytes.", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r408154919", "createdAt": "2020-04-14T13:54:48Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStreamStatistics.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.classification.InterfaceStability;\n+\n+/**\n+ * Interface for {@link AbfsOutputStream} statistics.\n+ */\n+@InterfaceStability.Unstable\n+public interface AbfsOutputStreamStatistics {\n+\n+  /**\n+   * Number of bytes to be uploaded.\n+   *\n+   * @param bytes number of bytes to upload.\n+   */\n+  void bytesToUpload(long bytes);\n+\n+  /**\n+   * Records a successful upload and the number of bytes uploaded.\n+   *\n+   * @param bytes number of bytes that were successfully uploaded.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a667ab0820443fde451225be1f628f7f451005da"}, "originalPosition": 39}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkyOTc5ODY3", "url": "https://github.com/apache/hadoop/pull/1899#pullrequestreview-392979867", "createdAt": "2020-04-14T14:25:25Z", "commit": {"oid": "a667ab0820443fde451225be1f628f7f451005da"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxNDoyNToyNVrOGFRO5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxNDoyNToyNVrOGFRO5A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE3ODQwNA==", "bodyText": "A better test would to be call multiple times and see if the summation is working fine rather that resetting the stats?", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r408178404", "createdAt": "2020-04-14T14:25:25Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/TestAbfsOutputStreamStatistics.java", "diffHunk": "@@ -0,0 +1,120 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+import java.util.Random;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStreamStatisticsImpl;\n+\n+/**\n+ * Unit Tests for AbfsOutputStream Statistics.\n+ */\n+public class TestAbfsOutputStreamStatistics\n+    extends AbstractAbfsIntegrationTest {\n+\n+  private static final int LOW_RANGE_FOR_RANDOM_VALUE = 49;\n+  private static final int HIGH_RANGE_FOR_RANDOM_VALUE = 9999;\n+\n+  public TestAbfsOutputStreamStatistics() throws Exception {\n+  }\n+\n+  /**\n+   * Tests to check bytes failed to Upload in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamBytesFailed() {\n+    describe(\"Testing Bytes Failed during uploading in AbfsOutputSteam\");\n+\n+    AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics =\n+        new AbfsOutputStreamStatisticsImpl();\n+\n+    //Test for zero bytes uploaded.\n+    assertValues(\"number fo bytes failed to upload\", 0,\n+        abfsOutputStreamStatistics.getBytesUploadFailed());\n+\n+    //Populating small random value for bytesFailed.\n+    int randomBytesFailed = new Random().nextInt(LOW_RANGE_FOR_RANDOM_VALUE);\n+    abfsOutputStreamStatistics.uploadFailed(randomBytesFailed);\n+    //Test for bytes failed to upload.\n+    assertValues(\"number fo bytes failed to upload\", randomBytesFailed,\n+        abfsOutputStreamStatistics.getBytesUploadFailed());\n+\n+    //Initializing again to reset the statistics.\n+    abfsOutputStreamStatistics = new AbfsOutputStreamStatisticsImpl();\n+\n+    //Populating large random values for bytesFailed.\n+    randomBytesFailed = new Random().nextInt(HIGH_RANGE_FOR_RANDOM_VALUE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a667ab0820443fde451225be1f628f7f451005da"}, "originalPosition": 68}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkyOTgwNTEz", "url": "https://github.com/apache/hadoop/pull/1899#pullrequestreview-392980513", "createdAt": "2020-04-14T14:26:02Z", "commit": {"oid": "a667ab0820443fde451225be1f628f7f451005da"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxNDoyNjowM1rOGFRQ3g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxNDoyNjowM1rOGFRQ3g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE3ODkxMA==", "bodyText": "Indentation.", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r408178910", "createdAt": "2020-04-14T14:26:03Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/TestAbfsOutputStreamStatistics.java", "diffHunk": "@@ -0,0 +1,120 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+import java.util.Random;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStreamStatisticsImpl;\n+\n+/**\n+ * Unit Tests for AbfsOutputStream Statistics.\n+ */\n+public class TestAbfsOutputStreamStatistics\n+    extends AbstractAbfsIntegrationTest {\n+\n+  private static final int LOW_RANGE_FOR_RANDOM_VALUE = 49;\n+  private static final int HIGH_RANGE_FOR_RANDOM_VALUE = 9999;\n+\n+  public TestAbfsOutputStreamStatistics() throws Exception {\n+  }\n+\n+  /**\n+   * Tests to check bytes failed to Upload in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamBytesFailed() {\n+    describe(\"Testing Bytes Failed during uploading in AbfsOutputSteam\");\n+\n+    AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics =\n+        new AbfsOutputStreamStatisticsImpl();\n+\n+    //Test for zero bytes uploaded.\n+    assertValues(\"number fo bytes failed to upload\", 0,\n+        abfsOutputStreamStatistics.getBytesUploadFailed());\n+\n+    //Populating small random value for bytesFailed.\n+    int randomBytesFailed = new Random().nextInt(LOW_RANGE_FOR_RANDOM_VALUE);\n+    abfsOutputStreamStatistics.uploadFailed(randomBytesFailed);\n+    //Test for bytes failed to upload.\n+    assertValues(\"number fo bytes failed to upload\", randomBytesFailed,\n+        abfsOutputStreamStatistics.getBytesUploadFailed());\n+\n+    //Initializing again to reset the statistics.\n+    abfsOutputStreamStatistics = new AbfsOutputStreamStatisticsImpl();\n+\n+    //Populating large random values for bytesFailed.\n+    randomBytesFailed = new Random().nextInt(HIGH_RANGE_FOR_RANDOM_VALUE);\n+    abfsOutputStreamStatistics.uploadFailed(randomBytesFailed);\n+    //Test for bytes failed to upload.\n+    assertValues(\"number fo bytes failed to upload\", randomBytesFailed,\n+        abfsOutputStreamStatistics.getBytesUploadFailed());\n+  }\n+\n+  /**\n+   * Tests to check time spent on waiting for tasks to be complete on a\n+   * blocking queue in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamTimeSpentOnWaitTask() {\n+    describe(\"Testing Time Spend on Waiting for Task to be complete\");\n+\n+    AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics =\n+        new AbfsOutputStreamStatisticsImpl();\n+\n+    //Test for initial value of timeSpentWaitTask.\n+    assertValues(\"Time spend on waiting for tasks to complete\", 0,\n+        abfsOutputStreamStatistics.getTimeSpendOnTaskWait());\n+\n+    int smallRandomStartTime =\n+        new Random().nextInt(LOW_RANGE_FOR_RANDOM_VALUE);\n+    int smallRandomEndTime =\n+        new Random().nextInt(LOW_RANGE_FOR_RANDOM_VALUE)\n+            + smallRandomStartTime;\n+    int smallDiff = smallRandomEndTime - smallRandomStartTime;\n+    abfsOutputStreamStatistics\n+        .timeSpentTaskWait(smallRandomStartTime, smallRandomEndTime);\n+    //Test for small random value of timeSpentWaitTask.\n+    assertValues(\"Time spend on waiting for tasks to complete\", smallDiff,\n+        abfsOutputStreamStatistics.getTimeSpendOnTaskWait());\n+\n+    int largeRandomStartTime =\n+        new Random().nextInt(HIGH_RANGE_FOR_RANDOM_VALUE);\n+    int largeRandomEndTime = new Random().nextInt(HIGH_RANGE_FOR_RANDOM_VALUE)\n+        + largeRandomStartTime;\n+    int randomDiff = largeRandomEndTime - largeRandomStartTime;\n+    abfsOutputStreamStatistics\n+        .timeSpentTaskWait(largeRandomStartTime, largeRandomEndTime);\n+      /*", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a667ab0820443fde451225be1f628f7f451005da"}, "originalPosition": 111}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkyOTgxMDky", "url": "https://github.com/apache/hadoop/pull/1899#pullrequestreview-392981092", "createdAt": "2020-04-14T14:26:38Z", "commit": {"oid": "a667ab0820443fde451225be1f628f7f451005da"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxNDoyNjozOFrOGFRShQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxNDoyNjozOFrOGFRShQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE3OTMzMw==", "bodyText": "same as comment on L68.", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r408179333", "createdAt": "2020-04-14T14:26:38Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/TestAbfsOutputStreamStatistics.java", "diffHunk": "@@ -0,0 +1,120 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+import java.util.Random;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStreamStatisticsImpl;\n+\n+/**\n+ * Unit Tests for AbfsOutputStream Statistics.\n+ */\n+public class TestAbfsOutputStreamStatistics\n+    extends AbstractAbfsIntegrationTest {\n+\n+  private static final int LOW_RANGE_FOR_RANDOM_VALUE = 49;\n+  private static final int HIGH_RANGE_FOR_RANDOM_VALUE = 9999;\n+\n+  public TestAbfsOutputStreamStatistics() throws Exception {\n+  }\n+\n+  /**\n+   * Tests to check bytes failed to Upload in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamBytesFailed() {\n+    describe(\"Testing Bytes Failed during uploading in AbfsOutputSteam\");\n+\n+    AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics =\n+        new AbfsOutputStreamStatisticsImpl();\n+\n+    //Test for zero bytes uploaded.\n+    assertValues(\"number fo bytes failed to upload\", 0,\n+        abfsOutputStreamStatistics.getBytesUploadFailed());\n+\n+    //Populating small random value for bytesFailed.\n+    int randomBytesFailed = new Random().nextInt(LOW_RANGE_FOR_RANDOM_VALUE);\n+    abfsOutputStreamStatistics.uploadFailed(randomBytesFailed);\n+    //Test for bytes failed to upload.\n+    assertValues(\"number fo bytes failed to upload\", randomBytesFailed,\n+        abfsOutputStreamStatistics.getBytesUploadFailed());\n+\n+    //Initializing again to reset the statistics.\n+    abfsOutputStreamStatistics = new AbfsOutputStreamStatisticsImpl();\n+\n+    //Populating large random values for bytesFailed.\n+    randomBytesFailed = new Random().nextInt(HIGH_RANGE_FOR_RANDOM_VALUE);\n+    abfsOutputStreamStatistics.uploadFailed(randomBytesFailed);\n+    //Test for bytes failed to upload.\n+    assertValues(\"number fo bytes failed to upload\", randomBytesFailed,\n+        abfsOutputStreamStatistics.getBytesUploadFailed());\n+  }\n+\n+  /**\n+   * Tests to check time spent on waiting for tasks to be complete on a\n+   * blocking queue in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamTimeSpentOnWaitTask() {\n+    describe(\"Testing Time Spend on Waiting for Task to be complete\");\n+\n+    AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics =\n+        new AbfsOutputStreamStatisticsImpl();\n+\n+    //Test for initial value of timeSpentWaitTask.\n+    assertValues(\"Time spend on waiting for tasks to complete\", 0,\n+        abfsOutputStreamStatistics.getTimeSpendOnTaskWait());\n+\n+    int smallRandomStartTime =\n+        new Random().nextInt(LOW_RANGE_FOR_RANDOM_VALUE);\n+    int smallRandomEndTime =\n+        new Random().nextInt(LOW_RANGE_FOR_RANDOM_VALUE)\n+            + smallRandomStartTime;\n+    int smallDiff = smallRandomEndTime - smallRandomStartTime;\n+    abfsOutputStreamStatistics\n+        .timeSpentTaskWait(smallRandomStartTime, smallRandomEndTime);\n+    //Test for small random value of timeSpentWaitTask.\n+    assertValues(\"Time spend on waiting for tasks to complete\", smallDiff,\n+        abfsOutputStreamStatistics.getTimeSpendOnTaskWait());\n+\n+    int largeRandomStartTime =\n+        new Random().nextInt(HIGH_RANGE_FOR_RANDOM_VALUE);\n+    int largeRandomEndTime = new Random().nextInt(HIGH_RANGE_FOR_RANDOM_VALUE)\n+        + largeRandomStartTime;\n+    int randomDiff = largeRandomEndTime - largeRandomStartTime;\n+    abfsOutputStreamStatistics", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a667ab0820443fde451225be1f628f7f451005da"}, "originalPosition": 109}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkyOTgzMjMx", "url": "https://github.com/apache/hadoop/pull/1899#pullrequestreview-392983231", "createdAt": "2020-04-14T14:28:43Z", "commit": {"oid": "a667ab0820443fde451225be1f628f7f451005da"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxNDoyODo0NFrOGFRZEQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxNDoyODo0NFrOGFRZEQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE4MTAwOQ==", "bodyText": "remove bytes.", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r408181009", "createdAt": "2020-04-14T14:28:44Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStreamStatisticsImpl.java", "diffHunk": "@@ -0,0 +1,177 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+/**\n+ * OutputStream Statistics Implementation for Abfs.\n+ */\n+public class AbfsOutputStreamStatisticsImpl\n+    implements AbfsOutputStreamStatistics {\n+  private long bytesToUpload;\n+  private long bytesUploadSuccessful;\n+  private long bytesUploadFailed;\n+  /**\n+   * counter to get the total time spent while waiting for tasks to complete\n+   * in the Blocking queue inside the thread executor.\n+   */\n+  private long timeSpendOnTaskWait;\n+  /**\n+   * counter to get the total number of queue shrink operations done{@code\n+   * AbfsOutputStream#shrinkWriteOperationQueue()} by\n+   * AbfsOutputStream to remove the write operations which were successfully\n+   * done by AbfsOutputStream from the Blocking Queue.\n+   */\n+  private long queueShrunkOps;\n+  /**\n+   * counter to get the total number of times the current buffer is written\n+   * to the service{@code AbfsOutputStream#writeCurrentBufferToService()} via\n+   * AbfsClient and appended to the\n+   * Data store by\n+   * AbfsResOperation.\n+   */\n+  private long writeCurrentBufferOperations;\n+\n+  /**\n+   * Records the need to upload bytes and increments the total bytes that\n+   * needs to be uploaded.\n+   *\n+   * @param bytes Total bytes to upload. Negative bytes are ignored.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a667ab0820443fde451225be1f628f7f451005da"}, "originalPosition": 54}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkyOTcxOTcx", "url": "https://github.com/apache/hadoop/pull/1899#pullrequestreview-392971971", "createdAt": "2020-04-14T14:17:16Z", "commit": {"oid": "a667ab0820443fde451225be1f628f7f451005da"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxNDoxNzoxNlrOGFQ2Rg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxNDoyNzowOVrOGFRUFw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE3MjEwMg==", "bodyText": "just create your own static LOG", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r408172102", "createdAt": "2020-04-14T14:17:16Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -279,6 +283,9 @@ public synchronized void close() throws IOException {\n         threadExecutor.shutdownNow();\n       }\n     }\n+    if (LOG.isDebugEnabled()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a667ab0820443fde451225be1f628f7f451005da"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE3MzgzMA==", "bodyText": "remember that discussion we had about volatile vs long and you concluded that we could shut yetus up by going non volatile?\nIn #1820 I've moved s3a input stream stats to volatile so that the IOStatistics gets the latest values without blocking...and then turned off findbugs warnings (or at least, I'm trying to)", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r408173830", "createdAt": "2020-04-14T14:19:30Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStreamStatisticsImpl.java", "diffHunk": "@@ -0,0 +1,177 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+/**\n+ * OutputStream Statistics Implementation for Abfs.\n+ */\n+public class AbfsOutputStreamStatisticsImpl\n+    implements AbfsOutputStreamStatistics {\n+  private long bytesToUpload;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a667ab0820443fde451225be1f628f7f451005da"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE3NDMyNQ==", "bodyText": "not needed; just cut it", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r408174325", "createdAt": "2020-04-14T14:20:08Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsOutputStreamStatistics.java", "diffHunk": "@@ -0,0 +1,233 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStreamStatisticsImpl;\n+\n+/**\n+ * Test AbfsOutputStream statistics.\n+ */\n+public class ITestAbfsOutputStreamStatistics\n+    extends AbstractAbfsIntegrationTest {\n+  private static final int LARGE_OPERATIONS = 10;\n+\n+  public ITestAbfsOutputStreamStatistics() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a667ab0820443fde451225be1f628f7f451005da"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE3NDg5OA==", "bodyText": "nit: pull up to previous line", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r408174898", "createdAt": "2020-04-14T14:20:53Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsOutputStreamStatistics.java", "diffHunk": "@@ -0,0 +1,233 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStreamStatisticsImpl;\n+\n+/**\n+ * Test AbfsOutputStream statistics.\n+ */\n+public class ITestAbfsOutputStreamStatistics\n+    extends AbstractAbfsIntegrationTest {\n+  private static final int LARGE_OPERATIONS = 10;\n+\n+  public ITestAbfsOutputStreamStatistics() throws Exception {\n+  }\n+\n+  /**\n+   * Tests to check bytes Uploaded successfully in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamUploadingBytes() throws IOException {\n+    describe(\"Testing Bytes uploaded successfully in AbfsOutputSteam\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path uploadBytesFilePath = path(getMethodName());\n+    String testBytesToUpload = \"bytes\";\n+\n+    try (\n+        AbfsOutputStream outForSomeBytes = createAbfsOutputStreamWithFlushEnabled(\n+            fs,\n+            uploadBytesFilePath)\n+    ) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a667ab0820443fde451225be1f628f7f451005da"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE3NTM2NA==", "bodyText": "nit: just cut these lines from the javadoc. Nice to see the rest of the detail", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r408175364", "createdAt": "2020-04-14T14:21:28Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsOutputStreamStatistics.java", "diffHunk": "@@ -0,0 +1,233 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStreamStatisticsImpl;\n+\n+/**\n+ * Test AbfsOutputStream statistics.\n+ */\n+public class ITestAbfsOutputStreamStatistics\n+    extends AbstractAbfsIntegrationTest {\n+  private static final int LARGE_OPERATIONS = 10;\n+\n+  public ITestAbfsOutputStreamStatistics() throws Exception {\n+  }\n+\n+  /**\n+   * Tests to check bytes Uploaded successfully in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamUploadingBytes() throws IOException {\n+    describe(\"Testing Bytes uploaded successfully in AbfsOutputSteam\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path uploadBytesFilePath = path(getMethodName());\n+    String testBytesToUpload = \"bytes\";\n+\n+    try (\n+        AbfsOutputStream outForSomeBytes = createAbfsOutputStreamWithFlushEnabled(\n+            fs,\n+            uploadBytesFilePath)\n+    ) {\n+\n+      AbfsOutputStreamStatisticsImpl abfsOutputStreamStatisticsForUploadBytes =\n+          outForSomeBytes.getOutputStreamStatistics();\n+\n+      //Test for zero bytes To upload.\n+      assertValues(\"bytes to upload\", 0,\n+          abfsOutputStreamStatisticsForUploadBytes.getBytesToUpload());\n+\n+      outForSomeBytes.write(testBytesToUpload.getBytes());\n+      outForSomeBytes.flush();\n+      abfsOutputStreamStatisticsForUploadBytes =\n+          outForSomeBytes.getOutputStreamStatistics();\n+\n+      //Test for bytes to upload.\n+      assertValues(\"bytes to upload\", testBytesToUpload.getBytes().length,\n+          abfsOutputStreamStatisticsForUploadBytes.getBytesToUpload());\n+\n+      //Test for successful bytes uploaded.\n+      assertValues(\"successful bytes uploaded\",\n+          testBytesToUpload.getBytes().length,\n+          abfsOutputStreamStatisticsForUploadBytes.getBytesUploadSuccessful());\n+\n+    }\n+\n+    try (\n+        AbfsOutputStream outForLargeBytes = createAbfsOutputStreamWithFlushEnabled(\n+            fs,\n+            uploadBytesFilePath)) {\n+\n+      for (int i = 0; i < LARGE_OPERATIONS; i++) {\n+        outForLargeBytes.write(testBytesToUpload.getBytes());\n+      }\n+      outForLargeBytes.flush();\n+      AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics =\n+          outForLargeBytes.getOutputStreamStatistics();\n+\n+      //Test for bytes to upload.\n+      assertValues(\"bytes to upload\",\n+          LARGE_OPERATIONS * (testBytesToUpload.getBytes().length),\n+          abfsOutputStreamStatistics.getBytesToUpload());\n+\n+      //Test for successful bytes uploaded.\n+      assertValues(\"successful bytes uploaded\",\n+          LARGE_OPERATIONS * (testBytesToUpload.getBytes().length),\n+          abfsOutputStreamStatistics.getBytesUploadSuccessful());\n+\n+    }\n+  }\n+\n+  /**\n+   * Tests to check number of {@code\n+   * AbfsOutputStream#shrinkWriteOperationQueue()} calls.\n+   * After writing data, AbfsOutputStream doesn't upload the data until\n+   * Flushed. Hence, flush() method is called after write() to test Queue\n+   * shrink calls.\n+   *\n+   * @throws IOException", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a667ab0820443fde451225be1f628f7f451005da"}, "originalPosition": 112}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE3NjU1OA==", "bodyText": "why is this being cast rather than returned as is?", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r408176558", "createdAt": "2020-04-14T14:22:57Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -436,4 +453,28 @@ private void waitForTaskToComplete() throws IOException {\n   public synchronized void waitForPendingUploads() throws IOException {\n     waitForTaskToComplete();\n   }\n+\n+  /**\n+   * Getter method for AbfsOutputStream Statistics.\n+   *\n+   * @return statistics for AbfsOutputStream.\n+   */\n+  @VisibleForTesting\n+  public AbfsOutputStreamStatisticsImpl getOutputStreamStatistics() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a667ab0820443fde451225be1f628f7f451005da"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE3NzkzMg==", "bodyText": "too vague a name and no obvious difference with assertEquals.\nPropose:\n\nleave existing test alone,\nand then one of\nuse Junit assertEquals()\nor Assertions.assertThat(object).describedAs().equals().", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r408177932", "createdAt": "2020-04-14T14:24:44Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/AbstractAbfsIntegrationTest.java", "diffHunk": "@@ -383,4 +391,34 @@ protected AbfsDelegationTokenManager getDelegationTokenManager()\n       throws IOException {\n     return getFileSystem().getDelegationTokenManager();\n   }\n+\n+  /**\n+   * Generic assert method.\n+   *\n+   * @param operation     operation being asserted.\n+   * @param expectedValue value that is expected.\n+   * @param actualValue   value that is actual.\n+   */\n+  protected void assertValues(String operation, long expectedValue,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a667ab0820443fde451225be1f628f7f451005da"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE3ODE5OA==", "bodyText": "nit: typo", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r408178198", "createdAt": "2020-04-14T14:25:08Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/TestAbfsOutputStreamStatistics.java", "diffHunk": "@@ -0,0 +1,120 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+import java.util.Random;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStreamStatisticsImpl;\n+\n+/**\n+ * Unit Tests for AbfsOutputStream Statistics.\n+ */\n+public class TestAbfsOutputStreamStatistics\n+    extends AbstractAbfsIntegrationTest {\n+\n+  private static final int LOW_RANGE_FOR_RANDOM_VALUE = 49;\n+  private static final int HIGH_RANGE_FOR_RANDOM_VALUE = 9999;\n+\n+  public TestAbfsOutputStreamStatistics() throws Exception {\n+  }\n+\n+  /**\n+   * Tests to check bytes failed to Upload in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamBytesFailed() {\n+    describe(\"Testing Bytes Failed during uploading in AbfsOutputSteam\");\n+\n+    AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics =\n+        new AbfsOutputStreamStatisticsImpl();\n+\n+    //Test for zero bytes uploaded.\n+    assertValues(\"number fo bytes failed to upload\", 0,\n+        abfsOutputStreamStatistics.getBytesUploadFailed());\n+\n+    //Populating small random value for bytesFailed.\n+    int randomBytesFailed = new Random().nextInt(LOW_RANGE_FOR_RANDOM_VALUE);\n+    abfsOutputStreamStatistics.uploadFailed(randomBytesFailed);\n+    //Test for bytes failed to upload.\n+    assertValues(\"number fo bytes failed to upload\", randomBytesFailed,\n+        abfsOutputStreamStatistics.getBytesUploadFailed());\n+\n+    //Initializing again to reset the statistics.\n+    abfsOutputStreamStatistics = new AbfsOutputStreamStatisticsImpl();\n+\n+    //Populating large random values for bytesFailed.\n+    randomBytesFailed = new Random().nextInt(HIGH_RANGE_FOR_RANDOM_VALUE);\n+    abfsOutputStreamStatistics.uploadFailed(randomBytesFailed);\n+    //Test for bytes failed to upload.\n+    assertValues(\"number fo bytes failed to upload\", randomBytesFailed,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a667ab0820443fde451225be1f628f7f451005da"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE3ODMxMg==", "bodyText": "typo", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r408178312", "createdAt": "2020-04-14T14:25:18Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/TestAbfsOutputStreamStatistics.java", "diffHunk": "@@ -0,0 +1,120 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+import java.util.Random;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStreamStatisticsImpl;\n+\n+/**\n+ * Unit Tests for AbfsOutputStream Statistics.\n+ */\n+public class TestAbfsOutputStreamStatistics\n+    extends AbstractAbfsIntegrationTest {\n+\n+  private static final int LOW_RANGE_FOR_RANDOM_VALUE = 49;\n+  private static final int HIGH_RANGE_FOR_RANDOM_VALUE = 9999;\n+\n+  public TestAbfsOutputStreamStatistics() throws Exception {\n+  }\n+\n+  /**\n+   * Tests to check bytes failed to Upload in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamBytesFailed() {\n+    describe(\"Testing Bytes Failed during uploading in AbfsOutputSteam\");\n+\n+    AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics =\n+        new AbfsOutputStreamStatisticsImpl();\n+\n+    //Test for zero bytes uploaded.\n+    assertValues(\"number fo bytes failed to upload\", 0,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a667ab0820443fde451225be1f628f7f451005da"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE3ODQ5OA==", "bodyText": "nit: newline", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r408178498", "createdAt": "2020-04-14T14:25:32Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/TestAbfsOutputStreamStatistics.java", "diffHunk": "@@ -0,0 +1,120 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+import java.util.Random;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStreamStatisticsImpl;\n+\n+/**\n+ * Unit Tests for AbfsOutputStream Statistics.\n+ */\n+public class TestAbfsOutputStreamStatistics\n+    extends AbstractAbfsIntegrationTest {\n+\n+  private static final int LOW_RANGE_FOR_RANDOM_VALUE = 49;\n+  private static final int HIGH_RANGE_FOR_RANDOM_VALUE = 9999;\n+\n+  public TestAbfsOutputStreamStatistics() throws Exception {\n+  }\n+\n+  /**\n+   * Tests to check bytes failed to Upload in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamBytesFailed() {\n+    describe(\"Testing Bytes Failed during uploading in AbfsOutputSteam\");\n+\n+    AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics =\n+        new AbfsOutputStreamStatisticsImpl();\n+\n+    //Test for zero bytes uploaded.\n+    assertValues(\"number fo bytes failed to upload\", 0,\n+        abfsOutputStreamStatistics.getBytesUploadFailed());\n+\n+    //Populating small random value for bytesFailed.\n+    int randomBytesFailed = new Random().nextInt(LOW_RANGE_FOR_RANDOM_VALUE);\n+    abfsOutputStreamStatistics.uploadFailed(randomBytesFailed);\n+    //Test for bytes failed to upload.\n+    assertValues(\"number fo bytes failed to upload\", randomBytesFailed,\n+        abfsOutputStreamStatistics.getBytesUploadFailed());\n+\n+    //Initializing again to reset the statistics.\n+    abfsOutputStreamStatistics = new AbfsOutputStreamStatisticsImpl();\n+\n+    //Populating large random values for bytesFailed.\n+    randomBytesFailed = new Random().nextInt(HIGH_RANGE_FOR_RANDOM_VALUE);\n+    abfsOutputStreamStatistics.uploadFailed(randomBytesFailed);\n+    //Test for bytes failed to upload.\n+    assertValues(\"number fo bytes failed to upload\", randomBytesFailed,\n+        abfsOutputStreamStatistics.getBytesUploadFailed());\n+  }\n+\n+  /**\n+   * Tests to check time spent on waiting for tasks to be complete on a\n+   * blocking queue in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamTimeSpentOnWaitTask() {\n+    describe(\"Testing Time Spend on Waiting for Task to be complete\");\n+\n+    AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics =\n+        new AbfsOutputStreamStatisticsImpl();\n+\n+    //Test for initial value of timeSpentWaitTask.\n+    assertValues(\"Time spend on waiting for tasks to complete\", 0,\n+        abfsOutputStreamStatistics.getTimeSpendOnTaskWait());\n+\n+    int smallRandomStartTime =\n+        new Random().nextInt(LOW_RANGE_FOR_RANDOM_VALUE);\n+    int smallRandomEndTime =\n+        new Random().nextInt(LOW_RANGE_FOR_RANDOM_VALUE)\n+            + smallRandomStartTime;\n+    int smallDiff = smallRandomEndTime - smallRandomStartTime;\n+    abfsOutputStreamStatistics\n+        .timeSpentTaskWait(smallRandomStartTime, smallRandomEndTime);\n+    //Test for small random value of timeSpentWaitTask.\n+    assertValues(\"Time spend on waiting for tasks to complete\", smallDiff,\n+        abfsOutputStreamStatistics.getTimeSpendOnTaskWait());\n+\n+    int largeRandomStartTime =\n+        new Random().nextInt(HIGH_RANGE_FOR_RANDOM_VALUE);\n+    int largeRandomEndTime = new Random().nextInt(HIGH_RANGE_FOR_RANDOM_VALUE)\n+        + largeRandomStartTime;\n+    int randomDiff = largeRandomEndTime - largeRandomStartTime;\n+    abfsOutputStreamStatistics\n+        .timeSpentTaskWait(largeRandomStartTime, largeRandomEndTime);\n+      /*\n+      Test for large random value of timeSpentWaitTask plus the time spent\n+      waiting in previous test.\n+       */\n+    assertValues(\"Time spend on waiting for tasks to complete\",\n+        smallDiff + randomDiff,\n+        abfsOutputStreamStatistics.getTimeSpendOnTaskWait());\n+  }\n+\n+}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a667ab0820443fde451225be1f628f7f451005da"}, "originalPosition": 120}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE3OTA5MQ==", "bodyText": "don't think we need capitals here", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r408179091", "createdAt": "2020-04-14T14:26:17Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStreamStatisticsImpl.java", "diffHunk": "@@ -0,0 +1,177 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+/**\n+ * OutputStream Statistics Implementation for Abfs.\n+ */\n+public class AbfsOutputStreamStatisticsImpl\n+    implements AbfsOutputStreamStatistics {\n+  private long bytesToUpload;\n+  private long bytesUploadSuccessful;\n+  private long bytesUploadFailed;\n+  /**\n+   * counter to get the total time spent while waiting for tasks to complete\n+   * in the Blocking queue inside the thread executor.\n+   */\n+  private long timeSpendOnTaskWait;\n+  /**\n+   * counter to get the total number of queue shrink operations done{@code\n+   * AbfsOutputStream#shrinkWriteOperationQueue()} by\n+   * AbfsOutputStream to remove the write operations which were successfully\n+   * done by AbfsOutputStream from the Blocking Queue.\n+   */\n+  private long queueShrunkOps;\n+  /**\n+   * counter to get the total number of times the current buffer is written\n+   * to the service{@code AbfsOutputStream#writeCurrentBufferToService()} via\n+   * AbfsClient and appended to the\n+   * Data store by\n+   * AbfsResOperation.\n+   */\n+  private long writeCurrentBufferOperations;\n+\n+  /**\n+   * Records the need to upload bytes and increments the total bytes that\n+   * needs to be uploaded.\n+   *\n+   * @param bytes Total bytes to upload. Negative bytes are ignored.\n+   */\n+  @Override\n+  public void bytesToUpload(long bytes) {\n+    if (bytes > 0) {\n+      bytesToUpload += bytes;\n+    }\n+  }\n+\n+  /**\n+   * Records the total bytes successfully uploaded through AbfsOutputStream.\n+   *\n+   * @param bytes number of bytes that were successfully uploaded. Negative\n+   *              bytes are ignored.\n+   */\n+  @Override\n+  public void uploadSuccessful(long bytes) {\n+    if (bytes > 0) {\n+      bytesUploadSuccessful += bytes;\n+    }\n+  }\n+\n+  /**\n+   * Records the total bytes failed to upload through AbfsOutputStream.\n+   *\n+   * @param bytes number of bytes failed to upload. Negative bytes are ignored.\n+   */\n+  @Override\n+  public void uploadFailed(long bytes) {\n+    if (bytes > 0) {\n+      bytesUploadFailed += bytes;\n+    }\n+  }\n+\n+  /**\n+   * {@inheritDoc}\n+   *\n+   * Records the total time spent waiting for a task.\n+   * When the thread executor has a task\n+   * queue{@link java.util.concurrent.BlockingQueue} of size greater than or equal to 2\n+   * times the maxConcurrentRequestCounts then, it waits for a task in that\n+   * queue to finish, then do the next task in the queue.\n+   *\n+   * This time spent while waiting for the task to be completed is being\n+   * recorded in this counter.\n+   *\n+   * @param startTime time(in milliseconds) before the wait for task to be\n+   *                  completed is begin.\n+   * @param endTime   time(in milliseconds) after the wait for the task to be\n+   *                  completed is done.\n+   */\n+  @Override\n+  public void timeSpentTaskWait(long startTime, long endTime) {\n+    timeSpendOnTaskWait += endTime - startTime;\n+  }\n+\n+  /**\n+   * {@inheritDoc}\n+   *\n+   * Records the number of times AbfsOutputStream try to remove the completed\n+   * write operations from the beginning of write operation FIFO queue.\n+   */\n+  @Override\n+  public void queueShrunk() {\n+    queueShrunkOps++;\n+  }\n+\n+  /**\n+   * {@inheritDoc}\n+   *\n+   * Records the number of times AbfsOutputStream writes the buffer to the\n+   * service via the AbfsClient and appends the buffer to the service.\n+   */\n+  @Override\n+  public void writeCurrentBuffer() {\n+    writeCurrentBufferOperations++;\n+  }\n+\n+  public long getBytesToUpload() {\n+    return bytesToUpload;\n+  }\n+\n+  public long getBytesUploadSuccessful() {\n+    return bytesUploadSuccessful;\n+  }\n+\n+  public long getBytesUploadFailed() {\n+    return bytesUploadFailed;\n+  }\n+\n+  public long getTimeSpendOnTaskWait() {\n+    return timeSpendOnTaskWait;\n+  }\n+\n+  public long getQueueShrunkOps() {\n+    return queueShrunkOps;\n+  }\n+\n+  public long getWriteCurrentBufferOperations() {\n+    return writeCurrentBufferOperations;\n+  }\n+\n+  /**\n+   * String to show AbfsOutputStream statistics values in AbfsOutputStream.\n+   *\n+   * @return String with AbfsOutputStream statistics.\n+   */\n+  @Override public String toString() {\n+    final StringBuilder outputStreamStats = new StringBuilder(\n+        \"OutputStream Statistics{\");\n+    outputStreamStats.append(\", BYTES_UPLOAD=\").append(bytesToUpload);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a667ab0820443fde451225be1f628f7f451005da"}, "originalPosition": 164}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE3OTU5Mw==", "bodyText": "timeSpent", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r408179593", "createdAt": "2020-04-14T14:26:58Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStreamStatisticsImpl.java", "diffHunk": "@@ -0,0 +1,177 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+/**\n+ * OutputStream Statistics Implementation for Abfs.\n+ */\n+public class AbfsOutputStreamStatisticsImpl\n+    implements AbfsOutputStreamStatistics {\n+  private long bytesToUpload;\n+  private long bytesUploadSuccessful;\n+  private long bytesUploadFailed;\n+  /**\n+   * counter to get the total time spent while waiting for tasks to complete\n+   * in the Blocking queue inside the thread executor.\n+   */\n+  private long timeSpendOnTaskWait;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a667ab0820443fde451225be1f628f7f451005da"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE3OTczNQ==", "bodyText": "typo", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r408179735", "createdAt": "2020-04-14T14:27:09Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStreamStatisticsImpl.java", "diffHunk": "@@ -0,0 +1,177 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+/**\n+ * OutputStream Statistics Implementation for Abfs.\n+ */\n+public class AbfsOutputStreamStatisticsImpl\n+    implements AbfsOutputStreamStatistics {\n+  private long bytesToUpload;\n+  private long bytesUploadSuccessful;\n+  private long bytesUploadFailed;\n+  /**\n+   * counter to get the total time spent while waiting for tasks to complete\n+   * in the Blocking queue inside the thread executor.\n+   */\n+  private long timeSpendOnTaskWait;\n+  /**\n+   * counter to get the total number of queue shrink operations done{@code\n+   * AbfsOutputStream#shrinkWriteOperationQueue()} by\n+   * AbfsOutputStream to remove the write operations which were successfully\n+   * done by AbfsOutputStream from the Blocking Queue.\n+   */\n+  private long queueShrunkOps;\n+  /**\n+   * counter to get the total number of times the current buffer is written\n+   * to the service{@code AbfsOutputStream#writeCurrentBufferToService()} via\n+   * AbfsClient and appended to the\n+   * Data store by\n+   * AbfsResOperation.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a667ab0820443fde451225be1f628f7f451005da"}, "originalPosition": 46}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkyOTgzOTEz", "url": "https://github.com/apache/hadoop/pull/1899#pullrequestreview-392983913", "createdAt": "2020-04-14T14:29:23Z", "commit": {"oid": "a667ab0820443fde451225be1f628f7f451005da"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxNDoyOToyM1rOGFRbQA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxNDoyOToyM1rOGFRbQA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE4MTU2OA==", "bodyText": "Remove bytes.", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r408181568", "createdAt": "2020-04-14T14:29:23Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStreamStatisticsImpl.java", "diffHunk": "@@ -0,0 +1,177 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+/**\n+ * OutputStream Statistics Implementation for Abfs.\n+ */\n+public class AbfsOutputStreamStatisticsImpl\n+    implements AbfsOutputStreamStatistics {\n+  private long bytesToUpload;\n+  private long bytesUploadSuccessful;\n+  private long bytesUploadFailed;\n+  /**\n+   * counter to get the total time spent while waiting for tasks to complete\n+   * in the Blocking queue inside the thread executor.\n+   */\n+  private long timeSpendOnTaskWait;\n+  /**\n+   * counter to get the total number of queue shrink operations done{@code\n+   * AbfsOutputStream#shrinkWriteOperationQueue()} by\n+   * AbfsOutputStream to remove the write operations which were successfully\n+   * done by AbfsOutputStream from the Blocking Queue.\n+   */\n+  private long queueShrunkOps;\n+  /**\n+   * counter to get the total number of times the current buffer is written\n+   * to the service{@code AbfsOutputStream#writeCurrentBufferToService()} via\n+   * AbfsClient and appended to the\n+   * Data store by\n+   * AbfsResOperation.\n+   */\n+  private long writeCurrentBufferOperations;\n+\n+  /**\n+   * Records the need to upload bytes and increments the total bytes that\n+   * needs to be uploaded.\n+   *\n+   * @param bytes Total bytes to upload. Negative bytes are ignored.\n+   */\n+  @Override\n+  public void bytesToUpload(long bytes) {\n+    if (bytes > 0) {\n+      bytesToUpload += bytes;\n+    }\n+  }\n+\n+  /**\n+   * Records the total bytes successfully uploaded through AbfsOutputStream.\n+   *\n+   * @param bytes number of bytes that were successfully uploaded. Negative\n+   *              bytes are ignored.\n+   */\n+  @Override\n+  public void uploadSuccessful(long bytes) {\n+    if (bytes > 0) {\n+      bytesUploadSuccessful += bytes;\n+    }\n+  }\n+\n+  /**\n+   * Records the total bytes failed to upload through AbfsOutputStream.\n+   *\n+   * @param bytes number of bytes failed to upload. Negative bytes are ignored.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a667ab0820443fde451225be1f628f7f451005da"}, "originalPosition": 79}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkyOTkyNTQ4", "url": "https://github.com/apache/hadoop/pull/1899#pullrequestreview-392992548", "createdAt": "2020-04-14T14:38:04Z", "commit": {"oid": "a667ab0820443fde451225be1f628f7f451005da"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxNDozODowNFrOGFR1cw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxNDozODowNFrOGFR1cw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE4ODI3NQ==", "bodyText": "No new line at the end. I think this will cause checkstyle issue.", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r408188275", "createdAt": "2020-04-14T14:38:04Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/TestAbfsOutputStreamStatistics.java", "diffHunk": "@@ -0,0 +1,120 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+import java.util.Random;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStreamStatisticsImpl;\n+\n+/**\n+ * Unit Tests for AbfsOutputStream Statistics.\n+ */\n+public class TestAbfsOutputStreamStatistics\n+    extends AbstractAbfsIntegrationTest {\n+\n+  private static final int LOW_RANGE_FOR_RANDOM_VALUE = 49;\n+  private static final int HIGH_RANGE_FOR_RANDOM_VALUE = 9999;\n+\n+  public TestAbfsOutputStreamStatistics() throws Exception {\n+  }\n+\n+  /**\n+   * Tests to check bytes failed to Upload in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamBytesFailed() {\n+    describe(\"Testing Bytes Failed during uploading in AbfsOutputSteam\");\n+\n+    AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics =\n+        new AbfsOutputStreamStatisticsImpl();\n+\n+    //Test for zero bytes uploaded.\n+    assertValues(\"number fo bytes failed to upload\", 0,\n+        abfsOutputStreamStatistics.getBytesUploadFailed());\n+\n+    //Populating small random value for bytesFailed.\n+    int randomBytesFailed = new Random().nextInt(LOW_RANGE_FOR_RANDOM_VALUE);\n+    abfsOutputStreamStatistics.uploadFailed(randomBytesFailed);\n+    //Test for bytes failed to upload.\n+    assertValues(\"number fo bytes failed to upload\", randomBytesFailed,\n+        abfsOutputStreamStatistics.getBytesUploadFailed());\n+\n+    //Initializing again to reset the statistics.\n+    abfsOutputStreamStatistics = new AbfsOutputStreamStatisticsImpl();\n+\n+    //Populating large random values for bytesFailed.\n+    randomBytesFailed = new Random().nextInt(HIGH_RANGE_FOR_RANDOM_VALUE);\n+    abfsOutputStreamStatistics.uploadFailed(randomBytesFailed);\n+    //Test for bytes failed to upload.\n+    assertValues(\"number fo bytes failed to upload\", randomBytesFailed,\n+        abfsOutputStreamStatistics.getBytesUploadFailed());\n+  }\n+\n+  /**\n+   * Tests to check time spent on waiting for tasks to be complete on a\n+   * blocking queue in {@link AbfsOutputStream}.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testAbfsOutputStreamTimeSpentOnWaitTask() {\n+    describe(\"Testing Time Spend on Waiting for Task to be complete\");\n+\n+    AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics =\n+        new AbfsOutputStreamStatisticsImpl();\n+\n+    //Test for initial value of timeSpentWaitTask.\n+    assertValues(\"Time spend on waiting for tasks to complete\", 0,\n+        abfsOutputStreamStatistics.getTimeSpendOnTaskWait());\n+\n+    int smallRandomStartTime =\n+        new Random().nextInt(LOW_RANGE_FOR_RANDOM_VALUE);\n+    int smallRandomEndTime =\n+        new Random().nextInt(LOW_RANGE_FOR_RANDOM_VALUE)\n+            + smallRandomStartTime;\n+    int smallDiff = smallRandomEndTime - smallRandomStartTime;\n+    abfsOutputStreamStatistics\n+        .timeSpentTaskWait(smallRandomStartTime, smallRandomEndTime);\n+    //Test for small random value of timeSpentWaitTask.\n+    assertValues(\"Time spend on waiting for tasks to complete\", smallDiff,\n+        abfsOutputStreamStatistics.getTimeSpendOnTaskWait());\n+\n+    int largeRandomStartTime =\n+        new Random().nextInt(HIGH_RANGE_FOR_RANDOM_VALUE);\n+    int largeRandomEndTime = new Random().nextInt(HIGH_RANGE_FOR_RANDOM_VALUE)\n+        + largeRandomStartTime;\n+    int randomDiff = largeRandomEndTime - largeRandomStartTime;\n+    abfsOutputStreamStatistics\n+        .timeSpentTaskWait(largeRandomStartTime, largeRandomEndTime);\n+      /*\n+      Test for large random value of timeSpentWaitTask plus the time spent\n+      waiting in previous test.\n+       */\n+    assertValues(\"Time spend on waiting for tasks to complete\",\n+        smallDiff + randomDiff,\n+        abfsOutputStreamStatistics.getTimeSpendOnTaskWait());\n+  }\n+\n+}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a667ab0820443fde451225be1f628f7f451005da"}, "originalPosition": 120}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkzOTYwOTEw", "url": "https://github.com/apache/hadoop/pull/1899#pullrequestreview-393960910", "createdAt": "2020-04-15T16:55:59Z", "commit": {"oid": "a667ab0820443fde451225be1f628f7f451005da"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQxNjo1NjowMFrOGGC5vQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQxNjo1NjowMFrOGGC5vQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODk5MjE4OQ==", "bodyText": "Queue is not actually getting shrunk here, rather inside the while loop. I'll change this and the respective tests.", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r408992189", "createdAt": "2020-04-15T16:56:00Z", "author": {"login": "mehakmeet"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -384,6 +400,7 @@ private synchronized void flushWrittenBytesToServiceInternal(final long offset,\n    * operation FIFO queue.\n    */\n   private synchronized void shrinkWriteOperationQueue() throws IOException {\n+    outputStreamStatistics.queueShrunk();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a667ab0820443fde451225be1f628f7f451005da"}, "originalPosition": 78}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a667ab0820443fde451225be1f628f7f451005da", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/a667ab0820443fde451225be1f628f7f451005da", "committedDate": "2020-04-14T12:27:26Z", "message": "HADOOP-16914 AbfsOutputStream Counter\n\nChange-Id: Ie976f23c6c3e2f5cf9167794357cfc669a232c80"}, "afterCommit": {"oid": "e3f98e6a60c1e9f7bff7772cbc1a82b98af96e9e", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/e3f98e6a60c1e9f7bff7772cbc1a82b98af96e9e", "committedDate": "2020-04-16T08:22:06Z", "message": "HADOOP-16914. queueShrunkOps and tests"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e3f98e6a60c1e9f7bff7772cbc1a82b98af96e9e", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/e3f98e6a60c1e9f7bff7772cbc1a82b98af96e9e", "committedDate": "2020-04-16T08:22:06Z", "message": "HADOOP-16914. queueShrunkOps and tests"}, "afterCommit": {"oid": "490d33c1e9ce88c2d9340ee2ed471ab30bc6e48d", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/490d33c1e9ce88c2d9340ee2ed471ab30bc6e48d", "committedDate": "2020-04-16T09:02:06Z", "message": "HADOOP-16914. queueShrunkOps and tests"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "490d33c1e9ce88c2d9340ee2ed471ab30bc6e48d", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/490d33c1e9ce88c2d9340ee2ed471ab30bc6e48d", "committedDate": "2020-04-16T09:02:06Z", "message": "HADOOP-16914. queueShrunkOps and tests"}, "afterCommit": {"oid": "51ea8fbaa3ce030182c6eda841fb9228d3eb352d", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/51ea8fbaa3ce030182c6eda841fb9228d3eb352d", "committedDate": "2020-04-16T09:56:49Z", "message": "HADOOP-16914. queueShrunkOps and tests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk2MjE4NDU4", "url": "https://github.com/apache/hadoop/pull/1899#pullrequestreview-396218458", "createdAt": "2020-04-20T07:53:42Z", "commit": {"oid": "51ea8fbaa3ce030182c6eda841fb9228d3eb352d"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk2NTY2NjEz", "url": "https://github.com/apache/hadoop/pull/1899#pullrequestreview-396566613", "createdAt": "2020-04-20T15:31:31Z", "commit": {"oid": "51ea8fbaa3ce030182c6eda841fb9228d3eb352d"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQxNTozMTozMVrOGIaacQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQxNTozMTozMVrOGIaacQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTQ3NDU0NQ==", "bodyText": "this would be coding into the implementation/semi-public API something only relevant for testing -and make it very hard to ever change to a different implementation.\nJust add some static method in the test suite\nAbfsOutputStreamStatisticsImp getStreamStatistics(AbfsOutputStream)\n\nand you could do the casting in just one place.", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r411474545", "createdAt": "2020-04-20T15:31:31Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -436,4 +453,28 @@ private void waitForTaskToComplete() throws IOException {\n   public synchronized void waitForPendingUploads() throws IOException {\n     waitForTaskToComplete();\n   }\n+\n+  /**\n+   * Getter method for AbfsOutputStream Statistics.\n+   *\n+   * @return statistics for AbfsOutputStream.\n+   */\n+  @VisibleForTesting\n+  public AbfsOutputStreamStatisticsImpl getOutputStreamStatistics() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE3NjU1OA=="}, "originalCommit": {"oid": "a667ab0820443fde451225be1f628f7f451005da"}, "originalPosition": 93}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "51ea8fbaa3ce030182c6eda841fb9228d3eb352d", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/51ea8fbaa3ce030182c6eda841fb9228d3eb352d", "committedDate": "2020-04-16T09:56:49Z", "message": "HADOOP-16914. queueShrunkOps and tests"}, "afterCommit": {"oid": "01880f57c1ed21e208bb1c17330ac7b613c8b990", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/01880f57c1ed21e208bb1c17330ac7b613c8b990", "committedDate": "2020-04-21T06:12:43Z", "message": "HADOOP-16914. fixing review comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "01880f57c1ed21e208bb1c17330ac7b613c8b990", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/01880f57c1ed21e208bb1c17330ac7b613c8b990", "committedDate": "2020-04-21T06:12:43Z", "message": "HADOOP-16914. fixing review comments"}, "afterCommit": {"oid": "f0543e1bd0d994b6f284c825fffffe92ddd06218", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/f0543e1bd0d994b6f284c825fffffe92ddd06218", "committedDate": "2020-04-22T07:03:56Z", "message": "HADOOP-16914. fixing review comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk4OTc2Mjg5", "url": "https://github.com/apache/hadoop/pull/1899#pullrequestreview-398976289", "createdAt": "2020-04-23T10:42:15Z", "commit": {"oid": "73c4c69fec5a6ac3b19db1619e1e83c9354e24ce"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QxMDo0MjoxNVrOGKiyZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QxMDo0MjoxNVrOGKiyZg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzcwODkwMg==", "bodyText": "needs to go in with the org.apache block", "url": "https://github.com/apache/hadoop/pull/1899#discussion_r413708902", "createdAt": "2020-04-23T10:42:15Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "diffHunk": "@@ -51,6 +51,7 @@\n import com.google.common.annotations.VisibleForTesting;\n import com.google.common.base.Preconditions;\n import com.google.common.base.Strings;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStreamStatisticsImpl;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "73c4c69fec5a6ac3b19db1619e1e83c9354e24ce"}, "originalPosition": 4}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "745c377d1a0c57c845ce18298b999566379079a1", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/745c377d1a0c57c845ce18298b999566379079a1", "committedDate": "2020-04-23T10:47:46Z", "message": "HADOOP-16914 AbfsOutputStream Counter\n\nChange-Id: Ie976f23c6c3e2f5cf9167794357cfc669a232c80"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fd6fe8976fd7ee03a22709ec779eba3cc632b0bb", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/fd6fe8976fd7ee03a22709ec779eba3cc632b0bb", "committedDate": "2020-04-23T10:47:46Z", "message": "HADOOP-16914. review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b2f08581ce853e6cb26a8ca39a3c73c520c00d40", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/b2f08581ce853e6cb26a8ca39a3c73c520c00d40", "committedDate": "2020-04-23T10:47:46Z", "message": "HADOOP-16914. queueShrunkOps and tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f98a76c746e68fce59593841e881387f7dd74b0d", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/f98a76c746e68fce59593841e881387f7dd74b0d", "committedDate": "2020-04-23T10:47:46Z", "message": "HADOOP-16914. fixing review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "424bb884bd9e96b3893075bcd8ef4158f4f8187b", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/424bb884bd9e96b3893075bcd8ef4158f4f8187b", "committedDate": "2020-04-23T10:47:46Z", "message": "HADOOP-16914. Passing statistics through abfsOutputStreamContext"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "73c4c69fec5a6ac3b19db1619e1e83c9354e24ce", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/73c4c69fec5a6ac3b19db1619e1e83c9354e24ce", "committedDate": "2020-04-22T13:27:56Z", "message": "HADOOP-16914. Passing statistics through abfsOutputStreamContext"}, "afterCommit": {"oid": "424bb884bd9e96b3893075bcd8ef4158f4f8187b", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/424bb884bd9e96b3893075bcd8ef4158f4f8187b", "committedDate": "2020-04-23T10:47:46Z", "message": "HADOOP-16914. Passing statistics through abfsOutputStreamContext"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4500, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}