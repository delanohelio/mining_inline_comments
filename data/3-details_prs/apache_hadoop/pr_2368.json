{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk5NTU2MDMy", "number": 2368, "title": "HADOOP-17296. ABFS: Force reads to be always of buffer size.", "bodyText": "Customers migrating from Gen1 to Gen2 often are observing different read patterns for the same workload. The optimization in Gen2 which reads only requested data size once detected as random read pattern is usually the cause of difference.\nIn this PR, config option to force Gen2 driver to read always in buffer size even for random is being introduced. With this enabled the read pattern for the job will be similar to Gen1 and be full buffer sizes to backend.\nHave also accommodated the request to config control the readahead size to help cases such as small row groups in parquet files, where more data can be captured.\nThese configs are not determined to be performant on the official parquet recommended row group sizes of 512-1024 MB and hence will not be enabled by default.\nTests are added to verify various combinations of config values. Also modified tests in file ITestAzureBlobFileSystemRandomRead which were using same file and hence test debugging was getting harder.", "createdAt": "2020-10-07T22:43:18Z", "url": "https://github.com/apache/hadoop/pull/2368", "merged": true, "mergeCommit": {"oid": "142941b96e221fc1b4524476ce445714d7f6eec3"}, "closed": true, "closedAt": "2020-11-27T14:22:34Z", "author": {"login": "snvijaya"}, "timelineItems": {"totalCount": 28, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdPgGlNgH2gAyNDk5NTU2MDMyOjI1OGYyMDcxZGFiMDAyNmY2N2U3OTU2NjMzODJjYzc3OGFiZmYwYjU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdgoURAgFqTU0MDAxMzk1NA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "258f2071dab0026f67e795663382cc778abff0b5", "author": {"user": {"login": "snvijaya", "name": "Sneha Vijayarajan"}}, "url": "https://github.com/apache/hadoop/commit/258f2071dab0026f67e795663382cc778abff0b5", "committedDate": "2020-10-05T09:10:15Z", "message": "Force full buffer read always as in the case of Gen1\nMake readahead block size and number of readahead buffers configurable\nFixes to RAH"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ab2951b427efbc0550ce0d6d92b569ab2eebf9fc", "author": {"user": {"login": "snvijaya", "name": "Sneha Vijayarajan"}}, "url": "https://github.com/apache/hadoop/commit/ab2951b427efbc0550ce0d6d92b569ab2eebf9fc", "committedDate": "2020-10-05T09:19:28Z", "message": "Removing minor reduntant changes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "48bee6d5eeaf77ab79eca399ff35f3bc93dc0bc6", "author": {"user": {"login": "snvijaya", "name": "Sneha Vijayarajan"}}, "url": "https://github.com/apache/hadoop/commit/48bee6d5eeaf77ab79eca399ff35f3bc93dc0bc6", "committedDate": "2020-10-05T16:26:23Z", "message": "updates"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3acfc685303b89d01eb543cde061660f5a72d7eb", "author": {"user": {"login": "snvijaya", "name": "Sneha Vijayarajan"}}, "url": "https://github.com/apache/hadoop/commit/3acfc685303b89d01eb543cde061660f5a72d7eb", "committedDate": "2020-10-05T17:08:53Z", "message": "updates"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "db9b7cf5beedb74af210a6cdeaedb4c2394e6398", "author": {"user": {"login": "snvijaya", "name": "Sneha Vijayarajan"}}, "url": "https://github.com/apache/hadoop/commit/db9b7cf5beedb74af210a6cdeaedb4c2394e6398", "committedDate": "2020-10-05T17:12:05Z", "message": "minor fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "23342cc59da120d76a8bf98a47808641634c32d2", "author": {"user": {"login": "snvijaya", "name": "Sneha Vijayarajan"}}, "url": "https://github.com/apache/hadoop/commit/23342cc59da120d76a8bf98a47808641634c32d2", "committedDate": "2020-10-05T18:02:48Z", "message": "test updates"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ce5638709e618d617914f64016a1962ac6f296c2", "author": {"user": {"login": "snvijaya", "name": "Sneha Vijayarajan"}}, "url": "https://github.com/apache/hadoop/commit/ce5638709e618d617914f64016a1962ac6f296c2", "committedDate": "2020-10-05T18:52:37Z", "message": "review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b5a68a1c463cb22e70776fc9950a3e6b6292365b", "author": {"user": {"login": "snvijaya", "name": "Sneha Vijayarajan"}}, "url": "https://github.com/apache/hadoop/commit/b5a68a1c463cb22e70776fc9950a3e6b6292365b", "committedDate": "2020-10-05T20:29:27Z", "message": "fix config comment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "713bb0229724034ff37d8ab9c00895bba97d2790", "author": {"user": {"login": "snvijaya", "name": "Sneha Vijayarajan"}}, "url": "https://github.com/apache/hadoop/commit/713bb0229724034ff37d8ab9c00895bba97d2790", "committedDate": "2020-10-07T15:45:34Z", "message": "test updates"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dc45a14910f5ef091b7caec4f7edfb4daca02c70", "author": {"user": {"login": "snvijaya", "name": "Sneha Vijayarajan"}}, "url": "https://github.com/apache/hadoop/commit/dc45a14910f5ef091b7caec4f7edfb4daca02c70", "committedDate": "2020-10-07T16:01:17Z", "message": "minor updates"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "54e07871896feb88fa94f0ef063323fe3dfb1fed", "author": {"user": {"login": "snvijaya", "name": "Sneha Vijayarajan"}}, "url": "https://github.com/apache/hadoop/commit/54e07871896feb88fa94f0ef063323fe3dfb1fed", "committedDate": "2020-10-07T16:46:29Z", "message": "removing redundant updates"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a1d95d6a31e052b905191681eb356d8915a1a4ea", "author": {"user": {"login": "snvijaya", "name": "Sneha Vijayarajan"}}, "url": "https://github.com/apache/hadoop/commit/a1d95d6a31e052b905191681eb356d8915a1a4ea", "committedDate": "2020-10-07T21:21:50Z", "message": "checkstyle issues and test fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "df9359dc9e26a299479b168013949320298596b8", "author": {"user": {"login": "snvijaya", "name": "Sneha Vijayarajan"}}, "url": "https://github.com/apache/hadoop/commit/df9359dc9e26a299479b168013949320298596b8", "committedDate": "2020-10-08T10:58:39Z", "message": "Findbugs fixes"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA0ODUwMzEx", "url": "https://github.com/apache/hadoop/pull/2368#pullrequestreview-504850311", "createdAt": "2020-10-08T14:34:24Z", "commit": {"oid": "df9359dc9e26a299479b168013949320298596b8"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA0ODIyMzM0", "url": "https://github.com/apache/hadoop/pull/2368#pullrequestreview-504822334", "createdAt": "2020-10-08T14:07:49Z", "commit": {"oid": "df9359dc9e26a299479b168013949320298596b8"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxNDowNzo1MFrOHegZkg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxNTowNjo0MVrOHejOiA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTc1MDE2Mg==", "bodyText": "Can this LOG/validation be moved to AbfsInputStreamContext.build() ?", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r501750162", "createdAt": "2020-10-08T14:07:50Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -89,9 +91,24 @@ public AbfsInputStream(\n     this.tolerateOobAppends = abfsInputStreamContext.isTolerateOobAppends();\n     this.eTag = eTag;\n     this.readAheadEnabled = true;\n+    this.alwaysReadBufferSize\n+        = abfsInputStreamContext.shouldReadBufferSizeAlways();\n     this.cachedSasToken = new CachedSASToken(\n         abfsInputStreamContext.getSasTokenRenewPeriodForStreamsInSeconds());\n     this.streamStatistics = abfsInputStreamContext.getStreamStatistics();\n+    readAheadBlockSize = abfsInputStreamContext.getReadAheadBlockSize();\n+    if (this.bufferSize > readAheadBlockSize) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "df9359dc9e26a299479b168013949320298596b8"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTc1MDc5Mw==", "bodyText": "nit: typo? initialize it get can set", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r501750793", "createdAt": "2020-10-08T14:08:38Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -89,9 +91,24 @@ public AbfsInputStream(\n     this.tolerateOobAppends = abfsInputStreamContext.isTolerateOobAppends();\n     this.eTag = eTag;\n     this.readAheadEnabled = true;\n+    this.alwaysReadBufferSize\n+        = abfsInputStreamContext.shouldReadBufferSizeAlways();\n     this.cachedSasToken = new CachedSASToken(\n         abfsInputStreamContext.getSasTokenRenewPeriodForStreamsInSeconds());\n     this.streamStatistics = abfsInputStreamContext.getStreamStatistics();\n+    readAheadBlockSize = abfsInputStreamContext.getReadAheadBlockSize();\n+    if (this.bufferSize > readAheadBlockSize) {\n+      LOG.debug(\n+          \"fs.azure.read.request.size[={}] is configured for higher size than \"\n+              + \"fs.azure.read.readahead.blocksize[={}]. Auto-align \"\n+              + \"readAhead block size to be same as readRequestSize.\",\n+          bufferSize, readAheadBlockSize);\n+      readAheadBlockSize = this.bufferSize;\n+    }\n+\n+    // Propagate the config values to ReadBufferManager so that the first instance\n+    // to initialize it get can set the readAheadBlockSize", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "df9359dc9e26a299479b168013949320298596b8"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTc1Njg4NQ==", "bodyText": "I think putting these config together with DEFAULT_READ_BUFFER_SIZE would make code more readable. Also use 4 * ONE_MB as used above.", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r501756885", "createdAt": "2020-10-08T14:16:31Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java", "diffHunk": "@@ -74,6 +74,9 @@\n   public static final String DEFAULT_FS_AZURE_APPEND_BLOB_DIRECTORIES = \"\";\n \n   public static final int DEFAULT_READ_AHEAD_QUEUE_DEPTH = -1;\n+  public static final boolean DEFAULT_ALWAYS_READ_BUFFER_SIZE = false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "df9359dc9e26a299479b168013949320298596b8"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTc2MTY2Mw==", "bodyText": "JIRA and PR description says we are trying to read till bufferSize always rather than just the requested length but as per this line we are enabling the buffer manager readahead as well which is bypassed in random read in gen2 as per line 205 below. PS: I have never seen gen1 code though.", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r501761663", "createdAt": "2020-10-08T14:22:40Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -178,11 +195,15 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n         buffer = new byte[bufferSize];\n       }\n \n-      // Enable readAhead when reading sequentially\n-      if (-1 == fCursorAfterLastRead || fCursorAfterLastRead == fCursor || b.length >= bufferSize) {\n+      if (alwaysReadBufferSize) {\n         bytesRead = readInternal(fCursor, buffer, 0, bufferSize, false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "df9359dc9e26a299479b168013949320298596b8"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTc2MjY5Mw==", "bodyText": "Would like to understand the reasoning behind this. Thanks.", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r501762693", "createdAt": "2020-10-08T14:23:55Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -223,16 +244,19 @@ private int readInternal(final long position, final byte[] b, final int offset,\n \n       // queue read-aheads\n       int numReadAheads = this.readAheadQueueDepth;\n-      long nextSize;\n       long nextOffset = position;\n+      // First read to queue needs to be of readBufferSize and later", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "df9359dc9e26a299479b168013949320298596b8"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTc2Mzc5Nw==", "bodyText": "nit: use 4 * ONE_MB consistent as everywhere else.", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r501763797", "createdAt": "2020-10-08T14:25:13Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -37,10 +39,10 @@\n   private static final Logger LOGGER = LoggerFactory.getLogger(ReadBufferManager.class);\n \n   private static final int NUM_BUFFERS = 16;\n-  private static final int BLOCK_SIZE = 4 * 1024 * 1024;\n   private static final int NUM_THREADS = 8;\n   private static final int DEFAULT_THRESHOLD_AGE_MILLISECONDS = 3000; // have to see if 3 seconds is a good threshold\n \n+  private static int blockSize = 4 * 1024 * 1024;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "df9359dc9e26a299479b168013949320298596b8"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTc5NTQ4OA==", "bodyText": "Why all these changes ? Why not just initilize the blockSize in init() ?", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r501795488", "createdAt": "2020-10-08T15:05:14Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -49,21 +51,37 @@\n   private Queue<ReadBuffer> readAheadQueue = new LinkedList<>(); // queue of requests that are not picked up by any worker thread yet\n   private LinkedList<ReadBuffer> inProgressList = new LinkedList<>(); // requests being processed by worker threads\n   private LinkedList<ReadBuffer> completedReadList = new LinkedList<>(); // buffers available for reading\n-  private static final ReadBufferManager BUFFER_MANAGER; // singleton, initialized in static initialization block\n+  private static ReadBufferManager bufferManager; // singleton, initialized in static initialization block\n+  private static final ReentrantLock LOCK = new ReentrantLock();\n \n-  static {\n-    BUFFER_MANAGER = new ReadBufferManager();\n-    BUFFER_MANAGER.init();\n+  static ReadBufferManager getBufferManager() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "df9359dc9e26a299479b168013949320298596b8"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTc5NjQ4OA==", "bodyText": "please add some reasoning/docs around these changes. Thanks.", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r501796488", "createdAt": "2020-10-08T15:06:41Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -464,4 +483,53 @@ int getCompletedReadListSize() {\n   void callTryEvict() {\n     tryEvict();\n   }\n+\n+  @VisibleForTesting\n+  void testResetReadBufferManager() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "df9359dc9e26a299479b168013949320298596b8"}, "originalPosition": 100}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA1MzcwMjgz", "url": "https://github.com/apache/hadoop/pull/2368#pullrequestreview-505370283", "createdAt": "2020-10-09T05:22:51Z", "commit": {"oid": "df9359dc9e26a299479b168013949320298596b8"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d8664c3fd203a0d72688de9ca93e765c5c096c67", "author": {"user": {"login": "snvijaya", "name": "Sneha Vijayarajan"}}, "url": "https://github.com/apache/hadoop/commit/d8664c3fd203a0d72688de9ca93e765c5c096c67", "committedDate": "2020-10-13T05:16:34Z", "message": "Incorporate review comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA3NTAxMzc4", "url": "https://github.com/apache/hadoop/pull/2368#pullrequestreview-507501378", "createdAt": "2020-10-13T14:32:03Z", "commit": {"oid": "d8664c3fd203a0d72688de9ca93e765c5c096c67"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QxNDozMjowM1rOHgp1XA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QxNDo1MzowOFrOHgq3Ag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDAwMTg4NA==", "bodyText": "AssertJ has rich api's to tackle these kind of assertions. Try that\nexample : Assertions.assertThat(list)\n.hasSameElementsAs(list2)", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r504001884", "createdAt": "2020-10-13T14:32:03Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "diffHunk": "@@ -447,4 +490,168 @@ public void testReadAheadManagerForSuccessfulReadAhead() throws Exception {\n     checkEvictedStatus(inputStream, 0, true);\n   }\n \n+  /**\n+   * Test readahead with different config settings for request request size and\n+   * readAhead block size\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testDiffReadRequestSizeAndRAHBlockSize() throws Exception {\n+    // Set requestRequestSize = 4MB and readAheadBufferSize=8MB\n+    ReadBufferManager.getBufferManager()\n+        .testResetReadBufferManager(FOUR_MB, INCREASED_READ_BUFFER_AGE_THRESHOLD);\n+    testReadAheadConfigs(FOUR_MB, TEST_READAHEAD_DEPTH_4, false, EIGHT_MB);\n+\n+    // Test for requestRequestSize =16KB and readAheadBufferSize=16KB\n+    ReadBufferManager.getBufferManager()\n+        .testResetReadBufferManager(SIXTEEN_KB, INCREASED_READ_BUFFER_AGE_THRESHOLD);\n+    AbfsInputStream inputStream = testReadAheadConfigs(SIXTEEN_KB,\n+        TEST_READAHEAD_DEPTH_2, true, SIXTEEN_KB);\n+    testReadAheads(inputStream, SIXTEEN_KB, SIXTEEN_KB);\n+\n+    // Test for requestRequestSize =16KB and readAheadBufferSize=48KB\n+    ReadBufferManager.getBufferManager()\n+        .testResetReadBufferManager(FORTY_EIGHT_KB, INCREASED_READ_BUFFER_AGE_THRESHOLD);\n+    inputStream = testReadAheadConfigs(SIXTEEN_KB, TEST_READAHEAD_DEPTH_2, true,\n+        FORTY_EIGHT_KB);\n+    testReadAheads(inputStream, SIXTEEN_KB, FORTY_EIGHT_KB);\n+\n+    // Test for requestRequestSize =48KB and readAheadBufferSize=16KB\n+    ReadBufferManager.getBufferManager()\n+        .testResetReadBufferManager(FORTY_EIGHT_KB, INCREASED_READ_BUFFER_AGE_THRESHOLD);\n+    inputStream = testReadAheadConfigs(FORTY_EIGHT_KB, TEST_READAHEAD_DEPTH_2,\n+        true,\n+        SIXTEEN_KB);\n+    testReadAheads(inputStream, FORTY_EIGHT_KB, SIXTEEN_KB);\n+  }\n+\n+\n+  private void testReadAheads(AbfsInputStream inputStream,\n+      int readRequestSize,\n+      int readAheadRequestSize)\n+      throws Exception {\n+    if (readRequestSize > readAheadRequestSize) {\n+      readAheadRequestSize = readRequestSize;\n+    }\n+\n+    byte[] firstReadBuffer = new byte[readRequestSize];\n+    byte[] secondReadBuffer = new byte[readAheadRequestSize];\n+\n+    // get the expected bytes to compare\n+    byte[] expectedFirstReadAheadBufferContents = new byte[readRequestSize];\n+    byte[] expectedSecondReadAheadBufferContents = new byte[readAheadRequestSize];\n+    getExpectedBufferData(0, readRequestSize, expectedFirstReadAheadBufferContents);\n+    getExpectedBufferData(readRequestSize, readAheadRequestSize,\n+        expectedSecondReadAheadBufferContents);\n+\n+    assertTrue(\"Read should be of exact requested size\",\n+              inputStream.read(firstReadBuffer, 0, readRequestSize) == readRequestSize);\n+    assertTrue(\"Data mismatch found in RAH1\",\n+        Arrays.equals(firstReadBuffer,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d8664c3fd203a0d72688de9ca93e765c5c096c67"}, "originalPosition": 137}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDAwMjMzNA==", "bodyText": "Better to use assert equals here inspite of assertTrue no?", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r504002334", "createdAt": "2020-10-13T14:32:40Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "diffHunk": "@@ -447,4 +490,168 @@ public void testReadAheadManagerForSuccessfulReadAhead() throws Exception {\n     checkEvictedStatus(inputStream, 0, true);\n   }\n \n+  /**\n+   * Test readahead with different config settings for request request size and\n+   * readAhead block size\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testDiffReadRequestSizeAndRAHBlockSize() throws Exception {\n+    // Set requestRequestSize = 4MB and readAheadBufferSize=8MB\n+    ReadBufferManager.getBufferManager()\n+        .testResetReadBufferManager(FOUR_MB, INCREASED_READ_BUFFER_AGE_THRESHOLD);\n+    testReadAheadConfigs(FOUR_MB, TEST_READAHEAD_DEPTH_4, false, EIGHT_MB);\n+\n+    // Test for requestRequestSize =16KB and readAheadBufferSize=16KB\n+    ReadBufferManager.getBufferManager()\n+        .testResetReadBufferManager(SIXTEEN_KB, INCREASED_READ_BUFFER_AGE_THRESHOLD);\n+    AbfsInputStream inputStream = testReadAheadConfigs(SIXTEEN_KB,\n+        TEST_READAHEAD_DEPTH_2, true, SIXTEEN_KB);\n+    testReadAheads(inputStream, SIXTEEN_KB, SIXTEEN_KB);\n+\n+    // Test for requestRequestSize =16KB and readAheadBufferSize=48KB\n+    ReadBufferManager.getBufferManager()\n+        .testResetReadBufferManager(FORTY_EIGHT_KB, INCREASED_READ_BUFFER_AGE_THRESHOLD);\n+    inputStream = testReadAheadConfigs(SIXTEEN_KB, TEST_READAHEAD_DEPTH_2, true,\n+        FORTY_EIGHT_KB);\n+    testReadAheads(inputStream, SIXTEEN_KB, FORTY_EIGHT_KB);\n+\n+    // Test for requestRequestSize =48KB and readAheadBufferSize=16KB\n+    ReadBufferManager.getBufferManager()\n+        .testResetReadBufferManager(FORTY_EIGHT_KB, INCREASED_READ_BUFFER_AGE_THRESHOLD);\n+    inputStream = testReadAheadConfigs(FORTY_EIGHT_KB, TEST_READAHEAD_DEPTH_2,\n+        true,\n+        SIXTEEN_KB);\n+    testReadAheads(inputStream, FORTY_EIGHT_KB, SIXTEEN_KB);\n+  }\n+\n+\n+  private void testReadAheads(AbfsInputStream inputStream,\n+      int readRequestSize,\n+      int readAheadRequestSize)\n+      throws Exception {\n+    if (readRequestSize > readAheadRequestSize) {\n+      readAheadRequestSize = readRequestSize;\n+    }\n+\n+    byte[] firstReadBuffer = new byte[readRequestSize];\n+    byte[] secondReadBuffer = new byte[readAheadRequestSize];\n+\n+    // get the expected bytes to compare\n+    byte[] expectedFirstReadAheadBufferContents = new byte[readRequestSize];\n+    byte[] expectedSecondReadAheadBufferContents = new byte[readAheadRequestSize];\n+    getExpectedBufferData(0, readRequestSize, expectedFirstReadAheadBufferContents);\n+    getExpectedBufferData(readRequestSize, readAheadRequestSize,\n+        expectedSecondReadAheadBufferContents);\n+\n+    assertTrue(\"Read should be of exact requested size\",\n+              inputStream.read(firstReadBuffer, 0, readRequestSize) == readRequestSize);\n+    assertTrue(\"Data mismatch found in RAH1\",\n+        Arrays.equals(firstReadBuffer,\n+            expectedFirstReadAheadBufferContents));\n+\n+\n+    assertTrue(\"Read should be of exact requested size\",\n+        inputStream.read(secondReadBuffer, 0, readAheadRequestSize) == readAheadRequestSize);\n+    assertTrue(\"Data mismatch found in RAH2\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d8664c3fd203a0d72688de9ca93e765c5c096c67"}, "originalPosition": 143}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDAwOTIxOA==", "bodyText": "See if you can reuse the data generation and new file creation code from ContractTestUtils.dataset() and ContractTestUtils.createFile.", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r504009218", "createdAt": "2020-10-13T14:41:24Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "diffHunk": "@@ -447,4 +490,168 @@ public void testReadAheadManagerForSuccessfulReadAhead() throws Exception {\n     checkEvictedStatus(inputStream, 0, true);\n   }\n \n+  /**\n+   * Test readahead with different config settings for request request size and\n+   * readAhead block size\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testDiffReadRequestSizeAndRAHBlockSize() throws Exception {\n+    // Set requestRequestSize = 4MB and readAheadBufferSize=8MB\n+    ReadBufferManager.getBufferManager()\n+        .testResetReadBufferManager(FOUR_MB, INCREASED_READ_BUFFER_AGE_THRESHOLD);\n+    testReadAheadConfigs(FOUR_MB, TEST_READAHEAD_DEPTH_4, false, EIGHT_MB);\n+\n+    // Test for requestRequestSize =16KB and readAheadBufferSize=16KB\n+    ReadBufferManager.getBufferManager()\n+        .testResetReadBufferManager(SIXTEEN_KB, INCREASED_READ_BUFFER_AGE_THRESHOLD);\n+    AbfsInputStream inputStream = testReadAheadConfigs(SIXTEEN_KB,\n+        TEST_READAHEAD_DEPTH_2, true, SIXTEEN_KB);\n+    testReadAheads(inputStream, SIXTEEN_KB, SIXTEEN_KB);\n+\n+    // Test for requestRequestSize =16KB and readAheadBufferSize=48KB\n+    ReadBufferManager.getBufferManager()\n+        .testResetReadBufferManager(FORTY_EIGHT_KB, INCREASED_READ_BUFFER_AGE_THRESHOLD);\n+    inputStream = testReadAheadConfigs(SIXTEEN_KB, TEST_READAHEAD_DEPTH_2, true,\n+        FORTY_EIGHT_KB);\n+    testReadAheads(inputStream, SIXTEEN_KB, FORTY_EIGHT_KB);\n+\n+    // Test for requestRequestSize =48KB and readAheadBufferSize=16KB\n+    ReadBufferManager.getBufferManager()\n+        .testResetReadBufferManager(FORTY_EIGHT_KB, INCREASED_READ_BUFFER_AGE_THRESHOLD);\n+    inputStream = testReadAheadConfigs(FORTY_EIGHT_KB, TEST_READAHEAD_DEPTH_2,\n+        true,\n+        SIXTEEN_KB);\n+    testReadAheads(inputStream, FORTY_EIGHT_KB, SIXTEEN_KB);\n+  }\n+\n+\n+  private void testReadAheads(AbfsInputStream inputStream,\n+      int readRequestSize,\n+      int readAheadRequestSize)\n+      throws Exception {\n+    if (readRequestSize > readAheadRequestSize) {\n+      readAheadRequestSize = readRequestSize;\n+    }\n+\n+    byte[] firstReadBuffer = new byte[readRequestSize];\n+    byte[] secondReadBuffer = new byte[readAheadRequestSize];\n+\n+    // get the expected bytes to compare\n+    byte[] expectedFirstReadAheadBufferContents = new byte[readRequestSize];\n+    byte[] expectedSecondReadAheadBufferContents = new byte[readAheadRequestSize];\n+    getExpectedBufferData(0, readRequestSize, expectedFirstReadAheadBufferContents);\n+    getExpectedBufferData(readRequestSize, readAheadRequestSize,\n+        expectedSecondReadAheadBufferContents);\n+\n+    assertTrue(\"Read should be of exact requested size\",\n+              inputStream.read(firstReadBuffer, 0, readRequestSize) == readRequestSize);\n+    assertTrue(\"Data mismatch found in RAH1\",\n+        Arrays.equals(firstReadBuffer,\n+            expectedFirstReadAheadBufferContents));\n+\n+\n+    assertTrue(\"Read should be of exact requested size\",\n+        inputStream.read(secondReadBuffer, 0, readAheadRequestSize) == readAheadRequestSize);\n+    assertTrue(\"Data mismatch found in RAH2\",\n+        Arrays.equals(secondReadBuffer,\n+            expectedSecondReadAheadBufferContents));\n+  }\n+\n+  public AbfsInputStream testReadAheadConfigs(int readRequestSize,\n+      int readAheadQueueDepth,\n+      boolean alwaysReadBufferSizeEnabled,\n+      int readAheadBlockSize) throws Exception {\n+    Configuration\n+        config = new Configuration(\n+        this.getRawConfiguration());\n+    config.set(\"fs.azure.read.request.size\", Integer.toString(readRequestSize));\n+    config.set(\"fs.azure.readaheadqueue.depth\",\n+        Integer.toString(readAheadQueueDepth));\n+    config.set(\"fs.azure.read.alwaysReadBufferSize\",\n+        Boolean.toString(alwaysReadBufferSizeEnabled));\n+    config.set(\"fs.azure.read.readahead.blocksize\",\n+        Integer.toString(readAheadBlockSize));\n+    if (readRequestSize > readAheadBlockSize) {\n+      readAheadBlockSize = readRequestSize;\n+    }\n+\n+    Path testPath = new Path(\n+        \"/testReadAheadConfigs\");\n+    final AzureBlobFileSystem fs = createTestFile(testPath,\n+        ALWAYS_READ_BUFFER_SIZE_TEST_FILE_SIZE, config);\n+    byte[] byteBuffer = new byte[ONE_MB];\n+    AbfsInputStream inputStream = this.getAbfsStore(fs)\n+        .openFileForRead(testPath, null);\n+\n+    assertEquals(\"Unexpected AbfsInputStream buffer size\", readRequestSize,\n+        inputStream.getBufferSize());\n+    assertEquals(\"Unexpected ReadAhead queue depth\", readAheadQueueDepth,\n+        inputStream.getReadAheadQueueDepth());\n+    assertEquals(\"Unexpected AlwaysReadBufferSize settings\",\n+        alwaysReadBufferSizeEnabled,\n+        inputStream.shouldAlwaysReadBufferSize());\n+    assertEquals(\"Unexpected readAhead block size\", readAheadBlockSize,\n+        ReadBufferManager.getBufferManager().getReadAheadBlockSize());\n+\n+    return inputStream;\n+  }\n+\n+  private void getExpectedBufferData(int offset, int length, byte[] b) {\n+    boolean startFillingIn = false;\n+    int indexIntoBuffer = 0;\n+    char character = 'a';\n+\n+    for (int i = 0; i < (offset + length); i++) {\n+      if (i == offset) {\n+        startFillingIn = true;\n+      }\n+\n+      if ((startFillingIn) && (indexIntoBuffer < length)) {\n+        b[indexIntoBuffer] = (byte) character;\n+        indexIntoBuffer++;\n+      }\n+\n+      character = (character == 'z') ? 'a' : (char) ((int) character + 1);\n+    }\n+  }\n+\n+  private AzureBlobFileSystem createTestFile(Path testFilePath, long testFileSize,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d8664c3fd203a0d72688de9ca93e765c5c096c67"}, "originalPosition": 206}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDAxODY5MA==", "bodyText": "Why creating a new method here if we are just doing a passthrough?", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r504018690", "createdAt": "2020-10-13T14:53:08Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemRandomRead.java", "diffHunk": "@@ -448,15 +477,119 @@ public void testRandomReadPerformance() throws Exception {\n             ratio < maxAcceptableRatio);\n   }\n \n+  /**\n+   * With this test we should see a full buffer read being triggered in case\n+   * alwaysReadBufferSize is on, else only the requested buffer size.\n+   * Hence a seek done few bytes away from last read position will trigger\n+   * a network read when alwaysReadBufferSize is off, whereas it will return\n+   * from the internal buffer when it is on.\n+   * Reading a full buffer size is the Gen1 behaviour.\n+   * @throws Throwable\n+   */\n+  @Test\n+  public void testAlwaysReadBufferSizeConfig() throws Throwable {\n+    testAlwaysReadBufferSizeConfig(false);\n+    testAlwaysReadBufferSizeConfig(true);\n+  }\n+\n+  private void assertStatistics(AzureBlobFileSystem fs,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d8664c3fd203a0d72688de9ca93e765c5c096c67"}, "originalPosition": 221}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4eeeca67556fcaf8234b1d85c1b3e5d3b6054d32", "author": {"user": {"login": "snvijaya", "name": "Sneha Vijayarajan"}}, "url": "https://github.com/apache/hadoop/commit/4eeeca67556fcaf8234b1d85c1b3e5d3b6054d32", "committedDate": "2020-10-14T11:55:25Z", "message": "Review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ac9c464bd28be94d90575973f5edfb5a9c253d46", "author": {"user": {"login": "snvijaya", "name": "Sneha Vijayarajan"}}, "url": "https://github.com/apache/hadoop/commit/ac9c464bd28be94d90575973f5edfb5a9c253d46", "committedDate": "2020-10-14T12:20:35Z", "message": "Merge from trunk"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dbb42c04b70bbd87d7aee5059da3ae582d899407", "author": {"user": {"login": "snvijaya", "name": "Sneha Vijayarajan"}}, "url": "https://github.com/apache/hadoop/commit/dbb42c04b70bbd87d7aee5059da3ae582d899407", "committedDate": "2020-10-14T13:39:49Z", "message": "Remove redundant test method"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7a2ec7416f1bb27f3f93f3bdcdb82a27bd3b8636", "author": {"user": {"login": "snvijaya", "name": "Sneha Vijayarajan"}}, "url": "https://github.com/apache/hadoop/commit/7a2ec7416f1bb27f3f93f3bdcdb82a27bd3b8636", "committedDate": "2020-10-22T10:11:49Z", "message": "Add documentation for the new configs added"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ef2539bbff2578558180a571f82b8eb9b8eedd02", "author": {"user": {"login": "snvijaya", "name": "Sneha Vijayarajan"}}, "url": "https://github.com/apache/hadoop/commit/ef2539bbff2578558180a571f82b8eb9b8eedd02", "committedDate": "2020-10-22T15:20:37Z", "message": "Merge branch 'trunk' into HADOOP-17296"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c4cb3b23d83a34bbe6aa8ededdfefb4f31f9096a", "author": {"user": {"login": "snvijaya", "name": "Sneha Vijayarajan"}}, "url": "https://github.com/apache/hadoop/commit/c4cb3b23d83a34bbe6aa8ededdfefb4f31f9096a", "committedDate": "2020-10-29T12:08:11Z", "message": "Fix test generated OOM due to multiple consecutive buffer re-allocations"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6fc3914fd950322db66fe2b0dc710678699839b1", "author": {"user": {"login": "snvijaya", "name": "Sneha Vijayarajan"}}, "url": "https://github.com/apache/hadoop/commit/6fc3914fd950322db66fe2b0dc710678699839b1", "committedDate": "2020-11-25T07:16:00Z", "message": "Merge branch 'trunk' into HADOOP-17296"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM5MDA2NjAw", "url": "https://github.com/apache/hadoop/pull/2368#pullrequestreview-539006600", "createdAt": "2020-11-26T05:38:41Z", "commit": {"oid": "6fc3914fd950322db66fe2b0dc710678699839b1"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQwMDEyNzc5", "url": "https://github.com/apache/hadoop/pull/2368#pullrequestreview-540012779", "createdAt": "2020-11-27T14:19:23Z", "commit": {"oid": "6fc3914fd950322db66fe2b0dc710678699839b1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yN1QxNDoxOToyM1rOH7AABw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yN1QxNDoxOToyM1rOH7AABw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTYyODAzOQ==", "bodyText": "FWIW I use a Junit rule to get the method name, then you can hava a path() method which dynamically creates the unique path, including when you use parameterized tests.", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r531628039", "createdAt": "2020-11-27T14:19:23Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemRandomRead.java", "diffHunk": "@@ -99,12 +115,14 @@ public void testBasicRead() throws Exception {\n   public void testRandomRead() throws Exception {\n     Assume.assumeFalse(\"This test does not support namespace enabled account\",\n             this.getFileSystem().getIsNamespaceEnabled());\n-    assumeHugeFileExists();\n+    Path testPath = new Path(TEST_FILE_PREFIX + \"_testRandomRead\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6fc3914fd950322db66fe2b0dc710678699839b1"}, "originalPosition": 82}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQwMDEzOTU0", "url": "https://github.com/apache/hadoop/pull/2368#pullrequestreview-540013954", "createdAt": "2020-11-27T14:21:09Z", "commit": {"oid": "6fc3914fd950322db66fe2b0dc710678699839b1"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3433, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}