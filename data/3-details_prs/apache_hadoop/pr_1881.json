{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzg0NjgzMTc3", "number": 1881, "title": "HADOOP-16910 Adding file system counters in ABFS", "bodyText": "Write_ops\nRead_ops\nBytes_written (already updated)\nBytes_Read (already updated)\n\nChange-Id: I77349fdd158babd66df665713201fa9c8606f191\nNOTICE\nPlease create an issue in ASF JIRA before opening a pull request,\nand you need to set the title of the pull request which starts with\nthe corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.)\nFor more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute", "createdAt": "2020-03-06T07:54:36Z", "url": "https://github.com/apache/hadoop/pull/1881", "merged": true, "mergeCommit": {"oid": "e2c7ac71b5ee47bb40294acd10c0c21dd6ee430f"}, "closed": true, "closedAt": "2020-03-23T13:50:19Z", "author": {"login": "mehakmeet"}, "timelineItems": {"totalCount": 33, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcK-rkvAFqTM3MDI1OTQ1NQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcQcxZZAH2gAyMzg0NjgzMTc3OmNmNGI3ZDQ1OWM5ZGYzZmIwNGFlZjU2MjQ0OTZkNTNkZWI1MzNkM2Q=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcwMjU5NDU1", "url": "https://github.com/apache/hadoop/pull/1881#pullrequestreview-370259455", "createdAt": "2020-03-06T11:34:24Z", "commit": {"oid": "bba6f319bc157593ac9b89c37d9b874c6c3b0f3f"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQxMTozNDoyNVrOFy1z1g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQxMTo0Njo1NFrOFy2HAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODg1NDc0Mg==", "bodyText": "nit: topmost import block", "url": "https://github.com/apache/hadoop/pull/1881#discussion_r388854742", "createdAt": "2020-03-06T11:34:25Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/AbstractAbfsTestWithTimeout.java", "diffHunk": "@@ -17,12 +17,19 @@\n  */\n package org.apache.hadoop.fs.azurebfs;\n \n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.Path;\n import org.junit.Assert;\n import org.junit.Before;\n import org.junit.BeforeClass;\n import org.junit.Rule;\n import org.junit.rules.TestName;\n import org.junit.rules.Timeout;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bba6f319bc157593ac9b89c37d9b874c6c3b0f3f"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODg1NDg3Ng==", "bodyText": "nit: should be in its own block after the non org-apache-hadoop imports", "url": "https://github.com/apache/hadoop/pull/1881#discussion_r388854876", "createdAt": "2020-03-06T11:34:44Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/AbstractAbfsTestWithTimeout.java", "diffHunk": "@@ -17,12 +17,19 @@\n  */\n package org.apache.hadoop.fs.azurebfs;\n \n+import org.apache.hadoop.fs.FSDataInputStream;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bba6f319bc157593ac9b89c37d9b874c6c3b0f3f"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODg1NTAzMQ==", "bodyText": "nit: add a trailing . to keep javadoc happy", "url": "https://github.com/apache/hadoop/pull/1881#discussion_r388855031", "createdAt": "2020-03-06T11:35:09Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/AbstractAbfsTestWithTimeout.java", "diffHunk": "@@ -67,4 +77,42 @@ public void nameThread() {\n   protected int getTestTimeoutMillis() {\n     return TEST_TIMEOUT;\n   }\n+\n+  /**\n+   * Describe a test in the logs.\n+   *\n+   * @param text text to print\n+   * @param args arguments to format in the printing\n+   */\n+  protected void describe(String text, Object... args) {\n+    LOG.info(\"\\n\\n{}: {}\\n\",\n+        methodName.getMethodName(),\n+        String.format(text, args));\n+  }\n+\n+  /**\n+   * Validate Contents written on a file in Abfs", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bba6f319bc157593ac9b89c37d9b874c6c3b0f3f"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODg1NTQ5OA==", "bodyText": "nit: trailing .", "url": "https://github.com/apache/hadoop/pull/1881#discussion_r388855498", "createdAt": "2020-03-06T11:36:14Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -252,6 +255,13 @@ int readRemote(long position, byte[] b, int offset, int length) throws IOExcepti\n     return (int) bytesRead;\n   }\n \n+  /**\n+   * Increment Read Operations", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bba6f319bc157593ac9b89c37d9b874c6c3b0f3f"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODg1NjEzMw==", "bodyText": "read() always moves the stream forward, so you don't need these seek/seekPos++; they can only slow things down", "url": "https://github.com/apache/hadoop/pull/1881#discussion_r388856133", "createdAt": "2020-03-06T11:37:53Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/AbstractAbfsTestWithTimeout.java", "diffHunk": "@@ -67,4 +77,42 @@ public void nameThread() {\n   protected int getTestTimeoutMillis() {\n     return TEST_TIMEOUT;\n   }\n+\n+  /**\n+   * Describe a test in the logs.\n+   *\n+   * @param text text to print\n+   * @param args arguments to format in the printing\n+   */\n+  protected void describe(String text, Object... args) {\n+    LOG.info(\"\\n\\n{}: {}\\n\",\n+        methodName.getMethodName(),\n+        String.format(text, args));\n+  }\n+\n+  /**\n+   * Validate Contents written on a file in Abfs\n+   *\n+   * @param fs AzureBlobFileSystem\n+   * @param path Path of the file\n+   * @param originalByteArray original byte array\n+   * @return\n+   * @throws IOException\n+   */\n+  protected boolean validateContent(AzureBlobFileSystem fs, Path path,\n+      byte[] originalByteArray)\n+      throws IOException {\n+    FSDataInputStream in = fs.open(path);\n+    byte[] contentByteArray = new byte[originalByteArray.length];\n+    int seekPos = 0;\n+    while (in.read() != -1) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bba6f319bc157593ac9b89c37d9b874c6c3b0f3f"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODg1NjYwNA==", "bodyText": "add a test for writes here too", "url": "https://github.com/apache/hadoop/pull/1881#discussion_r388856604", "createdAt": "2020-03-06T11:39:06Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsInputStream.java", "diffHunk": "@@ -0,0 +1,81 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsInputStream;\n+\n+/**\n+ * Test Abfs Input Stream.\n+ */\n+\n+public class ITestAbfsInputStream extends AbstractAbfsIntegrationTest {\n+  public ITestAbfsInputStream() throws Exception {\n+  }\n+\n+  /***\n+   * {@link AbfsInputStream#incrementReadOps()}\n+   *\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testAbfsInputStreamReadOps() throws Exception {\n+    describe(\"Test to see correct population of Read operations in Abfs\");\n+\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path smallFile = new Path(\"testOneReadCall\");\n+    Path largeFile = new Path(\"testLargeReadCalls\");\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+    String testReadOps = \"test this\";\n+    statistics.reset();\n+\n+    //Test for zero read operation\n+    Assert.assertEquals(0, statistics.getReadOps());\n+\n+    FSDataOutputStream outForOneCall = fs.create(smallFile);\n+    statistics.reset();\n+    outForOneCall.write(testReadOps.getBytes());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bba6f319bc157593ac9b89c37d9b874c6c3b0f3f"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODg1NzA3Nw==", "bodyText": "does this really create large reads? it's still only small amounts of data", "url": "https://github.com/apache/hadoop/pull/1881#discussion_r388857077", "createdAt": "2020-03-06T11:40:18Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsInputStream.java", "diffHunk": "@@ -0,0 +1,81 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsInputStream;\n+\n+/**\n+ * Test Abfs Input Stream.\n+ */\n+\n+public class ITestAbfsInputStream extends AbstractAbfsIntegrationTest {\n+  public ITestAbfsInputStream() throws Exception {\n+  }\n+\n+  /***\n+   * {@link AbfsInputStream#incrementReadOps()}\n+   *\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testAbfsInputStreamReadOps() throws Exception {\n+    describe(\"Test to see correct population of Read operations in Abfs\");\n+\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path smallFile = new Path(\"testOneReadCall\");\n+    Path largeFile = new Path(\"testLargeReadCalls\");\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+    String testReadOps = \"test this\";\n+    statistics.reset();\n+\n+    //Test for zero read operation\n+    Assert.assertEquals(0, statistics.getReadOps());\n+\n+    FSDataOutputStream outForOneCall = fs.create(smallFile);\n+    statistics.reset();\n+    outForOneCall.write(testReadOps.getBytes());\n+    FSDataInputStream inForOneCall = fs.open(smallFile);\n+    inForOneCall.read(testReadOps.getBytes(), 0, testReadOps.getBytes().length);\n+\n+    //Test for one read operation\n+    Assert.assertEquals(1, statistics.getReadOps());\n+\n+    FSDataOutputStream outForLargeCalls = fs.create(largeFile);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bba6f319bc157593ac9b89c37d9b874c6c3b0f3f"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODg1ODY3OQ==", "bodyText": "nit, add a space after the comma", "url": "https://github.com/apache/hadoop/pull/1881#discussion_r388858679", "createdAt": "2020-03-06T11:44:20Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemOauth.java", "diffHunk": "@@ -143,7 +143,7 @@ public void testBlobDataReader() throws Exception {\n \n     // TEST WRITE FILE\n     try {\n-      abfsStore.openFileForWrite(EXISTED_FILE_PATH, true);\n+      abfsStore.openFileForWrite(EXISTED_FILE_PATH, fs.getFsStatistics(),true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bba6f319bc157593ac9b89c37d9b874c6c3b0f3f"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODg1ODc2Mg==", "bodyText": "This is a good test.\nIf you call it ITestAbfsStreamStatistics you can test the writes as well as the reads -and you should test those writes already in the test suite.. If you put the write ops test there, into a single test suite, backporting is easier -less conflict.\nFor the assertions, I'd like every assert to have some meaningful string,\nassertEquals(\"read ops\", 1000, statistics.getReadOps)\nImagine \"what would you want in the error text if this failed on a jenkins run\"?", "url": "https://github.com/apache/hadoop/pull/1881#discussion_r388858762", "createdAt": "2020-03-06T11:44:35Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsInputStream.java", "diffHunk": "@@ -0,0 +1,81 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsInputStream;\n+\n+/**\n+ * Test Abfs Input Stream.\n+ */\n+\n+public class ITestAbfsInputStream extends AbstractAbfsIntegrationTest {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bba6f319bc157593ac9b89c37d9b874c6c3b0f3f"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODg1ODk0OA==", "bodyText": "nit: this line is long enough its time to split", "url": "https://github.com/apache/hadoop/pull/1881#discussion_r388858948", "createdAt": "2020-03-06T11:45:05Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "diffHunk": "@@ -392,7 +392,7 @@ public void deleteFilesystem() throws AzureBlobFileSystemException {\n     }\n   }\n \n-  public OutputStream createFile(final Path path, final boolean overwrite, final FsPermission permission,\n+  public OutputStream createFile(final Path path, final FileSystem.Statistics statistics, final boolean overwrite, final FsPermission permission,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bba6f319bc157593ac9b89c37d9b874c6c3b0f3f"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODg1OTY1MQ==", "bodyText": "move into the same tests as the reads for ease of backports", "url": "https://github.com/apache/hadoop/pull/1881#discussion_r388859651", "createdAt": "2020-03-06T11:46:54Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemCreate.java", "diffHunk": "@@ -59,6 +62,59 @@ public void testEnsureFileCreatedImmediately() throws Exception {\n     assertIsFile(fs, TEST_FILE_PATH);\n   }\n \n+  /**\n+   * {@link AbfsOutputStream#incrementWriteOps()}\n+   *\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testWriteOpsMetric() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bba6f319bc157593ac9b89c37d9b874c6c3b0f3f"}, "originalPosition": 20}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcxMTAzNjIz", "url": "https://github.com/apache/hadoop/pull/1881#pullrequestreview-371103623", "createdAt": "2020-03-09T11:56:48Z", "commit": {"oid": "bba6f319bc157593ac9b89c37d9b874c6c3b0f3f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQxMTo1Njo0OFrOFzkMGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQxMTo1Njo0OFrOFzkMGA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTYxNDYxNg==", "bodyText": "All assert calls should have meaningful message as suggested by Steve. This is important for debugging test failures easily. You can check the examples in PR I sent you.\nSorry I missed this in internal review. :)", "url": "https://github.com/apache/hadoop/pull/1881#discussion_r389614616", "createdAt": "2020-03-09T11:56:48Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemCreate.java", "diffHunk": "@@ -59,6 +62,59 @@ public void testEnsureFileCreatedImmediately() throws Exception {\n     assertIsFile(fs, TEST_FILE_PATH);\n   }\n \n+  /**\n+   * {@link AbfsOutputStream#incrementWriteOps()}\n+   *\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testWriteOpsMetric() throws Exception {\n+    describe(\"Test to see correct population of write operations in Abfs\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path smallFile = new Path(\"testOneCall\");\n+    Path largeFile = new Path(\"testLargeCalls\");\n+    String testWriteOps = \"test\";\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+    statistics.reset();\n+\n+    //Test for zero write operation\n+    Assert.assertEquals(0, statistics.getWriteOps());\n+\n+    FSDataOutputStream outForOneCall = fs.create(smallFile);\n+    statistics.reset();\n+    //Writing \"test\" 1 time\n+    outForOneCall\n+        .write(testWriteOps.getBytes(), 0, testWriteOps.getBytes().length);\n+\n+    //Test for one write operation\n+    Assert.assertEquals(1, statistics.getWriteOps());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bba6f319bc157593ac9b89c37d9b874c6c3b0f3f"}, "originalPosition": 39}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcxMTA3MTQy", "url": "https://github.com/apache/hadoop/pull/1881#pullrequestreview-371107142", "createdAt": "2020-03-09T12:03:06Z", "commit": {"oid": "bba6f319bc157593ac9b89c37d9b874c6c3b0f3f"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyMTc4ODA4", "url": "https://github.com/apache/hadoop/pull/1881#pullrequestreview-372178808", "createdAt": "2020-03-10T17:39:52Z", "commit": {"oid": "bba6f319bc157593ac9b89c37d9b874c6c3b0f3f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQxNzozOTo1MlrOF0Z28A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQxNzozOTo1MlrOF0Z28A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDQ5MzkzNg==", "bodyText": "Will be using StringBuilder here and use append() istead of += for String concatenation.", "url": "https://github.com/apache/hadoop/pull/1881#discussion_r390493936", "createdAt": "2020-03-10T17:39:52Z", "author": {"login": "mehakmeet"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemCreate.java", "diffHunk": "@@ -59,6 +62,59 @@ public void testEnsureFileCreatedImmediately() throws Exception {\n     assertIsFile(fs, TEST_FILE_PATH);\n   }\n \n+  /**\n+   * {@link AbfsOutputStream#incrementWriteOps()}\n+   *\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testWriteOpsMetric() throws Exception {\n+    describe(\"Test to see correct population of write operations in Abfs\");\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path smallFile = new Path(\"testOneCall\");\n+    Path largeFile = new Path(\"testLargeCalls\");\n+    String testWriteOps = \"test\";\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+    statistics.reset();\n+\n+    //Test for zero write operation\n+    Assert.assertEquals(0, statistics.getWriteOps());\n+\n+    FSDataOutputStream outForOneCall = fs.create(smallFile);\n+    statistics.reset();\n+    //Writing \"test\" 1 time\n+    outForOneCall\n+        .write(testWriteOps.getBytes(), 0, testWriteOps.getBytes().length);\n+\n+    //Test for one write operation\n+    Assert.assertEquals(1, statistics.getWriteOps());\n+\n+    outForOneCall.close();\n+    //validating Content of file\n+    Assert.assertEquals(true, validateContent(fs, smallFile,\n+        testWriteOps.getBytes()));\n+\n+    String largeFileValidationString = \"\";\n+    FSDataOutputStream outForLargeCalls = fs.create(largeFile);\n+    statistics.reset();\n+    //Writing \"test\" 1000 times\n+    for (int i = 0; i < 1000; i++) {\n+      outForLargeCalls.write(testWriteOps.getBytes(), 0,\n+          testWriteOps.getBytes().length);\n+\n+      //Creating Validation string of \"test\" 1000 times\n+      largeFileValidationString += testWriteOps;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bba6f319bc157593ac9b89c37d9b874c6c3b0f3f"}, "originalPosition": 55}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzczNTMwNzgw", "url": "https://github.com/apache/hadoop/pull/1881#pullrequestreview-373530780", "createdAt": "2020-03-12T12:58:09Z", "commit": {"oid": "e3fff8808f25b1424814d55a26d1ac731811882d"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQxMjo1ODowOVrOF1dggg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQxMjo1ODowOVrOF1dggg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTYwMjMwNg==", "bodyText": "Change to \"Mismatch in read operation count.\"", "url": "https://github.com/apache/hadoop/pull/1881#discussion_r391602306", "createdAt": "2020-03-12T12:58:09Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsStreamStatistics.java", "diffHunk": "@@ -0,0 +1,109 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsInputStream;\n+\n+/**\n+ * Test Abfs Stream.\n+ */\n+\n+public class ITestAbfsStreamStatistics extends AbstractAbfsIntegrationTest {\n+  public ITestAbfsStreamStatistics() throws Exception {\n+  }\n+\n+  /***\n+   * {@link AbfsInputStream#incrementReadOps()}.\n+   *\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testAbfsStreamOps() throws Exception {\n+    describe(\"Test to see correct population of read and write operations in \"\n+        + \"Abfs\");\n+\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path smallOperaionsFile = new Path(\"testOneReadWriteOps\");\n+    Path largeOperationsFile = new Path(\"testLargeReadWriteOps\");\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+    String testReadWriteOps = \"test this\";\n+    statistics.reset();\n+\n+    //Test for zero read and write operation\n+    Assert.assertEquals(\"Zero read operations\", 0, statistics.getReadOps());\n+    Assert.assertEquals(\"Zero write operations\", 0, statistics.getWriteOps());\n+\n+    FSDataOutputStream outForOneOperation = fs.create(smallOperaionsFile);\n+    statistics.reset();\n+    outForOneOperation.write(testReadWriteOps.getBytes());\n+    FSDataInputStream inForOneCall = fs.open(smallOperaionsFile);\n+    inForOneCall.read(testReadWriteOps.getBytes(), 0,\n+        testReadWriteOps.getBytes().length);\n+\n+    //Test for one read and write operation\n+    Assert.assertEquals(\"one read operation is performed\", 1,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e3fff8808f25b1424814d55a26d1ac731811882d"}, "originalPosition": 67}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzczNTMxMTUw", "url": "https://github.com/apache/hadoop/pull/1881#pullrequestreview-373531150", "createdAt": "2020-03-12T12:58:40Z", "commit": {"oid": "e3fff8808f25b1424814d55a26d1ac731811882d"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQxMjo1ODo0MFrOF1dhog==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQxMjo1ODo0MFrOF1dhog==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTYwMjU5NA==", "bodyText": "Same here.", "url": "https://github.com/apache/hadoop/pull/1881#discussion_r391602594", "createdAt": "2020-03-12T12:58:40Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsStreamStatistics.java", "diffHunk": "@@ -0,0 +1,109 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsInputStream;\n+\n+/**\n+ * Test Abfs Stream.\n+ */\n+\n+public class ITestAbfsStreamStatistics extends AbstractAbfsIntegrationTest {\n+  public ITestAbfsStreamStatistics() throws Exception {\n+  }\n+\n+  /***\n+   * {@link AbfsInputStream#incrementReadOps()}.\n+   *\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testAbfsStreamOps() throws Exception {\n+    describe(\"Test to see correct population of read and write operations in \"\n+        + \"Abfs\");\n+\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path smallOperaionsFile = new Path(\"testOneReadWriteOps\");\n+    Path largeOperationsFile = new Path(\"testLargeReadWriteOps\");\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+    String testReadWriteOps = \"test this\";\n+    statistics.reset();\n+\n+    //Test for zero read and write operation\n+    Assert.assertEquals(\"Zero read operations\", 0, statistics.getReadOps());\n+    Assert.assertEquals(\"Zero write operations\", 0, statistics.getWriteOps());\n+\n+    FSDataOutputStream outForOneOperation = fs.create(smallOperaionsFile);\n+    statistics.reset();\n+    outForOneOperation.write(testReadWriteOps.getBytes());\n+    FSDataInputStream inForOneCall = fs.open(smallOperaionsFile);\n+    inForOneCall.read(testReadWriteOps.getBytes(), 0,\n+        testReadWriteOps.getBytes().length);\n+\n+    //Test for one read and write operation\n+    Assert.assertEquals(\"one read operation is performed\", 1,\n+        statistics.getReadOps());\n+    Assert.assertEquals(\"one write operation is performed\", 1,\n+        statistics.getWriteOps());\n+\n+    outForOneOperation.close();\n+    //validating Content of file\n+    Assert.assertEquals(\"one operation Content validation\", true,\n+        validateContent(fs, smallOperaionsFile,\n+            testReadWriteOps.getBytes()));\n+\n+    FSDataOutputStream outForLargeOperations = fs.create(largeOperationsFile);\n+    statistics.reset();\n+\n+    StringBuilder largeOperationsValidationString = new StringBuilder();\n+    for (int i = 0; i < 1000000; i++) {\n+      outForLargeOperations.write(testReadWriteOps.getBytes());\n+\n+      //Creating the String for content Validation\n+      largeOperationsValidationString.append(testReadWriteOps);\n+    }\n+\n+    FSDataInputStream inForLargeCalls = fs.open(largeOperationsFile);\n+\n+    for (int i = 0; i < 1000000; i++)\n+      inForLargeCalls\n+          .read(testReadWriteOps.getBytes(), 0,\n+              testReadWriteOps.getBytes().length);\n+\n+    //Test for one million read and write operations\n+    Assert.assertEquals(\"Large read operations\", 1000000,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e3fff8808f25b1424814d55a26d1ac731811882d"}, "originalPosition": 97}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzczNTMxMzc0", "url": "https://github.com/apache/hadoop/pull/1881#pullrequestreview-373531374", "createdAt": "2020-03-12T12:58:58Z", "commit": {"oid": "e3fff8808f25b1424814d55a26d1ac731811882d"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQxMjo1ODo1OFrOF1diUg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQxMjo1ODo1OFrOF1diUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTYwMjc3MA==", "bodyText": "Mismatch in file content.", "url": "https://github.com/apache/hadoop/pull/1881#discussion_r391602770", "createdAt": "2020-03-12T12:58:58Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsStreamStatistics.java", "diffHunk": "@@ -0,0 +1,109 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsInputStream;\n+\n+/**\n+ * Test Abfs Stream.\n+ */\n+\n+public class ITestAbfsStreamStatistics extends AbstractAbfsIntegrationTest {\n+  public ITestAbfsStreamStatistics() throws Exception {\n+  }\n+\n+  /***\n+   * {@link AbfsInputStream#incrementReadOps()}.\n+   *\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testAbfsStreamOps() throws Exception {\n+    describe(\"Test to see correct population of read and write operations in \"\n+        + \"Abfs\");\n+\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path smallOperaionsFile = new Path(\"testOneReadWriteOps\");\n+    Path largeOperationsFile = new Path(\"testLargeReadWriteOps\");\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+    String testReadWriteOps = \"test this\";\n+    statistics.reset();\n+\n+    //Test for zero read and write operation\n+    Assert.assertEquals(\"Zero read operations\", 0, statistics.getReadOps());\n+    Assert.assertEquals(\"Zero write operations\", 0, statistics.getWriteOps());\n+\n+    FSDataOutputStream outForOneOperation = fs.create(smallOperaionsFile);\n+    statistics.reset();\n+    outForOneOperation.write(testReadWriteOps.getBytes());\n+    FSDataInputStream inForOneCall = fs.open(smallOperaionsFile);\n+    inForOneCall.read(testReadWriteOps.getBytes(), 0,\n+        testReadWriteOps.getBytes().length);\n+\n+    //Test for one read and write operation\n+    Assert.assertEquals(\"one read operation is performed\", 1,\n+        statistics.getReadOps());\n+    Assert.assertEquals(\"one write operation is performed\", 1,\n+        statistics.getWriteOps());\n+\n+    outForOneOperation.close();\n+    //validating Content of file\n+    Assert.assertEquals(\"one operation Content validation\", true,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e3fff8808f25b1424814d55a26d1ac731811882d"}, "originalPosition": 74}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzczNTM4OTgw", "url": "https://github.com/apache/hadoop/pull/1881#pullrequestreview-373538980", "createdAt": "2020-03-12T13:09:42Z", "commit": {"oid": "e3fff8808f25b1424814d55a26d1ac731811882d"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzczNzY4NDMy", "url": "https://github.com/apache/hadoop/pull/1881#pullrequestreview-373768432", "createdAt": "2020-03-12T17:40:28Z", "commit": {"oid": "839c43d540d4a1eadd83e4c45e31621dcda70524"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQxNzo0MDoyOFrOF1ouXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQxNzo0MDoyOFrOF1ouXQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTc4NjA3Nw==", "bodyText": "just saw this typo. will fix it in the next commit.", "url": "https://github.com/apache/hadoop/pull/1881#discussion_r391786077", "createdAt": "2020-03-12T17:40:28Z", "author": {"login": "mehakmeet"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsStreamStatistics.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsInputStream;\n+\n+/**\n+ * Test Abfs Stream.\n+ */\n+\n+public class ITestAbfsStreamStatistics extends AbstractAbfsIntegrationTest {\n+  public ITestAbfsStreamStatistics() throws Exception {\n+  }\n+\n+  /***\n+   * {@link AbfsInputStream#incrementReadOps()}.\n+   *\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testAbfsStreamOps() throws Exception {\n+    describe(\"Test to see correct population of read and write operations in \"\n+        + \"Abfs\");\n+\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path smallOperaionsFile = new Path(\"testOneReadWriteOps\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "839c43d540d4a1eadd83e4c45e31621dcda70524"}, "originalPosition": 49}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzczNzY5MDY1", "url": "https://github.com/apache/hadoop/pull/1881#pullrequestreview-373769065", "createdAt": "2020-03-12T17:41:16Z", "commit": {"oid": "839c43d540d4a1eadd83e4c45e31621dcda70524"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQxNzo0MToxN1rOF1owmQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQxNzo0MToxN1rOF1owmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTc4NjY0OQ==", "bodyText": "this comment needs to change", "url": "https://github.com/apache/hadoop/pull/1881#discussion_r391786649", "createdAt": "2020-03-12T17:41:17Z", "author": {"login": "mehakmeet"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsStreamStatistics.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsInputStream;\n+\n+/**\n+ * Test Abfs Stream.\n+ */\n+\n+public class ITestAbfsStreamStatistics extends AbstractAbfsIntegrationTest {\n+  public ITestAbfsStreamStatistics() throws Exception {\n+  }\n+\n+  /***\n+   * {@link AbfsInputStream#incrementReadOps()}.\n+   *\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testAbfsStreamOps() throws Exception {\n+    describe(\"Test to see correct population of read and write operations in \"\n+        + \"Abfs\");\n+\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path smallOperaionsFile = new Path(\"testOneReadWriteOps\");\n+    Path largeOperationsFile = new Path(\"testLargeReadWriteOps\");\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+    String testReadWriteOps = \"test this\";\n+    statistics.reset();\n+\n+    //Test for zero read and write operation\n+    Assert.assertEquals(\"Mismatch in read operations\", 0,\n+        statistics.getReadOps());\n+    Assert.assertEquals(\"Mismatch in write operations\", 0,\n+        statistics.getWriteOps());\n+\n+    FSDataOutputStream outForOneOperation = fs.create(smallOperaionsFile);\n+    statistics.reset();\n+    outForOneOperation.write(testReadWriteOps.getBytes());\n+    FSDataInputStream inForOneCall = fs.open(smallOperaionsFile);\n+    inForOneCall.read(testReadWriteOps.getBytes(), 0,\n+        testReadWriteOps.getBytes().length);\n+\n+    //Test for one read and write operation\n+    Assert.assertEquals(\"Mismatch in read operations\", 1,\n+        statistics.getReadOps());\n+    Assert.assertEquals(\"Mismatch in write operations\", 1,\n+        statistics.getWriteOps());\n+\n+    outForOneOperation.close();\n+    //Validating if Content is being written in the smallFile\n+    Assert.assertEquals(\"Mismatch in content validation\", true,\n+        validateContent(fs, smallOperaionsFile,\n+            testReadWriteOps.getBytes()));\n+\n+    FSDataOutputStream outForLargeOperations = fs.create(largeOperationsFile);\n+    statistics.reset();\n+\n+    StringBuilder largeOperationsValidationString = new StringBuilder();\n+    for (int i = 0; i < 1000000; i++) {\n+      outForLargeOperations.write(testReadWriteOps.getBytes());\n+\n+      //Creating the String for content Validation\n+      largeOperationsValidationString.append(testReadWriteOps);\n+    }\n+\n+    FSDataInputStream inForLargeCalls = fs.open(largeOperationsFile);\n+\n+    for (int i = 0; i < 1000000; i++)\n+      inForLargeCalls\n+          .read(testReadWriteOps.getBytes(), 0,\n+              testReadWriteOps.getBytes().length);\n+\n+    //Test for one million read and write operations\n+    Assert.assertEquals(\"Mismatch in read operations\", 1000000,\n+        statistics.getReadOps());\n+    Assert.assertEquals(\"Mismatch in write operations\", 1000000,\n+        statistics.getWriteOps());\n+\n+    outForLargeOperations.close();\n+    //Validating if actually \"test\" is being written million times in largeOperationsFile", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "839c43d540d4a1eadd83e4c45e31621dcda70524"}, "originalPosition": 105}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc0Mjg5OTI4", "url": "https://github.com/apache/hadoop/pull/1881#pullrequestreview-374289928", "createdAt": "2020-03-13T13:16:56Z", "commit": {"oid": "89d73dddd337a340904b8e7a7bd9743fb3cff97e"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc1MTMwMjEz", "url": "https://github.com/apache/hadoop/pull/1881#pullrequestreview-375130213", "createdAt": "2020-03-16T11:40:31Z", "commit": {"oid": "89d73dddd337a340904b8e7a7bd9743fb3cff97e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxMTo0MDozMVrOF2wLFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxMTo0MDozMVrOF2wLFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjk1NjY5Mw==", "bodyText": "nit: Say what you return", "url": "https://github.com/apache/hadoop/pull/1881#discussion_r392956693", "createdAt": "2020-03-16T11:40:31Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/AbstractAbfsTestWithTimeout.java", "diffHunk": "@@ -67,4 +77,46 @@ public void nameThread() {\n   protected int getTestTimeoutMillis() {\n     return TEST_TIMEOUT;\n   }\n+\n+  /**\n+   * Describe a test in the logs.\n+   *\n+   * @param text text to print\n+   * @param args arguments to format in the printing\n+   */\n+  protected void describe(String text, Object... args) {\n+    LOG.info(\"\\n\\n{}: {}\\n\",\n+        methodName.getMethodName(),\n+        String.format(text, args));\n+  }\n+\n+  /**\n+   * Validate Contents written on a file in Abfs.\n+   *\n+   * @param fs                AzureBlobFileSystem\n+   * @param path              Path of the file\n+   * @param originalByteArray original byte array\n+   * @return", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "89d73dddd337a340904b8e7a7bd9743fb3cff97e"}, "originalPosition": 53}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc1MTMwNjAw", "url": "https://github.com/apache/hadoop/pull/1881#pullrequestreview-375130600", "createdAt": "2020-03-16T11:41:08Z", "commit": {"oid": "89d73dddd337a340904b8e7a7bd9743fb3cff97e"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxMTo0MToxMFrOF2wMZw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxMTo1NTo1OVrOF2wpYA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjk1NzAzMQ==", "bodyText": "just makes @code", "url": "https://github.com/apache/hadoop/pull/1881#discussion_r392957031", "createdAt": "2020-03-16T11:41:10Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsStreamStatistics.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsInputStream;\n+\n+/**\n+ * Test Abfs Stream.\n+ */\n+\n+public class ITestAbfsStreamStatistics extends AbstractAbfsIntegrationTest {\n+  public ITestAbfsStreamStatistics() throws Exception {\n+  }\n+\n+  /***\n+   * {@link AbfsInputStream#incrementReadOps()}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "89d73dddd337a340904b8e7a7bd9743fb3cff97e"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjk1NzcxNg==", "bodyText": "factor into assertReadWriteOps(operation, stats, read, write)", "url": "https://github.com/apache/hadoop/pull/1881#discussion_r392957716", "createdAt": "2020-03-16T11:42:38Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsStreamStatistics.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsInputStream;\n+\n+/**\n+ * Test Abfs Stream.\n+ */\n+\n+public class ITestAbfsStreamStatistics extends AbstractAbfsIntegrationTest {\n+  public ITestAbfsStreamStatistics() throws Exception {\n+  }\n+\n+  /***\n+   * {@link AbfsInputStream#incrementReadOps()}.\n+   *\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testAbfsStreamOps() throws Exception {\n+    describe(\"Test to see correct population of read and write operations in \"\n+        + \"Abfs\");\n+\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path smallOperationsFile = new Path(\"testOneReadWriteOps\");\n+    Path largeOperationsFile = new Path(\"testLargeReadWriteOps\");\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+    String testReadWriteOps = \"test this\";\n+    statistics.reset();\n+\n+    //Test for zero read and write operation\n+    Assert.assertEquals(\"Mismatch in read operations\", 0,\n+        statistics.getReadOps());\n+    Assert.assertEquals(\"Mismatch in write operations\", 0,\n+        statistics.getWriteOps());\n+\n+    FSDataOutputStream outForOneOperation = fs.create(smallOperationsFile);\n+    statistics.reset();\n+    outForOneOperation.write(testReadWriteOps.getBytes());\n+    FSDataInputStream inForOneCall = fs.open(smallOperationsFile);\n+    inForOneCall.read(testReadWriteOps.getBytes(), 0,\n+        testReadWriteOps.getBytes().length);\n+\n+    //Test for one read and write operation", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "89d73dddd337a340904b8e7a7bd9743fb3cff97e"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjk1ODAyMQ==", "bodyText": "close in a try/finally", "url": "https://github.com/apache/hadoop/pull/1881#discussion_r392958021", "createdAt": "2020-03-16T11:43:18Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsStreamStatistics.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsInputStream;\n+\n+/**\n+ * Test Abfs Stream.\n+ */\n+\n+public class ITestAbfsStreamStatistics extends AbstractAbfsIntegrationTest {\n+  public ITestAbfsStreamStatistics() throws Exception {\n+  }\n+\n+  /***\n+   * {@link AbfsInputStream#incrementReadOps()}.\n+   *\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testAbfsStreamOps() throws Exception {\n+    describe(\"Test to see correct population of read and write operations in \"\n+        + \"Abfs\");\n+\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path smallOperationsFile = new Path(\"testOneReadWriteOps\");\n+    Path largeOperationsFile = new Path(\"testLargeReadWriteOps\");\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+    String testReadWriteOps = \"test this\";\n+    statistics.reset();\n+\n+    //Test for zero read and write operation\n+    Assert.assertEquals(\"Mismatch in read operations\", 0,\n+        statistics.getReadOps());\n+    Assert.assertEquals(\"Mismatch in write operations\", 0,\n+        statistics.getWriteOps());\n+\n+    FSDataOutputStream outForOneOperation = fs.create(smallOperationsFile);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "89d73dddd337a340904b8e7a7bd9743fb3cff97e"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjk1ODQ4Mg==", "bodyText": "skip if statistics == null", "url": "https://github.com/apache/hadoop/pull/1881#discussion_r392958482", "createdAt": "2020-03-16T11:44:10Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -252,6 +255,13 @@ int readRemote(long position, byte[] b, int offset, int length) throws IOExcepti\n     return (int) bytesRead;\n   }\n \n+  /**\n+   * Increment Read Operations.\n+   */\n+  public void incrementReadOps() {\n+    statistics.incrementReadOps(1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "89d73dddd337a340904b8e7a7bd9743fb3cff97e"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjk1ODU4Mg==", "bodyText": "skip if statistics == null\nmake private", "url": "https://github.com/apache/hadoop/pull/1881#discussion_r392958582", "createdAt": "2020-03-16T11:44:22Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -181,6 +185,14 @@ public synchronized void write(final byte[] data, final int off, final int lengt\n \n       writableBytes = bufferSize - bufferIndex;\n     }\n+    incrementWriteOps();\n+  }\n+\n+  /**\n+   * Increment Write Operations.\n+   */\n+  public void incrementWriteOps() {\n+    statistics.incrementWriteOps(1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "89d73dddd337a340904b8e7a7bd9743fb3cff97e"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjk2MzQ1Mw==", "bodyText": "nit: ordering", "url": "https://github.com/apache/hadoop/pull/1881#discussion_r392963453", "createdAt": "2020-03-16T11:53:53Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -36,6 +36,7 @@\n import com.google.common.annotations.VisibleForTesting;\n import com.google.common.base.Preconditions;\n \n+import org.apache.hadoop.fs.FileSystem.Statistics;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "89d73dddd337a340904b8e7a7bd9743fb3cff97e"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjk2NDQ0OA==", "bodyText": "minor: nit: ordering", "url": "https://github.com/apache/hadoop/pull/1881#discussion_r392964448", "createdAt": "2020-03-16T11:55:59Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsStreamStatistics.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "89d73dddd337a340904b8e7a7bd9743fb3cff97e"}, "originalPosition": 26}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc2NTgyMzgy", "url": "https://github.com/apache/hadoop/pull/1881#pullrequestreview-376582382", "createdAt": "2020-03-18T05:59:33Z", "commit": {"oid": "9c782035581160d066d0d3d5a78072a653c34309"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQwNTo1OTozM1rOF33QSw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQwNTo1OTozM1rOF33QSw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDEyMTI5MQ==", "bodyText": "This is wrong. How can you compare the expected value with both read and write value? What if read and write values are different?\nassertReadWriteOps(operation, expected, actual) is what we discussed right?", "url": "https://github.com/apache/hadoop/pull/1881#discussion_r394121291", "createdAt": "2020-03-18T05:59:33Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsStreamStatistics.java", "diffHunk": "@@ -46,66 +46,89 @@ public void testAbfsStreamOps() throws Exception {\n         + \"Abfs\");\n \n     final AzureBlobFileSystem fs = getFileSystem();\n-    Path smallOperaionsFile = new Path(\"testOneReadWriteOps\");\n+    Path smallOperationsFile = new Path(\"testOneReadWriteOps\");\n     Path largeOperationsFile = new Path(\"testLargeReadWriteOps\");\n     FileSystem.Statistics statistics = fs.getFsStatistics();\n     String testReadWriteOps = \"test this\";\n     statistics.reset();\n \n     //Test for zero read and write operation\n-    Assert.assertEquals(\"Mismatch in read operations\", 0,\n-        statistics.getReadOps());\n-    Assert.assertEquals(\"Mismatch in write operations\", 0,\n-        statistics.getWriteOps());\n-\n-    FSDataOutputStream outForOneOperation = fs.create(smallOperaionsFile);\n-    statistics.reset();\n-    outForOneOperation.write(testReadWriteOps.getBytes());\n-    FSDataInputStream inForOneCall = fs.open(smallOperaionsFile);\n-    inForOneCall.read(testReadWriteOps.getBytes(), 0,\n-        testReadWriteOps.getBytes().length);\n-\n-    //Test for one read and write operation\n-    Assert.assertEquals(\"Mismatch in read operations\", 1,\n-        statistics.getReadOps());\n-    Assert.assertEquals(\"Mismatch in write operations\", 1,\n-        statistics.getWriteOps());\n-\n-    outForOneOperation.close();\n-    //Validating if Content is being written in the smallFile\n-    Assert.assertEquals(\"Mismatch in content validation\", true,\n-        validateContent(fs, smallOperaionsFile,\n+    assertReadWriteOps(0, statistics);\n+\n+    FSDataOutputStream outForOneOperation = null;\n+    FSDataInputStream inForOneOperation = null;\n+    try {\n+      outForOneOperation = fs.create(smallOperationsFile);\n+      statistics.reset();\n+      outForOneOperation.write(testReadWriteOps.getBytes());\n+      inForOneOperation = fs.open(smallOperationsFile);\n+      inForOneOperation.read(testReadWriteOps.getBytes(), 0,\n+          testReadWriteOps.getBytes().length);\n+\n+      //Test for one read and write operation\n+      assertReadWriteOps(1, statistics);\n+    } finally {\n+      if (inForOneOperation != null) {\n+        inForOneOperation.close();\n+      }\n+      if (outForOneOperation != null) {\n+        outForOneOperation.close();\n+      }\n+    }\n+    //Validating if content is being written in the smallOperationsFile\n+    Assert.assertTrue(\"Mismatch in content validation\",\n+        validateContent(fs, smallOperationsFile,\n             testReadWriteOps.getBytes()));\n \n-    FSDataOutputStream outForLargeOperations = fs.create(largeOperationsFile);\n-    statistics.reset();\n-\n+    FSDataOutputStream outForLargeOperations = null;\n+    FSDataInputStream inForLargeOperations = null;\n     StringBuilder largeOperationsValidationString = new StringBuilder();\n-    for (int i = 0; i < 1000000; i++) {\n-      outForLargeOperations.write(testReadWriteOps.getBytes());\n-\n-      //Creating the String for content Validation\n-      largeOperationsValidationString.append(testReadWriteOps);\n+    try {\n+      outForLargeOperations = fs.create(largeOperationsFile);\n+      statistics.reset();\n+      int largeValue = 1000000;\n+      for (int i = 0; i < largeValue; i++) {\n+        outForLargeOperations.write(testReadWriteOps.getBytes());\n+\n+        //Creating the String for content Validation\n+        largeOperationsValidationString.append(testReadWriteOps);\n+      }\n+\n+      inForLargeOperations = fs.open(largeOperationsFile);\n+      for (int i = 0; i < largeValue; i++)\n+        inForLargeOperations\n+            .read(testReadWriteOps.getBytes(), 0,\n+                testReadWriteOps.getBytes().length);\n+\n+      //Test for one million read and write operations\n+      assertReadWriteOps(largeValue, statistics);\n+    } finally {\n+      if (inForLargeOperations != null) {\n+        inForLargeOperations.close();\n+      }\n+      if (outForLargeOperations != null) {\n+        outForLargeOperations.close();\n+      }\n     }\n \n-    FSDataInputStream inForLargeCalls = fs.open(largeOperationsFile);\n+    //Validating if content is being written in largeOperationsFile\n+    Assert.assertTrue(\"Mismatch in content validation\",\n+        validateContent(fs, largeOperationsFile,\n+            largeOperationsValidationString.toString().getBytes()));\n \n-    for (int i = 0; i < 1000000; i++)\n-      inForLargeCalls\n-          .read(testReadWriteOps.getBytes(), 0,\n-              testReadWriteOps.getBytes().length);\n+  }\n \n-    //Test for one million read and write operations\n-    Assert.assertEquals(\"Mismatch in read operations\", 1000000,\n+  /**\n+   * Method for Read and Write Ops Assertion.\n+   *\n+   * @param expectedReadWriteOps Expected Value\n+   * @param statistics           fs stats to get Actual Values\n+   */\n+  private void assertReadWriteOps(long expectedReadWriteOps,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9c782035581160d066d0d3d5a78072a653c34309"}, "originalPosition": 138}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc2NTgzMjI1", "url": "https://github.com/apache/hadoop/pull/1881#pullrequestreview-376583225", "createdAt": "2020-03-18T06:02:02Z", "commit": {"oid": "9c782035581160d066d0d3d5a78072a653c34309"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQwNjowMjowM1rOF33THA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQwNjowMjowM1rOF33THA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDEyMjAxMg==", "bodyText": "remove", "url": "https://github.com/apache/hadoop/pull/1881#discussion_r394122012", "createdAt": "2020-03-18T06:02:03Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/AbstractAbfsTestWithTimeout.java", "diffHunk": "@@ -6,9 +6,9 @@\n  * to you under the Apache License, Version 2.0 (the\n  * \"License\"); you may not use this file except in compliance\n  * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n+ * <p>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9c782035581160d066d0d3d5a78072a653c34309"}, "originalPosition": 7}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc2NTg0MDA4", "url": "https://github.com/apache/hadoop/pull/1881#pullrequestreview-376584008", "createdAt": "2020-03-18T06:04:32Z", "commit": {"oid": "51fdba085a887653b5e62133ca34cc7d930e50ca"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc2NzY4NzYy", "url": "https://github.com/apache/hadoop/pull/1881#pullrequestreview-376768762", "createdAt": "2020-03-18T11:16:45Z", "commit": {"oid": "89d73dddd337a340904b8e7a7bd9743fb3cff97e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQxMToxNjo0NVrOF4Abiw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQxMToxNjo0NVrOF4Abiw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDI3MTYyNw==", "bodyText": "needs to be closed. you can use try (FSDataInputStream ...) { } to manage this automatically", "url": "https://github.com/apache/hadoop/pull/1881#discussion_r394271627", "createdAt": "2020-03-18T11:16:45Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/AbstractAbfsTestWithTimeout.java", "diffHunk": "@@ -67,4 +77,46 @@ public void nameThread() {\n   protected int getTestTimeoutMillis() {\n     return TEST_TIMEOUT;\n   }\n+\n+  /**\n+   * Describe a test in the logs.\n+   *\n+   * @param text text to print\n+   * @param args arguments to format in the printing\n+   */\n+  protected void describe(String text, Object... args) {\n+    LOG.info(\"\\n\\n{}: {}\\n\",\n+        methodName.getMethodName(),\n+        String.format(text, args));\n+  }\n+\n+  /**\n+   * Validate Contents written on a file in Abfs.\n+   *\n+   * @param fs                AzureBlobFileSystem\n+   * @param path              Path of the file\n+   * @param originalByteArray original byte array\n+   * @return\n+   * @throws IOException\n+   */\n+  protected boolean validateContent(AzureBlobFileSystem fs, Path path,\n+      byte[] originalByteArray)\n+      throws IOException {\n+    FSDataInputStream in = fs.open(path);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "89d73dddd337a340904b8e7a7bd9743fb3cff97e"}, "originalPosition": 59}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc2NzcwNjg0", "url": "https://github.com/apache/hadoop/pull/1881#pullrequestreview-376770684", "createdAt": "2020-03-18T11:19:45Z", "commit": {"oid": "83a23b5054b6910b33541bf52f7a1a8113bab16a"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQxMToxOTo0N1rOF4Ahrg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQxMToyMDo1NFrOF4Aj4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDI3MzE5OA==", "bodyText": "IOUtils.cleanupWithLogger(LOG, inForLargeOperations, outForLargeOperations)\n\nthat does close on all non-null arguments, catches failures so they don't get in the way of whatever was thrown earlier. We use this throughout the hadoop codebase to clean up robustly, so get used to it", "url": "https://github.com/apache/hadoop/pull/1881#discussion_r394273198", "createdAt": "2020-03-18T11:19:47Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsStreamStatistics.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+\n+/**\n+ * Test Abfs Stream.\n+ */\n+\n+public class ITestAbfsStreamStatistics extends AbstractAbfsIntegrationTest {\n+  public ITestAbfsStreamStatistics() throws Exception {\n+  }\n+\n+  /***\n+   * Testing {@code incrementReadOps()} in class {@code AbfsInputStream} and\n+   * {@code incrementWriteOps()} in class {@code AbfsOutputStream}.\n+   *\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testAbfsStreamOps() throws Exception {\n+    describe(\"Test to see correct population of read and write operations in \"\n+        + \"Abfs\");\n+\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    Path smallOperationsFile = new Path(\"testOneReadWriteOps\");\n+    Path largeOperationsFile = new Path(\"testLargeReadWriteOps\");\n+    FileSystem.Statistics statistics = fs.getFsStatistics();\n+    String testReadWriteOps = \"test this\";\n+    statistics.reset();\n+\n+    //Test for zero write operation\n+    assertReadWriteOps(\"write\", 0, statistics.getWriteOps());\n+\n+    //Test for zero read operation\n+    assertReadWriteOps(\"read\", 0, statistics.getReadOps());\n+\n+    FSDataOutputStream outForOneOperation = null;\n+    FSDataInputStream inForOneOperation = null;\n+    try {\n+      outForOneOperation = fs.create(smallOperationsFile);\n+      statistics.reset();\n+      outForOneOperation.write(testReadWriteOps.getBytes());\n+\n+      //Test for a single write operation\n+      assertReadWriteOps(\"write\", 1, statistics.getWriteOps());\n+\n+      inForOneOperation = fs.open(smallOperationsFile);\n+      inForOneOperation.read(testReadWriteOps.getBytes(), 0,\n+          testReadWriteOps.getBytes().length);\n+\n+      //Test for a single read operation\n+      assertReadWriteOps(\"read\", 1, statistics.getReadOps());\n+\n+    } finally {\n+      if (inForOneOperation != null) {\n+        inForOneOperation.close();\n+      }\n+      if (outForOneOperation != null) {\n+        outForOneOperation.close();\n+      }\n+    }\n+\n+    //Validating if content is being written in the smallOperationsFile\n+    Assert.assertTrue(\"Mismatch in content validation\",\n+        validateContent(fs, smallOperationsFile,\n+            testReadWriteOps.getBytes()));\n+\n+    FSDataOutputStream outForLargeOperations = null;\n+    FSDataInputStream inForLargeOperations = null;\n+    StringBuilder largeOperationsValidationString = new StringBuilder();\n+    try {\n+      outForLargeOperations = fs.create(largeOperationsFile);\n+      statistics.reset();\n+      int largeValue = 1000000;\n+      for (int i = 0; i < largeValue; i++) {\n+        outForLargeOperations.write(testReadWriteOps.getBytes());\n+\n+        //Creating the String for content Validation\n+        largeOperationsValidationString.append(testReadWriteOps);\n+      }\n+\n+      //Test for 1000000 write operations\n+      assertReadWriteOps(\"write\", largeValue, statistics.getWriteOps());\n+\n+      inForLargeOperations = fs.open(largeOperationsFile);\n+      for (int i = 0; i < largeValue; i++)\n+        inForLargeOperations\n+            .read(testReadWriteOps.getBytes(), 0,\n+                testReadWriteOps.getBytes().length);\n+\n+      //Test for 1000000 read operations\n+      assertReadWriteOps(\"read\", largeValue, statistics.getReadOps());\n+\n+    } finally {\n+      if (inForLargeOperations != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "83a23b5054b6910b33541bf52f7a1a8113bab16a"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDI3Mzc2MQ==", "bodyText": "add a .close() at the end so if something went wrong and the file was opened, we close the stream", "url": "https://github.com/apache/hadoop/pull/1881#discussion_r394273761", "createdAt": "2020-03-18T11:20:54Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemOauth.java", "diffHunk": "@@ -143,7 +143,7 @@ public void testBlobDataReader() throws Exception {\n \n     // TEST WRITE FILE\n     try {\n-      abfsStore.openFileForWrite(EXISTED_FILE_PATH, true);\n+      abfsStore.openFileForWrite(EXISTED_FILE_PATH, fs.getFsStatistics(), true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "83a23b5054b6910b33541bf52f7a1a8113bab16a"}, "originalPosition": 5}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc2NzcyMDM0", "url": "https://github.com/apache/hadoop/pull/1881#pullrequestreview-376772034", "createdAt": "2020-03-18T11:21:53Z", "commit": {"oid": "83a23b5054b6910b33541bf52f7a1a8113bab16a"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc4NjE5MTQy", "url": "https://github.com/apache/hadoop/pull/1881#pullrequestreview-378619142", "createdAt": "2020-03-20T16:09:56Z", "commit": {"oid": "d3233d1030112a7dc84e18d00dcdab6690453b91"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQxNjowOTo1NlrOF5aGrQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQxNjowOTo1NlrOF5aGrQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc0MDg0NQ==", "bodyText": "you are going to need to create a logger in this test case and pass it down i'm afraid", "url": "https://github.com/apache/hadoop/pull/1881#discussion_r395740845", "createdAt": "2020-03-20T16:09:56Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsStreamStatistics.java", "diffHunk": "@@ -76,12 +79,8 @@ public void testAbfsStreamOps() throws Exception {\n       assertReadWriteOps(\"read\", 1, statistics.getReadOps());\n \n     } finally {\n-      if (inForOneOperation != null) {\n-        inForOneOperation.close();\n-      }\n-      if (outForOneOperation != null) {\n-        outForOneOperation.close();\n-      }\n+      IOUtils.cleanupWithLogger(null, inForOneOperation,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d3233d1030112a7dc84e18d00dcdab6690453b91"}, "originalPosition": 27}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc5MTUxODMz", "url": "https://github.com/apache/hadoop/pull/1881#pullrequestreview-379151833", "createdAt": "2020-03-23T06:07:46Z", "commit": {"oid": "a607839150cc8576f0ffe3708a8271094ca12e39"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yM1QwNjowNzo0NlrOF534zw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yM1QwNjowNzo0NlrOF534zw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjIyODgxNQ==", "bodyText": "Is this logging required ? @steveloughran", "url": "https://github.com/apache/hadoop/pull/1881#discussion_r396228815", "createdAt": "2020-03-23T06:07:46Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsStreamStatistics.java", "diffHunk": "@@ -101,6 +115,8 @@ public void testAbfsStreamOps() throws Exception {\n         //Creating the String for content Validation\n         largeOperationsValidationString.append(testReadWriteOps);\n       }\n+      LOG.info(\"Number of bytes of Large data written: {}\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a607839150cc8576f0ffe3708a8271094ca12e39"}, "originalPosition": 62}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc5MTUzNzc0", "url": "https://github.com/apache/hadoop/pull/1881#pullrequestreview-379153774", "createdAt": "2020-03-23T06:14:26Z", "commit": {"oid": "a607839150cc8576f0ffe3708a8271094ca12e39"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yM1QwNjoxNDoyNlrOF53_Dg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yM1QwNjoxNDoyNlrOF53_Dg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjIzMDQxNA==", "bodyText": "Use IOUtils.cleanupWithLogger() here as well. Or you can just say this code is old.", "url": "https://github.com/apache/hadoop/pull/1881#discussion_r396230414", "createdAt": "2020-03-23T06:14:26Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemOauth.java", "diffHunk": "@@ -146,6 +146,8 @@ public void testBlobDataReader() throws Exception {\n       abfsStore.openFileForWrite(EXISTED_FILE_PATH, fs.getFsStatistics(), true);\n     } catch (AbfsRestOperationException e) {\n       assertEquals(AzureServiceErrorCode.AUTHORIZATION_PERMISSION_MISS_MATCH, e.getErrorCode());\n+    } finally {\n+      abfsStore.close();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a607839150cc8576f0ffe3708a8271094ca12e39"}, "originalPosition": 21}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a607839150cc8576f0ffe3708a8271094ca12e39", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/a607839150cc8576f0ffe3708a8271094ca12e39", "committedDate": "2020-03-23T03:25:38Z", "message": "Logging, closing, modifying"}, "afterCommit": {"oid": "afba6cc18b82c7d6fc68169b32c165fa46123ed8", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/afba6cc18b82c7d6fc68169b32c165fa46123ed8", "committedDate": "2020-03-23T06:51:35Z", "message": "Logging, closing, modifying"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bfb4afab8d49bbc9730691ae3ca5cde5d8b5963d", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/bfb4afab8d49bbc9730691ae3ca5cde5d8b5963d", "committedDate": "2020-03-23T08:15:25Z", "message": "CDPD-8485 Adding file system counters\n\n- Write_ops\n- Read_ops\n- Bytes_written (already updated)\n- Bytes_Read (already updated)\n\nChange-Id: I77349fdd158babd66df665713201fa9c8606f191"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ff01bcf889eccb566e069386065769a77a96b01e", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/ff01bcf889eccb566e069386065769a77a96b01e", "committedDate": "2020-03-23T08:15:25Z", "message": "Fixing Review Comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3e59ca51ef546aa303a7c50d092cffaa17ff664d", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/3e59ca51ef546aa303a7c50d092cffaa17ff664d", "committedDate": "2020-03-23T08:15:25Z", "message": "Meaningful assert messages"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ecc4c80fda62d497a4ff4df8de1515d30a255652", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/ecc4c80fda62d497a4ff4df8de1515d30a255652", "committedDate": "2020-03-23T08:15:25Z", "message": "minor nits in test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "536d1e11cc1d01084d510380774d045e717cee09", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/536d1e11cc1d01084d510380774d045e717cee09", "committedDate": "2020-03-23T08:15:25Z", "message": "minor nits in test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0e33beaf5d7c3fb9524c2c4ad8c8343d62351a72", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/0e33beaf5d7c3fb9524c2c4ad8c8343d62351a72", "committedDate": "2020-03-23T08:15:25Z", "message": "Fixing review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "72f7181f03ba835ac16cace1d096c857f3967ab4", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/72f7181f03ba835ac16cace1d096c857f3967ab4", "committedDate": "2020-03-23T08:15:25Z", "message": "closing streams"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6b28cf2a041adae52f38dc080616566506a52120", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/6b28cf2a041adae52f38dc080616566506a52120", "committedDate": "2020-03-23T08:15:25Z", "message": "Logging, closing, modifying"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "afba6cc18b82c7d6fc68169b32c165fa46123ed8", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/afba6cc18b82c7d6fc68169b32c165fa46123ed8", "committedDate": "2020-03-23T06:51:35Z", "message": "Logging, closing, modifying"}, "afterCommit": {"oid": "6b28cf2a041adae52f38dc080616566506a52120", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/6b28cf2a041adae52f38dc080616566506a52120", "committedDate": "2020-03-23T08:15:25Z", "message": "Logging, closing, modifying"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cf4b7d459c9df3fb04aef5624496d53deb533d3d", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/cf4b7d459c9df3fb04aef5624496d53deb533d3d", "committedDate": "2020-03-23T11:40:10Z", "message": "cleanup"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4445, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}