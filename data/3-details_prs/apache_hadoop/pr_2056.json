{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDI4NTM5NTM1", "number": 2056, "title": "HADOOP-17065. Add Network Counters to ABFS", "bodyText": "Contributed by: Mehakmeet Singh\nTested by: mvn -T 1C -Dparallel-tests=abfs clean verify\nRegion: East US, West US\n[INFO] Results:\n[INFO]\n[INFO] Tests run: 77, Failures: 0, Errors: 0, Skipped: 0\n\n[INFO] Results:\n[INFO]\n[WARNING] Tests run: 442, Failures: 0, Errors: 0, Skipped: 70\n\n[INFO] Results:\n[INFO]\n[WARNING] Tests run: 206, Failures: 0, Errors: 0, Skipped: 29", "createdAt": "2020-06-05T15:44:27Z", "url": "https://github.com/apache/hadoop/pull/2056", "merged": true, "mergeCommit": {"oid": "3472c3efc0014237d0cc4d9a989393b8513d2ab6"}, "closed": true, "closedAt": "2020-06-19T13:03:50Z", "author": {"login": "mehakmeet"}, "timelineItems": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcoUlaSgH2gAyNDI4NTM5NTM1OjIxOTQxMDRkNTMzODU3YTMzMzYzMjMyY2IwZjhiZjRjMjUwZjg2YzY=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcsysefgH2gAyNDI4NTM5NTM1OmJiM2E0Yzk4YTI4NGVkMzBhZTcxZjAxZjgzYmQ0ZmIxMTM2Y2NkMzI=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "2194104d533857a33363232cb0f8bf4c250f86c6", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/2194104d533857a33363232cb0f8bf4c250f86c6", "committedDate": "2020-06-05T15:42:01Z", "message": "HADOOP-17065. Adding Network Counters in ABFS"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI5NjA2Njc3", "url": "https://github.com/apache/hadoop/pull/2056#pullrequestreview-429606677", "createdAt": "2020-06-12T09:45:34Z", "commit": {"oid": "2194104d533857a33363232cb0f8bf4c250f86c6"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQwOTo0NTozNFrOGi96gA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQwOTo1MDowMlrOGi-CuQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTMxOTE2OA==", "bodyText": "\"were sent\"", "url": "https://github.com/apache/hadoop/pull/2056#discussion_r439319168", "createdAt": "2020-06-12T09:45:34Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsStatistic.java", "diffHunk": "@@ -57,7 +57,23 @@\n   FILES_DELETED(\"files_deleted\",\n       \"Total number of files deleted from the object store.\"),\n   ERROR_IGNORED(\"error_ignored\",\n-      \"Errors caught and ignored.\");\n+      \"Errors caught and ignored.\"),\n+\n+  //Network statistics.\n+  CONNECTIONS_MADE(\"connections_made\",\n+      \"Total number of times connection was made with Data store.\"),\n+  SEND_REQUESTS(\"send_requests\",\n+      \"Total number of times http requests was sent to the data store.\"),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2194104d533857a33363232cb0f8bf4c250f86c6"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTMxOTMyMw==", "bodyText": "\"a response\"", "url": "https://github.com/apache/hadoop/pull/2056#discussion_r439319323", "createdAt": "2020-06-12T09:45:52Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsStatistic.java", "diffHunk": "@@ -57,7 +57,23 @@\n   FILES_DELETED(\"files_deleted\",\n       \"Total number of files deleted from the object store.\"),\n   ERROR_IGNORED(\"error_ignored\",\n-      \"Errors caught and ignored.\");\n+      \"Errors caught and ignored.\"),\n+\n+  //Network statistics.\n+  CONNECTIONS_MADE(\"connections_made\",\n+      \"Total number of times connection was made with Data store.\"),\n+  SEND_REQUESTS(\"send_requests\",\n+      \"Total number of times http requests was sent to the data store.\"),\n+  GET_RESPONSE(\"get_response\",\n+      \"Total number of times response was recorded after sending requests.\"),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2194104d533857a33363232cb0f8bf4c250f86c6"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTMxOTgwMg==", "bodyText": "how about \"bytes sent from Azure Datalake\"", "url": "https://github.com/apache/hadoop/pull/2056#discussion_r439319802", "createdAt": "2020-06-12T09:46:54Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsStatistic.java", "diffHunk": "@@ -57,7 +57,23 @@\n   FILES_DELETED(\"files_deleted\",\n       \"Total number of files deleted from the object store.\"),\n   ERROR_IGNORED(\"error_ignored\",\n-      \"Errors caught and ignored.\");\n+      \"Errors caught and ignored.\"),\n+\n+  //Network statistics.\n+  CONNECTIONS_MADE(\"connections_made\",\n+      \"Total number of times connection was made with Data store.\"),\n+  SEND_REQUESTS(\"send_requests\",\n+      \"Total number of times http requests was sent to the data store.\"),\n+  GET_RESPONSE(\"get_response\",\n+      \"Total number of times response was recorded after sending requests.\"),\n+  BYTES_SEND(\"bytes_send\",\n+      \"Total bytes sent through http requests.\"),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2194104d533857a33363232cb0f8bf4c250f86c6"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTMxOTg5OQ==", "bodyText": "\"bytes sent\"", "url": "https://github.com/apache/hadoop/pull/2056#discussion_r439319899", "createdAt": "2020-06-12T09:47:06Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsStatistic.java", "diffHunk": "@@ -57,7 +57,23 @@\n   FILES_DELETED(\"files_deleted\",\n       \"Total number of files deleted from the object store.\"),\n   ERROR_IGNORED(\"error_ignored\",\n-      \"Errors caught and ignored.\");\n+      \"Errors caught and ignored.\"),\n+\n+  //Network statistics.\n+  CONNECTIONS_MADE(\"connections_made\",\n+      \"Total number of times connection was made with Data store.\"),\n+  SEND_REQUESTS(\"send_requests\",\n+      \"Total number of times http requests was sent to the data store.\"),\n+  GET_RESPONSE(\"get_response\",\n+      \"Total number of times response was recorded after sending requests.\"),\n+  BYTES_SEND(\"bytes_send\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2194104d533857a33363232cb0f8bf4c250f86c6"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTMyMDAzNw==", "bodyText": "responses", "url": "https://github.com/apache/hadoop/pull/2056#discussion_r439320037", "createdAt": "2020-06-12T09:47:22Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsStatistic.java", "diffHunk": "@@ -57,7 +57,23 @@\n   FILES_DELETED(\"files_deleted\",\n       \"Total number of files deleted from the object store.\"),\n   ERROR_IGNORED(\"error_ignored\",\n-      \"Errors caught and ignored.\");\n+      \"Errors caught and ignored.\"),\n+\n+  //Network statistics.\n+  CONNECTIONS_MADE(\"connections_made\",\n+      \"Total number of times connection was made with Data store.\"),\n+  SEND_REQUESTS(\"send_requests\",\n+      \"Total number of times http requests was sent to the data store.\"),\n+  GET_RESPONSE(\"get_response\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2194104d533857a33363232cb0f8bf4c250f86c6"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTMyMDA1Nw==", "bodyText": "how about \"bytes received from Azure Datalake\"", "url": "https://github.com/apache/hadoop/pull/2056#discussion_r439320057", "createdAt": "2020-06-12T09:47:25Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsStatistic.java", "diffHunk": "@@ -57,7 +57,23 @@\n   FILES_DELETED(\"files_deleted\",\n       \"Total number of files deleted from the object store.\"),\n   ERROR_IGNORED(\"error_ignored\",\n-      \"Errors caught and ignored.\");\n+      \"Errors caught and ignored.\"),\n+\n+  //Network statistics.\n+  CONNECTIONS_MADE(\"connections_made\",\n+      \"Total number of times connection was made with Data store.\"),\n+  SEND_REQUESTS(\"send_requests\",\n+      \"Total number of times http requests was sent to the data store.\"),\n+  GET_RESPONSE(\"get_response\",\n+      \"Total number of times response was recorded after sending requests.\"),\n+  BYTES_SEND(\"bytes_send\",\n+      \"Total bytes sent through http requests.\"),\n+  BYTES_RECEIVED(\"bytes_received\",\n+      \"Total bytes received in response to the requests sent.\"),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2194104d533857a33363232cb0f8bf4c250f86c6"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTMyMDI1Mg==", "bodyText": "indentation?", "url": "https://github.com/apache/hadoop/pull/2056#discussion_r439320252", "createdAt": "2020-06-12T09:47:49Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "diffHunk": "@@ -143,7 +144,8 @@\n   private final IdentityTransformerInterface identityTransformer;\n   private final AbfsPerfTracker abfsPerfTracker;\n \n-  public AzureBlobFileSystemStore(URI uri, boolean isSecureScheme, Configuration configuration)\n+  public AzureBlobFileSystemStore(URI uri, boolean isSecureScheme,\n+      Configuration configuration, AbfsCounters instrumentation)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2194104d533857a33363232cb0f8bf4c250f86c6"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTMyMTI3Mw==", "bodyText": "I'd be reluctant to re-init the same FS. better to create a new instance\nEven better, save the values as they are now, then assert the diff between the later values and these are  as expected", "url": "https://github.com/apache/hadoop/pull/2056#discussion_r439321273", "createdAt": "2020-06-12T09:50:02Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsNetworkStatistics.java", "diffHunk": "@@ -0,0 +1,245 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.io.IOUtils;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation;\n+\n+public class ITestAbfsNetworkStatistics extends AbstractAbfsIntegrationTest {\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(ITestAbfsNetworkStatistics.class);\n+  private static final int LARGE_OPERATIONS = 10;\n+\n+  public ITestAbfsNetworkStatistics() throws Exception {\n+  }\n+\n+  /**\n+   * Testing connections_made, send_request and bytes_send statistics in\n+   * {@link AbfsRestOperation}.\n+   */\n+  @Test\n+  public void testAbfsHttpSendStatistics() throws IOException {\n+    describe(\"Test to check correct values of statistics after Abfs http send \"\n+        + \"request is done.\");\n+\n+    AzureBlobFileSystem fs = getFileSystem();\n+    Map<String, Long> metricMap;\n+    Path sendRequestPath = path(getMethodName());\n+    String testNetworkStatsString = \"http_send\";\n+\n+    /*\n+     * Creating AbfsOutputStream will result in 1 connection made and 1 send\n+     * request.\n+     */\n+    try (AbfsOutputStream out = createAbfsOutputStreamWithFlushEnabled(fs,\n+        sendRequestPath)) {\n+      out.write(testNetworkStatsString.getBytes());\n+\n+      /*\n+       * Flushes all outstanding data (i.e. the current unfinished packet)\n+       * from the client into the service on all DataNode replicas.\n+       */\n+      out.hflush();\n+\n+      metricMap = fs.getInstrumentationMap();\n+\n+      /*\n+       * Testing the network stats with 1 write operation.\n+       *\n+       * connections_made : 3(getFileSystem()) + 1(AbfsOutputStream) + 2(flush).\n+       * send_requests : 1(getFileSystem()) + 1(AbfsOutputStream) + 2(flush).\n+       * bytes_send : bytes wrote in AbfsOutputStream.\n+       */\n+      assertAbfsStatistics(AbfsStatistic.CONNECTIONS_MADE, 6, metricMap);\n+      assertAbfsStatistics(AbfsStatistic.SEND_REQUESTS, 4, metricMap);\n+      assertAbfsStatistics(AbfsStatistic.BYTES_SEND,\n+          testNetworkStatsString.getBytes().length, metricMap);\n+\n+    }\n+\n+    /*\n+     * Re-initializing the FS for the next test.\n+     *\n+     * 2 connections are made during initialize. Hence, initial value of\n+     * connections_made = 2.\n+     */\n+    fs.initialize(fs.getUri(), fs.getConf());\n+\n+    try (AbfsOutputStream out = createAbfsOutputStreamWithFlushEnabled(fs,\n+        sendRequestPath)) {\n+\n+      for (int i = 0; i < LARGE_OPERATIONS; i++) {\n+        out.write(testNetworkStatsString.getBytes());\n+\n+        /*\n+         * 1 flush call would create 2 connections and 2 send requests.\n+         * when hflush() is called it will essentially trigger append() and\n+         * flush() inside AbfsRestOperation. Both of which calls\n+         * executeHttpOperation() method which creates a connection and sends\n+         * requests.\n+         */\n+        out.hflush();\n+      }\n+\n+      metricMap = fs.getInstrumentationMap();\n+\n+      /*\n+       * Testing the network stats with Large amount of bytes sent.\n+       *\n+       * connections made : 2(initialize) + 1(AbfsOutputStream) +\n+       * LARGE_OPERATIONS * 2(flush).\n+       * send requests : 1(AbfsOutputStream) + LARGE_OPERATIONS * 2(flush).\n+       * bytes send : LARGE_OPERATIONS * (bytes wrote each time).\n+       *\n+       */\n+      assertAbfsStatistics(AbfsStatistic.CONNECTIONS_MADE,\n+          3 + LARGE_OPERATIONS * 2, metricMap);\n+      assertAbfsStatistics(AbfsStatistic.SEND_REQUESTS,\n+          1 + LARGE_OPERATIONS * 2, metricMap);\n+      assertAbfsStatistics(AbfsStatistic.BYTES_SEND,\n+          LARGE_OPERATIONS * (testNetworkStatsString.getBytes().length),\n+          metricMap);\n+\n+    }\n+\n+  }\n+\n+  /**\n+   * Testing get_response and bytes_received in {@link AbfsRestOperation}.\n+   */\n+  @Test\n+  public void testAbfsHttpResponseStatistics() throws IOException {\n+    describe(\"Test to check correct values of statistics after Http \"\n+        + \"Response is processed.\");\n+\n+    AzureBlobFileSystem fs = getFileSystem();\n+    Path getResponsePath = path(getMethodName());\n+    Map<String, Long> metricMap;\n+    String testResponseString = \"some response\";\n+\n+    FSDataOutputStream out = null;\n+    FSDataInputStream in = null;\n+    try {\n+\n+      /*\n+       * Creating a File and writing some bytes in it.\n+       *\n+       * get_response : 3(getFileSystem) + 1(OutputStream creation) + 2\n+       * (Writing data in Data store).\n+       *\n+       */\n+      out = fs.create(getResponsePath);\n+      out.write(testResponseString.getBytes());\n+      out.hflush();\n+\n+      // open would require 1 get response.\n+      in = fs.open(getResponsePath);\n+      // read would require 1 get response and also get the bytes received.\n+      int result = in.read();\n+\n+      // Confirming read isn't -1.\n+      LOG.info(\"Result of read operation : {}\", result);\n+\n+      metricMap = fs.getInstrumentationMap();\n+\n+      /*\n+       * Testing values of statistics after writing and reading a buffer.\n+       *\n+       * get_response - 6(above operations) + 1(open()) + 1 (read()).\n+       * bytes_received - bytes send in the file.\n+       */\n+      assertAbfsStatistics(AbfsStatistic.GET_RESPONSE, 8, metricMap);\n+      // Testing that bytes received is equal to bytes sent.\n+      long bytesSend = metricMap.get(AbfsStatistic.BYTES_SEND.getStatName());\n+      assertAbfsStatistics(AbfsStatistic.BYTES_RECEIVED, bytesSend, metricMap);\n+\n+    } finally {\n+      IOUtils.cleanupWithLogger(LOG, out, in);\n+    }\n+\n+    /*\n+     * Re-initializing FS.\n+     * get_response required : 2.\n+     */\n+    fs.initialize(fs.getUri(), fs.getConf());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2194104d533857a33363232cb0f8bf4c250f86c6"}, "originalPosition": 192}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "223f9ad0d9c635b7ab979542bccfac97e161fa9b", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/223f9ad0d9c635b7ab979542bccfac97e161fa9b", "committedDate": "2020-06-14T02:49:34Z", "message": "HADOOP-17065. fix review comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMwNDI4OTMx", "url": "https://github.com/apache/hadoop/pull/2056#pullrequestreview-430428931", "createdAt": "2020-06-15T08:31:28Z", "commit": {"oid": "223f9ad0d9c635b7ab979542bccfac97e161fa9b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQwODozMToyOFrOGjoRjg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQwODozMToyOFrOGjoRjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDAxMzE5OA==", "bodyText": "typo: is received?", "url": "https://github.com/apache/hadoop/pull/2056#discussion_r440013198", "createdAt": "2020-06-15T08:31:28Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsNetworkStatistics.java", "diffHunk": "@@ -0,0 +1,253 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.io.IOUtils;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation;\n+\n+public class ITestAbfsNetworkStatistics extends AbstractAbfsIntegrationTest {\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(ITestAbfsNetworkStatistics.class);\n+  private static final int LARGE_OPERATIONS = 10;\n+\n+  public ITestAbfsNetworkStatistics() throws Exception {\n+  }\n+\n+  /**\n+   * Testing connections_made, send_request and bytes_send statistics in\n+   * {@link AbfsRestOperation}.\n+   */\n+  @Test\n+  public void testAbfsHttpSendStatistics() throws IOException {\n+    describe(\"Test to check correct values of statistics after Abfs http send \"\n+        + \"request is done.\");\n+\n+    AzureBlobFileSystem fs = getFileSystem();\n+    Map<String, Long> metricMap;\n+    Path sendRequestPath = path(getMethodName());\n+    String testNetworkStatsString = \"http_send\";\n+    long connectionsMade, requestsSent, bytesSent;\n+\n+    /*\n+     * Creating AbfsOutputStream will result in 1 connection made and 1 send\n+     * request.\n+     */\n+    try (AbfsOutputStream out = createAbfsOutputStreamWithFlushEnabled(fs,\n+        sendRequestPath)) {\n+      out.write(testNetworkStatsString.getBytes());\n+\n+      /*\n+       * Flushes all outstanding data (i.e. the current unfinished packet)\n+       * from the client into the service on all DataNode replicas.\n+       */\n+      out.hflush();\n+\n+      metricMap = fs.getInstrumentationMap();\n+\n+      /*\n+       * Testing the network stats with 1 write operation.\n+       *\n+       * connections_made : 3(getFileSystem()) + 1(AbfsOutputStream) + 2(flush).\n+       *\n+       * send_requests : 1(getFileSystem()) + 1(AbfsOutputStream) + 2(flush).\n+       *\n+       * bytes_sent : bytes wrote in AbfsOutputStream.\n+       */\n+      connectionsMade = assertAbfsStatistics(AbfsStatistic.CONNECTIONS_MADE,\n+          6, metricMap);\n+      requestsSent = assertAbfsStatistics(AbfsStatistic.SEND_REQUESTS, 4,\n+          metricMap);\n+      bytesSent = assertAbfsStatistics(AbfsStatistic.BYTES_SENT,\n+          testNetworkStatsString.getBytes().length, metricMap);\n+\n+    }\n+\n+    // To close the AbfsOutputStream 1 connection is made and 1 request is sent.\n+    connectionsMade++;\n+    requestsSent++;\n+\n+    try (AbfsOutputStream out = createAbfsOutputStreamWithFlushEnabled(fs,\n+        sendRequestPath)) {\n+\n+      for (int i = 0; i < LARGE_OPERATIONS; i++) {\n+        out.write(testNetworkStatsString.getBytes());\n+\n+        /*\n+         * 1 flush call would create 2 connections and 2 send requests.\n+         * when hflush() is called it will essentially trigger append() and\n+         * flush() inside AbfsRestOperation. Both of which calls\n+         * executeHttpOperation() method which creates a connection and sends\n+         * requests.\n+         */\n+        out.hflush();\n+      }\n+\n+      metricMap = fs.getInstrumentationMap();\n+\n+      /*\n+       * Testing the network stats with Large amount of bytes sent.\n+       *\n+       * connections made : connections_made(Last assertion) + 1\n+       * (AbfsOutputStream) + LARGE_OPERATIONS * 2(flush).\n+       *\n+       * send requests : requests_sent(Last assertion) + 1(AbfsOutputStream) +\n+       * LARGE_OPERATIONS * 2(flush).\n+       *\n+       * bytes sent : bytes_sent(Last assertion) + LARGE_OPERATIONS * (bytes\n+       * wrote each time).\n+       *\n+       */\n+      assertAbfsStatistics(AbfsStatistic.CONNECTIONS_MADE,\n+          connectionsMade + 1 + LARGE_OPERATIONS * 2, metricMap);\n+      assertAbfsStatistics(AbfsStatistic.SEND_REQUESTS,\n+          requestsSent + 1 + LARGE_OPERATIONS * 2, metricMap);\n+      assertAbfsStatistics(AbfsStatistic.BYTES_SENT,\n+          bytesSent + LARGE_OPERATIONS * (testNetworkStatsString.getBytes().length),\n+          metricMap);\n+\n+    }\n+\n+  }\n+\n+  /**\n+   * Testing get_response and bytes_received in {@link AbfsRestOperation}.\n+   */\n+  @Test\n+  public void testAbfsHttpResponseStatistics() throws IOException {\n+    describe(\"Test to check correct values of statistics after Http \"\n+        + \"Response is processed.\");\n+\n+    AzureBlobFileSystem fs = getFileSystem();\n+    Path getResponsePath = path(getMethodName());\n+    Map<String, Long> metricMap;\n+    String testResponseString = \"some response\";\n+    long getResponses, bytesReceived;\n+\n+    FSDataOutputStream out = null;\n+    FSDataInputStream in = null;\n+    try {\n+\n+      /*\n+       * Creating a File and writing some bytes in it.\n+       *\n+       * get_response : 3(getFileSystem) + 1(OutputStream creation) + 2\n+       * (Writing data in Data store).\n+       *\n+       */\n+      out = fs.create(getResponsePath);\n+      out.write(testResponseString.getBytes());\n+      out.hflush();\n+\n+      // open would require 1 get response.\n+      in = fs.open(getResponsePath);\n+      // read would require 1 get response and also get the bytes received.\n+      int result = in.read();\n+\n+      // Confirming read isn't -1.\n+      LOG.info(\"Result of read operation : {}\", result);\n+\n+      metricMap = fs.getInstrumentationMap();\n+\n+      /*\n+       * Testing values of statistics after writing and reading a buffer.\n+       *\n+       * get_responses - 6(above operations) + 1(open()) + 1 (read()).\n+       *\n+       * bytes_received - This should be equal to bytes sent earlier.\n+       */\n+      getResponses = assertAbfsStatistics(AbfsStatistic.GET_RESPONSES, 8,\n+          metricMap);\n+      // Testing that bytes received is equal to bytes sent.\n+      long bytesSend = metricMap.get(AbfsStatistic.BYTES_SENT.getStatName());\n+      bytesReceived = assertAbfsStatistics(AbfsStatistic.BYTES_RECEIVED,\n+          bytesSend,\n+          metricMap);\n+\n+    } finally {\n+      IOUtils.cleanupWithLogger(LOG, out, in);\n+    }\n+\n+    // To close the streams 1 response is gotten.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "223f9ad0d9c635b7ab979542bccfac97e161fa9b"}, "originalPosition": 198}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMwNDQwNzIx", "url": "https://github.com/apache/hadoop/pull/2056#pullrequestreview-430440721", "createdAt": "2020-06-15T08:46:58Z", "commit": {"oid": "223f9ad0d9c635b7ab979542bccfac97e161fa9b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQwODo0Njo1OFrOGjo0kQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQwODo0Njo1OFrOGjo0kQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDAyMjE2MQ==", "bodyText": "At this point the variable name is instrumentation and then it AbfsClient() it is statistics. I know these classes has been created earlier but I feel the names are bit confusing.\nA few suggestions.\n\nAbfsCounters could be AbfsInstrumentation and AbfsInstrumentation could be AbfsInstrumentationImpl\nLet AbfsCounters be same. change AbfsInstrumentation to AbfsCountersImpl.\nIt is not important to change but if everybody feels the same then sooner the better.\nCC @steveloughran", "url": "https://github.com/apache/hadoop/pull/2056#discussion_r440022161", "createdAt": "2020-06-15T08:46:58Z", "author": {"login": "mukund-thakur"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "diffHunk": "@@ -1214,11 +1217,11 @@ private void initializeClient(URI uri, String fileSystemName, String accountName\n     if (tokenProvider != null) {\n       this.client = new AbfsClient(baseUrl, creds, abfsConfiguration,\n           new ExponentialRetryPolicy(abfsConfiguration.getMaxIoRetries()),\n-          tokenProvider, abfsPerfTracker);\n+          tokenProvider, abfsPerfTracker, instrumentation);\n     } else {\n       this.client = new AbfsClient(baseUrl, creds, abfsConfiguration,\n           new ExponentialRetryPolicy(abfsConfiguration.getMaxIoRetries()),\n-          sasTokenProvider, abfsPerfTracker);\n+          sasTokenProvider, abfsPerfTracker, instrumentation);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "223f9ad0d9c635b7ab979542bccfac97e161fa9b"}, "originalPosition": 49}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7b1d241cd088a6e9fe5f97434bb172f79aafe234", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/7b1d241cd088a6e9fe5f97434bb172f79aafe234", "committedDate": "2020-06-16T12:24:22Z", "message": "HADOOP-17065. Renaming AbfsInstrumentation to AbfsCountersImpl"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMyNDMwOTQ2", "url": "https://github.com/apache/hadoop/pull/2056#pullrequestreview-432430946", "createdAt": "2020-06-17T13:59:12Z", "commit": {"oid": "7b1d241cd088a6e9fe5f97434bb172f79aafe234"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7eb7e9dcf421976e9da9998140cbf534b3e8b645", "author": {"user": null}, "url": "https://github.com/apache/hadoop/commit/7eb7e9dcf421976e9da9998140cbf534b3e8b645", "committedDate": "2020-06-18T11:04:33Z", "message": "HADOOP-17065. AbfsStatistic descriptions."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c108dce21e76b3b3e37aecf3724c26dc52049feb", "author": {"user": {"login": "steveloughran", "name": "Steve Loughran"}}, "url": "https://github.com/apache/hadoop/commit/c108dce21e76b3b3e37aecf3724c26dc52049feb", "committedDate": "2020-06-19T13:01:39Z", "message": "tweak some of the descriptions\n\n\r\ncut an accidentally pasted test name and add \"a\" in some of the descriptions"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bb3a4c98a284ed30ae71f01f83bd4fb1136ccd32", "author": {"user": {"login": "steveloughran", "name": "Steve Loughran"}}, "url": "https://github.com/apache/hadoop/commit/bb3a4c98a284ed30ae71f01f83bd4fb1136ccd32", "committedDate": "2020-06-19T13:02:35Z", "message": "get my own edits right"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4383, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}