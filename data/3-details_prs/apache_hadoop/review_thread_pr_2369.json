{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk5NTk1OTY0", "number": 2369, "reviewThreads": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMjo1Mjo1MlrOErmViA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMjo1NzoyMFrOErmcog==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0MTUyMzI4OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMjo1Mjo1MlrOHedDwQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwMjozMTowNFrOHgTPQA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTY5NTQyNQ==", "bodyText": "pull currentTimeMillis() outside the for loop as its an OS call with potential cost, and things probably work best if the same value is used through the loop and the code at L269", "url": "https://github.com/apache/hadoop/pull/2369#discussion_r501695425", "createdAt": "2020-10-08T12:52:52Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -242,13 +243,29 @@ private synchronized boolean tryEvict() {\n     }\n \n     // next, try any old nodes that have not been consumed\n+    // Failed read buffers (with buffer index=-1) that are older than\n+    // thresholdAge should be cleaned up, but at the same time should not\n+    // report successful eviction.\n+    // Queue logic expects that a buffer is freed up for read ahead when\n+    // eviction is successful, whereas a failed ReadBuffer would have released\n+    // its buffer when its status was set to READ_FAILED.\n     long earliestBirthday = Long.MAX_VALUE;\n+    ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>();\n     for (ReadBuffer buf : completedReadList) {\n-      if (buf.getTimeStamp() < earliestBirthday) {\n+      if ((buf.getBufferindex() != -1)\n+          && (buf.getTimeStamp() < earliestBirthday)) {\n         nodeToEvict = buf;\n         earliestBirthday = buf.getTimeStamp();\n+      } else if ((buf.getBufferindex() == -1)\n+          && (currentTimeMillis() - buf.getTimeStamp()) > thresholdAgeMilliseconds) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c22734b1f1cfdc6ea0e350f028d8e9f72a04eb42"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzYzMTY4MA==", "bodyText": "Done.", "url": "https://github.com/apache/hadoop/pull/2369#discussion_r503631680", "createdAt": "2020-10-13T02:31:04Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -242,13 +243,29 @@ private synchronized boolean tryEvict() {\n     }\n \n     // next, try any old nodes that have not been consumed\n+    // Failed read buffers (with buffer index=-1) that are older than\n+    // thresholdAge should be cleaned up, but at the same time should not\n+    // report successful eviction.\n+    // Queue logic expects that a buffer is freed up for read ahead when\n+    // eviction is successful, whereas a failed ReadBuffer would have released\n+    // its buffer when its status was set to READ_FAILED.\n     long earliestBirthday = Long.MAX_VALUE;\n+    ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>();\n     for (ReadBuffer buf : completedReadList) {\n-      if (buf.getTimeStamp() < earliestBirthday) {\n+      if ((buf.getBufferindex() != -1)\n+          && (buf.getTimeStamp() < earliestBirthday)) {\n         nodeToEvict = buf;\n         earliestBirthday = buf.getTimeStamp();\n+      } else if ((buf.getBufferindex() == -1)\n+          && (currentTimeMillis() - buf.getTimeStamp()) > thresholdAgeMilliseconds) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTY5NTQyNQ=="}, "originalCommit": {"oid": "c22734b1f1cfdc6ea0e350f028d8e9f72a04eb42"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0MTUyODI2OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMjo1NDowNlrOHedG8A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwMjozMToxOVrOHgTPeA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTY5NjI0MA==", "bodyText": "add (minimal) javadoc", "url": "https://github.com/apache/hadoop/pull/2369#discussion_r501696240", "createdAt": "2020-10-08T12:54:06Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -464,4 +480,10 @@ int getCompletedReadListSize() {\n   void callTryEvict() {\n     tryEvict();\n   }\n+\n+  @VisibleForTesting", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c22734b1f1cfdc6ea0e350f028d8e9f72a04eb42"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzYzMTQ4NQ==", "bodyText": "Done", "url": "https://github.com/apache/hadoop/pull/2369#discussion_r503631485", "createdAt": "2020-10-13T02:30:17Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -464,4 +480,10 @@ int getCompletedReadListSize() {\n   void callTryEvict() {\n     tryEvict();\n   }\n+\n+  @VisibleForTesting", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTY5NjI0MA=="}, "originalCommit": {"oid": "c22734b1f1cfdc6ea0e350f028d8e9f72a04eb42"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzYzMTczNg==", "bodyText": "Done", "url": "https://github.com/apache/hadoop/pull/2369#discussion_r503631736", "createdAt": "2020-10-13T02:31:19Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -464,4 +480,10 @@ int getCompletedReadListSize() {\n   void callTryEvict() {\n     tryEvict();\n   }\n+\n+  @VisibleForTesting", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTY5NjI0MA=="}, "originalCommit": {"oid": "c22734b1f1cfdc6ea0e350f028d8e9f72a04eb42"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0MTUzMDE0OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMjo1NDozNFrOHedIFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMjo1NDozNFrOHedIFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTY5NjUzNA==", "bodyText": "use 30_000 style integer", "url": "https://github.com/apache/hadoop/pull/2369#discussion_r501696534", "createdAt": "2020-10-08T12:54:34Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "diffHunk": "@@ -49,6 +49,7 @@\n   private static final int TWO_KB = 2 * 1024;\n   private static final int THREE_KB = 3 * 1024;\n   private static final int REDUCED_READ_BUFFER_AGE_THRESHOLD = 3000; // 3 sec\n+  private static final int INCREASED_READ_BUFFER_AGE_THRESHOLD = 30000; // 30 sec", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c22734b1f1cfdc6ea0e350f028d8e9f72a04eb42"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0MTUzMjk0OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMjo1NToxMlrOHedJ0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwMjozMToyOFrOHgTPnw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTY5Njk3Ng==", "bodyText": "import the field rather than a full reference", "url": "https://github.com/apache/hadoop/pull/2369#discussion_r501696976", "createdAt": "2020-10-08T12:55:12Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "diffHunk": "@@ -182,7 +183,39 @@ public void testFailedReadAhead() throws Exception {\n     checkEvictedStatus(inputStream, 0, false);\n   }\n \n+  @Test\n+  public void testFailedReadAheadEviction() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+    ReadBufferManager.setThresholdAgeMilliseconds(INCREASED_READ_BUFFER_AGE_THRESHOLD);\n+    // Stub :\n+    // Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // Actual read request fails with the failure in readahead thread\n+    doThrow(new TimeoutException(\"Internal Server error\"))\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class),\n+            any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testFailedReadAheadEviction.txt\");\n+\n+    // Add a failed buffer to completed queue and set to no free buffers to read ahead.\n+    ReadBuffer buff = new ReadBuffer();\n+    buff.setStatus(\n+        org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus.READ_FAILED);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c22734b1f1cfdc6ea0e350f028d8e9f72a04eb42"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzYzMTc3NQ==", "bodyText": "Done", "url": "https://github.com/apache/hadoop/pull/2369#discussion_r503631775", "createdAt": "2020-10-13T02:31:28Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "diffHunk": "@@ -182,7 +183,39 @@ public void testFailedReadAhead() throws Exception {\n     checkEvictedStatus(inputStream, 0, false);\n   }\n \n+  @Test\n+  public void testFailedReadAheadEviction() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+    ReadBufferManager.setThresholdAgeMilliseconds(INCREASED_READ_BUFFER_AGE_THRESHOLD);\n+    // Stub :\n+    // Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // Actual read request fails with the failure in readahead thread\n+    doThrow(new TimeoutException(\"Internal Server error\"))\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class),\n+            any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testFailedReadAheadEviction.txt\");\n+\n+    // Add a failed buffer to completed queue and set to no free buffers to read ahead.\n+    ReadBuffer buff = new ReadBuffer();\n+    buff.setStatus(\n+        org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus.READ_FAILED);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTY5Njk3Ng=="}, "originalCommit": {"oid": "c22734b1f1cfdc6ea0e350f028d8e9f72a04eb42"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0MTU0MTQ2OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMjo1NzoyMFrOHedPDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwMjozMTozOVrOHgTPww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTY5ODMxNg==", "bodyText": "use Assertions.assertThat with an explicit isLessThanOrEqualTo(3) assertion.", "url": "https://github.com/apache/hadoop/pull/2369#discussion_r501698316", "createdAt": "2020-10-08T12:57:20Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "diffHunk": "@@ -264,12 +297,24 @@ public void testSuccessfulReadAhead() throws Exception {\n             any(String.class));\n \n     AbfsInputStream inputStream = getAbfsInputStream(client, \"testSuccessfulReadAhead.txt\");\n+    int beforeReadCompletedListSize = ReadBufferManager.getBufferManager().getCompletedReadListSize();\n \n     // First read request that triggers readAheads.\n     inputStream.read(new byte[ONE_KB]);\n \n     // Only the 3 readAhead threads should have triggered client.read\n     verifyReadCallCount(client, 3);\n+    int newAdditionsToCompletedRead =\n+        ReadBufferManager.getBufferManager().getCompletedReadListSize()\n+            - beforeReadCompletedListSize;\n+    // read buffer might be dumped if the ReadBufferManager getblock preceded\n+    // the action of buffer being picked for reading from readaheadqueue, so that\n+    // inputstream can proceed with read and not be blocked on readahead thread\n+    // availability. So the count of buffers in completedReadQueue for the stream\n+    // can be same or lesser than the requests triggered to queue readahead.\n+    assertTrue(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c22734b1f1cfdc6ea0e350f028d8e9f72a04eb42"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzYzMTgxMQ==", "bodyText": "Done", "url": "https://github.com/apache/hadoop/pull/2369#discussion_r503631811", "createdAt": "2020-10-13T02:31:39Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "diffHunk": "@@ -264,12 +297,24 @@ public void testSuccessfulReadAhead() throws Exception {\n             any(String.class));\n \n     AbfsInputStream inputStream = getAbfsInputStream(client, \"testSuccessfulReadAhead.txt\");\n+    int beforeReadCompletedListSize = ReadBufferManager.getBufferManager().getCompletedReadListSize();\n \n     // First read request that triggers readAheads.\n     inputStream.read(new byte[ONE_KB]);\n \n     // Only the 3 readAhead threads should have triggered client.read\n     verifyReadCallCount(client, 3);\n+    int newAdditionsToCompletedRead =\n+        ReadBufferManager.getBufferManager().getCompletedReadListSize()\n+            - beforeReadCompletedListSize;\n+    // read buffer might be dumped if the ReadBufferManager getblock preceded\n+    // the action of buffer being picked for reading from readaheadqueue, so that\n+    // inputstream can proceed with read and not be blocked on readahead thread\n+    // availability. So the count of buffers in completedReadQueue for the stream\n+    // can be same or lesser than the requests triggered to queue readahead.\n+    assertTrue(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTY5ODMxNg=="}, "originalCommit": {"oid": "c22734b1f1cfdc6ea0e350f028d8e9f72a04eb42"}, "originalPosition": 67}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3201, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}