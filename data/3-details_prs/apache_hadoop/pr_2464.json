{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTIxMzU2OTIy", "number": 2464, "title": "HADOOP-17347. ABFS: Read optimizations", "bodyText": "Optimize read performance for the following scenarios\nRead small files completely\nFiles that are of size smaller than the read buffer size can be considered as small files. In case of such files it would be better to read the full file into the AbfsInputStream buffer.\nRead last block if the read is for footer\nIf the read is for the last 8 bytes, read the file completely.\nThis will optimize reads for parquet files.\nBoth these optimizations will be present under configs as follows\nfs.azure.read.smallfilescompletely\nfs.azure.read.optimizefooterread", "createdAt": "2020-11-16T05:03:52Z", "url": "https://github.com/apache/hadoop/pull/2464", "merged": true, "mergeCommit": {"oid": "1448add08fcd4a23e59eab5f75ef46fca6b1c3d1"}, "closed": true, "closedAt": "2021-01-02T18:37:10Z", "author": {"login": "bilaharith"}, "timelineItems": {"totalCount": 64, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdcWELBgH2gAyNTIxMzU2OTIyOjAwZmY1YzJkYjJhMTVjMDc1NzBiMmI1ODE2MjBhMzQzMDM0NGZiMDY=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdsRiqagFqTU2MDY0OTM1Mw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "00ff5c2db2a15c07570b2b581620a3430344fb06", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/00ff5c2db2a15c07570b2b581620a3430344fb06", "committedDate": "2020-11-14T06:49:35Z", "message": "ABFS: Read small files completely"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dce042af52715ff0b9c8cea7bfbd99248fa4afb5", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/dce042af52715ff0b9c8cea7bfbd99248fa4afb5", "committedDate": "2020-11-14T06:49:35Z", "message": "Footer readoptimisation"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "eda2b7684a89600ecacae3c96b88ffb48906fc82", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/eda2b7684a89600ecacae3c96b88ffb48906fc82", "committedDate": "2020-11-14T06:49:35Z", "message": "Splitting the method readOneBlock to 3 different methods"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "04acdac63bbd648269f9346d97eafd3653e8d9a5", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/04acdac63bbd648269f9346d97eafd3653e8d9a5", "committedDate": "2020-11-14T06:49:35Z", "message": "Test case fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "147b5a73e8e6c98084024c0502f3549ad842331b", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/147b5a73e8e6c98084024c0502f3549ad842331b", "committedDate": "2020-11-14T11:13:40Z", "message": "Checkstyle fixes"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwOTI4OTE4", "url": "https://github.com/apache/hadoop/pull/2464#pullrequestreview-530928918", "createdAt": "2020-11-16T05:39:00Z", "commit": {"oid": "147b5a73e8e6c98084024c0502f3549ad842331b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQwNTozOTowMFrOHzo2sA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQwNTozOTowMFrOHzo2sA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzkwODc4NA==", "bodyText": "smallfilescompletely\noptimizefooterread", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r523908784", "createdAt": "2020-11-16T05:39:00Z", "author": {"login": "vinaysbadami"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java", "diffHunk": "@@ -56,6 +56,8 @@\n   public static final String AZURE_WRITE_MAX_REQUESTS_TO_QUEUE = \"fs.azure.write.max.requests.to.queue\";\n   public static final String AZURE_WRITE_BUFFER_SIZE = \"fs.azure.write.request.size\";\n   public static final String AZURE_READ_BUFFER_SIZE = \"fs.azure.read.request.size\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "147b5a73e8e6c98084024c0502f3549ad842331b"}, "originalPosition": 3}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwOTMxNjMw", "url": "https://github.com/apache/hadoop/pull/2464#pullrequestreview-530931630", "createdAt": "2020-11-16T05:49:25Z", "commit": {"oid": "147b5a73e8e6c98084024c0502f3549ad842331b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQwNTo0OToyNVrOHzo_9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQwNTo0OToyNVrOHzo_9A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzkxMTE1Ng==", "bodyText": "why the change? Please revert if not require", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r523911156", "createdAt": "2020-11-16T05:49:25Z", "author": {"login": "vinaysbadami"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -128,7 +138,7 @@ public synchronized int read(final byte[] b, final int off, final int len) throw\n     }\n     incrementReadOps();\n     do {\n-      lastReadBytes = readOneBlock(b, currentOff, currentLen);\n+      lastReadBytes = readToUserBuffer(b, currentOff, currentLen);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "147b5a73e8e6c98084024c0502f3549ad842331b"}, "originalPosition": 36}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwOTM3Njgx", "url": "https://github.com/apache/hadoop/pull/2464#pullrequestreview-530937681", "createdAt": "2020-11-16T06:11:23Z", "commit": {"oid": "147b5a73e8e6c98084024c0502f3549ad842331b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQwNjoxMToyM1rOHzpUsw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQwNjoxMToyM1rOHzpUsw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzkxNjQ2Nw==", "bodyText": "default false", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r523916467", "createdAt": "2020-11-16T06:11:23Z", "author": {"login": "vinaysbadami"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java", "diffHunk": "@@ -50,13 +50,15 @@\n   public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_MAX_BACKOFF_INTERVAL = SIXTY_SECONDS;\n   public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_DELTA_BACKOFF = 2;\n \n-  private static final int ONE_KB = 1024;\n-  private static final int ONE_MB = ONE_KB * ONE_KB;\n+  public static final int ONE_KB = 1024;\n+  public static final int ONE_MB = ONE_KB * ONE_KB;\n \n   // Default upload and download buffer size\n   public static final int DEFAULT_WRITE_BUFFER_SIZE = 8 * ONE_MB;  // 8 MB\n   public static final int APPENDBLOB_MAX_WRITE_BUFFER_SIZE = 4 * ONE_MB;  // 4 MB\n   public static final int DEFAULT_READ_BUFFER_SIZE = 4 * ONE_MB;  // 4 MB\n+  public static final boolean DEFAULT_READ_SMALL_FILES_COMPLETELY = true;\n+  public static final boolean DEFAULT_OPTIMIZE_FOOTER_READ = true;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "147b5a73e8e6c98084024c0502f3549ad842331b"}, "originalPosition": 14}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwOTM4MzM1", "url": "https://github.com/apache/hadoop/pull/2464#pullrequestreview-530938335", "createdAt": "2020-11-16T06:13:45Z", "commit": {"oid": "147b5a73e8e6c98084024c0502f3549ad842331b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQwNjoxMzo0NVrOHzpXDg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQwNjoxMzo0NVrOHzpXDg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzkxNzA3MA==", "bodyText": "we are passed in an AbfsInputStreamContext. We should hold onto that instance instead of creating member variables for each of the members of AbfsInputStreamContext.", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r523917070", "createdAt": "2020-11-16T06:13:45Z", "author": {"login": "vinaysbadami"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -92,6 +99,9 @@ public AbfsInputStream(\n     this.cachedSasToken = new CachedSASToken(\n         abfsInputStreamContext.getSasTokenRenewPeriodForStreamsInSeconds());\n     this.streamStatistics = abfsInputStreamContext.getStreamStatistics();\n+    this.readSmallFilesCompletely = abfsInputStreamContext\n+        .readSmallFilesCompletely();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "147b5a73e8e6c98084024c0502f3549ad842331b"}, "originalPosition": 26}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwOTM5NTQ2", "url": "https://github.com/apache/hadoop/pull/2464#pullrequestreview-530939546", "createdAt": "2020-11-16T06:18:06Z", "commit": {"oid": "147b5a73e8e6c98084024c0502f3549ad842331b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQwNjoxODowNlrOHzpbbg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQwNjoxODowNlrOHzpbbg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzkxODE5MA==", "bodyText": "&& len ==  FOOTER_DELTA", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r523918190", "createdAt": "2020-11-16T06:18:06Z", "author": {"login": "vinaysbadami"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -162,14 +172,86 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n       throw new IndexOutOfBoundsException();\n     }\n \n-    //If buffer is empty, then fill the buffer.\n-    if (bCursor == limit) {\n+    if (firstRead && this.readSmallFilesCompletely\n+        && contentLength <= bufferSize) { //  Read small files completely\n+      if (readFileCompletely() == -1) {\n+        return -1;\n+      }\n+    } else if (firstRead && this.optimizeFooterRead && fCursor\n+        == contentLength - FOOTER_DELTA) {  //  Read the last one block if the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "147b5a73e8e6c98084024c0502f3549ad842331b"}, "originalPosition": 61}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwOTQwNTU4", "url": "https://github.com/apache/hadoop/pull/2464#pullrequestreview-530940558", "createdAt": "2020-11-16T06:21:24Z", "commit": {"oid": "147b5a73e8e6c98084024c0502f3549ad842331b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQwNjoyMToyNVrOHzpe3g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQwNjoyMToyNVrOHzpe3g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzkxOTA3MA==", "bodyText": "if (firstRead) {\nif (smallfilecase)\n{\nres = readfilecompletelty;\nfirstread = false;\n}\nelse if (readfootercase)\n{\nres = readfooter\nfirstread = false\n}\n}", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r523919070", "createdAt": "2020-11-16T06:21:25Z", "author": {"login": "vinaysbadami"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -162,14 +172,86 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n       throw new IndexOutOfBoundsException();\n     }\n \n-    //If buffer is empty, then fill the buffer.\n-    if (bCursor == limit) {\n+    if (firstRead && this.readSmallFilesCompletely", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "147b5a73e8e6c98084024c0502f3549ad842331b"}, "originalPosition": 55}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwOTQxNDMx", "url": "https://github.com/apache/hadoop/pull/2464#pullrequestreview-530941431", "createdAt": "2020-11-16T06:24:15Z", "commit": {"oid": "147b5a73e8e6c98084024c0502f3549ad842331b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQwNjoyNDoxNVrOHzphzA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQwNjoyNDoxNVrOHzphzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzkxOTgyMA==", "bodyText": "limit = bytesread", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r523919820", "createdAt": "2020-11-16T06:24:15Z", "author": {"login": "vinaysbadami"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -162,14 +172,86 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n       throw new IndexOutOfBoundsException();\n     }\n \n-    //If buffer is empty, then fill the buffer.\n-    if (bCursor == limit) {\n+    if (firstRead && this.readSmallFilesCompletely\n+        && contentLength <= bufferSize) { //  Read small files completely\n+      if (readFileCompletely() == -1) {\n+        return -1;\n+      }\n+    } else if (firstRead && this.optimizeFooterRead && fCursor\n+        == contentLength - FOOTER_DELTA) {  //  Read the last one block if the\n+      // read is for the footer\n+      if (readLastBlock() == -1) {\n+        return -1;\n+      }\n+    } else {\n+      if (readOneBlock(b) == -1) {\n+        return -1;\n+      }\n+    }\n+    fCursorAfterLastRead = fCursor;\n+\n+    //If there is anything in the buffer, then return lesser of (requested bytes) and (bytes in buffer)\n+    //(bytes returned may be less than requested)\n+    int bytesRemaining = limit - bCursor;\n+    int bytesToRead = Math.min(len, bytesRemaining);\n+    System.arraycopy(buffer, bCursor, b, off, bytesToRead);\n+    bCursor += bytesToRead;\n+    if (statistics != null) {\n+      statistics.incrementBytesRead(bytesToRead);\n+    }\n+    if (streamStatistics != null) {\n+      // Bytes read from the local buffer.\n+      streamStatistics.bytesReadFromBuffer(bytesToRead);\n+      streamStatistics.bytesRead(bytesToRead);\n+    }\n+    return bytesToRead;\n+  }\n+\n+  private long readFileCompletely() throws IOException {\n+    bCursor = (int) getPos();\n+    //  Read full file in case the file size <= buffer size\n+    buffer = new byte[bufferSize];\n+    long bytesRead = readInternal(0, buffer, 0, (int) contentLength, true);\n+    firstRead = false;\n+\n+    if (bytesRead == -1) {\n+      return -1;\n+    }\n+\n+    limit = (int) bytesRead;\n+    fCursor = bytesRead;\n+    return bytesRead;\n+  }\n+\n+  private long readLastBlock() throws IOException {\n+    bCursor = (int) (\n+        ((contentLength < bufferSize) ? contentLength : bufferSize)\n+            - FOOTER_DELTA);\n+    buffer = new byte[bufferSize];\n+    long startPos = (contentLength < bufferSize)\n+        ? 0\n+        : contentLength - bufferSize;\n+    long bytesRead = readInternal(startPos, buffer, 0, bufferSize, true);\n+    firstRead = false;\n+\n+    if (bytesRead == -1) {\n+      return -1;\n+    }\n+\n+    limit += (int) bytesRead;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "147b5a73e8e6c98084024c0502f3549ad842331b"}, "originalPosition": 121}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwOTQzNzk2", "url": "https://github.com/apache/hadoop/pull/2464#pullrequestreview-530943796", "createdAt": "2020-11-16T06:31:25Z", "commit": {"oid": "147b5a73e8e6c98084024c0502f3549ad842331b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQwNjozMToyNVrOHzppYQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQwNjozMToyNVrOHzppYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzkyMTc2MQ==", "bodyText": "check this - should be the offset into buffer passed into read", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r523921761", "createdAt": "2020-11-16T06:31:25Z", "author": {"login": "vinaysbadami"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -162,14 +172,86 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n       throw new IndexOutOfBoundsException();\n     }\n \n-    //If buffer is empty, then fill the buffer.\n-    if (bCursor == limit) {\n+    if (firstRead && this.readSmallFilesCompletely\n+        && contentLength <= bufferSize) { //  Read small files completely\n+      if (readFileCompletely() == -1) {\n+        return -1;\n+      }\n+    } else if (firstRead && this.optimizeFooterRead && fCursor\n+        == contentLength - FOOTER_DELTA) {  //  Read the last one block if the\n+      // read is for the footer\n+      if (readLastBlock() == -1) {\n+        return -1;\n+      }\n+    } else {\n+      if (readOneBlock(b) == -1) {\n+        return -1;\n+      }\n+    }\n+    fCursorAfterLastRead = fCursor;\n+\n+    //If there is anything in the buffer, then return lesser of (requested bytes) and (bytes in buffer)\n+    //(bytes returned may be less than requested)\n+    int bytesRemaining = limit - bCursor;\n+    int bytesToRead = Math.min(len, bytesRemaining);\n+    System.arraycopy(buffer, bCursor, b, off, bytesToRead);\n+    bCursor += bytesToRead;\n+    if (statistics != null) {\n+      statistics.incrementBytesRead(bytesToRead);\n+    }\n+    if (streamStatistics != null) {\n+      // Bytes read from the local buffer.\n+      streamStatistics.bytesReadFromBuffer(bytesToRead);\n+      streamStatistics.bytesRead(bytesToRead);\n+    }\n+    return bytesToRead;\n+  }\n+\n+  private long readFileCompletely() throws IOException {\n+    bCursor = (int) getPos();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "147b5a73e8e6c98084024c0502f3549ad842331b"}, "originalPosition": 91}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwOTQ0NDQ1", "url": "https://github.com/apache/hadoop/pull/2464#pullrequestreview-530944445", "createdAt": "2020-11-16T06:33:21Z", "commit": {"oid": "147b5a73e8e6c98084024c0502f3549ad842331b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQwNjozMzoyMVrOHzprIA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQwNjozMzoyMVrOHzprIA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzkyMjIwOA==", "bodyText": "is this correct?", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r523922208", "createdAt": "2020-11-16T06:33:21Z", "author": {"login": "vinaysbadami"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -162,14 +172,86 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n       throw new IndexOutOfBoundsException();\n     }\n \n-    //If buffer is empty, then fill the buffer.\n-    if (bCursor == limit) {\n+    if (firstRead && this.readSmallFilesCompletely\n+        && contentLength <= bufferSize) { //  Read small files completely\n+      if (readFileCompletely() == -1) {\n+        return -1;\n+      }\n+    } else if (firstRead && this.optimizeFooterRead && fCursor\n+        == contentLength - FOOTER_DELTA) {  //  Read the last one block if the\n+      // read is for the footer\n+      if (readLastBlock() == -1) {\n+        return -1;\n+      }\n+    } else {\n+      if (readOneBlock(b) == -1) {\n+        return -1;\n+      }\n+    }\n+    fCursorAfterLastRead = fCursor;\n+\n+    //If there is anything in the buffer, then return lesser of (requested bytes) and (bytes in buffer)\n+    //(bytes returned may be less than requested)\n+    int bytesRemaining = limit - bCursor;\n+    int bytesToRead = Math.min(len, bytesRemaining);\n+    System.arraycopy(buffer, bCursor, b, off, bytesToRead);\n+    bCursor += bytesToRead;\n+    if (statistics != null) {\n+      statistics.incrementBytesRead(bytesToRead);\n+    }\n+    if (streamStatistics != null) {\n+      // Bytes read from the local buffer.\n+      streamStatistics.bytesReadFromBuffer(bytesToRead);\n+      streamStatistics.bytesRead(bytesToRead);\n+    }\n+    return bytesToRead;\n+  }\n+\n+  private long readFileCompletely() throws IOException {\n+    bCursor = (int) getPos();\n+    //  Read full file in case the file size <= buffer size\n+    buffer = new byte[bufferSize];\n+    long bytesRead = readInternal(0, buffer, 0, (int) contentLength, true);\n+    firstRead = false;\n+\n+    if (bytesRead == -1) {\n+      return -1;\n+    }\n+\n+    limit = (int) bytesRead;\n+    fCursor = bytesRead;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "147b5a73e8e6c98084024c0502f3549ad842331b"}, "originalPosition": 102}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "78f5a1b57bb2611c9682ac8f7349dc67465d15fc", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/78f5a1b57bb2611c9682ac8f7349dc67465d15fc", "committedDate": "2020-11-23T04:54:29Z", "message": "Addressing review comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM3MDkzMTY5", "url": "https://github.com/apache/hadoop/pull/2464#pullrequestreview-537093169", "createdAt": "2020-11-24T05:43:16Z", "commit": {"oid": "78f5a1b57bb2611c9682ac8f7349dc67465d15fc"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQwNTo0MzoxNlrOH4syTQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQwNTo0MzoxNlrOH4syTQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTIxNjA3Nw==", "bodyText": "move above the if check", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r529216077", "createdAt": "2020-11-24T05:43:16Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -162,14 +169,87 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n       throw new IndexOutOfBoundsException();\n     }\n \n-    //If buffer is empty, then fill the buffer.\n-    if (bCursor == limit) {\n+    int bytesRead = 0;\n+    if (shouldReadFully()) {\n+      bytesRead = readFileCompletely();\n+    } else if (shouldReadLastBlock(len)) {\n+      bytesRead = readLastBlock();\n+    } else {\n+      bytesRead = readOneBlock(b);\n+    }\n+    if (bytesRead == -1) {\n+      return -1;\n+    }\n+    fCursorAfterLastRead = fCursor;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78f5a1b57bb2611c9682ac8f7349dc67465d15fc"}, "originalPosition": 63}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f3a88a395ddd561a483220cdcf0661a62973a5c5", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/f3a88a395ddd561a483220cdcf0661a62973a5c5", "committedDate": "2020-11-24T10:06:31Z", "message": "Moving fCursorAfterLastRead to the place where backend calls made"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4d446994dae4ef6566c93c40bc7f95c07b9565b3", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/4d446994dae4ef6566c93c40bc7f95c07b9565b3", "committedDate": "2020-11-25T04:03:51Z", "message": "Moving common code around the readOneBlock to separate methods and the same is used across all the 3 read forks from the start itself"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "00d1328cb85fcb5ebb1778319bc36e7797ae1272", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/00d1328cb85fcb5ebb1778319bc36e7797ae1272", "committedDate": "2020-11-25T04:08:35Z", "message": "Rearranging methods"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM5Njg4MDgw", "url": "https://github.com/apache/hadoop/pull/2464#pullrequestreview-539688080", "createdAt": "2020-11-27T03:23:56Z", "commit": {"oid": "00d1328cb85fcb5ebb1778319bc36e7797ae1272"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 17, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yN1QwMzoyMzo1N1rOH6v4zA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yN1QwNDoyNjo0OFrOH6wobw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM2NDA0NA==", "bodyText": "Making methods public for test purposes is not a good idea, esp. for AzureBlobFileSystem class. Find alternative.", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r531364044", "createdAt": "2020-11-27T03:23:57Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java", "diffHunk": "@@ -1218,7 +1218,7 @@ public boolean failed() {\n   }\n \n   @VisibleForTesting\n-  AzureBlobFileSystemStore getAbfsStore() {\n+  public AzureBlobFileSystemStore getAbfsStore() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00d1328cb85fcb5ebb1778319bc36e7797ae1272"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM2NDA3NQ==", "bodyText": "Same as above.", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r531364075", "createdAt": "2020-11-27T03:24:06Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java", "diffHunk": "@@ -1242,7 +1242,7 @@ boolean getIsNamespaceEnabled() throws AzureBlobFileSystemException {\n   }\n \n   @VisibleForTesting\n-  Map<String, Long> getInstrumentationMap() {\n+  public Map<String, Long> getInstrumentationMap() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00d1328cb85fcb5ebb1778319bc36e7797ae1272"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM2NDQwMQ==", "bodyText": "static finals together, non-static together .. no need for new lines above and below for every new field", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r531364401", "createdAt": "2020-11-27T03:25:42Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -47,6 +47,10 @@\n         StreamCapabilities {\n   private static final Logger LOG = LoggerFactory.getLogger(AbfsInputStream.class);\n \n+  public static final int FOOTER_SIZE = 8;\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00d1328cb85fcb5ebb1778319bc36e7797ae1272"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM2NTA0NA==", "bodyText": "Why does validate return an int ? There is a return from what validate does in below method, even if its for error case, its a wrong case.\nreadOneBlock is supposed to written size of the data read, validate as such does no data read.\nChange to return boolean true for success here.", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r531365044", "createdAt": "2020-11-27T03:29:02Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -141,7 +154,7 @@ public synchronized int read(final byte[] b, final int off, final int len) throw\n     return totalReadBytes > 0 ? totalReadBytes : lastReadBytes;\n   }\n \n-  private int readOneBlock(final byte[] b, final int off, final int len) throws IOException {\n+  private int validate(byte[] b, int off, int len) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00d1328cb85fcb5ebb1778319bc36e7797ae1272"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM2NTUzMg==", "bodyText": "validate should return a boolean , true if validation is successful and false if not.\nFor cases it needs to throw exception is already present, so the if check wont be hit. Change to:\nif (!validate(b, off, len)) { return -1; }", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r531365532", "createdAt": "2020-11-27T03:31:15Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -161,6 +174,14 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n     if (off < 0 || len < 0 || len > b.length - off) {\n       throw new IndexOutOfBoundsException();\n     }\n+    return 1; // 1 indicate success\n+  }\n+\n+  private int readOneBlock(final byte[] b, final int off, final int len) throws IOException {\n+    int validation = validate(b, off, len);\n+    if (validation < 1) {\n+      return validation;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00d1328cb85fcb5ebb1778319bc36e7797ae1272"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM2NjQxNg==", "bodyText": "Refer to validate method comments", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r531366416", "createdAt": "2020-11-27T03:35:34Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -211,6 +235,59 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n     return bytesToRead;\n   }\n \n+  private boolean shouldReadFully() {\n+    return this.firstRead && this.context.readSmallFilesCompletely()\n+        && this.contentLength <= this.bufferSize;\n+  }\n+\n+  private boolean shouldReadLastBlock(int len) {\n+    return this.firstRead && this.context.optimizeFooterRead()\n+        && len == FOOTER_SIZE\n+        && this.fCursor == this.contentLength - FOOTER_SIZE;\n+  }\n+\n+  private int readFileCompletely(final byte[] b, final int off, final int len) throws IOException {\n+    int validation = validate(b, off, len);\n+    if (validation < 1) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00d1328cb85fcb5ebb1778319bc36e7797ae1272"}, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM2NzIzOA==", "bodyText": "If read failed due to some reason, say throttling. Will this line still be hit ? Is there a scenario to handle if it doesnt ?", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r531367238", "createdAt": "2020-11-27T03:39:34Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -211,6 +235,59 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n     return bytesToRead;\n   }\n \n+  private boolean shouldReadFully() {\n+    return this.firstRead && this.context.readSmallFilesCompletely()\n+        && this.contentLength <= this.bufferSize;\n+  }\n+\n+  private boolean shouldReadLastBlock(int len) {\n+    return this.firstRead && this.context.optimizeFooterRead()\n+        && len == FOOTER_SIZE\n+        && this.fCursor == this.contentLength - FOOTER_SIZE;\n+  }\n+\n+  private int readFileCompletely(final byte[] b, final int off, final int len) throws IOException {\n+    int validation = validate(b, off, len);\n+    if (validation < 1) {\n+      return validation;\n+    }\n+\n+    buffer = new byte[bufferSize];\n+    bCursor = (int) fCursor;\n+    int bytesRead = readInternal(0, buffer, 0, (int) contentLength, true);\n+    firstRead = false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00d1328cb85fcb5ebb1778319bc36e7797ae1272"}, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM2NzQwOQ==", "bodyText": "Refer to comments in validate.", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r531367409", "createdAt": "2020-11-27T03:40:26Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -211,6 +235,59 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n     return bytesToRead;\n   }\n \n+  private boolean shouldReadFully() {\n+    return this.firstRead && this.context.readSmallFilesCompletely()\n+        && this.contentLength <= this.bufferSize;\n+  }\n+\n+  private boolean shouldReadLastBlock(int len) {\n+    return this.firstRead && this.context.optimizeFooterRead()\n+        && len == FOOTER_SIZE\n+        && this.fCursor == this.contentLength - FOOTER_SIZE;\n+  }\n+\n+  private int readFileCompletely(final byte[] b, final int off, final int len) throws IOException {\n+    int validation = validate(b, off, len);\n+    if (validation < 1) {\n+      return validation;\n+    }\n+\n+    buffer = new byte[bufferSize];\n+    bCursor = (int) fCursor;\n+    int bytesRead = readInternal(0, buffer, 0, (int) contentLength, true);\n+    firstRead = false;\n+    if (bytesRead == -1) {\n+      return -1;\n+    }\n+    fCursorAfterLastRead = fCursor;\n+    limit = bytesRead;\n+    fCursor = bytesRead;\n+    return copyToUserBuffer(b, off, len);\n+  }\n+\n+  private int readLastBlock(final byte[] b, final int off, final int len) throws IOException {\n+    int validation = validate(b, off, len);\n+    if (validation < 1) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00d1328cb85fcb5ebb1778319bc36e7797ae1272"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM2OTk4Nw==", "bodyText": "Validation for read beyond EOF -  fCursor > contentLength - can also be added here", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r531369987", "createdAt": "2020-11-27T03:53:39Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -161,6 +174,14 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n     if (off < 0 || len < 0 || len > b.length - off) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00d1328cb85fcb5ebb1778319bc36e7797ae1272"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM3MTQxOQ==", "bodyText": "In both the new handling, we are bypassing readAheads.\n(Unless readAhead is disabled) if read.request.size is configured to 100MB, data read from store would still be in 4MB chunks as that is the readAhead buffer size.\nIf any of these logics determines that more data from an earlier offset needs to be read, it is triggering reads by bypassing readAheads. So wont it end up in a 100MB direct read to store ?", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r531371419", "createdAt": "2020-11-27T04:00:54Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -211,6 +235,59 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n     return bytesToRead;\n   }\n \n+  private boolean shouldReadFully() {\n+    return this.firstRead && this.context.readSmallFilesCompletely()\n+        && this.contentLength <= this.bufferSize;\n+  }\n+\n+  private boolean shouldReadLastBlock(int len) {\n+    return this.firstRead && this.context.optimizeFooterRead()\n+        && len == FOOTER_SIZE\n+        && this.fCursor == this.contentLength - FOOTER_SIZE;\n+  }\n+\n+  private int readFileCompletely(final byte[] b, final int off, final int len) throws IOException {\n+    int validation = validate(b, off, len);\n+    if (validation < 1) {\n+      return validation;\n+    }\n+\n+    buffer = new byte[bufferSize];\n+    bCursor = (int) fCursor;\n+    int bytesRead = readInternal(0, buffer, 0, (int) contentLength, true);\n+    firstRead = false;\n+    if (bytesRead == -1) {\n+      return -1;\n+    }\n+    fCursorAfterLastRead = fCursor;\n+    limit = bytesRead;\n+    fCursor = bytesRead;\n+    return copyToUserBuffer(b, off, len);\n+  }\n+\n+  private int readLastBlock(final byte[] b, final int off, final int len) throws IOException {\n+    int validation = validate(b, off, len);\n+    if (validation < 1) {\n+      return validation;\n+    }\n+\n+    buffer = new byte[bufferSize];\n+    bCursor = (int) (((contentLength < bufferSize) ? contentLength : bufferSize)\n+        - FOOTER_SIZE);\n+    long lastBlockStartPos = (contentLength < bufferSize)\n+        ? 0\n+        : contentLength - bufferSize;\n+    int bytesRead = readInternal(lastBlockStartPos, buffer, 0, bufferSize,\n+        true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00d1328cb85fcb5ebb1778319bc36e7797ae1272"}, "originalPosition": 125}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM3MTcxMA==", "bodyText": "Shouldnt fCursorAfterLastRead be set after this line ?", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r531371710", "createdAt": "2020-11-27T04:02:31Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -211,6 +235,59 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n     return bytesToRead;\n   }\n \n+  private boolean shouldReadFully() {\n+    return this.firstRead && this.context.readSmallFilesCompletely()\n+        && this.contentLength <= this.bufferSize;\n+  }\n+\n+  private boolean shouldReadLastBlock(int len) {\n+    return this.firstRead && this.context.optimizeFooterRead()\n+        && len == FOOTER_SIZE\n+        && this.fCursor == this.contentLength - FOOTER_SIZE;\n+  }\n+\n+  private int readFileCompletely(final byte[] b, final int off, final int len) throws IOException {\n+    int validation = validate(b, off, len);\n+    if (validation < 1) {\n+      return validation;\n+    }\n+\n+    buffer = new byte[bufferSize];\n+    bCursor = (int) fCursor;\n+    int bytesRead = readInternal(0, buffer, 0, (int) contentLength, true);\n+    firstRead = false;\n+    if (bytesRead == -1) {\n+      return -1;\n+    }\n+    fCursorAfterLastRead = fCursor;\n+    limit = bytesRead;\n+    fCursor = bytesRead;\n+    return copyToUserBuffer(b, off, len);\n+  }\n+\n+  private int readLastBlock(final byte[] b, final int off, final int len) throws IOException {\n+    int validation = validate(b, off, len);\n+    if (validation < 1) {\n+      return validation;\n+    }\n+\n+    buffer = new byte[bufferSize];\n+    bCursor = (int) (((contentLength < bufferSize) ? contentLength : bufferSize)\n+        - FOOTER_SIZE);\n+    long lastBlockStartPos = (contentLength < bufferSize)\n+        ? 0\n+        : contentLength - bufferSize;\n+    int bytesRead = readInternal(lastBlockStartPos, buffer, 0, bufferSize,\n+        true);\n+    firstRead = false;\n+    if (bytesRead == -1) {\n+      return -1;\n+    }\n+    fCursorAfterLastRead = fCursor;\n+    limit = bytesRead;\n+    fCursor = lastBlockStartPos + bytesRead;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00d1328cb85fcb5ebb1778319bc36e7797ae1272"}, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM3MzE0Nw==", "bodyText": "So readInternal will go directly to server to fetch the data and server returning partial data is a valid case. Expectation is that the client will loop over and continue calling read until it has fetched all the data it needs.\nTake a case of client requesting last 8 bytes of a file which here will translate to a read of last 4 MB. ReadInternal returned 3MB data, and so we havent got the last 8 bytes that needs to be returned to client app. bCursor on the other hand has already been set to 4MB - 8 bytes location. This will end up returning what ever is present in the allocated buffer which is corrupt data.\nbytesRead has to be of bufferSize else the whole logic falls apart. As server can return partial data, respective handling is needed.", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r531373147", "createdAt": "2020-11-27T04:10:31Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -211,6 +235,59 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n     return bytesToRead;\n   }\n \n+  private boolean shouldReadFully() {\n+    return this.firstRead && this.context.readSmallFilesCompletely()\n+        && this.contentLength <= this.bufferSize;\n+  }\n+\n+  private boolean shouldReadLastBlock(int len) {\n+    return this.firstRead && this.context.optimizeFooterRead()\n+        && len == FOOTER_SIZE\n+        && this.fCursor == this.contentLength - FOOTER_SIZE;\n+  }\n+\n+  private int readFileCompletely(final byte[] b, final int off, final int len) throws IOException {\n+    int validation = validate(b, off, len);\n+    if (validation < 1) {\n+      return validation;\n+    }\n+\n+    buffer = new byte[bufferSize];\n+    bCursor = (int) fCursor;\n+    int bytesRead = readInternal(0, buffer, 0, (int) contentLength, true);\n+    firstRead = false;\n+    if (bytesRead == -1) {\n+      return -1;\n+    }\n+    fCursorAfterLastRead = fCursor;\n+    limit = bytesRead;\n+    fCursor = bytesRead;\n+    return copyToUserBuffer(b, off, len);\n+  }\n+\n+  private int readLastBlock(final byte[] b, final int off, final int len) throws IOException {\n+    int validation = validate(b, off, len);\n+    if (validation < 1) {\n+      return validation;\n+    }\n+\n+    buffer = new byte[bufferSize];\n+    bCursor = (int) (((contentLength < bufferSize) ? contentLength : bufferSize)\n+        - FOOTER_SIZE);\n+    long lastBlockStartPos = (contentLength < bufferSize)\n+        ? 0\n+        : contentLength - bufferSize;\n+    int bytesRead = readInternal(lastBlockStartPos, buffer, 0, bufferSize,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00d1328cb85fcb5ebb1778319bc36e7797ae1272"}, "originalPosition": 124}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM3MzU0MA==", "bodyText": "For testing of footer read optimized, better to have a file that is atleast 3 or 4 times buffer size. For testing purposes, filesize of 1 mb is good, set the buffer to a much smaller size over config.", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r531373540", "createdAt": "2020-11-27T04:12:53Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsInputStreamReadFooter.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import java.util.Random;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem;\n+\n+import static org.apache.hadoop.fs.azurebfs.AbfsStatistic.CONNECTIONS_MADE;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.ONE_KB;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.ONE_MB;\n+\n+public class ITestAbfsInputStreamReadFooter\n+    extends AbstractAbfsIntegrationTest {\n+\n+  private static final int TEN = 10;\n+  private static final int TWENTY = 20;\n+\n+  public ITestAbfsInputStreamReadFooter() throws Exception {\n+  }\n+\n+  @Test\n+  public void testOnlyOneServerCallIsMadeWhenTheConfIsTrue() throws Exception {\n+    testNumBackendCalls(true);\n+  }\n+\n+  @Test\n+  public void testMultipleServerCallsAreMadeWhenTheConfIsFalse()\n+      throws Exception {\n+    testNumBackendCalls(false);\n+  }\n+\n+  private void testNumBackendCalls(boolean optimizeFooterRead)\n+      throws Exception {\n+    final AzureBlobFileSystem fs = getFileSystem(optimizeFooterRead);\n+    for (int i = 1; i <= 4; i++) {\n+      String fileName = methodName.getMethodName() + i;\n+      int fileSize = i * ONE_MB;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00d1328cb85fcb5ebb1778319bc36e7797ae1272"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM3NDE1Nw==", "bodyText": "This shouldnt be enforced by config. As mentioned in earlier comments, align buffer size and file size for it to become eligible for footer read optimization.\nWe need the test to validate that it goes to footer checks even when the other config is on.", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r531374157", "createdAt": "2020-11-27T04:15:59Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsInputStreamReadFooter.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import java.util.Random;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem;\n+\n+import static org.apache.hadoop.fs.azurebfs.AbfsStatistic.CONNECTIONS_MADE;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.ONE_KB;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.ONE_MB;\n+\n+public class ITestAbfsInputStreamReadFooter\n+    extends AbstractAbfsIntegrationTest {\n+\n+  private static final int TEN = 10;\n+  private static final int TWENTY = 20;\n+\n+  public ITestAbfsInputStreamReadFooter() throws Exception {\n+  }\n+\n+  @Test\n+  public void testOnlyOneServerCallIsMadeWhenTheConfIsTrue() throws Exception {\n+    testNumBackendCalls(true);\n+  }\n+\n+  @Test\n+  public void testMultipleServerCallsAreMadeWhenTheConfIsFalse()\n+      throws Exception {\n+    testNumBackendCalls(false);\n+  }\n+\n+  private void testNumBackendCalls(boolean optimizeFooterRead)\n+      throws Exception {\n+    final AzureBlobFileSystem fs = getFileSystem(optimizeFooterRead);\n+    for (int i = 1; i <= 4; i++) {\n+      String fileName = methodName.getMethodName() + i;\n+      int fileSize = i * ONE_MB;\n+      byte[] fileContent = getRandomBytesArray(fileSize);\n+      Path testFilePath = createFileWithContent(fs, fileName, fileContent);\n+      int length = AbfsInputStream.FOOTER_SIZE;\n+      try (FSDataInputStream iStream = fs.open(testFilePath)) {\n+        byte[] buffer = new byte[length];\n+\n+        Map<String, Long> metricMap = fs.getInstrumentationMap();\n+        long requestsMadeBeforeTest = metricMap\n+            .get(CONNECTIONS_MADE.getStatName());\n+\n+        iStream.seek(fileSize - 8);\n+        iStream.read(buffer, 0, length);\n+\n+        iStream.seek(fileSize - (TEN * ONE_KB));\n+        iStream.read(buffer, 0, length);\n+\n+        iStream.seek(fileSize - (TWENTY * ONE_KB));\n+        iStream.read(buffer, 0, length);\n+\n+        metricMap = fs.getInstrumentationMap();\n+        long requestsMadeAfterTest = metricMap\n+            .get(CONNECTIONS_MADE.getStatName());\n+\n+        if (optimizeFooterRead) {\n+          assertEquals(1, requestsMadeAfterTest - requestsMadeBeforeTest);\n+        } else {\n+          assertEquals(3, requestsMadeAfterTest - requestsMadeBeforeTest);\n+        }\n+      }\n+    }\n+  }\n+\n+  @Test\n+  public void testSeekToEndAndReadWithConfTrue() throws Exception {\n+    testSeekToEndAndReadWithConf(true);\n+  }\n+\n+  @Test\n+  public void testSeekToEndAndReadWithConfFalse() throws Exception {\n+    testSeekToEndAndReadWithConf(false);\n+  }\n+\n+  private void testSeekToEndAndReadWithConf(boolean optimizeFooterRead) throws Exception {\n+    final AzureBlobFileSystem fs = getFileSystem(optimizeFooterRead);\n+    for (int i = 5; i <= 10; i++) {\n+      String fileName = methodName.getMethodName() + i;\n+      int fileSize = i * ONE_MB;\n+      byte[] fileContent = getRandomBytesArray(fileSize);\n+      Path testFilePath = createFileWithContent(fs, fileName, fileContent);\n+      seekReadAndTest(fs, testFilePath, fileSize - AbfsInputStream.FOOTER_SIZE,\n+          AbfsInputStream.FOOTER_SIZE, fileContent);\n+    }\n+  }\n+\n+  private AzureBlobFileSystem getFileSystem(boolean optimizeFooterRead)\n+      throws IOException {\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    fs.getAbfsStore().getAbfsConfiguration()\n+        .setOptimizeFooterRead(optimizeFooterRead);\n+    fs.getAbfsStore().getAbfsConfiguration()\n+        .setReadSmallFilesCompletely(false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00d1328cb85fcb5ebb1778319bc36e7797ae1272"}, "originalPosition": 125}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM3NDU5Mw==", "bodyText": "File content asserts are needed on the data that is returned through the client provided buffer as well.", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r531374593", "createdAt": "2020-11-27T04:18:19Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsInputStreamReadFooter.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import java.util.Random;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem;\n+\n+import static org.apache.hadoop.fs.azurebfs.AbfsStatistic.CONNECTIONS_MADE;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.ONE_KB;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.ONE_MB;\n+\n+public class ITestAbfsInputStreamReadFooter\n+    extends AbstractAbfsIntegrationTest {\n+\n+  private static final int TEN = 10;\n+  private static final int TWENTY = 20;\n+\n+  public ITestAbfsInputStreamReadFooter() throws Exception {\n+  }\n+\n+  @Test\n+  public void testOnlyOneServerCallIsMadeWhenTheConfIsTrue() throws Exception {\n+    testNumBackendCalls(true);\n+  }\n+\n+  @Test\n+  public void testMultipleServerCallsAreMadeWhenTheConfIsFalse()\n+      throws Exception {\n+    testNumBackendCalls(false);\n+  }\n+\n+  private void testNumBackendCalls(boolean optimizeFooterRead)\n+      throws Exception {\n+    final AzureBlobFileSystem fs = getFileSystem(optimizeFooterRead);\n+    for (int i = 1; i <= 4; i++) {\n+      String fileName = methodName.getMethodName() + i;\n+      int fileSize = i * ONE_MB;\n+      byte[] fileContent = getRandomBytesArray(fileSize);\n+      Path testFilePath = createFileWithContent(fs, fileName, fileContent);\n+      int length = AbfsInputStream.FOOTER_SIZE;\n+      try (FSDataInputStream iStream = fs.open(testFilePath)) {\n+        byte[] buffer = new byte[length];\n+\n+        Map<String, Long> metricMap = fs.getInstrumentationMap();\n+        long requestsMadeBeforeTest = metricMap\n+            .get(CONNECTIONS_MADE.getStatName());\n+\n+        iStream.seek(fileSize - 8);\n+        iStream.read(buffer, 0, length);\n+\n+        iStream.seek(fileSize - (TEN * ONE_KB));\n+        iStream.read(buffer, 0, length);\n+\n+        iStream.seek(fileSize - (TWENTY * ONE_KB));\n+        iStream.read(buffer, 0, length);\n+\n+        metricMap = fs.getInstrumentationMap();\n+        long requestsMadeAfterTest = metricMap\n+            .get(CONNECTIONS_MADE.getStatName());\n+\n+        if (optimizeFooterRead) {\n+          assertEquals(1, requestsMadeAfterTest - requestsMadeBeforeTest);\n+        } else {\n+          assertEquals(3, requestsMadeAfterTest - requestsMadeBeforeTest);\n+        }\n+      }\n+    }\n+  }\n+\n+  @Test\n+  public void testSeekToEndAndReadWithConfTrue() throws Exception {\n+    testSeekToEndAndReadWithConf(true);\n+  }\n+\n+  @Test\n+  public void testSeekToEndAndReadWithConfFalse() throws Exception {\n+    testSeekToEndAndReadWithConf(false);\n+  }\n+\n+  private void testSeekToEndAndReadWithConf(boolean optimizeFooterRead) throws Exception {\n+    final AzureBlobFileSystem fs = getFileSystem(optimizeFooterRead);\n+    for (int i = 5; i <= 10; i++) {\n+      String fileName = methodName.getMethodName() + i;\n+      int fileSize = i * ONE_MB;\n+      byte[] fileContent = getRandomBytesArray(fileSize);\n+      Path testFilePath = createFileWithContent(fs, fileName, fileContent);\n+      seekReadAndTest(fs, testFilePath, fileSize - AbfsInputStream.FOOTER_SIZE,\n+          AbfsInputStream.FOOTER_SIZE, fileContent);\n+    }\n+  }\n+\n+  private AzureBlobFileSystem getFileSystem(boolean optimizeFooterRead)\n+      throws IOException {\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    fs.getAbfsStore().getAbfsConfiguration()\n+        .setOptimizeFooterRead(optimizeFooterRead);\n+    fs.getAbfsStore().getAbfsConfiguration()\n+        .setReadSmallFilesCompletely(false);\n+    return fs;\n+  }\n+\n+  private Path createFileWithContent(FileSystem fs, String fileName,\n+      byte[] fileContent) throws IOException {\n+    Path testFilePath = path(fileName);\n+    try (FSDataOutputStream oStream = fs.create(testFilePath)) {\n+      oStream.write(fileContent);\n+      oStream.flush();\n+    }\n+    return testFilePath;\n+  }\n+\n+  private void seekReadAndTest(final FileSystem fs, final Path testFilePath,\n+      final int seekPos, final int length, final byte[] fileContent) throws IOException {\n+    try (FSDataInputStream iStream = fs.open(testFilePath)) {\n+      iStream.seek(seekPos);\n+      byte[] buffer = new byte[length];\n+      iStream.read(buffer, 0, length);\n+      assertSuccessfulRead(fileContent, seekPos, length, buffer);\n+      AbfsInputStream abfsInputStream = (AbfsInputStream) iStream\n+          .getWrappedStream();\n+\n+      AzureBlobFileSystem abfs = (AzureBlobFileSystem) fs;\n+      AbfsConfiguration conf = abfs.getAbfsStore().getAbfsConfiguration();\n+\n+      int expectedFCursor = fileContent.length;\n+      int expectedLimit;\n+      int expectedBCursor;\n+      if (conf.optimizeFooterRead()) {\n+        expectedBCursor = ((conf.getReadBufferSize() < fileContent.length)\n+            ? conf.getReadBufferSize()\n+            : fileContent.length);\n+        expectedLimit = (conf.getReadBufferSize() < fileContent.length)\n+            ? conf.getReadBufferSize()\n+            : fileContent.length;\n+      } else {\n+        expectedBCursor = length;\n+        expectedLimit = length;\n+      }\n+      assertSuccessfulRead(fileContent, abfsInputStream.getBuffer(),\n+          conf, length);\n+      assertEquals(expectedFCursor, abfsInputStream.getFCursor());\n+      assertEquals(expectedBCursor, abfsInputStream.getBCursor());\n+      assertEquals(expectedLimit, abfsInputStream.getLimit());\n+    }\n+  }\n+\n+  private void assertSuccessfulRead(byte[] actualFileContent,\n+      byte[] contentRead, AbfsConfiguration conf, int len) {\n+    int buffersize = conf.getReadBufferSize();\n+    int actualContentSize = actualFileContent.length;\n+    if (conf.optimizeFooterRead()) {\n+    len = (actualContentSize < buffersize)\n+        ? actualContentSize\n+        : buffersize;\n+    }\n+    assertSuccessfulRead(actualFileContent, actualContentSize - len, len,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00d1328cb85fcb5ebb1778319bc36e7797ae1272"}, "originalPosition": 183}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM3NTIxNA==", "bodyText": "I recall it was discussed that small file criteria will be capped at 4Mb file size. I dont see that in the review. Was it scratched back to small file being one less than buffer size ?", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r531375214", "createdAt": "2020-11-27T04:21:13Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java", "diffHunk": "@@ -517,6 +527,14 @@ public int getWriteBufferSize() {\n     return this.writeBufferSize;\n   }\n \n+  public boolean readSmallFilesCompletely() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00d1328cb85fcb5ebb1778319bc36e7797ae1272"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM3NjIzOQ==", "bodyText": "Run read tests in particular and contract test on a whole multiple times to check for any random failures.", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r531376239", "createdAt": "2020-11-27T04:26:48Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsInputStreamReadFooter.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/**", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00d1328cb85fcb5ebb1778319bc36e7797ae1272"}, "originalPosition": 1}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5bb5784b34d53b721f2b06795ceb8ecef14f097f", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/5bb5784b34d53b721f2b06795ceb8ecef14f097f", "committedDate": "2020-12-01T04:15:09Z", "message": "Cleaning the test cases"}, "afterCommit": {"oid": "ce1777e33efe34c6af70ec7a4a49fa1b82330610", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/ce1777e33efe34c6af70ec7a4a49fa1b82330610", "committedDate": "2020-12-01T04:29:16Z", "message": "Cleaning the test cases"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ad90fb96a6e58a1180c981b6ba4977a93355b1c7", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/ad90fb96a6e58a1180c981b6ba4977a93355b1c7", "committedDate": "2020-12-01T06:24:32Z", "message": "Cleaning the test cases"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ce1777e33efe34c6af70ec7a4a49fa1b82330610", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/ce1777e33efe34c6af70ec7a4a49fa1b82330610", "committedDate": "2020-12-01T04:29:16Z", "message": "Cleaning the test cases"}, "afterCommit": {"oid": "ad90fb96a6e58a1180c981b6ba4977a93355b1c7", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/ad90fb96a6e58a1180c981b6ba4977a93355b1c7", "committedDate": "2020-12-01T06:24:32Z", "message": "Cleaning the test cases"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b92a52701746e5ca09d02ce73fa691c04aa89bf8", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/b92a52701746e5ca09d02ce73fa691c04aa89bf8", "committedDate": "2020-12-01T15:35:06Z", "message": "Merge branch 'trunk' into readoptimizations"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "08c504b57712536485462c8ff0cbd853f9748bd6", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/08c504b57712536485462c8ff0cbd853f9748bd6", "committedDate": "2020-12-01T16:42:15Z", "message": "Addressing eview comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQzNjU1NzYw", "url": "https://github.com/apache/hadoop/pull/2464#pullrequestreview-543655760", "createdAt": "2020-12-03T07:03:02Z", "commit": {"oid": "08c504b57712536485462c8ff0cbd853f9748bd6"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wM1QwNzowMzowM1rOH9_toQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wM1QxMToyNTo1OVrOH-VyOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDc2OTA1Nw==", "bodyText": "In that case, skipping readAhead wont be ideal when reads are done by means of this optimizations. if 100 MB is set as buffer size, current logic is forcing a single server read request for 100 MB.", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r534769057", "createdAt": "2020-12-03T07:03:03Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java", "diffHunk": "@@ -517,6 +527,14 @@ public int getWriteBufferSize() {\n     return this.writeBufferSize;\n   }\n \n+  public boolean readSmallFilesCompletely() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM3NTIxNA=="}, "originalCommit": {"oid": "00d1328cb85fcb5ebb1778319bc36e7797ae1272"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTEyOTU3OA==", "bodyText": "Returning int from validate which is used by caller as bytes read is wrong. Any failure in validate should return Boolean false and the caller in turn can convert it to -1 for bytes read.", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r535129578", "createdAt": "2020-12-03T11:24:55Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -141,7 +154,7 @@ public synchronized int read(final byte[] b, final int off, final int len) throw\n     return totalReadBytes > 0 ? totalReadBytes : lastReadBytes;\n   }\n \n-  private int readOneBlock(final byte[] b, final int off, final int len) throws IOException {\n+  private int validate(byte[] b, int off, int len) throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM2NTA0NA=="}, "originalCommit": {"oid": "00d1328cb85fcb5ebb1778319bc36e7797ae1272"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTEzMDY4MA==", "bodyText": "see comment above", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r535130680", "createdAt": "2020-12-03T11:25:59Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -161,6 +174,14 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n     if (off < 0 || len < 0 || len > b.length - off) {\n       throw new IndexOutOfBoundsException();\n     }\n+    return 1; // 1 indicate success\n+  }\n+\n+  private int readOneBlock(final byte[] b, final int off, final int len) throws IOException {\n+    int validation = validate(b, off, len);\n+    if (validation < 1) {\n+      return validation;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM2NTUzMg=="}, "originalCommit": {"oid": "00d1328cb85fcb5ebb1778319bc36e7797ae1272"}, "originalPosition": 62}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ0NjIzODEw", "url": "https://github.com/apache/hadoop/pull/2464#pullrequestreview-544623810", "createdAt": "2020-12-04T03:02:48Z", "commit": {"oid": "08c504b57712536485462c8ff0cbd853f9748bd6"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "319a79f0046102168f51f113238443e1aeca8f02", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/319a79f0046102168f51f113238443e1aeca8f02", "committedDate": "2020-12-09T03:58:50Z", "message": "Addressing review comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3OTU1MDQ5", "url": "https://github.com/apache/hadoop/pull/2464#pullrequestreview-547955049", "createdAt": "2020-12-09T08:45:58Z", "commit": {"oid": "319a79f0046102168f51f113238443e1aeca8f02"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3239a8c95203adae381db68b6a9fb0bcbdcd79db", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/3239a8c95203adae381db68b6a9fb0bcbdcd79db", "committedDate": "2020-12-10T05:45:25Z", "message": "Setting firstRead to false after the existing flow"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5MDU4MzYw", "url": "https://github.com/apache/hadoop/pull/2464#pullrequestreview-549058360", "createdAt": "2020-12-10T10:31:05Z", "commit": {"oid": "3239a8c95203adae381db68b6a9fb0bcbdcd79db"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxMDozMTowNlrOIDCWYA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxMToxNDoyNFrOIDEGQA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDA1NTEzNg==", "bodyText": "validate logic should be grouped into a method and be called from here so that code duplication can be avoided in the 3 flows below.", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r540055136", "createdAt": "2020-12-10T10:31:06Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -137,7 +142,13 @@ public synchronized int read(final byte[] b, final int off, final int len) throw\n     }\n     incrementReadOps();\n     do {\n-      lastReadBytes = readOneBlock(b, currentOff, currentLen);\n+      if (shouldReadFully()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3239a8c95203adae381db68b6a9fb0bcbdcd79db"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDA4MjcwMQ==", "bodyText": "As discussed, this logic needs to be relooked for\n\nWhat if the requested data is already available in the partial read done ?\nReduce the loopCount as the retry logic on ABFS driver can make the client read overall expensive. Fail faster with just 1 or 2 tries.\nNever fail the read request because optimization code failed to read full file. Fail fast and send read for client requested position.\nAll pointer fields need to be in valid state (bCursor, fCursor, fCursorAfterLastRead). In failure case currently fCursor could be in a different position that last seek done.", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r540082701", "createdAt": "2020-12-10T11:12:32Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -224,6 +240,123 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n     return bytesToRead;\n   }\n \n+  private boolean shouldReadFully() {\n+    return this.firstRead && this.context.readSmallFilesCompletely()\n+        && this.contentLength <= this.bufferSize;\n+  }\n+\n+  private boolean shouldReadLastBlock(int len) {\n+    return this.firstRead && this.context.optimizeFooterRead()\n+        && len == FOOTER_SIZE\n+        && this.fCursor == this.contentLength - FOOTER_SIZE;\n+  }\n+\n+  private int readFileCompletely(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (closed) {\n+      throw new IOException(FSExceptionMessages.STREAM_IS_CLOSED);\n+    }\n+\n+    Preconditions.checkNotNull(b);\n+    LOG.debug(\"read one block requested b.length = {} off {} len {}\", b.length,\n+        off, len);\n+\n+    if (len == 0) {\n+      return 0;\n+    }\n+\n+    if (this.available() == 0) {\n+      return -1;\n+    }\n+\n+    if (off < 0 || len < 0 || len > b.length - off) {\n+      throw new IndexOutOfBoundsException();\n+    }\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor, bCursor has\n+    // to be the current fCusor\n+    bCursor = (int) fCursor;\n+    fCursorAfterLastRead = fCursor;\n+    int totalBytesRead = 0;\n+    int loopCount = 0;\n+    // Read from begining\n+    fCursor = 0;\n+    while (fCursor < contentLength) {\n+      int bytesRead = readInternal(fCursor, buffer, limit,\n+          (int) contentLength - limit, true);\n+      if (bytesRead > 0) {\n+        totalBytesRead += bytesRead;\n+        limit += bytesRead;\n+        fCursor += bytesRead;\n+      }\n+      if (loopCount++ >= 10) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3239a8c95203adae381db68b6a9fb0bcbdcd79db"}, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDA4Mzc3Ng==", "bodyText": "Comments added in readSmallFileCompletely will apply here too.\nAlso, failure state recovery might lead to throwing away the data that was retried till then. Using readAhead threads to read will help to save the partial data read and be of use for next read call.", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r540083776", "createdAt": "2020-12-10T11:14:24Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -224,6 +240,123 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n     return bytesToRead;\n   }\n \n+  private boolean shouldReadFully() {\n+    return this.firstRead && this.context.readSmallFilesCompletely()\n+        && this.contentLength <= this.bufferSize;\n+  }\n+\n+  private boolean shouldReadLastBlock(int len) {\n+    return this.firstRead && this.context.optimizeFooterRead()\n+        && len == FOOTER_SIZE\n+        && this.fCursor == this.contentLength - FOOTER_SIZE;\n+  }\n+\n+  private int readFileCompletely(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (closed) {\n+      throw new IOException(FSExceptionMessages.STREAM_IS_CLOSED);\n+    }\n+\n+    Preconditions.checkNotNull(b);\n+    LOG.debug(\"read one block requested b.length = {} off {} len {}\", b.length,\n+        off, len);\n+\n+    if (len == 0) {\n+      return 0;\n+    }\n+\n+    if (this.available() == 0) {\n+      return -1;\n+    }\n+\n+    if (off < 0 || len < 0 || len > b.length - off) {\n+      throw new IndexOutOfBoundsException();\n+    }\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor, bCursor has\n+    // to be the current fCusor\n+    bCursor = (int) fCursor;\n+    fCursorAfterLastRead = fCursor;\n+    int totalBytesRead = 0;\n+    int loopCount = 0;\n+    // Read from begining\n+    fCursor = 0;\n+    while (fCursor < contentLength) {\n+      int bytesRead = readInternal(fCursor, buffer, limit,\n+          (int) contentLength - limit, true);\n+      if (bytesRead > 0) {\n+        totalBytesRead += bytesRead;\n+        limit += bytesRead;\n+        fCursor += bytesRead;\n+      }\n+      if (loopCount++ >= 10) {\n+        throw new IOException(\n+            \"Too many attempts in reading whole file \" + path);\n+      }\n+    }\n+    firstRead = false;\n+    if (totalBytesRead == -1) {\n+      return -1;\n+    }\n+    return copyToUserBuffer(b, off, len);\n+  }\n+\n+  private int readLastBlock(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (closed) {\n+      throw new IOException(FSExceptionMessages.STREAM_IS_CLOSED);\n+    }\n+\n+    Preconditions.checkNotNull(b);\n+    LOG.debug(\"read one block requested b.length = {} off {} len {}\", b.length,\n+        off, len);\n+\n+    if (len == 0) {\n+      return 0;\n+    }\n+\n+    if (this.available() == 0) {\n+      return -1;\n+    }\n+\n+    if (off < 0 || len < 0 || len > b.length - off) {\n+      throw new IndexOutOfBoundsException();\n+    }\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor, for small\n+    // files the bCursor will be contentlength - footer size,\n+    // otherwise buffersize - footer size\n+    bCursor = (int) (Math.min(contentLength, bufferSize) - FOOTER_SIZE);\n+    // read API call is considered 1 single operation in reality server could\n+    // return partial data and client has to retry untill the last full block\n+    // is read. So setting the fCursorAfterLastRead before the possible\n+    // multiple server calls\n+    fCursorAfterLastRead = fCursor;\n+    // 0 if contentlength is < buffersize\n+    fCursor = Math.max(0, contentLength - bufferSize);\n+    int totalBytesRead = 0;\n+    int loopCount = 0;\n+    while (fCursor < contentLength) {\n+      int bytesRead = readInternal(fCursor, buffer, limit, bufferSize - limit,\n+          true);\n+      if (bytesRead > 0) {\n+        totalBytesRead += bytesRead;\n+        limit += bytesRead;\n+        fCursor += bytesRead;\n+      }\n+      if (loopCount++ >= 10) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3239a8c95203adae381db68b6a9fb0bcbdcd79db"}, "originalPosition": 187}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b223ce5f9e6aa140b3681a42957aa654e2215997", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/b223ce5f9e6aa140b3681a42957aa654e2215997", "committedDate": "2020-12-10T13:25:35Z", "message": "Improved validate method"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a75bb1065463fd44d9846c898c83d63cf6d74c2f", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/a75bb1065463fd44d9846c898c83d63cf6d74c2f", "committedDate": "2020-12-10T13:52:50Z", "message": "Improved method for optimized reads"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a268eaecda8a49f1cc2bba78fc1f47ad30b9c0e5", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/a268eaecda8a49f1cc2bba78fc1f47ad30b9c0e5", "committedDate": "2020-12-10T17:18:33Z", "message": "Saving current state of pointers for better resilience"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5daee36b17c86a3c8637773b07b334be6dda7ce9", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/5daee36b17c86a3c8637773b07b334be6dda7ce9", "committedDate": "2020-12-11T13:39:05Z", "message": "Partial read scenarios"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "baddc1d7bfb70513e348fe08659411547d848c64", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/baddc1d7bfb70513e348fe08659411547d848c64", "committedDate": "2020-12-11T13:45:31Z", "message": "Merge branch 'trunk' into readoptimizations"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "59682d79753d3e09016276b1d4d24f345aa42daa", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/59682d79753d3e09016276b1d4d24f345aa42daa", "committedDate": "2020-12-11T15:17:10Z", "message": "Checkstye fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a61983f749b25a416ca19006318a7e34901b694a", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/a61983f749b25a416ca19006318a7e34901b694a", "committedDate": "2020-12-14T10:29:13Z", "message": "Making changes to the footer read logic"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1a358d068bd4e93e99b6dc0f6aa234aa2d585975", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/1a358d068bd4e93e99b6dc0f6aa234aa2d585975", "committedDate": "2020-12-14T16:07:50Z", "message": "Adding more test cases"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUyMTYyMzcz", "url": "https://github.com/apache/hadoop/pull/2464#pullrequestreview-552162373", "createdAt": "2020-12-15T07:17:27Z", "commit": {"oid": "1a358d068bd4e93e99b6dc0f6aa234aa2d585975"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwNzoxNzoyN1rOIF8Sfw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwNzoxNzoyN1rOIF8Sfw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzEwMTU2Nw==", "bodyText": "Add a comment that footer size is set to qualify for both ORC and parquet files.", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r543101567", "createdAt": "2020-12-15T07:17:27Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -46,6 +50,7 @@\n public class AbfsInputStream extends FSInputStream implements CanUnbuffer,\n         StreamCapabilities {\n   private static final Logger LOG = LoggerFactory.getLogger(AbfsInputStream.class);\n+  public static final int FOOTER_SIZE = 16 * ONE_KB;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a358d068bd4e93e99b6dc0f6aa234aa2d585975"}, "originalPosition": 15}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUyMTY5NTQ2", "url": "https://github.com/apache/hadoop/pull/2464#pullrequestreview-552169546", "createdAt": "2020-12-15T07:30:40Z", "commit": {"oid": "1a358d068bd4e93e99b6dc0f6aa234aa2d585975"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwNzozMDo0MVrOIF8s0g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwNzozMDo0MVrOIF8s0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzEwODMwNg==", "bodyText": "2 => field name", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r543108306", "createdAt": "2020-12-15T07:30:41Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -206,11 +229,120 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n       fCursor += bytesRead;\n       fCursorAfterLastRead = fCursor;\n     }\n+    return copyToUserBuffer(b, off, len);\n+  }\n+\n+  private int readFileCompletely(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor, bCursor has\n+    // to be the current fCusor\n+    bCursor = (int) fCursor;\n+    return optimisedRead(b, off, len, 0, contentLength);\n+  }\n+\n+  private int readLastBlock(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor,\n+    // AbfsInutStream buffer is going p contain dta from footer start. In\n+    // that case bCursor will be set to footerStart - fCursor\n+    long lastBlockStart = max(0, contentLength - bufferSize);\n+    bCursor = (int) (fCursor - lastBlockStart);\n+    // read API call is considered 1 single operation in reality server could\n+    // return partial data and client has to retry untill the last full block\n+    // is read. So setting the fCursorAfterLastRead before the possible\n+    // multiple server calls\n+    fCursorAfterLastRead = fCursor;\n+    // 0 if contentlength is < buffersize\n+    long actualLenToRead = min(bufferSize, contentLength);\n+    return optimisedRead(b, off, len, lastBlockStart, actualLenToRead);\n+  }\n+\n+  private int optimisedRead(final byte[] b, final int off, final int len,\n+      final long readFrom, final long actualLen) throws IOException {\n+    int totalBytesRead = 0;\n+    fCursor = readFrom;\n+    for (int i = 0; i < 2 && fCursor < contentLength; i++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a358d068bd4e93e99b6dc0f6aa234aa2d585975"}, "originalPosition": 175}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUyMTY5NjAy", "url": "https://github.com/apache/hadoop/pull/2464#pullrequestreview-552169602", "createdAt": "2020-12-15T07:30:47Z", "commit": {"oid": "1a358d068bd4e93e99b6dc0f6aa234aa2d585975"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwNzozMDo0OFrOIF8tAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwNzozMDo0OFrOIF8tAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzEwODM1Mw==", "bodyText": "re-write the comment", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r543108353", "createdAt": "2020-12-15T07:30:48Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -206,11 +229,120 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n       fCursor += bytesRead;\n       fCursorAfterLastRead = fCursor;\n     }\n+    return copyToUserBuffer(b, off, len);\n+  }\n+\n+  private int readFileCompletely(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor, bCursor has\n+    // to be the current fCusor\n+    bCursor = (int) fCursor;\n+    return optimisedRead(b, off, len, 0, contentLength);\n+  }\n+\n+  private int readLastBlock(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor,\n+    // AbfsInutStream buffer is going p contain dta from footer start. In\n+    // that case bCursor will be set to footerStart - fCursor\n+    long lastBlockStart = max(0, contentLength - bufferSize);\n+    bCursor = (int) (fCursor - lastBlockStart);\n+    // read API call is considered 1 single operation in reality server could\n+    // return partial data and client has to retry untill the last full block\n+    // is read. So setting the fCursorAfterLastRead before the possible\n+    // multiple server calls\n+    fCursorAfterLastRead = fCursor;\n+    // 0 if contentlength is < buffersize\n+    long actualLenToRead = min(bufferSize, contentLength);\n+    return optimisedRead(b, off, len, lastBlockStart, actualLenToRead);\n+  }\n+\n+  private int optimisedRead(final byte[] b, final int off, final int len,\n+      final long readFrom, final long actualLen) throws IOException {\n+    int totalBytesRead = 0;\n+    fCursor = readFrom;\n+    for (int i = 0; i < 2 && fCursor < contentLength; i++) {\n+      int bytesRead = readInternal(fCursor, buffer, limit,\n+          (int) actualLen - limit, true);\n+      if (bytesRead > 0) {\n+        totalBytesRead += bytesRead;\n+        limit += bytesRead;\n+        fCursor += bytesRead;\n+      }\n+    }\n+    //  if the read was not success and the user requested part of data has", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a358d068bd4e93e99b6dc0f6aa234aa2d585975"}, "originalPosition": 184}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUyMTY5OTg2", "url": "https://github.com/apache/hadoop/pull/2464#pullrequestreview-552169986", "createdAt": "2020-12-15T07:31:29Z", "commit": {"oid": "1a358d068bd4e93e99b6dc0f6aa234aa2d585975"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwNzozMToyOVrOIF8uag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwNzozMToyOVrOIF8uag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzEwODcxNA==", "bodyText": "In failure cases, file pointers can be in incorrect position. Need proper restore.", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r543108714", "createdAt": "2020-12-15T07:31:29Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -206,11 +229,120 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n       fCursor += bytesRead;\n       fCursorAfterLastRead = fCursor;\n     }\n+    return copyToUserBuffer(b, off, len);\n+  }\n+\n+  private int readFileCompletely(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor, bCursor has\n+    // to be the current fCusor\n+    bCursor = (int) fCursor;\n+    return optimisedRead(b, off, len, 0, contentLength);\n+  }\n+\n+  private int readLastBlock(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor,\n+    // AbfsInutStream buffer is going p contain dta from footer start. In\n+    // that case bCursor will be set to footerStart - fCursor\n+    long lastBlockStart = max(0, contentLength - bufferSize);\n+    bCursor = (int) (fCursor - lastBlockStart);\n+    // read API call is considered 1 single operation in reality server could\n+    // return partial data and client has to retry untill the last full block\n+    // is read. So setting the fCursorAfterLastRead before the possible\n+    // multiple server calls\n+    fCursorAfterLastRead = fCursor;\n+    // 0 if contentlength is < buffersize\n+    long actualLenToRead = min(bufferSize, contentLength);\n+    return optimisedRead(b, off, len, lastBlockStart, actualLenToRead);\n+  }\n+\n+  private int optimisedRead(final byte[] b, final int off, final int len,\n+      final long readFrom, final long actualLen) throws IOException {\n+    int totalBytesRead = 0;\n+    fCursor = readFrom;\n+    for (int i = 0; i < 2 && fCursor < contentLength; i++) {\n+      int bytesRead = readInternal(fCursor, buffer, limit,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a358d068bd4e93e99b6dc0f6aa234aa2d585975"}, "originalPosition": 176}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUyMTcyMDE2", "url": "https://github.com/apache/hadoop/pull/2464#pullrequestreview-552172016", "createdAt": "2020-12-15T07:34:46Z", "commit": {"oid": "1a358d068bd4e93e99b6dc0f6aa234aa2d585975"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2c647c58ef2598549c38b74e665116e76d4d6985", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/2c647c58ef2598549c38b74e665116e76d4d6985", "committedDate": "2020-12-15T08:56:10Z", "message": "Addressing review comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUzMzI4NjI4", "url": "https://github.com/apache/hadoop/pull/2464#pullrequestreview-553328628", "createdAt": "2020-12-16T03:22:42Z", "commit": {"oid": "2c647c58ef2598549c38b74e665116e76d4d6985"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQwMzoyMjo0M1rOIGr5YA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQwMzoyMjo0M1rOIGr5YA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mzg4MTU2OA==", "bodyText": "Fix comments", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r543881568", "createdAt": "2020-12-16T03:22:43Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -206,11 +231,127 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n       fCursor += bytesRead;\n       fCursorAfterLastRead = fCursor;\n     }\n+    return copyToUserBuffer(b, off, len);\n+  }\n \n+  private int readFileCompletely(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor, bCursor has\n+    // to be the current fCusor\n+    bCursor = (int) fCursor;\n+    return optimisedRead(b, off, len, 0, contentLength);\n+  }\n+\n+  private int readLastBlock(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor,\n+    // AbfsInutStream buffer is going p contain dta from footer start. In", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c647c58ef2598549c38b74e665116e76d4d6985"}, "originalPosition": 159}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUzMzMxMTc3", "url": "https://github.com/apache/hadoop/pull/2464#pullrequestreview-553331177", "createdAt": "2020-12-16T03:30:43Z", "commit": {"oid": "2c647c58ef2598549c38b74e665116e76d4d6985"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQwMzozMDo0M1rOIGsR9w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQwMzozMDo0M1rOIGsR9w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mzg4Nzg2Mw==", "bodyText": "Rename OPTIMIZATION_ATTEMPTS to MAX_OPTIMIZED_READ_ATTEMPTS", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r543887863", "createdAt": "2020-12-16T03:30:43Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -206,11 +231,127 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n       fCursor += bytesRead;\n       fCursorAfterLastRead = fCursor;\n     }\n+    return copyToUserBuffer(b, off, len);\n+  }\n \n+  private int readFileCompletely(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor, bCursor has\n+    // to be the current fCusor\n+    bCursor = (int) fCursor;\n+    return optimisedRead(b, off, len, 0, contentLength);\n+  }\n+\n+  private int readLastBlock(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor,\n+    // AbfsInutStream buffer is going p contain dta from footer start. In\n+    // that case bCursor will be set to footerStart - fCursor\n+    long lastBlockStart = max(0, contentLength - bufferSize);\n+    bCursor = (int) (fCursor - lastBlockStart);\n+    // read API call is considered 1 single operation in reality server could\n+    // return partial data and client has to retry untill the last full block\n+    // is read. So setting the fCursorAfterLastRead before the possible\n+    // multiple server calls\n+    fCursorAfterLastRead = fCursor;\n+    // 0 if contentlength is < buffersize\n+    long actualLenToRead = min(bufferSize, contentLength);\n+    return optimisedRead(b, off, len, lastBlockStart, actualLenToRead);\n+  }\n+\n+  private int optimisedRead(final byte[] b, final int off, final int len,\n+      final long readFrom, final long actualLen) throws IOException {\n+    int totalBytesRead = 0;\n+    fCursor = readFrom;\n+    try {\n+      for (int i = 0;\n+           i < OPTIMIZATION_ATTEMPTS && fCursor < contentLength; i++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c647c58ef2598549c38b74e665116e76d4d6985"}, "originalPosition": 179}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUzMzM1OTUx", "url": "https://github.com/apache/hadoop/pull/2464#pullrequestreview-553335951", "createdAt": "2020-12-16T03:46:41Z", "commit": {"oid": "2c647c58ef2598549c38b74e665116e76d4d6985"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQwMzo0Njo0MVrOIGs-_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQwMzo0Njo0MVrOIGs-_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mzg5OTM5MA==", "bodyText": "\"Optimized read failed. Defaulting to readOneBlock\"", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r543899390", "createdAt": "2020-12-16T03:46:41Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -206,11 +231,127 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n       fCursor += bytesRead;\n       fCursorAfterLastRead = fCursor;\n     }\n+    return copyToUserBuffer(b, off, len);\n+  }\n \n+  private int readFileCompletely(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor, bCursor has\n+    // to be the current fCusor\n+    bCursor = (int) fCursor;\n+    return optimisedRead(b, off, len, 0, contentLength);\n+  }\n+\n+  private int readLastBlock(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor,\n+    // AbfsInutStream buffer is going p contain dta from footer start. In\n+    // that case bCursor will be set to footerStart - fCursor\n+    long lastBlockStart = max(0, contentLength - bufferSize);\n+    bCursor = (int) (fCursor - lastBlockStart);\n+    // read API call is considered 1 single operation in reality server could\n+    // return partial data and client has to retry untill the last full block\n+    // is read. So setting the fCursorAfterLastRead before the possible\n+    // multiple server calls\n+    fCursorAfterLastRead = fCursor;\n+    // 0 if contentlength is < buffersize\n+    long actualLenToRead = min(bufferSize, contentLength);\n+    return optimisedRead(b, off, len, lastBlockStart, actualLenToRead);\n+  }\n+\n+  private int optimisedRead(final byte[] b, final int off, final int len,\n+      final long readFrom, final long actualLen) throws IOException {\n+    int totalBytesRead = 0;\n+    fCursor = readFrom;\n+    try {\n+      for (int i = 0;\n+           i < OPTIMIZATION_ATTEMPTS && fCursor < contentLength; i++) {\n+        int bytesRead = readInternal(fCursor, buffer, limit,\n+            (int) actualLen - limit, true);\n+        if (bytesRead > 0) {\n+          totalBytesRead += bytesRead;\n+          limit += bytesRead;\n+          fCursor += bytesRead;\n+        }\n+      }\n+    } catch (IOException e) {\n+      LOG.debug(\"Optimized read failed. alling back to readOneBlock {}\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c647c58ef2598549c38b74e665116e76d4d6985"}, "originalPosition": 189}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUzMzM2MzAw", "url": "https://github.com/apache/hadoop/pull/2464#pullrequestreview-553336300", "createdAt": "2020-12-16T03:47:44Z", "commit": {"oid": "2c647c58ef2598549c38b74e665116e76d4d6985"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQwMzo0Nzo0NFrOIGtCxg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQwMzo0Nzo0NFrOIGtCxg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzkwMDM1OA==", "bodyText": "Initialize buffer here", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r543900358", "createdAt": "2020-12-16T03:47:44Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -206,11 +231,127 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n       fCursor += bytesRead;\n       fCursorAfterLastRead = fCursor;\n     }\n+    return copyToUserBuffer(b, off, len);\n+  }\n \n+  private int readFileCompletely(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor, bCursor has\n+    // to be the current fCusor\n+    bCursor = (int) fCursor;\n+    return optimisedRead(b, off, len, 0, contentLength);\n+  }\n+\n+  private int readLastBlock(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor,\n+    // AbfsInutStream buffer is going p contain dta from footer start. In\n+    // that case bCursor will be set to footerStart - fCursor\n+    long lastBlockStart = max(0, contentLength - bufferSize);\n+    bCursor = (int) (fCursor - lastBlockStart);\n+    // read API call is considered 1 single operation in reality server could\n+    // return partial data and client has to retry untill the last full block\n+    // is read. So setting the fCursorAfterLastRead before the possible\n+    // multiple server calls\n+    fCursorAfterLastRead = fCursor;\n+    // 0 if contentlength is < buffersize\n+    long actualLenToRead = min(bufferSize, contentLength);\n+    return optimisedRead(b, off, len, lastBlockStart, actualLenToRead);\n+  }\n+\n+  private int optimisedRead(final byte[] b, final int off, final int len,\n+      final long readFrom, final long actualLen) throws IOException {\n+    int totalBytesRead = 0;\n+    fCursor = readFrom;\n+    try {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c647c58ef2598549c38b74e665116e76d4d6985"}, "originalPosition": 177}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUzMzM3NzQ4", "url": "https://github.com/apache/hadoop/pull/2464#pullrequestreview-553337748", "createdAt": "2020-12-16T03:52:11Z", "commit": {"oid": "2c647c58ef2598549c38b74e665116e76d4d6985"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQwMzo1MjoxMVrOIGtPsg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQwMzo1MjoxMVrOIGtPsg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzkwMzY2Ng==", "bodyText": "Shouldnt this happen before the partial read check block ?", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r543903666", "createdAt": "2020-12-16T03:52:11Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -206,11 +231,127 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n       fCursor += bytesRead;\n       fCursorAfterLastRead = fCursor;\n     }\n+    return copyToUserBuffer(b, off, len);\n+  }\n \n+  private int readFileCompletely(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor, bCursor has\n+    // to be the current fCusor\n+    bCursor = (int) fCursor;\n+    return optimisedRead(b, off, len, 0, contentLength);\n+  }\n+\n+  private int readLastBlock(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor,\n+    // AbfsInutStream buffer is going p contain dta from footer start. In\n+    // that case bCursor will be set to footerStart - fCursor\n+    long lastBlockStart = max(0, contentLength - bufferSize);\n+    bCursor = (int) (fCursor - lastBlockStart);\n+    // read API call is considered 1 single operation in reality server could\n+    // return partial data and client has to retry untill the last full block\n+    // is read. So setting the fCursorAfterLastRead before the possible\n+    // multiple server calls\n+    fCursorAfterLastRead = fCursor;\n+    // 0 if contentlength is < buffersize\n+    long actualLenToRead = min(bufferSize, contentLength);\n+    return optimisedRead(b, off, len, lastBlockStart, actualLenToRead);\n+  }\n+\n+  private int optimisedRead(final byte[] b, final int off, final int len,\n+      final long readFrom, final long actualLen) throws IOException {\n+    int totalBytesRead = 0;\n+    fCursor = readFrom;\n+    try {\n+      for (int i = 0;\n+           i < OPTIMIZATION_ATTEMPTS && fCursor < contentLength; i++) {\n+        int bytesRead = readInternal(fCursor, buffer, limit,\n+            (int) actualLen - limit, true);\n+        if (bytesRead > 0) {\n+          totalBytesRead += bytesRead;\n+          limit += bytesRead;\n+          fCursor += bytesRead;\n+        }\n+      }\n+    } catch (IOException e) {\n+      LOG.debug(\"Optimized read failed. alling back to readOneBlock {}\", e);\n+      restorePointerState();\n+      return readOneBlock(b, off, len);\n+    }\n+    //  If the read was partial and the user requested part of data has\n+    //  not read then fallback to readoneblock. When limit is smaller than\n+    //  bCursor that means the user requested data has not been read.\n+    if (fCursor < contentLength && bCursor > limit) {\n+      restorePointerState();\n+      return readOneBlock(b, off, len);\n+    }\n+    firstRead = false;\n+    if (totalBytesRead == -1) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c647c58ef2598549c38b74e665116e76d4d6985"}, "originalPosition": 201}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUzMzQyODQ1", "url": "https://github.com/apache/hadoop/pull/2464#pullrequestreview-553342845", "createdAt": "2020-12-16T04:09:13Z", "commit": {"oid": "2c647c58ef2598549c38b74e665116e76d4d6985"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQwNDowOToxM1rOIGuCcA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQwNDowOToxM1rOIGuCcA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzkxNjY1Ng==", "bodyText": "What about fCursorAfterLastRead in readSmallFileCompletely case ? Other case too not clear why this should be set anywhere but here.", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r543916656", "createdAt": "2020-12-16T04:09:13Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -206,11 +231,127 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n       fCursor += bytesRead;\n       fCursorAfterLastRead = fCursor;\n     }\n+    return copyToUserBuffer(b, off, len);\n+  }\n \n+  private int readFileCompletely(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor, bCursor has\n+    // to be the current fCusor\n+    bCursor = (int) fCursor;\n+    return optimisedRead(b, off, len, 0, contentLength);\n+  }\n+\n+  private int readLastBlock(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor,\n+    // AbfsInutStream buffer is going p contain dta from footer start. In\n+    // that case bCursor will be set to footerStart - fCursor\n+    long lastBlockStart = max(0, contentLength - bufferSize);\n+    bCursor = (int) (fCursor - lastBlockStart);\n+    // read API call is considered 1 single operation in reality server could\n+    // return partial data and client has to retry untill the last full block\n+    // is read. So setting the fCursorAfterLastRead before the possible\n+    // multiple server calls\n+    fCursorAfterLastRead = fCursor;\n+    // 0 if contentlength is < buffersize\n+    long actualLenToRead = min(bufferSize, contentLength);\n+    return optimisedRead(b, off, len, lastBlockStart, actualLenToRead);\n+  }\n+\n+  private int optimisedRead(final byte[] b, final int off, final int len,\n+      final long readFrom, final long actualLen) throws IOException {\n+    int totalBytesRead = 0;\n+    fCursor = readFrom;\n+    try {\n+      for (int i = 0;\n+           i < OPTIMIZATION_ATTEMPTS && fCursor < contentLength; i++) {\n+        int bytesRead = readInternal(fCursor, buffer, limit,\n+            (int) actualLen - limit, true);\n+        if (bytesRead > 0) {\n+          totalBytesRead += bytesRead;\n+          limit += bytesRead;\n+          fCursor += bytesRead;\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c647c58ef2598549c38b74e665116e76d4d6985"}, "originalPosition": 186}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUzMzQzODc3", "url": "https://github.com/apache/hadoop/pull/2464#pullrequestreview-553343877", "createdAt": "2020-12-16T04:12:39Z", "commit": {"oid": "2c647c58ef2598549c38b74e665116e76d4d6985"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQwNDoxMjozOVrOIGuNpA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQwNDoxMjozOVrOIGuNpA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzkxOTUyNA==", "bodyText": "why is this done here ? Why cant this be updated as the read progresses ?", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r543919524", "createdAt": "2020-12-16T04:12:39Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -206,11 +231,127 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n       fCursor += bytesRead;\n       fCursorAfterLastRead = fCursor;\n     }\n+    return copyToUserBuffer(b, off, len);\n+  }\n \n+  private int readFileCompletely(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor, bCursor has\n+    // to be the current fCusor\n+    bCursor = (int) fCursor;\n+    return optimisedRead(b, off, len, 0, contentLength);\n+  }\n+\n+  private int readLastBlock(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor,\n+    // AbfsInutStream buffer is going p contain dta from footer start. In\n+    // that case bCursor will be set to footerStart - fCursor\n+    long lastBlockStart = max(0, contentLength - bufferSize);\n+    bCursor = (int) (fCursor - lastBlockStart);\n+    // read API call is considered 1 single operation in reality server could\n+    // return partial data and client has to retry untill the last full block\n+    // is read. So setting the fCursorAfterLastRead before the possible\n+    // multiple server calls\n+    fCursorAfterLastRead = fCursor;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c647c58ef2598549c38b74e665116e76d4d6985"}, "originalPosition": 167}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUzMzQ2NDcz", "url": "https://github.com/apache/hadoop/pull/2464#pullrequestreview-553346473", "createdAt": "2020-12-16T04:21:46Z", "commit": {"oid": "2c647c58ef2598549c38b74e665116e76d4d6985"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUzNDg5Mzcz", "url": "https://github.com/apache/hadoop/pull/2464#pullrequestreview-553489373", "createdAt": "2020-12-16T09:15:56Z", "commit": {"oid": "2c647c58ef2598549c38b74e665116e76d4d6985"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQwOToxNTo1NlrOIG7TYQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQwOToxNTo1NlrOIG7TYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDEzMzk4NQ==", "bodyText": "any reason for not impoting java.lang.math?", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r544133985", "createdAt": "2020-12-16T09:15:56Z", "author": {"login": "vinaysbadami"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -38,6 +38,10 @@\n import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException;\n import org.apache.hadoop.fs.azurebfs.utils.CachedSASToken;\n \n+import static java.lang.Math.max;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c647c58ef2598549c38b74e665116e76d4d6985"}, "originalPosition": 4}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9a5edd15333877440862ebfe3b2c59f2450a5658", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/9a5edd15333877440862ebfe3b2c59f2450a5658", "committedDate": "2020-12-16T10:30:08Z", "message": "Addressing review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "80bcd6161f28e9f016b932c235c2774453c0d88f", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/80bcd6161f28e9f016b932c235c2774453c0d88f", "committedDate": "2020-12-16T11:08:28Z", "message": "Addressing review comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU0MjkzNjkx", "url": "https://github.com/apache/hadoop/pull/2464#pullrequestreview-554293691", "createdAt": "2020-12-17T05:41:02Z", "commit": {"oid": "80bcd6161f28e9f016b932c235c2774453c0d88f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwNTo0MTowM1rOIHlkJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwNTo0MTowM1rOIHlkJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDgyNjQwNg==", "bodyText": "Restore needed in case of no read to get the file pointer to where app had set it earlier. Also return explicit -1 as it is a no data read case.", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r544826406", "createdAt": "2020-12-17T05:41:03Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -206,11 +231,121 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n       fCursor += bytesRead;\n       fCursorAfterLastRead = fCursor;\n     }\n+    return copyToUserBuffer(b, off, len);\n+  }\n+\n+  private int readFileCompletely(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+    // data need to be copied to user buffer from index bCursor, bCursor has\n+    // to be the current fCusor\n+    bCursor = (int) fCursor;\n+    return optimisedRead(b, off, len, 0, contentLength);\n+  }\n+\n+  private int readLastBlock(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+    // data need to be copied to user buffer from index bCursor,\n+    // AbfsInutStream buffer is going to contain data from last block start. In\n+    // that case bCursor will be set to fCursor - lastBlockStart\n+    long lastBlockStart = max(0, contentLength - bufferSize);\n+    bCursor = (int) (fCursor - lastBlockStart);\n+    // 0 if contentlength is < buffersize\n+    long actualLenToRead = min(bufferSize, contentLength);\n+    return optimisedRead(b, off, len, lastBlockStart, actualLenToRead);\n+  }\n+\n+  private int optimisedRead(final byte[] b, final int off, final int len,\n+      final long readFrom, final long actualLen) throws IOException {\n+    fCursor = readFrom;\n+    int totalBytesRead = 0;\n+    int lastBytesRead = 0;\n+    try {\n+      buffer = new byte[bufferSize];\n+      for (int i = 0;\n+           i < MAX_OPTIMIZED_READ_ATTEMPTS && fCursor < contentLength; i++) {\n+        lastBytesRead = readInternal(fCursor, buffer, limit,\n+            (int) actualLen - limit, true);\n+        if (lastBytesRead > 0) {\n+          totalBytesRead += lastBytesRead;\n+          limit += lastBytesRead;\n+          fCursor += lastBytesRead;\n+          fCursorAfterLastRead = fCursor;\n+        }\n+      }\n+    } catch (IOException e) {\n+      LOG.debug(\"Optimized read failed. Defaulting to readOneBlock {}\", e);\n+      restorePointerState();\n+      return readOneBlock(b, off, len);\n+    }\n+    firstRead = false;\n+    if (totalBytesRead < 1) {\n+      return lastBytesRead;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "80bcd6161f28e9f016b932c235c2774453c0d88f"}, "originalPosition": 189}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU0Mjk3Mzgx", "url": "https://github.com/apache/hadoop/pull/2464#pullrequestreview-554297381", "createdAt": "2020-12-17T05:52:13Z", "commit": {"oid": "80bcd6161f28e9f016b932c235c2774453c0d88f"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dfbba71b239ac06c02dbb0820099d5f461e93448", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/dfbba71b239ac06c02dbb0820099d5f461e93448", "committedDate": "2020-12-23T17:18:14Z", "message": "Adding more test cases"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "55ac64197a5eaef9400181a96d9cad66fef4c4e2", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/55ac64197a5eaef9400181a96d9cad66fef4c4e2", "committedDate": "2020-12-23T17:29:50Z", "message": "Merge branch 'trunk' into readoptimizations"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3d199634949381e307a88108fb558092ca7c0fe6", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/3d199634949381e307a88108fb558092ca7c0fe6", "committedDate": "2020-12-24T03:26:24Z", "message": "Checkstyle fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8631510c38de2e50954fdd099df0b6364f859baf", "author": {"user": {"login": "bilaharith", "name": null}}, "url": "https://github.com/apache/hadoop/commit/8631510c38de2e50954fdd099df0b6364f859baf", "committedDate": "2020-12-24T16:32:34Z", "message": "Adding more test cases"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU5MDk1NzE0", "url": "https://github.com/apache/hadoop/pull/2464#pullrequestreview-559095714", "createdAt": "2020-12-28T11:00:18Z", "commit": {"oid": "8631510c38de2e50954fdd099df0b6364f859baf"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYwNjQ5MzUz", "url": "https://github.com/apache/hadoop/pull/2464#pullrequestreview-560649353", "createdAt": "2021-01-02T18:36:10Z", "commit": {"oid": "8631510c38de2e50954fdd099df0b6364f859baf"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3260, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}