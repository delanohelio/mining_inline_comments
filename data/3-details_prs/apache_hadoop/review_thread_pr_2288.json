{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDgyNTI3ODE2", "number": 2288, "reviewThreads": {"totalCount": 18, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xN1QxMjo0NzozNVrOEkhMTA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwNzoyNjo1MVrOE2om7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA2NzI4MDEyOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xN1QxMjo0NzozNVrOHTgXIA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOFQwMjowMDozOFrOHT7V8g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDIxNTIwMA==", "bodyText": "reservedForArchival here is same as this.reservedForArchival when init?", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r490215200", "createdAt": "2020-09-17T12:47:35Z", "author": {"login": "Hexiaoqiao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java", "diffHunk": "@@ -412,16 +435,31 @@ long getBlockPoolUsed(String bpid) throws IOException {\n    */\n   @VisibleForTesting\n   public long getCapacity() {\n+    long capacity;\n     if (configuredCapacity < 0L) {\n       long remaining;\n       if (cachedCapacity > 0L) {\n         remaining = cachedCapacity - getReserved();\n       } else {\n         remaining = usage.getCapacity() - getReserved();\n       }\n-      return Math.max(remaining, 0L);\n+      capacity = Math.max(remaining, 0L);\n+    } else {\n+      capacity = configuredCapacity;\n+    }\n+\n+    if (enableSameDiskArchival) {\n+      double reservedForArchival = conf.getDouble(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "db0fb346b46fd410c3cf062f61db6ee06f890cec"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY1NzI2Ng==", "bodyText": "Oh yeah my mistake here, thanks for the catch!", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r490657266", "createdAt": "2020-09-18T02:00:38Z", "author": {"login": "LeonGao91"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java", "diffHunk": "@@ -412,16 +435,31 @@ long getBlockPoolUsed(String bpid) throws IOException {\n    */\n   @VisibleForTesting\n   public long getCapacity() {\n+    long capacity;\n     if (configuredCapacity < 0L) {\n       long remaining;\n       if (cachedCapacity > 0L) {\n         remaining = cachedCapacity - getReserved();\n       } else {\n         remaining = usage.getCapacity() - getReserved();\n       }\n-      return Math.max(remaining, 0L);\n+      capacity = Math.max(remaining, 0L);\n+    } else {\n+      capacity = configuredCapacity;\n+    }\n+\n+    if (enableSameDiskArchival) {\n+      double reservedForArchival = conf.getDouble(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDIxNTIwMA=="}, "originalCommit": {"oid": "db0fb346b46fd410c3cf062f61db6ee06f890cec"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA2NzMwOTY3OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xN1QxMjo1NDoxOFrOHTgpGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOFQwMjowMTowMVrOHT7WVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDIxOTgwMQ==", "bodyText": "what about using storageID replace device? IMO both of them are in order to index single volume, right?", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r490219801", "createdAt": "2020-09-17T12:54:18Z", "author": {"login": "Hexiaoqiao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java", "diffHunk": "@@ -134,6 +134,9 @@\n   private final FileIoProvider fileIoProvider;\n   private final DataNodeVolumeMetrics metrics;\n   private URI baseURI;\n+  private boolean enableSameDiskArchival;\n+  private final String device;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "db0fb346b46fd410c3cf062f61db6ee06f890cec"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY1NzM2NA==", "bodyText": "The \"device\" here is the string value of the filesystem mount point. I wanted to use it to keep track of which two volumes are on the same mount (thus the same disk). Datanode can use the existing DF#getMount() to detect it automatically.\nI can probably change the name to \"mount\" to make it more clear.", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r490657364", "createdAt": "2020-09-18T02:01:01Z", "author": {"login": "LeonGao91"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java", "diffHunk": "@@ -134,6 +134,9 @@\n   private final FileIoProvider fileIoProvider;\n   private final DataNodeVolumeMetrics metrics;\n   private URI baseURI;\n+  private boolean enableSameDiskArchival;\n+  private final String device;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDIxOTgwMQ=="}, "originalCommit": {"oid": "db0fb346b46fd410c3cf062f61db6ee06f890cec"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA2NzMyODI4OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xN1QxMjo1ODozMVrOHTg0aA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOFQwMjowMTozM1rOHT7W4w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDIyMjY5Ng==", "bodyText": "Why reservedForArchive has to less than 1 here, IIUC it means that this is ARCHIVE device when reservedForArchive set to 1. Right?", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r490222696", "createdAt": "2020-09-17T12:58:31Z", "author": {"login": "Hexiaoqiao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java", "diffHunk": "@@ -190,6 +193,26 @@\n     }\n     this.conf = conf;\n     this.fileIoProvider = fileIoProvider;\n+    this.enableSameDiskArchival =\n+        conf.getBoolean(DFSConfigKeys.DFS_DATANODE_ALLOW_SAME_DISK_TIERING,\n+            DFSConfigKeys.DFS_DATANODE_ALLOW_SAME_DISK_TIERING_DEFAULT);\n+    if (enableSameDiskArchival) {\n+      this.device = usage.getMount();\n+      reservedForArchive = conf.getDouble(\n+          DFSConfigKeys.DFS_DATANODE_RESERVE_FOR_ARCHIVE_PERCENTAGE,\n+          DFSConfigKeys.DFS_DATANODE_RESERVE_FOR_ARCHIVE_PERCENTAGE_DEFAULT);\n+      if (reservedForArchive >= 1) {\n+        FsDatasetImpl.LOG.warn(\"Value of reserve-for-archival is >= 100% for \"\n+            + currentDir + \". Setting it to 99%.\");\n+        reservedForArchive = 0.99;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "db0fb346b46fd410c3cf062f61db6ee06f890cec"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY1NzUwNw==", "bodyText": "Yeah I think you are right, I will update and make this at most 1.", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r490657507", "createdAt": "2020-09-18T02:01:33Z", "author": {"login": "LeonGao91"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java", "diffHunk": "@@ -190,6 +193,26 @@\n     }\n     this.conf = conf;\n     this.fileIoProvider = fileIoProvider;\n+    this.enableSameDiskArchival =\n+        conf.getBoolean(DFSConfigKeys.DFS_DATANODE_ALLOW_SAME_DISK_TIERING,\n+            DFSConfigKeys.DFS_DATANODE_ALLOW_SAME_DISK_TIERING_DEFAULT);\n+    if (enableSameDiskArchival) {\n+      this.device = usage.getMount();\n+      reservedForArchive = conf.getDouble(\n+          DFSConfigKeys.DFS_DATANODE_RESERVE_FOR_ARCHIVE_PERCENTAGE,\n+          DFSConfigKeys.DFS_DATANODE_RESERVE_FOR_ARCHIVE_PERCENTAGE_DEFAULT);\n+      if (reservedForArchive >= 1) {\n+        FsDatasetImpl.LOG.warn(\"Value of reserve-for-archival is >= 100% for \"\n+            + currentDir + \". Setting it to 99%.\");\n+        reservedForArchive = 0.99;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDIyMjY5Ng=="}, "originalCommit": {"oid": "db0fb346b46fd410c3cf062f61db6ee06f890cec"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MjI1MDQyOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwNjo1NjowMFrOHXMOAw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQyMTo1Njo0NlrOHXuBQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA3OTQ5MQ==", "bodyText": "enableSameDiskTiering here vs enableSameDiskArchival at FsVolumeImpl,  we should unified variable name.", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r494079491", "createdAt": "2020-09-24T06:56:00Z", "author": {"login": "Hexiaoqiao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java", "diffHunk": "@@ -62,9 +64,14 @@\n   private final VolumeChoosingPolicy<FsVolumeImpl> blockChooser;\n   private final BlockScanner blockScanner;\n \n+  private boolean enableSameDiskTiering;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9d089234659e7a26f362eb8afab774cbb90e49d8"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDYzMzI4Mw==", "bodyText": "Good catch! I will update them to be the same name.", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r494633283", "createdAt": "2020-09-24T21:56:46Z", "author": {"login": "LeonGao91"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java", "diffHunk": "@@ -62,9 +64,14 @@\n   private final VolumeChoosingPolicy<FsVolumeImpl> blockChooser;\n   private final BlockScanner blockScanner;\n \n+  private boolean enableSameDiskTiering;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA3OTQ5MQ=="}, "originalCommit": {"oid": "9d089234659e7a26f362eb8afab774cbb90e49d8"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MjI2MDQ2OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwNjo1OTowMFrOHXMT8A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQyMDo0NDowMFrOHXr81g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA4MTAwOA==", "bodyText": "reservedForArchive try to define reserve for archive percentage. If there are heterogeneous disks located one node, do we need config them separate\uff1f", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r494081008", "createdAt": "2020-09-24T06:59:00Z", "author": {"login": "Hexiaoqiao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java", "diffHunk": "@@ -190,6 +193,26 @@\n     }\n     this.conf = conf;\n     this.fileIoProvider = fileIoProvider;\n+    this.enableSameDiskArchival =\n+        conf.getBoolean(DFSConfigKeys.DFS_DATANODE_ALLOW_SAME_DISK_TIERING,\n+            DFSConfigKeys.DFS_DATANODE_ALLOW_SAME_DISK_TIERING_DEFAULT);\n+    if (enableSameDiskArchival) {\n+      this.mount = usage.getMount();\n+      reservedForArchive = conf.getDouble(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9d089234659e7a26f362eb8afab774cbb90e49d8"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDU5OTM4Mg==", "bodyText": "Yeah, it's a good point. The reason I put it this way is to make configuration less verbose for normal use cases that datanode only has one type of disk. Otherwise, users will need to tag all the disks which is less readable and easy to make mistakes.\nI think we can introduce additional config for the use case you mentioned later, to list out each volume and target ratio.", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r494599382", "createdAt": "2020-09-24T20:44:00Z", "author": {"login": "LeonGao91"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java", "diffHunk": "@@ -190,6 +193,26 @@\n     }\n     this.conf = conf;\n     this.fileIoProvider = fileIoProvider;\n+    this.enableSameDiskArchival =\n+        conf.getBoolean(DFSConfigKeys.DFS_DATANODE_ALLOW_SAME_DISK_TIERING,\n+            DFSConfigKeys.DFS_DATANODE_ALLOW_SAME_DISK_TIERING_DEFAULT);\n+    if (enableSameDiskArchival) {\n+      this.mount = usage.getMount();\n+      reservedForArchive = conf.getDouble(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA4MTAwOA=="}, "originalCommit": {"oid": "9d089234659e7a26f362eb8afab774cbb90e49d8"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MjI4OTMwOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwNzowNzo0NVrOHXMlLw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQwOTo0MToxNVrOHY2FZg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA4NTQyMw==", "bodyText": "The return value seems not expected as annotation says if enable this feature.\n\nthe capacity of the file system excluding space reserved for non-HDFS.\n\nIMO, the part for ARCHIVE should also be calculated. It seems be not differentiated by NameNode for DISK or ARCHIVE per storage of DataNode. Please correct if something wrong.", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r494085423", "createdAt": "2020-09-24T07:07:45Z", "author": {"login": "Hexiaoqiao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java", "diffHunk": "@@ -412,16 +435,28 @@ long getBlockPoolUsed(String bpid) throws IOException {\n    */\n   @VisibleForTesting\n   public long getCapacity() {\n+    long capacity;\n     if (configuredCapacity < 0L) {\n       long remaining;\n       if (cachedCapacity > 0L) {\n         remaining = cachedCapacity - getReserved();\n       } else {\n         remaining = usage.getCapacity() - getReserved();\n       }\n-      return Math.max(remaining, 0L);\n+      capacity = Math.max(remaining, 0L);\n+    } else {\n+      capacity = configuredCapacity;\n+    }\n+\n+    if (enableSameDiskArchival) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9d089234659e7a26f362eb8afab774cbb90e49d8"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDYzMjg2Mw==", "bodyText": "This is actually the important part to enable this feature, to allow users to configure the capacity of a fsVolume.\nAs we are configuring two fsVolume on the same underlying filesystem, if we do nothing the capacity will be calculated twice thus all the stats being reported will be incorrect.\nHere is an example:\nLet's say we want to configure [DISK]/data01/dfs and [ARCHIVE]/data01/dfs_archive on a 4TB disk mount /data01, and we want to assign 1 TB to [DISK]/data01/dfs and 3 TB for [ARCHIVE]/data01/dfs_archive, we can make reservedForArchive to be 0.75 and put those two dirs in the volume list.\nIn this case, /data01/dfs will be reported as a 1TB volume and /data01/dfs_archive will be reported as 3TB volume to HDFS. Logically, HDFS will just treat them as two separate volumes.\nIf we don't make the change here, HDFS will see two volumes and each of them is 4TB, in that case, the 4TB disk will be counted as 4 * 2 = 8TB capacity in namenode and all the related stats will be wrong.\nAnother change we need to make is the getActualNonDfsUsed() as below. Let's say in the above 4TB disk setup we use 0.1TB as reserved, and [ARCHIVE]/data01/dfs_archive already has 2TB capacity used, in this case when we are calculating the getActualNonDfsUsed() for [DISK]/data01/dfs it will always return 0, which is not correct and it will cause other weird issues. As the two fsVolumes are on the same filesystem, the reserved space should be shared.\nAccording to our analysis and cluster testing result, updating these two functions getCapacity() and getActualNonDfsUsed() is enough to keep stats correct for the two \"logical\" fsVolumes on same disk.\nI can update the java doc to reflect this when the feature is turned on.", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r494632863", "createdAt": "2020-09-24T21:55:42Z", "author": {"login": "LeonGao91"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java", "diffHunk": "@@ -412,16 +435,28 @@ long getBlockPoolUsed(String bpid) throws IOException {\n    */\n   @VisibleForTesting\n   public long getCapacity() {\n+    long capacity;\n     if (configuredCapacity < 0L) {\n       long remaining;\n       if (cachedCapacity > 0L) {\n         remaining = cachedCapacity - getReserved();\n       } else {\n         remaining = usage.getCapacity() - getReserved();\n       }\n-      return Math.max(remaining, 0L);\n+      capacity = Math.max(remaining, 0L);\n+    } else {\n+      capacity = configuredCapacity;\n+    }\n+\n+    if (enableSameDiskArchival) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA4NTQyMw=="}, "originalCommit": {"oid": "9d089234659e7a26f362eb8afab774cbb90e49d8"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTgxMzk5MA==", "bodyText": "Thanks @LeonGao91 for your detailed comments. It makes sense for me.", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r495813990", "createdAt": "2020-09-28T09:41:15Z", "author": {"login": "Hexiaoqiao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java", "diffHunk": "@@ -412,16 +435,28 @@ long getBlockPoolUsed(String bpid) throws IOException {\n    */\n   @VisibleForTesting\n   public long getCapacity() {\n+    long capacity;\n     if (configuredCapacity < 0L) {\n       long remaining;\n       if (cachedCapacity > 0L) {\n         remaining = cachedCapacity - getReserved();\n       } else {\n         remaining = usage.getCapacity() - getReserved();\n       }\n-      return Math.max(remaining, 0L);\n+      capacity = Math.max(remaining, 0L);\n+    } else {\n+      capacity = configuredCapacity;\n+    }\n+\n+    if (enableSameDiskArchival) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA4NTQyMw=="}, "originalCommit": {"oid": "9d089234659e7a26f362eb8afab774cbb90e49d8"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MjI5MjQzOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwNzowODo1MVrOHXMnJA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQyMTo1Njo0MFrOHXuA_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA4NTkyNA==", "bodyText": "same confused as the last comment.", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r494085924", "createdAt": "2020-09-24T07:08:51Z", "author": {"login": "Hexiaoqiao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java", "diffHunk": "@@ -452,7 +487,33 @@ public long getAvailable() throws IOException {\n   }\n \n   long getActualNonDfsUsed() throws IOException {\n-    return usage.getUsed() - getDfsUsed();\n+    // DISK and ARCHIVAL on same disk", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9d089234659e7a26f362eb8afab774cbb90e49d8"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDYzMzIxNA==", "bodyText": "Commented with an example use case as above, hopefully it explains well : )", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r494633214", "createdAt": "2020-09-24T21:56:40Z", "author": {"login": "LeonGao91"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java", "diffHunk": "@@ -452,7 +487,33 @@ public long getAvailable() throws IOException {\n   }\n \n   long getActualNonDfsUsed() throws IOException {\n-    return usage.getUsed() - getDfsUsed();\n+    // DISK and ARCHIVAL on same disk", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA4NTkyNA=="}, "originalCommit": {"oid": "9d089234659e7a26f362eb8afab774cbb90e49d8"}, "originalPosition": 73}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyNDI5NjQ5OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQyMTo0MDozOFrOHqvu0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQwNToyMTo0N1rOHrBf3Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDU4NDI3Mw==", "bodyText": "Take another look at the patch, I think it may be better to have the percentage as a tag added to the configuration \"dfs.datanode.data.dir\", just following the storage type tag. In this way on the same datanode we can have different percentage settings for different mount points. What do you think?", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r514584273", "createdAt": "2020-10-29T21:40:38Z", "author": {"login": "Jing9"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java", "diffHunk": "@@ -1503,6 +1503,20 @@\n   public static final boolean DFS_PROTECTED_SUBDIRECTORIES_ENABLE_DEFAULT =\n       false;\n \n+  public static final String DFS_DATANODE_ALLOW_SAME_DISK_TIERING =\n+      \"dfs.datanode.same-disk-tiering.enabled\";\n+  public static final boolean DFS_DATANODE_ALLOW_SAME_DISK_TIERING_DEFAULT =\n+      false;\n+\n+  // HDFS-15548 to allow DISK/ARCHIVE configured on the same disk mount.\n+  // Beware that capacity usage might be >100% if there are already\n+  // data blocks exist and the configured ratio is small, which will\n+  // prevent the volume from taking new blocks until capacity is balanced out.\n+  public static final String DFS_DATANODE_RESERVE_FOR_ARCHIVE_PERCENTAGE =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1e91d78f1d84b050bbdad52a04cf7206924f4d40"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDg3NTM1Nw==", "bodyText": "The intention is to have a configuration as a \"default value\" for all disks, as in normal cases one datanode server comes with the same type of HDDs. Therefore we can keep the DN configuration less verbose for most of the use cases.\nHowever, you are right that we should allow users to configure different values, and it is a good idea to put it under \"dfs.datanode.data.dir\".\nI will create a follow-up JIRA to address it, so we can keep this PR from being too big, as that could involve quite some change.", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r514875357", "createdAt": "2020-10-30T05:21:47Z", "author": {"login": "LeonGao91"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java", "diffHunk": "@@ -1503,6 +1503,20 @@\n   public static final boolean DFS_PROTECTED_SUBDIRECTORIES_ENABLE_DEFAULT =\n       false;\n \n+  public static final String DFS_DATANODE_ALLOW_SAME_DISK_TIERING =\n+      \"dfs.datanode.same-disk-tiering.enabled\";\n+  public static final boolean DFS_DATANODE_ALLOW_SAME_DISK_TIERING_DEFAULT =\n+      false;\n+\n+  // HDFS-15548 to allow DISK/ARCHIVE configured on the same disk mount.\n+  // Beware that capacity usage might be >100% if there are already\n+  // data blocks exist and the configured ratio is small, which will\n+  // prevent the volume from taking new blocks until capacity is balanced out.\n+  public static final String DFS_DATANODE_RESERVE_FOR_ARCHIVE_PERCENTAGE =", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDU4NDI3Mw=="}, "originalCommit": {"oid": "1e91d78f1d84b050bbdad52a04cf7206924f4d40"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyNDgzMDI2OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MountVolumeMap.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQwMToyODoxNFrOHq0vZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQwNToyNToyMlrOHrBpOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDY2NjM0Mg==", "bodyText": "what if we have a mount with one single volume? Following the current implementation we may assign an unnecessary capacity ratio to it. We only need to calculate and assign the ratio for volumes sharing the same mount with others.", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r514666342", "createdAt": "2020-10-30T01:28:14Z", "author": {"login": "Jing9"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MountVolumeMap.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.datanode.fsdataset.impl;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.StorageType;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeReference;\n+\n+import java.nio.channels.ClosedChannelException;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n+\n+/**\n+ * MountVolumeMap contains information of the relationship\n+ * between underlying filesystem mount and datanode volumes.\n+ *\n+ * This is useful when configuring block tiering on same disk mount (HDFS-15548)\n+ * For now,\n+ * we don't configure multiple volumes with same storage type on a mount.\n+ */\n+@InterfaceAudience.Private\n+class MountVolumeMap {\n+  private ConcurrentMap<String, Map<StorageType, VolumeInfo>>\n+      mountVolumeMapping;\n+  private double reservedForArchive;\n+\n+  MountVolumeMap(Configuration conf) {\n+    mountVolumeMapping = new ConcurrentHashMap<>();\n+    reservedForArchive = conf.getDouble(\n+        DFSConfigKeys.DFS_DATANODE_RESERVE_FOR_ARCHIVE_PERCENTAGE,\n+        DFSConfigKeys.DFS_DATANODE_RESERVE_FOR_ARCHIVE_PERCENTAGE_DEFAULT);\n+    if (reservedForArchive > 1) {\n+      FsDatasetImpl.LOG.warn(\"Value of reserve-for-archival is > 100%.\" +\n+          \" Setting it to 100%.\");\n+      reservedForArchive = 1;\n+    }\n+  }\n+\n+  FsVolumeReference getVolumeRefByMountAndStorageType(String mount,\n+      StorageType storageType) {\n+    if (mountVolumeMapping != null\n+        && mountVolumeMapping.containsKey(mount)) {\n+      try {\n+        VolumeInfo volumeInfo = mountVolumeMapping\n+            .get(mount).getOrDefault(storageType, null);\n+        if (volumeInfo != null) {\n+          return volumeInfo.getFsVolume().obtainReference();\n+        }\n+      } catch (ClosedChannelException e) {\n+        FsDatasetImpl.LOG.warn(\"Volume closed when getting volume\" +\n+            \" by mount and storage type: \"\n+            + mount + \", \" + storageType);\n+      }\n+    }\n+    return null;\n+  }\n+\n+  /**\n+   * Return configured capacity ratio. Otherwise return 1 as default\n+   */\n+  double getCapacityRatioByMountAndStorageType(String mount,\n+      StorageType storageType) {\n+    if (mountVolumeMapping != null\n+        && mountVolumeMapping.containsKey(mount)) {\n+      return mountVolumeMapping\n+          .get(mount).getOrDefault(storageType, null).getCapacityRatio();\n+    }\n+    return 1;\n+  }\n+\n+  void addVolume(FsVolumeImpl volume) {\n+    String mount = volume.getMount();\n+    if (!mount.isEmpty()) {\n+      Map<StorageType, VolumeInfo> storageTypeMap =\n+          mountVolumeMapping\n+              .getOrDefault(mount, new ConcurrentHashMap<>());\n+      if (storageTypeMap.containsKey(volume.getStorageType())) {\n+        FsDatasetImpl.LOG.error(\"Found storage type already exist.\" +\n+            \" Skipping for now. Please check disk configuration\");\n+      } else {\n+        VolumeInfo volumeInfo = new VolumeInfo(volume, 1);\n+        if (volume.getStorageType() == StorageType.ARCHIVE) {\n+          volumeInfo.setCapacityRatio(reservedForArchive);\n+        } else if (volume.getStorageType() == StorageType.DISK) {\n+          volumeInfo.setCapacityRatio(1 - reservedForArchive);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1e91d78f1d84b050bbdad52a04cf7206924f4d40"}, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDg3Nzc1Mw==", "bodyText": "Thats a good point. I will make the change to ignore the capacity ratio of the volume if there is only one on the mount.", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r514877753", "createdAt": "2020-10-30T05:25:22Z", "author": {"login": "LeonGao91"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MountVolumeMap.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.datanode.fsdataset.impl;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.StorageType;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeReference;\n+\n+import java.nio.channels.ClosedChannelException;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n+\n+/**\n+ * MountVolumeMap contains information of the relationship\n+ * between underlying filesystem mount and datanode volumes.\n+ *\n+ * This is useful when configuring block tiering on same disk mount (HDFS-15548)\n+ * For now,\n+ * we don't configure multiple volumes with same storage type on a mount.\n+ */\n+@InterfaceAudience.Private\n+class MountVolumeMap {\n+  private ConcurrentMap<String, Map<StorageType, VolumeInfo>>\n+      mountVolumeMapping;\n+  private double reservedForArchive;\n+\n+  MountVolumeMap(Configuration conf) {\n+    mountVolumeMapping = new ConcurrentHashMap<>();\n+    reservedForArchive = conf.getDouble(\n+        DFSConfigKeys.DFS_DATANODE_RESERVE_FOR_ARCHIVE_PERCENTAGE,\n+        DFSConfigKeys.DFS_DATANODE_RESERVE_FOR_ARCHIVE_PERCENTAGE_DEFAULT);\n+    if (reservedForArchive > 1) {\n+      FsDatasetImpl.LOG.warn(\"Value of reserve-for-archival is > 100%.\" +\n+          \" Setting it to 100%.\");\n+      reservedForArchive = 1;\n+    }\n+  }\n+\n+  FsVolumeReference getVolumeRefByMountAndStorageType(String mount,\n+      StorageType storageType) {\n+    if (mountVolumeMapping != null\n+        && mountVolumeMapping.containsKey(mount)) {\n+      try {\n+        VolumeInfo volumeInfo = mountVolumeMapping\n+            .get(mount).getOrDefault(storageType, null);\n+        if (volumeInfo != null) {\n+          return volumeInfo.getFsVolume().obtainReference();\n+        }\n+      } catch (ClosedChannelException e) {\n+        FsDatasetImpl.LOG.warn(\"Volume closed when getting volume\" +\n+            \" by mount and storage type: \"\n+            + mount + \", \" + storageType);\n+      }\n+    }\n+    return null;\n+  }\n+\n+  /**\n+   * Return configured capacity ratio. Otherwise return 1 as default\n+   */\n+  double getCapacityRatioByMountAndStorageType(String mount,\n+      StorageType storageType) {\n+    if (mountVolumeMapping != null\n+        && mountVolumeMapping.containsKey(mount)) {\n+      return mountVolumeMapping\n+          .get(mount).getOrDefault(storageType, null).getCapacityRatio();\n+    }\n+    return 1;\n+  }\n+\n+  void addVolume(FsVolumeImpl volume) {\n+    String mount = volume.getMount();\n+    if (!mount.isEmpty()) {\n+      Map<StorageType, VolumeInfo> storageTypeMap =\n+          mountVolumeMapping\n+              .getOrDefault(mount, new ConcurrentHashMap<>());\n+      if (storageTypeMap.containsKey(volume.getStorageType())) {\n+        FsDatasetImpl.LOG.error(\"Found storage type already exist.\" +\n+            \" Skipping for now. Please check disk configuration\");\n+      } else {\n+        VolumeInfo volumeInfo = new VolumeInfo(volume, 1);\n+        if (volume.getStorageType() == StorageType.ARCHIVE) {\n+          volumeInfo.setCapacityRatio(reservedForArchive);\n+        } else if (volume.getStorageType() == StorageType.DISK) {\n+          volumeInfo.setCapacityRatio(1 - reservedForArchive);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDY2NjM0Mg=="}, "originalCommit": {"oid": "1e91d78f1d84b050bbdad52a04cf7206924f4d40"}, "originalPosition": 103}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzNjAxOTUxOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MountVolumeInfo.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QwMDoxOToyOVrOHsb2Kw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMTo1NzoyMFrOHtB5fQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM1NTYyNw==", "bodyText": "Do we need to throw an exception here? Silently dropping the volume may not be a good option.", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r516355627", "createdAt": "2020-11-03T00:19:29Z", "author": {"login": "Jing9"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MountVolumeInfo.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.datanode.fsdataset.impl;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.StorageType;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeReference;\n+\n+import java.nio.channels.ClosedChannelException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n+\n+/**\n+ * MountVolumeInfo is a wrapper of\n+ * detailed volume information for MountVolumeMap.\n+ */\n+@InterfaceAudience.Private\n+class MountVolumeInfo {\n+  private ConcurrentMap<StorageType, FsVolumeImpl>\n+      storageTypeVolumeMap;\n+  private double reservedForArchiveDefault;\n+\n+  MountVolumeInfo(Configuration conf) {\n+    storageTypeVolumeMap = new ConcurrentHashMap<>();\n+    reservedForArchiveDefault = conf.getDouble(\n+        DFSConfigKeys.DFS_DATANODE_RESERVE_FOR_ARCHIVE_DEFAULT_PERCENTAGE,\n+        DFSConfigKeys\n+            .DFS_DATANODE_RESERVE_FOR_ARCHIVE_DEFAULT_PERCENTAGE_DEFAULT);\n+    if (reservedForArchiveDefault > 1) {\n+      FsDatasetImpl.LOG.warn(\"Value of reserve-for-archival is > 100%.\" +\n+          \" Setting it to 100%.\");\n+      reservedForArchiveDefault = 1;\n+    }\n+  }\n+\n+  FsVolumeReference getVolumeRef(StorageType storageType) {\n+    try {\n+      FsVolumeImpl volumeImpl = storageTypeVolumeMap\n+          .getOrDefault(storageType, null);\n+      if (volumeImpl != null) {\n+        return volumeImpl.obtainReference();\n+      }\n+    } catch (ClosedChannelException e) {\n+      FsDatasetImpl.LOG.warn(\"Volume closed when getting volume\" +\n+          \" by storage type: \" + storageType);\n+    }\n+    return null;\n+  }\n+\n+  /**\n+   * Return configured capacity ratio.\n+   * If the volume is the only one on the mount,\n+   * return 1 to avoid unnecessary allocation.\n+   */\n+  double getCapacityRatio(StorageType storageType) {\n+    if (storageTypeVolumeMap.containsKey(storageType)\n+        && storageTypeVolumeMap.size() > 1) {\n+      if (storageType == StorageType.ARCHIVE) {\n+        return reservedForArchiveDefault;\n+      } else if (storageType == StorageType.DISK) {\n+        return 1 - reservedForArchiveDefault;\n+      }\n+    }\n+    return 1;\n+  }\n+\n+  void addVolume(FsVolumeImpl volume) {\n+    if (storageTypeVolumeMap.containsKey(volume.getStorageType())) {\n+      FsDatasetImpl.LOG.error(\"Found storage type already exist.\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "57c52d6d059d29013615642ecf9ad90cddb35c70"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM4NDk5MA==", "bodyText": "I have already put a pre-check in activateVolume, following how other check is done. This seems cleaner and following the existing ways to validate volumes before doing any operation.\nIn that case, we should never run into this, I was thinking to keep it just for debugging in case of some weird bug results in this situation.", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r516384990", "createdAt": "2020-11-03T01:35:14Z", "author": {"login": "LeonGao91"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MountVolumeInfo.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.datanode.fsdataset.impl;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.StorageType;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeReference;\n+\n+import java.nio.channels.ClosedChannelException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n+\n+/**\n+ * MountVolumeInfo is a wrapper of\n+ * detailed volume information for MountVolumeMap.\n+ */\n+@InterfaceAudience.Private\n+class MountVolumeInfo {\n+  private ConcurrentMap<StorageType, FsVolumeImpl>\n+      storageTypeVolumeMap;\n+  private double reservedForArchiveDefault;\n+\n+  MountVolumeInfo(Configuration conf) {\n+    storageTypeVolumeMap = new ConcurrentHashMap<>();\n+    reservedForArchiveDefault = conf.getDouble(\n+        DFSConfigKeys.DFS_DATANODE_RESERVE_FOR_ARCHIVE_DEFAULT_PERCENTAGE,\n+        DFSConfigKeys\n+            .DFS_DATANODE_RESERVE_FOR_ARCHIVE_DEFAULT_PERCENTAGE_DEFAULT);\n+    if (reservedForArchiveDefault > 1) {\n+      FsDatasetImpl.LOG.warn(\"Value of reserve-for-archival is > 100%.\" +\n+          \" Setting it to 100%.\");\n+      reservedForArchiveDefault = 1;\n+    }\n+  }\n+\n+  FsVolumeReference getVolumeRef(StorageType storageType) {\n+    try {\n+      FsVolumeImpl volumeImpl = storageTypeVolumeMap\n+          .getOrDefault(storageType, null);\n+      if (volumeImpl != null) {\n+        return volumeImpl.obtainReference();\n+      }\n+    } catch (ClosedChannelException e) {\n+      FsDatasetImpl.LOG.warn(\"Volume closed when getting volume\" +\n+          \" by storage type: \" + storageType);\n+    }\n+    return null;\n+  }\n+\n+  /**\n+   * Return configured capacity ratio.\n+   * If the volume is the only one on the mount,\n+   * return 1 to avoid unnecessary allocation.\n+   */\n+  double getCapacityRatio(StorageType storageType) {\n+    if (storageTypeVolumeMap.containsKey(storageType)\n+        && storageTypeVolumeMap.size() > 1) {\n+      if (storageType == StorageType.ARCHIVE) {\n+        return reservedForArchiveDefault;\n+      } else if (storageType == StorageType.DISK) {\n+        return 1 - reservedForArchiveDefault;\n+      }\n+    }\n+    return 1;\n+  }\n+\n+  void addVolume(FsVolumeImpl volume) {\n+    if (storageTypeVolumeMap.containsKey(volume.getStorageType())) {\n+      FsDatasetImpl.LOG.error(\"Found storage type already exist.\" +", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM1NTYyNw=="}, "originalCommit": {"oid": "57c52d6d059d29013615642ecf9ad90cddb35c70"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk0OTkwNQ==", "bodyText": "What if in the future MountVolumeInfo#addVolume is called by other code other than activateVolume? If an existing storage type is not allowed, we can return a boolean to indicate if the call succeeds or not.", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r516949905", "createdAt": "2020-11-03T20:55:20Z", "author": {"login": "Jing9"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MountVolumeInfo.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.datanode.fsdataset.impl;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.StorageType;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeReference;\n+\n+import java.nio.channels.ClosedChannelException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n+\n+/**\n+ * MountVolumeInfo is a wrapper of\n+ * detailed volume information for MountVolumeMap.\n+ */\n+@InterfaceAudience.Private\n+class MountVolumeInfo {\n+  private ConcurrentMap<StorageType, FsVolumeImpl>\n+      storageTypeVolumeMap;\n+  private double reservedForArchiveDefault;\n+\n+  MountVolumeInfo(Configuration conf) {\n+    storageTypeVolumeMap = new ConcurrentHashMap<>();\n+    reservedForArchiveDefault = conf.getDouble(\n+        DFSConfigKeys.DFS_DATANODE_RESERVE_FOR_ARCHIVE_DEFAULT_PERCENTAGE,\n+        DFSConfigKeys\n+            .DFS_DATANODE_RESERVE_FOR_ARCHIVE_DEFAULT_PERCENTAGE_DEFAULT);\n+    if (reservedForArchiveDefault > 1) {\n+      FsDatasetImpl.LOG.warn(\"Value of reserve-for-archival is > 100%.\" +\n+          \" Setting it to 100%.\");\n+      reservedForArchiveDefault = 1;\n+    }\n+  }\n+\n+  FsVolumeReference getVolumeRef(StorageType storageType) {\n+    try {\n+      FsVolumeImpl volumeImpl = storageTypeVolumeMap\n+          .getOrDefault(storageType, null);\n+      if (volumeImpl != null) {\n+        return volumeImpl.obtainReference();\n+      }\n+    } catch (ClosedChannelException e) {\n+      FsDatasetImpl.LOG.warn(\"Volume closed when getting volume\" +\n+          \" by storage type: \" + storageType);\n+    }\n+    return null;\n+  }\n+\n+  /**\n+   * Return configured capacity ratio.\n+   * If the volume is the only one on the mount,\n+   * return 1 to avoid unnecessary allocation.\n+   */\n+  double getCapacityRatio(StorageType storageType) {\n+    if (storageTypeVolumeMap.containsKey(storageType)\n+        && storageTypeVolumeMap.size() > 1) {\n+      if (storageType == StorageType.ARCHIVE) {\n+        return reservedForArchiveDefault;\n+      } else if (storageType == StorageType.DISK) {\n+        return 1 - reservedForArchiveDefault;\n+      }\n+    }\n+    return 1;\n+  }\n+\n+  void addVolume(FsVolumeImpl volume) {\n+    if (storageTypeVolumeMap.containsKey(volume.getStorageType())) {\n+      FsDatasetImpl.LOG.error(\"Found storage type already exist.\" +", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM1NTYyNw=="}, "originalCommit": {"oid": "57c52d6d059d29013615642ecf9ad90cddb35c70"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk3OTA2OQ==", "bodyText": "Yeah, that makes sense, will add a return value for this function.", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r516979069", "createdAt": "2020-11-03T21:57:20Z", "author": {"login": "LeonGao91"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MountVolumeInfo.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.datanode.fsdataset.impl;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.StorageType;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeReference;\n+\n+import java.nio.channels.ClosedChannelException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n+\n+/**\n+ * MountVolumeInfo is a wrapper of\n+ * detailed volume information for MountVolumeMap.\n+ */\n+@InterfaceAudience.Private\n+class MountVolumeInfo {\n+  private ConcurrentMap<StorageType, FsVolumeImpl>\n+      storageTypeVolumeMap;\n+  private double reservedForArchiveDefault;\n+\n+  MountVolumeInfo(Configuration conf) {\n+    storageTypeVolumeMap = new ConcurrentHashMap<>();\n+    reservedForArchiveDefault = conf.getDouble(\n+        DFSConfigKeys.DFS_DATANODE_RESERVE_FOR_ARCHIVE_DEFAULT_PERCENTAGE,\n+        DFSConfigKeys\n+            .DFS_DATANODE_RESERVE_FOR_ARCHIVE_DEFAULT_PERCENTAGE_DEFAULT);\n+    if (reservedForArchiveDefault > 1) {\n+      FsDatasetImpl.LOG.warn(\"Value of reserve-for-archival is > 100%.\" +\n+          \" Setting it to 100%.\");\n+      reservedForArchiveDefault = 1;\n+    }\n+  }\n+\n+  FsVolumeReference getVolumeRef(StorageType storageType) {\n+    try {\n+      FsVolumeImpl volumeImpl = storageTypeVolumeMap\n+          .getOrDefault(storageType, null);\n+      if (volumeImpl != null) {\n+        return volumeImpl.obtainReference();\n+      }\n+    } catch (ClosedChannelException e) {\n+      FsDatasetImpl.LOG.warn(\"Volume closed when getting volume\" +\n+          \" by storage type: \" + storageType);\n+    }\n+    return null;\n+  }\n+\n+  /**\n+   * Return configured capacity ratio.\n+   * If the volume is the only one on the mount,\n+   * return 1 to avoid unnecessary allocation.\n+   */\n+  double getCapacityRatio(StorageType storageType) {\n+    if (storageTypeVolumeMap.containsKey(storageType)\n+        && storageTypeVolumeMap.size() > 1) {\n+      if (storageType == StorageType.ARCHIVE) {\n+        return reservedForArchiveDefault;\n+      } else if (storageType == StorageType.DISK) {\n+        return 1 - reservedForArchiveDefault;\n+      }\n+    }\n+    return 1;\n+  }\n+\n+  void addVolume(FsVolumeImpl volume) {\n+    if (storageTypeVolumeMap.containsKey(volume.getStorageType())) {\n+      FsDatasetImpl.LOG.error(\"Found storage type already exist.\" +", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM1NTYyNw=="}, "originalCommit": {"oid": "57c52d6d059d29013615642ecf9ad90cddb35c70"}, "originalPosition": 86}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzNjAyNjI1OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MountVolumeInfo.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QwMDoyMTo0N1rOHsb6TQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QwMToyMzoyMlrOHsdddw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM1NjY4NQ==", "bodyText": "we can add a TODO here explaining we plan to support different ratios per mount point.", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r516356685", "createdAt": "2020-11-03T00:21:47Z", "author": {"login": "Jing9"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MountVolumeInfo.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.datanode.fsdataset.impl;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.StorageType;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeReference;\n+\n+import java.nio.channels.ClosedChannelException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n+\n+/**\n+ * MountVolumeInfo is a wrapper of\n+ * detailed volume information for MountVolumeMap.\n+ */\n+@InterfaceAudience.Private\n+class MountVolumeInfo {\n+  private ConcurrentMap<StorageType, FsVolumeImpl>\n+      storageTypeVolumeMap;\n+  private double reservedForArchiveDefault;\n+\n+  MountVolumeInfo(Configuration conf) {\n+    storageTypeVolumeMap = new ConcurrentHashMap<>();\n+    reservedForArchiveDefault = conf.getDouble(\n+        DFSConfigKeys.DFS_DATANODE_RESERVE_FOR_ARCHIVE_DEFAULT_PERCENTAGE,\n+        DFSConfigKeys\n+            .DFS_DATANODE_RESERVE_FOR_ARCHIVE_DEFAULT_PERCENTAGE_DEFAULT);\n+    if (reservedForArchiveDefault > 1) {\n+      FsDatasetImpl.LOG.warn(\"Value of reserve-for-archival is > 100%.\" +\n+          \" Setting it to 100%.\");\n+      reservedForArchiveDefault = 1;\n+    }\n+  }\n+\n+  FsVolumeReference getVolumeRef(StorageType storageType) {\n+    try {\n+      FsVolumeImpl volumeImpl = storageTypeVolumeMap\n+          .getOrDefault(storageType, null);\n+      if (volumeImpl != null) {\n+        return volumeImpl.obtainReference();\n+      }\n+    } catch (ClosedChannelException e) {\n+      FsDatasetImpl.LOG.warn(\"Volume closed when getting volume\" +\n+          \" by storage type: \" + storageType);\n+    }\n+    return null;\n+  }\n+\n+  /**\n+   * Return configured capacity ratio.\n+   * If the volume is the only one on the mount,\n+   * return 1 to avoid unnecessary allocation.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "57c52d6d059d29013615642ecf9ad90cddb35c70"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM4MjA3MQ==", "bodyText": "will do", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r516382071", "createdAt": "2020-11-03T01:23:22Z", "author": {"login": "LeonGao91"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MountVolumeInfo.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.datanode.fsdataset.impl;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.StorageType;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeReference;\n+\n+import java.nio.channels.ClosedChannelException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n+\n+/**\n+ * MountVolumeInfo is a wrapper of\n+ * detailed volume information for MountVolumeMap.\n+ */\n+@InterfaceAudience.Private\n+class MountVolumeInfo {\n+  private ConcurrentMap<StorageType, FsVolumeImpl>\n+      storageTypeVolumeMap;\n+  private double reservedForArchiveDefault;\n+\n+  MountVolumeInfo(Configuration conf) {\n+    storageTypeVolumeMap = new ConcurrentHashMap<>();\n+    reservedForArchiveDefault = conf.getDouble(\n+        DFSConfigKeys.DFS_DATANODE_RESERVE_FOR_ARCHIVE_DEFAULT_PERCENTAGE,\n+        DFSConfigKeys\n+            .DFS_DATANODE_RESERVE_FOR_ARCHIVE_DEFAULT_PERCENTAGE_DEFAULT);\n+    if (reservedForArchiveDefault > 1) {\n+      FsDatasetImpl.LOG.warn(\"Value of reserve-for-archival is > 100%.\" +\n+          \" Setting it to 100%.\");\n+      reservedForArchiveDefault = 1;\n+    }\n+  }\n+\n+  FsVolumeReference getVolumeRef(StorageType storageType) {\n+    try {\n+      FsVolumeImpl volumeImpl = storageTypeVolumeMap\n+          .getOrDefault(storageType, null);\n+      if (volumeImpl != null) {\n+        return volumeImpl.obtainReference();\n+      }\n+    } catch (ClosedChannelException e) {\n+      FsDatasetImpl.LOG.warn(\"Volume closed when getting volume\" +\n+          \" by storage type: \" + storageType);\n+    }\n+    return null;\n+  }\n+\n+  /**\n+   * Return configured capacity ratio.\n+   * If the volume is the only one on the mount,\n+   * return 1 to avoid unnecessary allocation.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM1NjY4NQ=="}, "originalCommit": {"oid": "57c52d6d059d29013615642ecf9ad90cddb35c70"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzNjAyOTc2OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MountVolumeInfo.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QwMDoyMzowNFrOHsb8gw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QwMToyMzoxM1rOHsddVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM1NzI1MQ==", "bodyText": "This field can be declared as \"final\"", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r516357251", "createdAt": "2020-11-03T00:23:04Z", "author": {"login": "Jing9"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MountVolumeInfo.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.datanode.fsdataset.impl;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.StorageType;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeReference;\n+\n+import java.nio.channels.ClosedChannelException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n+\n+/**\n+ * MountVolumeInfo is a wrapper of\n+ * detailed volume information for MountVolumeMap.\n+ */\n+@InterfaceAudience.Private\n+class MountVolumeInfo {\n+  private ConcurrentMap<StorageType, FsVolumeImpl>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "57c52d6d059d29013615642ecf9ad90cddb35c70"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM4MjAzOQ==", "bodyText": "will do", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r516382039", "createdAt": "2020-11-03T01:23:13Z", "author": {"login": "LeonGao91"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MountVolumeInfo.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.datanode.fsdataset.impl;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.StorageType;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeReference;\n+\n+import java.nio.channels.ClosedChannelException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n+\n+/**\n+ * MountVolumeInfo is a wrapper of\n+ * detailed volume information for MountVolumeMap.\n+ */\n+@InterfaceAudience.Private\n+class MountVolumeInfo {\n+  private ConcurrentMap<StorageType, FsVolumeImpl>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM1NzI1MQ=="}, "originalCommit": {"oid": "57c52d6d059d29013615642ecf9ad90cddb35c70"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzNjAzMDI2OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MountVolumeMap.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QwMDoyMzoxOVrOHsb81w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QwMTozNToyM1rOHsdpCQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM1NzMzNQ==", "bodyText": "This field can be declared as \"final\"", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r516357335", "createdAt": "2020-11-03T00:23:19Z", "author": {"login": "Jing9"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MountVolumeMap.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.datanode.fsdataset.impl;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.StorageType;\n+import org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeReference;\n+\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n+\n+/**\n+ * MountVolumeMap contains information of the relationship\n+ * between underlying filesystem mount and datanode volumes.\n+ *\n+ * This is useful when configuring block tiering on same disk mount\n+ * (HDFS-15548). For now,\n+ * we don't configure multiple volumes with same storage type on one mount.\n+ */\n+@InterfaceAudience.Private\n+class MountVolumeMap {\n+  private ConcurrentMap<String, MountVolumeInfo>\n+      mountVolumeMapping;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "57c52d6d059d29013615642ecf9ad90cddb35c70"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM4NTAzMw==", "bodyText": "+1 will do", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r516385033", "createdAt": "2020-11-03T01:35:23Z", "author": {"login": "LeonGao91"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MountVolumeMap.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.datanode.fsdataset.impl;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.StorageType;\n+import org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeReference;\n+\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n+\n+/**\n+ * MountVolumeMap contains information of the relationship\n+ * between underlying filesystem mount and datanode volumes.\n+ *\n+ * This is useful when configuring block tiering on same disk mount\n+ * (HDFS-15548). For now,\n+ * we don't configure multiple volumes with same storage type on one mount.\n+ */\n+@InterfaceAudience.Private\n+class MountVolumeMap {\n+  private ConcurrentMap<String, MountVolumeInfo>\n+      mountVolumeMapping;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM1NzMzNQ=="}, "originalCommit": {"oid": "57c52d6d059d29013615642ecf9ad90cddb35c70"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzNjA0NDk5OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QwMDoyODozNFrOHscGSA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QwMToyMjo0NVrOHsdc_Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM1OTc1Mg==", "bodyText": "let's also add a check \" && usage != null\"", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r516359752", "createdAt": "2020-11-03T00:28:34Z", "author": {"login": "Jing9"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java", "diffHunk": "@@ -190,6 +193,18 @@\n     }\n     this.conf = conf;\n     this.fileIoProvider = fileIoProvider;\n+    this.enableSameDiskTiering =\n+        conf.getBoolean(DFSConfigKeys.DFS_DATANODE_ALLOW_SAME_DISK_TIERING,\n+            DFSConfigKeys.DFS_DATANODE_ALLOW_SAME_DISK_TIERING_DEFAULT);\n+    if (enableSameDiskTiering) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "57c52d6d059d29013615642ecf9ad90cddb35c70"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM4MTk0OQ==", "bodyText": "Will do", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r516381949", "createdAt": "2020-11-03T01:22:45Z", "author": {"login": "LeonGao91"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java", "diffHunk": "@@ -190,6 +193,18 @@\n     }\n     this.conf = conf;\n     this.fileIoProvider = fileIoProvider;\n+    this.enableSameDiskTiering =\n+        conf.getBoolean(DFSConfigKeys.DFS_DATANODE_ALLOW_SAME_DISK_TIERING,\n+            DFSConfigKeys.DFS_DATANODE_ALLOW_SAME_DISK_TIERING_DEFAULT);\n+    if (enableSameDiskTiering) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM1OTc1Mg=="}, "originalCommit": {"oid": "57c52d6d059d29013615642ecf9ad90cddb35c70"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzNjA1ODg1OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QwMDozMzozMVrOHscPIQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QwMToyMjo1N1rOHsddJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM2MjAxNw==", "bodyText": "These two fields can be declared as \"final\"", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r516362017", "createdAt": "2020-11-03T00:33:31Z", "author": {"login": "Jing9"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java", "diffHunk": "@@ -62,9 +63,13 @@\n   private final VolumeChoosingPolicy<FsVolumeImpl> blockChooser;\n   private final BlockScanner blockScanner;\n \n+  private boolean enableSameDiskTiering;\n+  private MountVolumeMap mountVolumeMap;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "57c52d6d059d29013615642ecf9ad90cddb35c70"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM4MTk5MA==", "bodyText": "+1, will fix", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r516381990", "createdAt": "2020-11-03T01:22:57Z", "author": {"login": "LeonGao91"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java", "diffHunk": "@@ -62,9 +63,13 @@\n   private final VolumeChoosingPolicy<FsVolumeImpl> blockChooser;\n   private final BlockScanner blockScanner;\n \n+  private boolean enableSameDiskTiering;\n+  private MountVolumeMap mountVolumeMap;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM2MjAxNw=="}, "originalCommit": {"oid": "57c52d6d059d29013615642ecf9ad90cddb35c70"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzNjA1OTcwOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QwMDozMzo1MFrOHscPvw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QwMToyMzowNlrOHsddQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM2MjE3NQ==", "bodyText": "maybe consider putting this check into a method", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r516362175", "createdAt": "2020-11-03T00:33:50Z", "author": {"login": "Jing9"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java", "diffHunk": "@@ -291,6 +304,11 @@ public String toString() {\n   void addVolume(FsVolumeReference ref) {\n     FsVolumeImpl volume = (FsVolumeImpl) ref.getVolume();\n     volumes.add(volume);\n+    if (enableSameDiskTiering &&", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "57c52d6d059d29013615642ecf9ad90cddb35c70"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM4MjAxNw==", "bodyText": "Good idea, will do that", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r516382017", "createdAt": "2020-11-03T01:23:06Z", "author": {"login": "LeonGao91"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java", "diffHunk": "@@ -291,6 +304,11 @@ public String toString() {\n   void addVolume(FsVolumeReference ref) {\n     FsVolumeImpl volume = (FsVolumeImpl) ref.getVolume();\n     volumes.add(volume);\n+    if (enableSameDiskTiering &&", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM2MjE3NQ=="}, "originalCommit": {"oid": "57c52d6d059d29013615642ecf9ad90cddb35c70"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1NzE3MzA3OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MountVolumeInfo.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwNzoxMDowMFrOHvhMMw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxODoyODo1OVrOHv77sg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTU4ODkxNQ==", "bodyText": "check if it set a negative value?", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r519588915", "createdAt": "2020-11-09T07:10:00Z", "author": {"login": "Hexiaoqiao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MountVolumeInfo.java", "diffHunk": "@@ -0,0 +1,108 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.datanode.fsdataset.impl;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.StorageType;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeReference;\n+\n+import java.nio.channels.ClosedChannelException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n+\n+/**\n+ * MountVolumeInfo is a wrapper of\n+ * detailed volume information for MountVolumeMap.\n+ */\n+@InterfaceAudience.Private\n+class MountVolumeInfo {\n+  private final ConcurrentMap<StorageType, FsVolumeImpl>\n+      storageTypeVolumeMap;\n+  private double reservedForArchiveDefault;\n+\n+  MountVolumeInfo(Configuration conf) {\n+    storageTypeVolumeMap = new ConcurrentHashMap<>();\n+    reservedForArchiveDefault = conf.getDouble(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97f7c03238635f3bc04b13ef92ff0d275cfdada0"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDAyNzA1OA==", "bodyText": "+1", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r520027058", "createdAt": "2020-11-09T18:28:59Z", "author": {"login": "LeonGao91"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MountVolumeInfo.java", "diffHunk": "@@ -0,0 +1,108 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.datanode.fsdataset.impl;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.StorageType;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeReference;\n+\n+import java.nio.channels.ClosedChannelException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n+\n+/**\n+ * MountVolumeInfo is a wrapper of\n+ * detailed volume information for MountVolumeMap.\n+ */\n+@InterfaceAudience.Private\n+class MountVolumeInfo {\n+  private final ConcurrentMap<StorageType, FsVolumeImpl>\n+      storageTypeVolumeMap;\n+  private double reservedForArchiveDefault;\n+\n+  MountVolumeInfo(Configuration conf) {\n+    storageTypeVolumeMap = new ConcurrentHashMap<>();\n+    reservedForArchiveDefault = conf.getDouble(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTU4ODkxNQ=="}, "originalCommit": {"oid": "97f7c03238635f3bc04b13ef92ff0d275cfdada0"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1NzIzODg3OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNamenodeCapacityReport.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwNzoyNjo1MVrOHvh1GA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxODoyOTowM1rOHv77zg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTU5OTM4NA==", "bodyText": "codestyle: redundant empty line.", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r519599384", "createdAt": "2020-11-09T07:26:51Z", "author": {"login": "Hexiaoqiao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNamenodeCapacityReport.java", "diffHunk": "@@ -185,6 +186,59 @@ public void testVolumeSize() throws Exception {\n           (namesystem.getCapacityUsed() + namesystem.getCapacityRemaining()\n               + namesystem.getNonDfsUsedSpace() + fileCount * fs\n               .getDefaultBlockSize()) - configCapacity < 1 * 1024);\n+    } finally {\n+      if (cluster != null) {\n+        cluster.shutdown();\n+      }\n+    }\n+  }\n+\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97f7c03238635f3bc04b13ef92ff0d275cfdada0"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDAyNzA4Ng==", "bodyText": "Good catch, wonder if checksyle should catch it tho.", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r520027086", "createdAt": "2020-11-09T18:29:03Z", "author": {"login": "LeonGao91"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNamenodeCapacityReport.java", "diffHunk": "@@ -185,6 +186,59 @@ public void testVolumeSize() throws Exception {\n           (namesystem.getCapacityUsed() + namesystem.getCapacityRemaining()\n               + namesystem.getNonDfsUsedSpace() + fileCount * fs\n               .getDefaultBlockSize()) - configCapacity < 1 * 1024);\n+    } finally {\n+      if (cluster != null) {\n+        cluster.shutdown();\n+      }\n+    }\n+  }\n+\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTU5OTM4NA=="}, "originalCommit": {"oid": "97f7c03238635f3bc04b13ef92ff0d275cfdada0"}, "originalPosition": 19}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3276, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}