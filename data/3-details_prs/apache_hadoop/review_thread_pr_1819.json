{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzY3MzI2NDk3", "number": 1819, "reviewThreads": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yN1QxNTo1Mjo0MVrODa7WUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQwMjo1NDo1N1rODcH4Gw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI5NTYxOTM3OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yN1QxNTo1Mjo0MVrOFiHswg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQyMjo1MDoyMVrOFj8i5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTMyMjA1MA==", "bodyText": "This isn't required, This would be redundant check, You are having the same check in FSNamesystem. Its required only twice, once before taking lock and once after...", "url": "https://github.com/apache/hadoop/pull/1819#discussion_r371322050", "createdAt": "2020-01-27T15:52:41Z", "author": {"login": "ayushtkn"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java", "diffHunk": "@@ -2667,4 +2667,15 @@ public Long getNextSPSPath() throws IOException {\n     }\n     return namesystem.getBlockManager().getSPSManager().getNextPathId();\n   }\n+\n+  public boolean swapBlockList(String src, String dst, long maxTimestamp)\n+      throws IOException {\n+    checkNNStartup();\n+    if (stateChangeLog.isDebugEnabled()) {\n+      stateChangeLog.debug(\"*DIR* NameNode.swapBlockList: {} and {}\", src, dst);\n+    }\n+    namesystem.checkOperation(OperationCategory.WRITE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eaee621cd681730b1e4cf1c37c261d5e7292e3fc"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzIzNjQ1Mw==", "bodyText": "Fixed this.", "url": "https://github.com/apache/hadoop/pull/1819#discussion_r373236453", "createdAt": "2020-01-30T22:50:21Z", "author": {"login": "avijayanhwx"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java", "diffHunk": "@@ -2667,4 +2667,15 @@ public Long getNextSPSPath() throws IOException {\n     }\n     return namesystem.getBlockManager().getSPSManager().getNextPathId();\n   }\n+\n+  public boolean swapBlockList(String src, String dst, long maxTimestamp)\n+      throws IOException {\n+    checkNNStartup();\n+    if (stateChangeLog.isDebugEnabled()) {\n+      stateChangeLog.debug(\"*DIR* NameNode.swapBlockList: {} and {}\", src, dst);\n+    }\n+    namesystem.checkOperation(OperationCategory.WRITE);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTMyMjA1MA=="}, "originalCommit": {"oid": "eaee621cd681730b1e4cf1c37c261d5e7292e3fc"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI5NTYyOTY2OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SwapBlockListOp.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yN1QxNTo1NToyNFrOFiHzbQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yN1QxNTo1NToyNFrOFiHzbQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTMyMzc1Nw==", "bodyText": "Can use FsDirectory.resolveLastINode()", "url": "https://github.com/apache/hadoop/pull/1819#discussion_r371323757", "createdAt": "2020-01-27T15:55:24Z", "author": {"login": "ayushtkn"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SwapBlockListOp.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdfs.server.namenode;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo;\n+import org.apache.hadoop.hdfs.server.namenode.FSDirectory.DirOp;\n+import org.apache.hadoop.hdfs.server.namenode.INodeFile.HeaderFormat;\n+import org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot;\n+import org.apache.hadoop.util.Time;\n+\n+/**\n+ * Class to carry out the operation of swapping blocks from one file to another.\n+ * Along with swapping blocks, we can also optionally swap the block layout\n+ * of a file header, which is useful for client operations like converting\n+ * replicated to EC file.\n+ */\n+public final class SwapBlockListOp {\n+\n+  private SwapBlockListOp() {\n+  }\n+\n+  static SwapBlockListResult swapBlocks(FSDirectory fsd, FSPermissionChecker pc,\n+                          String src, String dst, long genTimestamp)\n+      throws IOException {\n+\n+    final INodesInPath srcIIP = fsd.resolvePath(pc, src, DirOp.WRITE);\n+    final INodesInPath dstIIP = fsd.resolvePath(pc, dst, DirOp.WRITE);\n+    if (fsd.isPermissionEnabled()) {\n+      fsd.checkAncestorAccess(pc, srcIIP, FsAction.WRITE);\n+      fsd.checkAncestorAccess(pc, dstIIP, FsAction.WRITE);\n+    }\n+    if (NameNode.stateChangeLog.isDebugEnabled()) {\n+      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.swapBlockList: \"\n+          + srcIIP.getPath() + \" and \" + dstIIP.getPath());\n+    }\n+    SwapBlockListResult result = null;\n+    fsd.writeLock();\n+    try {\n+      result = swapBlockList(fsd, srcIIP, dstIIP, genTimestamp);\n+    } finally {\n+      fsd.writeUnlock();\n+    }\n+    return result;\n+  }\n+\n+  private static SwapBlockListResult swapBlockList(FSDirectory fsd,\n+                                    final INodesInPath srcIIP,\n+                                    final INodesInPath dstIIP,\n+                                    long genTimestamp)\n+      throws IOException {\n+\n+    assert fsd.hasWriteLock();\n+    validateInode(srcIIP);\n+    validateInode(dstIIP);\n+    fsd.ezManager.checkMoveValidity(srcIIP, dstIIP);\n+\n+    final String src = srcIIP.getPath();\n+    final String dst = dstIIP.getPath();\n+    if (dst.equals(src)) {\n+      throw new FileAlreadyExistsException(\"The source \" + src +\n+          \" and destination \" + dst + \" are the same\");\n+    }\n+\n+    INodeFile srcINodeFile = (INodeFile) srcIIP.getLastINode();\n+    INodeFile dstINodeFile = (INodeFile) dstIIP.getLastINode();\n+\n+    String errorPrefix = \"DIR* FSDirectory.swapBlockList: \";\n+    String error = \"Swap Block List destination file \";\n+    BlockInfo lastBlock = dstINodeFile.getLastBlock();\n+    if (lastBlock != null && lastBlock.getGenerationStamp() != genTimestamp) {\n+      error  += dstIIP.getPath() +\n+          \" has last block with different gen timestamp.\";\n+      NameNode.stateChangeLog.warn(errorPrefix + error);\n+      throw new IOException(error);\n+    }\n+\n+    long mtime = Time.now();\n+    BlockInfo[] dstINodeFileBlocks = dstINodeFile.getBlocks();\n+    dstINodeFile.replaceBlocks(srcINodeFile.getBlocks());\n+    srcINodeFile.replaceBlocks(dstINodeFileBlocks);\n+\n+    long srcHeader = srcINodeFile.getHeaderLong();\n+    long dstHeader = dstINodeFile.getHeaderLong();\n+\n+    byte dstBlockLayoutPolicy =\n+        HeaderFormat.getBlockLayoutPolicy(dstHeader);\n+    byte srcBlockLayoutPolicy =\n+        HeaderFormat.getBlockLayoutPolicy(srcHeader);\n+\n+    byte dstStoragePolicyID = HeaderFormat.getStoragePolicyID(dstHeader);\n+    byte srcStoragePolicyID = HeaderFormat.getStoragePolicyID(srcHeader);\n+\n+    dstINodeFile.updateHeaderWithNewPolicy(srcBlockLayoutPolicy,\n+        srcStoragePolicyID);\n+    dstINodeFile.setModificationTime(mtime);\n+\n+    srcINodeFile.updateHeaderWithNewPolicy(dstBlockLayoutPolicy,\n+        dstStoragePolicyID);\n+    srcINodeFile.setModificationTime(mtime);\n+\n+    return new SwapBlockListResult(true,\n+        fsd.getAuditFileInfo(srcIIP),\n+        fsd.getAuditFileInfo(dstIIP));\n+  }\n+\n+  private static void validateInode(INodesInPath srcIIP)\n+      throws IOException {\n+\n+    String errorPrefix = \"DIR* FSDirectory.swapBlockList: \";\n+    String error = \"Swap Block List input \";\n+    final INode srcInode = srcIIP.getLastINode();\n+\n+    // Check if INode is null.\n+    if (srcInode == null) {\n+      error  += srcIIP.getPath() + \" is not found.\";\n+      NameNode.stateChangeLog.warn(errorPrefix + error);\n+      throw new FileNotFoundException(error);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eaee621cd681730b1e4cf1c37c261d5e7292e3fc"}, "originalPosition": 140}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI5NTYzMzU3OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SwapBlockListOp.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yN1QxNTo1NjoyMFrOFiH14Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQyMjo1MDoyNlrOFj8jAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTMyNDM4NQ==", "bodyText": "nit. My IDE complains that the null assignment is redundant. No need to assign null, it is already null.", "url": "https://github.com/apache/hadoop/pull/1819#discussion_r371324385", "createdAt": "2020-01-27T15:56:20Z", "author": {"login": "ayushtkn"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SwapBlockListOp.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdfs.server.namenode;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo;\n+import org.apache.hadoop.hdfs.server.namenode.FSDirectory.DirOp;\n+import org.apache.hadoop.hdfs.server.namenode.INodeFile.HeaderFormat;\n+import org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot;\n+import org.apache.hadoop.util.Time;\n+\n+/**\n+ * Class to carry out the operation of swapping blocks from one file to another.\n+ * Along with swapping blocks, we can also optionally swap the block layout\n+ * of a file header, which is useful for client operations like converting\n+ * replicated to EC file.\n+ */\n+public final class SwapBlockListOp {\n+\n+  private SwapBlockListOp() {\n+  }\n+\n+  static SwapBlockListResult swapBlocks(FSDirectory fsd, FSPermissionChecker pc,\n+                          String src, String dst, long genTimestamp)\n+      throws IOException {\n+\n+    final INodesInPath srcIIP = fsd.resolvePath(pc, src, DirOp.WRITE);\n+    final INodesInPath dstIIP = fsd.resolvePath(pc, dst, DirOp.WRITE);\n+    if (fsd.isPermissionEnabled()) {\n+      fsd.checkAncestorAccess(pc, srcIIP, FsAction.WRITE);\n+      fsd.checkAncestorAccess(pc, dstIIP, FsAction.WRITE);\n+    }\n+    if (NameNode.stateChangeLog.isDebugEnabled()) {\n+      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.swapBlockList: \"\n+          + srcIIP.getPath() + \" and \" + dstIIP.getPath());\n+    }\n+    SwapBlockListResult result = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eaee621cd681730b1e4cf1c37c261d5e7292e3fc"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzIzNjQ4MQ==", "bodyText": "Fixed this.", "url": "https://github.com/apache/hadoop/pull/1819#discussion_r373236481", "createdAt": "2020-01-30T22:50:26Z", "author": {"login": "avijayanhwx"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SwapBlockListOp.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdfs.server.namenode;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo;\n+import org.apache.hadoop.hdfs.server.namenode.FSDirectory.DirOp;\n+import org.apache.hadoop.hdfs.server.namenode.INodeFile.HeaderFormat;\n+import org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot;\n+import org.apache.hadoop.util.Time;\n+\n+/**\n+ * Class to carry out the operation of swapping blocks from one file to another.\n+ * Along with swapping blocks, we can also optionally swap the block layout\n+ * of a file header, which is useful for client operations like converting\n+ * replicated to EC file.\n+ */\n+public final class SwapBlockListOp {\n+\n+  private SwapBlockListOp() {\n+  }\n+\n+  static SwapBlockListResult swapBlocks(FSDirectory fsd, FSPermissionChecker pc,\n+                          String src, String dst, long genTimestamp)\n+      throws IOException {\n+\n+    final INodesInPath srcIIP = fsd.resolvePath(pc, src, DirOp.WRITE);\n+    final INodesInPath dstIIP = fsd.resolvePath(pc, dst, DirOp.WRITE);\n+    if (fsd.isPermissionEnabled()) {\n+      fsd.checkAncestorAccess(pc, srcIIP, FsAction.WRITE);\n+      fsd.checkAncestorAccess(pc, dstIIP, FsAction.WRITE);\n+    }\n+    if (NameNode.stateChangeLog.isDebugEnabled()) {\n+      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.swapBlockList: \"\n+          + srcIIP.getPath() + \" and \" + dstIIP.getPath());\n+    }\n+    SwapBlockListResult result = null;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTMyNDM4NQ=="}, "originalCommit": {"oid": "eaee621cd681730b1e4cf1c37c261d5e7292e3fc"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI5NTY3OTkxOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SwapBlockListOp.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yN1QxNjowNzo0NlrOFiISTQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQyMjo1MDozM1rOFj8jMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTMzMTY2MQ==", "bodyText": "Can use srcIIP.getLastINode().asFile(). Looks better. :)", "url": "https://github.com/apache/hadoop/pull/1819#discussion_r371331661", "createdAt": "2020-01-27T16:07:46Z", "author": {"login": "ayushtkn"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SwapBlockListOp.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdfs.server.namenode;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo;\n+import org.apache.hadoop.hdfs.server.namenode.FSDirectory.DirOp;\n+import org.apache.hadoop.hdfs.server.namenode.INodeFile.HeaderFormat;\n+import org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot;\n+import org.apache.hadoop.util.Time;\n+\n+/**\n+ * Class to carry out the operation of swapping blocks from one file to another.\n+ * Along with swapping blocks, we can also optionally swap the block layout\n+ * of a file header, which is useful for client operations like converting\n+ * replicated to EC file.\n+ */\n+public final class SwapBlockListOp {\n+\n+  private SwapBlockListOp() {\n+  }\n+\n+  static SwapBlockListResult swapBlocks(FSDirectory fsd, FSPermissionChecker pc,\n+                          String src, String dst, long genTimestamp)\n+      throws IOException {\n+\n+    final INodesInPath srcIIP = fsd.resolvePath(pc, src, DirOp.WRITE);\n+    final INodesInPath dstIIP = fsd.resolvePath(pc, dst, DirOp.WRITE);\n+    if (fsd.isPermissionEnabled()) {\n+      fsd.checkAncestorAccess(pc, srcIIP, FsAction.WRITE);\n+      fsd.checkAncestorAccess(pc, dstIIP, FsAction.WRITE);\n+    }\n+    if (NameNode.stateChangeLog.isDebugEnabled()) {\n+      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.swapBlockList: \"\n+          + srcIIP.getPath() + \" and \" + dstIIP.getPath());\n+    }\n+    SwapBlockListResult result = null;\n+    fsd.writeLock();\n+    try {\n+      result = swapBlockList(fsd, srcIIP, dstIIP, genTimestamp);\n+    } finally {\n+      fsd.writeUnlock();\n+    }\n+    return result;\n+  }\n+\n+  private static SwapBlockListResult swapBlockList(FSDirectory fsd,\n+                                    final INodesInPath srcIIP,\n+                                    final INodesInPath dstIIP,\n+                                    long genTimestamp)\n+      throws IOException {\n+\n+    assert fsd.hasWriteLock();\n+    validateInode(srcIIP);\n+    validateInode(dstIIP);\n+    fsd.ezManager.checkMoveValidity(srcIIP, dstIIP);\n+\n+    final String src = srcIIP.getPath();\n+    final String dst = dstIIP.getPath();\n+    if (dst.equals(src)) {\n+      throw new FileAlreadyExistsException(\"The source \" + src +\n+          \" and destination \" + dst + \" are the same\");\n+    }\n+\n+    INodeFile srcINodeFile = (INodeFile) srcIIP.getLastINode();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eaee621cd681730b1e4cf1c37c261d5e7292e3fc"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzIzNjUyOA==", "bodyText": "Fixed this.", "url": "https://github.com/apache/hadoop/pull/1819#discussion_r373236528", "createdAt": "2020-01-30T22:50:33Z", "author": {"login": "avijayanhwx"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SwapBlockListOp.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdfs.server.namenode;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo;\n+import org.apache.hadoop.hdfs.server.namenode.FSDirectory.DirOp;\n+import org.apache.hadoop.hdfs.server.namenode.INodeFile.HeaderFormat;\n+import org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot;\n+import org.apache.hadoop.util.Time;\n+\n+/**\n+ * Class to carry out the operation of swapping blocks from one file to another.\n+ * Along with swapping blocks, we can also optionally swap the block layout\n+ * of a file header, which is useful for client operations like converting\n+ * replicated to EC file.\n+ */\n+public final class SwapBlockListOp {\n+\n+  private SwapBlockListOp() {\n+  }\n+\n+  static SwapBlockListResult swapBlocks(FSDirectory fsd, FSPermissionChecker pc,\n+                          String src, String dst, long genTimestamp)\n+      throws IOException {\n+\n+    final INodesInPath srcIIP = fsd.resolvePath(pc, src, DirOp.WRITE);\n+    final INodesInPath dstIIP = fsd.resolvePath(pc, dst, DirOp.WRITE);\n+    if (fsd.isPermissionEnabled()) {\n+      fsd.checkAncestorAccess(pc, srcIIP, FsAction.WRITE);\n+      fsd.checkAncestorAccess(pc, dstIIP, FsAction.WRITE);\n+    }\n+    if (NameNode.stateChangeLog.isDebugEnabled()) {\n+      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.swapBlockList: \"\n+          + srcIIP.getPath() + \" and \" + dstIIP.getPath());\n+    }\n+    SwapBlockListResult result = null;\n+    fsd.writeLock();\n+    try {\n+      result = swapBlockList(fsd, srcIIP, dstIIP, genTimestamp);\n+    } finally {\n+      fsd.writeUnlock();\n+    }\n+    return result;\n+  }\n+\n+  private static SwapBlockListResult swapBlockList(FSDirectory fsd,\n+                                    final INodesInPath srcIIP,\n+                                    final INodesInPath dstIIP,\n+                                    long genTimestamp)\n+      throws IOException {\n+\n+    assert fsd.hasWriteLock();\n+    validateInode(srcIIP);\n+    validateInode(dstIIP);\n+    fsd.ezManager.checkMoveValidity(srcIIP, dstIIP);\n+\n+    final String src = srcIIP.getPath();\n+    final String dst = dstIIP.getPath();\n+    if (dst.equals(src)) {\n+      throw new FileAlreadyExistsException(\"The source \" + src +\n+          \" and destination \" + dst + \" are the same\");\n+    }\n+\n+    INodeFile srcINodeFile = (INodeFile) srcIIP.getLastINode();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTMzMTY2MQ=="}, "originalCommit": {"oid": "eaee621cd681730b1e4cf1c37c261d5e7292e3fc"}, "originalPosition": 86}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI5NTY5NjY1OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SwapBlockListOp.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yN1QxNjoxMTo1MVrOFiIcXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQyMjo1MjoxNVrOFj8luQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTMzNDIzNw==", "bodyText": "nit : Refactored dstInodeFIleBlocks and used only once, but didn't do for srcInodeFile. May be should keep same for both.", "url": "https://github.com/apache/hadoop/pull/1819#discussion_r371334237", "createdAt": "2020-01-27T16:11:51Z", "author": {"login": "ayushtkn"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SwapBlockListOp.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdfs.server.namenode;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo;\n+import org.apache.hadoop.hdfs.server.namenode.FSDirectory.DirOp;\n+import org.apache.hadoop.hdfs.server.namenode.INodeFile.HeaderFormat;\n+import org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot;\n+import org.apache.hadoop.util.Time;\n+\n+/**\n+ * Class to carry out the operation of swapping blocks from one file to another.\n+ * Along with swapping blocks, we can also optionally swap the block layout\n+ * of a file header, which is useful for client operations like converting\n+ * replicated to EC file.\n+ */\n+public final class SwapBlockListOp {\n+\n+  private SwapBlockListOp() {\n+  }\n+\n+  static SwapBlockListResult swapBlocks(FSDirectory fsd, FSPermissionChecker pc,\n+                          String src, String dst, long genTimestamp)\n+      throws IOException {\n+\n+    final INodesInPath srcIIP = fsd.resolvePath(pc, src, DirOp.WRITE);\n+    final INodesInPath dstIIP = fsd.resolvePath(pc, dst, DirOp.WRITE);\n+    if (fsd.isPermissionEnabled()) {\n+      fsd.checkAncestorAccess(pc, srcIIP, FsAction.WRITE);\n+      fsd.checkAncestorAccess(pc, dstIIP, FsAction.WRITE);\n+    }\n+    if (NameNode.stateChangeLog.isDebugEnabled()) {\n+      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.swapBlockList: \"\n+          + srcIIP.getPath() + \" and \" + dstIIP.getPath());\n+    }\n+    SwapBlockListResult result = null;\n+    fsd.writeLock();\n+    try {\n+      result = swapBlockList(fsd, srcIIP, dstIIP, genTimestamp);\n+    } finally {\n+      fsd.writeUnlock();\n+    }\n+    return result;\n+  }\n+\n+  private static SwapBlockListResult swapBlockList(FSDirectory fsd,\n+                                    final INodesInPath srcIIP,\n+                                    final INodesInPath dstIIP,\n+                                    long genTimestamp)\n+      throws IOException {\n+\n+    assert fsd.hasWriteLock();\n+    validateInode(srcIIP);\n+    validateInode(dstIIP);\n+    fsd.ezManager.checkMoveValidity(srcIIP, dstIIP);\n+\n+    final String src = srcIIP.getPath();\n+    final String dst = dstIIP.getPath();\n+    if (dst.equals(src)) {\n+      throw new FileAlreadyExistsException(\"The source \" + src +\n+          \" and destination \" + dst + \" are the same\");\n+    }\n+\n+    INodeFile srcINodeFile = (INodeFile) srcIIP.getLastINode();\n+    INodeFile dstINodeFile = (INodeFile) dstIIP.getLastINode();\n+\n+    String errorPrefix = \"DIR* FSDirectory.swapBlockList: \";\n+    String error = \"Swap Block List destination file \";\n+    BlockInfo lastBlock = dstINodeFile.getLastBlock();\n+    if (lastBlock != null && lastBlock.getGenerationStamp() != genTimestamp) {\n+      error  += dstIIP.getPath() +\n+          \" has last block with different gen timestamp.\";\n+      NameNode.stateChangeLog.warn(errorPrefix + error);\n+      throw new IOException(error);\n+    }\n+\n+    long mtime = Time.now();\n+    BlockInfo[] dstINodeFileBlocks = dstINodeFile.getBlocks();\n+    dstINodeFile.replaceBlocks(srcINodeFile.getBlocks());\n+    srcINodeFile.replaceBlocks(dstINodeFileBlocks);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eaee621cd681730b1e4cf1c37c261d5e7292e3fc"}, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjY0OTEyNg==", "bodyText": "@ayushtkn I don't quite understand the issue here. I implemented a temp variable based swap mechanism. Could you explain it more?", "url": "https://github.com/apache/hadoop/pull/1819#discussion_r372649126", "createdAt": "2020-01-29T21:48:22Z", "author": {"login": "avijayanhwx"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SwapBlockListOp.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdfs.server.namenode;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo;\n+import org.apache.hadoop.hdfs.server.namenode.FSDirectory.DirOp;\n+import org.apache.hadoop.hdfs.server.namenode.INodeFile.HeaderFormat;\n+import org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot;\n+import org.apache.hadoop.util.Time;\n+\n+/**\n+ * Class to carry out the operation of swapping blocks from one file to another.\n+ * Along with swapping blocks, we can also optionally swap the block layout\n+ * of a file header, which is useful for client operations like converting\n+ * replicated to EC file.\n+ */\n+public final class SwapBlockListOp {\n+\n+  private SwapBlockListOp() {\n+  }\n+\n+  static SwapBlockListResult swapBlocks(FSDirectory fsd, FSPermissionChecker pc,\n+                          String src, String dst, long genTimestamp)\n+      throws IOException {\n+\n+    final INodesInPath srcIIP = fsd.resolvePath(pc, src, DirOp.WRITE);\n+    final INodesInPath dstIIP = fsd.resolvePath(pc, dst, DirOp.WRITE);\n+    if (fsd.isPermissionEnabled()) {\n+      fsd.checkAncestorAccess(pc, srcIIP, FsAction.WRITE);\n+      fsd.checkAncestorAccess(pc, dstIIP, FsAction.WRITE);\n+    }\n+    if (NameNode.stateChangeLog.isDebugEnabled()) {\n+      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.swapBlockList: \"\n+          + srcIIP.getPath() + \" and \" + dstIIP.getPath());\n+    }\n+    SwapBlockListResult result = null;\n+    fsd.writeLock();\n+    try {\n+      result = swapBlockList(fsd, srcIIP, dstIIP, genTimestamp);\n+    } finally {\n+      fsd.writeUnlock();\n+    }\n+    return result;\n+  }\n+\n+  private static SwapBlockListResult swapBlockList(FSDirectory fsd,\n+                                    final INodesInPath srcIIP,\n+                                    final INodesInPath dstIIP,\n+                                    long genTimestamp)\n+      throws IOException {\n+\n+    assert fsd.hasWriteLock();\n+    validateInode(srcIIP);\n+    validateInode(dstIIP);\n+    fsd.ezManager.checkMoveValidity(srcIIP, dstIIP);\n+\n+    final String src = srcIIP.getPath();\n+    final String dst = dstIIP.getPath();\n+    if (dst.equals(src)) {\n+      throw new FileAlreadyExistsException(\"The source \" + src +\n+          \" and destination \" + dst + \" are the same\");\n+    }\n+\n+    INodeFile srcINodeFile = (INodeFile) srcIIP.getLastINode();\n+    INodeFile dstINodeFile = (INodeFile) dstIIP.getLastINode();\n+\n+    String errorPrefix = \"DIR* FSDirectory.swapBlockList: \";\n+    String error = \"Swap Block List destination file \";\n+    BlockInfo lastBlock = dstINodeFile.getLastBlock();\n+    if (lastBlock != null && lastBlock.getGenerationStamp() != genTimestamp) {\n+      error  += dstIIP.getPath() +\n+          \" has last block with different gen timestamp.\";\n+      NameNode.stateChangeLog.warn(errorPrefix + error);\n+      throw new IOException(error);\n+    }\n+\n+    long mtime = Time.now();\n+    BlockInfo[] dstINodeFileBlocks = dstINodeFile.getBlocks();\n+    dstINodeFile.replaceBlocks(srcINodeFile.getBlocks());\n+    srcINodeFile.replaceBlocks(dstINodeFileBlocks);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTMzNDIzNw=="}, "originalCommit": {"oid": "eaee621cd681730b1e4cf1c37c261d5e7292e3fc"}, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mjk4MDM1MA==", "bodyText": "I meant just refactor this too, srcINodeFile.getBlocks() as you did for dstINodeFile.getBlocks() into a variable, to match the code style with  srcINodeFile.replaceBlocks(dstINodeFileBlocks);\nOptional stuff, If you prefer to do. :)", "url": "https://github.com/apache/hadoop/pull/1819#discussion_r372980350", "createdAt": "2020-01-30T14:29:25Z", "author": {"login": "ayushtkn"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SwapBlockListOp.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdfs.server.namenode;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo;\n+import org.apache.hadoop.hdfs.server.namenode.FSDirectory.DirOp;\n+import org.apache.hadoop.hdfs.server.namenode.INodeFile.HeaderFormat;\n+import org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot;\n+import org.apache.hadoop.util.Time;\n+\n+/**\n+ * Class to carry out the operation of swapping blocks from one file to another.\n+ * Along with swapping blocks, we can also optionally swap the block layout\n+ * of a file header, which is useful for client operations like converting\n+ * replicated to EC file.\n+ */\n+public final class SwapBlockListOp {\n+\n+  private SwapBlockListOp() {\n+  }\n+\n+  static SwapBlockListResult swapBlocks(FSDirectory fsd, FSPermissionChecker pc,\n+                          String src, String dst, long genTimestamp)\n+      throws IOException {\n+\n+    final INodesInPath srcIIP = fsd.resolvePath(pc, src, DirOp.WRITE);\n+    final INodesInPath dstIIP = fsd.resolvePath(pc, dst, DirOp.WRITE);\n+    if (fsd.isPermissionEnabled()) {\n+      fsd.checkAncestorAccess(pc, srcIIP, FsAction.WRITE);\n+      fsd.checkAncestorAccess(pc, dstIIP, FsAction.WRITE);\n+    }\n+    if (NameNode.stateChangeLog.isDebugEnabled()) {\n+      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.swapBlockList: \"\n+          + srcIIP.getPath() + \" and \" + dstIIP.getPath());\n+    }\n+    SwapBlockListResult result = null;\n+    fsd.writeLock();\n+    try {\n+      result = swapBlockList(fsd, srcIIP, dstIIP, genTimestamp);\n+    } finally {\n+      fsd.writeUnlock();\n+    }\n+    return result;\n+  }\n+\n+  private static SwapBlockListResult swapBlockList(FSDirectory fsd,\n+                                    final INodesInPath srcIIP,\n+                                    final INodesInPath dstIIP,\n+                                    long genTimestamp)\n+      throws IOException {\n+\n+    assert fsd.hasWriteLock();\n+    validateInode(srcIIP);\n+    validateInode(dstIIP);\n+    fsd.ezManager.checkMoveValidity(srcIIP, dstIIP);\n+\n+    final String src = srcIIP.getPath();\n+    final String dst = dstIIP.getPath();\n+    if (dst.equals(src)) {\n+      throw new FileAlreadyExistsException(\"The source \" + src +\n+          \" and destination \" + dst + \" are the same\");\n+    }\n+\n+    INodeFile srcINodeFile = (INodeFile) srcIIP.getLastINode();\n+    INodeFile dstINodeFile = (INodeFile) dstIIP.getLastINode();\n+\n+    String errorPrefix = \"DIR* FSDirectory.swapBlockList: \";\n+    String error = \"Swap Block List destination file \";\n+    BlockInfo lastBlock = dstINodeFile.getLastBlock();\n+    if (lastBlock != null && lastBlock.getGenerationStamp() != genTimestamp) {\n+      error  += dstIIP.getPath() +\n+          \" has last block with different gen timestamp.\";\n+      NameNode.stateChangeLog.warn(errorPrefix + error);\n+      throw new IOException(error);\n+    }\n+\n+    long mtime = Time.now();\n+    BlockInfo[] dstINodeFileBlocks = dstINodeFile.getBlocks();\n+    dstINodeFile.replaceBlocks(srcINodeFile.getBlocks());\n+    srcINodeFile.replaceBlocks(dstINodeFileBlocks);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTMzNDIzNw=="}, "originalCommit": {"oid": "eaee621cd681730b1e4cf1c37c261d5e7292e3fc"}, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzIzNzE3Nw==", "bodyText": "@ayushtkn I had to store dstINodeFile.getBlocks() into a variable because without that, after line 101, I would have lost the original dstINodeFile blocks. For scrInodeFile, I don't need that. So I left that out.", "url": "https://github.com/apache/hadoop/pull/1819#discussion_r373237177", "createdAt": "2020-01-30T22:52:15Z", "author": {"login": "avijayanhwx"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SwapBlockListOp.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdfs.server.namenode;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo;\n+import org.apache.hadoop.hdfs.server.namenode.FSDirectory.DirOp;\n+import org.apache.hadoop.hdfs.server.namenode.INodeFile.HeaderFormat;\n+import org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot;\n+import org.apache.hadoop.util.Time;\n+\n+/**\n+ * Class to carry out the operation of swapping blocks from one file to another.\n+ * Along with swapping blocks, we can also optionally swap the block layout\n+ * of a file header, which is useful for client operations like converting\n+ * replicated to EC file.\n+ */\n+public final class SwapBlockListOp {\n+\n+  private SwapBlockListOp() {\n+  }\n+\n+  static SwapBlockListResult swapBlocks(FSDirectory fsd, FSPermissionChecker pc,\n+                          String src, String dst, long genTimestamp)\n+      throws IOException {\n+\n+    final INodesInPath srcIIP = fsd.resolvePath(pc, src, DirOp.WRITE);\n+    final INodesInPath dstIIP = fsd.resolvePath(pc, dst, DirOp.WRITE);\n+    if (fsd.isPermissionEnabled()) {\n+      fsd.checkAncestorAccess(pc, srcIIP, FsAction.WRITE);\n+      fsd.checkAncestorAccess(pc, dstIIP, FsAction.WRITE);\n+    }\n+    if (NameNode.stateChangeLog.isDebugEnabled()) {\n+      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.swapBlockList: \"\n+          + srcIIP.getPath() + \" and \" + dstIIP.getPath());\n+    }\n+    SwapBlockListResult result = null;\n+    fsd.writeLock();\n+    try {\n+      result = swapBlockList(fsd, srcIIP, dstIIP, genTimestamp);\n+    } finally {\n+      fsd.writeUnlock();\n+    }\n+    return result;\n+  }\n+\n+  private static SwapBlockListResult swapBlockList(FSDirectory fsd,\n+                                    final INodesInPath srcIIP,\n+                                    final INodesInPath dstIIP,\n+                                    long genTimestamp)\n+      throws IOException {\n+\n+    assert fsd.hasWriteLock();\n+    validateInode(srcIIP);\n+    validateInode(dstIIP);\n+    fsd.ezManager.checkMoveValidity(srcIIP, dstIIP);\n+\n+    final String src = srcIIP.getPath();\n+    final String dst = dstIIP.getPath();\n+    if (dst.equals(src)) {\n+      throw new FileAlreadyExistsException(\"The source \" + src +\n+          \" and destination \" + dst + \" are the same\");\n+    }\n+\n+    INodeFile srcINodeFile = (INodeFile) srcIIP.getLastINode();\n+    INodeFile dstINodeFile = (INodeFile) dstIIP.getLastINode();\n+\n+    String errorPrefix = \"DIR* FSDirectory.swapBlockList: \";\n+    String error = \"Swap Block List destination file \";\n+    BlockInfo lastBlock = dstINodeFile.getLastBlock();\n+    if (lastBlock != null && lastBlock.getGenerationStamp() != genTimestamp) {\n+      error  += dstIIP.getPath() +\n+          \" has last block with different gen timestamp.\";\n+      NameNode.stateChangeLog.warn(errorPrefix + error);\n+      throw new IOException(error);\n+    }\n+\n+    long mtime = Time.now();\n+    BlockInfo[] dstINodeFileBlocks = dstINodeFile.getBlocks();\n+    dstINodeFile.replaceBlocks(srcINodeFile.getBlocks());\n+    srcINodeFile.replaceBlocks(dstINodeFileBlocks);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTMzNDIzNw=="}, "originalCommit": {"oid": "eaee621cd681730b1e4cf1c37c261d5e7292e3fc"}, "originalPosition": 102}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI5NTgyNDA0OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestSwapBlockList.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yN1QxNjo0NTo0MlrOFiJr2A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQyMjo1MDozOVrOFj8jVg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTM1NDU4NA==", "bodyText": "For the validations can use LambdaTestUtils, Like this\n LambdaTestUtils.intercept(FileNotFoundException.class, \"/TestSwapBlockList/dir1/fileXYZ\", () -> fsn .swapBlockList(\"/TestSwapBlockList/dir1/fileXYZ\", \"/TestSwapBlockList/dir1/dir11/file3\", 0L));", "url": "https://github.com/apache/hadoop/pull/1819#discussion_r371354584", "createdAt": "2020-01-27T16:45:42Z", "author": {"login": "ayushtkn"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestSwapBlockList.java", "diffHunk": "@@ -0,0 +1,257 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdfs.server.namenode;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.DFSTestUtil;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo;\n+import org.apache.hadoop.hdfs.server.namenode.INodeFile.HeaderFormat;\n+import org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotTestHelper;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+/**\n+ * Test SwapBlockListOp working.\n+ */\n+public class TestSwapBlockList {\n+\n+  private static final short REPLICATION = 3;\n+\n+  private static final long SEED = 0;\n+  private final Path rootDir = new Path(\"/\" + getClass().getSimpleName());\n+\n+  private final Path subDir1 = new Path(rootDir, \"dir1\");\n+  private final Path file1 = new Path(subDir1, \"file1\");\n+  private final Path file2 = new Path(subDir1, \"file2\");\n+\n+  private final Path subDir11 = new Path(subDir1, \"dir11\");\n+  private final Path file3 = new Path(subDir11, \"file3\");\n+\n+  private final Path subDir2 = new Path(rootDir, \"dir2\");\n+  private final Path file4 = new Path(subDir2, \"file4\");\n+\n+  private Configuration conf;\n+  private MiniDFSCluster cluster;\n+  private FSNamesystem fsn;\n+  private FSDirectory fsdir;\n+\n+  private DistributedFileSystem hdfs;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    conf.setInt(DFSConfigKeys.DFS_NAMENODE_MAX_XATTRS_PER_INODE_KEY, 2);\n+    cluster = new MiniDFSCluster.Builder(conf)\n+        .numDataNodes(REPLICATION)\n+        .build();\n+    cluster.waitActive();\n+\n+    fsn = cluster.getNamesystem();\n+    fsdir = fsn.getFSDirectory();\n+\n+    hdfs = cluster.getFileSystem();\n+\n+    hdfs.mkdirs(subDir2);\n+\n+    DFSTestUtil.createFile(hdfs, file1, 1024, REPLICATION, SEED);\n+    DFSTestUtil.createFile(hdfs, file2, 1024, REPLICATION, SEED);\n+    DFSTestUtil.createFile(hdfs, file3, 1024, REPLICATION, SEED);\n+    DFSTestUtil.createFile(hdfs, file4, 1024, REPLICATION, SEED);\n+\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  @Test\n+  public void testInputValidation() throws Exception {\n+\n+    // Source file not found.\n+    try {\n+      fsn.swapBlockList(\"/TestSwapBlockList/dir1/fileXYZ\",\n+          \"/TestSwapBlockList/dir1/dir11/file3\", 0L);\n+      Assert.fail();\n+    } catch (IOException ioEx) {\n+      Assert.assertTrue(ioEx instanceof FileNotFoundException);\n+      Assert.assertTrue(\n+          ioEx.getMessage().contains(\"/TestSwapBlockList/dir1/fileXYZ\"));\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eaee621cd681730b1e4cf1c37c261d5e7292e3fc"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzIzNjU2Ng==", "bodyText": "Fixed this.", "url": "https://github.com/apache/hadoop/pull/1819#discussion_r373236566", "createdAt": "2020-01-30T22:50:39Z", "author": {"login": "avijayanhwx"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestSwapBlockList.java", "diffHunk": "@@ -0,0 +1,257 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdfs.server.namenode;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.DFSTestUtil;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo;\n+import org.apache.hadoop.hdfs.server.namenode.INodeFile.HeaderFormat;\n+import org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotTestHelper;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+/**\n+ * Test SwapBlockListOp working.\n+ */\n+public class TestSwapBlockList {\n+\n+  private static final short REPLICATION = 3;\n+\n+  private static final long SEED = 0;\n+  private final Path rootDir = new Path(\"/\" + getClass().getSimpleName());\n+\n+  private final Path subDir1 = new Path(rootDir, \"dir1\");\n+  private final Path file1 = new Path(subDir1, \"file1\");\n+  private final Path file2 = new Path(subDir1, \"file2\");\n+\n+  private final Path subDir11 = new Path(subDir1, \"dir11\");\n+  private final Path file3 = new Path(subDir11, \"file3\");\n+\n+  private final Path subDir2 = new Path(rootDir, \"dir2\");\n+  private final Path file4 = new Path(subDir2, \"file4\");\n+\n+  private Configuration conf;\n+  private MiniDFSCluster cluster;\n+  private FSNamesystem fsn;\n+  private FSDirectory fsdir;\n+\n+  private DistributedFileSystem hdfs;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    conf.setInt(DFSConfigKeys.DFS_NAMENODE_MAX_XATTRS_PER_INODE_KEY, 2);\n+    cluster = new MiniDFSCluster.Builder(conf)\n+        .numDataNodes(REPLICATION)\n+        .build();\n+    cluster.waitActive();\n+\n+    fsn = cluster.getNamesystem();\n+    fsdir = fsn.getFSDirectory();\n+\n+    hdfs = cluster.getFileSystem();\n+\n+    hdfs.mkdirs(subDir2);\n+\n+    DFSTestUtil.createFile(hdfs, file1, 1024, REPLICATION, SEED);\n+    DFSTestUtil.createFile(hdfs, file2, 1024, REPLICATION, SEED);\n+    DFSTestUtil.createFile(hdfs, file3, 1024, REPLICATION, SEED);\n+    DFSTestUtil.createFile(hdfs, file4, 1024, REPLICATION, SEED);\n+\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  @Test\n+  public void testInputValidation() throws Exception {\n+\n+    // Source file not found.\n+    try {\n+      fsn.swapBlockList(\"/TestSwapBlockList/dir1/fileXYZ\",\n+          \"/TestSwapBlockList/dir1/dir11/file3\", 0L);\n+      Assert.fail();\n+    } catch (IOException ioEx) {\n+      Assert.assertTrue(ioEx instanceof FileNotFoundException);\n+      Assert.assertTrue(\n+          ioEx.getMessage().contains(\"/TestSwapBlockList/dir1/fileXYZ\"));\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTM1NDU4NA=="}, "originalCommit": {"oid": "eaee621cd681730b1e4cf1c37c261d5e7292e3fc"}, "originalPosition": 109}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwODE1NzcxOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SwapBlockListOp.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQwMjo1NDo1N1rOFkANKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQyMTowNjoxMlrOFkX75A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzI5NjQyNw==", "bodyText": "Any reason for not assigning FSDirectory.resolveLastINode(srcIIP); to srcInode?", "url": "https://github.com/apache/hadoop/pull/1819#discussion_r373296427", "createdAt": "2020-01-31T02:54:57Z", "author": {"login": "ayushtkn"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SwapBlockListOp.java", "diffHunk": "@@ -130,15 +129,10 @@ private static void validateInode(INodesInPath srcIIP)\n \n     String errorPrefix = \"DIR* FSDirectory.swapBlockList: \";\n     String error = \"Swap Block List input \";\n-    final INode srcInode = srcIIP.getLastINode();\n \n-    // Check if INode is null.\n-    if (srcInode == null) {\n-      error  += srcIIP.getPath() + \" is not found.\";\n-      NameNode.stateChangeLog.warn(errorPrefix + error);\n-      throw new FileNotFoundException(error);\n-    }\n+    FSDirectory.resolveLastINode(srcIIP);\n \n+    final INode srcInode = srcIIP.getLastINode();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5a1e25dcbbc5f45d90d24a53b5daa6c6cdc028de"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzY4NTIyMA==", "bodyText": "Thanks, good catch. Missed it.", "url": "https://github.com/apache/hadoop/pull/1819#discussion_r373685220", "createdAt": "2020-01-31T21:06:12Z", "author": {"login": "avijayanhwx"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SwapBlockListOp.java", "diffHunk": "@@ -130,15 +129,10 @@ private static void validateInode(INodesInPath srcIIP)\n \n     String errorPrefix = \"DIR* FSDirectory.swapBlockList: \";\n     String error = \"Swap Block List input \";\n-    final INode srcInode = srcIIP.getLastINode();\n \n-    // Check if INode is null.\n-    if (srcInode == null) {\n-      error  += srcIIP.getPath() + \" is not found.\";\n-      NameNode.stateChangeLog.warn(errorPrefix + error);\n-      throw new FileNotFoundException(error);\n-    }\n+    FSDirectory.resolveLastINode(srcIIP);\n \n+    final INode srcInode = srcIIP.getLastINode();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzI5NjQyNw=="}, "originalCommit": {"oid": "5a1e25dcbbc5f45d90d24a53b5daa6c6cdc028de"}, "originalPosition": 42}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3690, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}