{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTMyNDcxOTUy", "number": 2520, "title": "HADOOP-17290. ABFS: Add Identifiers to Client Request Header", "bodyText": "Adding a set of identifiers to the X_MS_CLIENT_REQUEST_ID header to help correlate requests. This header contains IDs concatenated into a string, and appears in the storage diagnostic logs.\nThe clientRequestId guid used to identify requests is uniquely generated for each HTTP request. The additional identifiers will help group and/or filter requests for analysis. Of these, the clientCorrelationId is a unique identifier that can be provided by the user. The rest are generated by the driver.\nTwo configs introduced:\n\n\nfs.azure.client.correlation.id - takes String for clientCorrelationId (alphanumeric characters/hyphens, max length = 72)\n\n\nfs.azure.tracingcontext.format - option for selecting format of IDs to be included in header:\n\nSINGLE_ID_FORMAT (existing)    [b90b5a8e-220a-4aff-b43b-ed4724e62d70]\nALL_ID_FORMAT (new default) [corr-id:b90b5a8e-220a-4aff-b43b-ed4724e62d70:a9c605c9-f7f3-493a-8542-ebe843f4529d:1c93675d-9851-4f5e-94cd-aafffc912948::CR:0]\nTWO_ID_FORMAT  [corr-id:b90b5a8e-220a-4aff-b43b-ed4724e62d70]\n\n\n\nTests to check format of header and validate the identifiers have been added. The tests also ensure that an invalid config input does not result in request failure.", "createdAt": "2020-12-04T11:26:11Z", "url": "https://github.com/apache/hadoop/pull/2520", "merged": true, "mergeCommit": {"oid": "35570e414a6ab9130da8cac60e563168ee0d4793"}, "closed": true, "closedAt": "2021-07-02T13:43:20Z", "author": {"login": "sumangala-patki"}, "timelineItems": {"totalCount": 99, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdNt7ntAH2gAyNTMyNDcxOTUyOmNlMDViZDA2MjdlMTMyMDk0NTA2NzIyNzg4NGZkYzIyYTI2YTk3MWM=", "endCursor": "Y3Vyc29yOnYyOpPPAAABemcalCgH2gAyNTMyNDcxOTUyOjBkYzgzZjQzNjdjNmQ2NzRhN2Q4ZjBiZmJhMDQ5OGMyNmM0NTUxZTE=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "ce05bd0627e1320945067227884fdc22a26a971c", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/ce05bd0627e1320945067227884fdc22a26a971c", "committedDate": "2020-09-29T20:09:06Z", "message": "correlation-id : req-id : retry-count"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "da6c02538686015881cbc95f817472d0a01cefc5", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/da6c02538686015881cbc95f817472d0a01cefc5", "committedDate": "2020-10-06T08:43:06Z", "message": "adding IDs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e21f7c6b1604b515c1359532294b5f49552f36fc", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/e21f7c6b1604b515c1359532294b5f49552f36fc", "committedDate": "2020-10-07T06:57:53Z", "message": "add op id"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "67c53a3af0827014e4fffc7ae4ffe224edc2b2dd", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/67c53a3af0827014e4fffc7ae4ffe224edc2b2dd", "committedDate": "2020-10-07T11:01:35Z", "message": "undo formatting"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0e57f19f90c1b3325a22b8d67ab429ea4ff2f15a", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/0e57f19f90c1b3325a22b8d67ab429ea4ff2f15a", "committedDate": "2020-10-07T22:50:04Z", "message": "to pc"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "20c916d962fb10e9c9209c332a2aa00818c3f1aa", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/20c916d962fb10e9c9209c332a2aa00818c3f1aa", "committedDate": "2020-10-08T02:15:33Z", "message": "tc -> ops"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5e97c55b77905e441037b0af76db85c69da5f810", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/5e97c55b77905e441037b0af76db85c69da5f810", "committedDate": "2020-10-08T10:30:11Z", "message": "other IDs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2df54bfb9008324ebee052c052b1c85f88f37417", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/2df54bfb9008324ebee052c052b1c85f88f37417", "committedDate": "2020-10-08T16:05:41Z", "message": "debug"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d3ba55099abebafe7b6624f8aff17710c0b54fe9", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/d3ba55099abebafe7b6624f8aff17710c0b54fe9", "committedDate": "2020-10-09T03:18:48Z", "message": "debug"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "70e7d0f38fc26e7edd05b2e744390d03ae0ed387", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/70e7d0f38fc26e7edd05b2e744390d03ae0ed387", "committedDate": "2020-10-09T05:31:46Z", "message": "builds"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3cb2ccbacf66a9743019161bc7ead1236d42d0b8", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/3cb2ccbacf66a9743019161bc7ead1236d42d0b8", "committedDate": "2020-10-12T04:05:10Z", "message": "primary req id"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c3ddf824e24998ef0a4d7efd15d032a1bc98ba74", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/c3ddf824e24998ef0a4d7efd15d032a1bc98ba74", "committedDate": "2020-10-12T15:23:11Z", "message": "readahead"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2e882d4cfaa531ebfc1922223d60cbde315dfcd9", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/2e882d4cfaa531ebfc1922223d60cbde315dfcd9", "committedDate": "2020-10-13T12:24:14Z", "message": "dependent & client req id (readaheads)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "eeae13ee9a93dab4ea6713333842b543065529b3", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/eeae13ee9a93dab4ea6713333842b543065529b3", "committedDate": "2020-10-13T13:13:43Z", "message": "liststatus ok"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d4343fae35638e25952a3ce1cc8b472fb726a83b", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/d4343fae35638e25952a3ce1cc8b472fb726a83b", "committedDate": "2020-10-13T13:54:26Z", "message": "create overwrite case ok"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0ef9a27aa46e442f52bd57e4a336c94b5ea2e54d", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/0ef9a27aa46e442f52bd57e4a336c94b5ea2e54d", "committedDate": "2020-10-15T03:09:17Z", "message": "fixed some errors"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "107df060d5bca839cb0e266d6631d862e2fcfe0a", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/107df060d5bca839cb0e266d6631d862e2fcfe0a", "committedDate": "2020-10-15T19:14:22Z", "message": "tc changes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "166f99cd6239cab03076e6dd02192eebd3f79165", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/166f99cd6239cab03076e6dd02192eebd3f79165", "committedDate": "2020-10-19T01:44:34Z", "message": "fix errors"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d762d864206f3b88af01d1558b4101ce8746dedd", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/d762d864206f3b88af01d1558b4101ce8746dedd", "committedDate": "2020-10-20T10:03:27Z", "message": "test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d905c93c79a3db9d55505bbd022e9d8ae62f8050", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/d905c93c79a3db9d55505bbd022e9d8ae62f8050", "committedDate": "2020-10-20T13:47:23Z", "message": "1 test draft"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "56acf8059ff53d9a833df3e45951d2d6e64182db", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/56acf8059ff53d9a833df3e45951d2d6e64182db", "committedDate": "2020-10-23T05:47:35Z", "message": "test IDs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0a96fbe57d115c80277de7c11510bcbad9de3d55", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/0a96fbe57d115c80277de7c11510bcbad9de3d55", "committedDate": "2020-10-23T07:02:41Z", "message": "clear()"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ba8d98837dc0c5d291a9d1bcde096c09327a024d", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/ba8d98837dc0c5d291a9d1bcde096c09327a024d", "committedDate": "2020-10-27T17:52:22Z", "message": "minor edits"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6be948e986a26c754ba9306a5f6a740e42f25bdd", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/6be948e986a26c754ba9306a5f6a740e42f25bdd", "committedDate": "2020-10-27T18:27:35Z", "message": "minor edits"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5af42f61947953fd879f6a3db96206be88cf9db1", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/5af42f61947953fd879f6a3db96206be88cf9db1", "committedDate": "2020-10-27T18:31:32Z", "message": "minor edits"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "077b5bd26503e998071c3b4289d6ddbeab79c63c", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/077b5bd26503e998071c3b4289d6ddbeab79c63c", "committedDate": "2020-10-27T18:40:31Z", "message": "minor edits"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ee07bae1eb71781b11ae52aed473fa22f6095038", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/ee07bae1eb71781b11ae52aed473fa22f6095038", "committedDate": "2020-10-27T19:12:35Z", "message": "minor edits/whitespc"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9457a04dbcac6da42253c4f9bdc0c98758e608a2", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/9457a04dbcac6da42253c4f9bdc0c98758e608a2", "committedDate": "2020-11-02T10:48:06Z", "message": "merge conflict"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "16811384244eeb61895c7d04037000b646e5d717", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/16811384244eeb61895c7d04037000b646e5d717", "committedDate": "2020-11-03T10:32:00Z", "message": "pr changes + dummyTC"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a4755553881a1c8d39bdff737bf4a3d273238328", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/a4755553881a1c8d39bdff737bf4a3d273238328", "committedDate": "2020-11-03T14:06:56Z", "message": "test ns() + remove extra changes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "644293231d6eb34245742c1fde75fc022d7dae66", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/644293231d6eb34245742c1fde75fc022d7dae66", "committedDate": "2020-11-03T14:25:18Z", "message": "revert httpop formatting"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ec92f2576698dcb702898f1af61cfb71fb9d54b0", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/ec92f2576698dcb702898f1af61cfb71fb9d54b0", "committedDate": "2020-11-03T14:32:59Z", "message": "revert httpop formatting"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fecc00a2962009f17e4eca57da4b448f25136b4a", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/fecc00a2962009f17e4eca57da4b448f25136b4a", "committedDate": "2020-11-03T15:08:44Z", "message": "move tc init near usage"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1cead5325d825d9076063fef209b07e2f2ae9ca3", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/1cead5325d825d9076063fef209b07e2f2ae9ca3", "committedDate": "2020-11-04T05:52:05Z", "message": "minor change"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5dbc7830221092d91562723320e271901ba4eb82", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/5dbc7830221092d91562723320e271901ba4eb82", "committedDate": "2020-11-05T02:40:38Z", "message": "pr changes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6409e3fde14ea4167d6d6eeacaba2430d9ce15f1", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/6409e3fde14ea4167d6d6eeacaba2430d9ce15f1", "committedDate": "2020-11-05T06:23:28Z", "message": "enum, opnames"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d304123654e1cbe18c6617b317727c7e76a900a5", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/d304123654e1cbe18c6617b317727c7e76a900a5", "committedDate": "2020-11-08T08:46:42Z", "message": "enum"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1e4f46db77273da9ec37770aa2923eb4c8350df2", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/1e4f46db77273da9ec37770aa2923eb4c8350df2", "committedDate": "2020-11-12T04:57:37Z", "message": "format changes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "15a22cd4a3f9e57ec35b31d5b786ee19a1213ce8", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/15a22cd4a3f9e57ec35b31d5b786ee19a1213ce8", "committedDate": "2020-11-12T07:35:27Z", "message": "test code (#3)\n\n* adding callback structure\r\n\r\n* testListPath correlation header\r\n\r\n* validate IDs; readahead/streamid\r\n\r\n* add common tests + other changes\r\n\r\n* remove stream/extra stuff\r\n\r\n* handle parallel requests\r\n\r\n* clear\r\n\r\n* testTC, retryNum\r\n\r\n* rebase on HADOOP-17290"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e44c64c720485a058d8acd60844848980396ac97", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/e44c64c720485a058d8acd60844848980396ac97", "committedDate": "2020-11-12T12:15:40Z", "message": "other tests (main)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "725c98415030f856ce333b605cbf3de82bb2b028", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/725c98415030f856ce333b605cbf3de82bb2b028", "committedDate": "2020-11-16T05:02:22Z", "message": "all tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9b4f55809d54a4f7348859cede4c44a68ccd16ec", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/9b4f55809d54a4f7348859cede4c44a68ccd16ec", "committedDate": "2020-11-17T07:50:35Z", "message": "test tc for appendblob=true"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "858695bd6dba12363a7a57e4fe406697f3ce6f8e", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/858695bd6dba12363a7a57e4fe406697f3ce6f8e", "committedDate": "2020-11-23T05:39:08Z", "message": "clean up code"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a9a4f425c7b3f97f0ad44426374e3dcfaa34b019", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/a9a4f425c7b3f97f0ad44426374e3dcfaa34b019", "committedDate": "2020-11-23T09:19:05Z", "message": "simplify preq test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e18bab62749da71b56ecb6a4a3ec86983cdc7139", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/e18bab62749da71b56ecb6a4a3ec86983cdc7139", "committedDate": "2020-11-25T05:45:26Z", "message": "fix matchers error"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1553cf653a5c503aea310e9980c52925be0c05fa", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/1553cf653a5c503aea310e9980c52925be0c05fa", "committedDate": "2020-11-25T06:01:00Z", "message": "merge conflicts"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "22c3a8411e623bac3e2909620a13e666184e85cf", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/22c3a8411e623bac3e2909620a13e666184e85cf", "committedDate": "2020-11-25T11:48:45Z", "message": "pr revw changes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3a3a40eab3ff65ef6748c973f0df37e55d2af0ca", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/3a3a40eab3ff65ef6748c973f0df37e55d2af0ca", "committedDate": "2020-11-26T02:49:03Z", "message": "fix some test failures"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "472d0903972128c0634702712ab4d7fb84442b3d", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/472d0903972128c0634702712ab4d7fb84442b3d", "committedDate": "2020-11-27T04:06:06Z", "message": "code cleanup"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "21e2a8682ca9c9684f68e4c06185f16c991f85d7", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/21e2a8682ca9c9684f68e4c06185f16c991f85d7", "committedDate": "2020-11-27T06:40:32Z", "message": "fix sastoken matcher"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8eaad735b62f9a5ae18c6f726f3e58b0c1bce9c3", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/8eaad735b62f9a5ae18c6f726f3e58b0c1bce9c3", "committedDate": "2020-11-30T09:07:17Z", "message": "access test, formatting"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "db8d89586959545ff69e1c9ab95260759c8bf1a4", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/db8d89586959545ff69e1c9ab95260759c8bf1a4", "committedDate": "2020-11-30T10:41:03Z", "message": "format PR diff"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0521969b26ead9397ff6856bba4b7f1feaca5394", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/0521969b26ead9397ff6856bba4b7f1feaca5394", "committedDate": "2020-11-30T12:20:53Z", "message": "more formatting"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "907fc1b987d2006ebde2783eca73b0b0391526dc", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/907fc1b987d2006ebde2783eca73b0b0391526dc", "committedDate": "2020-12-03T06:35:52Z", "message": "merge conflict"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f3f91f4390a365f85ed64b34cd667163fa583c1b", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/f3f91f4390a365f85ed64b34cd667163fa583c1b", "committedDate": "2020-12-03T06:36:37Z", "message": "merge conflict"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "18ea7f0445555dfbf43aa175a0aa642521ff39f9", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/18ea7f0445555dfbf43aa175a0aa642521ff39f9", "committedDate": "2020-12-03T07:44:19Z", "message": "pr changes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "af74d8e464bfd736614dabbdd54cd37113d40126", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/af74d8e464bfd736614dabbdd54cd37113d40126", "committedDate": "2020-12-03T09:28:17Z", "message": "stream id test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3a4eb41661d9ffc9819a5de66de59ca56c0658d7", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/3a4eb41661d9ffc9819a5de66de59ca56c0658d7", "committedDate": "2020-12-04T08:50:24Z", "message": "documentation md"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dee9ec6970a8898ba9a399e5f5b52ac6140ee250", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/dee9ec6970a8898ba9a399e5f5b52ac6140ee250", "committedDate": "2020-12-04T10:26:30Z", "message": "Merge branch 'trunk' into HADOOP-17290"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f90bbb399437d461a9bc4ee614c27a3d03bae227", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/f90bbb399437d461a9bc4ee614c27a3d03bae227", "committedDate": "2020-12-05T06:57:23Z", "message": "fix yetus bugs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a5b64ed6af46fca88dbe7eca9c60b318cf8728ba", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/a5b64ed6af46fca88dbe7eca9c60b318cf8728ba", "committedDate": "2020-12-07T10:11:36Z", "message": "fix randomread getTC failure"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUyMTQ5ODQ5", "url": "https://github.com/apache/hadoop/pull/2520#pullrequestreview-552149849", "createdAt": "2020-12-15T06:51:37Z", "commit": {"oid": "a5b64ed6af46fca88dbe7eca9c60b318cf8728ba"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwNjo1MTozN1rOIF7jAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwNjo1MTozN1rOIF7jAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzA4OTQwOQ==", "bodyText": "This will only be used by test code ? If so, lower accessibility to private and set VisibleForTesting.", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r543089409", "createdAt": "2020-12-15T06:51:37Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java", "diffHunk": "@@ -1202,6 +1287,10 @@ public String getCanonicalServiceName() {\n     return this.statistics;\n   }\n \n+  public void setListenerOperation(String operation) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a5b64ed6af46fca88dbe7eca9c60b318cf8728ba"}, "originalPosition": 433}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUyMTU5NTQz", "url": "https://github.com/apache/hadoop/pull/2520#pullrequestreview-552159543", "createdAt": "2020-12-15T07:11:51Z", "commit": {"oid": "a5b64ed6af46fca88dbe7eca9c60b318cf8728ba"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwNzoxMTo1MlrOIF8ICg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwNzoxMTo1MlrOIF8ICg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzA5ODg5MA==", "bodyText": "Same as before. Methods used by test need to be private and annotated with VisibleForTesting", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r543098890", "createdAt": "2020-12-15T07:11:52Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -107,6 +122,15 @@ public String getPath() {\n     return path;\n   }\n \n+  private String getInputStreamID() {\n+    return StringUtils.right(UUID.randomUUID().toString(), STREAM_ID_LEN);\n+  }\n+\n+  public void registerListener(Listener listener1) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a5b64ed6af46fca88dbe7eca9c60b318cf8728ba"}, "originalPosition": 80}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUyMTYwMzk0", "url": "https://github.com/apache/hadoop/pull/2520#pullrequestreview-552160394", "createdAt": "2020-12-15T07:13:32Z", "commit": {"oid": "a5b64ed6af46fca88dbe7eca9c60b318cf8728ba"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6a5b51289e90fb5d4f590dc7c1812a22a28a7c24", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/6a5b51289e90fb5d4f590dc7c1812a22a28a7c24", "committedDate": "2020-12-24T04:50:05Z", "message": "Merge branch 'trunk' into HADOOP-17290"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d2bf54c1423aea99a21fa8194debea985f6ced01", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/d2bf54c1423aea99a21fa8194debea985f6ced01", "committedDate": "2020-12-24T04:59:00Z", "message": "addressing pr comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "84c620ccf03c0fc37316ac07bc44e60857b1c4d8", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/84c620ccf03c0fc37316ac07bc44e60857b1c4d8", "committedDate": "2021-02-23T07:09:50Z", "message": "merge conflicts"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "98e0fb0b0324cd7240cd17e956cdf1919495b8c2", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/98e0fb0b0324cd7240cd17e956cdf1919495b8c2", "committedDate": "2021-02-23T07:17:35Z", "message": "new file conflicts"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9f37b0f2a4cd7429e6ec55d6fac8d7a85e7b0be1", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/9f37b0f2a4cd7429e6ec55d6fac8d7a85e7b0be1", "committedDate": "2021-03-02T09:29:59Z", "message": "Merge branch 'trunk' into HADOOP-17290"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c8c1a04eff0f9bb1448b5f142b8b58d7fbd3472e", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/c8c1a04eff0f9bb1448b5f142b8b58d7fbd3472e", "committedDate": "2021-03-03T04:41:03Z", "message": "Merge branch 'trunk' into HADOOP-17290"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ec869fe0358b07d4548d4e6318c9d0f7d7f1cbd0", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/ec869fe0358b07d4548d4e6318c9d0f7d7f1cbd0", "committedDate": "2021-03-03T05:46:03Z", "message": "imports"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fe3f041b1b39725669fb3301745d3b9cbd1a33aa", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/fe3f041b1b39725669fb3301745d3b9cbd1a33aa", "committedDate": "2021-03-09T06:14:28Z", "message": "Merge branch 'trunk' into HADOOP-17290"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "22e9a40330fae359dc839b8062efe8f0e841a548", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/22e9a40330fae359dc839b8062efe8f0e841a548", "committedDate": "2021-03-10T10:24:01Z", "message": "import"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "29f59f84ee6db71ed342a800282c53035c38b584", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/29f59f84ee6db71ed342a800282c53035c38b584", "committedDate": "2021-03-10T10:25:01Z", "message": "Merge branch 'trunk' into HADOOP-17290"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a42e5b19a39e6858c790ced0a39edf43a2a262da", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/a42e5b19a39e6858c790ced0a39edf43a2a262da", "committedDate": "2021-03-10T10:57:40Z", "message": "use write code for out"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "51e1a8ff7bda74d05ba2ccd8e6c497acbfac4ad8", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/51e1a8ff7bda74d05ba2ccd8e6c497acbfac4ad8", "committedDate": "2021-03-11T04:34:25Z", "message": "handle invocation ex + write tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4ed465f8ce965343be63194a5034070ef932dfd2", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/4ed465f8ce965343be63194a5034070ef932dfd2", "committedDate": "2021-03-11T04:45:47Z", "message": "checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0e9770052dd12cd551c581e1b382b28ec1712147", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/0e9770052dd12cd551c581e1b382b28ec1712147", "committedDate": "2021-03-11T06:33:05Z", "message": "minor chkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1e529f4d5b3d70eae74fbeceb7365925f9c17b68", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/1e529f4d5b3d70eae74fbeceb7365925f9c17b68", "committedDate": "2021-03-15T06:27:54Z", "message": "Merge branch 'trunk' into HADOOP-17290"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1b8f58954d3a1d6c9e89e3d86a0f2eb148a09b9a", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/1b8f58954d3a1d6c9e89e3d86a0f2eb148a09b9a", "committedDate": "2021-04-11T04:21:35Z", "message": "fix merge conflict"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "302fc06772362b0d108f3aab42cc7b353d7a7f58", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/302fc06772362b0d108f3aab42cc7b353d7a7f58", "committedDate": "2021-04-15T12:11:31Z", "message": "Merge branch 'trunk' into HADOOP-17290"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3447977af7444bdd033ba091124bc70b130954e3", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/3447977af7444bdd033ba091124bc70b130954e3", "committedDate": "2021-04-15T18:06:00Z", "message": "lease rm acquire op"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "979f2b5dd6d23545b04ea00e62b6eaf8f0b93e4b", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/979f2b5dd6d23545b04ea00e62b6eaf8f0b93e4b", "committedDate": "2021-04-26T06:18:01Z", "message": "Merge branch 'trunk' into HADOOP-17290"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c9217d9492486f401cbcb9320e8d473f98595c1b", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/c9217d9492486f401cbcb9320e8d473f98595c1b", "committedDate": "2021-04-26T07:01:52Z", "message": "part of merge fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fd631219582b5bc6db47b69a4aa496682ac335a4", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/fd631219582b5bc6db47b69a4aa496682ac335a4", "committedDate": "2021-04-26T10:05:03Z", "message": "add active lease fn tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "78a15dc3d6d91a1adc11f6fd46c5aecc839b4105", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/78a15dc3d6d91a1adc11f6fd46c5aecc839b4105", "committedDate": "2021-04-26T11:29:10Z", "message": "Merge branch 'trunk' into HADOOP-17290"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d52ee795da47e98178b2498357fb9f55a4e78ef1", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/d52ee795da47e98178b2498357fb9f55a4e78ef1", "committedDate": "2021-04-26T11:32:07Z", "message": "javadoc"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6966924728e4234af175f839b27535166ab1b6d0", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/6966924728e4234af175f839b27535166ab1b6d0", "committedDate": "2021-05-30T09:06:51Z", "message": "Merge branch 'trunk' into HADOOP-17290"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d15d2cf2c07691ebbdbd4450ebe65b2e146fb8ef", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/d15d2cf2c07691ebbdbd4450ebe65b2e146fb8ef", "committedDate": "2021-05-30T09:36:46Z", "message": "merge"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d0b33c151b9331b9f542aa2a94d48b1a25e14d8c", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/d0b33c151b9331b9f542aa2a94d48b1a25e14d8c", "committedDate": "2021-05-30T09:40:12Z", "message": "typo"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njc2ODQ3NDQ4", "url": "https://github.com/apache/hadoop/pull/2520#pullrequestreview-676847448", "createdAt": "2021-06-06T09:51:00Z", "commit": {"oid": "d0b33c151b9331b9f542aa2a94d48b1a25e14d8c"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNi0wNlQwOTo1MTowMFrOJoLMsg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNi0xNFQwOToyNjozNFrOJso5DQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY0NjEwNjI5MA==", "bodyText": "Can you give me the example of possible value ?\nWhat if in same jvm two FS objects are there ?", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r646106290", "createdAt": "2021-06-06T09:51:00Z", "author": {"login": "surendralilhore"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java", "diffHunk": "@@ -109,6 +109,12 @@\n    *  Default value of this config is true. **/\n   public static final String FS_AZURE_DISABLE_OUTPUTSTREAM_FLUSH = \"fs.azure.disable.outputstream.flush\";\n   public static final String FS_AZURE_USER_AGENT_PREFIX_KEY = \"fs.azure.user.agent.prefix\";\n+  /**\n+   * The client correlation ID provided over config that will be added to\n+   * x-ms-client-request-Id header. Defaults to empty string if the length and\n+   * character constraints are not satisfied. **/\n+  public static final String FS_AZURE_CLIENT_CORRELATIONID = \"fs.azure.client.correlationid\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d0b33c151b9331b9f542aa2a94d48b1a25e14d8c"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1MDc4NzA4NQ==", "bodyText": "Why this applicable for only stream id, not for filesystem id ? ?", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r650787085", "createdAt": "2021-06-14T09:26:34Z", "author": {"login": "surendralilhore"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -142,6 +156,10 @@ public String getPath() {\n     return path;\n   }\n \n+  private String getInputStreamID() {\n+    return StringUtils.right(UUID.randomUUID().toString(), STREAM_ID_LEN);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d0b33c151b9331b9f542aa2a94d48b1a25e14d8c"}, "originalPosition": 84}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6250d04f25d1efed187a0835e70f53415f0e1378", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/6250d04f25d1efed187a0835e70f53415f0e1378", "committedDate": "2021-06-23T09:14:41Z", "message": "fix merge conflict"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjkwODg5ODc0", "url": "https://github.com/apache/hadoop/pull/2520#pullrequestreview-690889874", "createdAt": "2021-06-23T16:25:17Z", "commit": {"oid": "6250d04f25d1efed187a0835e70f53415f0e1378"}, "state": "COMMENTED", "comments": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNi0yM1QxNjoyNToxOFrOJy0pNQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNi0yNFQwNTozODo0M1rOJzLKhg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzI3MTA5Mw==", "bodyText": "clientCorrelationId ?   To be similar as 'userAgentId' etc?  And the getter also", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657271093", "createdAt": "2021-06-23T16:25:18Z", "author": {"login": "anoopsjohn"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java", "diffHunk": "@@ -264,6 +266,10 @@\n       DefaultValue = DEFAULT_VALUE_UNKNOWN)\n   private String clusterType;\n \n+  @StringConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_CLIENT_CORRELATIONID,\n+          DefaultValue = EMPTY_STRING)\n+  private String clientCorrelationID;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6250d04f25d1efed187a0835e70f53415f0e1378"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzI3Njk1OQ==", "bodyText": "All places ID to Id?", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657276959", "createdAt": "2021-06-23T16:32:39Z", "author": {"login": "anoopsjohn"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java", "diffHunk": "@@ -111,10 +116,14 @@\n   private Path workingDir;\n   private AzureBlobFileSystemStore abfsStore;\n   private boolean isClosed;\n+  private final String fileSystemID = UUID.randomUUID().toString();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6250d04f25d1efed187a0835e70f53415f0e1378"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzMzOTgzNA==", "bodyText": "Here is a call to getFileStatus(Path) but as part of LISTSTATUS op.  So we should the context created above , during getFileStatus also right?", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657339834", "createdAt": "2021-06-23T18:00:09Z", "author": {"login": "anoopsjohn"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java", "diffHunk": "@@ -1049,8 +1130,11 @@ public boolean exists(Path f) throws IOException {\n       throws IOException {\n     LOG.debug(\"AzureBlobFileSystem.listStatusIterator path : {}\", path);\n     if (abfsStore.getAbfsConfiguration().enableAbfsListIterator()) {\n+      TracingContext tracingContext = new TracingContext(clientCorrelationID,\n+          fileSystemID, HdfsOperationConstants.LISTSTATUS, true,\n+          tracingContextFormat, listener);\n       AbfsListStatusRemoteIterator abfsLsItr =\n-          new AbfsListStatusRemoteIterator(getFileStatus(path), abfsStore);\n+          new AbfsListStatusRemoteIterator(getFileStatus(path), abfsStore, tracingContext);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6250d04f25d1efed187a0835e70f53415f0e1378"}, "originalPosition": 419}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM0MTU3OA==", "bodyText": "Even this call to FileSystem#listStatusIterator() will create ADL gen2  calls right?  So should there be way for having a single context(above created) to be used for call from there too?", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657341578", "createdAt": "2021-06-23T18:02:55Z", "author": {"login": "anoopsjohn"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java", "diffHunk": "@@ -1049,8 +1130,11 @@ public boolean exists(Path f) throws IOException {\n       throws IOException {\n     LOG.debug(\"AzureBlobFileSystem.listStatusIterator path : {}\", path);\n     if (abfsStore.getAbfsConfiguration().enableAbfsListIterator()) {\n+      TracingContext tracingContext = new TracingContext(clientCorrelationID,\n+          fileSystemID, HdfsOperationConstants.LISTSTATUS, true,\n+          tracingContextFormat, listener);\n       AbfsListStatusRemoteIterator abfsLsItr =\n-          new AbfsListStatusRemoteIterator(getFileStatus(path), abfsStore);\n+          new AbfsListStatusRemoteIterator(getFileStatus(path), abfsStore, tracingContext);\n       return RemoteIterators.typeCastingRemoteIterator(abfsLsItr);\n     } else {\n       return super.listStatusIterator(path);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6250d04f25d1efed187a0835e70f53415f0e1378"}, "originalPosition": 422}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM0NTAyNw==", "bodyText": "Within tryGetFileStatus() there is call to getFileStatus.  We should be using this context created here.\ntryGetFileStatus() been called by createNonRecursive API also.\nHave to handle these.", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657345027", "createdAt": "2021-06-23T18:07:08Z", "author": {"login": "anoopsjohn"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java", "diffHunk": "@@ -335,7 +361,10 @@ public boolean rename(final Path src, final Path dst) throws IOException {\n     }\n \n     // Non-HNS account need to check dst status on driver side.\n-    if (!abfsStore.getIsNamespaceEnabled() && dstFileStatus == null) {\n+    TracingContext tracingContext = new TracingContext(clientCorrelationID,\n+        fileSystemID, HdfsOperationConstants.RENAME, true, tracingContextFormat,\n+        listener);\n+    if (!abfsStore.getIsNamespaceEnabled(tracingContext) && dstFileStatus == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6250d04f25d1efed187a0835e70f53415f0e1378"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM0NjMzMg==", "bodyText": "GET_FILESTATUS op?", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657346332", "createdAt": "2021-06-23T18:08:22Z", "author": {"login": "anoopsjohn"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java", "diffHunk": "@@ -1071,7 +1155,10 @@ private boolean fileSystemExists() throws IOException {\n     LOG.debug(\n             \"AzureBlobFileSystem.fileSystemExists uri: {}\", uri);\n     try {\n-      abfsStore.getFilesystemProperties();\n+      TracingContext tracingContext = new TracingContext(clientCorrelationID,\n+          fileSystemID, HdfsOperationConstants.GET_FILESTATUS,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6250d04f25d1efed187a0835e70f53415f0e1378"}, "originalPosition": 429}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM0NzQ5MA==", "bodyText": "What is this operation been set on Listener?", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657347490", "createdAt": "2021-06-23T18:09:26Z", "author": {"login": "anoopsjohn"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java", "diffHunk": "@@ -1283,6 +1373,11 @@ public String getCanonicalServiceName() {\n     return this.statistics;\n   }\n \n+  @VisibleForTesting\n+  void setListenerOperation(String operation) {\n+    listener.setOperation(operation);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6250d04f25d1efed187a0835e70f53415f0e1378"}, "originalPosition": 453}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM1NTcwMw==", "bodyText": "This is the tracing header format right?  Will that be a better name?", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657355703", "createdAt": "2021-06-23T18:16:59Z", "author": {"login": "anoopsjohn"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java", "diffHunk": "@@ -109,6 +109,12 @@\n    *  Default value of this config is true. **/\n   public static final String FS_AZURE_DISABLE_OUTPUTSTREAM_FLUSH = \"fs.azure.disable.outputstream.flush\";\n   public static final String FS_AZURE_USER_AGENT_PREFIX_KEY = \"fs.azure.user.agent.prefix\";\n+  /**\n+   * The client correlation ID provided over config that will be added to\n+   * x-ms-client-request-Id header. Defaults to empty string if the length and\n+   * character constraints are not satisfied. **/\n+  public static final String FS_AZURE_CLIENT_CORRELATIONID = \"fs.azure.client.correlationid\";\n+  public static final String FS_AZURE_TRACINGCONTEXT_FORMAT = \"fs.azure.tracingcontext.format\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6250d04f25d1efed187a0835e70f53415f0e1378"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM1OTc4NA==", "bodyText": "Can be Enum?", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657359784", "createdAt": "2021-06-23T18:21:51Z", "author": {"login": "anoopsjohn"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/HdfsOperationConstants.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.constants;\n+\n+public final class HdfsOperationConstants {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6250d04f25d1efed187a0835e70f53415f0e1378"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM2NDE1MA==", "bodyText": "Only used for tests?  Should be returning List only?", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657364150", "createdAt": "2021-06-23T18:28:28Z", "author": {"login": "anoopsjohn"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsHttpOperation.java", "diffHunk": "@@ -166,6 +166,11 @@ public String getResponseHeader(String httpHeader) {\n     return connection.getHeaderField(httpHeader);\n   }\n \n+  @VisibleForTesting\n+  public String getRequestHeader(String httpHeader) {\n+    return connection.getRequestProperties().get(httpHeader).toString();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6250d04f25d1efed187a0835e70f53415f0e1378"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM2ODM3OA==", "bodyText": "Can avoid this way of explicit call which for sure should get executed before setting of this header.\nCan we have a better method name than toString() for generation of required header value?  This has to consider the format and generate.", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657368378", "createdAt": "2021-06-23T18:34:45Z", "author": {"login": "anoopsjohn"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java", "diffHunk": "@@ -221,17 +226,27 @@ private void completeExecute() throws AzureBlobFileSystemException {\n     LOG.trace(\"{} REST operation complete\", operationType);\n   }\n \n+  private void updateClientRequestHeader(AbfsHttpOperation httpOperation,\n+      TracingContext tracingContext) {\n+    tracingContext.generateClientRequestID();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6250d04f25d1efed187a0835e70f53415f0e1378"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM3NTUyMw==", "bodyText": "Why passing 'tracingContext' when its set as instance member?", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657375523", "createdAt": "2021-06-23T18:45:50Z", "author": {"login": "anoopsjohn"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -451,15 +472,15 @@ private int readInternal(final long position, final byte[] b, final int offset,\n       }\n \n       // got nothing from read-ahead, do our own read now\n-      receivedBytes = readRemote(position, b, offset, length);\n+      receivedBytes = readRemote(position, b, offset, length, new TracingContext(tracingContext));\n       return receivedBytes;\n     } else {\n       LOG.debug(\"read ahead disabled, reading remote\");\n-      return readRemote(position, b, offset, length);\n+      return readRemote(position, b, offset, length, new TracingContext(tracingContext));\n     }\n   }\n \n-  int readRemote(long position, byte[] b, int offset, int length) throws IOException {\n+  int readRemote(long position, byte[] b, int offset, int length, TracingContext tracingContext) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6250d04f25d1efed187a0835e70f53415f0e1378"}, "originalPosition": 129}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzY0MDA3MA==", "bodyText": "Can we avoid the hdfs in this ?  Its after all Hadoop common FS API types", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657640070", "createdAt": "2021-06-24T05:38:43Z", "author": {"login": "anoopsjohn"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/HdfsOperationConstants.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.constants;\n+\n+public final class HdfsOperationConstants {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM1OTc4NA=="}, "originalCommit": {"oid": "6250d04f25d1efed187a0835e70f53415f0e1378"}, "originalPosition": 21}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1567e3e82240a242a6f11970d6d313db575a3787", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/1567e3e82240a242a6f11970d6d313db575a3787", "committedDate": "2021-06-24T19:01:16Z", "message": "address review comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjkyNjE3ODY4", "url": "https://github.com/apache/hadoop/pull/2520#pullrequestreview-692617868", "createdAt": "2021-06-25T09:07:10Z", "commit": {"oid": "1567e3e82240a242a6f11970d6d313db575a3787"}, "state": "COMMENTED", "comments": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNi0yNVQwOTowNzoxMFrOJ0Glag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNi0yNVQwOTo1NzowOVrOJ0IgiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYxMzYxMA==", "bodyText": "This clone of Context needed here?  What gets changed?", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658613610", "createdAt": "2021-06-25T09:07:10Z", "author": {"login": "anoopsjohn"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsLease.java", "diffHunk": "@@ -114,13 +119,15 @@ public AbfsLease(AbfsClient client, String path, int acquireMaxRetries,\n     LOG.debug(\"Acquired lease {} on {}\", leaseID, path);\n   }\n \n-  private void acquireLease(RetryPolicy retryPolicy, int numRetries, int retryInterval, long delay)\n+  private void acquireLease(RetryPolicy retryPolicy, int numRetries,\n+      int retryInterval, long delay)\n       throws LeaseException {\n     LOG.debug(\"Attempting to acquire lease on {}, retry {}\", path, numRetries);\n     if (future != null && !future.isDone()) {\n       throw new LeaseException(ERR_LEASE_FUTURE_EXISTS);\n     }\n-    future = client.schedule(() -> client.acquireLease(path, INFINITE_LEASE_DURATION),\n+    future = client.schedule(() -> client.acquireLease(path,\n+        INFINITE_LEASE_DURATION, new TracingContext(tracingContext)),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1567e3e82240a242a6f11970d6d313db575a3787"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYxNTUwMQ==", "bodyText": "getOutputStreamId() and getStreamID() -> Both create some confusion.  Normally the Getter just return a already available value.  getStreamID() make sense.\nYou can use createOutputStreamId() instead?", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658615501", "createdAt": "2021-06-25T09:10:08Z", "author": {"login": "anoopsjohn"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -160,6 +170,14 @@ public AbfsOutputStream(\n     if (outputStreamStatistics != null) {\n       this.ioStatistics = outputStreamStatistics.getIOStatistics();\n     }\n+    this.outputStreamId = getOutputStreamId();\n+    this.tracingContext = new TracingContext(tracingContext);\n+    this.tracingContext.setStreamID(outputStreamId);\n+    this.tracingContext.setOperation(FSOperationType.WRITE);\n+  }\n+\n+  private String getOutputStreamId() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1567e3e82240a242a6f11970d6d313db575a3787"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYxNTY5NA==", "bodyText": "The same thing in ABFSInputStream also", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658615694", "createdAt": "2021-06-25T09:10:23Z", "author": {"login": "anoopsjohn"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -160,6 +170,14 @@ public AbfsOutputStream(\n     if (outputStreamStatistics != null) {\n       this.ioStatistics = outputStreamStatistics.getIOStatistics();\n     }\n+    this.outputStreamId = getOutputStreamId();\n+    this.tracingContext = new TracingContext(tracingContext);\n+    this.tracingContext.setStreamID(outputStreamId);\n+    this.tracingContext.setOperation(FSOperationType.WRITE);\n+  }\n+\n+  private String getOutputStreamId() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYxNTUwMQ=="}, "originalCommit": {"oid": "1567e3e82240a242a6f11970d6d313db575a3787"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYxNjMzNw==", "bodyText": "Ok the context been passed here might get changed at least wrt the retryCount.  that is why been cloned here?", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658616337", "createdAt": "2021-06-25T09:11:22Z", "author": {"login": "anoopsjohn"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -385,7 +412,9 @@ private void writeAppendBlobCurrentBufferToService() throws IOException {\n             \"writeCurrentBufferToService\", \"append\")) {\n       AppendRequestParameters reqParams = new AppendRequestParameters(offset, 0,\n           bytesLength, APPEND_MODE, true, leaseId);\n-      AbfsRestOperation op = client.append(path, bytes, reqParams, cachedSasToken.get());\n+      AbfsRestOperation op = client\n+          .append(path, bytes, reqParams, cachedSasToken.get(),\n+              new TracingContext(tracingContext));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1567e3e82240a242a6f11970d6d313db575a3787"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYxNjY5MA==", "bodyText": "Yaaa here.", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658616690", "createdAt": "2021-06-25T09:11:54Z", "author": {"login": "anoopsjohn"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java", "diffHunk": "@@ -202,9 +206,10 @@ private void completeExecute() throws AzureBlobFileSystemException {\n \n     retryCount = 0;\n     LOG.debug(\"First execution of REST operation - {}\", operationType);\n-    while (!executeHttpOperation(retryCount)) {\n+    while (!executeHttpOperation(retryCount, tracingContext)) {\n       try {\n         ++retryCount;\n+        tracingContext.setRetryCount(retryCount);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1567e3e82240a242a6f11970d6d313db575a3787"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYxODIzMg==", "bodyText": "generateClientRequestId() can be done internally within constructHeader()?  Else its upto callee to set it proper before.  Actually the UUID clientReqId generation should be an internal responsibility of this tracingContext right?\nAll other methods are like setter.", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658618232", "createdAt": "2021-06-25T09:14:21Z", "author": {"login": "anoopsjohn"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java", "diffHunk": "@@ -221,17 +226,27 @@ private void completeExecute() throws AzureBlobFileSystemException {\n     LOG.trace(\"{} REST operation complete\", operationType);\n   }\n \n+  private void updateClientRequestHeader(AbfsHttpOperation httpOperation,\n+      TracingContext tracingContext) {\n+    tracingContext.generateClientRequestId();\n+    httpOperation.getConnection()\n+        .setRequestProperty(HttpHeaderConfigurations.X_MS_CLIENT_REQUEST_ID,\n+            tracingContext.constructHeader());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1567e3e82240a242a6f11970d6d313db575a3787"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYxODg0Nw==", "bodyText": "Its actually a fresh AbfsHttpOperation operation and here by we set the header.  Can we call it setXXX  than updateXXX?", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658618847", "createdAt": "2021-06-25T09:15:17Z", "author": {"login": "anoopsjohn"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java", "diffHunk": "@@ -221,17 +226,27 @@ private void completeExecute() throws AzureBlobFileSystemException {\n     LOG.trace(\"{} REST operation complete\", operationType);\n   }\n \n+  private void updateClientRequestHeader(AbfsHttpOperation httpOperation,\n+      TracingContext tracingContext) {\n+    tracingContext.generateClientRequestId();\n+    httpOperation.getConnection()\n+        .setRequestProperty(HttpHeaderConfigurations.X_MS_CLIENT_REQUEST_ID,\n+            tracingContext.constructHeader());\n+  }\n+\n   /**\n    * Executes a single HTTP operation to complete the REST operation.  If it\n    * fails, there may be a retry.  The retryCount is incremented with each\n    * attempt.\n    */\n-  private boolean executeHttpOperation(final int retryCount) throws AzureBlobFileSystemException {\n+  private boolean executeHttpOperation(final int retryCount,\n+    TracingContext tracingContext) throws AzureBlobFileSystemException {\n     AbfsHttpOperation httpOperation = null;\n     try {\n       // initialize the HTTP request and open the connection\n       httpOperation = new AbfsHttpOperation(url, method, requestHeaders);\n       incrementCounter(AbfsStatistic.CONNECTIONS_MADE, 1);\n+      updateClientRequestHeader(httpOperation, tracingContext);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1567e3e82240a242a6f11970d6d313db575a3787"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYyMDA0MQ==", "bodyText": "This is for testability only?", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658620041", "createdAt": "2021-06-25T09:17:10Z", "author": {"login": "anoopsjohn"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/Listener.java", "diffHunk": "@@ -0,0 +1,33 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+import org.apache.hadoop.fs.azurebfs.constants.FSOperationType;\n+\n+/**\n+ * Interface for testing identifiers tracked via TracingContext\n+ * Implemented in TracingHeaderValidator\n+ */\n+\n+public interface Listener {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1567e3e82240a242a6f11970d6d313db575a3787"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYzMDYwMg==", "bodyText": "primaryRequestID will be used in case of Read ahead?  Worth adding come comments about its use here", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658630602", "createdAt": "2021-06-25T09:34:03Z", "author": {"login": "anoopsjohn"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java", "diffHunk": "@@ -0,0 +1,170 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+import java.util.UUID;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hadoop.fs.azurebfs.constants.FSOperationType;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsClient;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.EMPTY_STRING;\n+\n+/**\n+ * The TracingContext class to correlate Store requests using unique\n+ * identifiers and resources common to requests (e.g. filesystem, stream)\n+ *\n+ * Implementing new HDFS method:\n+ * Create TracingContext instance in method of outer layer of\n+ * ABFS driver (AzureBlobFileSystem/AbfsInputStream/AbfsOutputStream), to be\n+ * passed through ABFS layers up to AbfsRestOperation.\n+ *\n+ * Add new operations to HdfsOperationConstants file.\n+ *\n+ * PrimaryRequestId can be enabled for individual HDFS API that invoke\n+ * multiple Store calls.\n+ *\n+ * Testing:\n+ * Pass an instance of TracingHeaderValidator to registerListener() of ABFS\n+ * filesystem/stream class before calling the API in tests.\n+ */\n+\n+public class TracingContext {\n+  private final String clientCorrelationID;\n+  private final String fileSystemID;\n+  private String clientRequestId = EMPTY_STRING;\n+  private String primaryRequestID;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1567e3e82240a242a6f11970d6d313db575a3787"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYzMTEzMQ==", "bodyText": "call it opType only?", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658631131", "createdAt": "2021-06-25T09:34:52Z", "author": {"login": "anoopsjohn"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java", "diffHunk": "@@ -0,0 +1,170 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+import java.util.UUID;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hadoop.fs.azurebfs.constants.FSOperationType;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsClient;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.EMPTY_STRING;\n+\n+/**\n+ * The TracingContext class to correlate Store requests using unique\n+ * identifiers and resources common to requests (e.g. filesystem, stream)\n+ *\n+ * Implementing new HDFS method:\n+ * Create TracingContext instance in method of outer layer of\n+ * ABFS driver (AzureBlobFileSystem/AbfsInputStream/AbfsOutputStream), to be\n+ * passed through ABFS layers up to AbfsRestOperation.\n+ *\n+ * Add new operations to HdfsOperationConstants file.\n+ *\n+ * PrimaryRequestId can be enabled for individual HDFS API that invoke\n+ * multiple Store calls.\n+ *\n+ * Testing:\n+ * Pass an instance of TracingHeaderValidator to registerListener() of ABFS\n+ * filesystem/stream class before calling the API in tests.\n+ */\n+\n+public class TracingContext {\n+  private final String clientCorrelationID;\n+  private final String fileSystemID;\n+  private String clientRequestId = EMPTY_STRING;\n+  private String primaryRequestID;\n+  private String streamID;\n+  private int retryCount;\n+  private FSOperationType hadoopOpName;\n+  private final TracingHeaderFormat format;\n+  private Listener listener = null;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(AbfsClient.class);\n+  public static final int MAX_CLIENT_CORRELATION_ID_LENGTH = 72;\n+  public static final String CLIENT_CORRELATION_ID_PATTERN = \"[a-zA-Z0-9-]*\";\n+\n+  /**\n+   * Initialize TracingContext\n+   * @param clientCorrelationID Provided over config by client\n+   * @param fileSystemID Unique guid for AzureBlobFileSystem instance\n+   * @param hadoopOpName Code indicating the high-level Hadoop operation that\n+   *                    triggered the current Store request\n+   * @param tracingHeaderFormat Format of IDs to be printed in header and logs\n+   * @param listener Holds instance of TracingHeaderValidator during testing,\n+   *                null otherwise\n+   */\n+  public TracingContext(String clientCorrelationID, String fileSystemID,\n+      FSOperationType hadoopOpName, TracingHeaderFormat tracingHeaderFormat,\n+      Listener listener) {\n+    this.fileSystemID = fileSystemID;\n+    this.hadoopOpName = hadoopOpName;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1567e3e82240a242a6f11970d6d313db575a3787"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYzMzQ1Ng==", "bodyText": "A regex matching call is not that cheap. We will end up calling this for every object creation of this TracingContext.  Can we limit this check only at the time of FS instantiation?", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658633456", "createdAt": "2021-06-25T09:38:32Z", "author": {"login": "anoopsjohn"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java", "diffHunk": "@@ -0,0 +1,170 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+import java.util.UUID;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hadoop.fs.azurebfs.constants.FSOperationType;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsClient;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.EMPTY_STRING;\n+\n+/**\n+ * The TracingContext class to correlate Store requests using unique\n+ * identifiers and resources common to requests (e.g. filesystem, stream)\n+ *\n+ * Implementing new HDFS method:\n+ * Create TracingContext instance in method of outer layer of\n+ * ABFS driver (AzureBlobFileSystem/AbfsInputStream/AbfsOutputStream), to be\n+ * passed through ABFS layers up to AbfsRestOperation.\n+ *\n+ * Add new operations to HdfsOperationConstants file.\n+ *\n+ * PrimaryRequestId can be enabled for individual HDFS API that invoke\n+ * multiple Store calls.\n+ *\n+ * Testing:\n+ * Pass an instance of TracingHeaderValidator to registerListener() of ABFS\n+ * filesystem/stream class before calling the API in tests.\n+ */\n+\n+public class TracingContext {\n+  private final String clientCorrelationID;\n+  private final String fileSystemID;\n+  private String clientRequestId = EMPTY_STRING;\n+  private String primaryRequestID;\n+  private String streamID;\n+  private int retryCount;\n+  private FSOperationType hadoopOpName;\n+  private final TracingHeaderFormat format;\n+  private Listener listener = null;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(AbfsClient.class);\n+  public static final int MAX_CLIENT_CORRELATION_ID_LENGTH = 72;\n+  public static final String CLIENT_CORRELATION_ID_PATTERN = \"[a-zA-Z0-9-]*\";\n+\n+  /**\n+   * Initialize TracingContext\n+   * @param clientCorrelationID Provided over config by client\n+   * @param fileSystemID Unique guid for AzureBlobFileSystem instance\n+   * @param hadoopOpName Code indicating the high-level Hadoop operation that\n+   *                    triggered the current Store request\n+   * @param tracingHeaderFormat Format of IDs to be printed in header and logs\n+   * @param listener Holds instance of TracingHeaderValidator during testing,\n+   *                null otherwise\n+   */\n+  public TracingContext(String clientCorrelationID, String fileSystemID,\n+      FSOperationType hadoopOpName, TracingHeaderFormat tracingHeaderFormat,\n+      Listener listener) {\n+    this.fileSystemID = fileSystemID;\n+    this.hadoopOpName = hadoopOpName;\n+    this.clientCorrelationID = validateClientCorrelationID(clientCorrelationID);\n+    streamID = EMPTY_STRING;\n+    retryCount = 0;\n+    primaryRequestID = EMPTY_STRING;\n+    format = tracingHeaderFormat;\n+    this.listener = listener;\n+  }\n+\n+  public TracingContext(String clientCorrelationID, String fileSystemID,\n+      FSOperationType hadoopOpName, boolean needsPrimaryReqId,\n+      TracingHeaderFormat tracingHeaderFormat, Listener listener) {\n+    this(clientCorrelationID, fileSystemID, hadoopOpName, tracingHeaderFormat,\n+        listener);\n+    primaryRequestID = needsPrimaryReqId ? UUID.randomUUID().toString() : \"\";\n+    if (listener != null) {\n+      listener.updatePrimaryRequestID(primaryRequestID);\n+    }\n+  }\n+\n+  public TracingContext(TracingContext originalTracingContext) {\n+    this.fileSystemID = originalTracingContext.fileSystemID;\n+    this.streamID = originalTracingContext.streamID;\n+    this.clientCorrelationID = originalTracingContext.clientCorrelationID;\n+    this.hadoopOpName = originalTracingContext.hadoopOpName;\n+    this.retryCount = 0;\n+    this.primaryRequestID = originalTracingContext.primaryRequestID;\n+    this.format = originalTracingContext.format;\n+    if (originalTracingContext.listener != null) {\n+      this.listener = originalTracingContext.listener.getClone();\n+    }\n+  }\n+\n+  public String validateClientCorrelationID(String clientCorrelationID) {\n+    if ((clientCorrelationID.length() > MAX_CLIENT_CORRELATION_ID_LENGTH)\n+        || (!clientCorrelationID.matches(CLIENT_CORRELATION_ID_PATTERN))) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1567e3e82240a242a6f11970d6d313db575a3787"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODY0NDAzMA==", "bodyText": "In any kind of op clientCorrelationID , clientRequestId , fileSystemID , hadoopOpName and retryCount will be present\nOptional things are primaryRequestID and streamID .  Correct?\nWorth detailing in some comments here.\nWhen these 2 are not there it will come like ::::...  ?", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658644030", "createdAt": "2021-06-25T09:55:21Z", "author": {"login": "anoopsjohn"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java", "diffHunk": "@@ -0,0 +1,170 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+import java.util.UUID;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hadoop.fs.azurebfs.constants.FSOperationType;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsClient;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.EMPTY_STRING;\n+\n+/**\n+ * The TracingContext class to correlate Store requests using unique\n+ * identifiers and resources common to requests (e.g. filesystem, stream)\n+ *\n+ * Implementing new HDFS method:\n+ * Create TracingContext instance in method of outer layer of\n+ * ABFS driver (AzureBlobFileSystem/AbfsInputStream/AbfsOutputStream), to be\n+ * passed through ABFS layers up to AbfsRestOperation.\n+ *\n+ * Add new operations to HdfsOperationConstants file.\n+ *\n+ * PrimaryRequestId can be enabled for individual HDFS API that invoke\n+ * multiple Store calls.\n+ *\n+ * Testing:\n+ * Pass an instance of TracingHeaderValidator to registerListener() of ABFS\n+ * filesystem/stream class before calling the API in tests.\n+ */\n+\n+public class TracingContext {\n+  private final String clientCorrelationID;\n+  private final String fileSystemID;\n+  private String clientRequestId = EMPTY_STRING;\n+  private String primaryRequestID;\n+  private String streamID;\n+  private int retryCount;\n+  private FSOperationType hadoopOpName;\n+  private final TracingHeaderFormat format;\n+  private Listener listener = null;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(AbfsClient.class);\n+  public static final int MAX_CLIENT_CORRELATION_ID_LENGTH = 72;\n+  public static final String CLIENT_CORRELATION_ID_PATTERN = \"[a-zA-Z0-9-]*\";\n+\n+  /**\n+   * Initialize TracingContext\n+   * @param clientCorrelationID Provided over config by client\n+   * @param fileSystemID Unique guid for AzureBlobFileSystem instance\n+   * @param hadoopOpName Code indicating the high-level Hadoop operation that\n+   *                    triggered the current Store request\n+   * @param tracingHeaderFormat Format of IDs to be printed in header and logs\n+   * @param listener Holds instance of TracingHeaderValidator during testing,\n+   *                null otherwise\n+   */\n+  public TracingContext(String clientCorrelationID, String fileSystemID,\n+      FSOperationType hadoopOpName, TracingHeaderFormat tracingHeaderFormat,\n+      Listener listener) {\n+    this.fileSystemID = fileSystemID;\n+    this.hadoopOpName = hadoopOpName;\n+    this.clientCorrelationID = validateClientCorrelationID(clientCorrelationID);\n+    streamID = EMPTY_STRING;\n+    retryCount = 0;\n+    primaryRequestID = EMPTY_STRING;\n+    format = tracingHeaderFormat;\n+    this.listener = listener;\n+  }\n+\n+  public TracingContext(String clientCorrelationID, String fileSystemID,\n+      FSOperationType hadoopOpName, boolean needsPrimaryReqId,\n+      TracingHeaderFormat tracingHeaderFormat, Listener listener) {\n+    this(clientCorrelationID, fileSystemID, hadoopOpName, tracingHeaderFormat,\n+        listener);\n+    primaryRequestID = needsPrimaryReqId ? UUID.randomUUID().toString() : \"\";\n+    if (listener != null) {\n+      listener.updatePrimaryRequestID(primaryRequestID);\n+    }\n+  }\n+\n+  public TracingContext(TracingContext originalTracingContext) {\n+    this.fileSystemID = originalTracingContext.fileSystemID;\n+    this.streamID = originalTracingContext.streamID;\n+    this.clientCorrelationID = originalTracingContext.clientCorrelationID;\n+    this.hadoopOpName = originalTracingContext.hadoopOpName;\n+    this.retryCount = 0;\n+    this.primaryRequestID = originalTracingContext.primaryRequestID;\n+    this.format = originalTracingContext.format;\n+    if (originalTracingContext.listener != null) {\n+      this.listener = originalTracingContext.listener.getClone();\n+    }\n+  }\n+\n+  public String validateClientCorrelationID(String clientCorrelationID) {\n+    if ((clientCorrelationID.length() > MAX_CLIENT_CORRELATION_ID_LENGTH)\n+        || (!clientCorrelationID.matches(CLIENT_CORRELATION_ID_PATTERN))) {\n+      LOG.debug(\n+          \"Invalid config provided; correlation id not included in header.\");\n+      return EMPTY_STRING;\n+    }\n+    return clientCorrelationID;\n+  }\n+\n+  public void generateClientRequestId() {\n+    clientRequestId = UUID.randomUUID().toString();\n+  }\n+\n+  public void setPrimaryRequestID() {\n+    primaryRequestID = UUID.randomUUID().toString();\n+    if (listener != null) {\n+      listener.updatePrimaryRequestID(primaryRequestID);\n+    }\n+  }\n+\n+  public void setStreamID(String stream) {\n+    streamID = stream;\n+  }\n+\n+  public void setOperation(FSOperationType operation) {\n+    this.hadoopOpName = operation;\n+  }\n+\n+  public void setRetryCount(int retryCount) {\n+    this.retryCount = retryCount;\n+  }\n+\n+  public void setListener(Listener listener) {\n+    this.listener = listener;\n+  }\n+\n+  public String constructHeader() {\n+    String header;\n+    switch (format) {\n+    case ALL_ID_FORMAT:\n+      header =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1567e3e82240a242a6f11970d6d313db575a3787"}, "originalPosition": 153}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODY0NTEyOQ==", "bodyText": "Will the below way will be more consistent?\nSINGLE_ID_FORMAT,  // client-req-id\nTWO_ID_FORMAT; // client-req-id:client-correlation-id\nALL_ID_FORMAT,  // client-req-id:client-correlation-id:filesystem-id:primary-req-id:stream-id:hdfs-operation:retry-count", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658645129", "createdAt": "2021-06-25T09:57:09Z", "author": {"login": "anoopsjohn"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderFormat.java", "diffHunk": "@@ -0,0 +1,28 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+public enum TracingHeaderFormat {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1567e3e82240a242a6f11970d6d313db575a3787"}, "originalPosition": 21}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ecee1809de89ed5b4fc76df79dd64895537aab99", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/ecee1809de89ed5b4fc76df79dd64895537aab99", "committedDate": "2021-06-28T20:02:10Z", "message": "revw comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b69d9e209e0f6646cd1f1ff6a1a84ac4e7f3bfc8", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/b69d9e209e0f6646cd1f1ff6a1a84ac4e7f3bfc8", "committedDate": "2021-06-30T09:18:17Z", "message": "correction"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8cf0fe32c89351a55c475cdd91261335591129f8", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/8cf0fe32c89351a55c475cdd91261335591129f8", "committedDate": "2021-07-01T09:00:24Z", "message": "checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0dc83f4367c6d674a7d8f0bfba0498c26c4551e1", "author": {"user": {"login": "sumangala-patki", "name": null}}, "url": "https://github.com/apache/hadoop/commit/0dc83f4367c6d674a7d8f0bfba0498c26c4551e1", "committedDate": "2021-07-02T12:03:53Z", "message": "set header in tc"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3372, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}