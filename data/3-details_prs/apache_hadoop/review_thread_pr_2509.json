{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTMwNTE1Mjc0", "number": 2509, "reviewThreads": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQwODo0ODozNFrOFDW5gw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNVQyMDoyMjoxNFrOFLbV1w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5MDY1MjE5OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQwODo0ODozNFrOIC93bw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwNzozNzoxNlrOIDsjcw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTk4MTY3OQ==", "bodyText": "ensure number of calls to append is correct", "url": "https://github.com/apache/hadoop/pull/2509#discussion_r539981679", "createdAt": "2020-12-10T08:48:34Z", "author": {"login": "vinaysbadami"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -108,13 +103,15 @@ public void verifyShortWriteRequest() throws Exception {\n \n     out.hsync();\n \n-    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acBufferOffset.capture(), acBufferLength.capture(),\n-                                    acSASToken.capture(), acAppendBlobAppend.capture());\n-    assertThat(Arrays.asList(PATH, PATH)).describedAs(\"Path of the requests\").isEqualTo(acString.getAllValues());\n-    assertThat(Arrays.asList(Long.valueOf(0), Long.valueOf(WRITE_SIZE))).describedAs(\"Write Position\").isEqualTo(acLong.getAllValues());\n-    assertThat(Arrays.asList(0, 0)).describedAs(\"Buffer Offset\").isEqualTo(acBufferOffset.getAllValues());\n-    assertThat(Arrays.asList(WRITE_SIZE, 2*WRITE_SIZE)).describedAs(\"Buffer length\").isEqualTo(acBufferLength.getAllValues());\n+    AppendRequestParameters firstReqParameters = new AppendRequestParameters(\n+        0, 0, WRITE_SIZE, APPEND_MODE, false);\n+    AppendRequestParameters secondReqParameters = new AppendRequestParameters(\n+        WRITE_SIZE, 0, 2 * WRITE_SIZE, APPEND_MODE, false);\n \n+    verify(client, times(1)).append(\n+        eq(PATH), any(byte[].class), refEq(firstReqParameters), any());\n+    verify(client, times(1)).append(\n+        eq(PATH), any(byte[].class), refEq(secondReqParameters), any());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "843a01510611c4a5a1e4a365f2a5a2e3b43a51f9"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDc0NjYxMQ==", "bodyText": "Done", "url": "https://github.com/apache/hadoop/pull/2509#discussion_r540746611", "createdAt": "2020-12-11T07:37:16Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -108,13 +103,15 @@ public void verifyShortWriteRequest() throws Exception {\n \n     out.hsync();\n \n-    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acBufferOffset.capture(), acBufferLength.capture(),\n-                                    acSASToken.capture(), acAppendBlobAppend.capture());\n-    assertThat(Arrays.asList(PATH, PATH)).describedAs(\"Path of the requests\").isEqualTo(acString.getAllValues());\n-    assertThat(Arrays.asList(Long.valueOf(0), Long.valueOf(WRITE_SIZE))).describedAs(\"Write Position\").isEqualTo(acLong.getAllValues());\n-    assertThat(Arrays.asList(0, 0)).describedAs(\"Buffer Offset\").isEqualTo(acBufferOffset.getAllValues());\n-    assertThat(Arrays.asList(WRITE_SIZE, 2*WRITE_SIZE)).describedAs(\"Buffer length\").isEqualTo(acBufferLength.getAllValues());\n+    AppendRequestParameters firstReqParameters = new AppendRequestParameters(\n+        0, 0, WRITE_SIZE, APPEND_MODE, false);\n+    AppendRequestParameters secondReqParameters = new AppendRequestParameters(\n+        WRITE_SIZE, 0, 2 * WRITE_SIZE, APPEND_MODE, false);\n \n+    verify(client, times(1)).append(\n+        eq(PATH), any(byte[].class), refEq(firstReqParameters), any());\n+    verify(client, times(1)).append(\n+        eq(PATH), any(byte[].class), refEq(secondReqParameters), any());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTk4MTY3OQ=="}, "originalCommit": {"oid": "843a01510611c4a5a1e4a365f2a5a2e3b43a51f9"}, "originalPosition": 76}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5MTU3OTA2OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/AbfsHttpConstants.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxMjoxMzozNVrOIDGTLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwNToxMzowNVrOIDpcSA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDExOTg1Mg==", "bodyText": "This can be class level constant right, the lieral \"true\" as such does not have anuthing to do with HTTP related operations.", "url": "https://github.com/apache/hadoop/pull/2509#discussion_r540119852", "createdAt": "2020-12-10T12:13:35Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/AbfsHttpConstants.java", "diffHunk": "@@ -76,6 +76,7 @@\n   public static final String AT = \"@\";\n   public static final String HTTP_HEADER_PREFIX = \"x-ms-\";\n   public static final String HASH = \"#\";\n+  public static final String TRUE = \"true\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1e7f30877dcc2e3baedf99eeb29c4abff7bf0423"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDY5NTYyNA==", "bodyText": "Boolean query param value field. Will retain.", "url": "https://github.com/apache/hadoop/pull/2509#discussion_r540695624", "createdAt": "2020-12-11T05:13:05Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/AbfsHttpConstants.java", "diffHunk": "@@ -76,6 +76,7 @@\n   public static final String AT = \"@\";\n   public static final String HTTP_HEADER_PREFIX = \"x-ms-\";\n   public static final String HASH = \"#\";\n+  public static final String TRUE = \"true\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDExOTg1Mg=="}, "originalCommit": {"oid": "1e7f30877dcc2e3baedf99eeb29c4abff7bf0423"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5MTU4OTE4OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxMjoxNTo1NVrOIDGY1w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwNToyMDozM1rOIDpkbA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDEyMTMwMw==", "bodyText": "Can this be 2 separate if blocks", "url": "https://github.com/apache/hadoop/pull/2509#discussion_r540121303", "createdAt": "2020-12-10T12:15:55Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java", "diffHunk": "@@ -395,38 +396,58 @@ public AbfsRestOperation renameIdempotencyCheckOp(\n     return op;\n   }\n \n-  public AbfsRestOperation append(final String path, final long position, final byte[] buffer, final int offset,\n-                                  final int length, final String cachedSasToken, final boolean isAppendBlob) throws AzureBlobFileSystemException {\n+  public AbfsRestOperation append(final String path, final byte[] buffer,\n+      AppendRequestParameters reqParams, final String cachedSasToken)\n+      throws AzureBlobFileSystemException {\n     final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\n     // JDK7 does not support PATCH, so to workaround the issue we will use\n     // PUT and specify the real method in the X-Http-Method-Override header.\n     requestHeaders.add(new AbfsHttpHeader(X_HTTP_METHOD_OVERRIDE,\n-            HTTP_METHOD_PATCH));\n+        HTTP_METHOD_PATCH));\n \n     final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\n     abfsUriQueryBuilder.addQuery(QUERY_PARAM_ACTION, APPEND_ACTION);\n-    abfsUriQueryBuilder.addQuery(QUERY_PARAM_POSITION, Long.toString(position));\n+    abfsUriQueryBuilder.addQuery(QUERY_PARAM_POSITION, Long.toString(reqParams.getPosition()));\n+\n+    if ((reqParams.getMode() == AppendRequestParameters.Mode.FLUSH_MODE) || (", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1e7f30877dcc2e3baedf99eeb29c4abff7bf0423"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDY5NzcwOA==", "bodyText": "On either condition being true, flush queryParam will be true. and for close alone close queryParam will be true.", "url": "https://github.com/apache/hadoop/pull/2509#discussion_r540697708", "createdAt": "2020-12-11T05:20:33Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java", "diffHunk": "@@ -395,38 +396,58 @@ public AbfsRestOperation renameIdempotencyCheckOp(\n     return op;\n   }\n \n-  public AbfsRestOperation append(final String path, final long position, final byte[] buffer, final int offset,\n-                                  final int length, final String cachedSasToken, final boolean isAppendBlob) throws AzureBlobFileSystemException {\n+  public AbfsRestOperation append(final String path, final byte[] buffer,\n+      AppendRequestParameters reqParams, final String cachedSasToken)\n+      throws AzureBlobFileSystemException {\n     final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\n     // JDK7 does not support PATCH, so to workaround the issue we will use\n     // PUT and specify the real method in the X-Http-Method-Override header.\n     requestHeaders.add(new AbfsHttpHeader(X_HTTP_METHOD_OVERRIDE,\n-            HTTP_METHOD_PATCH));\n+        HTTP_METHOD_PATCH));\n \n     final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\n     abfsUriQueryBuilder.addQuery(QUERY_PARAM_ACTION, APPEND_ACTION);\n-    abfsUriQueryBuilder.addQuery(QUERY_PARAM_POSITION, Long.toString(position));\n+    abfsUriQueryBuilder.addQuery(QUERY_PARAM_POSITION, Long.toString(reqParams.getPosition()));\n+\n+    if ((reqParams.getMode() == AppendRequestParameters.Mode.FLUSH_MODE) || (", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDEyMTMwMw=="}, "originalCommit": {"oid": "1e7f30877dcc2e3baedf99eeb29c4abff7bf0423"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5MTYzODkzOnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestSmallWriteOptimization.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxMjoyNzowOFrOIDG1MQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwNjoyNDo0NFrOIDq26g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDEyODU2MQ==", "bodyText": "you may remove this new line in between", "url": "https://github.com/apache/hadoop/pull/2509#discussion_r540128561", "createdAt": "2020-12-10T12:27:08Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestSmallWriteOptimization.java", "diffHunk": "@@ -0,0 +1,524 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.util.Arrays;\n+import java.util.Random;\n+import java.util.UUID;\n+import java.util.Map;\n+import java.io.IOException;\n+\n+import org.assertj.core.api.Assertions;\n+import org.junit.Assume;\n+import org.junit.runners.Parameterized;\n+import org.junit.runner.RunWith;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1e7f30877dcc2e3baedf99eeb29c4abff7bf0423"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDcxODgyNg==", "bodyText": "Done", "url": "https://github.com/apache/hadoop/pull/2509#discussion_r540718826", "createdAt": "2020-12-11T06:24:44Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestSmallWriteOptimization.java", "diffHunk": "@@ -0,0 +1,524 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.util.Arrays;\n+import java.util.Random;\n+import java.util.UUID;\n+import java.util.Map;\n+import java.io.IOException;\n+\n+import org.assertj.core.api.Assertions;\n+import org.junit.Assume;\n+import org.junit.runners.Parameterized;\n+import org.junit.runner.RunWith;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDEyODU2MQ=="}, "originalCommit": {"oid": "1e7f30877dcc2e3baedf99eeb29c4abff7bf0423"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ3NTI2NjE1OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNVQyMDoyMjoxNFrOIOl-Sg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQxNzowMjowM1rOIPM6Dg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjE3MzEzMA==", "bodyText": "for newly added config key, a little comment would be very helpful", "url": "https://github.com/apache/hadoop/pull/2509#discussion_r552173130", "createdAt": "2021-01-05T20:22:14Z", "author": {"login": "DadanielZ"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java", "diffHunk": "@@ -55,6 +55,7 @@\n   public static final String AZURE_WRITE_MAX_CONCURRENT_REQUESTS = \"fs.azure.write.max.concurrent.requests\";\n   public static final String AZURE_WRITE_MAX_REQUESTS_TO_QUEUE = \"fs.azure.write.max.requests.to.queue\";\n   public static final String AZURE_WRITE_BUFFER_SIZE = \"fs.azure.write.request.size\";\n+  public static final String AZURE_ENABLE_SMALL_WRITE_OPTIMIZATION = \"fs.azure.write.enableappendwithflush\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "21b8e6d822f7113ddd2c283a48716ac69813267e"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjgxMTAyMg==", "bodyText": "Done", "url": "https://github.com/apache/hadoop/pull/2509#discussion_r552811022", "createdAt": "2021-01-06T17:02:03Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java", "diffHunk": "@@ -55,6 +55,7 @@\n   public static final String AZURE_WRITE_MAX_CONCURRENT_REQUESTS = \"fs.azure.write.max.concurrent.requests\";\n   public static final String AZURE_WRITE_MAX_REQUESTS_TO_QUEUE = \"fs.azure.write.max.requests.to.queue\";\n   public static final String AZURE_WRITE_BUFFER_SIZE = \"fs.azure.write.request.size\";\n+  public static final String AZURE_ENABLE_SMALL_WRITE_OPTIMIZATION = \"fs.azure.write.enableappendwithflush\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjE3MzEzMA=="}, "originalCommit": {"oid": "21b8e6d822f7113ddd2c283a48716ac69813267e"}, "originalPosition": 4}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3153, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}