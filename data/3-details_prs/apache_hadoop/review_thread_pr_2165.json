{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU0OTc0Mzgx", "number": 2165, "reviewThreads": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQxNzo1OToyMVrOESEftg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQwNTo0OTo1N1rOETh5HQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3MzgzNDc4OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQxNzo1OToyMVrOG3GV-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQwMDozNDowNVrOG34W4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDQyODc5Mg==", "bodyText": "Lets move this to a different funcation like chooseSnapshottableDir, as this might be policy controleed later.", "url": "https://github.com/apache/hadoop/pull/2165#discussion_r460428792", "createdAt": "2020-07-25T17:59:21Z", "author": {"login": "mukul1987"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "diffHunk": "@@ -629,4 +641,36 @@ public void shutdown() {\n         s.getRoot().getLocalName(), s.getRoot().getFullPathName(),\n         s.getRoot().getModificationTime());\n   }\n-}\n+\n+  Snapshot.Root chooseDeletedSnapshot() {\n+    final List<INodeDirectory> dirs = getSnapshottableDirs();\n+    Collections.shuffle(dirs);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc1debd29a01c520fa344ff6a4dc934693c05f8e"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTI0ODIyNg==", "bodyText": "Sure.", "url": "https://github.com/apache/hadoop/pull/2165#discussion_r461248226", "createdAt": "2020-07-28T00:34:05Z", "author": {"login": "szetszwo"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "diffHunk": "@@ -629,4 +641,36 @@ public void shutdown() {\n         s.getRoot().getLocalName(), s.getRoot().getFullPathName(),\n         s.getRoot().getModificationTime());\n   }\n-}\n+\n+  Snapshot.Root chooseDeletedSnapshot() {\n+    final List<INodeDirectory> dirs = getSnapshottableDirs();\n+    Collections.shuffle(dirs);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDQyODc5Mg=="}, "originalCommit": {"oid": "dc1debd29a01c520fa344ff6a4dc934693c05f8e"}, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3MzgzNTUyOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotDeletionGc.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQxODowMDoxN1rOG3GWTQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQwMDozMzo1MlrOG34Wqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDQyODg3Nw==", "bodyText": "Lets add an assert that this is the first snapshot in the snapshottable dir ?", "url": "https://github.com/apache/hadoop/pull/2165#discussion_r460428877", "createdAt": "2020-07-25T18:00:17Z", "author": {"login": "mukul1987"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotDeletionGc.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.namenode.snapshot;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;\n+import org.apache.hadoop.util.StringUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Timer;\n+import java.util.TimerTask;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager.DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS;\n+import static org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager.DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS_DEFAULT;\n+\n+public class SnapshotDeletionGc {\n+  public static final Logger LOG = LoggerFactory.getLogger(\n+      SnapshotDeletionGc.class);\n+\n+  private final FSNamesystem namesystem;\n+  private final int deletionOrderedGcPeriodMs;\n+  private final AtomicReference<Timer> timer = new AtomicReference<>();\n+\n+  public SnapshotDeletionGc(FSNamesystem namesystem, Configuration conf) {\n+    this.namesystem = namesystem;\n+\n+    this.deletionOrderedGcPeriodMs = conf.getInt(\n+        DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS,\n+        DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS_DEFAULT);\n+    LOG.info(\"{} = {}\", DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS,\n+        deletionOrderedGcPeriodMs);\n+  }\n+\n+  public void schedule() {\n+    if (timer.get() != null) {\n+      return;\n+    }\n+    final Timer t = new Timer(getClass().getSimpleName(), true);\n+    if (timer.compareAndSet(null, t)) {\n+      LOG.info(\"Schedule at fixed rate {}\",\n+          StringUtils.formatTime(deletionOrderedGcPeriodMs));\n+      t.scheduleAtFixedRate(new GcTask(),\n+          deletionOrderedGcPeriodMs, deletionOrderedGcPeriodMs);\n+    }\n+  }\n+\n+  public void cancel() {\n+    final Timer t = timer.getAndSet(null);\n+    if (t != null) {\n+      LOG.info(\"cancel\");\n+      t.cancel();\n+    }\n+  }\n+\n+  private void gcDeletedSnapshot(String name) {\n+    final Snapshot.Root deleted;\n+    namesystem.readLock();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc1debd29a01c520fa344ff6a4dc934693c05f8e"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTI0ODE3MQ==", "bodyText": "This is a good idea.  Let's add the assertion to all the cases when the SnapshotDeletionOrdered feature is enabled.", "url": "https://github.com/apache/hadoop/pull/2165#discussion_r461248171", "createdAt": "2020-07-28T00:33:52Z", "author": {"login": "szetszwo"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotDeletionGc.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.namenode.snapshot;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;\n+import org.apache.hadoop.util.StringUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Timer;\n+import java.util.TimerTask;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager.DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS;\n+import static org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager.DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS_DEFAULT;\n+\n+public class SnapshotDeletionGc {\n+  public static final Logger LOG = LoggerFactory.getLogger(\n+      SnapshotDeletionGc.class);\n+\n+  private final FSNamesystem namesystem;\n+  private final int deletionOrderedGcPeriodMs;\n+  private final AtomicReference<Timer> timer = new AtomicReference<>();\n+\n+  public SnapshotDeletionGc(FSNamesystem namesystem, Configuration conf) {\n+    this.namesystem = namesystem;\n+\n+    this.deletionOrderedGcPeriodMs = conf.getInt(\n+        DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS,\n+        DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS_DEFAULT);\n+    LOG.info(\"{} = {}\", DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS,\n+        deletionOrderedGcPeriodMs);\n+  }\n+\n+  public void schedule() {\n+    if (timer.get() != null) {\n+      return;\n+    }\n+    final Timer t = new Timer(getClass().getSimpleName(), true);\n+    if (timer.compareAndSet(null, t)) {\n+      LOG.info(\"Schedule at fixed rate {}\",\n+          StringUtils.formatTime(deletionOrderedGcPeriodMs));\n+      t.scheduleAtFixedRate(new GcTask(),\n+          deletionOrderedGcPeriodMs, deletionOrderedGcPeriodMs);\n+    }\n+  }\n+\n+  public void cancel() {\n+    final Timer t = timer.getAndSet(null);\n+    if (t != null) {\n+      LOG.info(\"cancel\");\n+      t.cancel();\n+    }\n+  }\n+\n+  private void gcDeletedSnapshot(String name) {\n+    final Snapshot.Root deleted;\n+    namesystem.readLock();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDQyODg3Nw=="}, "originalCommit": {"oid": "dc1debd29a01c520fa344ff6a4dc934693c05f8e"}, "originalPosition": 75}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3NjE2OTExOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QwODo0NDoyMFrOG3ZK0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQwMDozNzowMFrOG34aRQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDczNzIzMg==", "bodyText": "I think its better to start the gc work in FSNameSystem#startActiveServices() after quota setup and initialization is done\nstartActiveServices()\n``\n// Initialize the quota.\ndir.updateCountForQuota();\n// Enable quota checks.\ndir.enableQuotaChecks();", "url": "https://github.com/apache/hadoop/pull/2165#discussion_r460737232", "createdAt": "2020-07-27T08:44:20Z", "author": {"login": "bshashikant"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java", "diffHunk": "@@ -1284,6 +1288,7 @@ void startCommonServices(Configuration conf, HAContext haContext) throws IOExcep\n       dir.setINodeAttributeProvider(inodeAttributeProvider);\n     }\n     snapshotManager.registerMXBean();\n+    snapshotDeletionGc.schedule();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc1debd29a01c520fa344ff6a4dc934693c05f8e"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTI0OTA5Mw==", "bodyText": "Sure.  Thanks for the suggestion.", "url": "https://github.com/apache/hadoop/pull/2165#discussion_r461249093", "createdAt": "2020-07-28T00:37:00Z", "author": {"login": "szetszwo"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java", "diffHunk": "@@ -1284,6 +1288,7 @@ void startCommonServices(Configuration conf, HAContext haContext) throws IOExcep\n       dir.setINodeAttributeProvider(inodeAttributeProvider);\n     }\n     snapshotManager.registerMXBean();\n+    snapshotDeletionGc.schedule();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDczNzIzMg=="}, "originalCommit": {"oid": "dc1debd29a01c520fa344ff6a4dc934693c05f8e"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4MjkwODEyOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQxODoyODo1N1rOG4ZNIA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQxOToyNToyMlrOG4bKxQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTc4NjQwMA==", "bodyText": "This assertion should also be inside gcDeletedSnapshot ?", "url": "https://github.com/apache/hadoop/pull/2165#discussion_r461786400", "createdAt": "2020-07-28T18:28:57Z", "author": {"login": "mukul1987"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "diffHunk": "@@ -382,6 +395,14 @@ public void deleteSnapshot(final INodesInPath iip, final String snapshotName,\n             EnumSet.of(XAttrSetFlag.CREATE, XAttrSetFlag.REPLACE));\n         return;\n       }\n+\n+      // assert if it is deleting the first snapshot\n+      final INodeDirectoryAttributes first = snapshottable.getDiffs().getFirstSnapshotINode();\n+      if (snapshot.getRoot() != first) {\n+        throw new IllegalStateException(\"Failed to delete snapshot \" + snapshotName", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "99dab64faea4426ad8c82eba6b581c984ac98943"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTgxODU2NQ==", "bodyText": "No.  gcDeletedSnapshot will end up here.", "url": "https://github.com/apache/hadoop/pull/2165#discussion_r461818565", "createdAt": "2020-07-28T19:25:22Z", "author": {"login": "szetszwo"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "diffHunk": "@@ -382,6 +395,14 @@ public void deleteSnapshot(final INodesInPath iip, final String snapshotName,\n             EnumSet.of(XAttrSetFlag.CREATE, XAttrSetFlag.REPLACE));\n         return;\n       }\n+\n+      // assert if it is deleting the first snapshot\n+      final INodeDirectoryAttributes first = snapshottable.getDiffs().getFirstSnapshotINode();\n+      if (snapshot.getRoot() != first) {\n+        throw new IllegalStateException(\"Failed to delete snapshot \" + snapshotName", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTc4NjQwMA=="}, "originalCommit": {"oid": "99dab64faea4426ad8c82eba6b581c984ac98943"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4MzAyNTI1OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQxOTowMTo1MlrOG4aWcQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQxOTo1NTozOFrOG4cbBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTgwNTE2OQ==", "bodyText": "Lets assert that xattr is set on the snapshot and this is the first snapshot to be deleted.", "url": "https://github.com/apache/hadoop/pull/2165#discussion_r461805169", "createdAt": "2020-07-28T19:01:52Z", "author": {"login": "mukul1987"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java", "diffHunk": "@@ -7166,6 +7175,29 @@ void deleteSnapshot(String snapshotRoot, String snapshotName,\n     logAuditEvent(true, operationName, rootPath, null, null);\n   }\n \n+  public void gcDeletedSnapshot(String snapshotRoot, String snapshotName)\n+      throws IOException {\n+    final String operationName = \"gcDeletedSnapshot\";\n+    String rootPath = null;\n+    final INode.BlocksMapUpdateInfo blocksToBeDeleted;\n+\n+    checkOperation(OperationCategory.WRITE);\n+    writeLock();\n+    try {\n+      checkOperation(OperationCategory.WRITE);\n+      rootPath = Snapshot.getSnapshotPath(snapshotRoot, snapshotName);\n+      checkNameNodeSafeMode(\"Cannot gcDeletedSnapshot for \" + rootPath);\n+\n+      final long now = Time.now();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "99dab64faea4426ad8c82eba6b581c984ac98943"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTgzOTExMQ==", "bodyText": "Let's check if the snapshot isMarkedAsDeleted.  The \"first snapshot\" check is already in the code.", "url": "https://github.com/apache/hadoop/pull/2165#discussion_r461839111", "createdAt": "2020-07-28T19:55:38Z", "author": {"login": "szetszwo"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java", "diffHunk": "@@ -7166,6 +7175,29 @@ void deleteSnapshot(String snapshotRoot, String snapshotName,\n     logAuditEvent(true, operationName, rootPath, null, null);\n   }\n \n+  public void gcDeletedSnapshot(String snapshotRoot, String snapshotName)\n+      throws IOException {\n+    final String operationName = \"gcDeletedSnapshot\";\n+    String rootPath = null;\n+    final INode.BlocksMapUpdateInfo blocksToBeDeleted;\n+\n+    checkOperation(OperationCategory.WRITE);\n+    writeLock();\n+    try {\n+      checkOperation(OperationCategory.WRITE);\n+      rootPath = Snapshot.getSnapshotPath(snapshotRoot, snapshotName);\n+      checkNameNodeSafeMode(\"Cannot gcDeletedSnapshot for \" + rootPath);\n+\n+      final long now = Time.now();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTgwNTE2OQ=="}, "originalCommit": {"oid": "99dab64faea4426ad8c82eba6b581c984ac98943"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4MzAzMjI2OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotDeletionGc.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQxOTowNDowNFrOG4aa7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQxOTo1OTo1NFrOG4ckWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTgwNjMxOQ==", "bodyText": "Lets change this to error and abort and exit from the namenode if the deletion is not possible.\nI feel we should do this earlier to identify issues with snapshot deletion code.", "url": "https://github.com/apache/hadoop/pull/2165#discussion_r461806319", "createdAt": "2020-07-28T19:04:04Z", "author": {"login": "mukul1987"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotDeletionGc.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.namenode.snapshot;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;\n+import org.apache.hadoop.util.StringUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Timer;\n+import java.util.TimerTask;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager.DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS;\n+import static org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager.DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS_DEFAULT;\n+\n+public class SnapshotDeletionGc {\n+  public static final Logger LOG = LoggerFactory.getLogger(\n+      SnapshotDeletionGc.class);\n+\n+  private final FSNamesystem namesystem;\n+  private final long deletionOrderedGcPeriodMs;\n+  private final AtomicReference<Timer> timer = new AtomicReference<>();\n+\n+  public SnapshotDeletionGc(FSNamesystem namesystem, Configuration conf) {\n+    this.namesystem = namesystem;\n+\n+    this.deletionOrderedGcPeriodMs = conf.getLong(\n+        DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS,\n+        DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS_DEFAULT);\n+    LOG.info(\"{} = {}\", DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS,\n+        deletionOrderedGcPeriodMs);\n+  }\n+\n+  public void schedule() {\n+    if (timer.get() != null) {\n+      return;\n+    }\n+    final Timer t = new Timer(getClass().getSimpleName(), true);\n+    if (timer.compareAndSet(null, t)) {\n+      LOG.info(\"Schedule at fixed rate {}\",\n+          StringUtils.formatTime(deletionOrderedGcPeriodMs));\n+      t.scheduleAtFixedRate(new GcTask(),\n+          deletionOrderedGcPeriodMs, deletionOrderedGcPeriodMs);\n+    }\n+  }\n+\n+  public void cancel() {\n+    final Timer t = timer.getAndSet(null);\n+    if (t != null) {\n+      LOG.info(\"cancel\");\n+      t.cancel();\n+    }\n+  }\n+\n+  private void gcDeletedSnapshot(String name) {\n+    final Snapshot.Root deleted;\n+    namesystem.readLock();\n+    try {\n+      deleted = namesystem.getSnapshotManager().chooseDeletedSnapshot();\n+    } finally {\n+      namesystem.readUnlock();\n+    }\n+    if (deleted == null) {\n+      LOG.trace(\"{}: no snapshots are marked as deleted.\", name);\n+      return;\n+    }\n+\n+    final String snapshotRoot = deleted.getRootFullPathName();\n+    final String snapshotName = deleted.getLocalName();\n+    LOG.info(\"{}: delete snapshot {} from {}\",\n+        name, snapshotName, snapshotRoot);\n+\n+    try {\n+      namesystem.gcDeletedSnapshot(snapshotRoot, snapshotName);\n+    } catch (Throwable e) {\n+      LOG.warn(\"Failed to gcDeletedSnapshot \" + deleted.getFullPathName(), e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "99dab64faea4426ad8c82eba6b581c984ac98943"}, "originalPosition": 94}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTg0MTQ5Ng==", "bodyText": "The user could be also deleting the snapshot since we only have read lock earlier.  The gcDeletedSnapshot here will fail.\nWe cannot kill a production system when a snapshot cannot be deleted.\nI could change the log to error.", "url": "https://github.com/apache/hadoop/pull/2165#discussion_r461841496", "createdAt": "2020-07-28T19:59:54Z", "author": {"login": "szetszwo"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotDeletionGc.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.namenode.snapshot;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;\n+import org.apache.hadoop.util.StringUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Timer;\n+import java.util.TimerTask;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager.DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS;\n+import static org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager.DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS_DEFAULT;\n+\n+public class SnapshotDeletionGc {\n+  public static final Logger LOG = LoggerFactory.getLogger(\n+      SnapshotDeletionGc.class);\n+\n+  private final FSNamesystem namesystem;\n+  private final long deletionOrderedGcPeriodMs;\n+  private final AtomicReference<Timer> timer = new AtomicReference<>();\n+\n+  public SnapshotDeletionGc(FSNamesystem namesystem, Configuration conf) {\n+    this.namesystem = namesystem;\n+\n+    this.deletionOrderedGcPeriodMs = conf.getLong(\n+        DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS,\n+        DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS_DEFAULT);\n+    LOG.info(\"{} = {}\", DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS,\n+        deletionOrderedGcPeriodMs);\n+  }\n+\n+  public void schedule() {\n+    if (timer.get() != null) {\n+      return;\n+    }\n+    final Timer t = new Timer(getClass().getSimpleName(), true);\n+    if (timer.compareAndSet(null, t)) {\n+      LOG.info(\"Schedule at fixed rate {}\",\n+          StringUtils.formatTime(deletionOrderedGcPeriodMs));\n+      t.scheduleAtFixedRate(new GcTask(),\n+          deletionOrderedGcPeriodMs, deletionOrderedGcPeriodMs);\n+    }\n+  }\n+\n+  public void cancel() {\n+    final Timer t = timer.getAndSet(null);\n+    if (t != null) {\n+      LOG.info(\"cancel\");\n+      t.cancel();\n+    }\n+  }\n+\n+  private void gcDeletedSnapshot(String name) {\n+    final Snapshot.Root deleted;\n+    namesystem.readLock();\n+    try {\n+      deleted = namesystem.getSnapshotManager().chooseDeletedSnapshot();\n+    } finally {\n+      namesystem.readUnlock();\n+    }\n+    if (deleted == null) {\n+      LOG.trace(\"{}: no snapshots are marked as deleted.\", name);\n+      return;\n+    }\n+\n+    final String snapshotRoot = deleted.getRootFullPathName();\n+    final String snapshotName = deleted.getLocalName();\n+    LOG.info(\"{}: delete snapshot {} from {}\",\n+        name, snapshotName, snapshotRoot);\n+\n+    try {\n+      namesystem.gcDeletedSnapshot(snapshotRoot, snapshotName);\n+    } catch (Throwable e) {\n+      LOG.warn(\"Failed to gcDeletedSnapshot \" + deleted.getFullPathName(), e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTgwNjMxOQ=="}, "originalCommit": {"oid": "99dab64faea4426ad8c82eba6b581c984ac98943"}, "originalPosition": 94}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4MzAzNjg0OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotDeletionGc.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQxOTowNToyN1rOG4ad1A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQyMDowMjo0MFrOG4cqlg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTgwNzA2MA==", "bodyText": "lets catch Throwable here and log error in case we get an exception here.", "url": "https://github.com/apache/hadoop/pull/2165#discussion_r461807060", "createdAt": "2020-07-28T19:05:27Z", "author": {"login": "mukul1987"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotDeletionGc.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.namenode.snapshot;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;\n+import org.apache.hadoop.util.StringUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Timer;\n+import java.util.TimerTask;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager.DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS;\n+import static org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager.DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS_DEFAULT;\n+\n+public class SnapshotDeletionGc {\n+  public static final Logger LOG = LoggerFactory.getLogger(\n+      SnapshotDeletionGc.class);\n+\n+  private final FSNamesystem namesystem;\n+  private final long deletionOrderedGcPeriodMs;\n+  private final AtomicReference<Timer> timer = new AtomicReference<>();\n+\n+  public SnapshotDeletionGc(FSNamesystem namesystem, Configuration conf) {\n+    this.namesystem = namesystem;\n+\n+    this.deletionOrderedGcPeriodMs = conf.getLong(\n+        DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS,\n+        DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS_DEFAULT);\n+    LOG.info(\"{} = {}\", DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS,\n+        deletionOrderedGcPeriodMs);\n+  }\n+\n+  public void schedule() {\n+    if (timer.get() != null) {\n+      return;\n+    }\n+    final Timer t = new Timer(getClass().getSimpleName(), true);\n+    if (timer.compareAndSet(null, t)) {\n+      LOG.info(\"Schedule at fixed rate {}\",\n+          StringUtils.formatTime(deletionOrderedGcPeriodMs));\n+      t.scheduleAtFixedRate(new GcTask(),\n+          deletionOrderedGcPeriodMs, deletionOrderedGcPeriodMs);\n+    }\n+  }\n+\n+  public void cancel() {\n+    final Timer t = timer.getAndSet(null);\n+    if (t != null) {\n+      LOG.info(\"cancel\");\n+      t.cancel();\n+    }\n+  }\n+\n+  private void gcDeletedSnapshot(String name) {\n+    final Snapshot.Root deleted;\n+    namesystem.readLock();\n+    try {\n+      deleted = namesystem.getSnapshotManager().chooseDeletedSnapshot();\n+    } finally {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "99dab64faea4426ad8c82eba6b581c984ac98943"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTg0MzA5NA==", "bodyText": "Sure", "url": "https://github.com/apache/hadoop/pull/2165#discussion_r461843094", "createdAt": "2020-07-28T20:02:40Z", "author": {"login": "szetszwo"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotDeletionGc.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.namenode.snapshot;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;\n+import org.apache.hadoop.util.StringUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Timer;\n+import java.util.TimerTask;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager.DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS;\n+import static org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager.DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS_DEFAULT;\n+\n+public class SnapshotDeletionGc {\n+  public static final Logger LOG = LoggerFactory.getLogger(\n+      SnapshotDeletionGc.class);\n+\n+  private final FSNamesystem namesystem;\n+  private final long deletionOrderedGcPeriodMs;\n+  private final AtomicReference<Timer> timer = new AtomicReference<>();\n+\n+  public SnapshotDeletionGc(FSNamesystem namesystem, Configuration conf) {\n+    this.namesystem = namesystem;\n+\n+    this.deletionOrderedGcPeriodMs = conf.getLong(\n+        DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS,\n+        DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS_DEFAULT);\n+    LOG.info(\"{} = {}\", DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS,\n+        deletionOrderedGcPeriodMs);\n+  }\n+\n+  public void schedule() {\n+    if (timer.get() != null) {\n+      return;\n+    }\n+    final Timer t = new Timer(getClass().getSimpleName(), true);\n+    if (timer.compareAndSet(null, t)) {\n+      LOG.info(\"Schedule at fixed rate {}\",\n+          StringUtils.formatTime(deletionOrderedGcPeriodMs));\n+      t.scheduleAtFixedRate(new GcTask(),\n+          deletionOrderedGcPeriodMs, deletionOrderedGcPeriodMs);\n+    }\n+  }\n+\n+  public void cancel() {\n+    final Timer t = timer.getAndSet(null);\n+    if (t != null) {\n+      LOG.info(\"cancel\");\n+      t.cancel();\n+    }\n+  }\n+\n+  private void gcDeletedSnapshot(String name) {\n+    final Snapshot.Root deleted;\n+    namesystem.readLock();\n+    try {\n+      deleted = namesystem.getSnapshotManager().chooseDeletedSnapshot();\n+    } finally {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTgwNzA2MA=="}, "originalCommit": {"oid": "99dab64faea4426ad8c82eba6b581c984ac98943"}, "originalPosition": 78}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4MzA1NzA2OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "isResolved": true, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQxOToxMToyNlrOG4aqjQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxOTowMDozOFrOG5F_MA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTgxMDMxNw==", "bodyText": "I feel all the deletion should happen via one code path, i.e. the background thread. some deletion from the user thread and others from background is error prone.", "url": "https://github.com/apache/hadoop/pull/2165#discussion_r461810317", "createdAt": "2020-07-28T19:11:26Z", "author": {"login": "mukul1987"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "diffHunk": "@@ -382,6 +395,14 @@ public void deleteSnapshot(final INodesInPath iip, final String snapshotName,\n             EnumSet.of(XAttrSetFlag.CREATE, XAttrSetFlag.REPLACE));\n         return;\n       }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "99dab64faea4426ad8c82eba6b581c984ac98943"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTg0NDE5MA==", "bodyText": "@jnp , what is your thought?", "url": "https://github.com/apache/hadoop/pull/2165#discussion_r461844190", "createdAt": "2020-07-28T20:04:57Z", "author": {"login": "szetszwo"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "diffHunk": "@@ -382,6 +395,14 @@ public void deleteSnapshot(final INodesInPath iip, final String snapshotName,\n             EnumSet.of(XAttrSetFlag.CREATE, XAttrSetFlag.REPLACE));\n         return;\n       }\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTgxMDMxNw=="}, "originalCommit": {"oid": "99dab64faea4426ad8c82eba6b581c984ac98943"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTg0NDk5Ng==", "bodyText": "IMO, if the user is delete the first snapshot.  We should do it right away.  Otherwise, there is no way for the user to free up the resource from the snapshots.", "url": "https://github.com/apache/hadoop/pull/2165#discussion_r461844996", "createdAt": "2020-07-28T20:06:29Z", "author": {"login": "szetszwo"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "diffHunk": "@@ -382,6 +395,14 @@ public void deleteSnapshot(final INodesInPath iip, final String snapshotName,\n             EnumSet.of(XAttrSetFlag.CREATE, XAttrSetFlag.REPLACE));\n         return;\n       }\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTgxMDMxNw=="}, "originalCommit": {"oid": "99dab64faea4426ad8c82eba6b581c984ac98943"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTk5NjU2NA==", "bodyText": "I agree with @szetszwo here. If the user follows the order of deletion, it doesn't make sense to delay it further by pushing it to a background thread. Snapshot deletion is still happening by the same code path in SnapshotManager but the triggers are different.\nSnapshotDeletiongc as well as User deleteSnapshotOperation directly calls FsDirSnapshot#deleteSnapshot() which eventually calls SnapshotManger function while Replay of edits will directly call SnapshotManager#deleteSnapshot bypassing all the checks. So, essentially every deletion in the system won't be taking the exact path.", "url": "https://github.com/apache/hadoop/pull/2165#discussion_r461996564", "createdAt": "2020-07-29T02:12:39Z", "author": {"login": "bshashikant"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "diffHunk": "@@ -382,6 +395,14 @@ public void deleteSnapshot(final INodesInPath iip, final String snapshotName,\n             EnumSet.of(XAttrSetFlag.CREATE, XAttrSetFlag.REPLACE));\n         return;\n       }\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTgxMDMxNw=="}, "originalCommit": {"oid": "99dab64faea4426ad8c82eba6b581c984ac98943"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTk5OTYxMw==", "bodyText": "I agree that code paths are similar. However, just wanted to discuss this once. :) The only thing which might not work is the sleep between every snap deletes here. It may happen that the user operation snap delete and the background thread based snap delete are not time spaced correctly. But that is not a big issue anyways.", "url": "https://github.com/apache/hadoop/pull/2165#discussion_r461999613", "createdAt": "2020-07-29T02:24:00Z", "author": {"login": "mukul1987"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "diffHunk": "@@ -382,6 +395,14 @@ public void deleteSnapshot(final INodesInPath iip, final String snapshotName,\n             EnumSet.of(XAttrSetFlag.CREATE, XAttrSetFlag.REPLACE));\n         return;\n       }\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTgxMDMxNw=="}, "originalCommit": {"oid": "99dab64faea4426ad8c82eba6b581c984ac98943"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUyMDExMg==", "bodyText": "Sure, thanks.", "url": "https://github.com/apache/hadoop/pull/2165#discussion_r462520112", "createdAt": "2020-07-29T19:00:38Z", "author": {"login": "szetszwo"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "diffHunk": "@@ -382,6 +395,14 @@ public void deleteSnapshot(final INodesInPath iip, final String snapshotName,\n             EnumSet.of(XAttrSetFlag.CREATE, XAttrSetFlag.REPLACE));\n         return;\n       }\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTgxMDMxNw=="}, "originalCommit": {"oid": "99dab64faea4426ad8c82eba6b581c984ac98943"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NDMxMjcxOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotDeletionGc.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwMjoyNTowOVrOG4mPNQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxODo1OTowN1rOG5F7tw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTk5OTkyNQ==", "bodyText": "If the snapshot delete for the same snapname failed multiple times after the namenode came out of safemode. Should we exit the NN process ?", "url": "https://github.com/apache/hadoop/pull/2165#discussion_r461999925", "createdAt": "2020-07-29T02:25:09Z", "author": {"login": "mukul1987"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotDeletionGc.java", "diffHunk": "@@ -0,0 +1,110 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.namenode.snapshot;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;\n+import org.apache.hadoop.util.StringUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Timer;\n+import java.util.TimerTask;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager.DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS;\n+import static org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager.DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS_DEFAULT;\n+\n+public class SnapshotDeletionGc {\n+  public static final Logger LOG = LoggerFactory.getLogger(\n+      SnapshotDeletionGc.class);\n+\n+  private final FSNamesystem namesystem;\n+  private final long deletionOrderedGcPeriodMs;\n+  private final AtomicReference<Timer> timer = new AtomicReference<>();\n+\n+  public SnapshotDeletionGc(FSNamesystem namesystem, Configuration conf) {\n+    this.namesystem = namesystem;\n+\n+    this.deletionOrderedGcPeriodMs = conf.getLong(\n+        DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS,\n+        DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS_DEFAULT);\n+    LOG.info(\"{} = {}\", DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS,\n+        deletionOrderedGcPeriodMs);\n+  }\n+\n+  public void schedule() {\n+    if (timer.get() != null) {\n+      return;\n+    }\n+    final Timer t = new Timer(getClass().getSimpleName(), true);\n+    if (timer.compareAndSet(null, t)) {\n+      LOG.info(\"Schedule at fixed rate {}\",\n+          StringUtils.formatTime(deletionOrderedGcPeriodMs));\n+      t.scheduleAtFixedRate(new GcTask(),\n+          deletionOrderedGcPeriodMs, deletionOrderedGcPeriodMs);\n+    }\n+  }\n+\n+  public void cancel() {\n+    final Timer t = timer.getAndSet(null);\n+    if (t != null) {\n+      LOG.info(\"cancel\");\n+      t.cancel();\n+    }\n+  }\n+\n+  private void gcDeletedSnapshot(String name) {\n+    final Snapshot.Root deleted;\n+    namesystem.readLock();\n+    try {\n+      deleted = namesystem.getSnapshotManager().chooseDeletedSnapshot();\n+    } catch (Throwable e) {\n+      LOG.error(\"Failed to chooseDeletedSnapshot\", e);\n+      throw e;\n+    } finally {\n+      namesystem.readUnlock();\n+    }\n+    if (deleted == null) {\n+      LOG.trace(\"{}: no snapshots are marked as deleted.\", name);\n+      return;\n+    }\n+\n+    final String snapshotRoot = deleted.getRootFullPathName();\n+    final String snapshotName = deleted.getLocalName();\n+    LOG.info(\"{}: delete snapshot {} from {}\",\n+        name, snapshotName, snapshotRoot);\n+\n+    try {\n+      namesystem.gcDeletedSnapshot(snapshotRoot, snapshotName);\n+    } catch (Throwable e) {\n+      LOG.error(\"Failed to gcDeletedSnapshot \" + deleted.getFullPathName(), e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "032729514c1406f0d329fca1c421bb09f8acf783"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUxOTIyMw==", "bodyText": "The Trash emptier won't kill the NN.  Let's do the same thing here.", "url": "https://github.com/apache/hadoop/pull/2165#discussion_r462519223", "createdAt": "2020-07-29T18:59:07Z", "author": {"login": "szetszwo"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotDeletionGc.java", "diffHunk": "@@ -0,0 +1,110 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.namenode.snapshot;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;\n+import org.apache.hadoop.util.StringUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Timer;\n+import java.util.TimerTask;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager.DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS;\n+import static org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager.DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS_DEFAULT;\n+\n+public class SnapshotDeletionGc {\n+  public static final Logger LOG = LoggerFactory.getLogger(\n+      SnapshotDeletionGc.class);\n+\n+  private final FSNamesystem namesystem;\n+  private final long deletionOrderedGcPeriodMs;\n+  private final AtomicReference<Timer> timer = new AtomicReference<>();\n+\n+  public SnapshotDeletionGc(FSNamesystem namesystem, Configuration conf) {\n+    this.namesystem = namesystem;\n+\n+    this.deletionOrderedGcPeriodMs = conf.getLong(\n+        DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS,\n+        DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS_DEFAULT);\n+    LOG.info(\"{} = {}\", DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED_GC_PERIOD_MS,\n+        deletionOrderedGcPeriodMs);\n+  }\n+\n+  public void schedule() {\n+    if (timer.get() != null) {\n+      return;\n+    }\n+    final Timer t = new Timer(getClass().getSimpleName(), true);\n+    if (timer.compareAndSet(null, t)) {\n+      LOG.info(\"Schedule at fixed rate {}\",\n+          StringUtils.formatTime(deletionOrderedGcPeriodMs));\n+      t.scheduleAtFixedRate(new GcTask(),\n+          deletionOrderedGcPeriodMs, deletionOrderedGcPeriodMs);\n+    }\n+  }\n+\n+  public void cancel() {\n+    final Timer t = timer.getAndSet(null);\n+    if (t != null) {\n+      LOG.info(\"cancel\");\n+      t.cancel();\n+    }\n+  }\n+\n+  private void gcDeletedSnapshot(String name) {\n+    final Snapshot.Root deleted;\n+    namesystem.readLock();\n+    try {\n+      deleted = namesystem.getSnapshotManager().chooseDeletedSnapshot();\n+    } catch (Throwable e) {\n+      LOG.error(\"Failed to chooseDeletedSnapshot\", e);\n+      throw e;\n+    } finally {\n+      namesystem.readUnlock();\n+    }\n+    if (deleted == null) {\n+      LOG.trace(\"{}: no snapshots are marked as deleted.\", name);\n+      return;\n+    }\n+\n+    final String snapshotRoot = deleted.getRootFullPathName();\n+    final String snapshotName = deleted.getLocalName();\n+    LOG.info(\"{}: delete snapshot {} from {}\",\n+        name, snapshotName, snapshotRoot);\n+\n+    try {\n+      namesystem.gcDeletedSnapshot(snapshotRoot, snapshotName);\n+    } catch (Throwable e) {\n+      LOG.error(\"Failed to gcDeletedSnapshot \" + deleted.getFullPathName(), e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTk5OTkyNQ=="}, "originalCommit": {"oid": "032729514c1406f0d329fca1c421bb09f8acf783"}, "originalPosition": 97}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NDMxODc3OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwMjoyOTowMFrOG4mS6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNzowNjoyNlrOG5B4qA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjAwMDg3NQ==", "bodyText": "The check here should be !snapshotRoot.isMarkedAsDeleted() ?", "url": "https://github.com/apache/hadoop/pull/2165#discussion_r462000875", "createdAt": "2020-07-29T02:29:00Z", "author": {"login": "mukul1987"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "diffHunk": "@@ -267,6 +280,20 @@ public INodeDirectory getSnapshottableRoot(final INodesInPath iip)\n     return dir;\n   }\n \n+  public void assertMarkedAsDeleted(INodesInPath iip, String snapshotName)\n+      throws IOException {\n+    final INodeDirectory dir = getSnapshottableRoot(iip);\n+    final Snapshot.Root snapshotRoot = dir.getDirectorySnapshottableFeature()\n+        .getSnapshotByName(dir, snapshotName)\n+        .getRoot();\n+\n+    if (snapshotRoot.isMarkedAsDeleted()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "032729514c1406f0d329fca1c421bb09f8acf783"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ1MjkwNA==", "bodyText": "Good catch. Thanks.", "url": "https://github.com/apache/hadoop/pull/2165#discussion_r462452904", "createdAt": "2020-07-29T17:06:26Z", "author": {"login": "szetszwo"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "diffHunk": "@@ -267,6 +280,20 @@ public INodeDirectory getSnapshottableRoot(final INodesInPath iip)\n     return dir;\n   }\n \n+  public void assertMarkedAsDeleted(INodesInPath iip, String snapshotName)\n+      throws IOException {\n+    final INodeDirectory dir = getSnapshottableRoot(iip);\n+    final Snapshot.Root snapshotRoot = dir.getDirectorySnapshottableFeature()\n+        .getSnapshotByName(dir, snapshotName)\n+        .getRoot();\n+\n+    if (snapshotRoot.isMarkedAsDeleted()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjAwMDg3NQ=="}, "originalCommit": {"oid": "032729514c1406f0d329fca1c421bb09f8acf783"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NDMyMjc4OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwMjozMDo1NFrOG4mVIg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwMjo0Mjo1NFrOG4mg8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjAwMTQ0Mg==", "bodyText": "should we also add an assert inside FSDirSnapshotOp.deleteSnapshot, which is common to both the code paths. That with the ordered delete flag set to true. The snapshot being deleted is the first snapshot in the list ?", "url": "https://github.com/apache/hadoop/pull/2165#discussion_r462001442", "createdAt": "2020-07-29T02:30:54Z", "author": {"login": "mukul1987"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java", "diffHunk": "@@ -7166,6 +7175,30 @@ void deleteSnapshot(String snapshotRoot, String snapshotName,\n     logAuditEvent(true, operationName, rootPath, null, null);\n   }\n \n+  public void gcDeletedSnapshot(String snapshotRoot, String snapshotName)\n+      throws IOException {\n+    final String operationName = \"gcDeletedSnapshot\";\n+    String rootPath = null;\n+    final INode.BlocksMapUpdateInfo blocksToBeDeleted;\n+\n+    checkOperation(OperationCategory.WRITE);\n+    writeLock();\n+    try {\n+      checkOperation(OperationCategory.WRITE);\n+      rootPath = Snapshot.getSnapshotPath(snapshotRoot, snapshotName);\n+      checkNameNodeSafeMode(\"Cannot gcDeletedSnapshot for \" + rootPath);\n+\n+      final long now = Time.now();\n+      final INodesInPath iip = dir.resolvePath(null, snapshotRoot, DirOp.WRITE);\n+      snapshotManager.assertMarkedAsDeleted(iip, snapshotName);\n+      blocksToBeDeleted = FSDirSnapshotOp.deleteSnapshot(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "032729514c1406f0d329fca1c421bb09f8acf783"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjAwNDQ2NA==", "bodyText": "It is already inside SnapshotManager#deleteSnapshot", "url": "https://github.com/apache/hadoop/pull/2165#discussion_r462004464", "createdAt": "2020-07-29T02:42:54Z", "author": {"login": "mukul1987"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java", "diffHunk": "@@ -7166,6 +7175,30 @@ void deleteSnapshot(String snapshotRoot, String snapshotName,\n     logAuditEvent(true, operationName, rootPath, null, null);\n   }\n \n+  public void gcDeletedSnapshot(String snapshotRoot, String snapshotName)\n+      throws IOException {\n+    final String operationName = \"gcDeletedSnapshot\";\n+    String rootPath = null;\n+    final INode.BlocksMapUpdateInfo blocksToBeDeleted;\n+\n+    checkOperation(OperationCategory.WRITE);\n+    writeLock();\n+    try {\n+      checkOperation(OperationCategory.WRITE);\n+      rootPath = Snapshot.getSnapshotPath(snapshotRoot, snapshotName);\n+      checkNameNodeSafeMode(\"Cannot gcDeletedSnapshot for \" + rootPath);\n+\n+      final long now = Time.now();\n+      final INodesInPath iip = dir.resolvePath(null, snapshotRoot, DirOp.WRITE);\n+      snapshotManager.assertMarkedAsDeleted(iip, snapshotName);\n+      blocksToBeDeleted = FSDirSnapshotOp.deleteSnapshot(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjAwMTQ0Mg=="}, "originalCommit": {"oid": "032729514c1406f0d329fca1c421bb09f8acf783"}, "originalPosition": 64}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4OTEzNjkzOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQwNTo0OTo1N1rOG5UBzg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNzoxMDo0NlrOG5sPEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mjc1MDE1OA==", "bodyText": "do we need to do a null check here before calling removeBlocks as its done in other code paths where remove blocks is getting called ?\ne.g. FsNamesystem#deleteSnapshot:\n---------// Breaking the pattern as removing blocks have to happen outside of the // global lock if (blocksToBeDeleted != null) { removeBlocks(blocksToBeDeleted); }", "url": "https://github.com/apache/hadoop/pull/2165#discussion_r462750158", "createdAt": "2020-07-30T05:49:57Z", "author": {"login": "bshashikant"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java", "diffHunk": "@@ -7166,6 +7178,30 @@ void deleteSnapshot(String snapshotRoot, String snapshotName,\n     logAuditEvent(true, operationName, rootPath, null, null);\n   }\n \n+  public void gcDeletedSnapshot(String snapshotRoot, String snapshotName)\n+      throws IOException {\n+    final String operationName = \"gcDeletedSnapshot\";\n+    String rootPath = null;\n+    final INode.BlocksMapUpdateInfo blocksToBeDeleted;\n+\n+    checkOperation(OperationCategory.WRITE);\n+    writeLock();\n+    try {\n+      checkOperation(OperationCategory.WRITE);\n+      rootPath = Snapshot.getSnapshotPath(snapshotRoot, snapshotName);\n+      checkNameNodeSafeMode(\"Cannot gcDeletedSnapshot for \" + rootPath);\n+\n+      final long now = Time.now();\n+      final INodesInPath iip = dir.resolvePath(null, snapshotRoot, DirOp.WRITE);\n+      snapshotManager.assertMarkedAsDeleted(iip, snapshotName);\n+      blocksToBeDeleted = FSDirSnapshotOp.deleteSnapshot(\n+          dir, snapshotManager, iip, snapshotName, now);\n+    } finally {\n+      writeUnlock(operationName, getLockReportInfoSupplier(rootPath));\n+    }\n+    removeBlocks(blocksToBeDeleted);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6fa88dcf07cbc8305ffaf7a07c89ab7e877d3c42"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE0Njc3MQ==", "bodyText": "No.  blocksToBeDeleted is never null.\nThe statement \"if (blocksToBeDeleted != null)\" in FsNamesystem#deleteSnapshot actually generates a warning in IntelIJ saying that \"the condition blocksToBeDeleted != null is always true\".", "url": "https://github.com/apache/hadoop/pull/2165#discussion_r463146771", "createdAt": "2020-07-30T17:10:46Z", "author": {"login": "szetszwo"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java", "diffHunk": "@@ -7166,6 +7178,30 @@ void deleteSnapshot(String snapshotRoot, String snapshotName,\n     logAuditEvent(true, operationName, rootPath, null, null);\n   }\n \n+  public void gcDeletedSnapshot(String snapshotRoot, String snapshotName)\n+      throws IOException {\n+    final String operationName = \"gcDeletedSnapshot\";\n+    String rootPath = null;\n+    final INode.BlocksMapUpdateInfo blocksToBeDeleted;\n+\n+    checkOperation(OperationCategory.WRITE);\n+    writeLock();\n+    try {\n+      checkOperation(OperationCategory.WRITE);\n+      rootPath = Snapshot.getSnapshotPath(snapshotRoot, snapshotName);\n+      checkNameNodeSafeMode(\"Cannot gcDeletedSnapshot for \" + rootPath);\n+\n+      final long now = Time.now();\n+      final INodesInPath iip = dir.resolvePath(null, snapshotRoot, DirOp.WRITE);\n+      snapshotManager.assertMarkedAsDeleted(iip, snapshotName);\n+      blocksToBeDeleted = FSDirSnapshotOp.deleteSnapshot(\n+          dir, snapshotManager, iip, snapshotName, now);\n+    } finally {\n+      writeUnlock(operationName, getLockReportInfoSupplier(rootPath));\n+    }\n+    removeBlocks(blocksToBeDeleted);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mjc1MDE1OA=="}, "originalCommit": {"oid": "6fa88dcf07cbc8305ffaf7a07c89ab7e877d3c42"}, "originalPosition": 72}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3331, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}