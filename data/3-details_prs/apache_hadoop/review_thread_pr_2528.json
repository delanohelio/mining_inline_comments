{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTMzODIzNTA2", "number": 2528, "reviewThreads": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxNzo1OTozMlrOFEGb_A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxNzo1OTozMlrOFEGb_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5ODQ0MDkyOnYy", "diffSide": "LEFT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestUpgradeDomainBlockPlacementPolicy.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxNzo1OTozMlrOIEDyWg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNFQyMTo1OTowNFrOIFsf_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTEyNzI1OA==", "bodyText": "Why not leave this static import?", "url": "https://github.com/apache/hadoop/pull/2528#discussion_r541127258", "createdAt": "2020-12-11T17:59:32Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestUpgradeDomainBlockPlacementPolicy.java", "diffHunk": "@@ -231,32 +229,34 @@ public Boolean get() {\n         } catch (IOException ioe) {\n           return false;\n         }\n-        for(LocatedBlock block : locatedBlocks.getLocatedBlocks()) {\n+        for (LocatedBlock block : locatedBlocks.getLocatedBlocks()) {\n           Set<DatanodeInfo> locs = new HashSet<>();\n           for (DatanodeInfo datanodeInfo : block.getLocations()) {\n-            if (datanodeInfo.getAdminState() ==\n-                DatanodeInfo.AdminStates.NORMAL) {\n+            if (datanodeInfo.getAdminState().equals(\n+                DatanodeInfo.AdminStates.NORMAL)) {\n               locs.add(datanodeInfo);\n             }\n           }\n           for (DatanodeID datanodeID : expectedDatanodeIDs) {\n-            successful = successful && locs.contains(datanodeID);\n+            if (!locs.contains(datanodeID)) {\n+              return false;\n+            }\n           }\n         }\n-        return successful;\n+        return true;\n       }\n-    }, 1000, 60000);\n+    }, 1000, WAIT_TIMEOUT_MS);\n \n     // Verify block placement policy of each block.\n-    LocatedBlocks locatedBlocks;\n-    locatedBlocks =\n+    LocatedBlocks locatedBlocks =\n         cluster.getFileSystem().getClient().getLocatedBlocks(\n             path.toString(), 0, fileSize);\n-    for(LocatedBlock block : locatedBlocks.getLocatedBlocks()) {\n-      BlockPlacementStatus status = cluster.getNamesystem().getBlockManager().\n-          getBlockPlacementPolicy().verifyBlockPlacement(\n-              block.getLocations(), REPLICATION_FACTOR);\n-      assertTrue(status.isPlacementPolicySatisfied());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "def653ae9ddbaf48a0eebc84ee800733fde67aa9"}, "originalPosition": 167}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mjg0Mjg3Ng==", "bodyText": "Sorry @goiri  I did not know that static imports are preferred. I remembered it used to be not favorable because of code readability.\nI will take a note of that for future changes.", "url": "https://github.com/apache/hadoop/pull/2528#discussion_r542842876", "createdAt": "2020-12-14T21:59:04Z", "author": {"login": "amahussein"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestUpgradeDomainBlockPlacementPolicy.java", "diffHunk": "@@ -231,32 +229,34 @@ public Boolean get() {\n         } catch (IOException ioe) {\n           return false;\n         }\n-        for(LocatedBlock block : locatedBlocks.getLocatedBlocks()) {\n+        for (LocatedBlock block : locatedBlocks.getLocatedBlocks()) {\n           Set<DatanodeInfo> locs = new HashSet<>();\n           for (DatanodeInfo datanodeInfo : block.getLocations()) {\n-            if (datanodeInfo.getAdminState() ==\n-                DatanodeInfo.AdminStates.NORMAL) {\n+            if (datanodeInfo.getAdminState().equals(\n+                DatanodeInfo.AdminStates.NORMAL)) {\n               locs.add(datanodeInfo);\n             }\n           }\n           for (DatanodeID datanodeID : expectedDatanodeIDs) {\n-            successful = successful && locs.contains(datanodeID);\n+            if (!locs.contains(datanodeID)) {\n+              return false;\n+            }\n           }\n         }\n-        return successful;\n+        return true;\n       }\n-    }, 1000, 60000);\n+    }, 1000, WAIT_TIMEOUT_MS);\n \n     // Verify block placement policy of each block.\n-    LocatedBlocks locatedBlocks;\n-    locatedBlocks =\n+    LocatedBlocks locatedBlocks =\n         cluster.getFileSystem().getClient().getLocatedBlocks(\n             path.toString(), 0, fileSize);\n-    for(LocatedBlock block : locatedBlocks.getLocatedBlocks()) {\n-      BlockPlacementStatus status = cluster.getNamesystem().getBlockManager().\n-          getBlockPlacementPolicy().verifyBlockPlacement(\n-              block.getLocations(), REPLICATION_FACTOR);\n-      assertTrue(status.isPlacementPolicySatisfied());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTEyNzI1OA=="}, "originalCommit": {"oid": "def653ae9ddbaf48a0eebc84ee800733fde67aa9"}, "originalPosition": 167}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3180, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}