{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDgyNTI3ODE2", "number": 2288, "title": "HDFS-15548. Allow configuring DISK/ARCHIVE storage types on same device mount", "bodyText": "NOTICE\nPlease create an issue in ASF JIRA before opening a pull request,\nand you need to set the title of the pull request which starts with\nthe corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.)\nFor more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute", "createdAt": "2020-09-09T05:53:27Z", "url": "https://github.com/apache/hadoop/pull/2288", "merged": true, "mergeCommit": {"oid": "9a9ab5b48e3c0db412ed86c946d7f728a649a4a0"}, "closed": true, "closedAt": "2020-11-09T23:06:17Z", "author": {"login": "LeonGao91"}, "timelineItems": {"totalCount": 31, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdHFbptAH2gAyNDgyNTI3ODE2OmY2NDY3ZDI4ZDc3YzJkMmIxNDFjMzRhZTA4YTgxMDc1ZGYwZDVhNDc=", "endCursor": "Y3Vyc29yOnYyOpPPAAABda5J9aAH2gAyNDgyNTI3ODE2OjJkZDUyMDk4NzYyZGU3MGUxZDkwNTlkYmZlNWM2MDNmNDhiZWVmMTQ=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "f6467d28d77c2d2b141c34ae08a81075df0d5a47", "author": {"user": {"login": "LeonGao91", "name": "LeonGao"}}, "url": "https://github.com/apache/hadoop/commit/f6467d28d77c2d2b141c34ae08a81075df0d5a47", "committedDate": "2020-09-09T05:34:26Z", "message": "Add poc code"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c95742f2809328574313ba1fac4ecd8a388a8ed2", "author": {"user": {"login": "LeonGao91", "name": "LeonGao"}}, "url": "https://github.com/apache/hadoop/commit/c95742f2809328574313ba1fac4ecd8a388a8ed2", "committedDate": "2020-09-09T05:34:46Z", "message": "Add unit teest"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e73ac9d4ae68625328d01a9adfcb3235932e7064", "author": {"user": {"login": "LeonGao91", "name": "LeonGao"}}, "url": "https://github.com/apache/hadoop/commit/e73ac9d4ae68625328d01a9adfcb3235932e7064", "committedDate": "2020-09-09T05:35:51Z", "message": "Remove logic to choose same disk"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "74a4a820466ad2a8e69f3d8623aae973ff1011fa", "author": {"user": {"login": "LeonGao91", "name": "LeonGao"}}, "url": "https://github.com/apache/hadoop/commit/74a4a820466ad2a8e69f3d8623aae973ff1011fa", "committedDate": "2020-09-09T05:35:57Z", "message": "Fix UT"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "551a9082319f7a39d4438546ed8f64efd2b6ec58", "author": {"user": {"login": "LeonGao91", "name": "LeonGao"}}, "url": "https://github.com/apache/hadoop/commit/551a9082319f7a39d4438546ed8f64efd2b6ec58", "committedDate": "2020-09-09T05:36:05Z", "message": "Resolve comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7d61d224e731f0dbed5b43dc2ecc69bccc0f0dd8", "author": {"user": {"login": "LeonGao91", "name": "LeonGao"}}, "url": "https://github.com/apache/hadoop/commit/7d61d224e731f0dbed5b43dc2ecc69bccc0f0dd8", "committedDate": "2020-09-09T05:36:10Z", "message": "Final"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "55031bee97c99c37d4066b4114279a1a265b2b8d", "author": {"user": {"login": "LeonGao91", "name": "LeonGao"}}, "url": "https://github.com/apache/hadoop/commit/55031bee97c99c37d4066b4114279a1a265b2b8d", "committedDate": "2020-09-10T05:17:19Z", "message": "Trigger Build"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "db0fb346b46fd410c3cf062f61db6ee06f890cec", "author": {"user": {"login": "LeonGao91", "name": "LeonGao"}}, "url": "https://github.com/apache/hadoop/commit/db0fb346b46fd410c3cf062f61db6ee06f890cec", "committedDate": "2020-09-11T22:48:28Z", "message": "Checkstyle"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkwNTU0Njk2", "url": "https://github.com/apache/hadoop/pull/2288#pullrequestreview-490554696", "createdAt": "2020-09-17T12:47:35Z", "commit": {"oid": "db0fb346b46fd410c3cf062f61db6ee06f890cec"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xN1QxMjo0NzozNVrOHTgXIA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xN1QxMjo1ODozMVrOHTg0aA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDIxNTIwMA==", "bodyText": "reservedForArchival here is same as this.reservedForArchival when init?", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r490215200", "createdAt": "2020-09-17T12:47:35Z", "author": {"login": "Hexiaoqiao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java", "diffHunk": "@@ -412,16 +435,31 @@ long getBlockPoolUsed(String bpid) throws IOException {\n    */\n   @VisibleForTesting\n   public long getCapacity() {\n+    long capacity;\n     if (configuredCapacity < 0L) {\n       long remaining;\n       if (cachedCapacity > 0L) {\n         remaining = cachedCapacity - getReserved();\n       } else {\n         remaining = usage.getCapacity() - getReserved();\n       }\n-      return Math.max(remaining, 0L);\n+      capacity = Math.max(remaining, 0L);\n+    } else {\n+      capacity = configuredCapacity;\n+    }\n+\n+    if (enableSameDiskArchival) {\n+      double reservedForArchival = conf.getDouble(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "db0fb346b46fd410c3cf062f61db6ee06f890cec"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDIxOTgwMQ==", "bodyText": "what about using storageID replace device? IMO both of them are in order to index single volume, right?", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r490219801", "createdAt": "2020-09-17T12:54:18Z", "author": {"login": "Hexiaoqiao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java", "diffHunk": "@@ -134,6 +134,9 @@\n   private final FileIoProvider fileIoProvider;\n   private final DataNodeVolumeMetrics metrics;\n   private URI baseURI;\n+  private boolean enableSameDiskArchival;\n+  private final String device;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "db0fb346b46fd410c3cf062f61db6ee06f890cec"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDIyMjY5Ng==", "bodyText": "Why reservedForArchive has to less than 1 here, IIUC it means that this is ARCHIVE device when reservedForArchive set to 1. Right?", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r490222696", "createdAt": "2020-09-17T12:58:31Z", "author": {"login": "Hexiaoqiao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java", "diffHunk": "@@ -190,6 +193,26 @@\n     }\n     this.conf = conf;\n     this.fileIoProvider = fileIoProvider;\n+    this.enableSameDiskArchival =\n+        conf.getBoolean(DFSConfigKeys.DFS_DATANODE_ALLOW_SAME_DISK_TIERING,\n+            DFSConfigKeys.DFS_DATANODE_ALLOW_SAME_DISK_TIERING_DEFAULT);\n+    if (enableSameDiskArchival) {\n+      this.device = usage.getMount();\n+      reservedForArchive = conf.getDouble(\n+          DFSConfigKeys.DFS_DATANODE_RESERVE_FOR_ARCHIVE_PERCENTAGE,\n+          DFSConfigKeys.DFS_DATANODE_RESERVE_FOR_ARCHIVE_PERCENTAGE_DEFAULT);\n+      if (reservedForArchive >= 1) {\n+        FsDatasetImpl.LOG.warn(\"Value of reserve-for-archival is >= 100% for \"\n+            + currentDir + \". Setting it to 99%.\");\n+        reservedForArchive = 0.99;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "db0fb346b46fd410c3cf062f61db6ee06f890cec"}, "originalPosition": 25}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "db2129ee10be2e30cb950b64e1c711e79e61ce60", "author": {"user": {"login": "LeonGao91", "name": "LeonGao"}}, "url": "https://github.com/apache/hadoop/commit/db2129ee10be2e30cb950b64e1c711e79e61ce60", "committedDate": "2020-09-18T03:29:03Z", "message": "Resolve comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0441fdfb9d909aaa72a9663d61be27991c87f993", "author": {"user": {"login": "LeonGao91", "name": "LeonGao"}}, "url": "https://github.com/apache/hadoop/commit/0441fdfb9d909aaa72a9663d61be27991c87f993", "committedDate": "2020-09-18T03:30:48Z", "message": "Resolve comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9d089234659e7a26f362eb8afab774cbb90e49d8", "author": {"user": {"login": "LeonGao91", "name": "LeonGao"}}, "url": "https://github.com/apache/hadoop/commit/9d089234659e7a26f362eb8afab774cbb90e49d8", "committedDate": "2020-09-21T03:20:31Z", "message": "Add check to avoid misconfig of volumes with same storage type to be created on same mount"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk1MjY4OTk2", "url": "https://github.com/apache/hadoop/pull/2288#pullrequestreview-495268996", "createdAt": "2020-09-24T06:56:00Z", "commit": {"oid": "9d089234659e7a26f362eb8afab774cbb90e49d8"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwNjo1NjowMFrOHXMOAw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwNzowODo1MVrOHXMnJA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA3OTQ5MQ==", "bodyText": "enableSameDiskTiering here vs enableSameDiskArchival at FsVolumeImpl,  we should unified variable name.", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r494079491", "createdAt": "2020-09-24T06:56:00Z", "author": {"login": "Hexiaoqiao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java", "diffHunk": "@@ -62,9 +64,14 @@\n   private final VolumeChoosingPolicy<FsVolumeImpl> blockChooser;\n   private final BlockScanner blockScanner;\n \n+  private boolean enableSameDiskTiering;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9d089234659e7a26f362eb8afab774cbb90e49d8"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA4MTAwOA==", "bodyText": "reservedForArchive try to define reserve for archive percentage. If there are heterogeneous disks located one node, do we need config them separate\uff1f", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r494081008", "createdAt": "2020-09-24T06:59:00Z", "author": {"login": "Hexiaoqiao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java", "diffHunk": "@@ -190,6 +193,26 @@\n     }\n     this.conf = conf;\n     this.fileIoProvider = fileIoProvider;\n+    this.enableSameDiskArchival =\n+        conf.getBoolean(DFSConfigKeys.DFS_DATANODE_ALLOW_SAME_DISK_TIERING,\n+            DFSConfigKeys.DFS_DATANODE_ALLOW_SAME_DISK_TIERING_DEFAULT);\n+    if (enableSameDiskArchival) {\n+      this.mount = usage.getMount();\n+      reservedForArchive = conf.getDouble(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9d089234659e7a26f362eb8afab774cbb90e49d8"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA4NTQyMw==", "bodyText": "The return value seems not expected as annotation says if enable this feature.\n\nthe capacity of the file system excluding space reserved for non-HDFS.\n\nIMO, the part for ARCHIVE should also be calculated. It seems be not differentiated by NameNode for DISK or ARCHIVE per storage of DataNode. Please correct if something wrong.", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r494085423", "createdAt": "2020-09-24T07:07:45Z", "author": {"login": "Hexiaoqiao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java", "diffHunk": "@@ -412,16 +435,28 @@ long getBlockPoolUsed(String bpid) throws IOException {\n    */\n   @VisibleForTesting\n   public long getCapacity() {\n+    long capacity;\n     if (configuredCapacity < 0L) {\n       long remaining;\n       if (cachedCapacity > 0L) {\n         remaining = cachedCapacity - getReserved();\n       } else {\n         remaining = usage.getCapacity() - getReserved();\n       }\n-      return Math.max(remaining, 0L);\n+      capacity = Math.max(remaining, 0L);\n+    } else {\n+      capacity = configuredCapacity;\n+    }\n+\n+    if (enableSameDiskArchival) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9d089234659e7a26f362eb8afab774cbb90e49d8"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA4NTkyNA==", "bodyText": "same confused as the last comment.", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r494085924", "createdAt": "2020-09-24T07:08:51Z", "author": {"login": "Hexiaoqiao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java", "diffHunk": "@@ -452,7 +487,33 @@ public long getAvailable() throws IOException {\n   }\n \n   long getActualNonDfsUsed() throws IOException {\n-    return usage.getUsed() - getDfsUsed();\n+    // DISK and ARCHIVAL on same disk", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9d089234659e7a26f362eb8afab774cbb90e49d8"}, "originalPosition": 73}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b2708f167022e9a020415cb62f1745ea0c55f574", "author": {"user": {"login": "LeonGao91", "name": "LeonGao"}}, "url": "https://github.com/apache/hadoop/commit/b2708f167022e9a020415cb62f1745ea0c55f574", "committedDate": "2020-09-24T22:20:54Z", "message": "Fix naming"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cc9e62318e3e5e881e27917bda31f9e967f08153", "author": {"user": {"login": "LeonGao91", "name": "LeonGao"}}, "url": "https://github.com/apache/hadoop/commit/cc9e62318e3e5e881e27917bda31f9e967f08153", "committedDate": "2020-09-24T22:28:59Z", "message": "Add javadoc"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9f2250efe675f8911bbdc039a9d3bc6dd59f9850", "author": {"user": {"login": "LeonGao91", "name": "LeonGao"}}, "url": "https://github.com/apache/hadoop/commit/9f2250efe675f8911bbdc039a9d3bc6dd59f9850", "committedDate": "2020-10-26T06:06:53Z", "message": "Resolve comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1e91d78f1d84b050bbdad52a04cf7206924f4d40", "author": {"user": {"login": "LeonGao91", "name": "LeonGao"}}, "url": "https://github.com/apache/hadoop/commit/1e91d78f1d84b050bbdad52a04cf7206924f4d40", "committedDate": "2020-10-27T01:09:58Z", "message": "Update"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIwMTA4OTU5", "url": "https://github.com/apache/hadoop/pull/2288#pullrequestreview-520108959", "createdAt": "2020-10-29T21:40:38Z", "commit": {"oid": "1e91d78f1d84b050bbdad52a04cf7206924f4d40"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQyMTo0MDozOFrOHqvu0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQwMToyODoxNFrOHq0vZg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDU4NDI3Mw==", "bodyText": "Take another look at the patch, I think it may be better to have the percentage as a tag added to the configuration \"dfs.datanode.data.dir\", just following the storage type tag. In this way on the same datanode we can have different percentage settings for different mount points. What do you think?", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r514584273", "createdAt": "2020-10-29T21:40:38Z", "author": {"login": "Jing9"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java", "diffHunk": "@@ -1503,6 +1503,20 @@\n   public static final boolean DFS_PROTECTED_SUBDIRECTORIES_ENABLE_DEFAULT =\n       false;\n \n+  public static final String DFS_DATANODE_ALLOW_SAME_DISK_TIERING =\n+      \"dfs.datanode.same-disk-tiering.enabled\";\n+  public static final boolean DFS_DATANODE_ALLOW_SAME_DISK_TIERING_DEFAULT =\n+      false;\n+\n+  // HDFS-15548 to allow DISK/ARCHIVE configured on the same disk mount.\n+  // Beware that capacity usage might be >100% if there are already\n+  // data blocks exist and the configured ratio is small, which will\n+  // prevent the volume from taking new blocks until capacity is balanced out.\n+  public static final String DFS_DATANODE_RESERVE_FOR_ARCHIVE_PERCENTAGE =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1e91d78f1d84b050bbdad52a04cf7206924f4d40"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDY2NjM0Mg==", "bodyText": "what if we have a mount with one single volume? Following the current implementation we may assign an unnecessary capacity ratio to it. We only need to calculate and assign the ratio for volumes sharing the same mount with others.", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r514666342", "createdAt": "2020-10-30T01:28:14Z", "author": {"login": "Jing9"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MountVolumeMap.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.datanode.fsdataset.impl;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.StorageType;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeReference;\n+\n+import java.nio.channels.ClosedChannelException;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n+\n+/**\n+ * MountVolumeMap contains information of the relationship\n+ * between underlying filesystem mount and datanode volumes.\n+ *\n+ * This is useful when configuring block tiering on same disk mount (HDFS-15548)\n+ * For now,\n+ * we don't configure multiple volumes with same storage type on a mount.\n+ */\n+@InterfaceAudience.Private\n+class MountVolumeMap {\n+  private ConcurrentMap<String, Map<StorageType, VolumeInfo>>\n+      mountVolumeMapping;\n+  private double reservedForArchive;\n+\n+  MountVolumeMap(Configuration conf) {\n+    mountVolumeMapping = new ConcurrentHashMap<>();\n+    reservedForArchive = conf.getDouble(\n+        DFSConfigKeys.DFS_DATANODE_RESERVE_FOR_ARCHIVE_PERCENTAGE,\n+        DFSConfigKeys.DFS_DATANODE_RESERVE_FOR_ARCHIVE_PERCENTAGE_DEFAULT);\n+    if (reservedForArchive > 1) {\n+      FsDatasetImpl.LOG.warn(\"Value of reserve-for-archival is > 100%.\" +\n+          \" Setting it to 100%.\");\n+      reservedForArchive = 1;\n+    }\n+  }\n+\n+  FsVolumeReference getVolumeRefByMountAndStorageType(String mount,\n+      StorageType storageType) {\n+    if (mountVolumeMapping != null\n+        && mountVolumeMapping.containsKey(mount)) {\n+      try {\n+        VolumeInfo volumeInfo = mountVolumeMapping\n+            .get(mount).getOrDefault(storageType, null);\n+        if (volumeInfo != null) {\n+          return volumeInfo.getFsVolume().obtainReference();\n+        }\n+      } catch (ClosedChannelException e) {\n+        FsDatasetImpl.LOG.warn(\"Volume closed when getting volume\" +\n+            \" by mount and storage type: \"\n+            + mount + \", \" + storageType);\n+      }\n+    }\n+    return null;\n+  }\n+\n+  /**\n+   * Return configured capacity ratio. Otherwise return 1 as default\n+   */\n+  double getCapacityRatioByMountAndStorageType(String mount,\n+      StorageType storageType) {\n+    if (mountVolumeMapping != null\n+        && mountVolumeMapping.containsKey(mount)) {\n+      return mountVolumeMapping\n+          .get(mount).getOrDefault(storageType, null).getCapacityRatio();\n+    }\n+    return 1;\n+  }\n+\n+  void addVolume(FsVolumeImpl volume) {\n+    String mount = volume.getMount();\n+    if (!mount.isEmpty()) {\n+      Map<StorageType, VolumeInfo> storageTypeMap =\n+          mountVolumeMapping\n+              .getOrDefault(mount, new ConcurrentHashMap<>());\n+      if (storageTypeMap.containsKey(volume.getStorageType())) {\n+        FsDatasetImpl.LOG.error(\"Found storage type already exist.\" +\n+            \" Skipping for now. Please check disk configuration\");\n+      } else {\n+        VolumeInfo volumeInfo = new VolumeInfo(volume, 1);\n+        if (volume.getStorageType() == StorageType.ARCHIVE) {\n+          volumeInfo.setCapacityRatio(reservedForArchive);\n+        } else if (volume.getStorageType() == StorageType.DISK) {\n+          volumeInfo.setCapacityRatio(1 - reservedForArchive);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1e91d78f1d84b050bbdad52a04cf7206924f4d40"}, "originalPosition": 103}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "46d01b757bd257afb9feb751504038eba27decb0", "author": {"user": {"login": "LeonGao91", "name": "LeonGao"}}, "url": "https://github.com/apache/hadoop/commit/46d01b757bd257afb9feb751504038eba27decb0", "committedDate": "2020-10-30T06:33:16Z", "message": "Ignore allocation if volume is not co-located."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8ef9e0c4d257411c12ab972aa45f4a602e62fbab", "author": {"user": {"login": "LeonGao91", "name": "LeonGao"}}, "url": "https://github.com/apache/hadoop/commit/8ef9e0c4d257411c12ab972aa45f4a602e62fbab", "committedDate": "2020-10-30T06:36:51Z", "message": "Fix typo"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "95aedecf77efa369c417f264aed89daa85616757", "author": {"user": {"login": "LeonGao91", "name": "LeonGao"}}, "url": "https://github.com/apache/hadoop/commit/95aedecf77efa369c417f264aed89daa85616757", "committedDate": "2020-10-30T17:45:22Z", "message": "Resolve javadoc"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b177dc6d64820bd2b4f9733a4f4ebb7e38d8c569", "author": {"user": {"login": "LeonGao91", "name": "LeonGao"}}, "url": "https://github.com/apache/hadoop/commit/b177dc6d64820bd2b4f9733a4f4ebb7e38d8c569", "committedDate": "2020-10-31T07:35:15Z", "message": "Refactor mount volume map"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "57c52d6d059d29013615642ecf9ad90cddb35c70", "author": {"user": {"login": "LeonGao91", "name": "LeonGao"}}, "url": "https://github.com/apache/hadoop/commit/57c52d6d059d29013615642ecf9ad90cddb35c70", "committedDate": "2020-10-31T19:10:20Z", "message": "Fix checkstyle"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIyMTIyMjMw", "url": "https://github.com/apache/hadoop/pull/2288#pullrequestreview-522122230", "createdAt": "2020-11-03T00:19:29Z", "commit": {"oid": "57c52d6d059d29013615642ecf9ad90cddb35c70"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QwMDoxOToyOVrOHsb2Kw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QwMDozMzo1MFrOHscPvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM1NTYyNw==", "bodyText": "Do we need to throw an exception here? Silently dropping the volume may not be a good option.", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r516355627", "createdAt": "2020-11-03T00:19:29Z", "author": {"login": "Jing9"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MountVolumeInfo.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.datanode.fsdataset.impl;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.StorageType;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeReference;\n+\n+import java.nio.channels.ClosedChannelException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n+\n+/**\n+ * MountVolumeInfo is a wrapper of\n+ * detailed volume information for MountVolumeMap.\n+ */\n+@InterfaceAudience.Private\n+class MountVolumeInfo {\n+  private ConcurrentMap<StorageType, FsVolumeImpl>\n+      storageTypeVolumeMap;\n+  private double reservedForArchiveDefault;\n+\n+  MountVolumeInfo(Configuration conf) {\n+    storageTypeVolumeMap = new ConcurrentHashMap<>();\n+    reservedForArchiveDefault = conf.getDouble(\n+        DFSConfigKeys.DFS_DATANODE_RESERVE_FOR_ARCHIVE_DEFAULT_PERCENTAGE,\n+        DFSConfigKeys\n+            .DFS_DATANODE_RESERVE_FOR_ARCHIVE_DEFAULT_PERCENTAGE_DEFAULT);\n+    if (reservedForArchiveDefault > 1) {\n+      FsDatasetImpl.LOG.warn(\"Value of reserve-for-archival is > 100%.\" +\n+          \" Setting it to 100%.\");\n+      reservedForArchiveDefault = 1;\n+    }\n+  }\n+\n+  FsVolumeReference getVolumeRef(StorageType storageType) {\n+    try {\n+      FsVolumeImpl volumeImpl = storageTypeVolumeMap\n+          .getOrDefault(storageType, null);\n+      if (volumeImpl != null) {\n+        return volumeImpl.obtainReference();\n+      }\n+    } catch (ClosedChannelException e) {\n+      FsDatasetImpl.LOG.warn(\"Volume closed when getting volume\" +\n+          \" by storage type: \" + storageType);\n+    }\n+    return null;\n+  }\n+\n+  /**\n+   * Return configured capacity ratio.\n+   * If the volume is the only one on the mount,\n+   * return 1 to avoid unnecessary allocation.\n+   */\n+  double getCapacityRatio(StorageType storageType) {\n+    if (storageTypeVolumeMap.containsKey(storageType)\n+        && storageTypeVolumeMap.size() > 1) {\n+      if (storageType == StorageType.ARCHIVE) {\n+        return reservedForArchiveDefault;\n+      } else if (storageType == StorageType.DISK) {\n+        return 1 - reservedForArchiveDefault;\n+      }\n+    }\n+    return 1;\n+  }\n+\n+  void addVolume(FsVolumeImpl volume) {\n+    if (storageTypeVolumeMap.containsKey(volume.getStorageType())) {\n+      FsDatasetImpl.LOG.error(\"Found storage type already exist.\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "57c52d6d059d29013615642ecf9ad90cddb35c70"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM1NjY4NQ==", "bodyText": "we can add a TODO here explaining we plan to support different ratios per mount point.", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r516356685", "createdAt": "2020-11-03T00:21:47Z", "author": {"login": "Jing9"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MountVolumeInfo.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.datanode.fsdataset.impl;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.StorageType;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeReference;\n+\n+import java.nio.channels.ClosedChannelException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n+\n+/**\n+ * MountVolumeInfo is a wrapper of\n+ * detailed volume information for MountVolumeMap.\n+ */\n+@InterfaceAudience.Private\n+class MountVolumeInfo {\n+  private ConcurrentMap<StorageType, FsVolumeImpl>\n+      storageTypeVolumeMap;\n+  private double reservedForArchiveDefault;\n+\n+  MountVolumeInfo(Configuration conf) {\n+    storageTypeVolumeMap = new ConcurrentHashMap<>();\n+    reservedForArchiveDefault = conf.getDouble(\n+        DFSConfigKeys.DFS_DATANODE_RESERVE_FOR_ARCHIVE_DEFAULT_PERCENTAGE,\n+        DFSConfigKeys\n+            .DFS_DATANODE_RESERVE_FOR_ARCHIVE_DEFAULT_PERCENTAGE_DEFAULT);\n+    if (reservedForArchiveDefault > 1) {\n+      FsDatasetImpl.LOG.warn(\"Value of reserve-for-archival is > 100%.\" +\n+          \" Setting it to 100%.\");\n+      reservedForArchiveDefault = 1;\n+    }\n+  }\n+\n+  FsVolumeReference getVolumeRef(StorageType storageType) {\n+    try {\n+      FsVolumeImpl volumeImpl = storageTypeVolumeMap\n+          .getOrDefault(storageType, null);\n+      if (volumeImpl != null) {\n+        return volumeImpl.obtainReference();\n+      }\n+    } catch (ClosedChannelException e) {\n+      FsDatasetImpl.LOG.warn(\"Volume closed when getting volume\" +\n+          \" by storage type: \" + storageType);\n+    }\n+    return null;\n+  }\n+\n+  /**\n+   * Return configured capacity ratio.\n+   * If the volume is the only one on the mount,\n+   * return 1 to avoid unnecessary allocation.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "57c52d6d059d29013615642ecf9ad90cddb35c70"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM1NzI1MQ==", "bodyText": "This field can be declared as \"final\"", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r516357251", "createdAt": "2020-11-03T00:23:04Z", "author": {"login": "Jing9"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MountVolumeInfo.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.datanode.fsdataset.impl;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.StorageType;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeReference;\n+\n+import java.nio.channels.ClosedChannelException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n+\n+/**\n+ * MountVolumeInfo is a wrapper of\n+ * detailed volume information for MountVolumeMap.\n+ */\n+@InterfaceAudience.Private\n+class MountVolumeInfo {\n+  private ConcurrentMap<StorageType, FsVolumeImpl>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "57c52d6d059d29013615642ecf9ad90cddb35c70"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM1NzMzNQ==", "bodyText": "This field can be declared as \"final\"", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r516357335", "createdAt": "2020-11-03T00:23:19Z", "author": {"login": "Jing9"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MountVolumeMap.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.datanode.fsdataset.impl;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.StorageType;\n+import org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeReference;\n+\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n+\n+/**\n+ * MountVolumeMap contains information of the relationship\n+ * between underlying filesystem mount and datanode volumes.\n+ *\n+ * This is useful when configuring block tiering on same disk mount\n+ * (HDFS-15548). For now,\n+ * we don't configure multiple volumes with same storage type on one mount.\n+ */\n+@InterfaceAudience.Private\n+class MountVolumeMap {\n+  private ConcurrentMap<String, MountVolumeInfo>\n+      mountVolumeMapping;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "57c52d6d059d29013615642ecf9ad90cddb35c70"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM1OTc1Mg==", "bodyText": "let's also add a check \" && usage != null\"", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r516359752", "createdAt": "2020-11-03T00:28:34Z", "author": {"login": "Jing9"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java", "diffHunk": "@@ -190,6 +193,18 @@\n     }\n     this.conf = conf;\n     this.fileIoProvider = fileIoProvider;\n+    this.enableSameDiskTiering =\n+        conf.getBoolean(DFSConfigKeys.DFS_DATANODE_ALLOW_SAME_DISK_TIERING,\n+            DFSConfigKeys.DFS_DATANODE_ALLOW_SAME_DISK_TIERING_DEFAULT);\n+    if (enableSameDiskTiering) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "57c52d6d059d29013615642ecf9ad90cddb35c70"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM2MjAxNw==", "bodyText": "These two fields can be declared as \"final\"", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r516362017", "createdAt": "2020-11-03T00:33:31Z", "author": {"login": "Jing9"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java", "diffHunk": "@@ -62,9 +63,13 @@\n   private final VolumeChoosingPolicy<FsVolumeImpl> blockChooser;\n   private final BlockScanner blockScanner;\n \n+  private boolean enableSameDiskTiering;\n+  private MountVolumeMap mountVolumeMap;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "57c52d6d059d29013615642ecf9ad90cddb35c70"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM2MjE3NQ==", "bodyText": "maybe consider putting this check into a method", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r516362175", "createdAt": "2020-11-03T00:33:50Z", "author": {"login": "Jing9"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java", "diffHunk": "@@ -291,6 +304,11 @@ public String toString() {\n   void addVolume(FsVolumeReference ref) {\n     FsVolumeImpl volume = (FsVolumeImpl) ref.getVolume();\n     volumes.add(volume);\n+    if (enableSameDiskTiering &&", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "57c52d6d059d29013615642ecf9ad90cddb35c70"}, "originalPosition": 42}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4381dec4fe9832b3f5b0b2a894fbb136ca871959", "author": {"user": {"login": "LeonGao91", "name": "LeonGao"}}, "url": "https://github.com/apache/hadoop/commit/4381dec4fe9832b3f5b0b2a894fbb136ca871959", "committedDate": "2020-11-03T02:22:19Z", "message": "Resolve comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "33f000680fcfae66c6f8d94551a8b1bab763acbe", "author": {"user": {"login": "LeonGao91", "name": "LeonGao"}}, "url": "https://github.com/apache/hadoop/commit/33f000680fcfae66c6f8d94551a8b1bab763acbe", "committedDate": "2020-11-03T04:30:49Z", "message": "Fix checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "21441d0c09783508e0fd6bd664131de5bc09b840", "author": {"user": {"login": "LeonGao91", "name": "LeonGao"}}, "url": "https://github.com/apache/hadoop/commit/21441d0c09783508e0fd6bd664131de5bc09b840", "committedDate": "2020-11-03T22:47:10Z", "message": "Resolve comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e456c2b5246514780ed57a6d73d349607d1e08e8", "author": {"user": {"login": "LeonGao91", "name": "LeonGao"}}, "url": "https://github.com/apache/hadoop/commit/e456c2b5246514780ed57a6d73d349607d1e08e8", "committedDate": "2020-11-04T04:45:55Z", "message": "Trigger jenkins"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "97f7c03238635f3bc04b13ef92ff0d275cfdada0", "author": {"user": {"login": "LeonGao91", "name": "LeonGao"}}, "url": "https://github.com/apache/hadoop/commit/97f7c03238635f3bc04b13ef92ff0d275cfdada0", "committedDate": "2020-11-05T01:00:42Z", "message": "Trigger Build"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI1OTY5MzY3", "url": "https://github.com/apache/hadoop/pull/2288#pullrequestreview-525969367", "createdAt": "2020-11-09T07:10:00Z", "commit": {"oid": "97f7c03238635f3bc04b13ef92ff0d275cfdada0"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwNzoxMDowMFrOHvhMMw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwNzoyNjo1MVrOHvh1GA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTU4ODkxNQ==", "bodyText": "check if it set a negative value?", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r519588915", "createdAt": "2020-11-09T07:10:00Z", "author": {"login": "Hexiaoqiao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MountVolumeInfo.java", "diffHunk": "@@ -0,0 +1,108 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.datanode.fsdataset.impl;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.StorageType;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeReference;\n+\n+import java.nio.channels.ClosedChannelException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n+\n+/**\n+ * MountVolumeInfo is a wrapper of\n+ * detailed volume information for MountVolumeMap.\n+ */\n+@InterfaceAudience.Private\n+class MountVolumeInfo {\n+  private final ConcurrentMap<StorageType, FsVolumeImpl>\n+      storageTypeVolumeMap;\n+  private double reservedForArchiveDefault;\n+\n+  MountVolumeInfo(Configuration conf) {\n+    storageTypeVolumeMap = new ConcurrentHashMap<>();\n+    reservedForArchiveDefault = conf.getDouble(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97f7c03238635f3bc04b13ef92ff0d275cfdada0"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTU5OTM4NA==", "bodyText": "codestyle: redundant empty line.", "url": "https://github.com/apache/hadoop/pull/2288#discussion_r519599384", "createdAt": "2020-11-09T07:26:51Z", "author": {"login": "Hexiaoqiao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNamenodeCapacityReport.java", "diffHunk": "@@ -185,6 +186,59 @@ public void testVolumeSize() throws Exception {\n           (namesystem.getCapacityUsed() + namesystem.getCapacityRemaining()\n               + namesystem.getNonDfsUsedSpace() + fileCount * fs\n               .getDefaultBlockSize()) - configCapacity < 1 * 1024);\n+    } finally {\n+      if (cluster != null) {\n+        cluster.shutdown();\n+      }\n+    }\n+  }\n+\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97f7c03238635f3bc04b13ef92ff0d275cfdada0"}, "originalPosition": 19}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2dd52098762de70e1d9059dbfe5c603f48beef14", "author": {"user": {"login": "LeonGao91", "name": "LeonGao"}}, "url": "https://github.com/apache/hadoop/commit/2dd52098762de70e1d9059dbfe5c603f48beef14", "committedDate": "2020-11-09T18:34:44Z", "message": "Resolve comments"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3630, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}