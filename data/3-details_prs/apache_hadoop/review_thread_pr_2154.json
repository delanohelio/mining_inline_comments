{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDUzNTc5ODMz", "number": 2154, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQxOTo1NDo1MlrOEQWXIQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxMDoxOTowM1rOEQjuwA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1NTc5MDQxOnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsInputStreamStatistics.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQxOTo1NDo1MlrOG0dD4w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQxOTo1NDo1MlrOG0dD4w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY1NTI2Nw==", "bodyText": "nice explanation", "url": "https://github.com/apache/hadoop/pull/2154#discussion_r457655267", "createdAt": "2020-07-20T19:54:52Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsInputStreamStatistics.java", "diffHunk": "@@ -285,6 +291,98 @@ public void testWithNullStreamStatistics() throws IOException {\n     }\n   }\n \n+  /**\n+   * Testing readAhead counters in AbfsInputStream with 30 seconds timeout.\n+   */\n+  @Test(timeout = TIMEOUT_30_SECONDS)\n+  public void testReadAheadCounters() throws IOException {\n+    describe(\"Test to check correct values for readAhead counters in \"\n+        + \"AbfsInputStream\");\n+\n+    AzureBlobFileSystem fs = getFileSystem();\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    Path readAheadCountersPath = path(getMethodName());\n+\n+    /*\n+     * Setting the block size for readAhead as 4KB.\n+     */\n+    abfss.getAbfsConfiguration().setReadBufferSize(CUSTOM_BLOCK_BUFFER_SIZE);\n+\n+    AbfsOutputStream out = null;\n+    AbfsInputStream in = null;\n+\n+    try {\n+\n+      /*\n+       * Creating a file of 1MB size.\n+       */\n+      out = createAbfsOutputStreamWithFlushEnabled(fs, readAheadCountersPath);\n+      out.write(defBuffer);\n+      out.close();\n+\n+      in = abfss.openFileForRead(readAheadCountersPath, fs.getFsStatistics());\n+\n+      /*\n+       * Reading 1KB after each i * KB positions. Hence the reads are from 0\n+       * to 1KB, 1KB to 2KB, and so on.. for 5 operations.\n+       */\n+      for (int i = 0; i < 5; i++) {\n+        in.seek(ONE_KB * i);\n+        in.read(defBuffer, ONE_KB * i, ONE_KB);\n+      }\n+      AbfsInputStreamStatisticsImpl stats =\n+          (AbfsInputStreamStatisticsImpl) in.getStreamStatistics();\n+\n+      /*\n+       * Since, readAhead is done in background threads. Sometimes, the\n+       * threads aren't finished in the background and could result in\n+       * inaccurate results. So, we wait till we have the accurate values\n+       * with a limit of 30 seconds as that's when the test times out.\n+       *\n+       */\n+      while (stats.getRemoteBytesRead() < CUSTOM_READ_AHEAD_BUFFER_SIZE\n+          || stats.getReadAheadBytesRead() < CUSTOM_BLOCK_BUFFER_SIZE) {\n+        Thread.sleep(THREAD_SLEEP_10_SECONDS);\n+      }\n+\n+      /*\n+       * Verifying the counter values of readAheadBytesRead and remoteBytesRead.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ba2869a0866f40d918e817b7e6119c2f65c71acb"}, "originalPosition": 79}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1NTc5MjkzOnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsInputStreamStatistics.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQxOTo1NTozNlrOG0dFWg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwNToxNTowMFrOG0oaRA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY1NTY0Mg==", "bodyText": "Try using AssertJ.assertThat here, it lets you declare the specific \"isGreaterThan\" assertion; it's describedAs() does the string formatting too.", "url": "https://github.com/apache/hadoop/pull/2154#discussion_r457655642", "createdAt": "2020-07-20T19:55:36Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsInputStreamStatistics.java", "diffHunk": "@@ -285,6 +291,98 @@ public void testWithNullStreamStatistics() throws IOException {\n     }\n   }\n \n+  /**\n+   * Testing readAhead counters in AbfsInputStream with 30 seconds timeout.\n+   */\n+  @Test(timeout = TIMEOUT_30_SECONDS)\n+  public void testReadAheadCounters() throws IOException {\n+    describe(\"Test to check correct values for readAhead counters in \"\n+        + \"AbfsInputStream\");\n+\n+    AzureBlobFileSystem fs = getFileSystem();\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    Path readAheadCountersPath = path(getMethodName());\n+\n+    /*\n+     * Setting the block size for readAhead as 4KB.\n+     */\n+    abfss.getAbfsConfiguration().setReadBufferSize(CUSTOM_BLOCK_BUFFER_SIZE);\n+\n+    AbfsOutputStream out = null;\n+    AbfsInputStream in = null;\n+\n+    try {\n+\n+      /*\n+       * Creating a file of 1MB size.\n+       */\n+      out = createAbfsOutputStreamWithFlushEnabled(fs, readAheadCountersPath);\n+      out.write(defBuffer);\n+      out.close();\n+\n+      in = abfss.openFileForRead(readAheadCountersPath, fs.getFsStatistics());\n+\n+      /*\n+       * Reading 1KB after each i * KB positions. Hence the reads are from 0\n+       * to 1KB, 1KB to 2KB, and so on.. for 5 operations.\n+       */\n+      for (int i = 0; i < 5; i++) {\n+        in.seek(ONE_KB * i);\n+        in.read(defBuffer, ONE_KB * i, ONE_KB);\n+      }\n+      AbfsInputStreamStatisticsImpl stats =\n+          (AbfsInputStreamStatisticsImpl) in.getStreamStatistics();\n+\n+      /*\n+       * Since, readAhead is done in background threads. Sometimes, the\n+       * threads aren't finished in the background and could result in\n+       * inaccurate results. So, we wait till we have the accurate values\n+       * with a limit of 30 seconds as that's when the test times out.\n+       *\n+       */\n+      while (stats.getRemoteBytesRead() < CUSTOM_READ_AHEAD_BUFFER_SIZE\n+          || stats.getReadAheadBytesRead() < CUSTOM_BLOCK_BUFFER_SIZE) {\n+        Thread.sleep(THREAD_SLEEP_10_SECONDS);\n+      }\n+\n+      /*\n+       * Verifying the counter values of readAheadBytesRead and remoteBytesRead.\n+       *\n+       * readAheadBytesRead : Since, we read 1KBs 5 times, that means we go\n+       * from 0 to 5KB in the file. The bufferSize is set to 4KB, and since\n+       * we have 8 blocks of readAhead buffer. We would have 8 blocks of 4KB\n+       * buffer. Our read is till 5KB, hence readAhead would ideally read 2\n+       * blocks of 4KB which is equal to 8KB. But, sometimes to get more than\n+       * one block from readAhead buffer we might have to wait for background\n+       * threads to fill the buffer and hence we might do remote read which\n+       * would be faster. Therefore, readAheadBytesRead would be equal to or\n+       * greater than 4KB.\n+       *\n+       * remoteBytesRead : Since, the bufferSize is set to 4KB and the number\n+       * of blocks or readAheadQueueDepth is equal to 8. We would read 8 * 4\n+       * KB buffer on the first read, which is equal to 32KB. But, if we are not\n+       * able to read some bytes that were in the buffer after doing\n+       * readAhead, we might use remote read again. Thus, the bytes read\n+       * remotely could also be greater than 32Kb.\n+       *\n+       */\n+      assertTrue(String.format(\"actual value of %d is not greater than or \"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ba2869a0866f40d918e817b7e6119c2f65c71acb"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg0MTIyMA==", "bodyText": "It would be good to add it in this patch. Thanks for the tip.", "url": "https://github.com/apache/hadoop/pull/2154#discussion_r457841220", "createdAt": "2020-07-21T05:15:00Z", "author": {"login": "mehakmeet"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsInputStreamStatistics.java", "diffHunk": "@@ -285,6 +291,98 @@ public void testWithNullStreamStatistics() throws IOException {\n     }\n   }\n \n+  /**\n+   * Testing readAhead counters in AbfsInputStream with 30 seconds timeout.\n+   */\n+  @Test(timeout = TIMEOUT_30_SECONDS)\n+  public void testReadAheadCounters() throws IOException {\n+    describe(\"Test to check correct values for readAhead counters in \"\n+        + \"AbfsInputStream\");\n+\n+    AzureBlobFileSystem fs = getFileSystem();\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    Path readAheadCountersPath = path(getMethodName());\n+\n+    /*\n+     * Setting the block size for readAhead as 4KB.\n+     */\n+    abfss.getAbfsConfiguration().setReadBufferSize(CUSTOM_BLOCK_BUFFER_SIZE);\n+\n+    AbfsOutputStream out = null;\n+    AbfsInputStream in = null;\n+\n+    try {\n+\n+      /*\n+       * Creating a file of 1MB size.\n+       */\n+      out = createAbfsOutputStreamWithFlushEnabled(fs, readAheadCountersPath);\n+      out.write(defBuffer);\n+      out.close();\n+\n+      in = abfss.openFileForRead(readAheadCountersPath, fs.getFsStatistics());\n+\n+      /*\n+       * Reading 1KB after each i * KB positions. Hence the reads are from 0\n+       * to 1KB, 1KB to 2KB, and so on.. for 5 operations.\n+       */\n+      for (int i = 0; i < 5; i++) {\n+        in.seek(ONE_KB * i);\n+        in.read(defBuffer, ONE_KB * i, ONE_KB);\n+      }\n+      AbfsInputStreamStatisticsImpl stats =\n+          (AbfsInputStreamStatisticsImpl) in.getStreamStatistics();\n+\n+      /*\n+       * Since, readAhead is done in background threads. Sometimes, the\n+       * threads aren't finished in the background and could result in\n+       * inaccurate results. So, we wait till we have the accurate values\n+       * with a limit of 30 seconds as that's when the test times out.\n+       *\n+       */\n+      while (stats.getRemoteBytesRead() < CUSTOM_READ_AHEAD_BUFFER_SIZE\n+          || stats.getReadAheadBytesRead() < CUSTOM_BLOCK_BUFFER_SIZE) {\n+        Thread.sleep(THREAD_SLEEP_10_SECONDS);\n+      }\n+\n+      /*\n+       * Verifying the counter values of readAheadBytesRead and remoteBytesRead.\n+       *\n+       * readAheadBytesRead : Since, we read 1KBs 5 times, that means we go\n+       * from 0 to 5KB in the file. The bufferSize is set to 4KB, and since\n+       * we have 8 blocks of readAhead buffer. We would have 8 blocks of 4KB\n+       * buffer. Our read is till 5KB, hence readAhead would ideally read 2\n+       * blocks of 4KB which is equal to 8KB. But, sometimes to get more than\n+       * one block from readAhead buffer we might have to wait for background\n+       * threads to fill the buffer and hence we might do remote read which\n+       * would be faster. Therefore, readAheadBytesRead would be equal to or\n+       * greater than 4KB.\n+       *\n+       * remoteBytesRead : Since, the bufferSize is set to 4KB and the number\n+       * of blocks or readAheadQueueDepth is equal to 8. We would read 8 * 4\n+       * KB buffer on the first read, which is equal to 32KB. But, if we are not\n+       * able to read some bytes that were in the buffer after doing\n+       * readAhead, we might use remote read again. Thus, the bytes read\n+       * remotely could also be greater than 32Kb.\n+       *\n+       */\n+      assertTrue(String.format(\"actual value of %d is not greater than or \"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY1NTY0Mg=="}, "originalCommit": {"oid": "ba2869a0866f40d918e817b7e6119c2f65c71acb"}, "originalPosition": 99}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1Nzk4MDgwOnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsInputStreamStatistics.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxMDoxOTowM1rOG0xjxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxMDoxOTowM1rOG0xjxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzk5MTEwOA==", "bodyText": "can't we just throw this? If not, at least use LOG", "url": "https://github.com/apache/hadoop/pull/2154#discussion_r457991108", "createdAt": "2020-07-21T10:19:03Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsInputStreamStatistics.java", "diffHunk": "@@ -285,6 +292,96 @@ public void testWithNullStreamStatistics() throws IOException {\n     }\n   }\n \n+  /**\n+   * Testing readAhead counters in AbfsInputStream with 30 seconds timeout.\n+   */\n+  @Test(timeout = TIMEOUT_30_SECONDS)\n+  public void testReadAheadCounters() throws IOException {\n+    describe(\"Test to check correct values for readAhead counters in \"\n+        + \"AbfsInputStream\");\n+\n+    AzureBlobFileSystem fs = getFileSystem();\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    Path readAheadCountersPath = path(getMethodName());\n+\n+    /*\n+     * Setting the block size for readAhead as 4KB.\n+     */\n+    abfss.getAbfsConfiguration().setReadBufferSize(CUSTOM_BLOCK_BUFFER_SIZE);\n+\n+    AbfsOutputStream out = null;\n+    AbfsInputStream in = null;\n+\n+    try {\n+\n+      /*\n+       * Creating a file of 1MB size.\n+       */\n+      out = createAbfsOutputStreamWithFlushEnabled(fs, readAheadCountersPath);\n+      out.write(defBuffer);\n+      out.close();\n+\n+      in = abfss.openFileForRead(readAheadCountersPath, fs.getFsStatistics());\n+\n+      /*\n+       * Reading 1KB after each i * KB positions. Hence the reads are from 0\n+       * to 1KB, 1KB to 2KB, and so on.. for 5 operations.\n+       */\n+      for (int i = 0; i < 5; i++) {\n+        in.seek(ONE_KB * i);\n+        in.read(defBuffer, ONE_KB * i, ONE_KB);\n+      }\n+      AbfsInputStreamStatisticsImpl stats =\n+          (AbfsInputStreamStatisticsImpl) in.getStreamStatistics();\n+\n+      /*\n+       * Since, readAhead is done in background threads. Sometimes, the\n+       * threads aren't finished in the background and could result in\n+       * inaccurate results. So, we wait till we have the accurate values\n+       * with a limit of 30 seconds as that's when the test times out.\n+       *\n+       */\n+      while (stats.getRemoteBytesRead() < CUSTOM_READ_AHEAD_BUFFER_SIZE\n+          || stats.getReadAheadBytesRead() < CUSTOM_BLOCK_BUFFER_SIZE) {\n+        Thread.sleep(THREAD_SLEEP_10_SECONDS);\n+      }\n+\n+      /*\n+       * Verifying the counter values of readAheadBytesRead and remoteBytesRead.\n+       *\n+       * readAheadBytesRead : Since, we read 1KBs 5 times, that means we go\n+       * from 0 to 5KB in the file. The bufferSize is set to 4KB, and since\n+       * we have 8 blocks of readAhead buffer. We would have 8 blocks of 4KB\n+       * buffer. Our read is till 5KB, hence readAhead would ideally read 2\n+       * blocks of 4KB which is equal to 8KB. But, sometimes to get more than\n+       * one block from readAhead buffer we might have to wait for background\n+       * threads to fill the buffer and hence we might do remote read which\n+       * would be faster. Therefore, readAheadBytesRead would be equal to or\n+       * greater than 4KB.\n+       *\n+       * remoteBytesRead : Since, the bufferSize is set to 4KB and the number\n+       * of blocks or readAheadQueueDepth is equal to 8. We would read 8 * 4\n+       * KB buffer on the first read, which is equal to 32KB. But, if we are not\n+       * able to read some bytes that were in the buffer after doing\n+       * readAhead, we might use remote read again. Thus, the bytes read\n+       * remotely could also be greater than 32Kb.\n+       *\n+       */\n+      Assertions.assertThat(stats.getReadAheadBytesRead()).describedAs(\n+          \"Mismatch in readAheadBytesRead counter value\")\n+          .isGreaterThanOrEqualTo(CUSTOM_BLOCK_BUFFER_SIZE);\n+\n+      Assertions.assertThat(stats.getRemoteBytesRead()).describedAs(\n+          \"Mismatch in remoteBytesRead counter value\")\n+          .isGreaterThanOrEqualTo(CUSTOM_READ_AHEAD_BUFFER_SIZE);\n+\n+    } catch (InterruptedException e) {\n+      e.printStackTrace();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2f3763d5fb11897063b1571cb546734d7b6d8c85"}, "originalPosition": 116}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3517, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}