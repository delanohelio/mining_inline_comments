{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDc3MDQ0NTc5", "number": 2265, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQxNTozMjoxOVrOEe9jOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNFQwNDo0ODo0NlrOEgPSXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAwOTAxMTc4OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQxNTozMjoxOVrOHK8MuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQwMjoxMTo1MVrOHLPyBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTIzNDEwNA==", "bodyText": "1\u3001Is it necessary to add log in Local dead nodes?\n2\u3001if add debug log in DFSInputStream#addToLocalDeadNodes, do you need add log in DFSInputStream#removeFromLocalDeadNodes?", "url": "https://github.com/apache/hadoop/pull/2265#discussion_r481234104", "createdAt": "2020-09-01T15:32:19Z", "author": {"login": "leosunli"}, "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java", "diffHunk": "@@ -181,6 +181,8 @@ private boolean isPeriodicRefreshEnabled() {\n   private byte[] oneByteBuf; // used for 'int read()'\n \n   protected void addToLocalDeadNodes(DatanodeInfo dnInfo) {\n+    DFSClient.LOG.debug(\"Add {} to local dead nodes, previously was {}\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec130e9ae0975010049695892addae02f26b36f0"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTU1NDk0OQ==", "bodyText": "From the statistics of the production environment, the log output of local deadnode information is too few, so that it is troublesome to find the problem reason\nThe reason to use the DEBUG log level is to separate from the INFO log level of the detector, and the impact is small.\nRemove logs can be added at the same time(I think both are OK). It was not added before considering that the local dead node removal method will almost never be called, unless the detector is turned on and there is already log output there.", "url": "https://github.com/apache/hadoop/pull/2265#discussion_r481554949", "createdAt": "2020-09-02T02:11:51Z", "author": {"login": "imbajin"}, "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java", "diffHunk": "@@ -181,6 +181,8 @@ private boolean isPeriodicRefreshEnabled() {\n   private byte[] oneByteBuf; // used for 'int read()'\n \n   protected void addToLocalDeadNodes(DatanodeInfo dnInfo) {\n+    DFSClient.LOG.debug(\"Add {} to local dead nodes, previously was {}\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTIzNDEwNA=="}, "originalCommit": {"oid": "ec130e9ae0975010049695892addae02f26b36f0"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyMjM5NDUzOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DeadNodeDetector.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNFQwNDo0MzoxMFrOHM_V3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNFQxMzozNDo1MlrOHNNw9A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzM4Mjc1MQ==", "bodyText": "One case: when a lot of stale relicas, will the log flood?", "url": "https://github.com/apache/hadoop/pull/2265#discussion_r483382751", "createdAt": "2020-09-04T04:43:10Z", "author": {"login": "leosunli"}, "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DeadNodeDetector.java", "diffHunk": "@@ -475,6 +475,7 @@ public synchronized void addNodeToDetect(DFSInputStream dfsInputStream,\n       datanodeInfos.add(datanodeInfo);\n     }\n \n+    LOG.warn(\"Add datanode {} to suspectAndDeadNodes\", datanodeInfo);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec130e9ae0975010049695892addae02f26b36f0"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzYxOTA2MA==", "bodyText": "use DEBUG as default now", "url": "https://github.com/apache/hadoop/pull/2265#discussion_r483619060", "createdAt": "2020-09-04T13:34:52Z", "author": {"login": "imbajin"}, "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DeadNodeDetector.java", "diffHunk": "@@ -475,6 +475,7 @@ public synchronized void addNodeToDetect(DFSInputStream dfsInputStream,\n       datanodeInfos.add(datanodeInfo);\n     }\n \n+    LOG.warn(\"Add datanode {} to suspectAndDeadNodes\", datanodeInfo);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzM4Mjc1MQ=="}, "originalCommit": {"oid": "ec130e9ae0975010049695892addae02f26b36f0"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyMjQwMzUwOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DeadNodeDetector.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNFQwNDo0ODo0NlrOHM_a7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQwMjo0NToyNVrOHOK52g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzM4NDA0Nw==", "bodyText": "when a lot of stale relicas,  it should have many supsect nodes  but not dead nodes.\nThese nodes all will print this log.\nWhat is the purpose of printing this log?\nThe client can access normally the suspect node.", "url": "https://github.com/apache/hadoop/pull/2265#discussion_r483384047", "createdAt": "2020-09-04T04:48:46Z", "author": {"login": "leosunli"}, "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DeadNodeDetector.java", "diffHunk": "@@ -396,13 +395,13 @@ private void probeCallBack(Probe probe, boolean success) {\n             probe.getDatanodeInfo());\n         removeDeadNode(probe.getDatanodeInfo());\n       } else if (probe.getType() == ProbeType.CHECK_SUSPECT) {\n-        LOG.debug(\"Remove the node out from suspect node list: {}.\",\n+        LOG.info(\"Remove the node out from suspect node list: {}.\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec130e9ae0975010049695892addae02f26b36f0"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYyMDc2Mg==", "bodyText": "ditto", "url": "https://github.com/apache/hadoop/pull/2265#discussion_r484620762", "createdAt": "2020-09-08T02:45:25Z", "author": {"login": "imbajin"}, "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DeadNodeDetector.java", "diffHunk": "@@ -396,13 +395,13 @@ private void probeCallBack(Probe probe, boolean success) {\n             probe.getDatanodeInfo());\n         removeDeadNode(probe.getDatanodeInfo());\n       } else if (probe.getType() == ProbeType.CHECK_SUSPECT) {\n-        LOG.debug(\"Remove the node out from suspect node list: {}.\",\n+        LOG.info(\"Remove the node out from suspect node list: {}.\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzM4NDA0Nw=="}, "originalCommit": {"oid": "ec130e9ae0975010049695892addae02f26b36f0"}, "originalPosition": 25}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3259, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}