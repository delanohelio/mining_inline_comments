{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDgzNTI1NDg2", "number": 2296, "reviewThreads": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxOToxOTowOVrOEiQlNQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQxNzozOToxNVrOEjPyVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0MzU4NzA5OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxOToxOTowOVrOHQCclA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxOToxOTowOVrOHQCclA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU3OTM0OA==", "bodyText": "Add javadoc", "url": "https://github.com/apache/hadoop/pull/2296#discussion_r486579348", "createdAt": "2020-09-10T19:19:09Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java", "diffHunk": "@@ -508,6 +508,10 @@ FSNamesystem getFSNamesystem() {\n     return namesystem;\n   }\n \n+  public boolean isImageLoaded() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c786c270907011ecaef879a98d931d275ab9a15b"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0MzU4NzczOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxOToxOToyMVrOHQCc7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxOToxOToyMVrOHQCc7g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU3OTQzOA==", "bodyText": "javadoc", "url": "https://github.com/apache/hadoop/pull/2296#discussion_r486579438", "createdAt": "2020-09-10T19:19:21Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "diffHunk": "@@ -368,6 +368,13 @@ void assertFirstSnapshot(INodeDirectory dir,\n     }\n   }\n \n+  boolean captureOpenFiles() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c786c270907011ecaef879a98d931d275ab9a15b"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0MzU4ODkzOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxOToxOTozN1rOHQCdmw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxOToxOTozN1rOHQCdmw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU3OTYxMQ==", "bodyText": "VisibleForTesting", "url": "https://github.com/apache/hadoop/pull/2296#discussion_r486579611", "createdAt": "2020-09-10T19:19:37Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "diffHunk": "@@ -368,6 +368,13 @@ void assertFirstSnapshot(INodeDirectory dir,\n     }\n   }\n \n+  boolean captureOpenFiles() {\n+    return captureOpenFiles;\n+  }\n+\n+  int getMaxSnapshotLimit() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c786c270907011ecaef879a98d931d275ab9a15b"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0MzU5MDM2OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotManager.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxOToxOTo1OFrOHQCeZw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxOToxOTo1OFrOHQCeZw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU3OTgxNQ==", "bodyText": "Use LambdaTestUtils#intercept", "url": "https://github.com/apache/hadoop/pull/2296#discussion_r486579815", "createdAt": "2020-09-10T19:19:58Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotManager.java", "diffHunk": "@@ -133,4 +137,68 @@ public void testValidateSnapshotIDWidth() throws Exception {\n         getMaxSnapshotID() < Snapshot.CURRENT_STATE_ID);\n   }\n \n+  @Test\n+  public void SnapshotLimitOnRestart() throws Exception {\n+    final Configuration conf = new Configuration();\n+    final Path snapshottableDir\n+        = new Path(\"/\" + getClass().getSimpleName());\n+    int numSnapshots = 5;\n+    conf.setInt(DFSConfigKeys.\n+            DFS_NAMENODE_SNAPSHOT_MAX_LIMIT, numSnapshots);\n+    conf.setInt(DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_FILESYSTEM_LIMIT,\n+        numSnapshots * 2);\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).\n+        numDataNodes(0).build();\n+    cluster.waitActive();\n+    DistributedFileSystem hdfs = cluster.getFileSystem();\n+    hdfs.mkdirs(snapshottableDir);\n+    hdfs.allowSnapshot(snapshottableDir);\n+    int i = 0;\n+    for (; i < numSnapshots; i++) {\n+      hdfs.createSnapshot(snapshottableDir, \"s\" + i);\n+    }\n+    try {\n+      hdfs.createSnapshot(snapshottableDir, \"s\" + i);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c786c270907011ecaef879a98d931d275ab9a15b"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0MzU5MTIxOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotManager.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxOToyMDoxNVrOHQCe5Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxOToyMDoxNVrOHQCe5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU3OTk0MQ==", "bodyText": "Extract the getSnapshotManager()", "url": "https://github.com/apache/hadoop/pull/2296#discussion_r486579941", "createdAt": "2020-09-10T19:20:15Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotManager.java", "diffHunk": "@@ -133,4 +137,68 @@ public void testValidateSnapshotIDWidth() throws Exception {\n         getMaxSnapshotID() < Snapshot.CURRENT_STATE_ID);\n   }\n \n+  @Test\n+  public void SnapshotLimitOnRestart() throws Exception {\n+    final Configuration conf = new Configuration();\n+    final Path snapshottableDir\n+        = new Path(\"/\" + getClass().getSimpleName());\n+    int numSnapshots = 5;\n+    conf.setInt(DFSConfigKeys.\n+            DFS_NAMENODE_SNAPSHOT_MAX_LIMIT, numSnapshots);\n+    conf.setInt(DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_FILESYSTEM_LIMIT,\n+        numSnapshots * 2);\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).\n+        numDataNodes(0).build();\n+    cluster.waitActive();\n+    DistributedFileSystem hdfs = cluster.getFileSystem();\n+    hdfs.mkdirs(snapshottableDir);\n+    hdfs.allowSnapshot(snapshottableDir);\n+    int i = 0;\n+    for (; i < numSnapshots; i++) {\n+      hdfs.createSnapshot(snapshottableDir, \"s\" + i);\n+    }\n+    try {\n+      hdfs.createSnapshot(snapshottableDir, \"s\" + i);\n+      Assert.fail(\"Expected SnapshotException not thrown\");\n+    } catch (SnapshotException se) {\n+      Assert.assertTrue(\n+          StringUtils.toLowerCase(se.getMessage()).contains(\n+              \"max snapshot limit\"));\n+    }\n+\n+    // now change max snapshot directory limit to 2 and restart namenode\n+    cluster.getNameNode().getConf().setInt(DFSConfigKeys.\n+        DFS_NAMENODE_SNAPSHOT_MAX_LIMIT, 2);\n+    cluster.restartNameNodes();\n+\n+    // make sure edits of all previous 5 create snapshots are replayed\n+    Assert.assertEquals(numSnapshots, cluster.getNamesystem().", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c786c270907011ecaef879a98d931d275ab9a15b"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0MzU5MjI4OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotManager.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxOToyMDozNlrOHQCflg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxOToyMDozNlrOHQCflg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU4MDExOA==", "bodyText": "LambdaTestUtils", "url": "https://github.com/apache/hadoop/pull/2296#discussion_r486580118", "createdAt": "2020-09-10T19:20:36Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotManager.java", "diffHunk": "@@ -133,4 +137,68 @@ public void testValidateSnapshotIDWidth() throws Exception {\n         getMaxSnapshotID() < Snapshot.CURRENT_STATE_ID);\n   }\n \n+  @Test\n+  public void SnapshotLimitOnRestart() throws Exception {\n+    final Configuration conf = new Configuration();\n+    final Path snapshottableDir\n+        = new Path(\"/\" + getClass().getSimpleName());\n+    int numSnapshots = 5;\n+    conf.setInt(DFSConfigKeys.\n+            DFS_NAMENODE_SNAPSHOT_MAX_LIMIT, numSnapshots);\n+    conf.setInt(DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_FILESYSTEM_LIMIT,\n+        numSnapshots * 2);\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).\n+        numDataNodes(0).build();\n+    cluster.waitActive();\n+    DistributedFileSystem hdfs = cluster.getFileSystem();\n+    hdfs.mkdirs(snapshottableDir);\n+    hdfs.allowSnapshot(snapshottableDir);\n+    int i = 0;\n+    for (; i < numSnapshots; i++) {\n+      hdfs.createSnapshot(snapshottableDir, \"s\" + i);\n+    }\n+    try {\n+      hdfs.createSnapshot(snapshottableDir, \"s\" + i);\n+      Assert.fail(\"Expected SnapshotException not thrown\");\n+    } catch (SnapshotException se) {\n+      Assert.assertTrue(\n+          StringUtils.toLowerCase(se.getMessage()).contains(\n+              \"max snapshot limit\"));\n+    }\n+\n+    // now change max snapshot directory limit to 2 and restart namenode\n+    cluster.getNameNode().getConf().setInt(DFSConfigKeys.\n+        DFS_NAMENODE_SNAPSHOT_MAX_LIMIT, 2);\n+    cluster.restartNameNodes();\n+\n+    // make sure edits of all previous 5 create snapshots are replayed\n+    Assert.assertEquals(numSnapshots, cluster.getNamesystem().\n+        getSnapshotManager().getNumSnapshots());\n+\n+    // make sure namenode has the new snapshot limit configured as 2\n+    Assert.assertEquals(2,\n+        cluster.getNamesystem().getSnapshotManager().getMaxSnapshotLimit());\n+\n+    // Any new snapshot creation should still fail\n+    try {\n+      hdfs.createSnapshot(snapshottableDir, \"s\" + i);\n+      Assert.fail(\"Expected SnapshotException not thrown\");\n+    } catch (SnapshotException se) {\n+      Assert.assertTrue(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c786c270907011ecaef879a98d931d275ab9a15b"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0MzY4MjkwOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxOTo0OToxNVrOHQDXrg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxOTo0OToxNVrOHQDXrg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU5NDQ3OA==", "bodyText": "I suggest to add limit type to the error message as below.\ndiff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/DirectorySnapshottableFeature.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/DirectorySnapshottableFeature.java\nindex 266c0a71241..7a47ab4000d 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/DirectorySnapshottableFeature.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/DirectorySnapshottableFeature.java\n@@ -190,8 +190,7 @@ public Snapshot addSnapshot(INodeDirectory snapshotRoot,\n           + n + \" snapshot(s) and the snapshot quota is \"\n           + snapshotQuota);\n     }\n-    snapshotManager.checkSnapshotLimit(snapshotManager.\n-        getMaxSnapshotLimit(), n);\n+    snapshotManager.checkPerDirectorySnapshotLimit(n);\n     final Snapshot s = new Snapshot(id, name, snapshotRoot);\n     final byte[] nameBytes = s.getRoot().getLocalNameBytes();\n     final int i = searchSnapshot(nameBytes);\ndiff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java\nindex 0a2e18c3dc3..7c482074486 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java\n@@ -455,7 +455,7 @@ public String createSnapshot(final LeaseManager leaseManager,\n           \"snapshot IDs and ID rollover is not supported.\");\n     }\n     int n = numSnapshots.get();\n-    checkSnapshotLimit(maxSnapshotFSLimit, n);\n+    checkFileSystemSnapshotLimit(n);\n     srcRoot.addSnapshot(this, snapshotName, leaseManager, mtime);\n       \n     //create success, update id\n@@ -464,12 +464,19 @@ public String createSnapshot(final LeaseManager leaseManager,\n     return Snapshot.getSnapshotPath(snapshotRoot, snapshotName);\n   }\n \n-  void checkSnapshotLimit(int limit, int numSnapshots)\n-      throws SnapshotException {\n+  void checkFileSystemSnapshotLimit(int n) throws SnapshotException {\n+    checkSnapshotLimit(maxSnapshotFSLimit, n, \"file system\");\n+  }\n+\n+  void checkPerDirectorySnapshotLimit(int n) throws SnapshotException {\n+    checkSnapshotLimit(maxSnapshotLimit, n, \"per directory\");\n+  }\n+\n+  private void checkSnapshotLimit(int limit, int numSnapshots,\n+      String type) throws SnapshotException {\n     if (numSnapshots >= limit) {\n-      String msg = \"there are already \" + (numSnapshots + 1)\n-          + \" snapshot(s) and the max snapshot limit is \"\n-          + limit;\n+      String msg = \"There are already \" + (numSnapshots + 1)\n+          + \" snapshot(s) and the \" + type + \" snapshot limit is \" + limit;\n       if (fsdir.isImageLoaded()) {\n         // We have reached the maximum snapshot limit\n         throw new SnapshotException(", "url": "https://github.com/apache/hadoop/pull/2296#discussion_r486594478", "createdAt": "2020-09-10T19:49:15Z", "author": {"login": "szetszwo"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "diffHunk": "@@ -448,22 +455,31 @@ public String createSnapshot(final LeaseManager leaseManager,\n           \"snapshot IDs and ID rollover is not supported.\");\n     }\n     int n = numSnapshots.get();\n-    if (n >= maxSnapshotFSLimit) {\n-      // We have reached the maximum snapshot limit\n-      throw new SnapshotException(\n-          \"Failed to create snapshot: there are already \" + (n + 1)\n-              + \" snapshot(s) and the max snapshot limit is \"\n-              + maxSnapshotFSLimit);\n-    }\n-\n-    srcRoot.addSnapshot(snapshotCounter, snapshotName, leaseManager,\n-        this.captureOpenFiles, maxSnapshotLimit, mtime);\n+    checkSnapshotLimit(maxSnapshotFSLimit, n);\n+    srcRoot.addSnapshot(this, snapshotName, leaseManager, mtime);\n       \n     //create success, update id\n     snapshotCounter++;\n     numSnapshots.getAndIncrement();\n     return Snapshot.getSnapshotPath(snapshotRoot, snapshotName);\n   }\n+\n+  void checkSnapshotLimit(int limit, int numSnapshots)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c786c270907011ecaef879a98d931d275ab9a15b"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0Nzc4NzQ4OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotManager.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMVQxOToxMjowMlrOHQqprw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMVQxOToxMjowMlrOHQqprw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzIzODA2Mw==", "bodyText": "Who uses this \"i\" later? in for loop def should be good.", "url": "https://github.com/apache/hadoop/pull/2296#discussion_r487238063", "createdAt": "2020-09-11T19:12:02Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotManager.java", "diffHunk": "@@ -133,4 +137,58 @@ public void testValidateSnapshotIDWidth() throws Exception {\n         getMaxSnapshotID() < Snapshot.CURRENT_STATE_ID);\n   }\n \n+  @Test\n+  public void testSnapshotLimitOnRestart() throws Exception {\n+    final Configuration conf = new Configuration();\n+    final Path snapshottableDir\n+        = new Path(\"/\" + getClass().getSimpleName());\n+    int numSnapshots = 5;\n+    conf.setInt(DFSConfigKeys.\n+            DFS_NAMENODE_SNAPSHOT_MAX_LIMIT, numSnapshots);\n+    conf.setInt(DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_FILESYSTEM_LIMIT,\n+        numSnapshots * 2);\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).\n+        numDataNodes(0).build();\n+    cluster.waitActive();\n+    DistributedFileSystem hdfs = cluster.getFileSystem();\n+    hdfs.mkdirs(snapshottableDir);\n+    hdfs.allowSnapshot(snapshottableDir);\n+    int i = 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca83922fe5e11535aee53c07495cd03113144669"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0Nzc4ODEwOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotManager.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMVQxOToxMjoxMlrOHQqqAA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMVQxOToxMjoxMlrOHQqqAA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzIzODE0NA==", "bodyText": "1 line?", "url": "https://github.com/apache/hadoop/pull/2296#discussion_r487238144", "createdAt": "2020-09-11T19:12:12Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotManager.java", "diffHunk": "@@ -133,4 +137,58 @@ public void testValidateSnapshotIDWidth() throws Exception {\n         getMaxSnapshotID() < Snapshot.CURRENT_STATE_ID);\n   }\n \n+  @Test\n+  public void testSnapshotLimitOnRestart() throws Exception {\n+    final Configuration conf = new Configuration();\n+    final Path snapshottableDir\n+        = new Path(\"/\" + getClass().getSimpleName());\n+    int numSnapshots = 5;\n+    conf.setInt(DFSConfigKeys.\n+            DFS_NAMENODE_SNAPSHOT_MAX_LIMIT, numSnapshots);\n+    conf.setInt(DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_FILESYSTEM_LIMIT,\n+        numSnapshots * 2);\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).\n+        numDataNodes(0).build();\n+    cluster.waitActive();\n+    DistributedFileSystem hdfs = cluster.getFileSystem();\n+    hdfs.mkdirs(snapshottableDir);\n+    hdfs.allowSnapshot(snapshottableDir);\n+    int i = 0;\n+    for (; i < numSnapshots; i++) {\n+      hdfs.createSnapshot(snapshottableDir, \"s\" + i);\n+    }\n+    LambdaTestUtils.intercept(SnapshotException.class,\n+        \"snapshot limit\",\n+        () -> hdfs.createSnapshot(snapshottableDir, \"s5\"));\n+\n+    // now change max snapshot directory limit to 2 and restart namenode\n+    cluster.getNameNode().getConf().setInt(DFSConfigKeys.\n+        DFS_NAMENODE_SNAPSHOT_MAX_LIMIT, 2);\n+    cluster.restartNameNodes();\n+    SnapshotManager snapshotManager = cluster.getNamesystem().\n+        getSnapshotManager();\n+\n+    // make sure edits of all previous 5 create snapshots are replayed\n+    Assert.assertEquals(numSnapshots, snapshotManager.getNumSnapshots());\n+\n+    // make sure namenode has the new snapshot limit configured as 2\n+    Assert.assertEquals(2, snapshotManager.getMaxSnapshotLimit());\n+\n+    // Any new snapshot creation should still fail\n+    LambdaTestUtils.intercept(SnapshotException.class,\n+        \"snapshot limit\",\n+        () -> hdfs.createSnapshot(snapshottableDir, \"s5\"));\n+    // now change max snapshot FS limit to 2 and restart namenode\n+    cluster.getNameNode().getConf().setInt(DFSConfigKeys.\n+        DFS_NAMENODE_SNAPSHOT_FILESYSTEM_LIMIT, 2);\n+    cluster.restartNameNodes();\n+    snapshotManager = cluster.getNamesystem().", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca83922fe5e11535aee53c07495cd03113144669"}, "originalPosition": 67}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0Nzc4OTY0OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotManager.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMVQxOToxMjo0NVrOHQqrBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMVQxOToxMjo0NVrOHQqrBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzIzODQwNw==", "bodyText": "should we clean this cluster?", "url": "https://github.com/apache/hadoop/pull/2296#discussion_r487238407", "createdAt": "2020-09-11T19:12:45Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotManager.java", "diffHunk": "@@ -133,4 +137,58 @@ public void testValidateSnapshotIDWidth() throws Exception {\n         getMaxSnapshotID() < Snapshot.CURRENT_STATE_ID);\n   }\n \n+  @Test\n+  public void testSnapshotLimitOnRestart() throws Exception {\n+    final Configuration conf = new Configuration();\n+    final Path snapshottableDir\n+        = new Path(\"/\" + getClass().getSimpleName());\n+    int numSnapshots = 5;\n+    conf.setInt(DFSConfigKeys.\n+            DFS_NAMENODE_SNAPSHOT_MAX_LIMIT, numSnapshots);\n+    conf.setInt(DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_FILESYSTEM_LIMIT,\n+        numSnapshots * 2);\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca83922fe5e11535aee53c07495cd03113144669"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA1Mzk0MjYzOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotManager.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQxNzozOToxNVrOHRf1eA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQxNzozOToxNVrOHRf1eA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODEwOTQzMg==", "bodyText": "We may need to do it in a finally to make sure we always clean it. Also check for null.", "url": "https://github.com/apache/hadoop/pull/2296#discussion_r488109432", "createdAt": "2020-09-14T17:39:15Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotManager.java", "diffHunk": "@@ -133,4 +138,57 @@ public void testValidateSnapshotIDWidth() throws Exception {\n         getMaxSnapshotID() < Snapshot.CURRENT_STATE_ID);\n   }\n \n+  @Test\n+  public void testSnapshotLimitOnRestart() throws Exception {\n+    final Configuration conf = new Configuration();\n+    final Path snapshottableDir\n+        = new Path(\"/\" + getClass().getSimpleName());\n+    int numSnapshots = 5;\n+    conf.setInt(DFSConfigKeys.\n+            DFS_NAMENODE_SNAPSHOT_MAX_LIMIT, numSnapshots);\n+    conf.setInt(DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_FILESYSTEM_LIMIT,\n+        numSnapshots * 2);\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).\n+        numDataNodes(0).build();\n+    cluster.waitActive();\n+    DistributedFileSystem hdfs = cluster.getFileSystem();\n+    hdfs.mkdirs(snapshottableDir);\n+    hdfs.allowSnapshot(snapshottableDir);\n+    for (int i = 0; i < numSnapshots; i++) {\n+      hdfs.createSnapshot(snapshottableDir, \"s\" + i);\n+    }\n+    LambdaTestUtils.intercept(SnapshotException.class,\n+        \"snapshot limit\",\n+        () -> hdfs.createSnapshot(snapshottableDir, \"s5\"));\n+\n+    // now change max snapshot directory limit to 2 and restart namenode\n+    cluster.getNameNode().getConf().setInt(DFSConfigKeys.\n+        DFS_NAMENODE_SNAPSHOT_MAX_LIMIT, 2);\n+    cluster.restartNameNodes();\n+    SnapshotManager snapshotManager = cluster.getNamesystem().\n+        getSnapshotManager();\n+\n+    // make sure edits of all previous 5 create snapshots are replayed\n+    Assert.assertEquals(numSnapshots, snapshotManager.getNumSnapshots());\n+\n+    // make sure namenode has the new snapshot limit configured as 2\n+    Assert.assertEquals(2, snapshotManager.getMaxSnapshotLimit());\n+\n+    // Any new snapshot creation should still fail\n+    LambdaTestUtils.intercept(SnapshotException.class,\n+        \"snapshot limit\", () -> hdfs.createSnapshot(snapshottableDir, \"s5\"));\n+    // now change max snapshot FS limit to 2 and restart namenode\n+    cluster.getNameNode().getConf().setInt(DFSConfigKeys.\n+        DFS_NAMENODE_SNAPSHOT_FILESYSTEM_LIMIT, 2);\n+    cluster.restartNameNodes();\n+    snapshotManager = cluster.getNamesystem().\n+        getSnapshotManager();\n+    // make sure edits of all previous 5 create snapshots are replayed\n+    Assert.assertEquals(numSnapshots, snapshotManager.getNumSnapshots());\n+\n+    // make sure namenode has the new snapshot limit configured as 2\n+    Assert.assertEquals(2, snapshotManager.getMaxSnapshotLimit());\n+    cluster.shutdown();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9db4fbce5f2eb888be5baceb10358cffa20f79bb"}, "originalPosition": 89}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3279, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}