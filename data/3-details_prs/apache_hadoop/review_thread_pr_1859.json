{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzc5NzY0NjE4", "number": 1859, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNVQyMDo1MTo0MVrODi2PCw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNlQxOToxOTo1M1rODjNKZw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM3ODY2NzYzOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNVQyMDo1MTo0MVrOFuUwNQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNVQyMjo1NToyNFrOFuYTPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDExODgzNw==", "bodyText": "we would get an IOException if the dfs client is closed or if it is unable to reach NameNode for FsServerDefaults.\nI think it makes sense for a HDFS client to assume the file is closed if the open doesn't complete successfully.", "url": "https://github.com/apache/hadoop/pull/1859#discussion_r384118837", "createdAt": "2020-02-25T20:51:41Z", "author": {"login": "jojochuang"}, "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java", "diffHunk": "@@ -329,7 +327,12 @@ public FSDataInputStream open(Path f, final int bufferSize)\n       public FSDataInputStream doCall(final Path p) throws IOException {\n         final DFSInputStream dfsis =\n             dfs.open(getPathName(p), bufferSize, verifyChecksum);\n-        return dfs.createWrappedInputStream(dfsis);\n+        try {\n+          return dfs.createWrappedInputStream(dfsis);\n+        } catch (IOException ex){", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "01f4c16ff4b8c984aa5956c46bcb80d739a3f07b"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDE3Njk1Nw==", "bodyText": "Agree. Here the case for create encrypted file is dfs.open succeeds and return a valid DFSIS, but the createWrappedInputStream throws when user does not have permission to decrypt the EDEK. The fix tries to ensure the file get closed properly in this case.", "url": "https://github.com/apache/hadoop/pull/1859#discussion_r384176957", "createdAt": "2020-02-25T22:55:24Z", "author": {"login": "xiaoyuyao"}, "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java", "diffHunk": "@@ -329,7 +327,12 @@ public FSDataInputStream open(Path f, final int bufferSize)\n       public FSDataInputStream doCall(final Path p) throws IOException {\n         final DFSInputStream dfsis =\n             dfs.open(getPathName(p), bufferSize, verifyChecksum);\n-        return dfs.createWrappedInputStream(dfsis);\n+        try {\n+          return dfs.createWrappedInputStream(dfsis);\n+        } catch (IOException ex){", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDExODgzNw=="}, "originalCommit": {"oid": "01f4c16ff4b8c984aa5956c46bcb80d739a3f07b"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM3ODY5OTQ0OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNVQyMTowMjowMFrOFuVDcg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNlQxOTowODo1OFrOFu4ZRg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDEyMzc2Mg==", "bodyText": "In fact, it looks like HBASE-16062 is related.", "url": "https://github.com/apache/hadoop/pull/1859#discussion_r384123762", "createdAt": "2020-02-25T21:02:00Z", "author": {"login": "jojochuang"}, "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java", "diffHunk": "@@ -697,6 +700,20 @@ public FSDataOutputStream next(final FileSystem fs, final Path p)\n     }.resolve(this, absF);\n   }\n \n+  // Private helper to ensure the wrapped inner stream is closed safely\n+  // upon IOException throw during wrap.\n+  // Assuming the caller owns the inner stream which needs to be closed upon\n+  // wrap failure.\n+  private HdfsDataOutputStream safelyCreateWrappedOutputStream(\n+      DFSOutputStream dfsos) throws IOException {\n+    try {\n+      return dfs.createWrappedOutputStream(dfsos, statistics);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "01f4c16ff4b8c984aa5956c46bcb80d739a3f07b"}, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDcwMjc5MA==", "bodyText": "It does indeed", "url": "https://github.com/apache/hadoop/pull/1859#discussion_r384702790", "createdAt": "2020-02-26T19:08:58Z", "author": {"login": "steveloughran"}, "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java", "diffHunk": "@@ -697,6 +700,20 @@ public FSDataOutputStream next(final FileSystem fs, final Path p)\n     }.resolve(this, absF);\n   }\n \n+  // Private helper to ensure the wrapped inner stream is closed safely\n+  // upon IOException throw during wrap.\n+  // Assuming the caller owns the inner stream which needs to be closed upon\n+  // wrap failure.\n+  private HdfsDataOutputStream safelyCreateWrappedOutputStream(\n+      DFSOutputStream dfsos) throws IOException {\n+    try {\n+      return dfs.createWrappedOutputStream(dfsos, statistics);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDEyMzc2Mg=="}, "originalCommit": {"oid": "01f4c16ff4b8c984aa5956c46bcb80d739a3f07b"}, "originalPosition": 88}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4MjQyNDA3OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandWithDestination.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNlQxOToxOTo1M1rOFu4vNA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNlQxOToxOTo1M1rOFu4vNA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDcwODQwNA==", "bodyText": "One thing I need to be confident that we're handling is the situation where the file exists at the end, create fails with an overwrite error -we don't want the deleteOnExit to suddenly delete that previous file, not if it is one we care about.\nFor direct writes then: we don't do that delete (good), For indirect ones, we are relying on the fact that the file being created is temporary. Because we're going to end up stamping on it aren't we?\nFrom an S3A perspective -I don't see the codepath creating a 404 as the deleteOnExit registration takes place after the upload has succeeded: the HEAD will find the file which has just been created.", "url": "https://github.com/apache/hadoop/pull/1859#discussion_r384708404", "createdAt": "2020-02-26T19:19:53Z", "author": {"login": "steveloughran"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandWithDestination.java", "diffHunk": "@@ -491,25 +490,18 @@ void writeStreamToFile(InputStream in, PathData target,\n         throws IOException {\n       FSDataOutputStream out = null;\n       try {\n-        out = create(target, lazyPersist, direct);\n+        out = create(target, lazyPersist);\n         IOUtils.copyBytes(in, out, getConf(), true);\n-      } catch (IOException e) {\n-        // failure: clean up if we got as far as creating the file\n-        if (!direct && out != null) {\n-          try {\n-            fs.delete(target.path, false);\n-          } catch (IOException ignored) {\n-          }\n-        }\n-        throw e;\n       } finally {\n+        if (!direct) {\n+          deleteOnExit(target.path);\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "01f4c16ff4b8c984aa5956c46bcb80d739a3f07b"}, "originalPosition": 27}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3596, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}