{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzU4OTM1NTQ1", "number": 1790, "reviewThreads": {"totalCount": 46, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwNDowMDo0NlrODV4g2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQyMDo1MDozN1rODofy3g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI0MjcyNjAzOnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwNDowMDo0NlrOFaVAnA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQxMDozNjoyNlrOFcQSrA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE1MTUxNg==", "bodyText": "This config value is passed until AbfsOutputStream.java -> writeCurrentBufferToService() as the first argument. but no specific action is seen inside writeCurrentBufferToService() if the value is true or false.\nWhat is intended of this config ? is it required ?", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r363151516", "createdAt": "2020-01-06T04:00:46Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java", "diffHunk": "@@ -146,6 +150,10 @@\n       DefaultValue = DEFAULT_ENABLE_FLUSH)\n   private boolean enableFlush;\n \n+  @BooleanConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_ENABLE_APPEND_WITH_FLUSH,\n+      DefaultValue = DEFAULT_ENABLE_APPEND_WITH_FLUSH)\n+  private boolean enableAppendWithFlush;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTE3MTM3Mg==", "bodyText": "Last iteration added has bought in the missing code piece.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r365171372", "createdAt": "2020-01-10T10:36:26Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java", "diffHunk": "@@ -146,6 +150,10 @@\n       DefaultValue = DEFAULT_ENABLE_FLUSH)\n   private boolean enableFlush;\n \n+  @BooleanConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_ENABLE_APPEND_WITH_FLUSH,\n+      DefaultValue = DEFAULT_ENABLE_APPEND_WITH_FLUSH)\n+  private boolean enableAppendWithFlush;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE1MTUxNg=="}, "originalCommit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI0MjcyNzAxOnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwNDowMTo0NVrOFaVBJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQxMDozNTo0OVrOFcQRsA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE1MTY1Mw==", "bodyText": "New configs need documentation. For reference, recent documentation done for flush related config : https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-azure/src/site/markdown/abfs.md#-flush-options", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r363151653", "createdAt": "2020-01-06T04:01:45Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java", "diffHunk": "@@ -130,6 +130,10 @@\n       DefaultValue = DEFAULT_FS_AZURE_ATOMIC_RENAME_DIRECTORIES)\n   private String azureAtomicDirs;\n \n+  @StringConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_APPEND_BLOB_KEY,\n+      DefaultValue = DEFAULT_FS_AZURE_APPEND_BLOB_DIRECTORIES)\n+  private String azureAppendBlobDirs;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTEwOTA2OA==", "bodyText": "what is the procedure for the documentation edit? who are the owners of this documentation edit? do I need to email someone for this?", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r365109068", "createdAt": "2020-01-10T07:57:35Z", "author": {"login": "ishaniahuja"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java", "diffHunk": "@@ -130,6 +130,10 @@\n       DefaultValue = DEFAULT_FS_AZURE_ATOMIC_RENAME_DIRECTORIES)\n   private String azureAtomicDirs;\n \n+  @StringConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_APPEND_BLOB_KEY,\n+      DefaultValue = DEFAULT_FS_AZURE_APPEND_BLOB_DIRECTORIES)\n+  private String azureAppendBlobDirs;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE1MTY1Mw=="}, "originalCommit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTE3MTEyMA==", "bodyText": "Dev introducing new config needs to add the documentation for it. You need to edit the abfs.md file mentioned in above comment. Follow format already present in it.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r365171120", "createdAt": "2020-01-10T10:35:49Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java", "diffHunk": "@@ -130,6 +130,10 @@\n       DefaultValue = DEFAULT_FS_AZURE_ATOMIC_RENAME_DIRECTORIES)\n   private String azureAtomicDirs;\n \n+  @StringConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_APPEND_BLOB_KEY,\n+      DefaultValue = DEFAULT_FS_AZURE_APPEND_BLOB_DIRECTORIES)\n+  private String azureAppendBlobDirs;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE1MTY1Mw=="}, "originalCommit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI0MjcyNzYwOnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwNDowMjoyNFrOFaVBdw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQwNzo1NToyOFrOFcMc_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE1MTczNQ==", "bodyText": "Why not return a HashSet as all callers are going to need it so.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r363151735", "createdAt": "2020-01-06T04:02:24Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java", "diffHunk": "@@ -411,6 +419,10 @@ public String getAzureAtomicRenameDirs() {\n     return this.azureAtomicDirs;\n   }\n \n+  public String getAppendBlobDirs() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTEwODQ3Ng==", "bodyText": "this is just the way it is done for other config parameter for dirs. see: getAzureAtomicRenameDirs", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r365108476", "createdAt": "2020-01-10T07:55:28Z", "author": {"login": "ishaniahuja"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java", "diffHunk": "@@ -411,6 +419,10 @@ public String getAzureAtomicRenameDirs() {\n     return this.azureAtomicDirs;\n   }\n \n+  public String getAppendBlobDirs() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE1MTczNQ=="}, "originalCommit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI0MjcyODk2OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwNDowNDowM1rOFaVCSg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQxMDo0OTo1M1rOFcQosQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE1MTk0Ng==", "bodyText": "Is this version present on all the production clusters already ? If not, this change should go in only after the support is available.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r363151946", "createdAt": "2020-01-06T04:04:03Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java", "diffHunk": "@@ -55,7 +55,7 @@\n   public static final Logger LOG = LoggerFactory.getLogger(AbfsClient.class);\n   private final URL baseUrl;\n   private final SharedKeyCredentials sharedKeyCredentials;\n-  private final String xMsVersion = \"2018-11-09\";\n+  private final String xMsVersion = \"2019-12-12\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTEwODA5Mw==", "bodyText": "nopes actually. how can we go about using the new version with the new API only? is there a way to do this in the driver?", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r365108093", "createdAt": "2020-01-10T07:53:39Z", "author": {"login": "ishaniahuja"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java", "diffHunk": "@@ -55,7 +55,7 @@\n   public static final Logger LOG = LoggerFactory.getLogger(AbfsClient.class);\n   private final URL baseUrl;\n   private final SharedKeyCredentials sharedKeyCredentials;\n-  private final String xMsVersion = \"2018-11-09\";\n+  private final String xMsVersion = \"2019-12-12\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE1MTk0Ng=="}, "originalCommit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTE3NzAwOQ==", "bodyText": "Server behaviour is controlled with this API version. So until server is ready, we can not bump up the api version support in driver.\nThis is not recommended to be a config control either, to prevent clients from changing api versions to any older one.\nAll dev work for new features is done with a feature config control that will turn off the feature. When the api version is incremented, the feature config will be turned on too.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r365177009", "createdAt": "2020-01-10T10:49:53Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java", "diffHunk": "@@ -55,7 +55,7 @@\n   public static final Logger LOG = LoggerFactory.getLogger(AbfsClient.class);\n   private final URL baseUrl;\n   private final SharedKeyCredentials sharedKeyCredentials;\n-  private final String xMsVersion = \"2018-11-09\";\n+  private final String xMsVersion = \"2019-12-12\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE1MTk0Ng=="}, "originalCommit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI0MjczMDA5OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwNDowNTozNVrOFaVC-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQxMDo1MTo0OFrOFcQr8g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE1MjEyMA==", "bodyText": "As mentioned in the comments for AbfsConfiguration.java, dont see flush in use inside the method. Also applies to the new isClose argument.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r363152120", "createdAt": "2020-01-06T04:05:35Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -257,17 +265,16 @@ public synchronized void close() throws IOException {\n \n   private synchronized void flushInternal(boolean isClose) throws IOException {\n     maybeThrowLastError();\n-    writeCurrentBufferToService();\n-    flushWrittenBytesToService(isClose);\n+    writeAndFlushWrittenBytesToService(isClose);\n   }\n \n   private synchronized void flushInternalAsync() throws IOException {\n     maybeThrowLastError();\n-    writeCurrentBufferToService();\n+    writeCurrentBufferToService(true, false);\n     flushWrittenBytesToServiceAsync();\n   }\n \n-  private synchronized void writeCurrentBufferToService() throws IOException {\n+  private synchronized void writeCurrentBufferToService(final boolean flush, final boolean isClose) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTEwNDQ5Mg==", "bodyText": "it does not call flush. but passes flush and close flag in the append method in AbfsClient.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r365104492", "createdAt": "2020-01-10T07:39:59Z", "author": {"login": "ishaniahuja"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -257,17 +265,16 @@ public synchronized void close() throws IOException {\n \n   private synchronized void flushInternal(boolean isClose) throws IOException {\n     maybeThrowLastError();\n-    writeCurrentBufferToService();\n-    flushWrittenBytesToService(isClose);\n+    writeAndFlushWrittenBytesToService(isClose);\n   }\n \n   private synchronized void flushInternalAsync() throws IOException {\n     maybeThrowLastError();\n-    writeCurrentBufferToService();\n+    writeCurrentBufferToService(true, false);\n     flushWrittenBytesToServiceAsync();\n   }\n \n-  private synchronized void writeCurrentBufferToService() throws IOException {\n+  private synchronized void writeCurrentBufferToService(final boolean flush, final boolean isClose) throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE1MjEyMA=="}, "originalCommit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTE3Nzg0Mg==", "bodyText": "Param passing was missing earlier iteration. Can see in new update.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r365177842", "createdAt": "2020-01-10T10:51:48Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -257,17 +265,16 @@ public synchronized void close() throws IOException {\n \n   private synchronized void flushInternal(boolean isClose) throws IOException {\n     maybeThrowLastError();\n-    writeCurrentBufferToService();\n-    flushWrittenBytesToService(isClose);\n+    writeAndFlushWrittenBytesToService(isClose);\n   }\n \n   private synchronized void flushInternalAsync() throws IOException {\n     maybeThrowLastError();\n-    writeCurrentBufferToService();\n+    writeCurrentBufferToService(true, false);\n     flushWrittenBytesToServiceAsync();\n   }\n \n-  private synchronized void writeCurrentBufferToService() throws IOException {\n+  private synchronized void writeCurrentBufferToService(final boolean flush, final boolean isClose) throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE1MjEyMA=="}, "originalCommit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "originalPosition": 64}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI0MjczNzUzOnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwNDoxMzo0MVrOFaVHNg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQwNzozNTo0NlrOFcMJ5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE1MzIwNg==", "bodyText": "AbfsOutputStream.close() call flow is:\nclose() -> flushInternal() -> flushWrittenBytesToService() -> flushWrittenBytesToServiceInternal()\nWithin flushWrittenBytesToServiceInternal() is where service \"Flush\" gets called.\nAbove change removes call to flushWrittenBytesToServiceInternal() and replaces it with shrinkWriteOperationQueue().  This is going to prevent flush for AbfsOutputStream.close() calls.\nThis might be ok for AppendBlob, but BlockBlob behaviour needs to be retained where AbfsClient->Flush() will get triggered.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r363153206", "createdAt": "2020-01-06T04:13:41Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -309,15 +316,23 @@ private synchronized void flushWrittenBytesToService(boolean isClose) throws IOE\n             throw new FileNotFoundException(ex.getMessage());\n           }\n         }\n-\n         if (ex.getCause() instanceof AzureBlobFileSystemException) {\n           ex = (AzureBlobFileSystemException) ex.getCause();\n         }\n         lastError = new IOException(ex);\n         throw lastError;\n       }\n     }\n-    flushWrittenBytesToServiceInternal(position, false, isClose);\n+    shrinkWriteOperationQueue();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTEwMzU5MQ==", "bodyText": "as discussed offline, please look at new REST API. the code is same for both the appendBlob and BlockBlob. THe difference at the OutputStream Code is to use a single upload thread for appendBlob and multiple upload threads for block Blob. Its a lot of code rewrite, hence the full file read might be more useful.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r365103591", "createdAt": "2020-01-10T07:35:46Z", "author": {"login": "ishaniahuja"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -309,15 +316,23 @@ private synchronized void flushWrittenBytesToService(boolean isClose) throws IOE\n             throw new FileNotFoundException(ex.getMessage());\n           }\n         }\n-\n         if (ex.getCause() instanceof AzureBlobFileSystemException) {\n           ex = (AzureBlobFileSystemException) ex.getCause();\n         }\n         lastError = new IOException(ex);\n         throw lastError;\n       }\n     }\n-    flushWrittenBytesToServiceInternal(position, false, isClose);\n+    shrinkWriteOperationQueue();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE1MzIwNg=="}, "originalCommit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "originalPosition": 81}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI0Mjc0Mzk1OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwNDoyMTo0MFrOFaVK5w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQxMDo1MDozMVrOFcQpwA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE1NDE1MQ==", "bodyText": "This method is supposed to be called from AbfsOutputStream.java -> writeCurrentBufferToService(), but there is no change for call to this API within that method. How did the compilation pass.\nAnd, (probably a repeat question of config query posted above, still) can you please explain why flush and close needs to be added to Append method.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r363154151", "createdAt": "2020-01-06T04:21:40Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java", "diffHunk": "@@ -278,7 +282,8 @@ public AbfsRestOperation renamePath(final String source, final String destinatio\n   }\n \n   public AbfsRestOperation append(final String path, final long position, final byte[] buffer, final int offset,\n-                                  final int length) throws AzureBlobFileSystemException {\n+                                  final int length, boolean flush, boolean isClose)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTE3NzI4MA==", "bodyText": "New iteration has the caller updated.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r365177280", "createdAt": "2020-01-10T10:50:31Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java", "diffHunk": "@@ -278,7 +282,8 @@ public AbfsRestOperation renamePath(final String source, final String destinatio\n   }\n \n   public AbfsRestOperation append(final String path, final long position, final byte[] buffer, final int offset,\n-                                  final int length) throws AzureBlobFileSystemException {\n+                                  final int length, boolean flush, boolean isClose)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE1NDE1MQ=="}, "originalCommit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI0MzE4Mzg0OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/pom.xml", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwOTo0MToxM1rOFaZLyg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwOTo0MToxM1rOFaZLyg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzIxOTkxNA==", "bodyText": "We specify dependency versions in the parent pom", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r363219914", "createdAt": "2020-01-06T09:41:13Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/pom.xml", "diffHunk": "@@ -143,6 +143,14 @@\n \n   <!-- see hadoop-project/pom.xml for version number declarations -->\n   <dependencies>\n+\t\t<!-- https://mvnrepository.com/artifact/org.mockito/mockito-all -->\n+\t\t<dependency>\n+\t\t\t\t<groupId>org.mockito</groupId>\n+\t\t\t\t<artifactId>mockito-all</artifactId>\n+\t\t\t\t<version>1.8.4</version>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI0MzIwODYyOnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwOTo1MToxOFrOFaZagg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOFQxMDozNDoyMlrOFigMSQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzIyMzY4Mg==", "bodyText": "can the lines 171-185 be made a separate method and reuse in the test methods", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r363223682", "createdAt": "2020-01-06T09:51:18Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,268 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.mockito.ArgumentCaptor;\n+import static org.mockito.Mockito.*;\n+/**\n+ * Test useragent of abfs client.\n+ *\n+ */\n+public final class TestAbfsOutputStream {\n+\n+  private static int bufferSize = 4096;\n+  private static int writeSize = 1000;\n+  private static String path = \"~/testpath\";\n+\n+  @Test\n+  public void verifyShortWriteRequest() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, true, false);\n+    final byte[] b = new byte[writeSize];\n+    new Random().nextBytes(b);\n+    out.write(b);\n+    out.hsync();\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    final byte[] b1 = new byte[2*writeSize];\n+    new Random().nextBytes(b1);\n+    out.write(b1);\n+    out.flush();\n+    out.hflush();\n+\n+    out.hsync();\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(0), Long.valueOf(writeSize)), acLong.getAllValues());\n+    //flush=true, close=false, flush=true, close=false\n+    Assert.assertEquals(Arrays.asList(true, false, true, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0,writeSize, 0, 2*writeSize), acInt.getAllValues());\n+\n+    verifyNoMoreInteractions(client);\n+\n+  }\n+\n+  @Test\n+  public void verifyWriteRequest() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, true, false);\n+    final byte[] b = new byte[writeSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 5; i++) {\n+      out.write(b);\n+    }\n+    out.close();\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize)), acLong.getAllValues());\n+    //flush=false,close=false, flush=true,close=true\n+    Assert.assertEquals(Arrays.asList(false, false, true, true), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, 5*writeSize-bufferSize), acInt.getAllValues());\n+\n+    verifyNoMoreInteractions(client);\n+  }\n+\n+  @Test\n+  public void verifyWriteRequestOfBufferSizeAndClose() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, true, false);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    out.close();\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(new HashSet<Long>(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize))), new HashSet<Long>(acLong.getAllValues()));\n+    //flush=false, close=false, flush=false, close=false\n+    Assert.assertEquals(Arrays.asList(false, false, false, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, bufferSize), acInt.getAllValues());\n+\n+    ArgumentCaptor<String> acFlushString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acFlushLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Boolean> acFlushBool = ArgumentCaptor.forClass(Boolean.class);\n+\n+    verify(client, times(1)).flush(acFlushString.capture(), acFlushLong.capture(), acFlushBool.capture(), acFlushBool.capture());\n+    Assert.assertEquals(Arrays.asList(path) , acFlushString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(2*bufferSize)), acFlushLong.getAllValues());\n+    Assert.assertEquals(Arrays.asList(false, true), acFlushBool.getAllValues());\n+\n+    verifyNoMoreInteractions(client);\n+  }\n+\n+  @Test\n+  public void verifyWriteRequestOfBufferSize() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, true, false);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    Thread.sleep(1000);\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(new HashSet<Long>(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize))), new HashSet<Long>(acLong.getAllValues()));\n+    //flush=false, close=false, flush=false, close=false\n+    Assert.assertEquals(Arrays.asList(false, false, false, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, bufferSize), acInt.getAllValues());\n+\n+    verifyNoMoreInteractions(client);\n+  }\n+\n+  @Test\n+  public void verifyWriteRequestOfBufferSizeWithAppendBlob() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, true, true);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    Thread.sleep(1000);\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "originalPosition": 185}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTcyMTg1MA==", "bodyText": "these are just initializations.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r371721850", "createdAt": "2020-01-28T10:31:15Z", "author": {"login": "ishaniahuja"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,268 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.mockito.ArgumentCaptor;\n+import static org.mockito.Mockito.*;\n+/**\n+ * Test useragent of abfs client.\n+ *\n+ */\n+public final class TestAbfsOutputStream {\n+\n+  private static int bufferSize = 4096;\n+  private static int writeSize = 1000;\n+  private static String path = \"~/testpath\";\n+\n+  @Test\n+  public void verifyShortWriteRequest() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, true, false);\n+    final byte[] b = new byte[writeSize];\n+    new Random().nextBytes(b);\n+    out.write(b);\n+    out.hsync();\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    final byte[] b1 = new byte[2*writeSize];\n+    new Random().nextBytes(b1);\n+    out.write(b1);\n+    out.flush();\n+    out.hflush();\n+\n+    out.hsync();\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(0), Long.valueOf(writeSize)), acLong.getAllValues());\n+    //flush=true, close=false, flush=true, close=false\n+    Assert.assertEquals(Arrays.asList(true, false, true, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0,writeSize, 0, 2*writeSize), acInt.getAllValues());\n+\n+    verifyNoMoreInteractions(client);\n+\n+  }\n+\n+  @Test\n+  public void verifyWriteRequest() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, true, false);\n+    final byte[] b = new byte[writeSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 5; i++) {\n+      out.write(b);\n+    }\n+    out.close();\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize)), acLong.getAllValues());\n+    //flush=false,close=false, flush=true,close=true\n+    Assert.assertEquals(Arrays.asList(false, false, true, true), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, 5*writeSize-bufferSize), acInt.getAllValues());\n+\n+    verifyNoMoreInteractions(client);\n+  }\n+\n+  @Test\n+  public void verifyWriteRequestOfBufferSizeAndClose() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, true, false);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    out.close();\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(new HashSet<Long>(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize))), new HashSet<Long>(acLong.getAllValues()));\n+    //flush=false, close=false, flush=false, close=false\n+    Assert.assertEquals(Arrays.asList(false, false, false, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, bufferSize), acInt.getAllValues());\n+\n+    ArgumentCaptor<String> acFlushString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acFlushLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Boolean> acFlushBool = ArgumentCaptor.forClass(Boolean.class);\n+\n+    verify(client, times(1)).flush(acFlushString.capture(), acFlushLong.capture(), acFlushBool.capture(), acFlushBool.capture());\n+    Assert.assertEquals(Arrays.asList(path) , acFlushString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(2*bufferSize)), acFlushLong.getAllValues());\n+    Assert.assertEquals(Arrays.asList(false, true), acFlushBool.getAllValues());\n+\n+    verifyNoMoreInteractions(client);\n+  }\n+\n+  @Test\n+  public void verifyWriteRequestOfBufferSize() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, true, false);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    Thread.sleep(1000);\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(new HashSet<Long>(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize))), new HashSet<Long>(acLong.getAllValues()));\n+    //flush=false, close=false, flush=false, close=false\n+    Assert.assertEquals(Arrays.asList(false, false, false, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, bufferSize), acInt.getAllValues());\n+\n+    verifyNoMoreInteractions(client);\n+  }\n+\n+  @Test\n+  public void verifyWriteRequestOfBufferSizeWithAppendBlob() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, true, true);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    Thread.sleep(1000);\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzIyMzY4Mg=="}, "originalCommit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "originalPosition": 185}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTcyMzMzNw==", "bodyText": "these are just inits.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r371723337", "createdAt": "2020-01-28T10:34:22Z", "author": {"login": "ishaniahuja"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,268 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.mockito.ArgumentCaptor;\n+import static org.mockito.Mockito.*;\n+/**\n+ * Test useragent of abfs client.\n+ *\n+ */\n+public final class TestAbfsOutputStream {\n+\n+  private static int bufferSize = 4096;\n+  private static int writeSize = 1000;\n+  private static String path = \"~/testpath\";\n+\n+  @Test\n+  public void verifyShortWriteRequest() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, true, false);\n+    final byte[] b = new byte[writeSize];\n+    new Random().nextBytes(b);\n+    out.write(b);\n+    out.hsync();\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    final byte[] b1 = new byte[2*writeSize];\n+    new Random().nextBytes(b1);\n+    out.write(b1);\n+    out.flush();\n+    out.hflush();\n+\n+    out.hsync();\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(0), Long.valueOf(writeSize)), acLong.getAllValues());\n+    //flush=true, close=false, flush=true, close=false\n+    Assert.assertEquals(Arrays.asList(true, false, true, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0,writeSize, 0, 2*writeSize), acInt.getAllValues());\n+\n+    verifyNoMoreInteractions(client);\n+\n+  }\n+\n+  @Test\n+  public void verifyWriteRequest() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, true, false);\n+    final byte[] b = new byte[writeSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 5; i++) {\n+      out.write(b);\n+    }\n+    out.close();\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize)), acLong.getAllValues());\n+    //flush=false,close=false, flush=true,close=true\n+    Assert.assertEquals(Arrays.asList(false, false, true, true), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, 5*writeSize-bufferSize), acInt.getAllValues());\n+\n+    verifyNoMoreInteractions(client);\n+  }\n+\n+  @Test\n+  public void verifyWriteRequestOfBufferSizeAndClose() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, true, false);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    out.close();\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(new HashSet<Long>(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize))), new HashSet<Long>(acLong.getAllValues()));\n+    //flush=false, close=false, flush=false, close=false\n+    Assert.assertEquals(Arrays.asList(false, false, false, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, bufferSize), acInt.getAllValues());\n+\n+    ArgumentCaptor<String> acFlushString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acFlushLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Boolean> acFlushBool = ArgumentCaptor.forClass(Boolean.class);\n+\n+    verify(client, times(1)).flush(acFlushString.capture(), acFlushLong.capture(), acFlushBool.capture(), acFlushBool.capture());\n+    Assert.assertEquals(Arrays.asList(path) , acFlushString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(2*bufferSize)), acFlushLong.getAllValues());\n+    Assert.assertEquals(Arrays.asList(false, true), acFlushBool.getAllValues());\n+\n+    verifyNoMoreInteractions(client);\n+  }\n+\n+  @Test\n+  public void verifyWriteRequestOfBufferSize() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, true, false);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    Thread.sleep(1000);\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(new HashSet<Long>(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize))), new HashSet<Long>(acLong.getAllValues()));\n+    //flush=false, close=false, flush=false, close=false\n+    Assert.assertEquals(Arrays.asList(false, false, false, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, bufferSize), acInt.getAllValues());\n+\n+    verifyNoMoreInteractions(client);\n+  }\n+\n+  @Test\n+  public void verifyWriteRequestOfBufferSizeWithAppendBlob() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, true, true);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    Thread.sleep(1000);\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzIyMzY4Mg=="}, "originalCommit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "originalPosition": 185}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI0MzIxODg4OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwOTo1NTowNVrOFaZgRA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOFQxMDoyOToyOFrOFigC-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzIyNTE1Ng==", "bodyText": "Shall we have these operations as a separate private method and call that from this constructor. This constructor is becoming lengthy.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r363225156", "createdAt": "2020-01-06T09:55:05Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "diffHunk": "@@ -164,6 +169,23 @@ public AzureBlobFileSystemStore(URI uri, boolean isSecureScheme, Configuration c\n     boolean useHttps = (usingOauth || abfsConfiguration.isHttpsAlwaysUsed()) ? true : isSecureScheme;\n     initializeClient(uri, fileSystemName, accountName, useHttps);\n     this.identityTransformer = new IdentityTransformer(abfsConfiguration.getRawConfiguration());\n+\n+    // Extract the directories that should contain page blobs", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTcyMDk1NA==", "bodyText": "I don't think that should be done as a part of this PR.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r371720954", "createdAt": "2020-01-28T10:29:28Z", "author": {"login": "ishaniahuja"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "diffHunk": "@@ -164,6 +169,23 @@ public AzureBlobFileSystemStore(URI uri, boolean isSecureScheme, Configuration c\n     boolean useHttps = (usingOauth || abfsConfiguration.isHttpsAlwaysUsed()) ? true : isSecureScheme;\n     initializeClient(uri, fileSystemName, accountName, useHttps);\n     this.identityTransformer = new IdentityTransformer(abfsConfiguration.getRawConfiguration());\n+\n+    // Extract the directories that should contain page blobs", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzIyNTE1Ng=="}, "originalCommit": {"oid": "11e0aac44fc8e8a9dd2745836e127e90cb08326f"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI1NTYyOTY3OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/pom.xml", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQxMDozMzozNlrOFcQOgQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOFQwOToxMzoyMlrOFidn0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTE3MDMwNQ==", "bodyText": "Wildfly jar version is picked from the parent project. Retain the original config.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r365170305", "createdAt": "2020-01-10T10:33:36Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/pom.xml", "diffHunk": "@@ -194,7 +203,8 @@\n     <dependency>\n       <groupId>org.wildfly.openssl</groupId>\n       <artifactId>wildfly-openssl</artifactId>\n-      <scope>runtime</scope>\n+      <version>1.0.9.Final</version>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c74b1ec54bbbea58fdf75f015d491d4bff221fb"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDQ3NDMzOQ==", "bodyText": "we are using 1.0.7.final: \n  \n    \n      hadoop/hadoop-project/pom.xml\n    \n    \n         Line 199\n      in\n      978c487\n    \n    \n    \n    \n\n        \n          \n           <openssl-wildfly.version>1.0.7.Final</openssl-wildfly.version> \n        \n    \n  \n\n\nany special reason to upgrade?", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r370474339", "createdAt": "2020-01-24T04:59:34Z", "author": {"login": "DadanielZ"}, "path": "hadoop-tools/hadoop-azure/pom.xml", "diffHunk": "@@ -194,7 +203,8 @@\n     <dependency>\n       <groupId>org.wildfly.openssl</groupId>\n       <artifactId>wildfly-openssl</artifactId>\n-      <scope>runtime</scope>\n+      <version>1.0.9.Final</version>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTE3MDMwNQ=="}, "originalCommit": {"oid": "0c74b1ec54bbbea58fdf75f015d491d4bff221fb"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTY4MTIzNA==", "bodyText": "I was seeing compilation failure with 1.0.7.Final", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r371681234", "createdAt": "2020-01-28T09:13:22Z", "author": {"login": "ishaniahuja"}, "path": "hadoop-tools/hadoop-azure/pom.xml", "diffHunk": "@@ -194,7 +203,8 @@\n     <dependency>\n       <groupId>org.wildfly.openssl</groupId>\n       <artifactId>wildfly-openssl</artifactId>\n-      <scope>runtime</scope>\n+      <version>1.0.9.Final</version>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTE3MDMwNQ=="}, "originalCommit": {"oid": "0c74b1ec54bbbea58fdf75f015d491d4bff221fb"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI1NTYzMDkxOnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/pom.xml", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQxMDozNDowOFrOFcQPOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQxMDozNDowOFrOFcQPOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTE3MDQ4OQ==", "bodyText": "Undo unintended config changes", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r365170489", "createdAt": "2020-01-10T10:34:08Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/pom.xml", "diffHunk": "@@ -291,6 +301,7 @@\n       </activation>\n       <build>\n         <plugins>\n+<!--", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c74b1ec54bbbea58fdf75f015d491d4bff221fb"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI1NTY1NjkwOnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQxMDo0NDowM1rOFcQfgA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQwNTo1MDowNFrOFpsNbA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTE3NDY1Ng==", "bodyText": "Is support for AppendBlob over queryparam \"blobtype\" also enabled with new Dec-2019 version ? If yes, add a config control for AppendBlob as well and have the config off, which can be turned on when the API version is upgraded in driver.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r365174656", "createdAt": "2020-01-10T10:44:03Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "diffHunk": "@@ -344,40 +366,53 @@ public void deleteFilesystem() throws AzureBlobFileSystemException {\n \n   public OutputStream createFile(final Path path, final boolean overwrite, final FsPermission permission,\n                                  final FsPermission umask) throws AzureBlobFileSystemException {\n-    boolean isNamespaceEnabled = getIsNamespaceEnabled();\n-    LOG.debug(\"createFile filesystem: {} path: {} overwrite: {} permission: {} umask: {} isNamespaceEnabled: {}\",\n-            client.getFileSystem(),\n-            path,\n-            overwrite,\n-            permission.toString(),\n-            umask.toString(),\n-            isNamespaceEnabled);\n-\n-    client.createPath(AbfsHttpConstants.FORWARD_SLASH + getRelativePath(path), true, overwrite,\n-        isNamespaceEnabled ? getOctalNotation(permission) : null,\n-        isNamespaceEnabled ? getOctalNotation(umask) : null);\n+    try (AbfsPerfInfo perfInfo = startTracking(\"createFile\", \"createPath\")) {\n+      boolean isNamespaceEnabled = getIsNamespaceEnabled();\n+      LOG.debug(\"createFile filesystem: {} path: {} overwrite: {} permission: {} umask: {} isNamespaceEnabled: {}\",\n+              client.getFileSystem(),\n+              path,\n+              overwrite,\n+              permission.toString(),\n+              umask.toString(),\n+              isNamespaceEnabled);\n+\n+        boolean appendBlob = false;\n+        if (isAppendBlobKey(path.toString())) {\n+          appendBlob = true;\n+        }\n \n-    return new AbfsOutputStream(\n-        client,\n-        AbfsHttpConstants.FORWARD_SLASH + getRelativePath(path),\n-        0,\n-        abfsConfiguration.getWriteBufferSize(),\n-        abfsConfiguration.isFlushEnabled());\n+      client.createPath(AbfsHttpConstants.FORWARD_SLASH + getRelativePath(path), true, overwrite,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c74b1ec54bbbea58fdf75f015d491d4bff221fb"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI2MDI2OA==", "bodyText": "its added.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r379260268", "createdAt": "2020-02-14T05:50:04Z", "author": {"login": "ishaniahuja"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "diffHunk": "@@ -344,40 +366,53 @@ public void deleteFilesystem() throws AzureBlobFileSystemException {\n \n   public OutputStream createFile(final Path path, final boolean overwrite, final FsPermission permission,\n                                  final FsPermission umask) throws AzureBlobFileSystemException {\n-    boolean isNamespaceEnabled = getIsNamespaceEnabled();\n-    LOG.debug(\"createFile filesystem: {} path: {} overwrite: {} permission: {} umask: {} isNamespaceEnabled: {}\",\n-            client.getFileSystem(),\n-            path,\n-            overwrite,\n-            permission.toString(),\n-            umask.toString(),\n-            isNamespaceEnabled);\n-\n-    client.createPath(AbfsHttpConstants.FORWARD_SLASH + getRelativePath(path), true, overwrite,\n-        isNamespaceEnabled ? getOctalNotation(permission) : null,\n-        isNamespaceEnabled ? getOctalNotation(umask) : null);\n+    try (AbfsPerfInfo perfInfo = startTracking(\"createFile\", \"createPath\")) {\n+      boolean isNamespaceEnabled = getIsNamespaceEnabled();\n+      LOG.debug(\"createFile filesystem: {} path: {} overwrite: {} permission: {} umask: {} isNamespaceEnabled: {}\",\n+              client.getFileSystem(),\n+              path,\n+              overwrite,\n+              permission.toString(),\n+              umask.toString(),\n+              isNamespaceEnabled);\n+\n+        boolean appendBlob = false;\n+        if (isAppendBlobKey(path.toString())) {\n+          appendBlob = true;\n+        }\n \n-    return new AbfsOutputStream(\n-        client,\n-        AbfsHttpConstants.FORWARD_SLASH + getRelativePath(path),\n-        0,\n-        abfsConfiguration.getWriteBufferSize(),\n-        abfsConfiguration.isFlushEnabled());\n+      client.createPath(AbfsHttpConstants.FORWARD_SLASH + getRelativePath(path), true, overwrite,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTE3NDY1Ng=="}, "originalCommit": {"oid": "0c74b1ec54bbbea58fdf75f015d491d4bff221fb"}, "originalPosition": 73}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI1NTY4NzI3OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQxMDo1NjowNFrOFcQyNQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQxMDo1NjowNFrOFcQyNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTE3OTQ0NQ==", "bodyText": "Many changes from trunk merge is showing up as new updates even though not added as part of this PR.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r365179445", "createdAt": "2020-01-10T10:56:04Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java", "diffHunk": "@@ -427,6 +443,14 @@ public boolean isFlushEnabled() {\n     return this.enableFlush;\n   }\n \n+  public boolean isAppendWithFlushEnabled() {\n+    return this.enableAppendWithFlush;\n+  }\n+\n+  public boolean isOutputStreamFlushDisabled() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c74b1ec54bbbea58fdf75f015d491d4bff221fb"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI1NTgwMDI1OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQxMTo0Mjo0OVrOFcR0uw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQxMTo0Mjo0OVrOFcR0uw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTE5NjQ3NQ==", "bodyText": "In the trunk, client calls have AbfsPerfTracker tracking all the calls. Not sure why the PR view is failing to show it as a delta from trunk. But looks like there is some issue here with merge from trunk. Please do a file to file compare from PR branch to trunk.\nexample: from trunk:\n      public Void call() throws Exception { AbfsPerfTracker tracker = client.getAbfsPerfTracker(); try (AbfsPerfInfo perfInfo = new AbfsPerfInfo(tracker, \"writeCurrentBufferToService\", \"append\")) { AbfsRestOperation op = client.append(path, offset, bytes, 0, bytesLength); perfInfo.registerResult(op.getResult()); byteBufferPool.putBuffer(ByteBuffer.wrap(bytes)); perfInfo.registerSuccess(true); return null; }\n(this is not the only delta)", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r365196475", "createdAt": "2020-01-10T11:42:49Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -287,13 +297,13 @@ private synchronized void writeCurrentBufferToService() throws IOException {\n       @Override\n       public Void call() throws Exception {\n         client.append(path, offset, bytes, 0,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c74b1ec54bbbea58fdf75f015d491d4bff221fb"}, "originalPosition": 74}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4OTkzOTE1OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/pom.xml", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yNFQwNDo1NDoyOVrOFhT6Hw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yNFQwNDo1NDoyOVrOFhT6Hw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDQ3MzUwMw==", "bodyText": "removed this commented section if not used", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r370473503", "createdAt": "2020-01-24T04:54:29Z", "author": {"login": "DadanielZ"}, "path": "hadoop-tools/hadoop-azure/pom.xml", "diffHunk": "@@ -143,6 +143,15 @@\n \n   <!-- see hadoop-project/pom.xml for version number declarations -->\n   <dependencies>\n+<!--\n+\t\t<dependency>\n+\t\t\t\t<groupId>org.mockito</groupId>\n+\t\t\t\t<artifactId>mockito-all</artifactId>\n+\t\t\t\t<version>1.8.4</version>\n+\t\t\t\t<scope>test</scope>\n+\t\t</dependency>\n+-->", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c74b1ec54bbbea58fdf75f015d491d4bff221fb"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4OTk0NTg4OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/pom.xml", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yNFQwNTowMDoyOFrOFhT90w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yNFQwNTowMDoyOFrOFhT90w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDQ3NDQ1MQ==", "bodyText": "why commented?", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r370474451", "createdAt": "2020-01-24T05:00:28Z", "author": {"login": "DadanielZ"}, "path": "hadoop-tools/hadoop-azure/pom.xml", "diffHunk": "@@ -194,7 +203,8 @@\n     <dependency>\n       <groupId>org.wildfly.openssl</groupId>\n       <artifactId>wildfly-openssl</artifactId>\n-      <scope>runtime</scope>\n+      <version>1.0.9.Final</version>\n+      <!--<scope>runtime</scope>-->", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c74b1ec54bbbea58fdf75f015d491d4bff221fb"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4OTk2MDgwOnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yNFQwNToxNjo1NVrOFhUHHw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wMlQxNjowNDowNVrOFwlODA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDQ3NjgzMQ==", "bodyText": "remove the *, import the actual class", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r370476831", "createdAt": "2020-01-24T05:16:55Z", "author": {"login": "DadanielZ"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,268 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.mockito.ArgumentCaptor;\n+import static org.mockito.Mockito.*;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c74b1ec54bbbea58fdf75f015d491d4bff221fb"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjQ4NTc3Mg==", "bodyText": "ideally, yes, though we are a bit more relaxed about static imports...", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r386485772", "createdAt": "2020-03-02T16:04:05Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,268 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.mockito.ArgumentCaptor;\n+import static org.mockito.Mockito.*;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDQ3NjgzMQ=="}, "originalCommit": {"oid": "0c74b1ec54bbbea58fdf75f015d491d4bff221fb"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4OTk2OTczOnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yNFQwNToyNjowMlrOFhUMtA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yNFQwNToyNjowMlrOFhUMtA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDQ3ODI2MA==", "bodyText": "Looks like there are some issue in this PR, this try logic is already in:\n\n  \n    \n      hadoop/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java\n    \n    \n         Line 423\n      in\n      978c487\n    \n    \n    \n    \n\n        \n          \n           try (AbfsPerfInfo perfInfo = startTracking(\"createDirectory\", \"createPath\")) {", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r370478260", "createdAt": "2020-01-24T05:26:02Z", "author": {"login": "DadanielZ"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "diffHunk": "@@ -344,40 +366,53 @@ public void deleteFilesystem() throws AzureBlobFileSystemException {\n \n   public OutputStream createFile(final Path path, final boolean overwrite, final FsPermission permission,\n                                  final FsPermission umask) throws AzureBlobFileSystemException {\n-    boolean isNamespaceEnabled = getIsNamespaceEnabled();\n-    LOG.debug(\"createFile filesystem: {} path: {} overwrite: {} permission: {} umask: {} isNamespaceEnabled: {}\",\n-            client.getFileSystem(),\n-            path,\n-            overwrite,\n-            permission.toString(),\n-            umask.toString(),\n-            isNamespaceEnabled);\n-\n-    client.createPath(AbfsHttpConstants.FORWARD_SLASH + getRelativePath(path), true, overwrite,\n-        isNamespaceEnabled ? getOctalNotation(permission) : null,\n-        isNamespaceEnabled ? getOctalNotation(umask) : null);\n+    try (AbfsPerfInfo perfInfo = startTracking(\"createFile\", \"createPath\")) {\n+      boolean isNamespaceEnabled = getIsNamespaceEnabled();\n+      LOG.debug(\"createFile filesystem: {} path: {} overwrite: {} permission: {} umask: {} isNamespaceEnabled: {}\",\n+              client.getFileSystem(),\n+              path,\n+              overwrite,\n+              permission.toString(),\n+              umask.toString(),\n+              isNamespaceEnabled);\n+\n+        boolean appendBlob = false;\n+        if (isAppendBlobKey(path.toString())) {\n+          appendBlob = true;\n+        }\n \n-    return new AbfsOutputStream(\n-        client,\n-        AbfsHttpConstants.FORWARD_SLASH + getRelativePath(path),\n-        0,\n-        abfsConfiguration.getWriteBufferSize(),\n-        abfsConfiguration.isFlushEnabled());\n+      client.createPath(AbfsHttpConstants.FORWARD_SLASH + getRelativePath(path), true, overwrite,\n+          isNamespaceEnabled ? getOctalNotation(permission) : null,\n+          isNamespaceEnabled ? getOctalNotation(umask) : null,\n+          appendBlob);\n+\n+      return new AbfsOutputStream(\n+          client,\n+          AbfsHttpConstants.FORWARD_SLASH + getRelativePath(path),\n+          0,\n+          abfsConfiguration.getWriteBufferSize(),\n+          abfsConfiguration.isFlushEnabled(),\n+          abfsConfiguration.isAppendWithFlushEnabled(),\n+          appendBlob);\n+    }\n   }\n \n   public void createDirectory(final Path path, final FsPermission permission, final FsPermission umask)\n       throws AzureBlobFileSystemException {\n-    boolean isNamespaceEnabled = getIsNamespaceEnabled();\n-    LOG.debug(\"createDirectory filesystem: {} path: {} permission: {} umask: {} isNamespaceEnabled: {}\",\n-            client.getFileSystem(),\n-            path,\n-            permission,\n-            umask,\n-            isNamespaceEnabled);\n-\n-    client.createPath(AbfsHttpConstants.FORWARD_SLASH + getRelativePath(path), false, true,\n-        isNamespaceEnabled ? getOctalNotation(permission) : null,\n-        isNamespaceEnabled ? getOctalNotation(umask) : null);\n+    try (AbfsPerfInfo perfInfo = startTracking(\"createDirectory\", \"createPath\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c74b1ec54bbbea58fdf75f015d491d4bff221fb"}, "originalPosition": 102}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4OTk3MjQ3OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yNFQwNToyODozMFrOFhUOUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yN1QwNjo0MTowN1rOFh4_Jg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDQ3ODY3Mg==", "bodyText": "could you elaborate here a little bit more why for appendblob the max concurrent requests is 1?", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r370478672", "createdAt": "2020-01-24T05:28:30Z", "author": {"login": "DadanielZ"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -80,20 +82,29 @@ public AbfsOutputStream(\n       final String path,\n       final long position,\n       final int bufferSize,\n-      final boolean supportFlush) {\n+      final boolean supportFlush,\n+      final boolean disableOutputStreamFlush,\n+      final boolean supportAppendWithFlush,\n+      final boolean appendBlob) {\n     this.client = client;\n     this.path = path;\n     this.position = position;\n     this.closed = false;\n+    this.disableOutputStreamFlush = disableOutputStreamFlush;\n     this.supportFlush = supportFlush;\n+    this.supportAppendWithFlush = supportAppendWithFlush;\n     this.lastError = null;\n     this.lastFlushOffset = 0;\n     this.bufferSize = bufferSize;\n     this.buffer = byteBufferPool.getBuffer(false, bufferSize).array();\n     this.bufferIndex = 0;\n     this.writeOperations = new ConcurrentLinkedDeque<>();\n \n-    this.maxConcurrentRequestCount = 4 * Runtime.getRuntime().availableProcessors();\n+    if (appendBlob) {\n+      this.maxConcurrentRequestCount = 1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c74b1ec54bbbea58fdf75f015d491d4bff221fb"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTA4MDk5OA==", "bodyText": "AppendBlob for an HDFS file write should not allow parallel upload because that would lead to random writes in the file.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r371080998", "createdAt": "2020-01-27T06:41:07Z", "author": {"login": "ishaniahuja"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -80,20 +82,29 @@ public AbfsOutputStream(\n       final String path,\n       final long position,\n       final int bufferSize,\n-      final boolean supportFlush) {\n+      final boolean supportFlush,\n+      final boolean disableOutputStreamFlush,\n+      final boolean supportAppendWithFlush,\n+      final boolean appendBlob) {\n     this.client = client;\n     this.path = path;\n     this.position = position;\n     this.closed = false;\n+    this.disableOutputStreamFlush = disableOutputStreamFlush;\n     this.supportFlush = supportFlush;\n+    this.supportAppendWithFlush = supportAppendWithFlush;\n     this.lastError = null;\n     this.lastFlushOffset = 0;\n     this.bufferSize = bufferSize;\n     this.buffer = byteBufferPool.getBuffer(false, bufferSize).array();\n     this.bufferIndex = 0;\n     this.writeOperations = new ConcurrentLinkedDeque<>();\n \n-    this.maxConcurrentRequestCount = 4 * Runtime.getRuntime().availableProcessors();\n+    if (appendBlob) {\n+      this.maxConcurrentRequestCount = 1;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDQ3ODY3Mg=="}, "originalCommit": {"oid": "0c74b1ec54bbbea58fdf75f015d491d4bff221fb"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0MTA1NTEzOnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQxNzowNzowNVrOFo3D1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QxMDoxNTo1OVrOFpOHkQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODM4OTQ2MQ==", "bodyText": "Are the trunk merge issues resolved ? This is an existing code in trunk but shown as new change here.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r378389461", "createdAt": "2020-02-12T17:07:05Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java", "diffHunk": "@@ -158,6 +162,14 @@\n       DefaultValue = DEFAULT_DISABLE_OUTPUTSTREAM_FLUSH)\n   private boolean disableOutputStreamFlush;\n \n+  @BooleanConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_ENABLE_APPEND_WITH_FLUSH,\n+      DefaultValue = DEFAULT_ENABLE_APPEND_WITH_FLUSH)\n+  private boolean enableAppendWithFlush;\n+\n+  @BooleanConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_DISABLE_OUTPUTSTREAM_FLUSH,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8590f79e9ed00f055b2fd822c6c964943dddbb91"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODc2NzI0OQ==", "bodyText": "there was a conflict for which I had to add this.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r378767249", "createdAt": "2020-02-13T10:15:59Z", "author": {"login": "ishaniahuja"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java", "diffHunk": "@@ -158,6 +162,14 @@\n       DefaultValue = DEFAULT_DISABLE_OUTPUTSTREAM_FLUSH)\n   private boolean disableOutputStreamFlush;\n \n+  @BooleanConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_ENABLE_APPEND_WITH_FLUSH,\n+      DefaultValue = DEFAULT_ENABLE_APPEND_WITH_FLUSH)\n+  private boolean enableAppendWithFlush;\n+\n+  @BooleanConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_DISABLE_OUTPUTSTREAM_FLUSH,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODM4OTQ2MQ=="}, "originalCommit": {"oid": "8590f79e9ed00f055b2fd822c6c964943dddbb91"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0MTA4NTY3OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQxNzoxNTo0MVrOFo3W6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQwNTo1MDo0MVrOFpsN4w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODM5NDM0Nw==", "bodyText": "Test the change against an account in prod tenant which still doesnt have Dec-12 bits .", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r378394347", "createdAt": "2020-02-12T17:15:41Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java", "diffHunk": "@@ -254,6 +273,9 @@ public AbfsRestOperation createPath(final String path, final boolean isFile, fin\n \n     final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\n     abfsUriQueryBuilder.addQuery(QUERY_PARAM_RESOURCE, isFile ? FILE : DIRECTORY);\n+    if (appendBlob) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8590f79e9ed00f055b2fd822c6c964943dddbb91"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI2MDM4Nw==", "bodyText": "tested the changes in a test tenant, with rest version older than Dec 12.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r379260387", "createdAt": "2020-02-14T05:50:41Z", "author": {"login": "ishaniahuja"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java", "diffHunk": "@@ -254,6 +273,9 @@ public AbfsRestOperation createPath(final String path, final boolean isFile, fin\n \n     final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\n     abfsUriQueryBuilder.addQuery(QUERY_PARAM_RESOURCE, isFile ? FILE : DIRECTORY);\n+    if (appendBlob) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODM5NDM0Nw=="}, "originalCommit": {"oid": "8590f79e9ed00f055b2fd822c6c964943dddbb91"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0MTEyMzg1OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQxNzoyNjoxN1rOFo3uvg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQwNTo1MTo1M1rOFpsOuw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODQwMDQ0Ng==", "bodyText": "Could you explain the logic here. Please add to code comments.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r378400446", "createdAt": "2020-02-12T17:26:17Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -307,10 +318,18 @@ public Void call() throws Exception {\n           perfInfo.registerSuccess(true);\n           return null;\n         }\n+        if (flush) {\n+          while(lastTotalAppendOffset <  lastFlushOffset);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8590f79e9ed00f055b2fd822c6c964943dddbb91"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI2MDYwMw==", "bodyText": "I have added comment in the code. This code ensures that the append+flush (new API is called only once all the append and flush have finished and there are no pending appends in the driver.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r379260603", "createdAt": "2020-02-14T05:51:53Z", "author": {"login": "ishaniahuja"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -307,10 +318,18 @@ public Void call() throws Exception {\n           perfInfo.registerSuccess(true);\n           return null;\n         }\n+        if (flush) {\n+          while(lastTotalAppendOffset <  lastFlushOffset);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODQwMDQ0Ng=="}, "originalCommit": {"oid": "8590f79e9ed00f055b2fd822c6c964943dddbb91"}, "originalPosition": 85}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5NDEyNjU2OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wMlQxNTo1Nzo1MVrOFwk9sQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wMlQxNTo1Nzo1MVrOFwk9sQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjQ4MTU4NQ==", "bodyText": "javadocs should be above the new option", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r386481585", "createdAt": "2020-03-02T15:57:51Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java", "diffHunk": "@@ -60,6 +61,12 @@\n    *  documentation does not have such expectations of data being persisted.\n    *  Default value of this config is true. **/\n   public static final String FS_AZURE_DISABLE_OUTPUTSTREAM_FLUSH = \"fs.azure.disable.outputstream.flush\";\n+  public static final String FS_AZURE_ENABLE_APPEND_WITH_FLUSH = \"fs.azure.enable.appendwithflush\";\n+  /** Provides a config control to disable or enable OutputStream Flush API", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0840c8aa5b2371a79705e8183fe4381aba0d0e52"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5NDEzNzg4OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/site/markdown/abfs.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wMlQxNjowMDoyOVrOFwlErQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wMlQxNjowMDoyOVrOFwlErQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjQ4MzM3Mw==", "bodyText": "should be nested (i.e. ####); duplicate name will confuse link generation. Just cut the \"a name\" tag from the second line", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r386483373", "createdAt": "2020-03-02T16:00:29Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/site/markdown/abfs.md", "diffHunk": "@@ -643,6 +643,10 @@ Consult the javadocs for `org.apache.hadoop.fs.azurebfs.constants.ConfigurationK\n `org.apache.hadoop.fs.azurebfs.AbfsConfiguration` for the full list\n of configuration options and their default values.\n \n+### <a name=\"appendblobkeyconfigoptions\"></a> Append Blob Directories Options\n+### <a name=\"appendblobkeyconfigoptions\"></a> Config `fs.azure.appendblob.key` provides", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0840c8aa5b2371a79705e8183fe4381aba0d0e52"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5NDE1MDgwOnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wMlQxNjowMzoyN1rOFwlMaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wMlQxNjowMzoyN1rOFwlMaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjQ4NTM1Mw==", "bodyText": "org.apache imports need to go into their own block just above any static imports, ordering imports\njava., javax.\n--\nother\n---\norg.apache\n---\nstatic\n\nThis is to try and keep cherry-picking under control.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r386485353", "createdAt": "2020-03-02T16:03:27Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,349 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0840c8aa5b2371a79705e8183fe4381aba0d0e52"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5NDE1OTIwOnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wMlQxNjowNTozMlrOFwlRoQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wMlQxNjowNTozMlrOFwlRoQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjQ4NjY4OQ==", "bodyText": "are these used", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r386486689", "createdAt": "2020-03-02T16:05:32Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,349 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.conf.Configuration;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.mockito.ArgumentCaptor;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.verifyNoMoreInteractions;\n+import static org.mockito.Mockito.*;\n+/**\n+ * Test useragent of abfs client.\n+ *\n+ */\n+public final class TestAbfsOutputStream {\n+\n+  private static int bufferSize = 4096;\n+  private static int writeSize = 1000;\n+  private static String path = \"~/testpath\";\n+  private final String globalKey = \"fs.azure.configuration\";\n+  private final String accountName1 = \"account1\";\n+  private final String accountKey1 = globalKey + \".\" + accountName1;\n+  private final String accountValue1 = \"one\";\n+\n+  @Test\n+  public void verifyShortWriteRequest() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[writeSize];\n+    new Random().nextBytes(b);\n+    out.write(b);\n+    out.hsync();\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    final byte[] b1 = new byte[2*writeSize];\n+    new Random().nextBytes(b1);\n+    out.write(b1);\n+    out.flush();\n+    out.hflush();\n+\n+    out.hsync();\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(0), Long.valueOf(writeSize)), acLong.getAllValues());\n+    //flush=true, close=false, flush=true, close=false\n+    Assert.assertEquals(Arrays.asList(true, false, true, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0,writeSize, 0, 2*writeSize), acInt.getAllValues());\n+\n+    //verifyNoMoreInteractions(client);\n+\n+  }\n+\n+  @Test\n+  public void verifyWriteRequest() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[writeSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 5; i++) {\n+      out.write(b);\n+    }\n+    out.close();\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize)), acLong.getAllValues());\n+    //flush=false,close=false, flush=true,close=true\n+    Assert.assertEquals(Arrays.asList(false, false, true, true), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, 5*writeSize-bufferSize), acInt.getAllValues());\n+\n+    //verifyNoMoreInteractions(client);\n+  }\n+\n+  @Test\n+  public void verifyWriteRequestOfBufferSizeAndClose() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+    when(client.flush(anyString(), anyLong(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    out.close();\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(new HashSet<Long>(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize))), new HashSet<Long>(acLong.getAllValues()));\n+    //flush=false, close=false, flush=false, close=false\n+    Assert.assertEquals(Arrays.asList(false, false, false, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, bufferSize), acInt.getAllValues());\n+\n+    ArgumentCaptor<String> acFlushString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acFlushLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Boolean> acFlushBool = ArgumentCaptor.forClass(Boolean.class);\n+\n+    verify(client, times(1)).flush(acFlushString.capture(), acFlushLong.capture(), acFlushBool.capture(), acFlushBool.capture());\n+    Assert.assertEquals(Arrays.asList(path) , acFlushString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(2*bufferSize)), acFlushLong.getAllValues());\n+    Assert.assertEquals(Arrays.asList(false, true), acFlushBool.getAllValues());\n+\n+//    verifyNoMoreInteractions(client);\n+  }\n+\n+  @Test\n+  public void verifyWriteRequestOfBufferSize() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    Thread.sleep(1000);\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(new HashSet<Long>(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize))), new HashSet<Long>(acLong.getAllValues()));\n+    //flush=false, close=false, flush=false, close=false\n+    Assert.assertEquals(Arrays.asList(false, false, false, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, bufferSize), acInt.getAllValues());\n+\n+ //   verifyNoMoreInteractions(client);\n+  }\n+\n+  @Test\n+  public void verifyWriteRequestOfBufferSizeWithAppendBlob() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    Thread.sleep(1000);\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize)), acLong.getAllValues());\n+    //flush=false, close=false, flush=false, close=false\n+    Assert.assertEquals(Arrays.asList(false, false, false, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, bufferSize), acInt.getAllValues());\n+\n+    //verifyNoMoreInteractions(client);\n+  }\n+\n+  @Test\n+  public void verifyWriteRequestOfBufferSizeAndHFlush() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+    when(client.flush(anyString(), anyLong(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    out.hflush();\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(new HashSet<Long>(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize))), new HashSet<Long>(acLong.getAllValues()));\n+    //flush=false, close=false, flush=false, close=false\n+    Assert.assertEquals(Arrays.asList(false, false, false, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, bufferSize), acInt.getAllValues());\n+\n+    ArgumentCaptor<String> acFlushString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acFlushLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Boolean> acFlushBool = ArgumentCaptor.forClass(Boolean.class);\n+\n+    verify(client, times(1)).flush(acFlushString.capture(), acFlushLong.capture(), acFlushBool.capture(), acFlushBool.capture());\n+    Assert.assertEquals(Arrays.asList(path) , acFlushString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(2*bufferSize)), acFlushLong.getAllValues());\n+    Assert.assertEquals(Arrays.asList(false, false), acFlushBool.getAllValues());\n+\n+    //verifyNoMoreInteractions(client);\n+  }\n+\n+  @Test\n+  public void verifyWriteRequestOfBufferSizeAndFlush() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    out.flush();\n+    Thread.sleep(1000);\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(new HashSet<Long>(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize))), new HashSet<Long>(acLong.getAllValues()));\n+    //flush=false, close=false, flush=false, close=false\n+    Assert.assertEquals(Arrays.asList(false, false, false, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, bufferSize), acInt.getAllValues());\n+\n+    ArgumentCaptor<String> acFlushString = ArgumentCaptor.forClass(String.class);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0840c8aa5b2371a79705e8183fe4381aba0d0e52"}, "originalPosition": 343}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5NDE2Nzc4OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wMlQxNjowNzozOFrOFwlW-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wMlQxNjowNzozOFrOFwlW-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjQ4ODA1OA==", "bodyText": "Mockito tests are always a maintenance pain because they are so brittle and sho hard to understand what is going on -for example, here I couldn't really understand any of the tests. Could you add some more detail as a comment for each test -at least to level of what each test case is looking for.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r386488058", "createdAt": "2020-03-02T16:07:38Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,349 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.conf.Configuration;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.mockito.ArgumentCaptor;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.verifyNoMoreInteractions;\n+import static org.mockito.Mockito.*;\n+/**\n+ * Test useragent of abfs client.\n+ *\n+ */\n+public final class TestAbfsOutputStream {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0840c8aa5b2371a79705e8183fe4381aba0d0e52"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5NDE2ODM0OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wMlQxNjowNzo0N1rOFwlXVQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wMlQxNjowNzo0N1rOFwlXVQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjQ4ODE0OQ==", "bodyText": "check this", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r386488149", "createdAt": "2020-03-02T16:07:47Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,349 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.conf.Configuration;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.mockito.ArgumentCaptor;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.verifyNoMoreInteractions;\n+import static org.mockito.Mockito.*;\n+/**\n+ * Test useragent of abfs client.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0840c8aa5b2371a79705e8183fe4381aba0d0e52"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQwMjA4MTY2OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQxNTowNzoyNlrOFxw87g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQxNTowNzoyNlrOFxw87g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzcyNjU3NA==", "bodyText": "nit: why not just 0?", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r387726574", "createdAt": "2020-03-04T15:07:26Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "diffHunk": "@@ -404,18 +425,25 @@ public OutputStream createFile(final Path path, final boolean overwrite, final F\n               umask.toString(),\n               isNamespaceEnabled);\n \n-      final AbfsRestOperation op = client.createPath(AbfsHttpConstants.FORWARD_SLASH + getRelativePath(path), true, overwrite,\n-              isNamespaceEnabled ? getOctalNotation(permission) : null,\n-              isNamespaceEnabled ? getOctalNotation(umask) : null);\n-      perfInfo.registerResult(op.getResult()).registerSuccess(true);\n+        boolean appendBlob = false;\n+        if (isAppendBlobKey(path.toString())) {\n+          appendBlob = true;\n+        }\n+\n+      client.createPath(AbfsHttpConstants.FORWARD_SLASH + getRelativePath(path), true, overwrite,\n+          isNamespaceEnabled ? getOctalNotation(permission) : null,\n+          isNamespaceEnabled ? getOctalNotation(umask) : null,\n+          appendBlob);\n \n       return new AbfsOutputStream(\n-              client,\n-              AbfsHttpConstants.FORWARD_SLASH + getRelativePath(path),\n-              0,\n-              abfsConfiguration.getWriteBufferSize(),\n-              abfsConfiguration.isFlushEnabled(),\n-              abfsConfiguration.isOutputStreamFlushDisabled());\n+          client,\n+          AbfsHttpConstants.FORWARD_SLASH + getRelativePath(path),\n+          Long.valueOf(0),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "520f504efa052e72645c36c9f1fde76574dc2bf7"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQwNDMzNTYwOnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemE2E.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNVQwNTozMzowMFrOFyG4JA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQwODo1Mzo1N1rOFze2iQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODA4NTc5Ng==", "bodyText": "Could you please point me to the check in close() call that prevents server call ?\nAlso have the tests been run on these different configurations:\n\nWith appenblob feature enabled\n\n\nWith and Without Namespace enabled account : fs.azure.enable.appendwithflush true and false\n\n\nWith appenblob feature disabled\n\n\nWith and Without Namespace enabled account : fs.azure.enable.appendwithflush true and false", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r388085796", "createdAt": "2020-03-05T05:33:00Z", "author": {"login": "snvijaya"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemE2E.java", "diffHunk": "@@ -209,9 +209,10 @@ public void testFlushWithFileNotFoundException() throws Exception {\n \n     fs.delete(testFilePath, true);\n     assertFalse(fs.exists(testFilePath));\n+    AbfsConfiguration configuration = this.getConfiguration();\n \n-    intercept(FileNotFoundException.class,\n-            () -> stream.close());\n+    //With the new code, it would not trigger a call to the backend", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "520f504efa052e72645c36c9f1fde76574dc2bf7"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTUyNzE3Nw==", "bodyText": "I updated the comment with more details. You can also look at the code to understand the behavior. AbfsOutputStream close(). Yes I have tested the different configurations (appendblob, blockblob, append+flush enabled, append+flush disabled, with and without namespace disabled account.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r389527177", "createdAt": "2020-03-09T08:53:57Z", "author": {"login": "ishaniahuja"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemE2E.java", "diffHunk": "@@ -209,9 +209,10 @@ public void testFlushWithFileNotFoundException() throws Exception {\n \n     fs.delete(testFilePath, true);\n     assertFalse(fs.exists(testFilePath));\n+    AbfsConfiguration configuration = this.getConfiguration();\n \n-    intercept(FileNotFoundException.class,\n-            () -> stream.close());\n+    //With the new code, it would not trigger a call to the backend", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODA4NTc5Ng=="}, "originalCommit": {"oid": "520f504efa052e72645c36c9f1fde76574dc2bf7"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyMDA5NzcyOnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/pom.xml", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQxODoxMjoxNVrOF0bIBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQxODoxMjoxNVrOF0bIBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDUxNDY5Mw==", "bodyText": "version number should not be defined here, please do not add this change.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r390514693", "createdAt": "2020-03-10T18:12:15Z", "author": {"login": "DadanielZ"}, "path": "hadoop-tools/hadoop-azure/pom.xml", "diffHunk": "@@ -260,6 +260,7 @@\n     <dependency>\n       <groupId>org.mockito</groupId>\n       <artifactId>mockito-core</artifactId>\n+      <version>3.3.0</version>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a5d7e0a49937cc8b0cc33560c87de89d137499f9"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQzMzc2OTE1OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNVQwOTowNjozNlrOF2dvZA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QwNjo1OTo0OFrOF3P5iA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjY1NDY5Mg==", "bodyText": "This comment looks misplaced", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r392654692", "createdAt": "2020-03-15T09:06:36Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java", "diffHunk": "@@ -60,6 +61,15 @@\n    *  documentation does not have such expectations of data being persisted.\n    *  Default value of this config is true. **/\n   public static final String FS_AZURE_DISABLE_OUTPUTSTREAM_FLUSH = \"fs.azure.disable.outputstream.flush\";\n+  /** Provides a config control to enable OutputStream AppendWithFlush API\n+   *  operations in AbfsOutputStream.\n+   *  Default value of this config is true. **/\n+  public static final String FS_AZURE_ENABLE_APPEND_WITH_FLUSH = \"fs.azure.enable.appendwithflush\";\n+  /** Provides a config control to disable or enable OutputStream Flush API", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzQ3NjQ4OA==", "bodyText": "@snvijaya Doesn't this look misplaced?\n@ishaniahuja Check with @snvijaya and then resolve this one", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r393476488", "createdAt": "2020-03-17T06:59:48Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java", "diffHunk": "@@ -60,6 +61,15 @@\n    *  documentation does not have such expectations of data being persisted.\n    *  Default value of this config is true. **/\n   public static final String FS_AZURE_DISABLE_OUTPUTSTREAM_FLUSH = \"fs.azure.disable.outputstream.flush\";\n+  /** Provides a config control to enable OutputStream AppendWithFlush API\n+   *  operations in AbfsOutputStream.\n+   *  Default value of this config is true. **/\n+  public static final String FS_AZURE_ENABLE_APPEND_WITH_FLUSH = \"fs.azure.enable.appendwithflush\";\n+  /** Provides a config control to disable or enable OutputStream Flush API", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjY1NDY5Mg=="}, "originalCommit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQzMzc3MDcyOnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNVQwOToxMDoxNlrOF2dwNg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QwMzoxODozNFrOF3Mqbg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjY1NDkwMg==", "bodyText": "Shouldn't boolean be named with the prefix \"is\"", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r392654902", "createdAt": "2020-03-15T09:10:16Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -55,6 +55,8 @@\n   private boolean closed;\n   private boolean supportFlush;\n   private boolean disableOutputStreamFlush;\n+  private boolean supportAppendWithFlush;\n+  private boolean appendBlob;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzQyMzQ3MA==", "bodyText": "it is the same as for other booleans in the file.", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r393423470", "createdAt": "2020-03-17T03:18:34Z", "author": {"login": "ishaniahuja"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -55,6 +55,8 @@\n   private boolean closed;\n   private boolean supportFlush;\n   private boolean disableOutputStreamFlush;\n+  private boolean supportAppendWithFlush;\n+  private boolean appendBlob;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjY1NDkwMg=="}, "originalCommit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQzMzc3Mjk5OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNVQwOToxNDowNVrOF2dxWw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNVQwOToxNDowNVrOF2dxWw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjY1NTE5NQ==", "bodyText": "Have the comment above the @test annotation", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r392655195", "createdAt": "2020-03-15T09:14:05Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,355 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.mockito.ArgumentCaptor;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.conf.Configuration;\n+\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.verifyNoMoreInteractions;\n+import static org.mockito.Mockito.*;\n+\n+public final class TestAbfsOutputStream {\n+\n+  private static int bufferSize = 4096;\n+  private static int writeSize = 1000;\n+  private static String path = \"~/testpath\";\n+  private final String globalKey = \"fs.azure.configuration\";\n+  private final String accountName1 = \"account1\";\n+  private final String accountKey1 = globalKey + \".\" + accountName1;\n+  private final String accountValue1 = \"one\";\n+\n+  @Test\n+  /**\n+   * The test verifies OutputStream shortwrite case(2000bytes write followed by flush, hflush, hsync) is making correct HTTP calls to the server\n+   */\n+  public void verifyShortWriteRequest() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[writeSize];\n+    new Random().nextBytes(b);\n+    out.write(b);\n+    out.hsync();\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    final byte[] b1 = new byte[2*writeSize];\n+    new Random().nextBytes(b1);\n+    out.write(b1);\n+    out.flush();\n+    out.hflush();\n+\n+    out.hsync();\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(0), Long.valueOf(writeSize)), acLong.getAllValues());\n+    //flush=true, close=false, flush=true, close=false\n+    Assert.assertEquals(Arrays.asList(true, false, true, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0,writeSize, 0, 2*writeSize), acInt.getAllValues());\n+\n+  }\n+\n+  @Test\n+  /**\n+   * The test verifies OutputStream Write of writeSize(1000 bytes) followed by a close is making correct HTTP calls to the server\n+   */\n+  public void verifyWriteRequest() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[writeSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 5; i++) {\n+      out.write(b);\n+    }\n+    out.close();\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize)), acLong.getAllValues());\n+    //flush=false,close=false, flush=true,close=true\n+    Assert.assertEquals(Arrays.asList(false, false, true, true), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, 5*writeSize-bufferSize), acInt.getAllValues());\n+\n+  }\n+\n+  @Test\n+  /**\n+   * The test verifies OutputStream Write of bufferSize(4KB) followed by a close is making correct HTTP calls to the server\n+   */\n+  public void verifyWriteRequestOfBufferSizeAndClose() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+    when(client.flush(anyString(), anyLong(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    out.close();\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(new HashSet<Long>(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize))), new HashSet<Long>(acLong.getAllValues()));\n+    //flush=false, close=false, flush=false, close=false\n+    Assert.assertEquals(Arrays.asList(false, false, false, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, bufferSize), acInt.getAllValues());\n+\n+    ArgumentCaptor<String> acFlushString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acFlushLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Boolean> acFlushBool = ArgumentCaptor.forClass(Boolean.class);\n+\n+    verify(client, times(1)).flush(acFlushString.capture(), acFlushLong.capture(), acFlushBool.capture(), acFlushBool.capture());\n+    Assert.assertEquals(Arrays.asList(path) , acFlushString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(2*bufferSize)), acFlushLong.getAllValues());\n+    Assert.assertEquals(Arrays.asList(false, true), acFlushBool.getAllValues());\n+\n+  }\n+\n+  @Test\n+  /**\n+   * The test verifies OutputStream Write of bufferSize(4KB) is making correct HTTP calls to the server\n+   */\n+  public void verifyWriteRequestOfBufferSize() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    Thread.sleep(1000);\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(new HashSet<Long>(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize))), new HashSet<Long>(acLong.getAllValues()));\n+    //flush=false, close=false, flush=false, close=false\n+    Assert.assertEquals(Arrays.asList(false, false, false, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, bufferSize), acInt.getAllValues());\n+\n+  }\n+\n+  @Test\n+  /**\n+   * The test verifies OutputStream Write of bufferSize(4KB) on a AppendBlob based stream is making correct HTTP calls to the server\n+   */\n+  public void verifyWriteRequestOfBufferSizeWithAppendBlob() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    Thread.sleep(1000);\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize)), acLong.getAllValues());\n+    //flush=false, close=false, flush=false, close=false\n+    Assert.assertEquals(Arrays.asList(false, false, false, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, bufferSize), acInt.getAllValues());\n+\n+  }\n+\n+  @Test\n+  /**\n+   * The test verifies OutputStream Write of bufferSize(4KB)  followed by a hflush call is making correct HTTP calls to the server\n+   */\n+  public void verifyWriteRequestOfBufferSizeAndHFlush() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+    when(client.flush(anyString(), anyLong(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    out.hflush();\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(new HashSet<Long>(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize))), new HashSet<Long>(acLong.getAllValues()));\n+    //flush=false, close=false, flush=false, close=false\n+    Assert.assertEquals(Arrays.asList(false, false, false, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, bufferSize), acInt.getAllValues());\n+\n+    ArgumentCaptor<String> acFlushString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acFlushLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Boolean> acFlushBool = ArgumentCaptor.forClass(Boolean.class);\n+\n+    verify(client, times(1)).flush(acFlushString.capture(), acFlushLong.capture(), acFlushBool.capture(), acFlushBool.capture());\n+    Assert.assertEquals(Arrays.asList(path) , acFlushString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(2*bufferSize)), acFlushLong.getAllValues());\n+    Assert.assertEquals(Arrays.asList(false, false), acFlushBool.getAllValues());\n+\n+  }\n+\n+  @Test\n+  /**", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "originalPosition": 316}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQzMzc3Nzk4OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNVQwOToyMzoyMFrOF2dz7A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNVQwOToyMzoyMFrOF2dz7A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjY1NTg1Mg==", "bodyText": "Pls add new line before the first import", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r392655852", "createdAt": "2020-03-15T09:23:20Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,355 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import java.util.Arrays;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQzMzc3OTAxOnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNVQwOToyNTozMlrOF2d0eQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QwODowMjo1NFrOF3RUlA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjY1NTk5Mw==", "bodyText": "Order and group the non-static imports in the order:\njava*\nany non org.apache imports\norg.apache imports", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r392655993", "createdAt": "2020-03-15T09:25:32Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,355 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzQ5OTc5Ng==", "bodyText": "Import order is still not fixed", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r393499796", "createdAt": "2020-03-17T08:02:54Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,355 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjY1NTk5Mw=="}, "originalCommit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQzMzc3OTE4OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNVQwOToyNTo0OVrOF2d0iw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNVQwOToyNTo0OVrOF2d0iw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjY1NjAxMQ==", "bodyText": "Same for static imports", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r392656011", "createdAt": "2020-03-15T09:25:49Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,355 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.mockito.ArgumentCaptor;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.conf.Configuration;\n+\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQzMzc3OTQ2OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNVQwOToyNjoxMVrOF2d0rA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QwODowNDozMVrOF3RXSQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjY1NjA0NA==", "bodyText": "Keep the comment above the @test annotation for all the tests", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r392656044", "createdAt": "2020-03-15T09:26:11Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,355 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.mockito.ArgumentCaptor;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.conf.Configuration;\n+\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.verifyNoMoreInteractions;\n+import static org.mockito.Mockito.*;\n+\n+public final class TestAbfsOutputStream {\n+\n+  private static int bufferSize = 4096;\n+  private static int writeSize = 1000;\n+  private static String path = \"~/testpath\";\n+  private final String globalKey = \"fs.azure.configuration\";\n+  private final String accountName1 = \"account1\";\n+  private final String accountKey1 = globalKey + \".\" + accountName1;\n+  private final String accountValue1 = \"one\";\n+\n+  @Test\n+  /**\n+   * The test verifies OutputStream shortwrite case(2000bytes write followed by flush, hflush, hsync) is making correct HTTP calls to the server", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzUwMDQ4OQ==", "bodyText": "Need to move @test below the comments", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r393500489", "createdAt": "2020-03-17T08:04:31Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,355 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.mockito.ArgumentCaptor;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.conf.Configuration;\n+\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.verifyNoMoreInteractions;\n+import static org.mockito.Mockito.*;\n+\n+public final class TestAbfsOutputStream {\n+\n+  private static int bufferSize = 4096;\n+  private static int writeSize = 1000;\n+  private static String path = \"~/testpath\";\n+  private final String globalKey = \"fs.azure.configuration\";\n+  private final String accountName1 = \"account1\";\n+  private final String accountKey1 = globalKey + \".\" + accountName1;\n+  private final String accountValue1 = \"one\";\n+\n+  @Test\n+  /**\n+   * The test verifies OutputStream shortwrite case(2000bytes write followed by flush, hflush, hsync) is making correct HTTP calls to the server", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjY1NjA0NA=="}, "originalCommit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQzNTY0MTA3OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxMDo1NzowMVrOF2u6nQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxMDo1NzowMVrOF2u6nQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNjA5Mw==", "bodyText": "Nit: Remove unused import", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r392936093", "createdAt": "2020-03-16T10:57:01Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,355 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.mockito.ArgumentCaptor;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.conf.Configuration;\n+\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.verifyNoMoreInteractions;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQzNTY0MzMzOnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxMDo1Nzo0MVrOF2u78g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxMDo1Nzo0MVrOF2u78g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNjQzNA==", "bodyText": "Nit: Using the '.*' form of import should be avoided", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r392936434", "createdAt": "2020-03-16T10:57:41Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,355 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.mockito.ArgumentCaptor;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.conf.Configuration;\n+\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.verifyNoMoreInteractions;\n+import static org.mockito.Mockito.*;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQzNTY0OTk1OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxMDo1OTozM1rOF2u_-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxMDo1OTozM1rOF2u_-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNzQ2Nw==", "bodyText": "Nit: These 2 can also be made constants", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r392937467", "createdAt": "2020-03-16T10:59:33Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,355 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.mockito.ArgumentCaptor;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.conf.Configuration;\n+\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.verifyNoMoreInteractions;\n+import static org.mockito.Mockito.*;\n+\n+public final class TestAbfsOutputStream {\n+\n+  private static int bufferSize = 4096;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQzNTY1MjU4OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxMTowMDoxOVrOF2vBkg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxMTowMDoxOVrOF2vBkg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNzg3NA==", "bodyText": "Nit: Line is longer than 160 characters\nTake care of the same in the entire file", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r392937874", "createdAt": "2020-03-16T11:00:19Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,355 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.mockito.ArgumentCaptor;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.conf.Configuration;\n+\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.verifyNoMoreInteractions;\n+import static org.mockito.Mockito.*;\n+\n+public final class TestAbfsOutputStream {\n+\n+  private static int bufferSize = 4096;\n+  private static int writeSize = 1000;\n+  private static String path = \"~/testpath\";\n+  private final String globalKey = \"fs.azure.configuration\";\n+  private final String accountName1 = \"account1\";\n+  private final String accountKey1 = globalKey + \".\" + accountName1;\n+  private final String accountValue1 = \"one\";\n+\n+  @Test\n+  /**\n+   * The test verifies OutputStream shortwrite case(2000bytes write followed by flush, hflush, hsync) is making correct HTTP calls to the server\n+   */\n+  public void verifyShortWriteRequest() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[writeSize];\n+    new Random().nextBytes(b);\n+    out.write(b);\n+    out.hsync();\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    final byte[] b1 = new byte[2*writeSize];\n+    new Random().nextBytes(b1);\n+    out.write(b1);\n+    out.flush();\n+    out.hflush();\n+\n+    out.hsync();\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "originalPosition": 81}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQzNTY1NjYxOnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxMTowMTozMFrOF2vEEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxMTowMTozMFrOF2vEEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzODUxNQ==", "bodyText": "Nit: ',' is preceded with whitespace\nTake care of the same in the entire file", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r392938515", "createdAt": "2020-03-16T11:01:30Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,355 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.mockito.ArgumentCaptor;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.conf.Configuration;\n+\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.verifyNoMoreInteractions;\n+import static org.mockito.Mockito.*;\n+\n+public final class TestAbfsOutputStream {\n+\n+  private static int bufferSize = 4096;\n+  private static int writeSize = 1000;\n+  private static String path = \"~/testpath\";\n+  private final String globalKey = \"fs.azure.configuration\";\n+  private final String accountName1 = \"account1\";\n+  private final String accountKey1 = globalKey + \".\" + accountName1;\n+  private final String accountValue1 = \"one\";\n+\n+  @Test\n+  /**\n+   * The test verifies OutputStream shortwrite case(2000bytes write followed by flush, hflush, hsync) is making correct HTTP calls to the server\n+   */\n+  public void verifyShortWriteRequest() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[writeSize];\n+    new Random().nextBytes(b);\n+    out.write(b);\n+    out.hsync();\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    final byte[] b1 = new byte[2*writeSize];\n+    new Random().nextBytes(b1);\n+    out.write(b1);\n+    out.flush();\n+    out.hflush();\n+\n+    out.hsync();\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "originalPosition": 82}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQzNTY1OTg0OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxMTowMjoxOFrOF2vF9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxMTowMjoxOFrOF2vF9A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzODk5Ng==", "bodyText": "Nit: ',' is not followed by whitespace. space before and after *\nTake care of the same in the entire file", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r392938996", "createdAt": "2020-03-16T11:02:18Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,355 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.mockito.ArgumentCaptor;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.conf.Configuration;\n+\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.verifyNoMoreInteractions;\n+import static org.mockito.Mockito.*;\n+\n+public final class TestAbfsOutputStream {\n+\n+  private static int bufferSize = 4096;\n+  private static int writeSize = 1000;\n+  private static String path = \"~/testpath\";\n+  private final String globalKey = \"fs.azure.configuration\";\n+  private final String accountName1 = \"account1\";\n+  private final String accountKey1 = globalKey + \".\" + accountName1;\n+  private final String accountValue1 = \"one\";\n+\n+  @Test\n+  /**\n+   * The test verifies OutputStream shortwrite case(2000bytes write followed by flush, hflush, hsync) is making correct HTTP calls to the server\n+   */\n+  public void verifyShortWriteRequest() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[writeSize];\n+    new Random().nextBytes(b);\n+    out.write(b);\n+    out.hsync();\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    final byte[] b1 = new byte[2*writeSize];\n+    new Random().nextBytes(b1);\n+    out.write(b1);\n+    out.flush();\n+    out.hflush();\n+\n+    out.hsync();\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(0), Long.valueOf(writeSize)), acLong.getAllValues());\n+    //flush=true, close=false, flush=true, close=false\n+    Assert.assertEquals(Arrays.asList(true, false, true, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0,writeSize, 0, 2*writeSize), acInt.getAllValues());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "originalPosition": 86}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQzNzkwNTU4OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQyMDo1MDozN1rOF3FSgQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQyMDo1MDozN1rOF3FSgQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzMwMjY1Nw==", "bodyText": "Can you look at moving to assertJ here; these declarations are complex enough they need one\nand add error strings via .describedAs()", "url": "https://github.com/apache/hadoop/pull/1790#discussion_r393302657", "createdAt": "2020-03-16T20:50:37Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -0,0 +1,355 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Random;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.mockito.ArgumentCaptor;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.conf.Configuration;\n+\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.verifyNoMoreInteractions;\n+import static org.mockito.Mockito.*;\n+\n+public final class TestAbfsOutputStream {\n+\n+  private static int bufferSize = 4096;\n+  private static int writeSize = 1000;\n+  private static String path = \"~/testpath\";\n+  private final String globalKey = \"fs.azure.configuration\";\n+  private final String accountName1 = \"account1\";\n+  private final String accountKey1 = globalKey + \".\" + accountName1;\n+  private final String accountValue1 = \"one\";\n+\n+  @Test\n+  /**\n+   * The test verifies OutputStream shortwrite case(2000bytes write followed by flush, hflush, hsync) is making correct HTTP calls to the server\n+   */\n+  public void verifyShortWriteRequest() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[writeSize];\n+    new Random().nextBytes(b);\n+    out.write(b);\n+    out.hsync();\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    final byte[] b1 = new byte[2*writeSize];\n+    new Random().nextBytes(b1);\n+    out.write(b1);\n+    out.flush();\n+    out.hflush();\n+\n+    out.hsync();\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(0), Long.valueOf(writeSize)), acLong.getAllValues());\n+    //flush=true, close=false, flush=true, close=false\n+    Assert.assertEquals(Arrays.asList(true, false, true, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0,writeSize, 0, 2*writeSize), acInt.getAllValues());\n+\n+  }\n+\n+  @Test\n+  /**\n+   * The test verifies OutputStream Write of writeSize(1000 bytes) followed by a close is making correct HTTP calls to the server\n+   */\n+  public void verifyWriteRequest() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[writeSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 5; i++) {\n+      out.write(b);\n+    }\n+    out.close();\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize)), acLong.getAllValues());\n+    //flush=false,close=false, flush=true,close=true\n+    Assert.assertEquals(Arrays.asList(false, false, true, true), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, 5*writeSize-bufferSize), acInt.getAllValues());\n+\n+  }\n+\n+  @Test\n+  /**\n+   * The test verifies OutputStream Write of bufferSize(4KB) followed by a close is making correct HTTP calls to the server\n+   */\n+  public void verifyWriteRequestOfBufferSizeAndClose() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+    when(client.flush(anyString(), anyLong(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    out.close();\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(new HashSet<Long>(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize))), new HashSet<Long>(acLong.getAllValues()));\n+    //flush=false, close=false, flush=false, close=false\n+    Assert.assertEquals(Arrays.asList(false, false, false, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, bufferSize), acInt.getAllValues());\n+\n+    ArgumentCaptor<String> acFlushString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acFlushLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Boolean> acFlushBool = ArgumentCaptor.forClass(Boolean.class);\n+\n+    verify(client, times(1)).flush(acFlushString.capture(), acFlushLong.capture(), acFlushBool.capture(), acFlushBool.capture());\n+    Assert.assertEquals(Arrays.asList(path) , acFlushString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(2*bufferSize)), acFlushLong.getAllValues());\n+    Assert.assertEquals(Arrays.asList(false, true), acFlushBool.getAllValues());\n+\n+  }\n+\n+  @Test\n+  /**\n+   * The test verifies OutputStream Write of bufferSize(4KB) is making correct HTTP calls to the server\n+   */\n+  public void verifyWriteRequestOfBufferSize() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    Thread.sleep(1000);\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(new HashSet<Long>(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize))), new HashSet<Long>(acLong.getAllValues()));\n+    //flush=false, close=false, flush=false, close=false\n+    Assert.assertEquals(Arrays.asList(false, false, false, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, bufferSize), acInt.getAllValues());\n+\n+  }\n+\n+  @Test\n+  /**\n+   * The test verifies OutputStream Write of bufferSize(4KB) on a AppendBlob based stream is making correct HTTP calls to the server\n+   */\n+  public void verifyWriteRequestOfBufferSizeWithAppendBlob() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    Thread.sleep(1000);\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize)), acLong.getAllValues());\n+    //flush=false, close=false, flush=false, close=false\n+    Assert.assertEquals(Arrays.asList(false, false, false, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, bufferSize), acInt.getAllValues());\n+\n+  }\n+\n+  @Test\n+  /**\n+   * The test verifies OutputStream Write of bufferSize(4KB)  followed by a hflush call is making correct HTTP calls to the server\n+   */\n+  public void verifyWriteRequestOfBufferSizeAndHFlush() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+    when(client.flush(anyString(), anyLong(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    out.hflush();\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());\n+    Assert.assertEquals(new HashSet<Long>(Arrays.asList(Long.valueOf(0), Long.valueOf(bufferSize))), new HashSet<Long>(acLong.getAllValues()));\n+    //flush=false, close=false, flush=false, close=false\n+    Assert.assertEquals(Arrays.asList(false, false, false, false), acBool.getAllValues());\n+    Assert.assertEquals(Arrays.asList(0, bufferSize, 0, bufferSize), acInt.getAllValues());\n+\n+    ArgumentCaptor<String> acFlushString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acFlushLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Boolean> acFlushBool = ArgumentCaptor.forClass(Boolean.class);\n+\n+    verify(client, times(1)).flush(acFlushString.capture(), acFlushLong.capture(), acFlushBool.capture(), acFlushBool.capture());\n+    Assert.assertEquals(Arrays.asList(path) , acFlushString.getAllValues());\n+    Assert.assertEquals(Arrays.asList(Long.valueOf(2*bufferSize)), acFlushLong.getAllValues());\n+    Assert.assertEquals(Arrays.asList(false, false), acFlushBool.getAllValues());\n+\n+  }\n+\n+  @Test\n+  /**\n+   * The test verifies OutputStream Write of bufferSize(4KB)  followed by a flush call is making correct HTTP calls to the server\n+   */\n+  public void verifyWriteRequestOfBufferSizeAndFlush() throws Exception {\n+\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsConfiguration abfsConf;\n+    final Configuration conf = new Configuration();\n+    conf.set(accountKey1, accountValue1);\n+    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.append(anyString(), anyLong(), any(byte[].class), anyInt(), anyInt(), anyBoolean(), anyBoolean())).thenReturn(op);\n+\n+    AbfsOutputStream out = new AbfsOutputStream(client, path, 0, bufferSize, true, false, true, false);\n+    final byte[] b = new byte[bufferSize];\n+    new Random().nextBytes(b);\n+\n+    for (int i = 0; i < 2; i++) {\n+      out.write(b);\n+    }\n+    out.flush();\n+    Thread.sleep(1000);\n+\n+    ArgumentCaptor<String> acString = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> acLong = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<Integer> acInt = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Boolean> acBool = ArgumentCaptor.forClass(Boolean.class);\n+    ArgumentCaptor<byte[]> acByteArray = ArgumentCaptor.forClass(byte[].class);\n+\n+    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acInt.capture(), acInt.capture(), acBool.capture(), acBool.capture());\n+    Assert.assertEquals(Arrays.asList(path, path) , acString.getAllValues());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be48931febe59c03becc9c8b8505bb41a65d5a68"}, "originalPosition": 348}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3671, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}