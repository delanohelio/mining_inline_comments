{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU3MTMxODQ4", "number": 2172, "reviewThreads": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwNDozMDo1NFrOETFvaw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQxMzo1ODoyOVrOEYeupA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NDUyNDU5OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirRenameOp.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwNDozMDo1NFrOG4oKLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxMDozNzozM1rOG4ytGw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjAzMTQwNA==", "bodyText": "There are 2 callers of      fsd.ezManager.checkMoveValidity(srcIIP, dstIIP);", "url": "https://github.com/apache/hadoop/pull/2172#discussion_r462031404", "createdAt": "2020-07-29T04:30:54Z", "author": {"login": "mukul1987"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirRenameOp.java", "diffHunk": "@@ -193,7 +194,7 @@ static INodesInPath unprotectedRenameTo(FSDirectory fsd,\n     }\n \n     validateNestSnapshot(fsd, src, dstParent.asDirectory(), snapshottableDirs);\n-\n+    FSDirSnapshotOp.checkUnderSameSnapshottableRoot(fsd, srcIIP, dstIIP);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da5c24a1523fa177e920a2bf8f1232a466d48f34"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjAzMTU2NQ==", "bodyText": "FSDirSnapshotOp.checkUnderSameSnapshottableRoot(fsd, srcIIP, dstIIP); lets add snapshot rename check at the same places.", "url": "https://github.com/apache/hadoop/pull/2172#discussion_r462031565", "createdAt": "2020-07-29T04:31:35Z", "author": {"login": "mukul1987"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirRenameOp.java", "diffHunk": "@@ -193,7 +194,7 @@ static INodesInPath unprotectedRenameTo(FSDirectory fsd,\n     }\n \n     validateNestSnapshot(fsd, src, dstParent.asDirectory(), snapshottableDirs);\n-\n+    FSDirSnapshotOp.checkUnderSameSnapshottableRoot(fsd, srcIIP, dstIIP);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjAzMTQwNA=="}, "originalCommit": {"oid": "da5c24a1523fa177e920a2bf8f1232a466d48f34"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjA4MTI0Mw==", "bodyText": "SnapshotRename check should happen irrespective of orderedDeletion config is set or not. Plus snapshotRename will always within the snapshottable root otherwise will fail irrespective of the config. Also, if just directories are marked snapshottable but snapshots don't exist, the rename will still fail across snapshottable roots but snashot rename is not possible in such case. The two checks seem to be mutually exclusive to me.\nAny specific reason to merge these two checks at once?", "url": "https://github.com/apache/hadoop/pull/2172#discussion_r462081243", "createdAt": "2020-07-29T07:01:21Z", "author": {"login": "bshashikant"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirRenameOp.java", "diffHunk": "@@ -193,7 +194,7 @@ static INodesInPath unprotectedRenameTo(FSDirectory fsd,\n     }\n \n     validateNestSnapshot(fsd, src, dstParent.asDirectory(), snapshottableDirs);\n-\n+    FSDirSnapshotOp.checkUnderSameSnapshottableRoot(fsd, srcIIP, dstIIP);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjAzMTQwNA=="}, "originalCommit": {"oid": "da5c24a1523fa177e920a2bf8f1232a466d48f34"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjA4MjEwNQ==", "bodyText": "There are 2 callers of fsd.ezManager.checkMoveValidity(srcIIP, dstIIP);\nFSDirSnapshotOp.checkUnderSameSnapshottableRoo() is also called from same two functions.", "url": "https://github.com/apache/hadoop/pull/2172#discussion_r462082105", "createdAt": "2020-07-29T07:03:06Z", "author": {"login": "bshashikant"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirRenameOp.java", "diffHunk": "@@ -193,7 +194,7 @@ static INodesInPath unprotectedRenameTo(FSDirectory fsd,\n     }\n \n     validateNestSnapshot(fsd, src, dstParent.asDirectory(), snapshottableDirs);\n-\n+    FSDirSnapshotOp.checkUnderSameSnapshottableRoot(fsd, srcIIP, dstIIP);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjAzMTQwNA=="}, "originalCommit": {"oid": "da5c24a1523fa177e920a2bf8f1232a466d48f34"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjIwNDE4Nw==", "bodyText": "Sorry I wasn't able to notice this earlier.", "url": "https://github.com/apache/hadoop/pull/2172#discussion_r462204187", "createdAt": "2020-07-29T10:37:33Z", "author": {"login": "mukul1987"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirRenameOp.java", "diffHunk": "@@ -193,7 +194,7 @@ static INodesInPath unprotectedRenameTo(FSDirectory fsd,\n     }\n \n     validateNestSnapshot(fsd, src, dstParent.asDirectory(), snapshottableDirs);\n-\n+    FSDirSnapshotOp.checkUnderSameSnapshottableRoot(fsd, srcIIP, dstIIP);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjAzMTQwNA=="}, "originalCommit": {"oid": "da5c24a1523fa177e920a2bf8f1232a466d48f34"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NDUzNDc4OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirSnapshotOp.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwNDozNjo0NlrOG4oP6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwNzowMzoxOVrOG4rQmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjAzMjg3Mw==", "bodyText": "we can replace snapshotManager.getSnapshottableAncestorDir(srcIIP))  with src here.", "url": "https://github.com/apache/hadoop/pull/2172#discussion_r462032873", "createdAt": "2020-07-29T04:36:46Z", "author": {"login": "mukul1987"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirSnapshotOp.java", "diffHunk": "@@ -341,4 +341,24 @@ static void checkSnapshot(FSDirectory fsd, INodesInPath iip,\n       checkSnapshot(iip.getLastINode(), snapshottableDirs);\n     }\n   }\n+\n+  static void checkUnderSameSnapshottableRoot(FSDirectory fsd,\n+      INodesInPath srcIIP, INodesInPath dstIIP) throws IOException {\n+    // Ensure rename out of a snapshottable root is not permitted if ordered\n+    // snapshot deletion feature is enabled\n+    if (fsd.isSnapshotDeletionOrdered()) {\n+      SnapshotManager snapshotManager = fsd.getFSNamesystem().\n+          getSnapshotManager();\n+      String errMsg = \"Source \" + srcIIP.getPath() +\n+          \" and dest \" + dstIIP.getPath() + \" are not under \" +\n+          \"the same snapshot root.\";\n+      INodeDirectory src = snapshotManager.\n+          getSnapshottableAncestorDir(srcIIP);\n+      INodeDirectory dst = snapshotManager.getSnapshottableAncestorDir(dstIIP);\n+      if (!(dstIIP.isDescendant(snapshotManager.\n+          getSnapshottableAncestorDir(srcIIP)))) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da5c24a1523fa177e920a2bf8f1232a466d48f34"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjA4MjIwMQ==", "bodyText": "done", "url": "https://github.com/apache/hadoop/pull/2172#discussion_r462082201", "createdAt": "2020-07-29T07:03:19Z", "author": {"login": "bshashikant"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirSnapshotOp.java", "diffHunk": "@@ -341,4 +341,24 @@ static void checkSnapshot(FSDirectory fsd, INodesInPath iip,\n       checkSnapshot(iip.getLastINode(), snapshottableDirs);\n     }\n   }\n+\n+  static void checkUnderSameSnapshottableRoot(FSDirectory fsd,\n+      INodesInPath srcIIP, INodesInPath dstIIP) throws IOException {\n+    // Ensure rename out of a snapshottable root is not permitted if ordered\n+    // snapshot deletion feature is enabled\n+    if (fsd.isSnapshotDeletionOrdered()) {\n+      SnapshotManager snapshotManager = fsd.getFSNamesystem().\n+          getSnapshotManager();\n+      String errMsg = \"Source \" + srcIIP.getPath() +\n+          \" and dest \" + dstIIP.getPath() + \" are not under \" +\n+          \"the same snapshot root.\";\n+      INodeDirectory src = snapshotManager.\n+          getSnapshottableAncestorDir(srcIIP);\n+      INodeDirectory dst = snapshotManager.getSnapshottableAncestorDir(dstIIP);\n+      if (!(dstIIP.isDescendant(snapshotManager.\n+          getSnapshottableAncestorDir(srcIIP)))) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjAzMjg3Mw=="}, "originalCommit": {"oid": "da5c24a1523fa177e920a2bf8f1232a466d48f34"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NDc0ODc1OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestRenameWithOrderedSnapshotDeletion.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwNjoyNDoyNVrOG4qPEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwNzo0NjowOVrOG4smXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjA2NTQyNw==", "bodyText": "Add Assert.equals or Assert true here ?", "url": "https://github.com/apache/hadoop/pull/2172#discussion_r462065427", "createdAt": "2020-07-29T06:24:25Z", "author": {"login": "mukul1987"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestRenameWithOrderedSnapshotDeletion.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.namenode;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.DFSTestUtil;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+\n+import java.io.IOException;\n+\n+import static org.apache.hadoop.hdfs.DFSConfigKeys.\n+    DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED;\n+\n+/**\n+ * Test Rename with ordered snapshot deletion.\n+ */\n+public class TestRenameWithOrderedSnapshotDeletion {\n+  private final Path snapshottableDir\n+      = new Path(\"/\" + getClass().getSimpleName());\n+  private DistributedFileSystem hdfs;\n+  private MiniDFSCluster cluster;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    final Configuration conf = new Configuration();\n+    conf.setBoolean(DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED, true);\n+\n+    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(0).build();\n+    cluster.waitActive();\n+    hdfs = cluster.getFileSystem();\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n+  }\n+\n+  @Test(timeout = 60000)\n+  public void testRename() throws Exception {\n+    final Path dir1 = new Path(\"/dir1\");\n+    final Path dir2 = new Path(\"/dir2\");\n+    final Path sub0 = new Path(snapshottableDir, \"sub0\");\n+    hdfs.mkdirs(sub0);\n+    final Path file1 = new Path(dir1, \"file1\");\n+    final Path file2 = new Path(sub0, \"file2\");\n+    hdfs.mkdirs(snapshottableDir);\n+    hdfs.mkdirs(dir1);\n+    hdfs.mkdirs(dir2);\n+    hdfs.mkdirs(sub0);\n+    DFSTestUtil.createFile(hdfs, file1, 0, (short) 1, 0);\n+    DFSTestUtil.createFile(hdfs, file2, 0, (short) 1, 0);\n+    hdfs.allowSnapshot(snapshottableDir);\n+    // rename from non snapshottable dir to snapshottable dir should fail\n+    validateRename(file1, sub0);\n+    hdfs.createSnapshot(snapshottableDir, \"s0\");\n+    validateRename(file1, sub0);\n+    // rename across non snapshottable dirs should work\n+    hdfs.rename(file1, dir2);\n+    // rename beyond snapshottable root should fail\n+    validateRename(file2, dir1);\n+    // rename within snapshottable root should work\n+    hdfs.rename(file2, snapshottableDir);\n+  }\n+\n+  private void validateRename(Path src, Path dest) {\n+    try {\n+      hdfs.rename(src, dest);\n+    } catch (IOException ioe) {\n+      ioe.getMessage().contains(\"are not under the same snapshot root.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da5c24a1523fa177e920a2bf8f1232a466d48f34"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjEwNDE1OA==", "bodyText": "addressed", "url": "https://github.com/apache/hadoop/pull/2172#discussion_r462104158", "createdAt": "2020-07-29T07:46:09Z", "author": {"login": "bshashikant"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestRenameWithOrderedSnapshotDeletion.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.namenode;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.DFSTestUtil;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+\n+import java.io.IOException;\n+\n+import static org.apache.hadoop.hdfs.DFSConfigKeys.\n+    DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED;\n+\n+/**\n+ * Test Rename with ordered snapshot deletion.\n+ */\n+public class TestRenameWithOrderedSnapshotDeletion {\n+  private final Path snapshottableDir\n+      = new Path(\"/\" + getClass().getSimpleName());\n+  private DistributedFileSystem hdfs;\n+  private MiniDFSCluster cluster;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    final Configuration conf = new Configuration();\n+    conf.setBoolean(DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED, true);\n+\n+    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(0).build();\n+    cluster.waitActive();\n+    hdfs = cluster.getFileSystem();\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n+  }\n+\n+  @Test(timeout = 60000)\n+  public void testRename() throws Exception {\n+    final Path dir1 = new Path(\"/dir1\");\n+    final Path dir2 = new Path(\"/dir2\");\n+    final Path sub0 = new Path(snapshottableDir, \"sub0\");\n+    hdfs.mkdirs(sub0);\n+    final Path file1 = new Path(dir1, \"file1\");\n+    final Path file2 = new Path(sub0, \"file2\");\n+    hdfs.mkdirs(snapshottableDir);\n+    hdfs.mkdirs(dir1);\n+    hdfs.mkdirs(dir2);\n+    hdfs.mkdirs(sub0);\n+    DFSTestUtil.createFile(hdfs, file1, 0, (short) 1, 0);\n+    DFSTestUtil.createFile(hdfs, file2, 0, (short) 1, 0);\n+    hdfs.allowSnapshot(snapshottableDir);\n+    // rename from non snapshottable dir to snapshottable dir should fail\n+    validateRename(file1, sub0);\n+    hdfs.createSnapshot(snapshottableDir, \"s0\");\n+    validateRename(file1, sub0);\n+    // rename across non snapshottable dirs should work\n+    hdfs.rename(file1, dir2);\n+    // rename beyond snapshottable root should fail\n+    validateRename(file2, dir1);\n+    // rename within snapshottable root should work\n+    hdfs.rename(file2, snapshottableDir);\n+  }\n+\n+  private void validateRename(Path src, Path dest) {\n+    try {\n+      hdfs.rename(src, dest);\n+    } catch (IOException ioe) {\n+      ioe.getMessage().contains(\"are not under the same snapshot root.\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjA2NTQyNw=="}, "originalCommit": {"oid": "da5c24a1523fa177e920a2bf8f1232a466d48f34"}, "originalPosition": 93}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NDc1MjE3OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestRenameWithOrderedSnapshotDeletion.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwNjoyNToxOVrOG4qQ1A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwNzo0NjoyMVrOG4smxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjA2NTg3Ng==", "bodyText": "Lets also add some directory level renames here", "url": "https://github.com/apache/hadoop/pull/2172#discussion_r462065876", "createdAt": "2020-07-29T06:25:19Z", "author": {"login": "mukul1987"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestRenameWithOrderedSnapshotDeletion.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.namenode;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.DFSTestUtil;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+\n+import java.io.IOException;\n+\n+import static org.apache.hadoop.hdfs.DFSConfigKeys.\n+    DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED;\n+\n+/**\n+ * Test Rename with ordered snapshot deletion.\n+ */\n+public class TestRenameWithOrderedSnapshotDeletion {\n+  private final Path snapshottableDir\n+      = new Path(\"/\" + getClass().getSimpleName());\n+  private DistributedFileSystem hdfs;\n+  private MiniDFSCluster cluster;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    final Configuration conf = new Configuration();\n+    conf.setBoolean(DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED, true);\n+\n+    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(0).build();\n+    cluster.waitActive();\n+    hdfs = cluster.getFileSystem();\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n+  }\n+\n+  @Test(timeout = 60000)\n+  public void testRename() throws Exception {\n+    final Path dir1 = new Path(\"/dir1\");\n+    final Path dir2 = new Path(\"/dir2\");\n+    final Path sub0 = new Path(snapshottableDir, \"sub0\");\n+    hdfs.mkdirs(sub0);\n+    final Path file1 = new Path(dir1, \"file1\");\n+    final Path file2 = new Path(sub0, \"file2\");\n+    hdfs.mkdirs(snapshottableDir);\n+    hdfs.mkdirs(dir1);\n+    hdfs.mkdirs(dir2);\n+    hdfs.mkdirs(sub0);\n+    DFSTestUtil.createFile(hdfs, file1, 0, (short) 1, 0);\n+    DFSTestUtil.createFile(hdfs, file2, 0, (short) 1, 0);\n+    hdfs.allowSnapshot(snapshottableDir);\n+    // rename from non snapshottable dir to snapshottable dir should fail\n+    validateRename(file1, sub0);\n+    hdfs.createSnapshot(snapshottableDir, \"s0\");\n+    validateRename(file1, sub0);\n+    // rename across non snapshottable dirs should work\n+    hdfs.rename(file1, dir2);\n+    // rename beyond snapshottable root should fail\n+    validateRename(file2, dir1);\n+    // rename within snapshottable root should work\n+    hdfs.rename(file2, snapshottableDir);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da5c24a1523fa177e920a2bf8f1232a466d48f34"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjEwNDI2MA==", "bodyText": "Addressed", "url": "https://github.com/apache/hadoop/pull/2172#discussion_r462104260", "createdAt": "2020-07-29T07:46:21Z", "author": {"login": "bshashikant"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestRenameWithOrderedSnapshotDeletion.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.namenode;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.DFSTestUtil;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+\n+import java.io.IOException;\n+\n+import static org.apache.hadoop.hdfs.DFSConfigKeys.\n+    DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED;\n+\n+/**\n+ * Test Rename with ordered snapshot deletion.\n+ */\n+public class TestRenameWithOrderedSnapshotDeletion {\n+  private final Path snapshottableDir\n+      = new Path(\"/\" + getClass().getSimpleName());\n+  private DistributedFileSystem hdfs;\n+  private MiniDFSCluster cluster;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    final Configuration conf = new Configuration();\n+    conf.setBoolean(DFS_NAMENODE_SNAPSHOT_DELETION_ORDERED, true);\n+\n+    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(0).build();\n+    cluster.waitActive();\n+    hdfs = cluster.getFileSystem();\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n+  }\n+\n+  @Test(timeout = 60000)\n+  public void testRename() throws Exception {\n+    final Path dir1 = new Path(\"/dir1\");\n+    final Path dir2 = new Path(\"/dir2\");\n+    final Path sub0 = new Path(snapshottableDir, \"sub0\");\n+    hdfs.mkdirs(sub0);\n+    final Path file1 = new Path(dir1, \"file1\");\n+    final Path file2 = new Path(sub0, \"file2\");\n+    hdfs.mkdirs(snapshottableDir);\n+    hdfs.mkdirs(dir1);\n+    hdfs.mkdirs(dir2);\n+    hdfs.mkdirs(sub0);\n+    DFSTestUtil.createFile(hdfs, file1, 0, (short) 1, 0);\n+    DFSTestUtil.createFile(hdfs, file2, 0, (short) 1, 0);\n+    hdfs.allowSnapshot(snapshottableDir);\n+    // rename from non snapshottable dir to snapshottable dir should fail\n+    validateRename(file1, sub0);\n+    hdfs.createSnapshot(snapshottableDir, \"s0\");\n+    validateRename(file1, sub0);\n+    // rename across non snapshottable dirs should work\n+    hdfs.rename(file1, dir2);\n+    // rename beyond snapshottable root should fail\n+    validateRename(file2, dir1);\n+    // rename within snapshottable root should work\n+    hdfs.rename(file2, snapshottableDir);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjA2NTg3Ng=="}, "originalCommit": {"oid": "da5c24a1523fa177e920a2bf8f1232a466d48f34"}, "originalPosition": 86}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzODA3ODE2OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxOToxMTowMVrOHAZ4Ww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxOToxMTowMVrOHAZ4Ww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDE4NjA3NQ==", "bodyText": "This check should be moved to right before the for-loop below (line 389).", "url": "https://github.com/apache/hadoop/pull/2172#discussion_r470186075", "createdAt": "2020-08-13T19:11:01Z", "author": {"login": "szetszwo"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "diffHunk": "@@ -363,23 +363,38 @@ void assertFirstSnapshot(INodeDirectory dir,\n    * @param iip INodesInPath for the directory to get snapshot root.\n    * @return the snapshot root INodeDirectory\n    */\n+  public INodeDirectory checkAndGetSnapshottableAncestorDir(\n+      final INodesInPath iip) throws IOException {\n+    final INodeDirectory dir = getSnapshottableAncestorDir(iip);\n+    if (dir == null) {\n+      throw new SnapshotException(\"Directory is neither snapshottable nor\" +\n+          \" under a snap root!\");\n+    }\n+    return dir;\n+  }\n+\n   public INodeDirectory getSnapshottableAncestorDir(final INodesInPath iip)\n       throws IOException {\n     final String path = iip.getPath();\n-    final INodeDirectory dir = INodeDirectory.valueOf(iip.getLastINode(), path);\n-    if (dir.isSnapshottable()) {\n-      return dir;\n+    final INode inode = iip.getLastINode();\n+    final INodeDirectory dir;\n+    if (inode instanceof INodeDirectory) {\n+      dir = INodeDirectory.valueOf(inode, path);\n+      if (dir.isSnapshottable()) {\n+        return dir;\n+      }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70f6603fe0cc4a6081622dee40ca50fa4d7f0a48"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzODA4MjYzOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirSnapshotOp.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxOToxMjoxNFrOHAZ6-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxOToxMjoxNFrOHAZ6-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDE4Njc0Nw==", "bodyText": "This is only used in FSDirRenameOp.  Let's move to there?", "url": "https://github.com/apache/hadoop/pull/2172#discussion_r470186747", "createdAt": "2020-08-13T19:12:14Z", "author": {"login": "szetszwo"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirSnapshotOp.java", "diffHunk": "@@ -358,4 +358,23 @@ static void checkSnapshot(FSDirectory fsd, INodesInPath iip,\n       checkSnapshot(iip.getLastINode(), snapshottableDirs);\n     }\n   }\n+\n+  static void checkUnderSameSnapshottableRoot(FSDirectory fsd,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70f6603fe0cc4a6081622dee40ca50fa4d7f0a48"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzODEwMjY4OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirSnapshotOp.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxOToxNjo1NVrOHAaGWg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxOToxNjo1NVrOHAaGWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDE4OTY1OA==", "bodyText": "The errMsg should be constructed right before \"throw new SnapshotException(errMsg);\"", "url": "https://github.com/apache/hadoop/pull/2172#discussion_r470189658", "createdAt": "2020-08-13T19:16:55Z", "author": {"login": "szetszwo"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirSnapshotOp.java", "diffHunk": "@@ -358,4 +358,23 @@ static void checkSnapshot(FSDirectory fsd, INodesInPath iip,\n       checkSnapshot(iip.getLastINode(), snapshottableDirs);\n     }\n   }\n+\n+  static void checkUnderSameSnapshottableRoot(FSDirectory fsd,\n+      INodesInPath srcIIP, INodesInPath dstIIP) throws IOException {\n+    // Ensure rename out of a snapshottable root is not permitted if ordered\n+    // snapshot deletion feature is enabled\n+    SnapshotManager snapshotManager = fsd.getFSNamesystem().\n+        getSnapshotManager();\n+    if (snapshotManager.isSnapshotDeletionOrdered() && fsd.getFSNamesystem()\n+        .isSnapshotTrashRootEnabled()) {\n+      String errMsg = \"Source \" + srcIIP.getPath() +\n+          \" and dest \" + dstIIP.getPath() + \" are not under \" +\n+          \"the same snapshot root.\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70f6603fe0cc4a6081622dee40ca50fa4d7f0a48"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzODEwMzA5OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirSnapshotOp.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxOToxNzowNVrOHAaGoQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQwNTo0MzowNVrOHAoPaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDE4OTcyOQ==", "bodyText": "src can be null.  We should add a test for this case.", "url": "https://github.com/apache/hadoop/pull/2172#discussion_r470189729", "createdAt": "2020-08-13T19:17:05Z", "author": {"login": "szetszwo"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirSnapshotOp.java", "diffHunk": "@@ -358,4 +358,23 @@ static void checkSnapshot(FSDirectory fsd, INodesInPath iip,\n       checkSnapshot(iip.getLastINode(), snapshottableDirs);\n     }\n   }\n+\n+  static void checkUnderSameSnapshottableRoot(FSDirectory fsd,\n+      INodesInPath srcIIP, INodesInPath dstIIP) throws IOException {\n+    // Ensure rename out of a snapshottable root is not permitted if ordered\n+    // snapshot deletion feature is enabled\n+    SnapshotManager snapshotManager = fsd.getFSNamesystem().\n+        getSnapshotManager();\n+    if (snapshotManager.isSnapshotDeletionOrdered() && fsd.getFSNamesystem()\n+        .isSnapshotTrashRootEnabled()) {\n+      String errMsg = \"Source \" + srcIIP.getPath() +\n+          \" and dest \" + dstIIP.getPath() + \" are not under \" +\n+          \"the same snapshot root.\";\n+      INodeDirectory src = snapshotManager.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70f6603fe0cc4a6081622dee40ca50fa4d7f0a48"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDQyMTM1Mw==", "bodyText": "Good catch! I think its better to check snapshottable root for both src and dst to be equal. isDescendent() check will be true if src snapshot root is ancesstor of dst snapshot root even if they are not equal and if nested snapshot feature is enabled.", "url": "https://github.com/apache/hadoop/pull/2172#discussion_r470421353", "createdAt": "2020-08-14T05:43:05Z", "author": {"login": "bshashikant"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirSnapshotOp.java", "diffHunk": "@@ -358,4 +358,23 @@ static void checkSnapshot(FSDirectory fsd, INodesInPath iip,\n       checkSnapshot(iip.getLastINode(), snapshottableDirs);\n     }\n   }\n+\n+  static void checkUnderSameSnapshottableRoot(FSDirectory fsd,\n+      INodesInPath srcIIP, INodesInPath dstIIP) throws IOException {\n+    // Ensure rename out of a snapshottable root is not permitted if ordered\n+    // snapshot deletion feature is enabled\n+    SnapshotManager snapshotManager = fsd.getFSNamesystem().\n+        getSnapshotManager();\n+    if (snapshotManager.isSnapshotDeletionOrdered() && fsd.getFSNamesystem()\n+        .isSnapshotTrashRootEnabled()) {\n+      String errMsg = \"Source \" + srcIIP.getPath() +\n+          \" and dest \" + dstIIP.getPath() + \" are not under \" +\n+          \"the same snapshot root.\";\n+      INodeDirectory src = snapshotManager.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDE4OTcyOQ=="}, "originalCommit": {"oid": "70f6603fe0cc4a6081622dee40ca50fa4d7f0a48"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MTA0NzQwOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirRenameOp.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQxMzo1ODoyOVrOHA1nDw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQxNToxODoyMVrOHA4lrA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDY0MDM5OQ==", "bodyText": "can we use equals here ? as for comparing inodes we can use equals", "url": "https://github.com/apache/hadoop/pull/2172#discussion_r470640399", "createdAt": "2020-08-14T13:58:29Z", "author": {"login": "hemanthboyina"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirRenameOp.java", "diffHunk": "@@ -821,6 +823,29 @@ void updateQuotasInSourceTree(BlockStoragePolicySuite bsps) {\n     }\n   }\n \n+  private static void checkUnderSameSnapshottableRoot(\n+      FSDirectory fsd, INodesInPath srcIIP, INodesInPath dstIIP)\n+      throws IOException {\n+    // Ensure rename out of a snapshottable root is not permitted if ordered\n+    // snapshot deletion feature is enabled\n+    SnapshotManager snapshotManager = fsd.getFSNamesystem().\n+        getSnapshotManager();\n+    if (snapshotManager.isSnapshotDeletionOrdered() && fsd.getFSNamesystem()\n+        .isSnapshotTrashRootEnabled()) {\n+      INodeDirectory srcRoot = snapshotManager.\n+          getSnapshottableAncestorDir(srcIIP);\n+      INodeDirectory dstRoot = snapshotManager.\n+          getSnapshottableAncestorDir(dstIIP);\n+      // Ensure snapshoottable root for both src and dest are same.\n+      if (srcRoot != dstRoot) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "edee22e378985607bcc813b388f1c80f18b996a9"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDY4OTE5Ng==", "bodyText": "We should not use equals here as it is overriden in INODE.java which just matches the inode id.", "url": "https://github.com/apache/hadoop/pull/2172#discussion_r470689196", "createdAt": "2020-08-14T15:18:21Z", "author": {"login": "bshashikant"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirRenameOp.java", "diffHunk": "@@ -821,6 +823,29 @@ void updateQuotasInSourceTree(BlockStoragePolicySuite bsps) {\n     }\n   }\n \n+  private static void checkUnderSameSnapshottableRoot(\n+      FSDirectory fsd, INodesInPath srcIIP, INodesInPath dstIIP)\n+      throws IOException {\n+    // Ensure rename out of a snapshottable root is not permitted if ordered\n+    // snapshot deletion feature is enabled\n+    SnapshotManager snapshotManager = fsd.getFSNamesystem().\n+        getSnapshotManager();\n+    if (snapshotManager.isSnapshotDeletionOrdered() && fsd.getFSNamesystem()\n+        .isSnapshotTrashRootEnabled()) {\n+      INodeDirectory srcRoot = snapshotManager.\n+          getSnapshottableAncestorDir(srcIIP);\n+      INodeDirectory dstRoot = snapshotManager.\n+          getSnapshottableAncestorDir(dstIIP);\n+      // Ensure snapshoottable root for both src and dest are same.\n+      if (srcRoot != dstRoot) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDY0MDM5OQ=="}, "originalCommit": {"oid": "edee22e378985607bcc813b388f1c80f18b996a9"}, "originalPosition": 43}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3339, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}