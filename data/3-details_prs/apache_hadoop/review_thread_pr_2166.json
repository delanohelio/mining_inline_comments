{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU1MjEzNTAw", "number": 2166, "reviewThreads": {"totalCount": 24, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxOTo0Nzo1MFrOERl4Kw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQxODowMjoyNlrOES7ToQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2ODgxODM1OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxOTo0Nzo1MFrOG2ZJaA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxOTo0Nzo1MFrOG2ZJaA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY4ODI5Ng==", "bodyText": "Should relative path be resolved as well?\n    Path absF = fixRelativePart(path);\nShould Increment Read Statistics as well.\n    statistics.incrementReadOps(1);", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r459688296", "createdAt": "2020-07-23T19:47:50Z", "author": {"login": "ayushtkn"}, "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java", "diffHunk": "@@ -2148,6 +2149,15 @@ public Void next(final FileSystem fs, final Path p)\n     return dfs.getSnapshottableDirListing();\n   }\n \n+  /**\n+   * @return all the snapshots for a snapshottable directory\n+   * @throws IOException\n+   */\n+  public SnapshotStatus[] getSnapshotListing(Path snapshotRoot)\n+      throws IOException {\n+    return dfs.getSnapshotListing(getPathName(snapshotRoot));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "51fc498d69c0e86fb7cf2bb217ed04ea36100991"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2ODgzNTI4OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxOTo1MzoxNVrOG2ZTvQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxOTo1MzoxNVrOG2ZTvQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY5MDk0MQ==", "bodyText": "Should be :\nClientProtocol#getSnapshotListing(String)", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r459690941", "createdAt": "2020-07-23T19:53:15Z", "author": {"login": "ayushtkn"}, "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java", "diffHunk": "@@ -2190,6 +2191,24 @@ public void renameSnapshot(String snapshotDir, String snapshotOldName,\n     }\n   }\n \n+  /**\n+   * Get listing of all the snapshots for a snapshottable directory\n+   *\n+   * @return Information about all the snapshots for a snapshottable directory\n+   * @throws IOException If an I/O error occurred\n+   * @see ClientProtocol#getSnapshotListing()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "51fc498d69c0e86fb7cf2bb217ed04ea36100991"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2ODg0MDgyOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxOTo1NDo1NFrOG2ZXJw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxOTo1NDo1NFrOG2ZXJw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY5MTgxNQ==", "bodyText": "Seems Copy-paste error, Change to\ntry (TraceScope ignored = tracer.newScope(\"getSnapshotListing\")) {", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r459691815", "createdAt": "2020-07-23T19:54:54Z", "author": {"login": "ayushtkn"}, "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java", "diffHunk": "@@ -2190,6 +2191,24 @@ public void renameSnapshot(String snapshotDir, String snapshotOldName,\n     }\n   }\n \n+  /**\n+   * Get listing of all the snapshots for a snapshottable directory\n+   *\n+   * @return Information about all the snapshots for a snapshottable directory\n+   * @throws IOException If an I/O error occurred\n+   * @see ClientProtocol#getSnapshotListing()\n+   */\n+  public SnapshotStatus[] getSnapshotListing(String snapshotRoot)\n+      throws IOException {\n+    checkOpen();\n+    try (TraceScope ignored = tracer.newScope(\"getSnapshottableDirListing\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "51fc498d69c0e86fb7cf2bb217ed04ea36100991"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2ODg2NzkwOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMDowMzoxNlrOG2ZoMA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMDowMzoxNlrOG2ZoMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY5NjE3Ng==", "bodyText": "Keep the argument name consistent, In ClientProtocol & Router its snapshotRoot, better keep same everywhere", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r459696176", "createdAt": "2020-07-23T20:03:16Z", "author": {"login": "ayushtkn"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java", "diffHunk": "@@ -2004,6 +2005,16 @@ public void renameSnapshot(String snapshotRoot, String snapshotOldName,\n     return status;\n   }\n \n+  @Override // Client Protocol\n+  public SnapshotStatus[] getSnapshotListing(String path)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "51fc498d69c0e86fb7cf2bb217ed04ea36100991"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2ODg4MDAyOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMDowNjo0OFrOG2Zvaw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMDowNjo0OFrOG2Zvaw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY5ODAyNw==", "bodyText": "Should put the path as well in the audit log too\n      logAuditEvent(success, \"listSnapshots\", snapshotRoot);", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r459698027", "createdAt": "2020-07-23T20:06:48Z", "author": {"login": "ayushtkn"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java", "diffHunk": "@@ -7001,7 +7002,33 @@ void renameSnapshot(\n     logAuditEvent(true, operationName, null, null, null);\n     return status;\n   }\n-  \n+\n+  /**\n+   * Get the list of snapshots for a given snapshottable directory.\n+   *\n+   * @return The list of all the snapshots for a snapshottable directory\n+   * @throws IOException\n+   */\n+  public SnapshotStatus[] getSnapshotListing(String snapshotRoot)\n+      throws IOException {\n+    SnapshotStatus[] status = null;\n+    checkOperation(OperationCategory.READ);\n+    boolean success = false;\n+    readLock();\n+    try {\n+      checkOperation(OperationCategory.READ);\n+      status = FSDirSnapshotOp.getSnapshotListing(dir, snapshotManager,\n+          snapshotRoot);\n+      success = true;\n+    } catch (AccessControlException ace) {\n+      logAuditEvent(success, \"listSnapshots\", null, null, null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "51fc498d69c0e86fb7cf2bb217ed04ea36100991"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2ODkwNTgzOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirSnapshotOp.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMDoxNToxNlrOG2Z_rQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMDoxNToxNlrOG2Z_rQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcwMjE4OQ==", "bodyText": "Can get pc out of fsn lock and pass on from FsNamesystem by getting before taking lock, Would save lock retention time, the FsDirectory lock below is just dummy just asserts whether FSN lock is there or not", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r459702189", "createdAt": "2020-07-23T20:15:16Z", "author": {"login": "ayushtkn"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirSnapshotOp.java", "diffHunk": "@@ -155,6 +156,23 @@ static void renameSnapshot(FSDirectory fsd, FSPermissionChecker pc,\n     }\n   }\n \n+  static SnapshotStatus[] getSnapshotListing(\n+      FSDirectory fsd, SnapshotManager snapshotManager, String path)\n+      throws IOException {\n+    FSPermissionChecker pc = fsd.getPermissionChecker();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "51fc498d69c0e86fb7cf2bb217ed04ea36100991"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2ODkwOTY1OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirSnapshotOp.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMDoxNjoyN1rOG2aCDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMDoxNjoyN1rOG2aCDA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcwMjc5Ng==", "bodyText": "Can use instead :\nfsd.checkPathAccess(pc,iip,FsAction.READ);", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r459702796", "createdAt": "2020-07-23T20:16:27Z", "author": {"login": "ayushtkn"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirSnapshotOp.java", "diffHunk": "@@ -155,6 +156,23 @@ static void renameSnapshot(FSDirectory fsd, FSPermissionChecker pc,\n     }\n   }\n \n+  static SnapshotStatus[] getSnapshotListing(\n+      FSDirectory fsd, SnapshotManager snapshotManager, String path)\n+      throws IOException {\n+    FSPermissionChecker pc = fsd.getPermissionChecker();\n+    fsd.readLock();\n+    try {\n+      INodesInPath iip = fsd.getINodesInPath(path, DirOp.READ);\n+      if (fsd.isPermissionEnabled()) {\n+        fsd.checkPermission(pc, iip, false, null, null, FsAction.READ,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "51fc498d69c0e86fb7cf2bb217ed04ea36100991"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2ODkyOTU1OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMDoyMjoxNFrOG2aOOA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMDoyMjoxNFrOG2aOOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcwNTkxMg==", "bodyText": "Isn't the size of the list always be equal to the size of snapshotList?, if so and size is already known then no need of having a list and then converting to array, can directly take an array of size same as that of snapshotList", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r459705912", "createdAt": "2020-07-23T20:22:14Z", "author": {"login": "ayushtkn"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "diffHunk": "@@ -471,7 +473,35 @@ public void write(DataOutput out) throws IOException {\n     return statusList.toArray(\n         new SnapshottableDirectoryStatus[statusList.size()]);\n   }\n-  \n+\n+  /**\n+   * List all the snapshots under a snapshottable directory.\n+   */\n+  public SnapshotStatus[] getSnapshotListing(INodesInPath iip)\n+      throws IOException {\n+    INodeDirectory srcRoot = getSnapshottableRoot(iip);\n+    ReadOnlyList<Snapshot> snapshotList = srcRoot.getDirectorySnapshottableFeature().\n+        getSnapshotList();\n+    if (snapshotList.isEmpty()) {\n+      return null;\n+    }\n+    List<SnapshotStatus> statusList =\n+        new ArrayList<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "51fc498d69c0e86fb7cf2bb217ed04ea36100991"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2ODkzODkxOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMDoyNToyNVrOG2aUTQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMDoyNToyNVrOG2aUTQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcwNzQ2OQ==", "bodyText": "Do you want to pass null, if there is no snapshots, Can't we pass an empty list?\nIt might create problem for some client checking the size of the array to conclude if there are snapshots or not or performing directly some operations without being too smart of having a null check, his code would break will a NPE. If there is no strong opposition, we may pass an empty array, I think, but I will keep the ball in your court", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r459707469", "createdAt": "2020-07-23T20:25:25Z", "author": {"login": "ayushtkn"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "diffHunk": "@@ -471,7 +473,35 @@ public void write(DataOutput out) throws IOException {\n     return statusList.toArray(\n         new SnapshottableDirectoryStatus[statusList.size()]);\n   }\n-  \n+\n+  /**\n+   * List all the snapshots under a snapshottable directory.\n+   */\n+  public SnapshotStatus[] getSnapshotListing(INodesInPath iip)\n+      throws IOException {\n+    INodeDirectory srcRoot = getSnapshottableRoot(iip);\n+    ReadOnlyList<Snapshot> snapshotList = srcRoot.getDirectorySnapshottableFeature().\n+        getSnapshotList();\n+    if (snapshotList.isEmpty()) {\n+      return null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "51fc498d69c0e86fb7cf2bb217ed04ea36100991"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2OTAwNTQ4OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterSnapshot.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMDo0NTozNlrOG2a81Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMDo0NTozNlrOG2a81Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcxNzg0NQ==", "bodyText": "This is messed up.\n\nlocations need to be passed, as of now you are ignoring the passed parameter itself.\ninvokeConcurrent only in case if the Path is of type isAll, you can use rpcServer.isInvokeConcurrent(snapshotRoot)\nOnce you make this change I think RouterRpcServer.merge(.) won't work, you need to write your own util to aggregate.\nThis needs to be covered by a UT as well, TestRouterRpc or TestRouterRPCMultipleDestinationMountTableResolver could be a good place to add one.\n\n** If you have any issue with RBF, let me know, will try to get you the code. :-)", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r459717845", "createdAt": "2020-07-23T20:45:36Z", "author": {"login": "ayushtkn"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterSnapshot.java", "diffHunk": "@@ -157,6 +158,22 @@ public void renameSnapshot(String snapshotRoot, String oldSnapshotName,\n     return RouterRpcServer.merge(ret, SnapshottableDirectoryStatus.class);\n   }\n \n+  public SnapshotStatus[] getSnapshotListing(String snapshotRoot)\n+      throws IOException {\n+    rpcServer.checkOperation(NameNode.OperationCategory.READ);\n+    final List<RemoteLocation> locations =\n+        rpcServer.getLocationsForPath(snapshotRoot, true, false);\n+    RemoteMethod method = new RemoteMethod(\"getSnapshotListing\",\n+        new Class<?>[] {String.class},\n+        new RemoteParam());\n+    Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\n+    Map<FederationNamespaceInfo, SnapshotStatus[]> ret =\n+        rpcClient.invokeConcurrent(\n+            nss, method, true, false, SnapshotStatus[].class);\n+\n+    return RouterRpcServer.merge(ret, SnapshotStatus.class);\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "51fc498d69c0e86fb7cf2bb217ed04ea36100991"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2OTAxNTY4OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/snapshot/LsSnapshot.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMDo0OTowMlrOG2bDLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMDo0OTowMlrOG2bDLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcxOTQ2OA==", "bodyText": "Would be good if there is a error message as well, Something like invalid number of arguments...", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r459719468", "createdAt": "2020-07-23T20:49:02Z", "author": {"login": "ayushtkn"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/snapshot/LsSnapshot.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.tools.snapshot;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.conf.Configured;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.protocol.SnapshotStatus;\n+import org.apache.hadoop.hdfs.protocol.SnapshottableDirectoryStatus;\n+import org.apache.hadoop.util.Tool;\n+import org.apache.hadoop.util.ToolRunner;\n+\n+/**\n+ * A tool used to list all snapshottable directories that are owned by the\n+ * current user. The tool returns all the snapshottable directories if the user\n+ * is a super user.\n+ */\n+@InterfaceAudience.Private\n+public class LsSnapshot extends Configured implements Tool {\n+  @Override\n+  public int run(String[] argv) throws Exception {\n+    String description = \"hdfs lsSnapshot <snapshotDir>: \\n\" +\n+        \"\\tGet the list of snapshots for a snapshottable directory.\\n\";\n+\n+    if(argv.length != 1) {\n+      System.err.println(\"Usage: \\n\" + description);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "51fc498d69c0e86fb7cf2bb217ed04ea36100991"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2OTAzNzU3OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/snapshot/LsSnapshot.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMDo1NTo0MVrOG2bQGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMDo1NTo0MVrOG2bQGA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcyMjc3Ng==", "bodyText": "Instead can use\n    DistributedFileSystem dfs = AdminHelper.getDFS(getConf());\nThis shall handle ViewFsOverloadScheme as well.\nThis would throw IllegalArgumentException so it should be inside the try block", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r459722776", "createdAt": "2020-07-23T20:55:41Z", "author": {"login": "ayushtkn"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/snapshot/LsSnapshot.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.tools.snapshot;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.conf.Configured;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.protocol.SnapshotStatus;\n+import org.apache.hadoop.hdfs.protocol.SnapshottableDirectoryStatus;\n+import org.apache.hadoop.util.Tool;\n+import org.apache.hadoop.util.ToolRunner;\n+\n+/**\n+ * A tool used to list all snapshottable directories that are owned by the\n+ * current user. The tool returns all the snapshottable directories if the user\n+ * is a super user.\n+ */\n+@InterfaceAudience.Private\n+public class LsSnapshot extends Configured implements Tool {\n+  @Override\n+  public int run(String[] argv) throws Exception {\n+    String description = \"hdfs lsSnapshot <snapshotDir>: \\n\" +\n+        \"\\tGet the list of snapshots for a snapshottable directory.\\n\";\n+\n+    if(argv.length != 1) {\n+      System.err.println(\"Usage: \\n\" + description);\n+      return 1;\n+    }\n+\n+    FileSystem fs = FileSystem.get(getConf());\n+    if (! (fs instanceof DistributedFileSystem)) {\n+      System.err.println(\n+          \"lsSnapshot can only be used in DistributedFileSystem\");\n+      return 1;\n+    }\n+    DistributedFileSystem dfs = (DistributedFileSystem) fs;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "51fc498d69c0e86fb7cf2bb217ed04ea36100991"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2OTA1ODk4OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/snapshot/LsSnapshot.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMTowMjowOVrOG2bdRg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMTowMjowOVrOG2bdRg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcyNjE1MA==", "bodyText": "I don't think we need to print the stack trace on the CLI? Just a line of error should work? Mostly it should trigger for FNF or for SnapshotException if the directory is not snapshottable. if required we can have the exception with trace in the debug log\nApart we should catch Exception as well for any runtime exceptions, propagating the exception in CLI won't look good", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r459726150", "createdAt": "2020-07-23T21:02:09Z", "author": {"login": "ayushtkn"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/snapshot/LsSnapshot.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.tools.snapshot;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.conf.Configured;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.protocol.SnapshotStatus;\n+import org.apache.hadoop.hdfs.protocol.SnapshottableDirectoryStatus;\n+import org.apache.hadoop.util.Tool;\n+import org.apache.hadoop.util.ToolRunner;\n+\n+/**\n+ * A tool used to list all snapshottable directories that are owned by the\n+ * current user. The tool returns all the snapshottable directories if the user\n+ * is a super user.\n+ */\n+@InterfaceAudience.Private\n+public class LsSnapshot extends Configured implements Tool {\n+  @Override\n+  public int run(String[] argv) throws Exception {\n+    String description = \"hdfs lsSnapshot <snapshotDir>: \\n\" +\n+        \"\\tGet the list of snapshots for a snapshottable directory.\\n\";\n+\n+    if(argv.length != 1) {\n+      System.err.println(\"Usage: \\n\" + description);\n+      return 1;\n+    }\n+\n+    FileSystem fs = FileSystem.get(getConf());\n+    if (! (fs instanceof DistributedFileSystem)) {\n+      System.err.println(\n+          \"lsSnapshot can only be used in DistributedFileSystem\");\n+      return 1;\n+    }\n+    DistributedFileSystem dfs = (DistributedFileSystem) fs;\n+    Path snapshotRoot = new Path(argv[0]);\n+\n+    try {\n+      SnapshotStatus[] stats = dfs.getSnapshotListing(snapshotRoot);\n+      SnapshotStatus.print(stats, System.out);\n+    } catch (IOException e) {\n+      String[] content = e.getLocalizedMessage().split(\"\\n\");\n+      System.err.println(\"lsSnapshot: \" + content[0]);\n+      e.printStackTrace(System.err);\n+      return 1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "51fc498d69c0e86fb7cf2bb217ed04ea36100991"}, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2OTA2NTAzOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMTowNDowNlrOG2bg8Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMTowNDowNlrOG2bg8Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcyNzA4OQ==", "bodyText": "fsn is already there, can change to:\nfsn.getSnapshotManager().setAllowNestedSnapshots(true);", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r459727089", "createdAt": "2020-07-23T21:04:06Z", "author": {"login": "ayushtkn"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.namenode.snapshot;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.protocol.SnapshotStatus;\n+import org.apache.hadoop.hdfs.protocol.SnapshottableDirectoryStatus;\n+import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertEquals;\n+\n+public class TestListSnapshot {\n+\n+  static final short REPLICATION = 3;\n+\n+  private final Path dir1 = new Path(\"/TestSnapshot1\");\n+\n+  Configuration conf;\n+  MiniDFSCluster cluster;\n+  FSNamesystem fsn;\n+  DistributedFileSystem hdfs;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(REPLICATION)\n+        .build();\n+    cluster.waitActive();\n+    fsn = cluster.getNamesystem();\n+    hdfs = cluster.getFileSystem();\n+    hdfs.mkdirs(dir1);\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n+  }\n+\n+  /**\n+   * Test listing all the snapshottable directories\n+   */\n+  @Test(timeout = 60000)\n+  public void testListSnapshot() throws Exception {\n+    cluster.getNamesystem().getSnapshotManager().setAllowNestedSnapshots(true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "51fc498d69c0e86fb7cf2bb217ed04ea36100991"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2OTA4MTY2OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMTowOTo0NlrOG2brUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMTowOTo0NlrOG2brUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcyOTc0NA==", "bodyText": "Can use LambdaTestUtils :\n``LambdaTestUtils.intercept(SnapshotException.class,\n    \"Directory is not a \" + \"snapshottable directory\",\n    () -> hdfs.getSnapshotListing(dir1)); ``", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r459729744", "createdAt": "2020-07-23T21:09:46Z", "author": {"login": "ayushtkn"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.namenode.snapshot;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.protocol.SnapshotStatus;\n+import org.apache.hadoop.hdfs.protocol.SnapshottableDirectoryStatus;\n+import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertEquals;\n+\n+public class TestListSnapshot {\n+\n+  static final short REPLICATION = 3;\n+\n+  private final Path dir1 = new Path(\"/TestSnapshot1\");\n+\n+  Configuration conf;\n+  MiniDFSCluster cluster;\n+  FSNamesystem fsn;\n+  DistributedFileSystem hdfs;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(REPLICATION)\n+        .build();\n+    cluster.waitActive();\n+    fsn = cluster.getNamesystem();\n+    hdfs = cluster.getFileSystem();\n+    hdfs.mkdirs(dir1);\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n+  }\n+\n+  /**\n+   * Test listing all the snapshottable directories\n+   */\n+  @Test(timeout = 60000)\n+  public void testListSnapshot() throws Exception {\n+    cluster.getNamesystem().getSnapshotManager().setAllowNestedSnapshots(true);\n+\n+    // Initially there is no snapshottable directories in the system\n+    SnapshotStatus[] snapshotStatuses = null;\n+    SnapshottableDirectoryStatus[] dirs = hdfs.getSnapshottableDirListing();\n+    assertNull(dirs);\n+    try {\n+      hdfs.getSnapshotListing(dir1);\n+    } catch (Exception e) {\n+      assertTrue(e.getMessage().contains(\n+          \"Directory is not a snapshottable directory\"));\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "51fc498d69c0e86fb7cf2bb217ed04ea36100991"}, "originalPosition": 81}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3NTQ4Mzg5OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/SnapshotStatus.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QwMzo1NzoxNVrOG3S3sw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QwNTozMDozN1rOG3UIWw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDYzNDAzNQ==", "bodyText": "SnapshotStatus should extend HDFSFileStatus, so that this can also be used as a status object if required,", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r460634035", "createdAt": "2020-07-27T03:57:15Z", "author": {"login": "mukul1987"}, "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/SnapshotStatus.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.protocol;\n+\n+import java.io.PrintStream;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.EnumSet;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.hdfs.DFSUtilClient;\n+\n+/**\n+ * Metadata about a snapshottable directory\n+ */\n+public class SnapshotStatus {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "51fc498d69c0e86fb7cf2bb217ed04ea36100991"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDY1NDY4Mw==", "bodyText": "I would prefer to not extend the status class as it doesn't inherently displays/constructs the filestatus object correctly regarding the children list info as well as permissions. The approach here is similar to what has been addresed in SnapshottableDirectoryStatus.", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r460654683", "createdAt": "2020-07-27T05:30:37Z", "author": {"login": "bshashikant"}, "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/SnapshotStatus.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.protocol;\n+\n+import java.io.PrintStream;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.EnumSet;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.hdfs.DFSUtilClient;\n+\n+/**\n+ * Metadata about a snapshottable directory\n+ */\n+public class SnapshotStatus {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDYzNDAzNQ=="}, "originalCommit": {"oid": "51fc498d69c0e86fb7cf2bb217ed04ea36100991"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3ODMyNzUwOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterSnapshot.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxODowNDo1MlrOG3tqFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxODowNDo1MlrOG3tqFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTA3MjkxNw==", "bodyText": "Here the SnapshotStatus contains parentpath as well, the path will be the one wrt the actual namespace not with respect to mount table. You need to replace the parentpath with path corresponding to the mount entry.\nif mount entry is : /mnt -> /dir in ns0, then if you trigger call on ns0 the path would be something like /dir/sub.. that you need to change to /mnt/sub..\nIn case of InvokeConcurrent you would be able to get Src and Dst from the ret and maybe something like this may work  -\n      response = ret.values().iterator().next();\n      String src = ret.keySet().iterator().next().getSrc();\n      String dst = ret.keySet().iterator().next().getDest();\n      for (SnapshotStatus s : response) {\n        String mountPath =\n            new String(s.getParentFullPath()).replaceFirst(dst, src);\n        s.setParentFullPath(mountPath.getBytes());\n      }\n\nFor the invokeSequential one you won't be having the detail on which location did the call got success, For that you have to get it returned back from RouterRpcClient#L858, I guess to get the location returned you would require a new InvokeSequential method which returns the location as well, may be can refactor and reuse this one...\nThis problem would be there I think in getSnapshottableDirListing() as well. If you want, you can put a TODO and handle the location stuff in a separate follow up jira for both API's.", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r461072917", "createdAt": "2020-07-27T18:04:52Z", "author": {"login": "ayushtkn"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterSnapshot.java", "diffHunk": "@@ -157,6 +158,24 @@ public void renameSnapshot(String snapshotRoot, String oldSnapshotName,\n     return RouterRpcServer.merge(ret, SnapshottableDirectoryStatus.class);\n   }\n \n+  public SnapshotStatus[] getSnapshotListing(String snapshotRoot)\n+      throws IOException {\n+    rpcServer.checkOperation(NameNode.OperationCategory.READ);\n+    final List<RemoteLocation> locations =\n+        rpcServer.getLocationsForPath(snapshotRoot, true, false);\n+    RemoteMethod remoteMethod = new RemoteMethod(\"getSnapshotListing\",\n+        new Class<?>[]{String.class},\n+        new RemoteParam());\n+    if (rpcServer.isInvokeConcurrent(snapshotRoot)) {\n+      Map<RemoteLocation, SnapshotStatus[]> ret = rpcClient.invokeConcurrent(\n+          locations, remoteMethod, true, false, SnapshotStatus[].class);\n+      return ret.values().iterator().next();\n+    } else {\n+      return rpcClient.invokeSequential(\n+          locations, remoteMethod, SnapshotStatus[].class, null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5bc83f1f685017c7182366e1c3ed3dd8cfa38bd6"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4MDE0NDg4OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterSnapshot.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQwNjozNjowN1rOG3-llA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQwODozNjo0MVrOG4Ccsw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTM1MDI5Mg==", "bodyText": "The actual location won't be always the first one in the list. RouterRpcClient#L858 iterates over the list and triggers call to them sequentially, if the first location doesn't give the specific response, then second and so on. Adding a line at RouterRpcClient at L872 just before  return ret;, shall make this logic work -\n          Collections.swap(locations, 0, locations.indexOf(loc));\nApart everything seems good here.", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r461350292", "createdAt": "2020-07-28T06:36:07Z", "author": {"login": "ayushtkn"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterSnapshot.java", "diffHunk": "@@ -166,14 +167,30 @@ public void renameSnapshot(String snapshotRoot, String oldSnapshotName,\n     RemoteMethod remoteMethod = new RemoteMethod(\"getSnapshotListing\",\n         new Class<?>[]{String.class},\n         new RemoteParam());\n+    SnapshotStatus[] response;\n     if (rpcServer.isInvokeConcurrent(snapshotRoot)) {\n       Map<RemoteLocation, SnapshotStatus[]> ret = rpcClient.invokeConcurrent(\n           locations, remoteMethod, true, false, SnapshotStatus[].class);\n-      return ret.values().iterator().next();\n+      response = ret.values().iterator().next();\n+      String src = ret.keySet().iterator().next().getSrc();\n+      String dst = ret.keySet().iterator().next().getDest();\n+      for (SnapshotStatus s : response) {\n+        String mountPath =\n+            new String(s.getParentFullPath()).replaceFirst(src, dst);\n+        s.setParentFullPath(mountPath.getBytes());\n+      }\n     } else {\n-      return rpcClient.invokeSequential(\n+      response = rpcClient.invokeSequential(\n           locations, remoteMethod, SnapshotStatus[].class, null);\n+      RemoteLocation loc = locations.get(0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "972ab57f8c6769f4291456eb37b880b836d29ce2"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQxMzU1NQ==", "bodyText": "Thanks @ayushtkn . I tried doing the similar change but it breaks many existing unit tests.\nMoreover, i see a similar path has been followed in createSnapshot as well.\nDo you think, createSnapshot/getSnapshottableDirectory Listing is broken as well?\nI am not very familiar with RouterProtocol code. In such a case, i would appreciate if you would fix/help fixing all of these at the same time in a separate jira?", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r461413555", "createdAt": "2020-07-28T08:36:41Z", "author": {"login": "bshashikant"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterSnapshot.java", "diffHunk": "@@ -166,14 +167,30 @@ public void renameSnapshot(String snapshotRoot, String oldSnapshotName,\n     RemoteMethod remoteMethod = new RemoteMethod(\"getSnapshotListing\",\n         new Class<?>[]{String.class},\n         new RemoteParam());\n+    SnapshotStatus[] response;\n     if (rpcServer.isInvokeConcurrent(snapshotRoot)) {\n       Map<RemoteLocation, SnapshotStatus[]> ret = rpcClient.invokeConcurrent(\n           locations, remoteMethod, true, false, SnapshotStatus[].class);\n-      return ret.values().iterator().next();\n+      response = ret.values().iterator().next();\n+      String src = ret.keySet().iterator().next().getSrc();\n+      String dst = ret.keySet().iterator().next().getDest();\n+      for (SnapshotStatus s : response) {\n+        String mountPath =\n+            new String(s.getParentFullPath()).replaceFirst(src, dst);\n+        s.setParentFullPath(mountPath.getBytes());\n+      }\n     } else {\n-      return rpcClient.invokeSequential(\n+      response = rpcClient.invokeSequential(\n           locations, remoteMethod, SnapshotStatus[].class, null);\n+      RemoteLocation loc = locations.get(0);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTM1MDI5Mg=="}, "originalCommit": {"oid": "972ab57f8c6769f4291456eb37b880b836d29ce2"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4MTA3NzcwOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterRpc.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQxMTowMzoyMVrOG4HjSw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQxNDoyMjoyOVrOG4PHJQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQ5NzE2Mw==", "bodyText": "Expand wildcard imports.", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r461497163", "createdAt": "2020-07-28T11:03:21Z", "author": {"login": "mukul1987"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterRpc.java", "diffHunk": "@@ -71,29 +71,9 @@\n import org.apache.hadoop.hdfs.MiniDFSCluster.DataNodeProperties;\n import org.apache.hadoop.hdfs.NameNodeProxies;\n import org.apache.hadoop.hdfs.client.HdfsDataOutputStream;\n-import org.apache.hadoop.hdfs.protocol.AddErasureCodingPolicyResponse;\n-import org.apache.hadoop.hdfs.protocol.BlockStoragePolicy;\n-import org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo;\n-import org.apache.hadoop.hdfs.protocol.CachePoolEntry;\n-import org.apache.hadoop.hdfs.protocol.CachePoolInfo;\n-import org.apache.hadoop.hdfs.protocol.ClientProtocol;\n-import org.apache.hadoop.hdfs.protocol.DatanodeInfo;\n-import org.apache.hadoop.hdfs.protocol.DirectoryListing;\n-import org.apache.hadoop.hdfs.protocol.ECBlockGroupStats;\n-import org.apache.hadoop.hdfs.protocol.ErasureCodingPolicy;\n-import org.apache.hadoop.hdfs.protocol.ErasureCodingPolicyInfo;\n-import org.apache.hadoop.hdfs.protocol.ErasureCodingPolicyState;\n-import org.apache.hadoop.hdfs.protocol.HdfsConstants;\n+import org.apache.hadoop.hdfs.protocol.*;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "972ab57f8c6769f4291456eb37b880b836d29ce2"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTYyMTAyOQ==", "bodyText": "Sure", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r461621029", "createdAt": "2020-07-28T14:22:29Z", "author": {"login": "bshashikant"}, "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterRpc.java", "diffHunk": "@@ -71,29 +71,9 @@\n import org.apache.hadoop.hdfs.MiniDFSCluster.DataNodeProperties;\n import org.apache.hadoop.hdfs.NameNodeProxies;\n import org.apache.hadoop.hdfs.client.HdfsDataOutputStream;\n-import org.apache.hadoop.hdfs.protocol.AddErasureCodingPolicyResponse;\n-import org.apache.hadoop.hdfs.protocol.BlockStoragePolicy;\n-import org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo;\n-import org.apache.hadoop.hdfs.protocol.CachePoolEntry;\n-import org.apache.hadoop.hdfs.protocol.CachePoolInfo;\n-import org.apache.hadoop.hdfs.protocol.ClientProtocol;\n-import org.apache.hadoop.hdfs.protocol.DatanodeInfo;\n-import org.apache.hadoop.hdfs.protocol.DirectoryListing;\n-import org.apache.hadoop.hdfs.protocol.ECBlockGroupStats;\n-import org.apache.hadoop.hdfs.protocol.ErasureCodingPolicy;\n-import org.apache.hadoop.hdfs.protocol.ErasureCodingPolicyInfo;\n-import org.apache.hadoop.hdfs.protocol.ErasureCodingPolicyState;\n-import org.apache.hadoop.hdfs.protocol.HdfsConstants;\n+import org.apache.hadoop.hdfs.protocol.*;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQ5NzE2Mw=="}, "originalCommit": {"oid": "972ab57f8c6769f4291456eb37b880b836d29ce2"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4MTA5ODc0OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQxMTowOTo0N1rOG4Hv0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQxNDoyMjo1NlrOG4PIbA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTUwMDM2OA==", "bodyText": "This snapid should be the id of the snapshot here ?", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r461500368", "createdAt": "2020-07-28T11:09:47Z", "author": {"login": "mukul1987"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "diffHunk": "@@ -501,7 +509,31 @@ public void write(DataOutput out) throws IOException {\n     return statusList.toArray(\n         new SnapshottableDirectoryStatus[statusList.size()]);\n   }\n-  \n+\n+  /**\n+   * List all the snapshots under a snapshottable directory.\n+   */\n+  public SnapshotStatus[] getSnapshotListing(INodesInPath iip)\n+      throws IOException {\n+    INodeDirectory srcRoot = getSnapshottableRoot(iip);\n+    ReadOnlyList<Snapshot> snapshotList = srcRoot.\n+        getDirectorySnapshottableFeature().getSnapshotList();\n+    SnapshotStatus[] statuses = new SnapshotStatus[snapshotList.size()];\n+    for (int count = 0; count < snapshotList.size(); count++) {\n+      Snapshot s = snapshotList.get(count);\n+      Snapshot.Root dir = s.getRoot();\n+      statuses[count] = new SnapshotStatus(dir.getModificationTime(),\n+          dir.getAccessTime(), dir.getFsPermission(),\n+          EnumSet.noneOf(HdfsFileStatus.Flags.class),\n+          dir.getUserName(), dir.getGroupName(),\n+          dir.getLocalNameBytes(), dir.getId(),\n+          dir.getChildrenNum(Snapshot.CURRENT_STATE_ID),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "972ab57f8c6769f4291456eb37b880b836d29ce2"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTUwMjQ4Nw==", "bodyText": "Can we add a comment here on why the ID here should not be snapID ?\nAnd that childrenNumber can be a wrong value.", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r461502487", "createdAt": "2020-07-28T11:13:38Z", "author": {"login": "mukul1987"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "diffHunk": "@@ -501,7 +509,31 @@ public void write(DataOutput out) throws IOException {\n     return statusList.toArray(\n         new SnapshottableDirectoryStatus[statusList.size()]);\n   }\n-  \n+\n+  /**\n+   * List all the snapshots under a snapshottable directory.\n+   */\n+  public SnapshotStatus[] getSnapshotListing(INodesInPath iip)\n+      throws IOException {\n+    INodeDirectory srcRoot = getSnapshottableRoot(iip);\n+    ReadOnlyList<Snapshot> snapshotList = srcRoot.\n+        getDirectorySnapshottableFeature().getSnapshotList();\n+    SnapshotStatus[] statuses = new SnapshotStatus[snapshotList.size()];\n+    for (int count = 0; count < snapshotList.size(); count++) {\n+      Snapshot s = snapshotList.get(count);\n+      Snapshot.Root dir = s.getRoot();\n+      statuses[count] = new SnapshotStatus(dir.getModificationTime(),\n+          dir.getAccessTime(), dir.getFsPermission(),\n+          EnumSet.noneOf(HdfsFileStatus.Flags.class),\n+          dir.getUserName(), dir.getGroupName(),\n+          dir.getLocalNameBytes(), dir.getId(),\n+          dir.getChildrenNum(Snapshot.CURRENT_STATE_ID),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTUwMDM2OA=="}, "originalCommit": {"oid": "972ab57f8c6769f4291456eb37b880b836d29ce2"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTYyMTM1Ng==", "bodyText": "Addressed in the latest patch.", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r461621356", "createdAt": "2020-07-28T14:22:56Z", "author": {"login": "bshashikant"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "diffHunk": "@@ -501,7 +509,31 @@ public void write(DataOutput out) throws IOException {\n     return statusList.toArray(\n         new SnapshottableDirectoryStatus[statusList.size()]);\n   }\n-  \n+\n+  /**\n+   * List all the snapshots under a snapshottable directory.\n+   */\n+  public SnapshotStatus[] getSnapshotListing(INodesInPath iip)\n+      throws IOException {\n+    INodeDirectory srcRoot = getSnapshottableRoot(iip);\n+    ReadOnlyList<Snapshot> snapshotList = srcRoot.\n+        getDirectorySnapshottableFeature().getSnapshotList();\n+    SnapshotStatus[] statuses = new SnapshotStatus[snapshotList.size()];\n+    for (int count = 0; count < snapshotList.size(); count++) {\n+      Snapshot s = snapshotList.get(count);\n+      Snapshot.Root dir = s.getRoot();\n+      statuses[count] = new SnapshotStatus(dir.getModificationTime(),\n+          dir.getAccessTime(), dir.getFsPermission(),\n+          EnumSet.noneOf(HdfsFileStatus.Flags.class),\n+          dir.getUserName(), dir.getGroupName(),\n+          dir.getLocalNameBytes(), dir.getId(),\n+          dir.getChildrenNum(Snapshot.CURRENT_STATE_ID),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTUwMDM2OA=="}, "originalCommit": {"oid": "972ab57f8c6769f4291456eb37b880b836d29ce2"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4MTEzMTAwOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQxMToxOToxMFrOG4IDMQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQxNDoyMzowOFrOG4PJBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTUwNTMyOQ==", "bodyText": "wildcard imports here.", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r461505329", "createdAt": "2020-07-28T11:19:10Z", "author": {"login": "mukul1987"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.namenode.snapshot;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.protocol.SnapshotException;\n+import org.apache.hadoop.hdfs.protocol.SnapshotStatus;\n+import org.apache.hadoop.hdfs.protocol.SnapshottableDirectoryStatus;\n+import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;\n+import org.apache.hadoop.test.LambdaTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.*;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "972ab57f8c6769f4291456eb37b880b836d29ce2"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTYyMTUwOA==", "bodyText": "sure", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r461621508", "createdAt": "2020-07-28T14:23:08Z", "author": {"login": "bshashikant"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.namenode.snapshot;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.protocol.SnapshotException;\n+import org.apache.hadoop.hdfs.protocol.SnapshotStatus;\n+import org.apache.hadoop.hdfs.protocol.SnapshottableDirectoryStatus;\n+import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;\n+import org.apache.hadoop.test.LambdaTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.*;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTUwNTMyOQ=="}, "originalCommit": {"oid": "972ab57f8c6769f4291456eb37b880b836d29ce2"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4MTEzMjgzOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQxMToxOTo0N1rOG4IEcg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQxNDoyMTozMFrOG4PESg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTUwNTY1MA==", "bodyText": "The flag should be set to false ?", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r461505650", "createdAt": "2020-07-28T11:19:47Z", "author": {"login": "mukul1987"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.namenode.snapshot;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.protocol.SnapshotException;\n+import org.apache.hadoop.hdfs.protocol.SnapshotStatus;\n+import org.apache.hadoop.hdfs.protocol.SnapshottableDirectoryStatus;\n+import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;\n+import org.apache.hadoop.test.LambdaTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.*;\n+\n+/**\n+ * Tests listSnapshot.\n+ */\n+public class TestListSnapshot {\n+\n+  static final short REPLICATION = 3;\n+\n+  private final Path dir1 = new Path(\"/TestSnapshot1\");\n+\n+  Configuration conf;\n+  MiniDFSCluster cluster;\n+  FSNamesystem fsn;\n+  DistributedFileSystem hdfs;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(REPLICATION)\n+        .build();\n+    cluster.waitActive();\n+    fsn = cluster.getNamesystem();\n+    hdfs = cluster.getFileSystem();\n+    hdfs.mkdirs(dir1);\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n+  }\n+\n+  /**\n+   * Test listing all the snapshottable directories.\n+   */\n+  @Test(timeout = 60000)\n+  public void testListSnapshot() throws Exception {\n+    fsn.getSnapshotManager().setAllowNestedSnapshots(true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "972ab57f8c6769f4291456eb37b880b836d29ce2"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTYyMDI5OA==", "bodyText": "The flag is dset to true bcoz in the test , first we are making \"/\" snapshottable and then \"/dir1\" snapshottable ensuring it works either ways.", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r461620298", "createdAt": "2020-07-28T14:21:30Z", "author": {"login": "bshashikant"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.namenode.snapshot;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.protocol.SnapshotException;\n+import org.apache.hadoop.hdfs.protocol.SnapshotStatus;\n+import org.apache.hadoop.hdfs.protocol.SnapshottableDirectoryStatus;\n+import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;\n+import org.apache.hadoop.test.LambdaTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.*;\n+\n+/**\n+ * Tests listSnapshot.\n+ */\n+public class TestListSnapshot {\n+\n+  static final short REPLICATION = 3;\n+\n+  private final Path dir1 = new Path(\"/TestSnapshot1\");\n+\n+  Configuration conf;\n+  MiniDFSCluster cluster;\n+  FSNamesystem fsn;\n+  DistributedFileSystem hdfs;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(REPLICATION)\n+        .build();\n+    cluster.waitActive();\n+    fsn = cluster.getNamesystem();\n+    hdfs = cluster.getFileSystem();\n+    hdfs.mkdirs(dir1);\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n+  }\n+\n+  /**\n+   * Test listing all the snapshottable directories.\n+   */\n+  @Test(timeout = 60000)\n+  public void testListSnapshot() throws Exception {\n+    fsn.getSnapshotManager().setAllowNestedSnapshots(true);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTUwNTY1MA=="}, "originalCommit": {"oid": "972ab57f8c6769f4291456eb37b880b836d29ce2"}, "originalPosition": 73}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4MTEzNzM2OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQxMToyMToyMlrOG4IHVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQxNDoyMzoyOFrOG4PJ1w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTUwNjM4OA==", "bodyText": "create 2 snapshots for dir1 ?", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r461506388", "createdAt": "2020-07-28T11:21:22Z", "author": {"login": "mukul1987"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.namenode.snapshot;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.protocol.SnapshotException;\n+import org.apache.hadoop.hdfs.protocol.SnapshotStatus;\n+import org.apache.hadoop.hdfs.protocol.SnapshottableDirectoryStatus;\n+import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;\n+import org.apache.hadoop.test.LambdaTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.*;\n+\n+/**\n+ * Tests listSnapshot.\n+ */\n+public class TestListSnapshot {\n+\n+  static final short REPLICATION = 3;\n+\n+  private final Path dir1 = new Path(\"/TestSnapshot1\");\n+\n+  Configuration conf;\n+  MiniDFSCluster cluster;\n+  FSNamesystem fsn;\n+  DistributedFileSystem hdfs;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(REPLICATION)\n+        .build();\n+    cluster.waitActive();\n+    fsn = cluster.getNamesystem();\n+    hdfs = cluster.getFileSystem();\n+    hdfs.mkdirs(dir1);\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n+  }\n+\n+  /**\n+   * Test listing all the snapshottable directories.\n+   */\n+  @Test(timeout = 60000)\n+  public void testListSnapshot() throws Exception {\n+    fsn.getSnapshotManager().setAllowNestedSnapshots(true);\n+\n+    // Initially there is no snapshottable directories in the system\n+    SnapshotStatus[] snapshotStatuses = null;\n+    SnapshottableDirectoryStatus[] dirs = hdfs.getSnapshottableDirListing();\n+    assertNull(dirs);\n+    LambdaTestUtils.intercept(SnapshotException.class,\n+        \"Directory is not a \" + \"snapshottable directory\",\n+        () -> hdfs.getSnapshotListing(dir1));\n+    // Make root as snapshottable\n+    final Path root = new Path(\"/\");\n+    hdfs.allowSnapshot(root);\n+    dirs = hdfs.getSnapshottableDirListing();\n+    assertEquals(1, dirs.length);\n+    assertEquals(\"\", dirs[0].getDirStatus().getLocalName());\n+    assertEquals(root, dirs[0].getFullPath());\n+    snapshotStatuses = hdfs.getSnapshotListing(root);\n+    assertTrue(snapshotStatuses.length == 0);\n+    // Make root non-snaphsottable\n+    hdfs.disallowSnapshot(root);\n+    dirs = hdfs.getSnapshottableDirListing();\n+    assertNull(dirs);\n+    snapshotStatuses = hdfs.getSnapshotListing(root);\n+    assertTrue(snapshotStatuses.length == 0);\n+\n+    // Make dir1 as snapshottable\n+    hdfs.allowSnapshot(dir1);\n+    hdfs.createSnapshot(dir1, \"s0\");\n+    snapshotStatuses = hdfs.getSnapshotListing(dir1);\n+    assertEquals(1, snapshotStatuses.length);\n+    assertEquals(\"s0\", snapshotStatuses[0].getDirStatus().\n+        getLocalName());\n+    assertEquals(SnapshotTestHelper.getSnapshotRoot(dir1, \"s0\"),\n+        snapshotStatuses[0].getFullPath());\n+    // snapshot id is zero\n+    assertEquals(0, snapshotStatuses[0].getSnapshotID());\n+    // Create a snapshot for dir2", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "972ab57f8c6769f4291456eb37b880b836d29ce2"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTYyMTcxOQ==", "bodyText": "addressed", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r461621719", "createdAt": "2020-07-28T14:23:28Z", "author": {"login": "bshashikant"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.namenode.snapshot;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.protocol.SnapshotException;\n+import org.apache.hadoop.hdfs.protocol.SnapshotStatus;\n+import org.apache.hadoop.hdfs.protocol.SnapshottableDirectoryStatus;\n+import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;\n+import org.apache.hadoop.test.LambdaTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.*;\n+\n+/**\n+ * Tests listSnapshot.\n+ */\n+public class TestListSnapshot {\n+\n+  static final short REPLICATION = 3;\n+\n+  private final Path dir1 = new Path(\"/TestSnapshot1\");\n+\n+  Configuration conf;\n+  MiniDFSCluster cluster;\n+  FSNamesystem fsn;\n+  DistributedFileSystem hdfs;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(REPLICATION)\n+        .build();\n+    cluster.waitActive();\n+    fsn = cluster.getNamesystem();\n+    hdfs = cluster.getFileSystem();\n+    hdfs.mkdirs(dir1);\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n+  }\n+\n+  /**\n+   * Test listing all the snapshottable directories.\n+   */\n+  @Test(timeout = 60000)\n+  public void testListSnapshot() throws Exception {\n+    fsn.getSnapshotManager().setAllowNestedSnapshots(true);\n+\n+    // Initially there is no snapshottable directories in the system\n+    SnapshotStatus[] snapshotStatuses = null;\n+    SnapshottableDirectoryStatus[] dirs = hdfs.getSnapshottableDirListing();\n+    assertNull(dirs);\n+    LambdaTestUtils.intercept(SnapshotException.class,\n+        \"Directory is not a \" + \"snapshottable directory\",\n+        () -> hdfs.getSnapshotListing(dir1));\n+    // Make root as snapshottable\n+    final Path root = new Path(\"/\");\n+    hdfs.allowSnapshot(root);\n+    dirs = hdfs.getSnapshottableDirListing();\n+    assertEquals(1, dirs.length);\n+    assertEquals(\"\", dirs[0].getDirStatus().getLocalName());\n+    assertEquals(root, dirs[0].getFullPath());\n+    snapshotStatuses = hdfs.getSnapshotListing(root);\n+    assertTrue(snapshotStatuses.length == 0);\n+    // Make root non-snaphsottable\n+    hdfs.disallowSnapshot(root);\n+    dirs = hdfs.getSnapshottableDirListing();\n+    assertNull(dirs);\n+    snapshotStatuses = hdfs.getSnapshotListing(root);\n+    assertTrue(snapshotStatuses.length == 0);\n+\n+    // Make dir1 as snapshottable\n+    hdfs.allowSnapshot(dir1);\n+    hdfs.createSnapshot(dir1, \"s0\");\n+    snapshotStatuses = hdfs.getSnapshotListing(dir1);\n+    assertEquals(1, snapshotStatuses.length);\n+    assertEquals(\"s0\", snapshotStatuses[0].getDirStatus().\n+        getLocalName());\n+    assertEquals(SnapshotTestHelper.getSnapshotRoot(dir1, \"s0\"),\n+        snapshotStatuses[0].getFullPath());\n+    // snapshot id is zero\n+    assertEquals(0, snapshotStatuses[0].getSnapshotID());\n+    // Create a snapshot for dir2", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTUwNjM4OA=="}, "originalCommit": {"oid": "972ab57f8c6769f4291456eb37b880b836d29ce2"}, "originalPosition": 109}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4MjgxNTA1OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/proto/hdfs.proto", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQxODowMjoyNlrOG4YSQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwMTo0ODozMVrOG4loyQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTc3MTMzMQ==", "bodyText": "we should also dispay if a particular snapshot is deleted/garbage collected or not.", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r461771331", "createdAt": "2020-07-28T18:02:26Z", "author": {"login": "mukul1987"}, "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/proto/hdfs.proto", "diffHunk": "@@ -563,13 +563,34 @@ message SnapshottableDirectoryStatusProto {\n   required bytes parent_fullpath = 4;\n }\n \n+/**\n+ * Status of a snapshot directory: besides the normal information for\n+ * a directory status, also include snapshot ID, and\n+ * the full path of the parent directory.\n+ */\n+message SnapshotStatusProto {\n+  required HdfsFileStatusProto dirStatus = 1;\n+\n+  // Fields specific for snapshot directory\n+  required uint32 snapshotID = 2;\n+  required bytes parent_fullpath = 3;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "972ab57f8c6769f4291456eb37b880b836d29ce2"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTk5MDA4OQ==", "bodyText": "I think the intent was to hide the information from the user whether a snapshot is deleted or not. This is a user cmd and displaying it won't server the purpose if we are supposed to hide it.", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r461990089", "createdAt": "2020-07-29T01:48:31Z", "author": {"login": "bshashikant"}, "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/proto/hdfs.proto", "diffHunk": "@@ -563,13 +563,34 @@ message SnapshottableDirectoryStatusProto {\n   required bytes parent_fullpath = 4;\n }\n \n+/**\n+ * Status of a snapshot directory: besides the normal information for\n+ * a directory status, also include snapshot ID, and\n+ * the full path of the parent directory.\n+ */\n+message SnapshotStatusProto {\n+  required HdfsFileStatusProto dirStatus = 1;\n+\n+  // Fields specific for snapshot directory\n+  required uint32 snapshotID = 2;\n+  required bytes parent_fullpath = 3;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTc3MTMzMQ=="}, "originalCommit": {"oid": "972ab57f8c6769f4291456eb37b880b836d29ce2"}, "originalPosition": 14}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3335, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}