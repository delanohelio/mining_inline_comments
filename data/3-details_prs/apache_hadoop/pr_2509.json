{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTMwNTE1Mjc0", "number": 2509, "title": "HADOOP-17404. ABFS: Small write - Merge append and flush", "bodyText": "When Hflush or Hsync APIs are called, a call is made to store backend to commit the data that was appended.\nIf the data size written by Hadoop app is small, i.e. data size :\n(a) before any of HFlush/HSync call is made or\n(b) between 2 HFlush/Hsync API calls\nis less than write buffer size, 2 separate calls, one for append and another for flush is made,\nApps that do such small writes eventually end up with almost similar number of calls for flush and append.\nThis commit enables Flush to be piggybacked onto append call for such short write scenarios. This is guarded with config \"fs.azure.write.enableappendwithflush\" which is set to off by default as it needs a relevant change in backend to propagate.\nTests asserting number of requests made, request data sizes, file sizes post append+flush and file content checks for various combinations of append/flush/close sets with and without the small write optimization is added.\nExisting tests in ITestAbfsNetworkStatistics asserting Http stats were rewritten for easy readability.\n(Test results published in end of PR conversation tab.)", "createdAt": "2020-12-01T19:13:04Z", "url": "https://github.com/apache/hadoop/pull/2509", "merged": true, "mergeCommit": {"oid": "b612c310c26394aa406c99d8598c9cb7621df052"}, "closed": true, "closedAt": "2021-01-06T18:43:37Z", "author": {"login": "snvijaya"}, "timelineItems": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdh-vRYgH2gAyNTMwNTE1Mjc0OjViMDg5ZDdjZjhkOWQyNmJkOTU3YWJmNmUxMzE3ZDhiZTUwNWRkOTA=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdtkCXyAFqTU2Mjk1MzczNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "5b089d7cf8d9d26bd957abf6e1317d8be505dd90", "author": {"user": {"login": "snvijaya", "name": "Sneha Vijayarajan"}}, "url": "https://github.com/apache/hadoop/commit/5b089d7cf8d9d26bd957abf6e1317d8be505dd90", "committedDate": "2020-12-01T19:02:29Z", "message": "Small write - Merge append and flush"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3301124e58def96e4e7caf9dca388c4d653abeb4", "author": {"user": {"login": "snvijaya", "name": "Sneha Vijayarajan"}}, "url": "https://github.com/apache/hadoop/commit/3301124e58def96e4e7caf9dca388c4d653abeb4", "committedDate": "2020-12-03T03:30:04Z", "message": "Findbug fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "843a01510611c4a5a1e4a365f2a5a2e3b43a51f9", "author": {"user": {"login": "snvijaya", "name": "Sneha Vijayarajan"}}, "url": "https://github.com/apache/hadoop/commit/843a01510611c4a5a1e4a365f2a5a2e3b43a51f9", "committedDate": "2020-12-10T04:22:16Z", "message": "Findbugs and checkstyle fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1e7f30877dcc2e3baedf99eeb29c4abff7bf0423", "author": {"user": {"login": "snvijaya", "name": "Sneha Vijayarajan"}}, "url": "https://github.com/apache/hadoop/commit/1e7f30877dcc2e3baedf99eeb29c4abff7bf0423", "committedDate": "2020-12-10T11:25:33Z", "message": "Review comment incorporation"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5MTM1NzY5", "url": "https://github.com/apache/hadoop/pull/2509#pullrequestreview-549135769", "createdAt": "2020-12-10T12:13:35Z", "commit": {"oid": "1e7f30877dcc2e3baedf99eeb29c4abff7bf0423"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxMjoxMzozNVrOIDGTLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxMjoxMzozNVrOIDGTLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDExOTg1Mg==", "bodyText": "This can be class level constant right, the lieral \"true\" as such does not have anuthing to do with HTTP related operations.", "url": "https://github.com/apache/hadoop/pull/2509#discussion_r540119852", "createdAt": "2020-12-10T12:13:35Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/AbfsHttpConstants.java", "diffHunk": "@@ -76,6 +76,7 @@\n   public static final String AT = \"@\";\n   public static final String HTTP_HEADER_PREFIX = \"x-ms-\";\n   public static final String HASH = \"#\";\n+  public static final String TRUE = \"true\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1e7f30877dcc2e3baedf99eeb29c4abff7bf0423"}, "originalPosition": 4}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5MTM3NDU3", "url": "https://github.com/apache/hadoop/pull/2509#pullrequestreview-549137457", "createdAt": "2020-12-10T12:15:54Z", "commit": {"oid": "1e7f30877dcc2e3baedf99eeb29c4abff7bf0423"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxMjoxNTo1NVrOIDGY1w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxMjoxNTo1NVrOIDGY1w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDEyMTMwMw==", "bodyText": "Can this be 2 separate if blocks", "url": "https://github.com/apache/hadoop/pull/2509#discussion_r540121303", "createdAt": "2020-12-10T12:15:55Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java", "diffHunk": "@@ -395,38 +396,58 @@ public AbfsRestOperation renameIdempotencyCheckOp(\n     return op;\n   }\n \n-  public AbfsRestOperation append(final String path, final long position, final byte[] buffer, final int offset,\n-                                  final int length, final String cachedSasToken, final boolean isAppendBlob) throws AzureBlobFileSystemException {\n+  public AbfsRestOperation append(final String path, final byte[] buffer,\n+      AppendRequestParameters reqParams, final String cachedSasToken)\n+      throws AzureBlobFileSystemException {\n     final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\n     // JDK7 does not support PATCH, so to workaround the issue we will use\n     // PUT and specify the real method in the X-Http-Method-Override header.\n     requestHeaders.add(new AbfsHttpHeader(X_HTTP_METHOD_OVERRIDE,\n-            HTTP_METHOD_PATCH));\n+        HTTP_METHOD_PATCH));\n \n     final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\n     abfsUriQueryBuilder.addQuery(QUERY_PARAM_ACTION, APPEND_ACTION);\n-    abfsUriQueryBuilder.addQuery(QUERY_PARAM_POSITION, Long.toString(position));\n+    abfsUriQueryBuilder.addQuery(QUERY_PARAM_POSITION, Long.toString(reqParams.getPosition()));\n+\n+    if ((reqParams.getMode() == AppendRequestParameters.Mode.FLUSH_MODE) || (", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1e7f30877dcc2e3baedf99eeb29c4abff7bf0423"}, "originalPosition": 29}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5MTQ1ODcy", "url": "https://github.com/apache/hadoop/pull/2509#pullrequestreview-549145872", "createdAt": "2020-12-10T12:27:08Z", "commit": {"oid": "1e7f30877dcc2e3baedf99eeb29c4abff7bf0423"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxMjoyNzowOFrOIDG1MQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxMjoyNzowOFrOIDG1MQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDEyODU2MQ==", "bodyText": "you may remove this new line in between", "url": "https://github.com/apache/hadoop/pull/2509#discussion_r540128561", "createdAt": "2020-12-10T12:27:08Z", "author": {"login": "bilaharith"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestSmallWriteOptimization.java", "diffHunk": "@@ -0,0 +1,524 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.util.Arrays;\n+import java.util.Random;\n+import java.util.UUID;\n+import java.util.Map;\n+import java.io.IOException;\n+\n+import org.assertj.core.api.Assertions;\n+import org.junit.Assume;\n+import org.junit.runners.Parameterized;\n+import org.junit.runner.RunWith;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1e7f30877dcc2e3baedf99eeb29c4abff7bf0423"}, "originalPosition": 37}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5MTQ2OTk1", "url": "https://github.com/apache/hadoop/pull/2509#pullrequestreview-549146995", "createdAt": "2020-12-10T12:28:41Z", "commit": {"oid": "1e7f30877dcc2e3baedf99eeb29c4abff7bf0423"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ4OTY3MTk0", "url": "https://github.com/apache/hadoop/pull/2509#pullrequestreview-548967194", "createdAt": "2020-12-10T08:48:33Z", "commit": {"oid": "843a01510611c4a5a1e4a365f2a5a2e3b43a51f9"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQwODo0ODozNFrOIC93bw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQwODo0ODozNFrOIC93bw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTk4MTY3OQ==", "bodyText": "ensure number of calls to append is correct", "url": "https://github.com/apache/hadoop/pull/2509#discussion_r539981679", "createdAt": "2020-12-10T08:48:34Z", "author": {"login": "vinaysbadami"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -108,13 +103,15 @@ public void verifyShortWriteRequest() throws Exception {\n \n     out.hsync();\n \n-    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acBufferOffset.capture(), acBufferLength.capture(),\n-                                    acSASToken.capture(), acAppendBlobAppend.capture());\n-    assertThat(Arrays.asList(PATH, PATH)).describedAs(\"Path of the requests\").isEqualTo(acString.getAllValues());\n-    assertThat(Arrays.asList(Long.valueOf(0), Long.valueOf(WRITE_SIZE))).describedAs(\"Write Position\").isEqualTo(acLong.getAllValues());\n-    assertThat(Arrays.asList(0, 0)).describedAs(\"Buffer Offset\").isEqualTo(acBufferOffset.getAllValues());\n-    assertThat(Arrays.asList(WRITE_SIZE, 2*WRITE_SIZE)).describedAs(\"Buffer length\").isEqualTo(acBufferLength.getAllValues());\n+    AppendRequestParameters firstReqParameters = new AppendRequestParameters(\n+        0, 0, WRITE_SIZE, APPEND_MODE, false);\n+    AppendRequestParameters secondReqParameters = new AppendRequestParameters(\n+        WRITE_SIZE, 0, 2 * WRITE_SIZE, APPEND_MODE, false);\n \n+    verify(client, times(1)).append(\n+        eq(PATH), any(byte[].class), refEq(firstReqParameters), any());\n+    verify(client, times(1)).append(\n+        eq(PATH), any(byte[].class), refEq(secondReqParameters), any());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "843a01510611c4a5a1e4a365f2a5a2e3b43a51f9"}, "originalPosition": 76}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "21b8e6d822f7113ddd2c283a48716ac69813267e", "author": {"user": {"login": "snvijaya", "name": "Sneha Vijayarajan"}}, "url": "https://github.com/apache/hadoop/commit/21b8e6d822f7113ddd2c283a48716ac69813267e", "committedDate": "2020-12-11T06:24:02Z", "message": "Addressing new line review comment"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUxMTExODQ3", "url": "https://github.com/apache/hadoop/pull/2509#pullrequestreview-551111847", "createdAt": "2020-12-14T07:51:07Z", "commit": {"oid": "21b8e6d822f7113ddd2c283a48716ac69813267e"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYyMTA0Njc2", "url": "https://github.com/apache/hadoop/pull/2509#pullrequestreview-562104676", "createdAt": "2021-01-05T20:22:14Z", "commit": {"oid": "21b8e6d822f7113ddd2c283a48716ac69813267e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNVQyMDoyMjoxNFrOIOl-Sg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNVQyMDoyMjoxNFrOIOl-Sg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjE3MzEzMA==", "bodyText": "for newly added config key, a little comment would be very helpful", "url": "https://github.com/apache/hadoop/pull/2509#discussion_r552173130", "createdAt": "2021-01-05T20:22:14Z", "author": {"login": "DadanielZ"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java", "diffHunk": "@@ -55,6 +55,7 @@\n   public static final String AZURE_WRITE_MAX_CONCURRENT_REQUESTS = \"fs.azure.write.max.concurrent.requests\";\n   public static final String AZURE_WRITE_MAX_REQUESTS_TO_QUEUE = \"fs.azure.write.max.requests.to.queue\";\n   public static final String AZURE_WRITE_BUFFER_SIZE = \"fs.azure.write.request.size\";\n+  public static final String AZURE_ENABLE_SMALL_WRITE_OPTIMIZATION = \"fs.azure.write.enableappendwithflush\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "21b8e6d822f7113ddd2c283a48716ac69813267e"}, "originalPosition": 4}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYyMTA2MTQx", "url": "https://github.com/apache/hadoop/pull/2509#pullrequestreview-562106141", "createdAt": "2021-01-05T20:24:35Z", "commit": {"oid": "21b8e6d822f7113ddd2c283a48716ac69813267e"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a8a9393a3a24e354fcd22225978e63568e7a643f", "author": {"user": {"login": "snvijaya", "name": "Sneha Vijayarajan"}}, "url": "https://github.com/apache/hadoop/commit/a8a9393a3a24e354fcd22225978e63568e7a643f", "committedDate": "2021-01-06T11:13:05Z", "message": "Merge to trunk"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fa2a50250e60bcd6f5416dc25e6bf37c33fbd4d7", "author": {"user": {"login": "snvijaya", "name": "Sneha Vijayarajan"}}, "url": "https://github.com/apache/hadoop/commit/fa2a50250e60bcd6f5416dc25e6bf37c33fbd4d7", "committedDate": "2021-01-06T17:00:22Z", "message": "Addressing review comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYyOTUzNzM3", "url": "https://github.com/apache/hadoop/pull/2509#pullrequestreview-562953737", "createdAt": "2021-01-06T18:43:00Z", "commit": {"oid": "fa2a50250e60bcd6f5416dc25e6bf37c33fbd4d7"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3352, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}