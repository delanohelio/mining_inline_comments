{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQ2NjY1NzIz", "number": 2578, "title": "HDFS-15754. Add DataNode packet metrics", "bodyText": "NOTICE\nPlease create an issue in ASF JIRA before opening a pull request,\nand you need to set the title of the pull request which starts with\nthe corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.)\nFor more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute", "createdAt": "2020-12-29T22:59:46Z", "url": "https://github.com/apache/hadoop/pull/2578", "merged": true, "mergeCommit": {"oid": "87bd4d2aca5bdb81a4c6e4980763adf26ba106e8"}, "closed": true, "closedAt": "2021-01-08T07:46:24Z", "author": {"login": "fengnanli"}, "timelineItems": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdrC5z4AH2gAyNTQ2NjY1NzIzOjFkYTBiOWY4NDkyNzE3NDQyMTRkOTQ3NGY2ZjBhNTE0MDg1ZGQzNmI=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdtvClHgH2gAyNTQ2NjY1NzIzOjdmYTAyODYyNTU4Y2U1ZmZiM2U0YjVmMjBlMWY3NTA2NzMwMzM5ZTY=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "1da0b9f849271744214d9474f6f0a514085dd36b", "author": {"user": {"login": "fengnanli", "name": "lfengnan"}}, "url": "https://github.com/apache/hadoop/commit/1da0b9f849271744214d9474f6f0a514085dd36b", "committedDate": "2020-12-29T22:58:56Z", "message": "[HDFS-15754] Add DataNode packet metrics"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "700bce989234f9c4dbcc5b92be551ea221c1073b", "author": {"user": {"login": "fengnanli", "name": "lfengnan"}}, "url": "https://github.com/apache/hadoop/commit/700bce989234f9c4dbcc5b92be551ea221c1073b", "committedDate": "2020-12-29T22:57:07Z", "message": "Add DataNode packet metrics"}, "afterCommit": {"oid": "1da0b9f849271744214d9474f6f0a514085dd36b", "author": {"user": {"login": "fengnanli", "name": "lfengnan"}}, "url": "https://github.com/apache/hadoop/commit/1da0b9f849271744214d9474f6f0a514085dd36b", "committedDate": "2020-12-29T22:58:56Z", "message": "[HDFS-15754] Add DataNode packet metrics"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU5NzM0OTYz", "url": "https://github.com/apache/hadoop/pull/2578#pullrequestreview-559734963", "createdAt": "2020-12-29T23:11:49Z", "commit": {"oid": "1da0b9f849271744214d9474f6f0a514085dd36b"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQyMzoxMTo1MFrOIMaMyA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQyMzoxMjowOVrOIMaNFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTg4MzA4MA==", "bodyText": "We'll need to add these new metrics to here right?", "url": "https://github.com/apache/hadoop/pull/2578#discussion_r549883080", "createdAt": "2020-12-29T23:11:50Z", "author": {"login": "sunchao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeMetrics.java", "diffHunk": "@@ -183,6 +183,11 @@\n   @Metric private MutableRate checkAndUpdateOp;\n   @Metric private MutableRate updateReplicaUnderRecoveryOp;\n \n+  @Metric MutableCounterLong totalPacketsReceived;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1da0b9f849271744214d9474f6f0a514085dd36b"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTg4MzE1OA==", "bodyText": "nit: code style", "url": "https://github.com/apache/hadoop/pull/2578#discussion_r549883158", "createdAt": "2020-12-29T23:12:09Z", "author": {"login": "sunchao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeMetrics.java", "diffHunk": "@@ -161,6 +163,65 @@ public void testReceivePacketMetrics() throws Exception {\n     }\n   }\n \n+  @Test\n+  public void testReceivePacketSlowMetrics() throws Exception {\n+    Configuration conf = new HdfsConfiguration();\n+    final int interval = 1;\n+    conf.set(DFSConfigKeys.DFS_METRICS_PERCENTILES_INTERVALS_KEY, \"\" + interval);\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf)\n+        .numDataNodes(3).build();\n+    try {\n+      cluster.waitActive();\n+      DistributedFileSystem fs = cluster.getFileSystem();\n+      final DataNodeFaultInjector injector =\n+          Mockito.mock(DataNodeFaultInjector.class);\n+      Mockito.doAnswer(new Answer() {\n+        @Override\n+        public Object answer(InvocationOnMock invocationOnMock)\n+            throws Throwable {\n+          // make the op taking longer time\n+          Thread.sleep(1000);\n+          return null;\n+        }\n+      }).when(injector).stopSendingPacketDownstream(Mockito.anyString());\n+      Mockito.doAnswer(new Answer() {\n+        @Override\n+        public Object answer(InvocationOnMock invocationOnMock)\n+            throws Throwable {\n+          // make the op taking longer time\n+          Thread.sleep(1000);\n+          return null;\n+        }\n+      }).when(injector).delayWriteToOsCache();\n+      Mockito.doAnswer(new Answer() {\n+        @Override\n+        public Object answer(InvocationOnMock invocationOnMock)\n+            throws Throwable {\n+          // make the op taking longer time\n+          Thread.sleep(1000);\n+          return null;\n+        }\n+      }).when(injector).delayWriteToDisk();\n+      DataNodeFaultInjector.set(injector);\n+      Path testFile = new Path(\"/testFlushNanosMetric.txt\");\n+      FSDataOutputStream fout = fs.create(testFile);\n+      fout.write(new byte[1]);\n+      fout.hsync();\n+      fout.close();\n+      List<DataNode> datanodes = cluster.getDataNodes();\n+      DataNode datanode = datanodes.get(0);\n+      MetricsRecordBuilder dnMetrics = getMetrics(datanode.getMetrics().name());\n+      assertTrue(\"More than 1 packet received\",\n+          getLongCounter(\"TotalPacketsReceived\", dnMetrics) > 1L);\n+      assertTrue(\"More than 1 slow packet to mirror\",\n+          getLongCounter(\"TotalPacketsSlowWriteToMirror\", dnMetrics) > 1L);\n+      assertCounter(\"TotalPacketsSlowWriteToDisk\", 1L, dnMetrics);\n+      assertCounter(\"TotalPacketsSlowWriteOsCache\", 0L, dnMetrics);\n+    } finally {\n+      if (cluster != null) {cluster.shutdown();}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1da0b9f849271744214d9474f6f0a514085dd36b"}, "originalPosition": 68}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYwMTQ2NDk2", "url": "https://github.com/apache/hadoop/pull/2578#pullrequestreview-560146496", "createdAt": "2020-12-30T17:55:47Z", "commit": {"oid": "1da0b9f849271744214d9474f6f0a514085dd36b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMFQxNzo1NTo0N1rOIMySYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMFQxNzo1NTo0N1rOIMySYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDI3NzczMA==", "bodyText": "As we are at it, let's use the logger format. We still need the Arrays toString so we need the isWarnEnabled though.", "url": "https://github.com/apache/hadoop/pull/2578#discussion_r550277730", "createdAt": "2020-12-30T17:55:47Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java", "diffHunk": "@@ -603,12 +604,15 @@ private int receivePacket() throws IOException {\n             mirrorAddr,\n             duration);\n         trackSendPacketToLastNodeInPipeline(duration);\n-        if (duration > datanodeSlowLogThresholdMs && LOG.isWarnEnabled()) {\n-          LOG.warn(\"Slow BlockReceiver write packet to mirror took \" + duration\n-              + \"ms (threshold=\" + datanodeSlowLogThresholdMs + \"ms), \"\n-              + \"downstream DNs=\" + Arrays.toString(downstreamDNs)\n-              + \", blockId=\" + replicaInfo.getBlockId()\n-              + \", seqno=\" + seqno);\n+        if (duration > datanodeSlowLogThresholdMs) {\n+          datanode.metrics.incrPacketSlowWriteToMirror();\n+          if (LOG.isWarnEnabled()) {\n+            LOG.warn(\"Slow BlockReceiver write packet to mirror took \" + duration\n+                + \"ms (threshold=\" + datanodeSlowLogThresholdMs + \"ms), \"\n+                + \"downstream DNs=\" + Arrays.toString(downstreamDNs)\n+                + \", blockId=\" + replicaInfo.getBlockId()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1da0b9f849271744214d9474f6f0a514085dd36b"}, "originalPosition": 24}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYwMTQ2ODIw", "url": "https://github.com/apache/hadoop/pull/2578#pullrequestreview-560146820", "createdAt": "2020-12-30T17:56:48Z", "commit": {"oid": "1da0b9f849271744214d9474f6f0a514085dd36b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMFQxNzo1Njo0OFrOIMyTrw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMFQxNzo1Njo0OFrOIMyTrw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDI3ODA2Mw==", "bodyText": "setInt", "url": "https://github.com/apache/hadoop/pull/2578#discussion_r550278063", "createdAt": "2020-12-30T17:56:48Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeMetrics.java", "diffHunk": "@@ -161,6 +163,65 @@ public void testReceivePacketMetrics() throws Exception {\n     }\n   }\n \n+  @Test\n+  public void testReceivePacketSlowMetrics() throws Exception {\n+    Configuration conf = new HdfsConfiguration();\n+    final int interval = 1;\n+    conf.set(DFSConfigKeys.DFS_METRICS_PERCENTILES_INTERVALS_KEY, \"\" + interval);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1da0b9f849271744214d9474f6f0a514085dd36b"}, "originalPosition": 17}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYwMTQ3MDI5", "url": "https://github.com/apache/hadoop/pull/2578#pullrequestreview-560147029", "createdAt": "2020-12-30T17:57:25Z", "commit": {"oid": "1da0b9f849271744214d9474f6f0a514085dd36b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMFQxNzo1NzoyNVrOIMyUZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMFQxNzo1NzoyNVrOIMyUZg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDI3ODI0Ng==", "bodyText": "Extract the sleeping answer and return for each?", "url": "https://github.com/apache/hadoop/pull/2578#discussion_r550278246", "createdAt": "2020-12-30T17:57:25Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeMetrics.java", "diffHunk": "@@ -161,6 +163,65 @@ public void testReceivePacketMetrics() throws Exception {\n     }\n   }\n \n+  @Test\n+  public void testReceivePacketSlowMetrics() throws Exception {\n+    Configuration conf = new HdfsConfiguration();\n+    final int interval = 1;\n+    conf.set(DFSConfigKeys.DFS_METRICS_PERCENTILES_INTERVALS_KEY, \"\" + interval);\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf)\n+        .numDataNodes(3).build();\n+    try {\n+      cluster.waitActive();\n+      DistributedFileSystem fs = cluster.getFileSystem();\n+      final DataNodeFaultInjector injector =\n+          Mockito.mock(DataNodeFaultInjector.class);\n+      Mockito.doAnswer(new Answer() {\n+        @Override\n+        public Object answer(InvocationOnMock invocationOnMock)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1da0b9f849271744214d9474f6f0a514085dd36b"}, "originalPosition": 27}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "05f04b1ca4ced0849b1fc0685077f88db2a47027", "author": {"user": {"login": "fengnanli", "name": "lfengnan"}}, "url": "https://github.com/apache/hadoop/commit/05f04b1ca4ced0849b1fc0685077f88db2a47027", "committedDate": "2021-01-04T07:30:33Z", "message": "Address comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYzMDAxMTIw", "url": "https://github.com/apache/hadoop/pull/2578#pullrequestreview-563001120", "createdAt": "2021-01-06T19:57:51Z", "commit": {"oid": "05f04b1ca4ced0849b1fc0685077f88db2a47027"}, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQxOTo1Nzo1MVrOIPUX2A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQxOTo1ODowNVrOIPUYQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkzMzMzNg==", "bodyText": "nit: name this to incrPacketsSlowWriteToOsCache?", "url": "https://github.com/apache/hadoop/pull/2578#discussion_r552933336", "createdAt": "2021-01-06T19:57:51Z", "author": {"login": "sunchao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeMetrics.java", "diffHunk": "@@ -690,4 +695,20 @@ public void addCheckAndUpdateOp(long latency) {\n   public void addUpdateReplicaUnderRecoveryOp(long latency) {\n     updateReplicaUnderRecoveryOp.add(latency);\n   }\n+\n+  public void incrPacketsReceived() {\n+    packetsReceived.incr();\n+  }\n+\n+  public void incrPacketsSlowWriteToMirror() {\n+    packetsSlowWriteToMirror.incr();\n+  }\n+\n+  public void incrPacketsSlowWriteToDisk() {\n+    packetsSlowWriteToDisk.incr();\n+  }\n+\n+  public void incrPacketsSlowWriteOsCache() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "05f04b1ca4ced0849b1fc0685077f88db2a47027"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkzMzQ0Mg==", "bodyText": "ditto", "url": "https://github.com/apache/hadoop/pull/2578#discussion_r552933442", "createdAt": "2021-01-06T19:58:05Z", "author": {"login": "sunchao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeMetrics.java", "diffHunk": "@@ -183,6 +183,11 @@\n   @Metric private MutableRate checkAndUpdateOp;\n   @Metric private MutableRate updateReplicaUnderRecoveryOp;\n \n+  @Metric MutableCounterLong packetsReceived;\n+  @Metric MutableCounterLong packetsSlowWriteToMirror;\n+  @Metric MutableCounterLong packetsSlowWriteToDisk;\n+  @Metric MutableCounterLong packetsSlowWriteOsCache;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "05f04b1ca4ced0849b1fc0685077f88db2a47027"}, "originalPosition": 7}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3be5d26cec4c93b5b47ef795ee320b38e5a9356d", "author": {"user": {"login": "fengnanli", "name": "lfengnan"}}, "url": "https://github.com/apache/hadoop/commit/3be5d26cec4c93b5b47ef795ee320b38e5a9356d", "committedDate": "2021-01-07T00:37:31Z", "message": "Rename"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYzMjMzNjk1", "url": "https://github.com/apache/hadoop/pull/2578#pullrequestreview-563233695", "createdAt": "2021-01-07T06:14:10Z", "commit": {"oid": "3be5d26cec4c93b5b47ef795ee320b38e5a9356d"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wN1QwNjoxNDoxMFrOIPgTgg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wN1QwNjoxNDoxMFrOIPgTgg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzEyODgzNA==", "bodyText": "I think this also needs update.", "url": "https://github.com/apache/hadoop/pull/2578#discussion_r553128834", "createdAt": "2021-01-07T06:14:10Z", "author": {"login": "sunchao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeMetrics.java", "diffHunk": "@@ -161,6 +163,53 @@ public void testReceivePacketMetrics() throws Exception {\n     }\n   }\n \n+  @Test\n+  public void testReceivePacketSlowMetrics() throws Exception {\n+    Configuration conf = new HdfsConfiguration();\n+    final int interval = 1;\n+    conf.setInt(DFSConfigKeys.DFS_METRICS_PERCENTILES_INTERVALS_KEY, interval);\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf)\n+        .numDataNodes(3).build();\n+    try {\n+      cluster.waitActive();\n+      DistributedFileSystem fs = cluster.getFileSystem();\n+      final DataNodeFaultInjector injector =\n+          Mockito.mock(DataNodeFaultInjector.class);\n+      Answer answer = new Answer() {\n+        @Override\n+        public Object answer(InvocationOnMock invocationOnMock)\n+            throws Throwable {\n+          // make the op taking longer time\n+          Thread.sleep(1000);\n+          return null;\n+        }\n+      };\n+      Mockito.doAnswer(answer).when(injector).\n+          stopSendingPacketDownstream(Mockito.anyString());\n+      Mockito.doAnswer(answer).when(injector).delayWriteToOsCache();\n+      Mockito.doAnswer(answer).when(injector).delayWriteToDisk();\n+      DataNodeFaultInjector.set(injector);\n+      Path testFile = new Path(\"/testFlushNanosMetric.txt\");\n+      FSDataOutputStream fout = fs.create(testFile);\n+      fout.write(new byte[1]);\n+      fout.hsync();\n+      fout.close();\n+      List<DataNode> datanodes = cluster.getDataNodes();\n+      DataNode datanode = datanodes.get(0);\n+      MetricsRecordBuilder dnMetrics = getMetrics(datanode.getMetrics().name());\n+      assertTrue(\"More than 1 packet received\",\n+          getLongCounter(\"TotalPacketsReceived\", dnMetrics) > 1L);\n+      assertTrue(\"More than 1 slow packet to mirror\",\n+          getLongCounter(\"TotalPacketsSlowWriteToMirror\", dnMetrics) > 1L);\n+      assertCounter(\"TotalPacketsSlowWriteToDisk\", 1L, dnMetrics);\n+      assertCounter(\"TotalPacketsSlowWriteOsCache\", 0L, dnMetrics);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3be5d26cec4c93b5b47ef795ee320b38e5a9356d"}, "originalPosition": 52}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7fa02862558ce5ffb3e4b5f20e1f7506730339e6", "author": {"user": {"login": "fengnanli", "name": "lfengnan"}}, "url": "https://github.com/apache/hadoop/commit/7fa02862558ce5ffb3e4b5f20e1f7506730339e6", "committedDate": "2021-01-07T07:32:11Z", "message": "Fix test"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3246, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}