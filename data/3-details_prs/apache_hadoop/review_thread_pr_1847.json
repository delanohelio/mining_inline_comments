{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzc1MDE4MzY4", "number": 1847, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QyMDoyNToxMFrODfr-Fg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNVQxNTozODoxOVrODgKh1A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0NTUyODU0OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsUnbuffer.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QyMDoyNToxMFrOFpiYhQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQyMjo1MzoxOFrOFrVbXw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTA5OTI2OQ==", "bodyText": "or you could just classcast the wrapped stream and let the JVM do the reporting...", "url": "https://github.com/apache/hadoop/pull/1847#discussion_r379099269", "createdAt": "2020-02-13T20:25:10Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsUnbuffer.java", "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.io.IOException;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.contract.ContractTestUtils;\n+\n+/**\n+ * Integration test for calling\n+ * {@link org.apache.hadoop.fs.CanUnbuffer#unbuffer} on {@link AbfsInputStream}.\n+ * Validates that the underlying stream's buffer is null.\n+ */\n+public class ITestAbfsUnbuffer extends AbstractAbfsIntegrationTest {\n+\n+  private Path dest;\n+\n+  public ITestAbfsUnbuffer() throws Exception {\n+  }\n+\n+  @Override\n+  public void setup() throws Exception {\n+    super.setup();\n+    dest = path(\"ITestAbfsUnbuffer\");\n+\n+    byte[] data = ContractTestUtils.dataset(16, 'a', 26);\n+    ContractTestUtils.writeDataset(getFileSystem(), dest, data, data.length,\n+            16, true);\n+  }\n+\n+  @Test\n+  public void testUnbuffer() throws IOException {\n+    // Open file, read half the data, and then call unbuffer\n+    try (FSDataInputStream inputStream = getFileSystem().open(dest)) {\n+      assertTrue(\"unexpected stream type \" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c5e7dafda1b53da2230814a3ab1b6b807a3d99c6"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk4NDE1OQ==", "bodyText": "Yeah, we could do that. I think the exception message is slightly friendlier this way though.", "url": "https://github.com/apache/hadoop/pull/1847#discussion_r380984159", "createdAt": "2020-02-18T22:53:18Z", "author": {"login": "sahilTakiar"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsUnbuffer.java", "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.io.IOException;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.contract.ContractTestUtils;\n+\n+/**\n+ * Integration test for calling\n+ * {@link org.apache.hadoop.fs.CanUnbuffer#unbuffer} on {@link AbfsInputStream}.\n+ * Validates that the underlying stream's buffer is null.\n+ */\n+public class ITestAbfsUnbuffer extends AbstractAbfsIntegrationTest {\n+\n+  private Path dest;\n+\n+  public ITestAbfsUnbuffer() throws Exception {\n+  }\n+\n+  @Override\n+  public void setup() throws Exception {\n+    super.setup();\n+    dest = path(\"ITestAbfsUnbuffer\");\n+\n+    byte[] data = ContractTestUtils.dataset(16, 'a', 26);\n+    ContractTestUtils.writeDataset(getFileSystem(), dest, data, data.length,\n+            16, true);\n+  }\n+\n+  @Test\n+  public void testUnbuffer() throws IOException {\n+    // Open file, read half the data, and then call unbuffer\n+    try (FSDataInputStream inputStream = getFileSystem().open(dest)) {\n+      assertTrue(\"unexpected stream type \" +", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTA5OTI2OQ=="}, "originalCommit": {"oid": "c5e7dafda1b53da2230814a3ab1b6b807a3d99c6"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM1MDUzNDE2OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsUnbuffer.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNVQxNTozNjowNlrOFqPftQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQyMjo1MzoyN1rOFrVbpA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTgzODM4OQ==", "bodyText": "ABFS", "url": "https://github.com/apache/hadoop/pull/1847#discussion_r379838389", "createdAt": "2020-02-15T15:36:06Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsUnbuffer.java", "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.io.IOException;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.contract.ContractTestUtils;\n+\n+/**\n+ * Integration test for calling\n+ * {@link org.apache.hadoop.fs.CanUnbuffer#unbuffer} on {@link AbfsInputStream}.\n+ * Validates that the underlying stream's buffer is null.\n+ */\n+public class ITestAbfsUnbuffer extends AbstractAbfsIntegrationTest {\n+\n+  private Path dest;\n+\n+  public ITestAbfsUnbuffer() throws Exception {\n+  }\n+\n+  @Override\n+  public void setup() throws Exception {\n+    super.setup();\n+    dest = path(\"ITestAbfsUnbuffer\");\n+\n+    byte[] data = ContractTestUtils.dataset(16, 'a', 26);\n+    ContractTestUtils.writeDataset(getFileSystem(), dest, data, data.length,\n+            16, true);\n+  }\n+\n+  @Test\n+  public void testUnbuffer() throws IOException {\n+    // Open file, read half the data, and then call unbuffer\n+    try (FSDataInputStream inputStream = getFileSystem().open(dest)) {\n+      assertTrue(\"unexpected stream type \" +\n+              inputStream.getWrappedStream().getClass().getSimpleName(),\n+              inputStream.getWrappedStream() instanceof AbfsInputStream);\n+      readAndAssertBytesRead(inputStream, 8);\n+      assertFalse(\"AbfsInputStream buffer should not be null\",\n+              isBufferNull(inputStream));\n+      inputStream.unbuffer();\n+\n+      // Check the the underlying buffer is null\n+      assertTrue(\"AbfsInputStream buffer should be null\",\n+              isBufferNull(inputStream));\n+    }\n+  }\n+\n+  private boolean isBufferNull(FSDataInputStream inputStream) {\n+    return ((AbfsInputStream) inputStream.getWrappedStream()).buffer == null;\n+  }\n+\n+  /**\n+   * Read the specified number of bytes from the given\n+   * {@link FSDataInputStream} and assert that\n+   * {@link FSDataInputStream#read(byte[])} read the specified number of bytes.\n+   */\n+  private static void readAndAssertBytesRead(FSDataInputStream inputStream,\n+                                             int bytesToRead) throws IOException {\n+    assertEquals(\"S3AInputStream#read did not read the correct number of \" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7a7ac8c5691bf3948e74222ad0e38126125d18a6"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk4NDIyOA==", "bodyText": "done", "url": "https://github.com/apache/hadoop/pull/1847#discussion_r380984228", "createdAt": "2020-02-18T22:53:27Z", "author": {"login": "sahilTakiar"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsUnbuffer.java", "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.io.IOException;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.contract.ContractTestUtils;\n+\n+/**\n+ * Integration test for calling\n+ * {@link org.apache.hadoop.fs.CanUnbuffer#unbuffer} on {@link AbfsInputStream}.\n+ * Validates that the underlying stream's buffer is null.\n+ */\n+public class ITestAbfsUnbuffer extends AbstractAbfsIntegrationTest {\n+\n+  private Path dest;\n+\n+  public ITestAbfsUnbuffer() throws Exception {\n+  }\n+\n+  @Override\n+  public void setup() throws Exception {\n+    super.setup();\n+    dest = path(\"ITestAbfsUnbuffer\");\n+\n+    byte[] data = ContractTestUtils.dataset(16, 'a', 26);\n+    ContractTestUtils.writeDataset(getFileSystem(), dest, data, data.length,\n+            16, true);\n+  }\n+\n+  @Test\n+  public void testUnbuffer() throws IOException {\n+    // Open file, read half the data, and then call unbuffer\n+    try (FSDataInputStream inputStream = getFileSystem().open(dest)) {\n+      assertTrue(\"unexpected stream type \" +\n+              inputStream.getWrappedStream().getClass().getSimpleName(),\n+              inputStream.getWrappedStream() instanceof AbfsInputStream);\n+      readAndAssertBytesRead(inputStream, 8);\n+      assertFalse(\"AbfsInputStream buffer should not be null\",\n+              isBufferNull(inputStream));\n+      inputStream.unbuffer();\n+\n+      // Check the the underlying buffer is null\n+      assertTrue(\"AbfsInputStream buffer should be null\",\n+              isBufferNull(inputStream));\n+    }\n+  }\n+\n+  private boolean isBufferNull(FSDataInputStream inputStream) {\n+    return ((AbfsInputStream) inputStream.getWrappedStream()).buffer == null;\n+  }\n+\n+  /**\n+   * Read the specified number of bytes from the given\n+   * {@link FSDataInputStream} and assert that\n+   * {@link FSDataInputStream#read(byte[])} read the specified number of bytes.\n+   */\n+  private static void readAndAssertBytesRead(FSDataInputStream inputStream,\n+                                             int bytesToRead) throws IOException {\n+    assertEquals(\"S3AInputStream#read did not read the correct number of \" +", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTgzODM4OQ=="}, "originalCommit": {"oid": "7a7ac8c5691bf3948e74222ad0e38126125d18a6"}, "originalPosition": 81}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM1MDUzNDcwOnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNVQxNTozNzowMVrOFqPf-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQyMjo1MzozMVrOFrVbyA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTgzODQ1OA==", "bodyText": "add a getter", "url": "https://github.com/apache/hadoop/pull/1847#discussion_r379838458", "createdAt": "2020-02-15T15:37:01Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -45,7 +52,8 @@\n   private final boolean tolerateOobAppends; // whether tolerate Oob Appends\n   private final boolean readAheadEnabled; // whether enable readAhead;\n \n-  private byte[] buffer = null;            // will be initialized on first use\n+  @VisibleForTesting\n+  byte[] buffer = null;            // will be initialized on first use", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7a7ac8c5691bf3948e74222ad0e38126125d18a6"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk4NDI2NA==", "bodyText": "done", "url": "https://github.com/apache/hadoop/pull/1847#discussion_r380984264", "createdAt": "2020-02-18T22:53:31Z", "author": {"login": "sahilTakiar"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -45,7 +52,8 @@\n   private final boolean tolerateOobAppends; // whether tolerate Oob Appends\n   private final boolean readAheadEnabled; // whether enable readAhead;\n \n-  private byte[] buffer = null;            // will be initialized on first use\n+  @VisibleForTesting\n+  byte[] buffer = null;            // will be initialized on first use", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTgzODQ1OA=="}, "originalCommit": {"oid": "7a7ac8c5691bf3948e74222ad0e38126125d18a6"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM1MDUzNTI0OnYy", "diffSide": "RIGHT", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNVQxNTozODoxOVrOFqPgNw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQyMjo1MzozN1rOFrVb8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTgzODUxOQ==", "bodyText": "should be synchronized", "url": "https://github.com/apache/hadoop/pull/1847#discussion_r379838519", "createdAt": "2020-02-15T15:38:19Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -390,4 +398,19 @@ public synchronized void reset() throws IOException {\n   public boolean markSupported() {\n     return false;\n   }\n+\n+  @Override\n+  public void unbuffer() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7a7ac8c5691bf3948e74222ad0e38126125d18a6"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk4NDMwNA==", "bodyText": "done", "url": "https://github.com/apache/hadoop/pull/1847#discussion_r380984304", "createdAt": "2020-02-18T22:53:37Z", "author": {"login": "sahilTakiar"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -390,4 +398,19 @@ public synchronized void reset() throws IOException {\n   public boolean markSupported() {\n     return false;\n   }\n+\n+  @Override\n+  public void unbuffer() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTgzODUxOQ=="}, "originalCommit": {"oid": "7a7ac8c5691bf3948e74222ad0e38126125d18a6"}, "originalPosition": 43}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3705, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}