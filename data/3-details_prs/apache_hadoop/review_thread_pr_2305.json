{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDg3MDgzNjg3", "number": 2305, "reviewThreads": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQwMzoyMDowMVrOEj3u8g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQwMzoyMDowMVrOEj3u8g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA2MDQ4NzU0OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestViewDistributedFileSystemWithMountLinks.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQwMzoyMDowMVrOHSekcg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQyMjowNTo1NFrOHTF2Hw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTEzNzI2Ng==", "bodyText": "I tried a couple of cases of rename. Can you give a check once.\n  public void testRenameOnInternalDirWithFallback() throws Exception {\n    Configuration conf = getConf();\n    URI defaultFSURI =\n        URI.create(conf.get(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY));\n    final Path hdfsTargetPath1 = new Path(defaultFSURI + \"/HDFSUser\");\n    final Path hdfsTargetPath2 = new Path(defaultFSURI + \"/dstNewHDFSUser\"\n        + \"/next\");\n    ViewFsTestSetup.addMountLinksToConf(defaultFSURI.getAuthority(),\n        new String[] {\"/HDFSUser\", \"/NewHDFSUser/next/next1\"},\n        new String[] {hdfsTargetPath1.toUri().toString(),\n            hdfsTargetPath2.toUri().toString()}, conf);\n    //Making sure parent dir structure as mount points available in fallback.\n    try (DistributedFileSystem dfs = new DistributedFileSystem()) {\n      dfs.initialize(defaultFSURI, conf);\n      dfs.mkdirs(hdfsTargetPath1);\n      dfs.mkdirs(hdfsTargetPath2);\n    }\n\n    try (FileSystem fs = FileSystem.get(conf)) {\n      // Case : 1\n      Path src = new Path(\"/newFileOnRoot\");\n      Path dst = new Path(\"/NewHDFSUser/next\");\n      fs.create(src).close();\n      verifyRename(fs, src, dst); // Fails. Shouldn't it move to\n      // /NewHDFSUser/next/newFileOnRoot ?\n\n       src = new Path(\"/newFileOnRoot\");\n       dst = new Path(\"/NewHDFSUser/next/file\");\n      verifyRename(fs, src, dst); // Fails. Guess since the parent structure\n      // isn't there at fallback?```\n\nI couldn't check more, but\nCASE 1: if the destination is a directory, shouldn't it move src inside it?\nCASE:2 Seems due to parent structure isn't there in fallback?\n\nI didn't try with ViewFs, maybe something similar there as well", "url": "https://github.com/apache/hadoop/pull/2305#discussion_r489137266", "createdAt": "2020-09-16T03:20:01Z", "author": {"login": "ayushtkn"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestViewDistributedFileSystemWithMountLinks.java", "diffHunk": "@@ -61,4 +64,55 @@ public void testCreateOnRoot() throws Exception {\n   public void testMountLinkWithNonExistentLink() throws Exception {\n     testMountLinkWithNonExistentLink(false);\n   }\n+\n+  @Test\n+  public void testRenameOnInternalDirWithFallback() throws Exception {\n+    Configuration conf = getConf();\n+    URI defaultFSURI =\n+        URI.create(conf.get(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY));\n+    final Path hdfsTargetPath1 = new Path(defaultFSURI + \"/HDFSUser\");\n+    final Path hdfsTargetPath2 = new Path(defaultFSURI + \"/NewHDFSUser/next\");\n+    ViewFsTestSetup.addMountLinksToConf(defaultFSURI.getAuthority(),\n+        new String[] {\"/HDFSUser\", \"/NewHDFSUser/next\"},\n+        new String[] {hdfsTargetPath1.toUri().toString(),\n+            hdfsTargetPath2.toUri().toString()}, conf);\n+    //Making sure parent dir structure as mount points available in fallback.\n+    try (DistributedFileSystem dfs = new DistributedFileSystem()) {\n+      dfs.initialize(defaultFSURI, conf);\n+      dfs.mkdirs(hdfsTargetPath1);\n+      dfs.mkdirs(hdfsTargetPath2);\n+    }\n+\n+    try (FileSystem fs = FileSystem.get(conf)) {\n+      Path src = new Path(\"/newFileOnRoot\");\n+      Path dst = new Path(\"/newFileOnRoot1\");\n+      fs.create(src).close();\n+      verifyRename(fs, src, dst);\n+\n+      src = new Path(\"/newFileOnRoot1\");\n+      dst = new Path(\"/NewHDFSUser/newFileOnRoot\");\n+      fs.mkdirs(dst.getParent());\n+      verifyRename(fs, src, dst);\n+\n+      src = new Path(\"/NewHDFSUser/newFileOnRoot\");\n+      dst = new Path(\"/NewHDFSUser/newFileOnRoot1\");\n+      verifyRename(fs, src, dst);\n+\n+      src = new Path(\"/NewHDFSUser/newFileOnRoot1\");\n+      dst = new Path(\"/newFileOnRoot\");\n+      verifyRename(fs, src, dst);\n+\n+      src = new Path(\"/HDFSUser/newFileOnRoot1\");\n+      dst = new Path(\"/HDFSUser/newFileOnRoot\");\n+      fs.create(src).close();\n+      verifyRename(fs, src, dst);\n+    }\n+  }\n+\n+  private void verifyRename(FileSystem fs, Path src, Path dst)\n+      throws IOException {\n+    fs.rename(src, dst);\n+    Assert.assertFalse(fs.exists(src));\n+    Assert.assertTrue(fs.exists(dst));\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7cab8b9ecabf7e508a1c9ef2f489b727e87478d"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTI2MjczMQ==", "bodyText": "@ayushtkn , Thanks a lot for the review!\nGood findings.\nCase1: Actually there are two rename behaviors, I think rename2 does expect both(src and dst) to be same type either dir or file. However I modified it to allow internalDir as dst with fallback, rename2 can fail later as usual. But the current rename(src,dst) also works as you verified. Please check the modified code whether it can satisfy you expected conditions.\nCase2: Yes, this is know fact that, if fallback does not have structure, rename does not create any dirs and it will fail.\nWhen user does not have any fallback structure, I think they should not expect all of this this should work very well as they might have created arbitrary mount point, but not created mount points with respective to fallback/default cluster structure.\nPlease note ViewFs rename behave like rename2 so, test assertions are slightly different than rename api from ViewFileSystem.", "url": "https://github.com/apache/hadoop/pull/2305#discussion_r489262731", "createdAt": "2020-09-16T08:36:26Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestViewDistributedFileSystemWithMountLinks.java", "diffHunk": "@@ -61,4 +64,55 @@ public void testCreateOnRoot() throws Exception {\n   public void testMountLinkWithNonExistentLink() throws Exception {\n     testMountLinkWithNonExistentLink(false);\n   }\n+\n+  @Test\n+  public void testRenameOnInternalDirWithFallback() throws Exception {\n+    Configuration conf = getConf();\n+    URI defaultFSURI =\n+        URI.create(conf.get(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY));\n+    final Path hdfsTargetPath1 = new Path(defaultFSURI + \"/HDFSUser\");\n+    final Path hdfsTargetPath2 = new Path(defaultFSURI + \"/NewHDFSUser/next\");\n+    ViewFsTestSetup.addMountLinksToConf(defaultFSURI.getAuthority(),\n+        new String[] {\"/HDFSUser\", \"/NewHDFSUser/next\"},\n+        new String[] {hdfsTargetPath1.toUri().toString(),\n+            hdfsTargetPath2.toUri().toString()}, conf);\n+    //Making sure parent dir structure as mount points available in fallback.\n+    try (DistributedFileSystem dfs = new DistributedFileSystem()) {\n+      dfs.initialize(defaultFSURI, conf);\n+      dfs.mkdirs(hdfsTargetPath1);\n+      dfs.mkdirs(hdfsTargetPath2);\n+    }\n+\n+    try (FileSystem fs = FileSystem.get(conf)) {\n+      Path src = new Path(\"/newFileOnRoot\");\n+      Path dst = new Path(\"/newFileOnRoot1\");\n+      fs.create(src).close();\n+      verifyRename(fs, src, dst);\n+\n+      src = new Path(\"/newFileOnRoot1\");\n+      dst = new Path(\"/NewHDFSUser/newFileOnRoot\");\n+      fs.mkdirs(dst.getParent());\n+      verifyRename(fs, src, dst);\n+\n+      src = new Path(\"/NewHDFSUser/newFileOnRoot\");\n+      dst = new Path(\"/NewHDFSUser/newFileOnRoot1\");\n+      verifyRename(fs, src, dst);\n+\n+      src = new Path(\"/NewHDFSUser/newFileOnRoot1\");\n+      dst = new Path(\"/newFileOnRoot\");\n+      verifyRename(fs, src, dst);\n+\n+      src = new Path(\"/HDFSUser/newFileOnRoot1\");\n+      dst = new Path(\"/HDFSUser/newFileOnRoot\");\n+      fs.create(src).close();\n+      verifyRename(fs, src, dst);\n+    }\n+  }\n+\n+  private void verifyRename(FileSystem fs, Path src, Path dst)\n+      throws IOException {\n+    fs.rename(src, dst);\n+    Assert.assertFalse(fs.exists(src));\n+    Assert.assertTrue(fs.exists(dst));\n+  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTEzNzI2Ng=="}, "originalCommit": {"oid": "f7cab8b9ecabf7e508a1c9ef2f489b727e87478d"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTU4OTk0OQ==", "bodyText": "Thanx @umamaheswararao for the update.\nRegarding the Case 2: When the same directory structure isn't available in the fallback.\nIn ViewFs I think this was handled and createParent was explicitly made true always. It would be just for rename this compulsion would be there.\nConsidering a mount entry like -- /mount/sub1/sub2 --> /nsPath\nif someone calls rename with dst as /mount/sub1/renameFile will fail, but if he calls create /mount/sub1/createFile without createParent it would pass and this create call will create the internal directory structure as well. So, now again the user calls the same rename command, it would succeed. Same for mkdir with createParent as false\nThis would be little intermittent behavior for the end user, one API behaving differently.\nSecondly creating the same directory structure at fallback just for rename to work doesn't seems feasible, It would be too many empty directories, increasing the number of inodes at NN. IIRC something like this, to create empty directories for mount entries in case of RBF was discussed for some issue recently, and UBER folks had concerns with inode numbers going high due to empty directories.\nI think we should explicitly take care of this in rename as well, May be in non-atomic way only? Later we might find a better way, Maybe adding one more flag to rename2 and argument to rename for createParent in a follow up.", "url": "https://github.com/apache/hadoop/pull/2305#discussion_r489589949", "createdAt": "2020-09-16T17:03:58Z", "author": {"login": "ayushtkn"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestViewDistributedFileSystemWithMountLinks.java", "diffHunk": "@@ -61,4 +64,55 @@ public void testCreateOnRoot() throws Exception {\n   public void testMountLinkWithNonExistentLink() throws Exception {\n     testMountLinkWithNonExistentLink(false);\n   }\n+\n+  @Test\n+  public void testRenameOnInternalDirWithFallback() throws Exception {\n+    Configuration conf = getConf();\n+    URI defaultFSURI =\n+        URI.create(conf.get(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY));\n+    final Path hdfsTargetPath1 = new Path(defaultFSURI + \"/HDFSUser\");\n+    final Path hdfsTargetPath2 = new Path(defaultFSURI + \"/NewHDFSUser/next\");\n+    ViewFsTestSetup.addMountLinksToConf(defaultFSURI.getAuthority(),\n+        new String[] {\"/HDFSUser\", \"/NewHDFSUser/next\"},\n+        new String[] {hdfsTargetPath1.toUri().toString(),\n+            hdfsTargetPath2.toUri().toString()}, conf);\n+    //Making sure parent dir structure as mount points available in fallback.\n+    try (DistributedFileSystem dfs = new DistributedFileSystem()) {\n+      dfs.initialize(defaultFSURI, conf);\n+      dfs.mkdirs(hdfsTargetPath1);\n+      dfs.mkdirs(hdfsTargetPath2);\n+    }\n+\n+    try (FileSystem fs = FileSystem.get(conf)) {\n+      Path src = new Path(\"/newFileOnRoot\");\n+      Path dst = new Path(\"/newFileOnRoot1\");\n+      fs.create(src).close();\n+      verifyRename(fs, src, dst);\n+\n+      src = new Path(\"/newFileOnRoot1\");\n+      dst = new Path(\"/NewHDFSUser/newFileOnRoot\");\n+      fs.mkdirs(dst.getParent());\n+      verifyRename(fs, src, dst);\n+\n+      src = new Path(\"/NewHDFSUser/newFileOnRoot\");\n+      dst = new Path(\"/NewHDFSUser/newFileOnRoot1\");\n+      verifyRename(fs, src, dst);\n+\n+      src = new Path(\"/NewHDFSUser/newFileOnRoot1\");\n+      dst = new Path(\"/newFileOnRoot\");\n+      verifyRename(fs, src, dst);\n+\n+      src = new Path(\"/HDFSUser/newFileOnRoot1\");\n+      dst = new Path(\"/HDFSUser/newFileOnRoot\");\n+      fs.create(src).close();\n+      verifyRename(fs, src, dst);\n+    }\n+  }\n+\n+  private void verifyRename(FileSystem fs, Path src, Path dst)\n+      throws IOException {\n+    fs.rename(src, dst);\n+    Assert.assertFalse(fs.exists(src));\n+    Assert.assertTrue(fs.exists(dst));\n+  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTEzNzI2Ng=="}, "originalCommit": {"oid": "f7cab8b9ecabf7e508a1c9ef2f489b727e87478d"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTc4MDc2Nw==", "bodyText": "What's your proposal for rename here? Could you elaborate a bit?\nI am thinking this way: If API nature is to create parent dirs, it will create. If the API nature is not create parents, I am ok to fail and let the users know the fact that it's redirected to fallback and they don't have structure?\nIn your above example, you made additional call to createFile, so that next rename succeeded.\nI see in ViewFS.java create, mkdir we made createParent by default true. I am ok to take the user passed flag itself. That way we can be consistent on API perspective. Also if I am correct, this problem only exist with ViewFs.java. In ViewFileSystem.java as we will compliant to API as mkdirs by default create parent. This way, users expected things only we do. What do you? If someone really wants to createParents bydefault in ViewFs.java, that discussion can be taken into separate JIRA for create,mkdir behavior. To summarize: in ViewFs.java, create, mkdir creates the parent in fallback if they don't exist (irrespective of createParent flag. ) We did this because, for the users perspective, we provide transparency and exist of parent dir returns true. However that fact is that, children actually goes to fallback. When users create mount points with respective to fallback cluster, then only this will be true. Otherwise it can be any random mount point without any relation to fallback. When API does not say createParent, then we will keep that semantics to fallback as well. Whats your thoughts?", "url": "https://github.com/apache/hadoop/pull/2305#discussion_r489780767", "createdAt": "2020-09-16T22:05:54Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestViewDistributedFileSystemWithMountLinks.java", "diffHunk": "@@ -61,4 +64,55 @@ public void testCreateOnRoot() throws Exception {\n   public void testMountLinkWithNonExistentLink() throws Exception {\n     testMountLinkWithNonExistentLink(false);\n   }\n+\n+  @Test\n+  public void testRenameOnInternalDirWithFallback() throws Exception {\n+    Configuration conf = getConf();\n+    URI defaultFSURI =\n+        URI.create(conf.get(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY));\n+    final Path hdfsTargetPath1 = new Path(defaultFSURI + \"/HDFSUser\");\n+    final Path hdfsTargetPath2 = new Path(defaultFSURI + \"/NewHDFSUser/next\");\n+    ViewFsTestSetup.addMountLinksToConf(defaultFSURI.getAuthority(),\n+        new String[] {\"/HDFSUser\", \"/NewHDFSUser/next\"},\n+        new String[] {hdfsTargetPath1.toUri().toString(),\n+            hdfsTargetPath2.toUri().toString()}, conf);\n+    //Making sure parent dir structure as mount points available in fallback.\n+    try (DistributedFileSystem dfs = new DistributedFileSystem()) {\n+      dfs.initialize(defaultFSURI, conf);\n+      dfs.mkdirs(hdfsTargetPath1);\n+      dfs.mkdirs(hdfsTargetPath2);\n+    }\n+\n+    try (FileSystem fs = FileSystem.get(conf)) {\n+      Path src = new Path(\"/newFileOnRoot\");\n+      Path dst = new Path(\"/newFileOnRoot1\");\n+      fs.create(src).close();\n+      verifyRename(fs, src, dst);\n+\n+      src = new Path(\"/newFileOnRoot1\");\n+      dst = new Path(\"/NewHDFSUser/newFileOnRoot\");\n+      fs.mkdirs(dst.getParent());\n+      verifyRename(fs, src, dst);\n+\n+      src = new Path(\"/NewHDFSUser/newFileOnRoot\");\n+      dst = new Path(\"/NewHDFSUser/newFileOnRoot1\");\n+      verifyRename(fs, src, dst);\n+\n+      src = new Path(\"/NewHDFSUser/newFileOnRoot1\");\n+      dst = new Path(\"/newFileOnRoot\");\n+      verifyRename(fs, src, dst);\n+\n+      src = new Path(\"/HDFSUser/newFileOnRoot1\");\n+      dst = new Path(\"/HDFSUser/newFileOnRoot\");\n+      fs.create(src).close();\n+      verifyRename(fs, src, dst);\n+    }\n+  }\n+\n+  private void verifyRename(FileSystem fs, Path src, Path dst)\n+      throws IOException {\n+    fs.rename(src, dst);\n+    Assert.assertFalse(fs.exists(src));\n+    Assert.assertTrue(fs.exists(dst));\n+  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTEzNzI2Ng=="}, "originalCommit": {"oid": "f7cab8b9ecabf7e508a1c9ef2f489b727e87478d"}, "originalPosition": 76}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3293, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}