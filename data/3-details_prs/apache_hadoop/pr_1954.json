{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDAyMTk0NzI2", "number": 1954, "title": "HDFS-15217 Add more information to longest write/read lock held log", "bodyText": "", "createdAt": "2020-04-11T14:04:59Z", "url": "https://github.com/apache/hadoop/pull/1954", "merged": true, "mergeCommit": {"oid": "1824aee9da4056de0fb638906b2172e486bbebe7"}, "closed": true, "closedAt": "2020-04-18T20:52:08Z", "author": {"login": "brfrn169"}, "timelineItems": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcWqk4ngFqTM5MTc5MjAxNw==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcX9z-NgBqjMyMzcwODk3ODI=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkxNzkyMDE3", "url": "https://github.com/apache/hadoop/pull/1954#pullrequestreview-391792017", "createdAt": "2020-04-11T19:00:11Z", "commit": {"oid": "4197ce4f6d4456f2fe1fee6b4de240f773b0db90"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMVQxOTowMDoxMlrOGEPVlg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMVQxOTowODoyNVrOGEPYyQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA5ODc3NA==", "bodyText": "Can we just keep the old constructor with the old parameters that pass the null to the new one instead of changing everywhere?", "url": "https://github.com/apache/hadoop/pull/1954#discussion_r407098774", "createdAt": "2020-04-11T19:00:12Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystemLock.java", "diffHunk": "@@ -106,8 +107,8 @@ public Long initialValue() {\n    * lock was held since the last report.\n    */\n   private final AtomicReference<LockHeldInfo> longestReadLockHeldInfo =\n-      new AtomicReference<>(new LockHeldInfo(0, 0, null));\n-  private LockHeldInfo longestWriteLockHeldInfo = new LockHeldInfo(0, 0, null);\n+      new AtomicReference<>(new LockHeldInfo(0, 0, null, null, null));\n+  private LockHeldInfo longestWriteLockHeldInfo = new LockHeldInfo(0, 0, null, null, null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4197ce4f6d4456f2fe1fee6b4de240f773b0db90"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA5ODk0OQ==", "bodyText": "As we are touching this, can we make this into a regular:\nboolean done = false;\nwhile (!done) {\n...\n} else {\ndone = true;\n}\n}", "url": "https://github.com/apache/hadoop/pull/1954#discussion_r407098949", "createdAt": "2020-04-11T19:02:08Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystemLock.java", "diffHunk": "@@ -176,13 +181,23 @@ public void readUnlock(String opName) {\n     final long readLockIntervalMs =\n         TimeUnit.NANOSECONDS.toMillis(readLockIntervalNanos);\n     if (needReport && readLockIntervalMs >= this.readLockReportingThresholdMs) {\n-      LockHeldInfo localLockHeldInfo;\n+      String lockReportInfo = null;\n       do {\n-        localLockHeldInfo = longestReadLockHeldInfo.get();\n-      } while (localLockHeldInfo.getIntervalMs() - readLockIntervalMs < 0 &&\n-          !longestReadLockHeldInfo.compareAndSet(localLockHeldInfo,\n-              new LockHeldInfo(currentTimeMs, readLockIntervalMs,\n-                  StringUtils.getStackTrace(Thread.currentThread()))));\n+        LockHeldInfo localLockHeldInfo = longestReadLockHeldInfo.get();\n+        if (localLockHeldInfo.getIntervalMs() <= readLockIntervalMs) {\n+          if (lockReportInfo == null) {\n+            lockReportInfo = lockReportInfoSupplier != null ? \" (\" +\n+                lockReportInfoSupplier.get() + \")\" : \"\";\n+          }\n+          if (longestReadLockHeldInfo.compareAndSet(localLockHeldInfo,\n+            new LockHeldInfo(currentTimeMs, readLockIntervalMs,\n+              StringUtils.getStackTrace(Thread.currentThread()), opName, lockReportInfo))) {\n+            break;\n+          }\n+        } else {\n+          break;\n+        }\n+      } while (true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4197ce4f6d4456f2fe1fee6b4de240f773b0db90"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA5OTAwMQ==", "bodyText": "Can we extract some of this? At least the stack trace.", "url": "https://github.com/apache/hadoop/pull/1954#discussion_r407099001", "createdAt": "2020-04-11T19:02:46Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystemLock.java", "diffHunk": "@@ -176,13 +181,23 @@ public void readUnlock(String opName) {\n     final long readLockIntervalMs =\n         TimeUnit.NANOSECONDS.toMillis(readLockIntervalNanos);\n     if (needReport && readLockIntervalMs >= this.readLockReportingThresholdMs) {\n-      LockHeldInfo localLockHeldInfo;\n+      String lockReportInfo = null;\n       do {\n-        localLockHeldInfo = longestReadLockHeldInfo.get();\n-      } while (localLockHeldInfo.getIntervalMs() - readLockIntervalMs < 0 &&\n-          !longestReadLockHeldInfo.compareAndSet(localLockHeldInfo,\n-              new LockHeldInfo(currentTimeMs, readLockIntervalMs,\n-                  StringUtils.getStackTrace(Thread.currentThread()))));\n+        LockHeldInfo localLockHeldInfo = longestReadLockHeldInfo.get();\n+        if (localLockHeldInfo.getIntervalMs() <= readLockIntervalMs) {\n+          if (lockReportInfo == null) {\n+            lockReportInfo = lockReportInfoSupplier != null ? \" (\" +\n+                lockReportInfoSupplier.get() + \")\" : \"\";\n+          }\n+          if (longestReadLockHeldInfo.compareAndSet(localLockHeldInfo,\n+            new LockHeldInfo(currentTimeMs, readLockIntervalMs,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4197ce4f6d4456f2fe1fee6b4de240f773b0db90"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA5OTEwOA==", "bodyText": "Extract the trace at least.", "url": "https://github.com/apache/hadoop/pull/1954#discussion_r407099108", "createdAt": "2020-04-11T19:03:45Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystemLock.java", "diffHunk": "@@ -253,10 +294,12 @@ public void writeUnlock(String opName, boolean suppressWriteLockReport) {\n     LogAction logAction = LogThrottlingHelper.DO_NOT_LOG;\n     if (needReport &&\n         writeLockIntervalMs >= this.writeLockReportingThresholdMs) {\n-      if (longestWriteLockHeldInfo.getIntervalMs() < writeLockIntervalMs) {\n+      if (longestWriteLockHeldInfo.getIntervalMs() <= writeLockIntervalMs) {\n+        String lockReportInfo = lockReportInfoSupplier != null ? \" (\" +\n+            lockReportInfoSupplier.get() + \")\" : \"\";\n         longestWriteLockHeldInfo =\n             new LockHeldInfo(currentTimeMs, writeLockIntervalMs,\n-                StringUtils.getStackTrace(Thread.currentThread()));\n+                StringUtils.getStackTrace(Thread.currentThread()), opName, lockReportInfo);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4197ce4f6d4456f2fe1fee6b4de240f773b0db90"}, "originalPosition": 153}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA5OTU5Mw==", "bodyText": "I'm having a hard time finding who uses this Supplier. Where are we using this?", "url": "https://github.com/apache/hadoop/pull/1954#discussion_r407099593", "createdAt": "2020-04-11T19:08:25Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystemLock.java", "diffHunk": "@@ -159,10 +160,14 @@ public void readLockInterruptibly() throws InterruptedException {\n   }\n \n   public void readUnlock() {\n-    readUnlock(OP_NAME_OTHER);\n+    readUnlock(OP_NAME_OTHER, null);\n   }\n \n   public void readUnlock(String opName) {\n+    readUnlock(opName, null);\n+  }\n+\n+  public void readUnlock(String opName, Supplier<String> lockReportInfoSupplier) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4197ce4f6d4456f2fe1fee6b4de240f773b0db90"}, "originalPosition": 31}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkxODE5MTY1", "url": "https://github.com/apache/hadoop/pull/1954#pullrequestreview-391819165", "createdAt": "2020-04-12T03:48:27Z", "commit": {"oid": "4197ce4f6d4456f2fe1fee6b4de240f773b0db90"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMlQwMzo0ODoyN1rOGER8cA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMlQwNjoxNTo1OFrOGESnSg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzE0MTQ4OA==", "bodyText": "Maybe it's better to have default constructor (without any parameters) of LockHeldInfo for those initial instances? I will try to do that.", "url": "https://github.com/apache/hadoop/pull/1954#discussion_r407141488", "createdAt": "2020-04-12T03:48:27Z", "author": {"login": "brfrn169"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystemLock.java", "diffHunk": "@@ -106,8 +107,8 @@ public Long initialValue() {\n    * lock was held since the last report.\n    */\n   private final AtomicReference<LockHeldInfo> longestReadLockHeldInfo =\n-      new AtomicReference<>(new LockHeldInfo(0, 0, null));\n-  private LockHeldInfo longestWriteLockHeldInfo = new LockHeldInfo(0, 0, null);\n+      new AtomicReference<>(new LockHeldInfo(0, 0, null, null, null));\n+  private LockHeldInfo longestWriteLockHeldInfo = new LockHeldInfo(0, 0, null, null, null);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA5ODc3NA=="}, "originalCommit": {"oid": "4197ce4f6d4456f2fe1fee6b4de240f773b0db90"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzE1MjQ1OA==", "bodyText": "I use this Supplier in the following places:\nhttps://github.com/brfrn169/hadoop/blob/HDFS-15217/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystemLock.java#L189-L190\nhttps://github.com/brfrn169/hadoop/blob/HDFS-15217/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystemLock.java#L298-L299\nThe reason why I use Supplier is that we don't always print the lock report, only when the lock interval is more than the threshold (dfs.namenode.write-lock-reporting-threshold-ms or dfs.namenode.read-lock-reporting-threshold-ms). We can do lazy building additional information with the lockReportInfoSupplier.", "url": "https://github.com/apache/hadoop/pull/1954#discussion_r407152458", "createdAt": "2020-04-12T06:15:58Z", "author": {"login": "brfrn169"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystemLock.java", "diffHunk": "@@ -159,10 +160,14 @@ public void readLockInterruptibly() throws InterruptedException {\n   }\n \n   public void readUnlock() {\n-    readUnlock(OP_NAME_OTHER);\n+    readUnlock(OP_NAME_OTHER, null);\n   }\n \n   public void readUnlock(String opName) {\n+    readUnlock(opName, null);\n+  }\n+\n+  public void readUnlock(String opName, Supplier<String> lockReportInfoSupplier) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA5OTU5Mw=="}, "originalCommit": {"oid": "4197ce4f6d4456f2fe1fee6b4de240f773b0db90"}, "originalPosition": 31}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4197ce4f6d4456f2fe1fee6b4de240f773b0db90", "author": {"user": {"login": "brfrn169", "name": "Toshihiro Suzuki"}}, "url": "https://github.com/apache/hadoop/commit/4197ce4f6d4456f2fe1fee6b4de240f773b0db90", "committedDate": "2020-04-11T13:59:53Z", "message": "HDFS-15217 Add more information to longest write/read lock held log"}, "afterCommit": {"oid": "07e50509932eb5565f6ff4e937116b932ac05998", "author": {"user": {"login": "brfrn169", "name": "Toshihiro Suzuki"}}, "url": "https://github.com/apache/hadoop/commit/07e50509932eb5565f6ff4e937116b932ac05998", "committedDate": "2020-04-12T07:01:54Z", "message": "HDFS-15217 Add more information to longest write/read lock held log"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "07e50509932eb5565f6ff4e937116b932ac05998", "author": {"user": {"login": "brfrn169", "name": "Toshihiro Suzuki"}}, "url": "https://github.com/apache/hadoop/commit/07e50509932eb5565f6ff4e937116b932ac05998", "committedDate": "2020-04-12T07:01:54Z", "message": "HDFS-15217 Add more information to longest write/read lock held log"}, "afterCommit": {"oid": "ef6802b3b05ba4f1ba92c89c2ddb6a6e031dd80c", "author": {"user": {"login": "brfrn169", "name": "Toshihiro Suzuki"}}, "url": "https://github.com/apache/hadoop/commit/ef6802b3b05ba4f1ba92c89c2ddb6a6e031dd80c", "committedDate": "2020-04-12T13:13:06Z", "message": "HDFS-15217 Add more information to longest write/read lock held log"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkxODgzOTg3", "url": "https://github.com/apache/hadoop/pull/1954#pullrequestreview-391883987", "createdAt": "2020-04-12T18:31:10Z", "commit": {"oid": "ef6802b3b05ba4f1ba92c89c2ddb6a6e031dd80c"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMlQxODozMToxMFrOGEXs5Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMlQxODozNjoyNlrOGEXvUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzIzNTgxMw==", "bodyText": "Add a comment with the example of the actual line.", "url": "https://github.com/apache/hadoop/pull/1954#discussion_r407235813", "createdAt": "2020-04-12T18:31:10Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSNamesystemLockReport.java", "diffHunk": "@@ -0,0 +1,185 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdfs.server.namenode;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.hdfs.DFSTestUtil;\n+import org.apache.hadoop.hdfs.HdfsConfiguration;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.slf4j.LoggerFactory;\n+\n+import java.security.PrivilegedExceptionAction;\n+\n+import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_LOCK_SUPPRESS_WARNING_INTERVAL_KEY;\n+import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_NAMENODE_READ_LOCK_REPORTING_THRESHOLD_MS_KEY;\n+import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_NAMENODE_WRITE_LOCK_REPORTING_THRESHOLD_MS_KEY;\n+import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_PERMISSIONS_SUPERUSERGROUP_KEY;\n+import static org.junit.Assert.assertTrue;\n+\n+public class TestFSNamesystemLockReport {\n+\n+  @FunctionalInterface\n+  private interface SupplierWithException<T> {\n+    T get() throws Exception;\n+  }\n+\n+  @FunctionalInterface\n+  private interface Procedure {\n+    void invoke() throws Exception;\n+  }\n+\n+  private Configuration conf;\n+  private MiniDFSCluster cluster;\n+  private FileSystem fs;\n+  private UserGroupInformation userGroupInfo;\n+  private GenericTestUtils.LogCapturer logs;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new HdfsConfiguration();\n+    conf.set(DFS_PERMISSIONS_SUPERUSERGROUP_KEY, \"hadoop\");\n+\n+    // Make the lock report always shown\n+    conf.setLong(DFS_NAMENODE_READ_LOCK_REPORTING_THRESHOLD_MS_KEY, 0);\n+    conf.setLong(DFS_NAMENODE_WRITE_LOCK_REPORTING_THRESHOLD_MS_KEY, 0);\n+    conf.setLong(DFS_LOCK_SUPPRESS_WARNING_INTERVAL_KEY, 0);\n+\n+    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(4).build();\n+    fs = cluster.getFileSystem();\n+\n+    userGroupInfo = UserGroupInformation.createUserForTesting(\"bob\",\n+        new String[] {\"hadoop\"});\n+\n+    logs = GenericTestUtils.LogCapturer.captureLogs(FSNamesystem.LOG);\n+    GenericTestUtils\n+        .setLogLevel(LoggerFactory.getLogger(FSNamesystem.class.getName()),\n+        org.slf4j.event.Level.INFO);\n+  }\n+\n+  @After\n+  public void cleanUp() throws Exception {\n+    if (fs != null) {\n+      fs.close();\n+      fs = null;\n+    }\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n+  }\n+\n+  @Test\n+  public void test() throws Exception {\n+    FileSystem userfs = DFSTestUtil.getFileSystemAs(userGroupInfo, conf);\n+\n+    FSDataOutputStream os = testLockReport(() ->", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef6802b3b05ba4f1ba92c89c2ddb6a6e031dd80c"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzIzNjQzMw==", "bodyText": "This is the same in the previous method. Extract?", "url": "https://github.com/apache/hadoop/pull/1954#discussion_r407236433", "createdAt": "2020-04-12T18:36:26Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSNamesystemLockReport.java", "diffHunk": "@@ -0,0 +1,185 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdfs.server.namenode;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.hdfs.DFSTestUtil;\n+import org.apache.hadoop.hdfs.HdfsConfiguration;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.slf4j.LoggerFactory;\n+\n+import java.security.PrivilegedExceptionAction;\n+\n+import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_LOCK_SUPPRESS_WARNING_INTERVAL_KEY;\n+import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_NAMENODE_READ_LOCK_REPORTING_THRESHOLD_MS_KEY;\n+import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_NAMENODE_WRITE_LOCK_REPORTING_THRESHOLD_MS_KEY;\n+import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_PERMISSIONS_SUPERUSERGROUP_KEY;\n+import static org.junit.Assert.assertTrue;\n+\n+public class TestFSNamesystemLockReport {\n+\n+  @FunctionalInterface\n+  private interface SupplierWithException<T> {\n+    T get() throws Exception;\n+  }\n+\n+  @FunctionalInterface\n+  private interface Procedure {\n+    void invoke() throws Exception;\n+  }\n+\n+  private Configuration conf;\n+  private MiniDFSCluster cluster;\n+  private FileSystem fs;\n+  private UserGroupInformation userGroupInfo;\n+  private GenericTestUtils.LogCapturer logs;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new HdfsConfiguration();\n+    conf.set(DFS_PERMISSIONS_SUPERUSERGROUP_KEY, \"hadoop\");\n+\n+    // Make the lock report always shown\n+    conf.setLong(DFS_NAMENODE_READ_LOCK_REPORTING_THRESHOLD_MS_KEY, 0);\n+    conf.setLong(DFS_NAMENODE_WRITE_LOCK_REPORTING_THRESHOLD_MS_KEY, 0);\n+    conf.setLong(DFS_LOCK_SUPPRESS_WARNING_INTERVAL_KEY, 0);\n+\n+    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(4).build();\n+    fs = cluster.getFileSystem();\n+\n+    userGroupInfo = UserGroupInformation.createUserForTesting(\"bob\",\n+        new String[] {\"hadoop\"});\n+\n+    logs = GenericTestUtils.LogCapturer.captureLogs(FSNamesystem.LOG);\n+    GenericTestUtils\n+        .setLogLevel(LoggerFactory.getLogger(FSNamesystem.class.getName()),\n+        org.slf4j.event.Level.INFO);\n+  }\n+\n+  @After\n+  public void cleanUp() throws Exception {\n+    if (fs != null) {\n+      fs.close();\n+      fs = null;\n+    }\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n+  }\n+\n+  @Test\n+  public void test() throws Exception {\n+    FileSystem userfs = DFSTestUtil.getFileSystemAs(userGroupInfo, conf);\n+\n+    FSDataOutputStream os = testLockReport(() ->\n+        userfs.create(new Path(\"/file\")),\n+        \".* by create \\\\(ugi=bob \\\\(auth:SIMPLE\\\\),\" +\n+        \"ip=/\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3},src=/file,dst=null,\" +\n+        \"perm=bob:hadoop:rw-r--r--\\\\) .*\");\n+    os.close();\n+\n+    FSDataInputStream is = testLockReport(() -> userfs.open(new Path(\"/file\")),\n+        \".* by open \\\\(ugi=bob \\\\(auth:SIMPLE\\\\),\" +\n+        \"ip=/\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3},src=/file,dst=null,\" +\n+        \"perm=null\\\\) .*\");\n+    is.close();\n+\n+    testLockReport(() ->\n+        userfs.setPermission(new Path(\"/file\"), new FsPermission(644)),\n+        \".* by setPermission \\\\(ugi=bob \\\\(auth:SIMPLE\\\\),\" +\n+        \"ip=/\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3},src=/file,dst=null,\" +\n+        \"perm=bob:hadoop:-w----r-T\\\\) .*\");\n+\n+    testLockReport(() -> userfs.setOwner(new Path(\"/file\"), \"alice\", \"group1\"),\n+        \".* by setOwner \\\\(ugi=bob \\\\(auth:SIMPLE\\\\),\" +\n+        \"ip=/\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3},src=/file,dst=null,\" +\n+        \"perm=alice:group1:-w----r-T\\\\) .*\");\n+\n+    testLockReport(() -> userfs.listStatus(new Path(\"/\")),\n+        \".* by listStatus \\\\(ugi=bob \\\\(auth:SIMPLE\\\\),\" +\n+        \"ip=/\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3},src=/,dst=null,\" +\n+        \"perm=null\\\\) .*\");\n+\n+    testLockReport(() -> userfs.getFileStatus(new Path(\"/file\")),\n+        \".* by getfileinfo \\\\(ugi=bob \\\\(auth:SIMPLE\\\\),\" +\n+        \"ip=/\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3},src=/file,dst=null,\" +\n+        \"perm=null\\\\) .*\");\n+\n+    testLockReport(() -> userfs.mkdirs(new Path(\"/dir\")),\n+        \".* by mkdirs \\\\(ugi=bob \\\\(auth:SIMPLE\\\\),\" +\n+        \"ip=/\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3},src=/dir,dst=null,\" +\n+        \"perm=bob:hadoop:rwxr-xr-x\\\\) .*\");\n+\n+    testLockReport(() -> userfs.rename(new Path(\"/file\"), new Path(\"/file2\")),\n+        \".* by rename \\\\(ugi=bob \\\\(auth:SIMPLE\\\\),\" +\n+        \"ip=/\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3},src=/file,dst=/file2,\" +\n+        \"perm=alice:group1:-w----r-T\\\\) .*\");\n+\n+    testLockReport(() -> userfs.delete(new Path(\"/file2\"), false),\n+        \".* by delete \\\\(ugi=bob \\\\(auth:SIMPLE\\\\),\" +\n+        \"ip=/\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3},src=/file2,dst=null,\" +\n+        \"perm=null\\\\) .*\");\n+  }\n+\n+  private void testLockReport(Procedure procedure,\n+      String expectedLockReportRegex) throws Exception {\n+    logs.clearOutput();\n+    userGroupInfo.doAs((PrivilegedExceptionAction<Void>) () -> {\n+      procedure.invoke();\n+      return null;\n+    });\n+\n+    boolean matches = false;\n+    for (String line : logs.getOutput().split(System.lineSeparator())) {\n+      if (line.matches(expectedLockReportRegex)) {\n+        matches = true;\n+        break;\n+      }\n+    }\n+    assertTrue(matches);\n+  }\n+\n+  private <T> T testLockReport(SupplierWithException<T> supplier,\n+      String expectedLockReportRegex) throws Exception {\n+    logs.clearOutput();\n+    T ret = userGroupInfo.doAs((PrivilegedExceptionAction<T>) supplier::get);\n+\n+    boolean matches = false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef6802b3b05ba4f1ba92c89c2ddb6a6e031dd80c"}, "originalPosition": 174}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ef6802b3b05ba4f1ba92c89c2ddb6a6e031dd80c", "author": {"user": {"login": "brfrn169", "name": "Toshihiro Suzuki"}}, "url": "https://github.com/apache/hadoop/commit/ef6802b3b05ba4f1ba92c89c2ddb6a6e031dd80c", "committedDate": "2020-04-12T13:13:06Z", "message": "HDFS-15217 Add more information to longest write/read lock held log"}, "afterCommit": {"oid": "855db207d0bd75142a44433016f5a26962cf6d26", "author": {"user": {"login": "brfrn169", "name": "Toshihiro Suzuki"}}, "url": "https://github.com/apache/hadoop/commit/855db207d0bd75142a44433016f5a26962cf6d26", "committedDate": "2020-04-13T03:01:18Z", "message": "HDFS-15217 Add more information to longest write/read lock held log"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkyMjk5ODc4", "url": "https://github.com/apache/hadoop/pull/1954#pullrequestreview-392299878", "createdAt": "2020-04-13T17:31:51Z", "commit": {"oid": "855db207d0bd75142a44433016f5a26962cf6d26"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "855db207d0bd75142a44433016f5a26962cf6d26", "author": {"user": {"login": "brfrn169", "name": "Toshihiro Suzuki"}}, "url": "https://github.com/apache/hadoop/commit/855db207d0bd75142a44433016f5a26962cf6d26", "committedDate": "2020-04-13T03:01:18Z", "message": "HDFS-15217 Add more information to longest write/read lock held log"}, "afterCommit": {"oid": "16ad10709dc319b740371af2ecbb22c836d4b45c", "author": {"user": {"login": "brfrn169", "name": "Toshihiro Suzuki"}}, "url": "https://github.com/apache/hadoop/commit/16ad10709dc319b740371af2ecbb22c836d4b45c", "committedDate": "2020-04-14T02:27:37Z", "message": "HDFS-15217 Add more information to longest write/read lock held log"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk0MDEwMzI4", "url": "https://github.com/apache/hadoop/pull/1954#pullrequestreview-394010328", "createdAt": "2020-04-15T18:02:56Z", "commit": {"oid": "16ad10709dc319b740371af2ecbb22c836d4b45c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQxODowMjo1NlrOGGFX-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQxODowMjo1NlrOGGFX-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTAzMjY5OQ==", "bodyText": "I think you need to rebase, right?", "url": "https://github.com/apache/hadoop/pull/1954#discussion_r409032699", "createdAt": "2020-04-15T18:02:56Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java", "diffHunk": "@@ -3204,11 +3235,12 @@ void renameTo(final String src, final String dst,\n         res = FSDirRenameOp.renameToInt(dir, pc, src, dst, logRetryCache,\n             options);\n       } finally {\n-        writeUnlock(operationName);\n+        FileStatus status = res != null ? res.auditStat : null;\n+        writeUnlock(operationName,\n+            getLockReportInfoSupplier(src, dst, status));\n       }\n     } catch (AccessControlException e) {\n-      logAuditEvent(false, operationName + \" (options=\" +\n-          Arrays.toString(options) + \")\", src, dst, null);\n+      logAuditEvent(false, operationName, src, dst, null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "16ad10709dc319b740371af2ecbb22c836d4b45c"}, "originalPosition": 330}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "190bdd3d2a57fddae04b4d9b1504c9d1fc7295e5", "author": {"user": {"login": "brfrn169", "name": "Toshihiro Suzuki"}}, "url": "https://github.com/apache/hadoop/commit/190bdd3d2a57fddae04b4d9b1504c9d1fc7295e5", "committedDate": "2020-04-15T20:06:11Z", "message": "HDFS-15217 Add more information to longest write/read lock held log"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "16ad10709dc319b740371af2ecbb22c836d4b45c", "author": {"user": {"login": "brfrn169", "name": "Toshihiro Suzuki"}}, "url": "https://github.com/apache/hadoop/commit/16ad10709dc319b740371af2ecbb22c836d4b45c", "committedDate": "2020-04-14T02:27:37Z", "message": "HDFS-15217 Add more information to longest write/read lock held log"}, "afterCommit": {"oid": "190bdd3d2a57fddae04b4d9b1504c9d1fc7295e5", "author": {"user": {"login": "brfrn169", "name": "Toshihiro Suzuki"}}, "url": "https://github.com/apache/hadoop/commit/190bdd3d2a57fddae04b4d9b1504c9d1fc7295e5", "committedDate": "2020-04-15T20:06:11Z", "message": "HDFS-15217 Add more information to longest write/read lock held log"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4592, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}