{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk5NTk1OTY0", "number": 2369, "title": "HADOOP-17301. ABFS: Fix bug introduced in HADOOP-16852 which reports read-ahead error back", "bodyText": "When reads done by readahead buffers failed, the exceptions where dropped and the failure was not getting reported to the calling app.\nJira HADOOP-16852: Report read-ahead error back\ntried to handle the scenario by reporting the error back to calling app. But the commit has introduced a bug which can lead to ReadBuffer being injected into read completed queue twice when it has finished the store operation.\nAdditionally, in a scenario where all readahead buffers are exhausted and the buffer chosen to evict is one which is failed read, there is no buffer returned for other reads to use. But successful eviction leads the queuing logic to determine there is a free buffer and while fetching the buffer index from free list, can lead to EmptyStack exceptions.\nThis PR fixes both these issues and also has added test checks for both scenarios.", "createdAt": "2020-10-08T01:04:54Z", "url": "https://github.com/apache/hadoop/pull/2369", "merged": true, "mergeCommit": {"oid": "c4fff74cc5841a4bf324c5096d16319fa860582f"}, "closed": true, "closedAt": "2020-10-13T15:30:35Z", "author": {"login": "snvijaya"}, "timelineItems": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdQWv0dAH2gAyNDk5NTk1OTY0OjMyYTFhMmZlMmQxYjg3NzIwMDIwZGE3YjQ5YjcwZGU0ZWM1ZTgwOTE=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdR_I6TAH2gAyNDk5NTk1OTY0OmEwYjNjZTVkYWNmNzAxOWYzN2Q4NmNmNDQ3ZGJmMDBhMzdmYTgxMWQ=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "32a1a2fe2d1b87720020da7b49b70de4ec5e8091", "author": {"user": {"login": "snvijaya", "name": "Sneha Vijayarajan"}}, "url": "https://github.com/apache/hadoop/commit/32a1a2fe2d1b87720020da7b49b70de4ec5e8091", "committedDate": "2020-10-08T00:50:10Z", "message": "Readahead fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c22734b1f1cfdc6ea0e350f028d8e9f72a04eb42", "author": {"user": {"login": "snvijaya", "name": "Sneha Vijayarajan"}}, "url": "https://github.com/apache/hadoop/commit/c22734b1f1cfdc6ea0e350f028d8e9f72a04eb42", "committedDate": "2020-10-08T08:14:50Z", "message": "checkstyle fixes"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA0ODQ4MjQ0", "url": "https://github.com/apache/hadoop/pull/2369#pullrequestreview-504848244", "createdAt": "2020-10-08T14:32:26Z", "commit": {"oid": "c22734b1f1cfdc6ea0e350f028d8e9f72a04eb42"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA0NzQ4NDc1", "url": "https://github.com/apache/hadoop/pull/2369#pullrequestreview-504748475", "createdAt": "2020-10-08T12:52:52Z", "commit": {"oid": "c22734b1f1cfdc6ea0e350f028d8e9f72a04eb42"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMjo1Mjo1MlrOHedDwQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMjo1NzoyMFrOHedPDA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTY5NTQyNQ==", "bodyText": "pull currentTimeMillis() outside the for loop as its an OS call with potential cost, and things probably work best if the same value is used through the loop and the code at L269", "url": "https://github.com/apache/hadoop/pull/2369#discussion_r501695425", "createdAt": "2020-10-08T12:52:52Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -242,13 +243,29 @@ private synchronized boolean tryEvict() {\n     }\n \n     // next, try any old nodes that have not been consumed\n+    // Failed read buffers (with buffer index=-1) that are older than\n+    // thresholdAge should be cleaned up, but at the same time should not\n+    // report successful eviction.\n+    // Queue logic expects that a buffer is freed up for read ahead when\n+    // eviction is successful, whereas a failed ReadBuffer would have released\n+    // its buffer when its status was set to READ_FAILED.\n     long earliestBirthday = Long.MAX_VALUE;\n+    ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>();\n     for (ReadBuffer buf : completedReadList) {\n-      if (buf.getTimeStamp() < earliestBirthday) {\n+      if ((buf.getBufferindex() != -1)\n+          && (buf.getTimeStamp() < earliestBirthday)) {\n         nodeToEvict = buf;\n         earliestBirthday = buf.getTimeStamp();\n+      } else if ((buf.getBufferindex() == -1)\n+          && (currentTimeMillis() - buf.getTimeStamp()) > thresholdAgeMilliseconds) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c22734b1f1cfdc6ea0e350f028d8e9f72a04eb42"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTY5NjI0MA==", "bodyText": "add (minimal) javadoc", "url": "https://github.com/apache/hadoop/pull/2369#discussion_r501696240", "createdAt": "2020-10-08T12:54:06Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -464,4 +480,10 @@ int getCompletedReadListSize() {\n   void callTryEvict() {\n     tryEvict();\n   }\n+\n+  @VisibleForTesting", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c22734b1f1cfdc6ea0e350f028d8e9f72a04eb42"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTY5NjUzNA==", "bodyText": "use 30_000 style integer", "url": "https://github.com/apache/hadoop/pull/2369#discussion_r501696534", "createdAt": "2020-10-08T12:54:34Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "diffHunk": "@@ -49,6 +49,7 @@\n   private static final int TWO_KB = 2 * 1024;\n   private static final int THREE_KB = 3 * 1024;\n   private static final int REDUCED_READ_BUFFER_AGE_THRESHOLD = 3000; // 3 sec\n+  private static final int INCREASED_READ_BUFFER_AGE_THRESHOLD = 30000; // 30 sec", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c22734b1f1cfdc6ea0e350f028d8e9f72a04eb42"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTY5Njk3Ng==", "bodyText": "import the field rather than a full reference", "url": "https://github.com/apache/hadoop/pull/2369#discussion_r501696976", "createdAt": "2020-10-08T12:55:12Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "diffHunk": "@@ -182,7 +183,39 @@ public void testFailedReadAhead() throws Exception {\n     checkEvictedStatus(inputStream, 0, false);\n   }\n \n+  @Test\n+  public void testFailedReadAheadEviction() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+    ReadBufferManager.setThresholdAgeMilliseconds(INCREASED_READ_BUFFER_AGE_THRESHOLD);\n+    // Stub :\n+    // Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // Actual read request fails with the failure in readahead thread\n+    doThrow(new TimeoutException(\"Internal Server error\"))\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class),\n+            any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testFailedReadAheadEviction.txt\");\n+\n+    // Add a failed buffer to completed queue and set to no free buffers to read ahead.\n+    ReadBuffer buff = new ReadBuffer();\n+    buff.setStatus(\n+        org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus.READ_FAILED);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c22734b1f1cfdc6ea0e350f028d8e9f72a04eb42"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTY5ODMxNg==", "bodyText": "use Assertions.assertThat with an explicit isLessThanOrEqualTo(3) assertion.", "url": "https://github.com/apache/hadoop/pull/2369#discussion_r501698316", "createdAt": "2020-10-08T12:57:20Z", "author": {"login": "steveloughran"}, "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "diffHunk": "@@ -264,12 +297,24 @@ public void testSuccessfulReadAhead() throws Exception {\n             any(String.class));\n \n     AbfsInputStream inputStream = getAbfsInputStream(client, \"testSuccessfulReadAhead.txt\");\n+    int beforeReadCompletedListSize = ReadBufferManager.getBufferManager().getCompletedReadListSize();\n \n     // First read request that triggers readAheads.\n     inputStream.read(new byte[ONE_KB]);\n \n     // Only the 3 readAhead threads should have triggered client.read\n     verifyReadCallCount(client, 3);\n+    int newAdditionsToCompletedRead =\n+        ReadBufferManager.getBufferManager().getCompletedReadListSize()\n+            - beforeReadCompletedListSize;\n+    // read buffer might be dumped if the ReadBufferManager getblock preceded\n+    // the action of buffer being picked for reading from readaheadqueue, so that\n+    // inputstream can proceed with read and not be blocked on readahead thread\n+    // availability. So the count of buffers in completedReadQueue for the stream\n+    // can be same or lesser than the requests triggered to queue readahead.\n+    assertTrue(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c22734b1f1cfdc6ea0e350f028d8e9f72a04eb42"}, "originalPosition": 67}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA1MzUxOTM3", "url": "https://github.com/apache/hadoop/pull/2369#pullrequestreview-505351937", "createdAt": "2020-10-09T04:12:05Z", "commit": {"oid": "c22734b1f1cfdc6ea0e350f028d8e9f72a04eb42"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a0b3ce5dacf7019f37d86cf447dbf00a37fa811d", "author": {"user": {"login": "snvijaya", "name": "Sneha Vijayarajan"}}, "url": "https://github.com/apache/hadoop/commit/a0b3ce5dacf7019f37d86cf447dbf00a37fa811d", "committedDate": "2020-10-13T02:27:42Z", "message": "Incorporate review comments"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3438, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}