{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDExMTk3ODYx", "number": 1988, "reviewThreads": {"totalCount": 25, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwMToyMzo1MlrOD4Qr9w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QwNDoyNToxMVrOD4uhlw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwMzIwMjQ3OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FsConstants.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwMToyMzo1MlrOGO_nDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwODoyNzozNVrOGPEzDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM3NTQzNw==", "bodyText": "Why add this here? This is just used in tests right?", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418375437", "createdAt": "2020-05-01T01:23:52Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FsConstants.java", "diffHunk": "@@ -42,4 +42,11 @@\n    */\n   public static final URI VIEWFS_URI = URI.create(\"viewfs:///\");\n   public static final String VIEWFS_SCHEME = \"viewfs\";\n+\n+  public static final String VIEWFS_OVERLOAD_SCHEME_KEY =\n+      \"fs.viewfs.overload.scheme\";\n+  public static final String VIEWFS_OVERLOAD_SCHEME_DEFAULT = \"hdfs\";\n+  public static final String FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN_KEY =\n+      \"fs.viewfs.overload.scheme.target.%s.impl\";\n+  public static final String FS_IMPL_PATTERN_KEY = \"fs.%s.impl\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "421efbd24c2653273cda895aca33c8a146cecb62"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQ2MDQyOQ==", "bodyText": "Initial commit was using first two in src, anyway now I removed config of VIEWFS_OVERLOAD_SCHEME_DEFAULT completely. So, only FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN is there now and using this in ViewFsOverloadScheme class.", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418460429", "createdAt": "2020-05-01T08:27:35Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FsConstants.java", "diffHunk": "@@ -42,4 +42,11 @@\n    */\n   public static final URI VIEWFS_URI = URI.create(\"viewfs:///\");\n   public static final String VIEWFS_SCHEME = \"viewfs\";\n+\n+  public static final String VIEWFS_OVERLOAD_SCHEME_KEY =\n+      \"fs.viewfs.overload.scheme\";\n+  public static final String VIEWFS_OVERLOAD_SCHEME_DEFAULT = \"hdfs\";\n+  public static final String FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN_KEY =\n+      \"fs.viewfs.overload.scheme.target.%s.impl\";\n+  public static final String FS_IMPL_PATTERN_KEY = \"fs.%s.impl\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM3NTQzNw=="}, "originalCommit": {"oid": "421efbd24c2653273cda895aca33c8a146cecb62"}, "originalPosition": 10}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwMzIwNjU2OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwMToyNjoyNlrOGO_pbA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwODoyODoxNlrOGPEz0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM3NjA0NA==", "bodyText": "javadoc for this method? This seems a bit hacky but I understand the need. I think initializeSuperFs is a slightly  better name but don't have a strong opinion here.", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418376044", "createdAt": "2020-05-01T01:26:26Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java", "diffHunk": "@@ -302,6 +320,11 @@ protected FileSystem getTargetFileSystem(final String settings,\n     }\n   }\n \n+  protected void superFSInit(final URI theUri, final Configuration conf)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "421efbd24c2653273cda895aca33c8a146cecb62"}, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQ2MDYyNg==", "bodyText": "Removed this method now. After refactoring initialization part, this got addressed along with that.", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418460626", "createdAt": "2020-05-01T08:28:16Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java", "diffHunk": "@@ -302,6 +320,11 @@ protected FileSystem getTargetFileSystem(final String settings,\n     }\n   }\n \n+  protected void superFSInit(final URI theUri, final Configuration conf)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM3NjA0NA=="}, "originalCommit": {"oid": "421efbd24c2653273cda895aca33c8a146cecb62"}, "originalPosition": 98}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwMzIwODA3OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwMToyNzoyM1rOGO_qVw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwODoyODozM1rOGPE0Kg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM3NjI3OQ==", "bodyText": "nits:\n\"object is\" -> \"objective here is to handle\"\n\"a multiple\" -> multiple.", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418376279", "createdAt": "2020-05-01T01:27:23Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+\n+/******************************************************************************\n+ * This class is extended from the ViewFileSystem for the overloaded scheme \n+ * file system. This object is the way end-user code interacts with a multiple", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "421efbd24c2653273cda895aca33c8a146cecb62"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQ2MDcxNA==", "bodyText": "Thanks corrected them.", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418460714", "createdAt": "2020-05-01T08:28:33Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+\n+/******************************************************************************\n+ * This class is extended from the ViewFileSystem for the overloaded scheme \n+ * file system. This object is the way end-user code interacts with a multiple", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM3NjI3OQ=="}, "originalCommit": {"oid": "421efbd24c2653273cda895aca33c8a146cecb62"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwMzIxMTQxOnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwMToyOTozNFrOGO_sPg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QyMzozMjo0NlrOGPwaHw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM3Njc2Ng==", "bodyText": "The explanation here is a little confusing. I think it's easier to provide an example on how it works rather than try to write about it.", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418376766", "createdAt": "2020-05-01T01:29:34Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+\n+/******************************************************************************", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "421efbd24c2653273cda895aca33c8a146cecb62"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQ2MDg1Nw==", "bodyText": "I added examples please let me know if they are make sense.", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418460857", "createdAt": "2020-05-01T08:29:07Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+\n+/******************************************************************************", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM3Njc2Ng=="}, "originalCommit": {"oid": "421efbd24c2653273cda895aca33c8a146cecb62"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0MDk3Nw==", "bodyText": "Thanks for adding these Uma. A couple of comments:\n(a) \"The objective here is to handle multiple mounted file systems transparently.\", \"Unlike ViewFileSystem\nscheme (viewfs://), the users would be able to use any scheme.\"   --> These are not functions of this class. ViewFileSystem already does this.\n(b) Configuring fs.SCHEME.impl = ViewFsOverloadScheme is not explained in these examples. That should also be brought up.", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419040977", "createdAt": "2020-05-03T03:31:28Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+\n+/******************************************************************************", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM3Njc2Ng=="}, "originalCommit": {"oid": "421efbd24c2653273cda895aca33c8a146cecb62"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTE3NDk0Mw==", "bodyText": "#b) added config details. Further more details, we will be covering in user guide.\n#a) first point, yes that ViewFS function. second part, we have overridden getScheme, and that will allow VFSOS to work with any scheme. Any scheme flexibility should come to this class only.", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419174943", "createdAt": "2020-05-03T23:32:46Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+\n+/******************************************************************************", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM3Njc2Ng=="}, "originalCommit": {"oid": "421efbd24c2653273cda895aca33c8a146cecb62"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwMzIzMDQxOnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwMTo0MjoyOFrOGO_3Sg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwODozMDozM1rOGPE2HA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM3OTU5NA==", "bodyText": "What if this is not set? No check for this currently.", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418379594", "createdAt": "2020-05-01T01:42:28Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+\n+/******************************************************************************\n+ * This class is extended from the ViewFileSystem for the overloaded scheme \n+ * file system. This object is the way end-user code interacts with a multiple\n+ * mounted file systems transparently. Overloaded scheme based uri can be\n+ * continued to use as end user interactive uri and mount links can be\n+ * configured to any Hadoop compatible file system. This class maintains all \n+ * the target file system instances and delegates the calls to respective \n+ * target file system based on mount link mapping. Mount link configuration\n+ * format and behavior is same as ViewFileSystem.\n+ *****************************************************************************/\n+@InterfaceAudience.LimitedPrivate({ \"MapReduce\", \"HBase\", \"Hive\" })\n+@InterfaceStability.Evolving\n+public class ViewFsOverloadScheme extends ViewFileSystem {\n+\n+  public ViewFsOverloadScheme() throws IOException {\n+    super();\n+  }\n+\n+  private FsCreator fsCreator;\n+  private String myScheme;\n+\n+  @Override\n+  public String getScheme() {\n+    return myScheme;\n+  }\n+\n+  @Override\n+  public void initialize(final URI theUri, final Configuration conf)\n+      throws IOException {\n+    superFSInit(theUri, conf);\n+    setConf(conf);\n+    config = conf;\n+    myScheme = config.get(FsConstants.VIEWFS_OVERLOAD_SCHEME_KEY);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "421efbd24c2653273cda895aca33c8a146cecb62"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQ2MTIxMg==", "bodyText": "Good point. This config is not required now. Allowing to init by any scheme URI.", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418461212", "createdAt": "2020-05-01T08:30:33Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+\n+/******************************************************************************\n+ * This class is extended from the ViewFileSystem for the overloaded scheme \n+ * file system. This object is the way end-user code interacts with a multiple\n+ * mounted file systems transparently. Overloaded scheme based uri can be\n+ * continued to use as end user interactive uri and mount links can be\n+ * configured to any Hadoop compatible file system. This class maintains all \n+ * the target file system instances and delegates the calls to respective \n+ * target file system based on mount link mapping. Mount link configuration\n+ * format and behavior is same as ViewFileSystem.\n+ *****************************************************************************/\n+@InterfaceAudience.LimitedPrivate({ \"MapReduce\", \"HBase\", \"Hive\" })\n+@InterfaceStability.Evolving\n+public class ViewFsOverloadScheme extends ViewFileSystem {\n+\n+  public ViewFsOverloadScheme() throws IOException {\n+    super();\n+  }\n+\n+  private FsCreator fsCreator;\n+  private String myScheme;\n+\n+  @Override\n+  public String getScheme() {\n+    return myScheme;\n+  }\n+\n+  @Override\n+  public void initialize(final URI theUri, final Configuration conf)\n+      throws IOException {\n+    superFSInit(theUri, conf);\n+    setConf(conf);\n+    config = conf;\n+    myScheme = config.get(FsConstants.VIEWFS_OVERLOAD_SCHEME_KEY);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM3OTU5NA=="}, "originalCommit": {"oid": "421efbd24c2653273cda895aca33c8a146cecb62"}, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwMzIzNDQ1OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwMTo0NDo1NVrOGO_5hg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwODozMToyMlrOGPE23w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM4MDE2Ng==", "bodyText": "Is there a reason why the scheme of this FS needs to be set using VIEWFS_OVERLOAD_SCHEME_KEY? I would have assumed that we use \"view://\" similar to ViewFileSystem. Will we have valid use cases where fs.getScheme() needs to match \"hdfs\"?", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418380166", "createdAt": "2020-05-01T01:44:55Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+\n+/******************************************************************************\n+ * This class is extended from the ViewFileSystem for the overloaded scheme \n+ * file system. This object is the way end-user code interacts with a multiple\n+ * mounted file systems transparently. Overloaded scheme based uri can be\n+ * continued to use as end user interactive uri and mount links can be\n+ * configured to any Hadoop compatible file system. This class maintains all \n+ * the target file system instances and delegates the calls to respective \n+ * target file system based on mount link mapping. Mount link configuration\n+ * format and behavior is same as ViewFileSystem.\n+ *****************************************************************************/\n+@InterfaceAudience.LimitedPrivate({ \"MapReduce\", \"HBase\", \"Hive\" })\n+@InterfaceStability.Evolving\n+public class ViewFsOverloadScheme extends ViewFileSystem {\n+\n+  public ViewFsOverloadScheme() throws IOException {\n+    super();\n+  }\n+\n+  private FsCreator fsCreator;\n+  private String myScheme;\n+\n+  @Override\n+  public String getScheme() {\n+    return myScheme;\n+  }\n+\n+  @Override\n+  public void initialize(final URI theUri, final Configuration conf)\n+      throws IOException {\n+    superFSInit(theUri, conf);\n+    setConf(conf);\n+    config = conf;\n+    myScheme = config.get(FsConstants.VIEWFS_OVERLOAD_SCHEME_KEY);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "421efbd24c2653273cda895aca33c8a146cecb62"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQ2MTQwNw==", "bodyText": "same as above. Removed this now and that should address your comment. Please let me know if that not the case.", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418461407", "createdAt": "2020-05-01T08:31:22Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+\n+/******************************************************************************\n+ * This class is extended from the ViewFileSystem for the overloaded scheme \n+ * file system. This object is the way end-user code interacts with a multiple\n+ * mounted file systems transparently. Overloaded scheme based uri can be\n+ * continued to use as end user interactive uri and mount links can be\n+ * configured to any Hadoop compatible file system. This class maintains all \n+ * the target file system instances and delegates the calls to respective \n+ * target file system based on mount link mapping. Mount link configuration\n+ * format and behavior is same as ViewFileSystem.\n+ *****************************************************************************/\n+@InterfaceAudience.LimitedPrivate({ \"MapReduce\", \"HBase\", \"Hive\" })\n+@InterfaceStability.Evolving\n+public class ViewFsOverloadScheme extends ViewFileSystem {\n+\n+  public ViewFsOverloadScheme() throws IOException {\n+    super();\n+  }\n+\n+  private FsCreator fsCreator;\n+  private String myScheme;\n+\n+  @Override\n+  public String getScheme() {\n+    return myScheme;\n+  }\n+\n+  @Override\n+  public void initialize(final URI theUri, final Configuration conf)\n+      throws IOException {\n+    superFSInit(theUri, conf);\n+    setConf(conf);\n+    config = conf;\n+    myScheme = config.get(FsConstants.VIEWFS_OVERLOAD_SCHEME_KEY);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM4MDE2Ng=="}, "originalCommit": {"oid": "421efbd24c2653273cda895aca33c8a146cecb62"}, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwMzIzOTc0OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwMTo0ODozNVrOGO_8vA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QyMzo1NDo0M1rOGPwhww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM4MDk4OA==", "bodyText": "Currently, the override only works for one scheme. This alone can prevent the circular dependency. However, should we consider calling createFileSystem for all schemes that have FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN_KEY set?", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418380988", "createdAt": "2020-05-01T01:48:35Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+\n+/******************************************************************************\n+ * This class is extended from the ViewFileSystem for the overloaded scheme \n+ * file system. This object is the way end-user code interacts with a multiple\n+ * mounted file systems transparently. Overloaded scheme based uri can be\n+ * continued to use as end user interactive uri and mount links can be\n+ * configured to any Hadoop compatible file system. This class maintains all \n+ * the target file system instances and delegates the calls to respective \n+ * target file system based on mount link mapping. Mount link configuration\n+ * format and behavior is same as ViewFileSystem.\n+ *****************************************************************************/\n+@InterfaceAudience.LimitedPrivate({ \"MapReduce\", \"HBase\", \"Hive\" })\n+@InterfaceStability.Evolving\n+public class ViewFsOverloadScheme extends ViewFileSystem {\n+\n+  public ViewFsOverloadScheme() throws IOException {\n+    super();\n+  }\n+\n+  private FsCreator fsCreator;\n+  private String myScheme;\n+\n+  @Override\n+  public String getScheme() {\n+    return myScheme;\n+  }\n+\n+  @Override\n+  public void initialize(final URI theUri, final Configuration conf)\n+      throws IOException {\n+    superFSInit(theUri, conf);\n+    setConf(conf);\n+    config = conf;\n+    myScheme = config.get(FsConstants.VIEWFS_OVERLOAD_SCHEME_KEY);\n+    fsCreator = new FsCreator() {\n+\n+      /**\n+       * This method is overridden because in ViewFsOverloadScheme if\n+       * overloaded scheme matches with mounted target fs scheme, file system\n+       * should be created without going into fs.<scheme>.impl based \n+       * resolution. Otherwise it will end up into loop as target will be \n+       * resolved again to ViewFsOverloadScheme as fs.<scheme>.impl points to\n+       * ViewFsOverloadScheme. So, below method will initialize the\n+       * fs.viewfs.overload.scheme.target.<scheme>.impl. Other schemes can\n+       * follow fs.newInstance\n+       */\n+      @Override\n+      public FileSystem createFs(URI uri, Configuration conf)\n+          throws IOException {\n+        if (uri.getScheme().equals(myScheme)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "421efbd24c2653273cda895aca33c8a146cecb62"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQ2MTgwOQ==", "bodyText": "Yes, addressed this. Thanks", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418461809", "createdAt": "2020-05-01T08:32:49Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+\n+/******************************************************************************\n+ * This class is extended from the ViewFileSystem for the overloaded scheme \n+ * file system. This object is the way end-user code interacts with a multiple\n+ * mounted file systems transparently. Overloaded scheme based uri can be\n+ * continued to use as end user interactive uri and mount links can be\n+ * configured to any Hadoop compatible file system. This class maintains all \n+ * the target file system instances and delegates the calls to respective \n+ * target file system based on mount link mapping. Mount link configuration\n+ * format and behavior is same as ViewFileSystem.\n+ *****************************************************************************/\n+@InterfaceAudience.LimitedPrivate({ \"MapReduce\", \"HBase\", \"Hive\" })\n+@InterfaceStability.Evolving\n+public class ViewFsOverloadScheme extends ViewFileSystem {\n+\n+  public ViewFsOverloadScheme() throws IOException {\n+    super();\n+  }\n+\n+  private FsCreator fsCreator;\n+  private String myScheme;\n+\n+  @Override\n+  public String getScheme() {\n+    return myScheme;\n+  }\n+\n+  @Override\n+  public void initialize(final URI theUri, final Configuration conf)\n+      throws IOException {\n+    superFSInit(theUri, conf);\n+    setConf(conf);\n+    config = conf;\n+    myScheme = config.get(FsConstants.VIEWFS_OVERLOAD_SCHEME_KEY);\n+    fsCreator = new FsCreator() {\n+\n+      /**\n+       * This method is overridden because in ViewFsOverloadScheme if\n+       * overloaded scheme matches with mounted target fs scheme, file system\n+       * should be created without going into fs.<scheme>.impl based \n+       * resolution. Otherwise it will end up into loop as target will be \n+       * resolved again to ViewFsOverloadScheme as fs.<scheme>.impl points to\n+       * ViewFsOverloadScheme. So, below method will initialize the\n+       * fs.viewfs.overload.scheme.target.<scheme>.impl. Other schemes can\n+       * follow fs.newInstance\n+       */\n+      @Override\n+      public FileSystem createFs(URI uri, Configuration conf)\n+          throws IOException {\n+        if (uri.getScheme().equals(myScheme)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM4MDk4OA=="}, "originalCommit": {"oid": "421efbd24c2653273cda895aca33c8a146cecb62"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0NTYwNA==", "bodyText": "Don't think this is addressed. I am saying createFileSystem should be called here for every scheme for which the appropriate FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN_KEY is set and only those where uri.getScheme().equals(myScheme) . Wdyt?", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419045604", "createdAt": "2020-05-03T04:31:44Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+\n+/******************************************************************************\n+ * This class is extended from the ViewFileSystem for the overloaded scheme \n+ * file system. This object is the way end-user code interacts with a multiple\n+ * mounted file systems transparently. Overloaded scheme based uri can be\n+ * continued to use as end user interactive uri and mount links can be\n+ * configured to any Hadoop compatible file system. This class maintains all \n+ * the target file system instances and delegates the calls to respective \n+ * target file system based on mount link mapping. Mount link configuration\n+ * format and behavior is same as ViewFileSystem.\n+ *****************************************************************************/\n+@InterfaceAudience.LimitedPrivate({ \"MapReduce\", \"HBase\", \"Hive\" })\n+@InterfaceStability.Evolving\n+public class ViewFsOverloadScheme extends ViewFileSystem {\n+\n+  public ViewFsOverloadScheme() throws IOException {\n+    super();\n+  }\n+\n+  private FsCreator fsCreator;\n+  private String myScheme;\n+\n+  @Override\n+  public String getScheme() {\n+    return myScheme;\n+  }\n+\n+  @Override\n+  public void initialize(final URI theUri, final Configuration conf)\n+      throws IOException {\n+    superFSInit(theUri, conf);\n+    setConf(conf);\n+    config = conf;\n+    myScheme = config.get(FsConstants.VIEWFS_OVERLOAD_SCHEME_KEY);\n+    fsCreator = new FsCreator() {\n+\n+      /**\n+       * This method is overridden because in ViewFsOverloadScheme if\n+       * overloaded scheme matches with mounted target fs scheme, file system\n+       * should be created without going into fs.<scheme>.impl based \n+       * resolution. Otherwise it will end up into loop as target will be \n+       * resolved again to ViewFsOverloadScheme as fs.<scheme>.impl points to\n+       * ViewFsOverloadScheme. So, below method will initialize the\n+       * fs.viewfs.overload.scheme.target.<scheme>.impl. Other schemes can\n+       * follow fs.newInstance\n+       */\n+      @Override\n+      public FileSystem createFs(URI uri, Configuration conf)\n+          throws IOException {\n+        if (uri.getScheme().equals(myScheme)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM4MDk4OA=="}, "originalCommit": {"oid": "421efbd24c2653273cda895aca33c8a146cecb62"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTE3Njg5OQ==", "bodyText": "For other schemes which are not matching will be able to get it resolved by FileSystem itself right?\nWhen target uri is scheme is same as OverloadScheme uri sheme, then only we need to bypass to avoid looping. Other cases, FileSystem#get or FileSystem#newInstance will get the right instance. Am I missing?\nSince your exposed uri scheme occupied with OverloadScheme impl, we need that additional config to get actual impl. This was the idea of that configuration.\n\nCurrently, the override only works for one scheme.\n\nUser is going to use one scheme from out side right per OverloadScheme instance right?\nBecause your OverloadScheme init will take one uri to initialize, That will be user initialized uri. Rest all will go as mapping uris.\nexample: inited uri is hdfs://xyz:9000\nmapping target uris are /target1 -> hdfs://target1:9000, /target2 -> hdfs://target2:9000, /target3 -> s3a://bucket1/target3\nHere when getting target fs: we would check if scheme matches with inited. Here hdfs was inited. So, for hdfs scheme, FileSystem#get resolution will always to OverloadSchem impl. To get actual impl we use the FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN_KEY to initialize target fs. Other impls should be able to resolved by FileSystem#get.\nIs your use case is for configuring Overload scheme for all fs..impl in same client process? example in same client process, you would configure, fs.hdfs.impl=ViewFSOverloadScheme and fs.s3a.impl=ViewFSOverloadScheme ?\nBefore changing it would be good for me to understand your use case here. -Thanks", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419176899", "createdAt": "2020-05-03T23:54:43Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+\n+/******************************************************************************\n+ * This class is extended from the ViewFileSystem for the overloaded scheme \n+ * file system. This object is the way end-user code interacts with a multiple\n+ * mounted file systems transparently. Overloaded scheme based uri can be\n+ * continued to use as end user interactive uri and mount links can be\n+ * configured to any Hadoop compatible file system. This class maintains all \n+ * the target file system instances and delegates the calls to respective \n+ * target file system based on mount link mapping. Mount link configuration\n+ * format and behavior is same as ViewFileSystem.\n+ *****************************************************************************/\n+@InterfaceAudience.LimitedPrivate({ \"MapReduce\", \"HBase\", \"Hive\" })\n+@InterfaceStability.Evolving\n+public class ViewFsOverloadScheme extends ViewFileSystem {\n+\n+  public ViewFsOverloadScheme() throws IOException {\n+    super();\n+  }\n+\n+  private FsCreator fsCreator;\n+  private String myScheme;\n+\n+  @Override\n+  public String getScheme() {\n+    return myScheme;\n+  }\n+\n+  @Override\n+  public void initialize(final URI theUri, final Configuration conf)\n+      throws IOException {\n+    superFSInit(theUri, conf);\n+    setConf(conf);\n+    config = conf;\n+    myScheme = config.get(FsConstants.VIEWFS_OVERLOAD_SCHEME_KEY);\n+    fsCreator = new FsCreator() {\n+\n+      /**\n+       * This method is overridden because in ViewFsOverloadScheme if\n+       * overloaded scheme matches with mounted target fs scheme, file system\n+       * should be created without going into fs.<scheme>.impl based \n+       * resolution. Otherwise it will end up into loop as target will be \n+       * resolved again to ViewFsOverloadScheme as fs.<scheme>.impl points to\n+       * ViewFsOverloadScheme. So, below method will initialize the\n+       * fs.viewfs.overload.scheme.target.<scheme>.impl. Other schemes can\n+       * follow fs.newInstance\n+       */\n+      @Override\n+      public FileSystem createFs(URI uri, Configuration conf)\n+          throws IOException {\n+        if (uri.getScheme().equals(myScheme)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM4MDk4OA=="}, "originalCommit": {"oid": "421efbd24c2653273cda895aca33c8a146cecb62"}, "originalPosition": 81}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwMzI0Nzk0OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwMTo1Mzo1MFrOGPABiA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwODozMTo0MFrOGPE3Tw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM4MjIxNg==", "bodyText": "nit: \"with mounted target fs scheme\" -> \"the scheme of the target fs\"\n\"file system should be created\" -> \"the target file system ..\"", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418382216", "createdAt": "2020-05-01T01:53:50Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+\n+/******************************************************************************\n+ * This class is extended from the ViewFileSystem for the overloaded scheme \n+ * file system. This object is the way end-user code interacts with a multiple\n+ * mounted file systems transparently. Overloaded scheme based uri can be\n+ * continued to use as end user interactive uri and mount links can be\n+ * configured to any Hadoop compatible file system. This class maintains all \n+ * the target file system instances and delegates the calls to respective \n+ * target file system based on mount link mapping. Mount link configuration\n+ * format and behavior is same as ViewFileSystem.\n+ *****************************************************************************/\n+@InterfaceAudience.LimitedPrivate({ \"MapReduce\", \"HBase\", \"Hive\" })\n+@InterfaceStability.Evolving\n+public class ViewFsOverloadScheme extends ViewFileSystem {\n+\n+  public ViewFsOverloadScheme() throws IOException {\n+    super();\n+  }\n+\n+  private FsCreator fsCreator;\n+  private String myScheme;\n+\n+  @Override\n+  public String getScheme() {\n+    return myScheme;\n+  }\n+\n+  @Override\n+  public void initialize(final URI theUri, final Configuration conf)\n+      throws IOException {\n+    superFSInit(theUri, conf);\n+    setConf(conf);\n+    config = conf;\n+    myScheme = config.get(FsConstants.VIEWFS_OVERLOAD_SCHEME_KEY);\n+    fsCreator = new FsCreator() {\n+\n+      /**\n+       * This method is overridden because in ViewFsOverloadScheme if\n+       * overloaded scheme matches with mounted target fs scheme, file system", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "421efbd24c2653273cda895aca33c8a146cecb62"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQ2MTUxOQ==", "bodyText": "corrected. Thanks", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418461519", "createdAt": "2020-05-01T08:31:40Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+\n+/******************************************************************************\n+ * This class is extended from the ViewFileSystem for the overloaded scheme \n+ * file system. This object is the way end-user code interacts with a multiple\n+ * mounted file systems transparently. Overloaded scheme based uri can be\n+ * continued to use as end user interactive uri and mount links can be\n+ * configured to any Hadoop compatible file system. This class maintains all \n+ * the target file system instances and delegates the calls to respective \n+ * target file system based on mount link mapping. Mount link configuration\n+ * format and behavior is same as ViewFileSystem.\n+ *****************************************************************************/\n+@InterfaceAudience.LimitedPrivate({ \"MapReduce\", \"HBase\", \"Hive\" })\n+@InterfaceStability.Evolving\n+public class ViewFsOverloadScheme extends ViewFileSystem {\n+\n+  public ViewFsOverloadScheme() throws IOException {\n+    super();\n+  }\n+\n+  private FsCreator fsCreator;\n+  private String myScheme;\n+\n+  @Override\n+  public String getScheme() {\n+    return myScheme;\n+  }\n+\n+  @Override\n+  public void initialize(final URI theUri, final Configuration conf)\n+      throws IOException {\n+    superFSInit(theUri, conf);\n+    setConf(conf);\n+    config = conf;\n+    myScheme = config.get(FsConstants.VIEWFS_OVERLOAD_SCHEME_KEY);\n+    fsCreator = new FsCreator() {\n+\n+      /**\n+       * This method is overridden because in ViewFsOverloadScheme if\n+       * overloaded scheme matches with mounted target fs scheme, file system", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM4MjIxNg=="}, "originalCommit": {"oid": "421efbd24c2653273cda895aca33c8a146cecb62"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwMzI1MDAwOnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwMTo1NTowNFrOGPACvw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwODozMzo0MFrOGPE5JQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM4MjUyNw==", "bodyText": "nit: \"into loop\" -> \"in an infinite loop as the target\"", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418382527", "createdAt": "2020-05-01T01:55:04Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+\n+/******************************************************************************\n+ * This class is extended from the ViewFileSystem for the overloaded scheme \n+ * file system. This object is the way end-user code interacts with a multiple\n+ * mounted file systems transparently. Overloaded scheme based uri can be\n+ * continued to use as end user interactive uri and mount links can be\n+ * configured to any Hadoop compatible file system. This class maintains all \n+ * the target file system instances and delegates the calls to respective \n+ * target file system based on mount link mapping. Mount link configuration\n+ * format and behavior is same as ViewFileSystem.\n+ *****************************************************************************/\n+@InterfaceAudience.LimitedPrivate({ \"MapReduce\", \"HBase\", \"Hive\" })\n+@InterfaceStability.Evolving\n+public class ViewFsOverloadScheme extends ViewFileSystem {\n+\n+  public ViewFsOverloadScheme() throws IOException {\n+    super();\n+  }\n+\n+  private FsCreator fsCreator;\n+  private String myScheme;\n+\n+  @Override\n+  public String getScheme() {\n+    return myScheme;\n+  }\n+\n+  @Override\n+  public void initialize(final URI theUri, final Configuration conf)\n+      throws IOException {\n+    superFSInit(theUri, conf);\n+    setConf(conf);\n+    config = conf;\n+    myScheme = config.get(FsConstants.VIEWFS_OVERLOAD_SCHEME_KEY);\n+    fsCreator = new FsCreator() {\n+\n+      /**\n+       * This method is overridden because in ViewFsOverloadScheme if\n+       * overloaded scheme matches with mounted target fs scheme, file system\n+       * should be created without going into fs.<scheme>.impl based \n+       * resolution. Otherwise it will end up into loop as target will be ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "421efbd24c2653273cda895aca33c8a146cecb62"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQ2MTk4OQ==", "bodyText": "done.", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418461989", "createdAt": "2020-05-01T08:33:40Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+\n+/******************************************************************************\n+ * This class is extended from the ViewFileSystem for the overloaded scheme \n+ * file system. This object is the way end-user code interacts with a multiple\n+ * mounted file systems transparently. Overloaded scheme based uri can be\n+ * continued to use as end user interactive uri and mount links can be\n+ * configured to any Hadoop compatible file system. This class maintains all \n+ * the target file system instances and delegates the calls to respective \n+ * target file system based on mount link mapping. Mount link configuration\n+ * format and behavior is same as ViewFileSystem.\n+ *****************************************************************************/\n+@InterfaceAudience.LimitedPrivate({ \"MapReduce\", \"HBase\", \"Hive\" })\n+@InterfaceStability.Evolving\n+public class ViewFsOverloadScheme extends ViewFileSystem {\n+\n+  public ViewFsOverloadScheme() throws IOException {\n+    super();\n+  }\n+\n+  private FsCreator fsCreator;\n+  private String myScheme;\n+\n+  @Override\n+  public String getScheme() {\n+    return myScheme;\n+  }\n+\n+  @Override\n+  public void initialize(final URI theUri, final Configuration conf)\n+      throws IOException {\n+    superFSInit(theUri, conf);\n+    setConf(conf);\n+    config = conf;\n+    myScheme = config.get(FsConstants.VIEWFS_OVERLOAD_SCHEME_KEY);\n+    fsCreator = new FsCreator() {\n+\n+      /**\n+       * This method is overridden because in ViewFsOverloadScheme if\n+       * overloaded scheme matches with mounted target fs scheme, file system\n+       * should be created without going into fs.<scheme>.impl based \n+       * resolution. Otherwise it will end up into loop as target will be ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM4MjUyNw=="}, "originalCommit": {"oid": "421efbd24c2653273cda895aca33c8a146cecb62"}, "originalPosition": 72}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwMzI2MzgyOnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwMjowNDozNVrOGPAK5w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwODozNDo1NlrOGPE6kw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM4NDYxNQ==", "bodyText": "If we add a protected method getFsCreator(String scheme) in ViewFileSystem and override it this class,I think initialize can be made much simpler. it just needs to conf.setBoolean(CONFIG_VIEWFS_ENABLE_INNER_CACHE, true) and call  super.initialize(theUri, conf). Is that correct?", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418384615", "createdAt": "2020-05-01T02:04:35Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+\n+/******************************************************************************\n+ * This class is extended from the ViewFileSystem for the overloaded scheme \n+ * file system. This object is the way end-user code interacts with a multiple\n+ * mounted file systems transparently. Overloaded scheme based uri can be\n+ * continued to use as end user interactive uri and mount links can be\n+ * configured to any Hadoop compatible file system. This class maintains all \n+ * the target file system instances and delegates the calls to respective \n+ * target file system based on mount link mapping. Mount link configuration\n+ * format and behavior is same as ViewFileSystem.\n+ *****************************************************************************/\n+@InterfaceAudience.LimitedPrivate({ \"MapReduce\", \"HBase\", \"Hive\" })\n+@InterfaceStability.Evolving\n+public class ViewFsOverloadScheme extends ViewFileSystem {\n+\n+  public ViewFsOverloadScheme() throws IOException {\n+    super();\n+  }\n+\n+  private FsCreator fsCreator;\n+  private String myScheme;\n+\n+  @Override\n+  public String getScheme() {\n+    return myScheme;\n+  }\n+\n+  @Override\n+  public void initialize(final URI theUri, final Configuration conf)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "421efbd24c2653273cda895aca33c8a146cecb62"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQ2MjM1NQ==", "bodyText": "Thanks. Refactored and please check if it is addressing this comment.", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418462355", "createdAt": "2020-05-01T08:34:56Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+\n+/******************************************************************************\n+ * This class is extended from the ViewFileSystem for the overloaded scheme \n+ * file system. This object is the way end-user code interacts with a multiple\n+ * mounted file systems transparently. Overloaded scheme based uri can be\n+ * continued to use as end user interactive uri and mount links can be\n+ * configured to any Hadoop compatible file system. This class maintains all \n+ * the target file system instances and delegates the calls to respective \n+ * target file system based on mount link mapping. Mount link configuration\n+ * format and behavior is same as ViewFileSystem.\n+ *****************************************************************************/\n+@InterfaceAudience.LimitedPrivate({ \"MapReduce\", \"HBase\", \"Hive\" })\n+@InterfaceStability.Evolving\n+public class ViewFsOverloadScheme extends ViewFileSystem {\n+\n+  public ViewFsOverloadScheme() throws IOException {\n+    super();\n+  }\n+\n+  private FsCreator fsCreator;\n+  private String myScheme;\n+\n+  @Override\n+  public String getScheme() {\n+    return myScheme;\n+  }\n+\n+  @Override\n+  public void initialize(final URI theUri, final Configuration conf)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM4NDYxNQ=="}, "originalCommit": {"oid": "421efbd24c2653273cda895aca33c8a146cecb62"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwNTUxNTQ0OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQyMDo1MDoyOVrOGPVYfg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNFQxOTowMTozM1rOGQN9qw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODczMjE1OA==", "bodyText": "why do need this method while we can just call new FsGetter() directly?", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418732158", "createdAt": "2020-05-01T20:50:29Z", "author": {"login": "chliang71"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java", "diffHunk": "@@ -96,16 +96,49 @@ static AccessControlException readOnlyMountTable(final String operation,\n     return readOnlyMountTable(operation, p.toString());\n   }\n \n+  /**\n+   * File system instance getter.\n+   */\n+  static class FsGetter {\n+\n+    /**\n+     * Gets new file system instance of given uri.\n+     */\n+    public FileSystem getNewInstance(URI uri, Configuration conf)\n+        throws IOException {\n+      return FileSystem.newInstance(uri, conf);\n+    }\n+\n+    /**\n+     * Gets file system instance of given uri.\n+     */\n+    public FileSystem get(URI uri, Configuration conf) throws IOException {\n+      return FileSystem.get(uri, conf);\n+    }\n+  }\n+\n+  /**\n+   * Gets file system creator instance.\n+   */\n+  protected FsGetter fsGetter() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8010048dc27959c10c7a982774b4312f4c64924c"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc0Mjk2MA==", "bodyText": "Thanks @chliang71  for the review. We have overridden this method in ViewFSOverloadScheme class. In ViewFSOverloadScheme, fsGetter gets its special FsGetter anonymous subclass instance with its own implemented methods. Does this answers your question?", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418742960", "createdAt": "2020-05-01T21:20:17Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java", "diffHunk": "@@ -96,16 +96,49 @@ static AccessControlException readOnlyMountTable(final String operation,\n     return readOnlyMountTable(operation, p.toString());\n   }\n \n+  /**\n+   * File system instance getter.\n+   */\n+  static class FsGetter {\n+\n+    /**\n+     * Gets new file system instance of given uri.\n+     */\n+    public FileSystem getNewInstance(URI uri, Configuration conf)\n+        throws IOException {\n+      return FileSystem.newInstance(uri, conf);\n+    }\n+\n+    /**\n+     * Gets file system instance of given uri.\n+     */\n+    public FileSystem get(URI uri, Configuration conf) throws IOException {\n+      return FileSystem.get(uri, conf);\n+    }\n+  }\n+\n+  /**\n+   * Gets file system creator instance.\n+   */\n+  protected FsGetter fsGetter() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODczMjE1OA=="}, "originalCommit": {"oid": "8010048dc27959c10c7a982774b4312f4c64924c"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTY1OTE3OQ==", "bodyText": "I see, thanks!", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419659179", "createdAt": "2020-05-04T19:01:33Z", "author": {"login": "chliang71"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java", "diffHunk": "@@ -96,16 +96,49 @@ static AccessControlException readOnlyMountTable(final String operation,\n     return readOnlyMountTable(operation, p.toString());\n   }\n \n+  /**\n+   * File system instance getter.\n+   */\n+  static class FsGetter {\n+\n+    /**\n+     * Gets new file system instance of given uri.\n+     */\n+    public FileSystem getNewInstance(URI uri, Configuration conf)\n+        throws IOException {\n+      return FileSystem.newInstance(uri, conf);\n+    }\n+\n+    /**\n+     * Gets file system instance of given uri.\n+     */\n+    public FileSystem get(URI uri, Configuration conf) throws IOException {\n+      return FileSystem.get(uri, conf);\n+    }\n+  }\n+\n+  /**\n+   * Gets file system creator instance.\n+   */\n+  protected FsGetter fsGetter() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODczMjE1OA=="}, "originalCommit": {"oid": "8010048dc27959c10c7a982774b4312f4c64924c"}, "originalPosition": 50}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwNTUyMjE2OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQyMDo1MzoxMFrOGPVcpA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQyMToyMDozNVrOGPWDDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODczMzIyMA==", "bodyText": "a general comment is it would be good to have some logging in this class (DEBUG level)", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418733220", "createdAt": "2020-05-01T20:53:10Z", "author": {"login": "chliang71"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "diffHunk": "@@ -0,0 +1,170 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n+import java.net.URI;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+\n+/******************************************************************************\n+ * This class is extended from the ViewFileSystem for the overloaded scheme file\n+ * system. The objective here is to handle multiple mounted file systems\n+ * transparently. Mount link configurations and in-memory mount table\n+ * building behaviors are inherited from ViewFileSystem. Unlike ViewFileSystem\n+ * scheme (viewfs://), the users would be able to use any scheme.\n+ *\n+ * Example 1:\n+ * If users want some of their existing cluster (hdfs://Cluster)\n+ * data to mount with other hdfs and object store clusters(hdfs://NN1,\n+ * o3fs://bucket1.volume1/, s3a://bucket1/)\n+ *\n+ * fs.viewfs.mounttable.Cluster./user = hdfs://NN1/user\n+ * fs.viewfs.mounttable.Cluster./data = o3fs://bucket1.volume1/data\n+ * fs.viewfs.mounttable.Cluster./backup = s3a://bucket1/backup/\n+ *\n+ * Op1: Create file hdfs://Cluster/user/fileA will go to hdfs://NN1/user/fileA\n+ * Op2: Create file hdfs://Cluster/data/datafile will go to\n+ *      o3fs://bucket1.volume1/data/datafile\n+ * Op3: Create file hdfs://Cluster/backup/data.zip will go to\n+ *      s3a://bucket1/backup/data.zip\n+ *\n+ * Example 2:\n+ * If users want some of their existing cluster (s3a://bucketA/)\n+ * data to mount with other hdfs and object store clusters\n+ * (hdfs://NN1, o3fs://bucket1.volume1/)\n+ *\n+ * fs.viewfs.mounttable.bucketA./user = hdfs://NN1/user\n+ * fs.viewfs.mounttable.bucketA./data = o3fs://bucket1.volume1/data\n+ * fs.viewfs.mounttable.bucketA./salesDB = s3a://bucketA/salesDB/\n+ *\n+ * Op1: Create file s3a://bucketA/user/fileA will go to hdfs://NN1/user/fileA\n+ * Op2: Create file s3a://bucketA/data/datafile will go to\n+ *      o3fs://bucket1.volume1/data/datafile\n+ * Op3: Create file s3a://bucketA/salesDB/dbfile will go to\n+ *      s3a://bucketA/salesDB/dbfile\n+ *****************************************************************************/\n+@InterfaceAudience.LimitedPrivate({ \"MapReduce\", \"HBase\", \"Hive\" })\n+@InterfaceStability.Evolving\n+public class ViewFsOverloadScheme extends ViewFileSystem {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8010048dc27959c10c7a982774b4312f4c64924c"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc0MzA1Mw==", "bodyText": "Sure I will add logging for this.", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418743053", "createdAt": "2020-05-01T21:20:35Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "diffHunk": "@@ -0,0 +1,170 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n+import java.net.URI;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+\n+/******************************************************************************\n+ * This class is extended from the ViewFileSystem for the overloaded scheme file\n+ * system. The objective here is to handle multiple mounted file systems\n+ * transparently. Mount link configurations and in-memory mount table\n+ * building behaviors are inherited from ViewFileSystem. Unlike ViewFileSystem\n+ * scheme (viewfs://), the users would be able to use any scheme.\n+ *\n+ * Example 1:\n+ * If users want some of their existing cluster (hdfs://Cluster)\n+ * data to mount with other hdfs and object store clusters(hdfs://NN1,\n+ * o3fs://bucket1.volume1/, s3a://bucket1/)\n+ *\n+ * fs.viewfs.mounttable.Cluster./user = hdfs://NN1/user\n+ * fs.viewfs.mounttable.Cluster./data = o3fs://bucket1.volume1/data\n+ * fs.viewfs.mounttable.Cluster./backup = s3a://bucket1/backup/\n+ *\n+ * Op1: Create file hdfs://Cluster/user/fileA will go to hdfs://NN1/user/fileA\n+ * Op2: Create file hdfs://Cluster/data/datafile will go to\n+ *      o3fs://bucket1.volume1/data/datafile\n+ * Op3: Create file hdfs://Cluster/backup/data.zip will go to\n+ *      s3a://bucket1/backup/data.zip\n+ *\n+ * Example 2:\n+ * If users want some of their existing cluster (s3a://bucketA/)\n+ * data to mount with other hdfs and object store clusters\n+ * (hdfs://NN1, o3fs://bucket1.volume1/)\n+ *\n+ * fs.viewfs.mounttable.bucketA./user = hdfs://NN1/user\n+ * fs.viewfs.mounttable.bucketA./data = o3fs://bucket1.volume1/data\n+ * fs.viewfs.mounttable.bucketA./salesDB = s3a://bucketA/salesDB/\n+ *\n+ * Op1: Create file s3a://bucketA/user/fileA will go to hdfs://NN1/user/fileA\n+ * Op2: Create file s3a://bucketA/data/datafile will go to\n+ *      o3fs://bucket1.volume1/data/datafile\n+ * Op3: Create file s3a://bucketA/salesDB/dbfile will go to\n+ *      s3a://bucketA/salesDB/dbfile\n+ *****************************************************************************/\n+@InterfaceAudience.LimitedPrivate({ \"MapReduce\", \"HBase\", \"Hive\" })\n+@InterfaceStability.Evolving\n+public class ViewFsOverloadScheme extends ViewFileSystem {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODczMzIyMA=="}, "originalCommit": {"oid": "8010048dc27959c10c7a982774b4312f4c64924c"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwNTU0NTY4OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeLocalFileSystem.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQyMTowMzo1NFrOGPVr0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQyMToyMTo1NVrOGPWE6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODczNzEwNQ==", "bodyText": "shouldn't this message be testLocalFsLinkSlashMerge? similar for the other test below", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418737105", "createdAt": "2020-05-01T21:03:54Z", "author": {"login": "chliang71"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeLocalFileSystem.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+/**\n+ *\n+ * Test the TestViewFsOverloadSchemeLocalFS using a file with authority:\n+ * file://mountTableName/ i.e, the authority is used to load a mount table.\n+ */\n+public class TestViewFsOverloadSchemeLocalFileSystem {\n+  private static final String FILE = \"file\";\n+  private static final Log LOG =\n+      LogFactory.getLog(TestViewFsOverloadSchemeLocalFileSystem.class);\n+  private FileSystem fsTarget;\n+  private Configuration conf;\n+  private Path targetTestRoot;\n+  private FileSystemTestHelper fileSystemTestHelper;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    conf.set(String.format(\"fs.%s.impl\",\n+        FILE),\n+        ViewFsOverloadScheme.class.getName());\n+    conf.set(String.format(\n+        FsConstants.FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN,\n+        FILE),\n+        LocalFileSystem.class.getName());\n+    fsTarget = new LocalFileSystem();\n+    fsTarget.initialize(new URI(\"file:///\"), conf);\n+    fileSystemTestHelper = new FileSystemTestHelper();\n+    // create the test root on local_fs\n+    targetTestRoot = fileSystemTestHelper.getAbsoluteTestRootPath(fsTarget);\n+    fsTarget.delete(targetTestRoot, true);\n+    fsTarget.mkdirs(targetTestRoot);\n+  }\n+\n+  /**\n+   * Tests write file and read file with ViewFSOverloadScheme.\n+   */\n+  @Test\n+  public void testLocalTargetLinkWriteSimple() throws IOException {\n+    LOG.info(\"Starting testLocalTargetLinkWriteSimple\");\n+    final String testString = \"Hello Local!...\";\n+    final Path lfsRoot = new Path(\"/lfsRoot\");\n+    ConfigUtil.addLink(conf, lfsRoot.toString(),\n+        URI.create(targetTestRoot + \"/local\"));\n+    final FileSystem lViewFs = FileSystem.get(URI.create(\"file:///\"), conf);\n+\n+    final Path testPath = new Path(lfsRoot, \"test.txt\");\n+    final FSDataOutputStream fsDos = lViewFs.create(testPath);\n+    try {\n+      fsDos.writeUTF(testString);\n+    } finally {\n+      fsDos.close();\n+    }\n+\n+    FSDataInputStream lViewIs = lViewFs.open(testPath);\n+    try {\n+      Assert.assertEquals(testString, lViewIs.readUTF());\n+    } finally {\n+      lViewIs.close();\n+    }\n+  }\n+\n+  /**\n+   * Tests create file and delete file with ViewFSOverloadScheme.\n+   */\n+  @Test\n+  public void testLocalFsCreateAndDelete() throws Exception {\n+    LOG.info(\"Starting testLocalFsCreateAndDelete\");\n+    ConfigUtil.addLink(conf, \"mt\", \"/lfsroot\",\n+        URI.create(targetTestRoot + \"/wd2\"));\n+    final URI mountURI = URI.create(\"file://mt/\");\n+    final FileSystem lViewFS = FileSystem.get(mountURI, conf);\n+    try {\n+      Path testPath = new Path(mountURI.toString() + \"/lfsroot/test\");\n+      lViewFS.create(testPath);\n+      Assert.assertTrue(lViewFS.exists(testPath));\n+      lViewFS.delete(testPath, true);\n+      Assert.assertFalse(lViewFS.exists(testPath));\n+    } finally {\n+      lViewFS.close();\n+    }\n+  }\n+\n+  /**\n+   * Tests root level file with linkMergeSlash with ViewFSOverloadScheme.\n+   */\n+  @Test\n+  public void testLocalFsLinkSlashMerge() throws Exception {\n+    LOG.info(\"Starting testLocalFSCreateAndDelete\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8010048dc27959c10c7a982774b4312f4c64924c"}, "originalPosition": 125}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc0MzUzMA==", "bodyText": "thanks. I will change them in next patch.", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r418743530", "createdAt": "2020-05-01T21:21:55Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeLocalFileSystem.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+/**\n+ *\n+ * Test the TestViewFsOverloadSchemeLocalFS using a file with authority:\n+ * file://mountTableName/ i.e, the authority is used to load a mount table.\n+ */\n+public class TestViewFsOverloadSchemeLocalFileSystem {\n+  private static final String FILE = \"file\";\n+  private static final Log LOG =\n+      LogFactory.getLog(TestViewFsOverloadSchemeLocalFileSystem.class);\n+  private FileSystem fsTarget;\n+  private Configuration conf;\n+  private Path targetTestRoot;\n+  private FileSystemTestHelper fileSystemTestHelper;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    conf.set(String.format(\"fs.%s.impl\",\n+        FILE),\n+        ViewFsOverloadScheme.class.getName());\n+    conf.set(String.format(\n+        FsConstants.FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN,\n+        FILE),\n+        LocalFileSystem.class.getName());\n+    fsTarget = new LocalFileSystem();\n+    fsTarget.initialize(new URI(\"file:///\"), conf);\n+    fileSystemTestHelper = new FileSystemTestHelper();\n+    // create the test root on local_fs\n+    targetTestRoot = fileSystemTestHelper.getAbsoluteTestRootPath(fsTarget);\n+    fsTarget.delete(targetTestRoot, true);\n+    fsTarget.mkdirs(targetTestRoot);\n+  }\n+\n+  /**\n+   * Tests write file and read file with ViewFSOverloadScheme.\n+   */\n+  @Test\n+  public void testLocalTargetLinkWriteSimple() throws IOException {\n+    LOG.info(\"Starting testLocalTargetLinkWriteSimple\");\n+    final String testString = \"Hello Local!...\";\n+    final Path lfsRoot = new Path(\"/lfsRoot\");\n+    ConfigUtil.addLink(conf, lfsRoot.toString(),\n+        URI.create(targetTestRoot + \"/local\"));\n+    final FileSystem lViewFs = FileSystem.get(URI.create(\"file:///\"), conf);\n+\n+    final Path testPath = new Path(lfsRoot, \"test.txt\");\n+    final FSDataOutputStream fsDos = lViewFs.create(testPath);\n+    try {\n+      fsDos.writeUTF(testString);\n+    } finally {\n+      fsDos.close();\n+    }\n+\n+    FSDataInputStream lViewIs = lViewFs.open(testPath);\n+    try {\n+      Assert.assertEquals(testString, lViewIs.readUTF());\n+    } finally {\n+      lViewIs.close();\n+    }\n+  }\n+\n+  /**\n+   * Tests create file and delete file with ViewFSOverloadScheme.\n+   */\n+  @Test\n+  public void testLocalFsCreateAndDelete() throws Exception {\n+    LOG.info(\"Starting testLocalFsCreateAndDelete\");\n+    ConfigUtil.addLink(conf, \"mt\", \"/lfsroot\",\n+        URI.create(targetTestRoot + \"/wd2\"));\n+    final URI mountURI = URI.create(\"file://mt/\");\n+    final FileSystem lViewFS = FileSystem.get(mountURI, conf);\n+    try {\n+      Path testPath = new Path(mountURI.toString() + \"/lfsroot/test\");\n+      lViewFS.create(testPath);\n+      Assert.assertTrue(lViewFS.exists(testPath));\n+      lViewFS.delete(testPath, true);\n+      Assert.assertFalse(lViewFS.exists(testPath));\n+    } finally {\n+      lViewFS.close();\n+    }\n+  }\n+\n+  /**\n+   * Tests root level file with linkMergeSlash with ViewFSOverloadScheme.\n+   */\n+  @Test\n+  public void testLocalFsLinkSlashMerge() throws Exception {\n+    LOG.info(\"Starting testLocalFSCreateAndDelete\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODczNzEwNQ=="}, "originalCommit": {"oid": "8010048dc27959c10c7a982774b4312f4c64924c"}, "originalPosition": 125}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwODA2NDQ4OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeLocalFileSystem.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QwMzo0Njo0NVrOGPoTTQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QwMzo0Njo0NVrOGPoTTQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0MjEyNQ==", "bodyText": "indentation of the conf.set lines is confusing. can you make this better?", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419042125", "createdAt": "2020-05-03T03:46:45Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeLocalFileSystem.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+/**\n+ *\n+ * Test the TestViewFsOverloadSchemeLocalFS using a file with authority:\n+ * file://mountTableName/ i.e, the authority is used to load a mount table.\n+ */\n+public class TestViewFsOverloadSchemeLocalFileSystem {\n+  private static final String FILE = \"file\";\n+  private static final Log LOG =\n+      LogFactory.getLog(TestViewFsOverloadSchemeLocalFileSystem.class);\n+  private FileSystem fsTarget;\n+  private Configuration conf;\n+  private Path targetTestRoot;\n+  private FileSystemTestHelper fileSystemTestHelper;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    conf.set(String.format(\"fs.%s.impl\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c1525f808f02ed40ca7451d4d71b7fa12708e106"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwODA2NjQ3OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeLocalFileSystem.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QwMzo0OToxN1rOGPoUMg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QwMzo0OToxN1rOGPoUMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0MjM1NA==", "bodyText": "try (FSDataOutputStream fsDos = lViewFs.create(testPath)) { }", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419042354", "createdAt": "2020-05-03T03:49:17Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeLocalFileSystem.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+/**\n+ *\n+ * Test the TestViewFsOverloadSchemeLocalFS using a file with authority:\n+ * file://mountTableName/ i.e, the authority is used to load a mount table.\n+ */\n+public class TestViewFsOverloadSchemeLocalFileSystem {\n+  private static final String FILE = \"file\";\n+  private static final Log LOG =\n+      LogFactory.getLog(TestViewFsOverloadSchemeLocalFileSystem.class);\n+  private FileSystem fsTarget;\n+  private Configuration conf;\n+  private Path targetTestRoot;\n+  private FileSystemTestHelper fileSystemTestHelper;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    conf.set(String.format(\"fs.%s.impl\",\n+        FILE),\n+        ViewFsOverloadScheme.class.getName());\n+    conf.set(String.format(\n+        FsConstants.FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN,\n+        FILE),\n+        LocalFileSystem.class.getName());\n+    fsTarget = new LocalFileSystem();\n+    fsTarget.initialize(new URI(\"file:///\"), conf);\n+    fileSystemTestHelper = new FileSystemTestHelper();\n+    // create the test root on local_fs\n+    targetTestRoot = fileSystemTestHelper.getAbsoluteTestRootPath(fsTarget);\n+    fsTarget.delete(targetTestRoot, true);\n+    fsTarget.mkdirs(targetTestRoot);\n+  }\n+\n+  /**\n+   * Tests write file and read file with ViewFSOverloadScheme.\n+   */\n+  @Test\n+  public void testLocalTargetLinkWriteSimple() throws IOException {\n+    LOG.info(\"Starting testLocalTargetLinkWriteSimple\");\n+    final String testString = \"Hello Local!...\";\n+    final Path lfsRoot = new Path(\"/lfsRoot\");\n+    ConfigUtil.addLink(conf, lfsRoot.toString(),\n+        URI.create(targetTestRoot + \"/local\"));\n+    final FileSystem lViewFs = FileSystem.get(URI.create(\"file:///\"), conf);\n+\n+    final Path testPath = new Path(lfsRoot, \"test.txt\");\n+    final FSDataOutputStream fsDos = lViewFs.create(testPath);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c1525f808f02ed40ca7451d4d71b7fa12708e106"}, "originalPosition": 84}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwODA2Njg3OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeLocalFileSystem.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QwMzo0OTo1NFrOGPoUXg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QwMzo0OTo1NFrOGPoUXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0MjM5OA==", "bodyText": "try(FSDataInputStream lViewIs = lViewFs.open(testPath)) {}", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419042398", "createdAt": "2020-05-03T03:49:54Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeLocalFileSystem.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+/**\n+ *\n+ * Test the TestViewFsOverloadSchemeLocalFS using a file with authority:\n+ * file://mountTableName/ i.e, the authority is used to load a mount table.\n+ */\n+public class TestViewFsOverloadSchemeLocalFileSystem {\n+  private static final String FILE = \"file\";\n+  private static final Log LOG =\n+      LogFactory.getLog(TestViewFsOverloadSchemeLocalFileSystem.class);\n+  private FileSystem fsTarget;\n+  private Configuration conf;\n+  private Path targetTestRoot;\n+  private FileSystemTestHelper fileSystemTestHelper;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    conf.set(String.format(\"fs.%s.impl\",\n+        FILE),\n+        ViewFsOverloadScheme.class.getName());\n+    conf.set(String.format(\n+        FsConstants.FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN,\n+        FILE),\n+        LocalFileSystem.class.getName());\n+    fsTarget = new LocalFileSystem();\n+    fsTarget.initialize(new URI(\"file:///\"), conf);\n+    fileSystemTestHelper = new FileSystemTestHelper();\n+    // create the test root on local_fs\n+    targetTestRoot = fileSystemTestHelper.getAbsoluteTestRootPath(fsTarget);\n+    fsTarget.delete(targetTestRoot, true);\n+    fsTarget.mkdirs(targetTestRoot);\n+  }\n+\n+  /**\n+   * Tests write file and read file with ViewFSOverloadScheme.\n+   */\n+  @Test\n+  public void testLocalTargetLinkWriteSimple() throws IOException {\n+    LOG.info(\"Starting testLocalTargetLinkWriteSimple\");\n+    final String testString = \"Hello Local!...\";\n+    final Path lfsRoot = new Path(\"/lfsRoot\");\n+    ConfigUtil.addLink(conf, lfsRoot.toString(),\n+        URI.create(targetTestRoot + \"/local\"));\n+    final FileSystem lViewFs = FileSystem.get(URI.create(\"file:///\"), conf);\n+\n+    final Path testPath = new Path(lfsRoot, \"test.txt\");\n+    final FSDataOutputStream fsDos = lViewFs.create(testPath);\n+    try {\n+      fsDos.writeUTF(testString);\n+    } finally {\n+      fsDos.close();\n+    }\n+\n+    FSDataInputStream lViewIs = lViewFs.open(testPath);\n+    try {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c1525f808f02ed40ca7451d4d71b7fa12708e106"}, "originalPosition": 92}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwODA2OTYzOnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeLocalFileSystem.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QwMzo1NDoxM1rOGPoVmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QwODoxOTo0OFrOGPp2cg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0MjcxNA==", "bodyText": "stream is not closed. may be use createNewFile?", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419042714", "createdAt": "2020-05-03T03:54:13Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeLocalFileSystem.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+/**\n+ *\n+ * Test the TestViewFsOverloadSchemeLocalFS using a file with authority:\n+ * file://mountTableName/ i.e, the authority is used to load a mount table.\n+ */\n+public class TestViewFsOverloadSchemeLocalFileSystem {\n+  private static final String FILE = \"file\";\n+  private static final Log LOG =\n+      LogFactory.getLog(TestViewFsOverloadSchemeLocalFileSystem.class);\n+  private FileSystem fsTarget;\n+  private Configuration conf;\n+  private Path targetTestRoot;\n+  private FileSystemTestHelper fileSystemTestHelper;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    conf.set(String.format(\"fs.%s.impl\",\n+        FILE),\n+        ViewFsOverloadScheme.class.getName());\n+    conf.set(String.format(\n+        FsConstants.FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN,\n+        FILE),\n+        LocalFileSystem.class.getName());\n+    fsTarget = new LocalFileSystem();\n+    fsTarget.initialize(new URI(\"file:///\"), conf);\n+    fileSystemTestHelper = new FileSystemTestHelper();\n+    // create the test root on local_fs\n+    targetTestRoot = fileSystemTestHelper.getAbsoluteTestRootPath(fsTarget);\n+    fsTarget.delete(targetTestRoot, true);\n+    fsTarget.mkdirs(targetTestRoot);\n+  }\n+\n+  /**\n+   * Tests write file and read file with ViewFSOverloadScheme.\n+   */\n+  @Test\n+  public void testLocalTargetLinkWriteSimple() throws IOException {\n+    LOG.info(\"Starting testLocalTargetLinkWriteSimple\");\n+    final String testString = \"Hello Local!...\";\n+    final Path lfsRoot = new Path(\"/lfsRoot\");\n+    ConfigUtil.addLink(conf, lfsRoot.toString(),\n+        URI.create(targetTestRoot + \"/local\"));\n+    final FileSystem lViewFs = FileSystem.get(URI.create(\"file:///\"), conf);\n+\n+    final Path testPath = new Path(lfsRoot, \"test.txt\");\n+    final FSDataOutputStream fsDos = lViewFs.create(testPath);\n+    try {\n+      fsDos.writeUTF(testString);\n+    } finally {\n+      fsDos.close();\n+    }\n+\n+    FSDataInputStream lViewIs = lViewFs.open(testPath);\n+    try {\n+      Assert.assertEquals(testString, lViewIs.readUTF());\n+    } finally {\n+      lViewIs.close();\n+    }\n+  }\n+\n+  /**\n+   * Tests create file and delete file with ViewFSOverloadScheme.\n+   */\n+  @Test\n+  public void testLocalFsCreateAndDelete() throws Exception {\n+    LOG.info(\"Starting testLocalFsCreateAndDelete\");\n+    ConfigUtil.addLink(conf, \"mt\", \"/lfsroot\",\n+        URI.create(targetTestRoot + \"/wd2\"));\n+    final URI mountURI = URI.create(\"file://mt/\");\n+    final FileSystem lViewFS = FileSystem.get(mountURI, conf);\n+    try {\n+      Path testPath = new Path(mountURI.toString() + \"/lfsroot/test\");\n+      lViewFS.create(testPath);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c1525f808f02ed40ca7451d4d71b7fa12708e106"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA2NzUwNg==", "bodyText": "Let me use this other method which closes itself. I dont need stream here.", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419067506", "createdAt": "2020-05-03T08:19:48Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeLocalFileSystem.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+/**\n+ *\n+ * Test the TestViewFsOverloadSchemeLocalFS using a file with authority:\n+ * file://mountTableName/ i.e, the authority is used to load a mount table.\n+ */\n+public class TestViewFsOverloadSchemeLocalFileSystem {\n+  private static final String FILE = \"file\";\n+  private static final Log LOG =\n+      LogFactory.getLog(TestViewFsOverloadSchemeLocalFileSystem.class);\n+  private FileSystem fsTarget;\n+  private Configuration conf;\n+  private Path targetTestRoot;\n+  private FileSystemTestHelper fileSystemTestHelper;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    conf.set(String.format(\"fs.%s.impl\",\n+        FILE),\n+        ViewFsOverloadScheme.class.getName());\n+    conf.set(String.format(\n+        FsConstants.FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN,\n+        FILE),\n+        LocalFileSystem.class.getName());\n+    fsTarget = new LocalFileSystem();\n+    fsTarget.initialize(new URI(\"file:///\"), conf);\n+    fileSystemTestHelper = new FileSystemTestHelper();\n+    // create the test root on local_fs\n+    targetTestRoot = fileSystemTestHelper.getAbsoluteTestRootPath(fsTarget);\n+    fsTarget.delete(targetTestRoot, true);\n+    fsTarget.mkdirs(targetTestRoot);\n+  }\n+\n+  /**\n+   * Tests write file and read file with ViewFSOverloadScheme.\n+   */\n+  @Test\n+  public void testLocalTargetLinkWriteSimple() throws IOException {\n+    LOG.info(\"Starting testLocalTargetLinkWriteSimple\");\n+    final String testString = \"Hello Local!...\";\n+    final Path lfsRoot = new Path(\"/lfsRoot\");\n+    ConfigUtil.addLink(conf, lfsRoot.toString(),\n+        URI.create(targetTestRoot + \"/local\"));\n+    final FileSystem lViewFs = FileSystem.get(URI.create(\"file:///\"), conf);\n+\n+    final Path testPath = new Path(lfsRoot, \"test.txt\");\n+    final FSDataOutputStream fsDos = lViewFs.create(testPath);\n+    try {\n+      fsDos.writeUTF(testString);\n+    } finally {\n+      fsDos.close();\n+    }\n+\n+    FSDataInputStream lViewIs = lViewFs.open(testPath);\n+    try {\n+      Assert.assertEquals(testString, lViewIs.readUTF());\n+    } finally {\n+      lViewIs.close();\n+    }\n+  }\n+\n+  /**\n+   * Tests create file and delete file with ViewFSOverloadScheme.\n+   */\n+  @Test\n+  public void testLocalFsCreateAndDelete() throws Exception {\n+    LOG.info(\"Starting testLocalFsCreateAndDelete\");\n+    ConfigUtil.addLink(conf, \"mt\", \"/lfsroot\",\n+        URI.create(targetTestRoot + \"/wd2\"));\n+    final URI mountURI = URI.create(\"file://mt/\");\n+    final FileSystem lViewFS = FileSystem.get(mountURI, conf);\n+    try {\n+      Path testPath = new Path(mountURI.toString() + \"/lfsroot/test\");\n+      lViewFS.create(testPath);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0MjcxNA=="}, "originalCommit": {"oid": "c1525f808f02ed40ca7451d4d71b7fa12708e106"}, "originalPosition": 111}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwODA3MDEzOnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeLocalFileSystem.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QwMzo1NDo0OFrOGPoV0g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QwMzo1NDo0OFrOGPoV0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0Mjc3MA==", "bodyText": "same comment here -- use createNewFile?", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419042770", "createdAt": "2020-05-03T03:54:48Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeLocalFileSystem.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+/**\n+ *\n+ * Test the TestViewFsOverloadSchemeLocalFS using a file with authority:\n+ * file://mountTableName/ i.e, the authority is used to load a mount table.\n+ */\n+public class TestViewFsOverloadSchemeLocalFileSystem {\n+  private static final String FILE = \"file\";\n+  private static final Log LOG =\n+      LogFactory.getLog(TestViewFsOverloadSchemeLocalFileSystem.class);\n+  private FileSystem fsTarget;\n+  private Configuration conf;\n+  private Path targetTestRoot;\n+  private FileSystemTestHelper fileSystemTestHelper;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    conf.set(String.format(\"fs.%s.impl\",\n+        FILE),\n+        ViewFsOverloadScheme.class.getName());\n+    conf.set(String.format(\n+        FsConstants.FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN,\n+        FILE),\n+        LocalFileSystem.class.getName());\n+    fsTarget = new LocalFileSystem();\n+    fsTarget.initialize(new URI(\"file:///\"), conf);\n+    fileSystemTestHelper = new FileSystemTestHelper();\n+    // create the test root on local_fs\n+    targetTestRoot = fileSystemTestHelper.getAbsoluteTestRootPath(fsTarget);\n+    fsTarget.delete(targetTestRoot, true);\n+    fsTarget.mkdirs(targetTestRoot);\n+  }\n+\n+  /**\n+   * Tests write file and read file with ViewFSOverloadScheme.\n+   */\n+  @Test\n+  public void testLocalTargetLinkWriteSimple() throws IOException {\n+    LOG.info(\"Starting testLocalTargetLinkWriteSimple\");\n+    final String testString = \"Hello Local!...\";\n+    final Path lfsRoot = new Path(\"/lfsRoot\");\n+    ConfigUtil.addLink(conf, lfsRoot.toString(),\n+        URI.create(targetTestRoot + \"/local\"));\n+    final FileSystem lViewFs = FileSystem.get(URI.create(\"file:///\"), conf);\n+\n+    final Path testPath = new Path(lfsRoot, \"test.txt\");\n+    final FSDataOutputStream fsDos = lViewFs.create(testPath);\n+    try {\n+      fsDos.writeUTF(testString);\n+    } finally {\n+      fsDos.close();\n+    }\n+\n+    FSDataInputStream lViewIs = lViewFs.open(testPath);\n+    try {\n+      Assert.assertEquals(testString, lViewIs.readUTF());\n+    } finally {\n+      lViewIs.close();\n+    }\n+  }\n+\n+  /**\n+   * Tests create file and delete file with ViewFSOverloadScheme.\n+   */\n+  @Test\n+  public void testLocalFsCreateAndDelete() throws Exception {\n+    LOG.info(\"Starting testLocalFsCreateAndDelete\");\n+    ConfigUtil.addLink(conf, \"mt\", \"/lfsroot\",\n+        URI.create(targetTestRoot + \"/wd2\"));\n+    final URI mountURI = URI.create(\"file://mt/\");\n+    final FileSystem lViewFS = FileSystem.get(mountURI, conf);\n+    try {\n+      Path testPath = new Path(mountURI.toString() + \"/lfsroot/test\");\n+      lViewFS.create(testPath);\n+      Assert.assertTrue(lViewFS.exists(testPath));\n+      lViewFS.delete(testPath, true);\n+      Assert.assertFalse(lViewFS.exists(testPath));\n+    } finally {\n+      lViewFS.close();\n+    }\n+  }\n+\n+  /**\n+   * Tests root level file with linkMergeSlash with ViewFSOverloadScheme.\n+   */\n+  @Test\n+  public void testLocalFsLinkSlashMerge() throws Exception {\n+    LOG.info(\"Starting testLocalFsLinkSlashMerge\");\n+    ConfigUtil.addLinkMergeSlash(conf, \"mt\",\n+        URI.create(targetTestRoot + \"/wd2\"));\n+    final URI mountURI = URI.create(\"file://mt/\");\n+    final FileSystem lViewFS = FileSystem.get(mountURI, conf);\n+    try {\n+      Path fileOnRoot = new Path(mountURI.toString() + \"/NewFile\");\n+      lViewFS.create(fileOnRoot);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c1525f808f02ed40ca7451d4d71b7fa12708e106"}, "originalPosition": 132}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwODA3MzE1OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeLocalFileSystem.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QwMzo1OTowOFrOGPoXHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QwMzo1OTowOFrOGPoXHg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0MzEwMg==", "bodyText": "fsTarget.close() as well?", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419043102", "createdAt": "2020-05-03T03:59:08Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeLocalFileSystem.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+/**\n+ *\n+ * Test the TestViewFsOverloadSchemeLocalFS using a file with authority:\n+ * file://mountTableName/ i.e, the authority is used to load a mount table.\n+ */\n+public class TestViewFsOverloadSchemeLocalFileSystem {\n+  private static final String FILE = \"file\";\n+  private static final Log LOG =\n+      LogFactory.getLog(TestViewFsOverloadSchemeLocalFileSystem.class);\n+  private FileSystem fsTarget;\n+  private Configuration conf;\n+  private Path targetTestRoot;\n+  private FileSystemTestHelper fileSystemTestHelper;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    conf.set(String.format(\"fs.%s.impl\",\n+        FILE),\n+        ViewFsOverloadScheme.class.getName());\n+    conf.set(String.format(\n+        FsConstants.FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN,\n+        FILE),\n+        LocalFileSystem.class.getName());\n+    fsTarget = new LocalFileSystem();\n+    fsTarget.initialize(new URI(\"file:///\"), conf);\n+    fileSystemTestHelper = new FileSystemTestHelper();\n+    // create the test root on local_fs\n+    targetTestRoot = fileSystemTestHelper.getAbsoluteTestRootPath(fsTarget);\n+    fsTarget.delete(targetTestRoot, true);\n+    fsTarget.mkdirs(targetTestRoot);\n+  }\n+\n+  /**\n+   * Tests write file and read file with ViewFSOverloadScheme.\n+   */\n+  @Test\n+  public void testLocalTargetLinkWriteSimple() throws IOException {\n+    LOG.info(\"Starting testLocalTargetLinkWriteSimple\");\n+    final String testString = \"Hello Local!...\";\n+    final Path lfsRoot = new Path(\"/lfsRoot\");\n+    ConfigUtil.addLink(conf, lfsRoot.toString(),\n+        URI.create(targetTestRoot + \"/local\"));\n+    final FileSystem lViewFs = FileSystem.get(URI.create(\"file:///\"), conf);\n+\n+    final Path testPath = new Path(lfsRoot, \"test.txt\");\n+    final FSDataOutputStream fsDos = lViewFs.create(testPath);\n+    try {\n+      fsDos.writeUTF(testString);\n+    } finally {\n+      fsDos.close();\n+    }\n+\n+    FSDataInputStream lViewIs = lViewFs.open(testPath);\n+    try {\n+      Assert.assertEquals(testString, lViewIs.readUTF());\n+    } finally {\n+      lViewIs.close();\n+    }\n+  }\n+\n+  /**\n+   * Tests create file and delete file with ViewFSOverloadScheme.\n+   */\n+  @Test\n+  public void testLocalFsCreateAndDelete() throws Exception {\n+    LOG.info(\"Starting testLocalFsCreateAndDelete\");\n+    ConfigUtil.addLink(conf, \"mt\", \"/lfsroot\",\n+        URI.create(targetTestRoot + \"/wd2\"));\n+    final URI mountURI = URI.create(\"file://mt/\");\n+    final FileSystem lViewFS = FileSystem.get(mountURI, conf);\n+    try {\n+      Path testPath = new Path(mountURI.toString() + \"/lfsroot/test\");\n+      lViewFS.create(testPath);\n+      Assert.assertTrue(lViewFS.exists(testPath));\n+      lViewFS.delete(testPath, true);\n+      Assert.assertFalse(lViewFS.exists(testPath));\n+    } finally {\n+      lViewFS.close();\n+    }\n+  }\n+\n+  /**\n+   * Tests root level file with linkMergeSlash with ViewFSOverloadScheme.\n+   */\n+  @Test\n+  public void testLocalFsLinkSlashMerge() throws Exception {\n+    LOG.info(\"Starting testLocalFsLinkSlashMerge\");\n+    ConfigUtil.addLinkMergeSlash(conf, \"mt\",\n+        URI.create(targetTestRoot + \"/wd2\"));\n+    final URI mountURI = URI.create(\"file://mt/\");\n+    final FileSystem lViewFS = FileSystem.get(mountURI, conf);\n+    try {\n+      Path fileOnRoot = new Path(mountURI.toString() + \"/NewFile\");\n+      lViewFS.create(fileOnRoot);\n+      Assert.assertTrue(lViewFS.exists(fileOnRoot));\n+    } finally {\n+      lViewFS.close();\n+    }\n+  }\n+\n+  /**\n+   * Tests with linkMergeSlash and other mounts in ViewFSOverloadScheme.\n+   */\n+  @Test(expected = IOException.class)\n+  public void testLocalFsLinkSlashMergeWithOtherMountLinks() throws Exception {\n+    LOG.info(\"Starting testLocalFsLinkSlashMergeWithOtherMountLinks\");\n+    ConfigUtil.addLink(conf, \"mt\", \"/lfsroot\",\n+        URI.create(targetTestRoot + \"/wd2\"));\n+    ConfigUtil.addLinkMergeSlash(conf, \"mt\",\n+        URI.create(targetTestRoot + \"/wd2\"));\n+    final URI mountURI = URI.create(\"file://mt/\");\n+    FileSystem.get(mountURI, conf);\n+    Assert.fail(\"A merge slash cannot be configured with other mount links.\");\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    fsTarget.delete(fileSystemTestHelper.getTestRootPath(fsTarget), true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c1525f808f02ed40ca7451d4d71b7fa12708e106"}, "originalPosition": 156}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwODA3ODA3OnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeHdfsFileSystemContract.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QwNDowNjoxMVrOGPoZSw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QwNDowNjoxMVrOGPoZSw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0MzY1OQ==", "bodyText": "cast not required.", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419043659", "createdAt": "2020-05-03T04:06:11Z", "author": {"login": "virajith"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeHdfsFileSystemContract.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import static org.junit.Assume.assumeTrue;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.CommonConfigurationKeys;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemContractBaseTest;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.AppendTestUtil;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.HdfsConfiguration;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.TestHDFSFileSystemContract;\n+import org.apache.hadoop.security.AccessControlException;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Ignore;\n+import org.junit.Test;\n+\n+/**\n+ * Tests ViewFsOverloadScheme with file system contract tests.\n+ */\n+public class TestViewFsOverloadSchemeHdfsFileSystemContract\n+    extends TestHDFSFileSystemContract {\n+\n+  private MiniDFSCluster cluster;\n+  private String defaultWorkingDirectory;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    final Configuration conf = new HdfsConfiguration();\n+    conf.set(CommonConfigurationKeys.FS_PERMISSIONS_UMASK_KEY,\n+        FileSystemContractBaseTest.TEST_UMASK);\n+    final File basedir = GenericTestUtils.getRandomizedTestDir();\n+    cluster = new MiniDFSCluster.Builder(conf, basedir)\n+        .numDataNodes(2)\n+        .build();\n+    defaultWorkingDirectory =\n+        \"/user/\" + UserGroupInformation.getCurrentUser().getShortUserName();\n+    conf.set(String.format(\"fs.%s.impl\", \"hdfs\"),\n+        ViewFsOverloadScheme.class.getName());\n+    conf.set(String.format(\n+        FsConstants.FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN,\n+        \"hdfs\"),\n+        DistributedFileSystem.class.getName());\n+    URI defaultFSURI =\n+        URI.create(conf.get(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY));\n+    ConfigUtil.addLink(conf, defaultFSURI.getAuthority(), \"/user\",\n+        defaultFSURI);\n+    ConfigUtil.addLink(conf, defaultFSURI.getAuthority(), \"/append\",\n+        defaultFSURI);\n+    ConfigUtil.addLink(conf, defaultFSURI.getAuthority(),\n+        \"/FileSystemContractBaseTest/\",\n+        new URI(defaultFSURI.toString() + \"/FileSystemContractBaseTest/\"));\n+    fs = (ViewFsOverloadScheme) FileSystem.get(conf);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c1525f808f02ed40ca7451d4d71b7fa12708e106"}, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwODA4MDM1OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeLocalFileSystem.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QwNDowOTowMVrOGPoaRA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QwODoyMTozOFrOGPp3Vw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0MzkwOA==", "bodyText": "Except for these two, can the rest be done @BeforeClass?", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419043908", "createdAt": "2020-05-03T04:09:01Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeLocalFileSystem.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+/**\n+ *\n+ * Test the TestViewFsOverloadSchemeLocalFS using a file with authority:\n+ * file://mountTableName/ i.e, the authority is used to load a mount table.\n+ */\n+public class TestViewFsOverloadSchemeLocalFileSystem {\n+  private static final String FILE = \"file\";\n+  private static final Log LOG =\n+      LogFactory.getLog(TestViewFsOverloadSchemeLocalFileSystem.class);\n+  private FileSystem fsTarget;\n+  private Configuration conf;\n+  private Path targetTestRoot;\n+  private FileSystemTestHelper fileSystemTestHelper;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    conf.set(String.format(\"fs.%s.impl\",\n+        FILE),\n+        ViewFsOverloadScheme.class.getName());\n+    conf.set(String.format(\n+        FsConstants.FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN,\n+        FILE),\n+        LocalFileSystem.class.getName());\n+    fsTarget = new LocalFileSystem();\n+    fsTarget.initialize(new URI(\"file:///\"), conf);\n+    fileSystemTestHelper = new FileSystemTestHelper();\n+    // create the test root on local_fs\n+    targetTestRoot = fileSystemTestHelper.getAbsoluteTestRootPath(fsTarget);\n+    fsTarget.delete(targetTestRoot, true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c1525f808f02ed40ca7451d4d71b7fa12708e106"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA2NzczNQ==", "bodyText": "Some tests has slashmerge config which will not work if there are other mount links in config. I think its ok to keep config inits in @before ( local fs is not heavy)", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419067735", "createdAt": "2020-05-03T08:21:38Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeLocalFileSystem.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+/**\n+ *\n+ * Test the TestViewFsOverloadSchemeLocalFS using a file with authority:\n+ * file://mountTableName/ i.e, the authority is used to load a mount table.\n+ */\n+public class TestViewFsOverloadSchemeLocalFileSystem {\n+  private static final String FILE = \"file\";\n+  private static final Log LOG =\n+      LogFactory.getLog(TestViewFsOverloadSchemeLocalFileSystem.class);\n+  private FileSystem fsTarget;\n+  private Configuration conf;\n+  private Path targetTestRoot;\n+  private FileSystemTestHelper fileSystemTestHelper;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    conf.set(String.format(\"fs.%s.impl\",\n+        FILE),\n+        ViewFsOverloadScheme.class.getName());\n+    conf.set(String.format(\n+        FsConstants.FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN,\n+        FILE),\n+        LocalFileSystem.class.getName());\n+    fsTarget = new LocalFileSystem();\n+    fsTarget.initialize(new URI(\"file:///\"), conf);\n+    fileSystemTestHelper = new FileSystemTestHelper();\n+    // create the test root on local_fs\n+    targetTestRoot = fileSystemTestHelper.getAbsoluteTestRootPath(fsTarget);\n+    fsTarget.delete(targetTestRoot, true);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0MzkwOA=="}, "originalCommit": {"oid": "c1525f808f02ed40ca7451d4d71b7fa12708e106"}, "originalPosition": 67}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwODA4MTUxOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeHdfsFileSystemContract.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QwNDoxMTowNVrOGPoaxQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNFQwMDoyNjowMFrOGPws6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0NDAzNw==", "bodyText": "do we need the cluster created for every test? seems heavy :|\nAlso, can we set FS_DEFAULT_NAME_KEY explicitly here?", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419044037", "createdAt": "2020-05-03T04:11:05Z", "author": {"login": "virajith"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeHdfsFileSystemContract.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import static org.junit.Assume.assumeTrue;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.CommonConfigurationKeys;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemContractBaseTest;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.AppendTestUtil;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.HdfsConfiguration;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.TestHDFSFileSystemContract;\n+import org.apache.hadoop.security.AccessControlException;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Ignore;\n+import org.junit.Test;\n+\n+/**\n+ * Tests ViewFsOverloadScheme with file system contract tests.\n+ */\n+public class TestViewFsOverloadSchemeHdfsFileSystemContract\n+    extends TestHDFSFileSystemContract {\n+\n+  private MiniDFSCluster cluster;\n+  private String defaultWorkingDirectory;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    final Configuration conf = new HdfsConfiguration();\n+    conf.set(CommonConfigurationKeys.FS_PERMISSIONS_UMASK_KEY,\n+        FileSystemContractBaseTest.TEST_UMASK);\n+    final File basedir = GenericTestUtils.getRandomizedTestDir();\n+    cluster = new MiniDFSCluster.Builder(conf, basedir)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c1525f808f02ed40ca7451d4d71b7fa12708e106"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA2OTc1MQ==", "bodyText": "Let me check that if I can start cluster once and use. We reused existing HDFSContract test.\nIf existing contract tests passes with one time cluster start, I should be good to do that. I will try that.\nWhy do we need to set  FS_DEFAULT_NAME_KEY explicitly? Minicluster will do that for is we use that same config.", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419069751", "createdAt": "2020-05-03T08:40:25Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeHdfsFileSystemContract.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import static org.junit.Assume.assumeTrue;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.CommonConfigurationKeys;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemContractBaseTest;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.AppendTestUtil;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.HdfsConfiguration;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.TestHDFSFileSystemContract;\n+import org.apache.hadoop.security.AccessControlException;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Ignore;\n+import org.junit.Test;\n+\n+/**\n+ * Tests ViewFsOverloadScheme with file system contract tests.\n+ */\n+public class TestViewFsOverloadSchemeHdfsFileSystemContract\n+    extends TestHDFSFileSystemContract {\n+\n+  private MiniDFSCluster cluster;\n+  private String defaultWorkingDirectory;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    final Configuration conf = new HdfsConfiguration();\n+    conf.set(CommonConfigurationKeys.FS_PERMISSIONS_UMASK_KEY,\n+        FileSystemContractBaseTest.TEST_UMASK);\n+    final File basedir = GenericTestUtils.getRandomizedTestDir();\n+    cluster = new MiniDFSCluster.Builder(conf, basedir)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0NDAzNw=="}, "originalCommit": {"oid": "c1525f808f02ed40ca7451d4d71b7fa12708e106"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTE3OTc1Mg==", "bodyText": "I tried to moved cluster init into Before class. Contract tests are passing. Please check.", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419179752", "createdAt": "2020-05-04T00:26:00Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeHdfsFileSystemContract.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import static org.junit.Assume.assumeTrue;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.CommonConfigurationKeys;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemContractBaseTest;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.AppendTestUtil;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.HdfsConfiguration;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.TestHDFSFileSystemContract;\n+import org.apache.hadoop.security.AccessControlException;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Ignore;\n+import org.junit.Test;\n+\n+/**\n+ * Tests ViewFsOverloadScheme with file system contract tests.\n+ */\n+public class TestViewFsOverloadSchemeHdfsFileSystemContract\n+    extends TestHDFSFileSystemContract {\n+\n+  private MiniDFSCluster cluster;\n+  private String defaultWorkingDirectory;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    final Configuration conf = new HdfsConfiguration();\n+    conf.set(CommonConfigurationKeys.FS_PERMISSIONS_UMASK_KEY,\n+        FileSystemContractBaseTest.TEST_UMASK);\n+    final File basedir = GenericTestUtils.getRandomizedTestDir();\n+    cluster = new MiniDFSCluster.Builder(conf, basedir)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0NDAzNw=="}, "originalCommit": {"oid": "c1525f808f02ed40ca7451d4d71b7fa12708e106"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwODA4MzY5OnYy", "diffSide": "RIGHT", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QwNDoxNDoxNVrOGPobuw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QwNjo1NjoyNFrOGPpTFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0NDI4Mw==", "bodyText": "I think ViewOverloadSchemeFilesystem is a better name. Also do we need an implementation extending ViewFs class (which extends AbstractFileSystem)?", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419044283", "createdAt": "2020-05-03T04:14:15Z", "author": {"login": "virajith"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "diffHunk": "@@ -0,0 +1,187 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n+import java.net.URI;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+\n+/******************************************************************************\n+ * This class is extended from the ViewFileSystem for the overloaded scheme file\n+ * system. The objective here is to handle multiple mounted file systems\n+ * transparently. Mount link configurations and in-memory mount table\n+ * building behaviors are inherited from ViewFileSystem. Unlike ViewFileSystem\n+ * scheme (viewfs://), the users would be able to use any scheme.\n+ *\n+ * Example 1:\n+ * If users want some of their existing cluster (hdfs://Cluster)\n+ * data to mount with other hdfs and object store clusters(hdfs://NN1,\n+ * o3fs://bucket1.volume1/, s3a://bucket1/)\n+ *\n+ * fs.viewfs.mounttable.Cluster./user = hdfs://NN1/user\n+ * fs.viewfs.mounttable.Cluster./data = o3fs://bucket1.volume1/data\n+ * fs.viewfs.mounttable.Cluster./backup = s3a://bucket1/backup/\n+ *\n+ * Op1: Create file hdfs://Cluster/user/fileA will go to hdfs://NN1/user/fileA\n+ * Op2: Create file hdfs://Cluster/data/datafile will go to\n+ *      o3fs://bucket1.volume1/data/datafile\n+ * Op3: Create file hdfs://Cluster/backup/data.zip will go to\n+ *      s3a://bucket1/backup/data.zip\n+ *\n+ * Example 2:\n+ * If users want some of their existing cluster (s3a://bucketA/)\n+ * data to mount with other hdfs and object store clusters\n+ * (hdfs://NN1, o3fs://bucket1.volume1/)\n+ *\n+ * fs.viewfs.mounttable.bucketA./user = hdfs://NN1/user\n+ * fs.viewfs.mounttable.bucketA./data = o3fs://bucket1.volume1/data\n+ * fs.viewfs.mounttable.bucketA./salesDB = s3a://bucketA/salesDB/\n+ *\n+ * Op1: Create file s3a://bucketA/user/fileA will go to hdfs://NN1/user/fileA\n+ * Op2: Create file s3a://bucketA/data/datafile will go to\n+ *      o3fs://bucket1.volume1/data/datafile\n+ * Op3: Create file s3a://bucketA/salesDB/dbfile will go to\n+ *      s3a://bucketA/salesDB/dbfile\n+ *****************************************************************************/\n+@InterfaceAudience.LimitedPrivate({ \"MapReduce\", \"HBase\", \"Hive\" })\n+@InterfaceStability.Evolving\n+public class ViewFsOverloadScheme extends ViewFileSystem {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c1525f808f02ed40ca7451d4d71b7fa12708e106"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA1ODQ1NA==", "bodyText": "This we discussed. Sanjay suggested to keep ViewFsOverloadScheme to indicate that most of the implementation is ViewFS itself but we get to overload the scheme. ViewOverloadSchemeFilesystem and ViewFsOverloadScheme are discussed names while writing the doc. :-)\nYes, we have that FileContext based implementation in consideration. I will be filing a JIRA for that.", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419058454", "createdAt": "2020-05-03T06:56:24Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsOverloadScheme.java", "diffHunk": "@@ -0,0 +1,187 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n+import java.net.URI;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+\n+/******************************************************************************\n+ * This class is extended from the ViewFileSystem for the overloaded scheme file\n+ * system. The objective here is to handle multiple mounted file systems\n+ * transparently. Mount link configurations and in-memory mount table\n+ * building behaviors are inherited from ViewFileSystem. Unlike ViewFileSystem\n+ * scheme (viewfs://), the users would be able to use any scheme.\n+ *\n+ * Example 1:\n+ * If users want some of their existing cluster (hdfs://Cluster)\n+ * data to mount with other hdfs and object store clusters(hdfs://NN1,\n+ * o3fs://bucket1.volume1/, s3a://bucket1/)\n+ *\n+ * fs.viewfs.mounttable.Cluster./user = hdfs://NN1/user\n+ * fs.viewfs.mounttable.Cluster./data = o3fs://bucket1.volume1/data\n+ * fs.viewfs.mounttable.Cluster./backup = s3a://bucket1/backup/\n+ *\n+ * Op1: Create file hdfs://Cluster/user/fileA will go to hdfs://NN1/user/fileA\n+ * Op2: Create file hdfs://Cluster/data/datafile will go to\n+ *      o3fs://bucket1.volume1/data/datafile\n+ * Op3: Create file hdfs://Cluster/backup/data.zip will go to\n+ *      s3a://bucket1/backup/data.zip\n+ *\n+ * Example 2:\n+ * If users want some of their existing cluster (s3a://bucketA/)\n+ * data to mount with other hdfs and object store clusters\n+ * (hdfs://NN1, o3fs://bucket1.volume1/)\n+ *\n+ * fs.viewfs.mounttable.bucketA./user = hdfs://NN1/user\n+ * fs.viewfs.mounttable.bucketA./data = o3fs://bucket1.volume1/data\n+ * fs.viewfs.mounttable.bucketA./salesDB = s3a://bucketA/salesDB/\n+ *\n+ * Op1: Create file s3a://bucketA/user/fileA will go to hdfs://NN1/user/fileA\n+ * Op2: Create file s3a://bucketA/data/datafile will go to\n+ *      o3fs://bucket1.volume1/data/datafile\n+ * Op3: Create file s3a://bucketA/salesDB/dbfile will go to\n+ *      s3a://bucketA/salesDB/dbfile\n+ *****************************************************************************/\n+@InterfaceAudience.LimitedPrivate({ \"MapReduce\", \"HBase\", \"Hive\" })\n+@InterfaceStability.Evolving\n+public class ViewFsOverloadScheme extends ViewFileSystem {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0NDI4Mw=="}, "originalCommit": {"oid": "c1525f808f02ed40ca7451d4d71b7fa12708e106"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwODA5MDgwOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeWithHdfsScheme.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QwNDoyNDozOVrOGPoe5Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QwOTo1NTowMlrOGPqh3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0NTA5Mw==", "bodyText": "calling createLinks isn't even needed here right?", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419045093", "createdAt": "2020-05-03T04:24:39Z", "author": {"login": "virajith"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeWithHdfsScheme.java", "diffHunk": "@@ -0,0 +1,351 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.CommonConfigurationKeys;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.RawLocalFileSystem;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.security.AccessControlException;\n+import org.apache.hadoop.test.PathUtils;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+/**\n+ * Tests ViewFsOverloadScheme with configured mount links.\n+ */\n+public class TestViewFsOverloadSchemeWithHdfsScheme {\n+  private static final String FS_IMPL_PATTERN_KEY = \"fs.%s.impl\";\n+  private static final String HDFS_SCHEME = \"hdfs\";\n+  private Configuration conf = null;\n+  private MiniDFSCluster cluster = null;\n+  private URI defaultFSURI;\n+  private File localTargetDir;\n+  private static final String TEST_ROOT_DIR =\n+      PathUtils.getTestDirName(TestViewFsOverloadSchemeWithHdfsScheme.class);\n+  private static final String HDFS_USER_FOLDER = \"/HDFSUser\";\n+  private static final String LOCAL_FOLDER = \"/local\";\n+\n+  @Before\n+  public void startCluster() throws IOException {\n+    conf = new Configuration();\n+    conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_DELEGATION_TOKEN_ALWAYS_USE_KEY,\n+        true);\n+    conf.set(String.format(FS_IMPL_PATTERN_KEY, HDFS_SCHEME),\n+        ViewFsOverloadScheme.class.getName());\n+    conf.set(String.format(\n+        FsConstants.FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN,\n+        HDFS_SCHEME), DistributedFileSystem.class.getName());\n+    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(2).build();\n+    cluster.waitClusterUp();\n+    defaultFSURI =\n+        URI.create(conf.get(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY));\n+    localTargetDir = new File(TEST_ROOT_DIR, \"/root/\");\n+    Assert.assertEquals(HDFS_SCHEME, defaultFSURI.getScheme()); // hdfs scheme.\n+  }\n+\n+  @After\n+  public void tearDown() throws IOException {\n+    if (cluster != null) {\n+      FileSystem.closeAll();\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  private void createLinks(boolean needFalbackLink, Path hdfsTargetPath,\n+      Path localTragetPath) {\n+    ConfigUtil.addLink(conf, defaultFSURI.getAuthority(), HDFS_USER_FOLDER,\n+        hdfsTargetPath.toUri());\n+    ConfigUtil.addLink(conf, defaultFSURI.getAuthority(), LOCAL_FOLDER,\n+        localTragetPath.toUri());\n+    if (needFalbackLink) {\n+      ConfigUtil.addLinkFallback(conf, defaultFSURI.getAuthority(),\n+          hdfsTargetPath.toUri());\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows.\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   *\n+   * create file /HDFSUser/testfile should create in hdfs\n+   * create file /local/test should create directory in local fs\n+   */\n+  @Test(timeout = 30000)\n+  public void testMountLinkWithLocalAndHDFS() throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(false, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    Assert.assertEquals(2, fs.getMountPoints().length);\n+\n+    // /HDFSUser/testfile\n+    Path hdfsFile = new Path(HDFS_USER_FOLDER + \"/testfile\");\n+    // /local/test\n+    Path localDir = new Path(LOCAL_FOLDER + \"/test\");\n+\n+    fs.create(hdfsFile); // /HDFSUser/testfile\n+    fs.mkdirs(localDir); // /local/test\n+\n+    // Initialize HDFS and test files exist in ls or not\n+    DistributedFileSystem dfs = new DistributedFileSystem();\n+    dfs.initialize(defaultFSURI, conf);\n+    try {\n+      Assert.assertTrue(dfs.exists(\n+          new Path(Path.getPathWithoutSchemeAndAuthority(hdfsTargetPath),\n+              hdfsFile.getName()))); // should be in hdfs.\n+      Assert.assertFalse(dfs.exists(\n+          new Path(Path.getPathWithoutSchemeAndAuthority(localTragetPath),\n+              localDir.getName()))); // should not be in local fs.\n+    } finally {\n+      dfs.close();\n+    }\n+\n+    RawLocalFileSystem lfs = new RawLocalFileSystem();\n+    lfs.initialize(localTragetPath.toUri(), conf);\n+    try {\n+      Assert.assertFalse(lfs.exists(\n+          new Path(Path.getPathWithoutSchemeAndAuthority(hdfsTargetPath),\n+              hdfsFile.getName()))); // should not be in hdfs.\n+      Assert.assertTrue(lfs.exists(\n+          new Path(Path.getPathWithoutSchemeAndAuthority(localTragetPath),\n+              localDir.getName()))); // should be in local fs.\n+    } finally {\n+      lfs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows.\n+   * hdfs://localhost:xxx/HDFSUser --> nonexistent://NonExistent/User/\n+   * It should fail to add non existent fs link.\n+   */\n+  @Test(expected = IOException.class, timeout = 30000)\n+  public void testMountLinkWithNonExistentLink() throws Exception {\n+    final String userFolder = \"/User\";\n+    final Path nonExistTargetPath =\n+        new Path(\"nonexistent://NonExistent\" + userFolder);\n+\n+    /**\n+     * Below addLink will create following mount points\n+     * hdfs://localhost:xxx/User --> nonexistent://NonExistent/User/\n+     */\n+    ConfigUtil.addLink(conf, defaultFSURI.getAuthority(), userFolder,\n+        nonExistTargetPath.toUri());\n+    FileSystem.get(conf);\n+    Assert.fail(\"Expected to fail with non existent link\");\n+  }\n+\n+  /**\n+   * Create mount links as follows.\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   * ListStatus on / should list the mount links.\n+   */\n+  @Test(timeout = 30000)\n+  public void testListStatusOnRootShouldListAllMountLinks() throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(false, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      FileStatus[] ls = fs.listStatus(new Path(\"/\"));\n+      Assert.assertEquals(2, ls.length);\n+      String lsPath1 =\n+          Path.getPathWithoutSchemeAndAuthority(ls[0].getPath()).toString();\n+      String lsPath2 =\n+          Path.getPathWithoutSchemeAndAuthority(ls[1].getPath()).toString();\n+      Assert.assertTrue(\n+          HDFS_USER_FOLDER.equals(lsPath1) || LOCAL_FOLDER.equals(lsPath1));\n+      Assert.assertTrue(\n+          HDFS_USER_FOLDER.equals(lsPath2) || LOCAL_FOLDER.equals(lsPath2));\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   * ListStatus non mount directory should fail.\n+   */\n+  @Test(expected = IOException.class, timeout = 30000)\n+  public void testListStatusOnNonMountedPath() throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(false, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      fs.listStatus(new Path(\"/nonMount\"));\n+      Assert.fail(\"It should fail as no mount link with /nonMount\");\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows hdfs://localhost:xxx/HDFSUser -->\n+   * hdfs://localhost:xxx/HDFSUser/ hdfs://localhost:xxx/local -->\n+   * file://TEST_ROOT_DIR/root/ fallback --> hdfs://localhost:xxx/HDFSUser/\n+   * Creating file or directory at non root level should succeed with fallback\n+   * links.\n+   */\n+  @Test(timeout = 30000)\n+  public void testWithLinkFallBack() throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(true, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      fs.create(new Path(\"/nonMount/myfile\"));\n+      FileStatus[] ls = fs.listStatus(new Path(\"/nonMount\"));\n+      Assert.assertEquals(1, ls.length);\n+      Assert.assertEquals(\n+          Path.getPathWithoutSchemeAndAuthority(ls[0].getPath()).getName(),\n+          \"myfile\");\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   *\n+   * It can not find any mount link. ViewFS expects a mount point from root.\n+   */\n+  @Test(expected = NotInMountpointException.class, timeout = 30000)\n+  public void testCreateOnRootShouldFailWhenMountLinkConfigured()\n+      throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(false, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      fs.create(new Path(\"/newFileOnRoot\"));\n+      Assert.fail(\"It should fail as root is read only in viewFS.\");\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   * fallback --> hdfs://localhost:xxx/HDFSUser/\n+   *\n+   * It will find fallback link, but root is not accessible and read only.\n+   */\n+  @Test(expected = AccessControlException.class, timeout = 30000)\n+  public void testCreateOnRootShouldFailEvenFallBackMountLinkConfigured()\n+      throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(true, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      fs.create(new Path(\"/onRootWhenFallBack\"));\n+      Assert.fail(\n+          \"It should fail as root is read only in viewFS, even when configured\"\n+              + \" with fallback.\");\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   * fallback --> hdfs://localhost:xxx/HDFSUser/\n+   *\n+   * It will find fallback link, but root is not accessible and read only.\n+   */\n+  @Test(expected = UnsupportedFileSystemException.class, timeout = 30000)\n+  public void testInvalidOverloadSchemeTargetFS() throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+    conf = new Configuration();\n+    createLinks(true, hdfsTargetPath, localTragetPath);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c1525f808f02ed40ca7451d4d71b7fa12708e106"}, "originalPosition": 313}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA3ODYyMA==", "bodyText": "Without creating links ViewFS fails with \"not mount points configured\". So, to move forward to the intended scenario, we just need to create links.", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419078620", "createdAt": "2020-05-03T09:55:02Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeWithHdfsScheme.java", "diffHunk": "@@ -0,0 +1,351 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.CommonConfigurationKeys;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.RawLocalFileSystem;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.security.AccessControlException;\n+import org.apache.hadoop.test.PathUtils;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+/**\n+ * Tests ViewFsOverloadScheme with configured mount links.\n+ */\n+public class TestViewFsOverloadSchemeWithHdfsScheme {\n+  private static final String FS_IMPL_PATTERN_KEY = \"fs.%s.impl\";\n+  private static final String HDFS_SCHEME = \"hdfs\";\n+  private Configuration conf = null;\n+  private MiniDFSCluster cluster = null;\n+  private URI defaultFSURI;\n+  private File localTargetDir;\n+  private static final String TEST_ROOT_DIR =\n+      PathUtils.getTestDirName(TestViewFsOverloadSchemeWithHdfsScheme.class);\n+  private static final String HDFS_USER_FOLDER = \"/HDFSUser\";\n+  private static final String LOCAL_FOLDER = \"/local\";\n+\n+  @Before\n+  public void startCluster() throws IOException {\n+    conf = new Configuration();\n+    conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_DELEGATION_TOKEN_ALWAYS_USE_KEY,\n+        true);\n+    conf.set(String.format(FS_IMPL_PATTERN_KEY, HDFS_SCHEME),\n+        ViewFsOverloadScheme.class.getName());\n+    conf.set(String.format(\n+        FsConstants.FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN,\n+        HDFS_SCHEME), DistributedFileSystem.class.getName());\n+    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(2).build();\n+    cluster.waitClusterUp();\n+    defaultFSURI =\n+        URI.create(conf.get(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY));\n+    localTargetDir = new File(TEST_ROOT_DIR, \"/root/\");\n+    Assert.assertEquals(HDFS_SCHEME, defaultFSURI.getScheme()); // hdfs scheme.\n+  }\n+\n+  @After\n+  public void tearDown() throws IOException {\n+    if (cluster != null) {\n+      FileSystem.closeAll();\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  private void createLinks(boolean needFalbackLink, Path hdfsTargetPath,\n+      Path localTragetPath) {\n+    ConfigUtil.addLink(conf, defaultFSURI.getAuthority(), HDFS_USER_FOLDER,\n+        hdfsTargetPath.toUri());\n+    ConfigUtil.addLink(conf, defaultFSURI.getAuthority(), LOCAL_FOLDER,\n+        localTragetPath.toUri());\n+    if (needFalbackLink) {\n+      ConfigUtil.addLinkFallback(conf, defaultFSURI.getAuthority(),\n+          hdfsTargetPath.toUri());\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows.\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   *\n+   * create file /HDFSUser/testfile should create in hdfs\n+   * create file /local/test should create directory in local fs\n+   */\n+  @Test(timeout = 30000)\n+  public void testMountLinkWithLocalAndHDFS() throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(false, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    Assert.assertEquals(2, fs.getMountPoints().length);\n+\n+    // /HDFSUser/testfile\n+    Path hdfsFile = new Path(HDFS_USER_FOLDER + \"/testfile\");\n+    // /local/test\n+    Path localDir = new Path(LOCAL_FOLDER + \"/test\");\n+\n+    fs.create(hdfsFile); // /HDFSUser/testfile\n+    fs.mkdirs(localDir); // /local/test\n+\n+    // Initialize HDFS and test files exist in ls or not\n+    DistributedFileSystem dfs = new DistributedFileSystem();\n+    dfs.initialize(defaultFSURI, conf);\n+    try {\n+      Assert.assertTrue(dfs.exists(\n+          new Path(Path.getPathWithoutSchemeAndAuthority(hdfsTargetPath),\n+              hdfsFile.getName()))); // should be in hdfs.\n+      Assert.assertFalse(dfs.exists(\n+          new Path(Path.getPathWithoutSchemeAndAuthority(localTragetPath),\n+              localDir.getName()))); // should not be in local fs.\n+    } finally {\n+      dfs.close();\n+    }\n+\n+    RawLocalFileSystem lfs = new RawLocalFileSystem();\n+    lfs.initialize(localTragetPath.toUri(), conf);\n+    try {\n+      Assert.assertFalse(lfs.exists(\n+          new Path(Path.getPathWithoutSchemeAndAuthority(hdfsTargetPath),\n+              hdfsFile.getName()))); // should not be in hdfs.\n+      Assert.assertTrue(lfs.exists(\n+          new Path(Path.getPathWithoutSchemeAndAuthority(localTragetPath),\n+              localDir.getName()))); // should be in local fs.\n+    } finally {\n+      lfs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows.\n+   * hdfs://localhost:xxx/HDFSUser --> nonexistent://NonExistent/User/\n+   * It should fail to add non existent fs link.\n+   */\n+  @Test(expected = IOException.class, timeout = 30000)\n+  public void testMountLinkWithNonExistentLink() throws Exception {\n+    final String userFolder = \"/User\";\n+    final Path nonExistTargetPath =\n+        new Path(\"nonexistent://NonExistent\" + userFolder);\n+\n+    /**\n+     * Below addLink will create following mount points\n+     * hdfs://localhost:xxx/User --> nonexistent://NonExistent/User/\n+     */\n+    ConfigUtil.addLink(conf, defaultFSURI.getAuthority(), userFolder,\n+        nonExistTargetPath.toUri());\n+    FileSystem.get(conf);\n+    Assert.fail(\"Expected to fail with non existent link\");\n+  }\n+\n+  /**\n+   * Create mount links as follows.\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   * ListStatus on / should list the mount links.\n+   */\n+  @Test(timeout = 30000)\n+  public void testListStatusOnRootShouldListAllMountLinks() throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(false, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      FileStatus[] ls = fs.listStatus(new Path(\"/\"));\n+      Assert.assertEquals(2, ls.length);\n+      String lsPath1 =\n+          Path.getPathWithoutSchemeAndAuthority(ls[0].getPath()).toString();\n+      String lsPath2 =\n+          Path.getPathWithoutSchemeAndAuthority(ls[1].getPath()).toString();\n+      Assert.assertTrue(\n+          HDFS_USER_FOLDER.equals(lsPath1) || LOCAL_FOLDER.equals(lsPath1));\n+      Assert.assertTrue(\n+          HDFS_USER_FOLDER.equals(lsPath2) || LOCAL_FOLDER.equals(lsPath2));\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   * ListStatus non mount directory should fail.\n+   */\n+  @Test(expected = IOException.class, timeout = 30000)\n+  public void testListStatusOnNonMountedPath() throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(false, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      fs.listStatus(new Path(\"/nonMount\"));\n+      Assert.fail(\"It should fail as no mount link with /nonMount\");\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows hdfs://localhost:xxx/HDFSUser -->\n+   * hdfs://localhost:xxx/HDFSUser/ hdfs://localhost:xxx/local -->\n+   * file://TEST_ROOT_DIR/root/ fallback --> hdfs://localhost:xxx/HDFSUser/\n+   * Creating file or directory at non root level should succeed with fallback\n+   * links.\n+   */\n+  @Test(timeout = 30000)\n+  public void testWithLinkFallBack() throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(true, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      fs.create(new Path(\"/nonMount/myfile\"));\n+      FileStatus[] ls = fs.listStatus(new Path(\"/nonMount\"));\n+      Assert.assertEquals(1, ls.length);\n+      Assert.assertEquals(\n+          Path.getPathWithoutSchemeAndAuthority(ls[0].getPath()).getName(),\n+          \"myfile\");\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   *\n+   * It can not find any mount link. ViewFS expects a mount point from root.\n+   */\n+  @Test(expected = NotInMountpointException.class, timeout = 30000)\n+  public void testCreateOnRootShouldFailWhenMountLinkConfigured()\n+      throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(false, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      fs.create(new Path(\"/newFileOnRoot\"));\n+      Assert.fail(\"It should fail as root is read only in viewFS.\");\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   * fallback --> hdfs://localhost:xxx/HDFSUser/\n+   *\n+   * It will find fallback link, but root is not accessible and read only.\n+   */\n+  @Test(expected = AccessControlException.class, timeout = 30000)\n+  public void testCreateOnRootShouldFailEvenFallBackMountLinkConfigured()\n+      throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(true, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      fs.create(new Path(\"/onRootWhenFallBack\"));\n+      Assert.fail(\n+          \"It should fail as root is read only in viewFS, even when configured\"\n+              + \" with fallback.\");\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   * fallback --> hdfs://localhost:xxx/HDFSUser/\n+   *\n+   * It will find fallback link, but root is not accessible and read only.\n+   */\n+  @Test(expected = UnsupportedFileSystemException.class, timeout = 30000)\n+  public void testInvalidOverloadSchemeTargetFS() throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+    conf = new Configuration();\n+    createLinks(true, hdfsTargetPath, localTragetPath);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0NTA5Mw=="}, "originalCommit": {"oid": "c1525f808f02ed40ca7451d4d71b7fa12708e106"}, "originalPosition": 313}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwODA5MTExOnYy", "diffSide": "RIGHT", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeWithHdfsScheme.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QwNDoyNToxMVrOGPofBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNFQwMDoyNDozMVrOGPwsWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0NTEyNQ==", "bodyText": "Can we add a test that cache actually works?", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419045125", "createdAt": "2020-05-03T04:25:11Z", "author": {"login": "virajith"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeWithHdfsScheme.java", "diffHunk": "@@ -0,0 +1,351 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.CommonConfigurationKeys;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.RawLocalFileSystem;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.security.AccessControlException;\n+import org.apache.hadoop.test.PathUtils;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+/**\n+ * Tests ViewFsOverloadScheme with configured mount links.\n+ */\n+public class TestViewFsOverloadSchemeWithHdfsScheme {\n+  private static final String FS_IMPL_PATTERN_KEY = \"fs.%s.impl\";\n+  private static final String HDFS_SCHEME = \"hdfs\";\n+  private Configuration conf = null;\n+  private MiniDFSCluster cluster = null;\n+  private URI defaultFSURI;\n+  private File localTargetDir;\n+  private static final String TEST_ROOT_DIR =\n+      PathUtils.getTestDirName(TestViewFsOverloadSchemeWithHdfsScheme.class);\n+  private static final String HDFS_USER_FOLDER = \"/HDFSUser\";\n+  private static final String LOCAL_FOLDER = \"/local\";\n+\n+  @Before\n+  public void startCluster() throws IOException {\n+    conf = new Configuration();\n+    conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_DELEGATION_TOKEN_ALWAYS_USE_KEY,\n+        true);\n+    conf.set(String.format(FS_IMPL_PATTERN_KEY, HDFS_SCHEME),\n+        ViewFsOverloadScheme.class.getName());\n+    conf.set(String.format(\n+        FsConstants.FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN,\n+        HDFS_SCHEME), DistributedFileSystem.class.getName());\n+    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(2).build();\n+    cluster.waitClusterUp();\n+    defaultFSURI =\n+        URI.create(conf.get(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY));\n+    localTargetDir = new File(TEST_ROOT_DIR, \"/root/\");\n+    Assert.assertEquals(HDFS_SCHEME, defaultFSURI.getScheme()); // hdfs scheme.\n+  }\n+\n+  @After\n+  public void tearDown() throws IOException {\n+    if (cluster != null) {\n+      FileSystem.closeAll();\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  private void createLinks(boolean needFalbackLink, Path hdfsTargetPath,\n+      Path localTragetPath) {\n+    ConfigUtil.addLink(conf, defaultFSURI.getAuthority(), HDFS_USER_FOLDER,\n+        hdfsTargetPath.toUri());\n+    ConfigUtil.addLink(conf, defaultFSURI.getAuthority(), LOCAL_FOLDER,\n+        localTragetPath.toUri());\n+    if (needFalbackLink) {\n+      ConfigUtil.addLinkFallback(conf, defaultFSURI.getAuthority(),\n+          hdfsTargetPath.toUri());\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows.\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   *\n+   * create file /HDFSUser/testfile should create in hdfs\n+   * create file /local/test should create directory in local fs\n+   */\n+  @Test(timeout = 30000)\n+  public void testMountLinkWithLocalAndHDFS() throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(false, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    Assert.assertEquals(2, fs.getMountPoints().length);\n+\n+    // /HDFSUser/testfile\n+    Path hdfsFile = new Path(HDFS_USER_FOLDER + \"/testfile\");\n+    // /local/test\n+    Path localDir = new Path(LOCAL_FOLDER + \"/test\");\n+\n+    fs.create(hdfsFile); // /HDFSUser/testfile\n+    fs.mkdirs(localDir); // /local/test\n+\n+    // Initialize HDFS and test files exist in ls or not\n+    DistributedFileSystem dfs = new DistributedFileSystem();\n+    dfs.initialize(defaultFSURI, conf);\n+    try {\n+      Assert.assertTrue(dfs.exists(\n+          new Path(Path.getPathWithoutSchemeAndAuthority(hdfsTargetPath),\n+              hdfsFile.getName()))); // should be in hdfs.\n+      Assert.assertFalse(dfs.exists(\n+          new Path(Path.getPathWithoutSchemeAndAuthority(localTragetPath),\n+              localDir.getName()))); // should not be in local fs.\n+    } finally {\n+      dfs.close();\n+    }\n+\n+    RawLocalFileSystem lfs = new RawLocalFileSystem();\n+    lfs.initialize(localTragetPath.toUri(), conf);\n+    try {\n+      Assert.assertFalse(lfs.exists(\n+          new Path(Path.getPathWithoutSchemeAndAuthority(hdfsTargetPath),\n+              hdfsFile.getName()))); // should not be in hdfs.\n+      Assert.assertTrue(lfs.exists(\n+          new Path(Path.getPathWithoutSchemeAndAuthority(localTragetPath),\n+              localDir.getName()))); // should be in local fs.\n+    } finally {\n+      lfs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows.\n+   * hdfs://localhost:xxx/HDFSUser --> nonexistent://NonExistent/User/\n+   * It should fail to add non existent fs link.\n+   */\n+  @Test(expected = IOException.class, timeout = 30000)\n+  public void testMountLinkWithNonExistentLink() throws Exception {\n+    final String userFolder = \"/User\";\n+    final Path nonExistTargetPath =\n+        new Path(\"nonexistent://NonExistent\" + userFolder);\n+\n+    /**\n+     * Below addLink will create following mount points\n+     * hdfs://localhost:xxx/User --> nonexistent://NonExistent/User/\n+     */\n+    ConfigUtil.addLink(conf, defaultFSURI.getAuthority(), userFolder,\n+        nonExistTargetPath.toUri());\n+    FileSystem.get(conf);\n+    Assert.fail(\"Expected to fail with non existent link\");\n+  }\n+\n+  /**\n+   * Create mount links as follows.\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   * ListStatus on / should list the mount links.\n+   */\n+  @Test(timeout = 30000)\n+  public void testListStatusOnRootShouldListAllMountLinks() throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(false, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      FileStatus[] ls = fs.listStatus(new Path(\"/\"));\n+      Assert.assertEquals(2, ls.length);\n+      String lsPath1 =\n+          Path.getPathWithoutSchemeAndAuthority(ls[0].getPath()).toString();\n+      String lsPath2 =\n+          Path.getPathWithoutSchemeAndAuthority(ls[1].getPath()).toString();\n+      Assert.assertTrue(\n+          HDFS_USER_FOLDER.equals(lsPath1) || LOCAL_FOLDER.equals(lsPath1));\n+      Assert.assertTrue(\n+          HDFS_USER_FOLDER.equals(lsPath2) || LOCAL_FOLDER.equals(lsPath2));\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   * ListStatus non mount directory should fail.\n+   */\n+  @Test(expected = IOException.class, timeout = 30000)\n+  public void testListStatusOnNonMountedPath() throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(false, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      fs.listStatus(new Path(\"/nonMount\"));\n+      Assert.fail(\"It should fail as no mount link with /nonMount\");\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows hdfs://localhost:xxx/HDFSUser -->\n+   * hdfs://localhost:xxx/HDFSUser/ hdfs://localhost:xxx/local -->\n+   * file://TEST_ROOT_DIR/root/ fallback --> hdfs://localhost:xxx/HDFSUser/\n+   * Creating file or directory at non root level should succeed with fallback\n+   * links.\n+   */\n+  @Test(timeout = 30000)\n+  public void testWithLinkFallBack() throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(true, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      fs.create(new Path(\"/nonMount/myfile\"));\n+      FileStatus[] ls = fs.listStatus(new Path(\"/nonMount\"));\n+      Assert.assertEquals(1, ls.length);\n+      Assert.assertEquals(\n+          Path.getPathWithoutSchemeAndAuthority(ls[0].getPath()).getName(),\n+          \"myfile\");\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   *\n+   * It can not find any mount link. ViewFS expects a mount point from root.\n+   */\n+  @Test(expected = NotInMountpointException.class, timeout = 30000)\n+  public void testCreateOnRootShouldFailWhenMountLinkConfigured()\n+      throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(false, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      fs.create(new Path(\"/newFileOnRoot\"));\n+      Assert.fail(\"It should fail as root is read only in viewFS.\");\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   * fallback --> hdfs://localhost:xxx/HDFSUser/\n+   *\n+   * It will find fallback link, but root is not accessible and read only.\n+   */\n+  @Test(expected = AccessControlException.class, timeout = 30000)\n+  public void testCreateOnRootShouldFailEvenFallBackMountLinkConfigured()\n+      throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(true, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      fs.create(new Path(\"/onRootWhenFallBack\"));\n+      Assert.fail(\n+          \"It should fail as root is read only in viewFS, even when configured\"\n+              + \" with fallback.\");\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   * fallback --> hdfs://localhost:xxx/HDFSUser/\n+   *\n+   * It will find fallback link, but root is not accessible and read only.\n+   */\n+  @Test(expected = UnsupportedFileSystemException.class, timeout = 30000)\n+  public void testInvalidOverloadSchemeTargetFS() throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+    conf = new Configuration();\n+    createLinks(true, hdfsTargetPath, localTragetPath);\n+    conf.set(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY,\n+        defaultFSURI.toString());\n+    conf.set(String.format(FS_IMPL_PATTERN_KEY, HDFS_SCHEME),\n+        ViewFsOverloadScheme.class.getName());\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      fs.create(new Path(\"/onRootWhenFallBack\"));\n+      Assert.fail(\"OverloadScheme target fs should be valid.\");\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   *\n+   * It should be able to create file using ViewFsOverloadScheme.\n+   */\n+  @Test(timeout = 30000)\n+  public void testViewFsOverloadSchemeWhenInnerCacheDisabled()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c1525f808f02ed40ca7451d4d71b7fa12708e106"}, "originalPosition": 336}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA3OTEyMw==", "bodyText": "We have not changed cache behavior itself. When cache disabled, we have varied path.\nAnyway Its should be easy to add a test for that I think. Let me add one.", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419079123", "createdAt": "2020-05-03T09:59:16Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeWithHdfsScheme.java", "diffHunk": "@@ -0,0 +1,351 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.CommonConfigurationKeys;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.RawLocalFileSystem;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.security.AccessControlException;\n+import org.apache.hadoop.test.PathUtils;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+/**\n+ * Tests ViewFsOverloadScheme with configured mount links.\n+ */\n+public class TestViewFsOverloadSchemeWithHdfsScheme {\n+  private static final String FS_IMPL_PATTERN_KEY = \"fs.%s.impl\";\n+  private static final String HDFS_SCHEME = \"hdfs\";\n+  private Configuration conf = null;\n+  private MiniDFSCluster cluster = null;\n+  private URI defaultFSURI;\n+  private File localTargetDir;\n+  private static final String TEST_ROOT_DIR =\n+      PathUtils.getTestDirName(TestViewFsOverloadSchemeWithHdfsScheme.class);\n+  private static final String HDFS_USER_FOLDER = \"/HDFSUser\";\n+  private static final String LOCAL_FOLDER = \"/local\";\n+\n+  @Before\n+  public void startCluster() throws IOException {\n+    conf = new Configuration();\n+    conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_DELEGATION_TOKEN_ALWAYS_USE_KEY,\n+        true);\n+    conf.set(String.format(FS_IMPL_PATTERN_KEY, HDFS_SCHEME),\n+        ViewFsOverloadScheme.class.getName());\n+    conf.set(String.format(\n+        FsConstants.FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN,\n+        HDFS_SCHEME), DistributedFileSystem.class.getName());\n+    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(2).build();\n+    cluster.waitClusterUp();\n+    defaultFSURI =\n+        URI.create(conf.get(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY));\n+    localTargetDir = new File(TEST_ROOT_DIR, \"/root/\");\n+    Assert.assertEquals(HDFS_SCHEME, defaultFSURI.getScheme()); // hdfs scheme.\n+  }\n+\n+  @After\n+  public void tearDown() throws IOException {\n+    if (cluster != null) {\n+      FileSystem.closeAll();\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  private void createLinks(boolean needFalbackLink, Path hdfsTargetPath,\n+      Path localTragetPath) {\n+    ConfigUtil.addLink(conf, defaultFSURI.getAuthority(), HDFS_USER_FOLDER,\n+        hdfsTargetPath.toUri());\n+    ConfigUtil.addLink(conf, defaultFSURI.getAuthority(), LOCAL_FOLDER,\n+        localTragetPath.toUri());\n+    if (needFalbackLink) {\n+      ConfigUtil.addLinkFallback(conf, defaultFSURI.getAuthority(),\n+          hdfsTargetPath.toUri());\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows.\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   *\n+   * create file /HDFSUser/testfile should create in hdfs\n+   * create file /local/test should create directory in local fs\n+   */\n+  @Test(timeout = 30000)\n+  public void testMountLinkWithLocalAndHDFS() throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(false, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    Assert.assertEquals(2, fs.getMountPoints().length);\n+\n+    // /HDFSUser/testfile\n+    Path hdfsFile = new Path(HDFS_USER_FOLDER + \"/testfile\");\n+    // /local/test\n+    Path localDir = new Path(LOCAL_FOLDER + \"/test\");\n+\n+    fs.create(hdfsFile); // /HDFSUser/testfile\n+    fs.mkdirs(localDir); // /local/test\n+\n+    // Initialize HDFS and test files exist in ls or not\n+    DistributedFileSystem dfs = new DistributedFileSystem();\n+    dfs.initialize(defaultFSURI, conf);\n+    try {\n+      Assert.assertTrue(dfs.exists(\n+          new Path(Path.getPathWithoutSchemeAndAuthority(hdfsTargetPath),\n+              hdfsFile.getName()))); // should be in hdfs.\n+      Assert.assertFalse(dfs.exists(\n+          new Path(Path.getPathWithoutSchemeAndAuthority(localTragetPath),\n+              localDir.getName()))); // should not be in local fs.\n+    } finally {\n+      dfs.close();\n+    }\n+\n+    RawLocalFileSystem lfs = new RawLocalFileSystem();\n+    lfs.initialize(localTragetPath.toUri(), conf);\n+    try {\n+      Assert.assertFalse(lfs.exists(\n+          new Path(Path.getPathWithoutSchemeAndAuthority(hdfsTargetPath),\n+              hdfsFile.getName()))); // should not be in hdfs.\n+      Assert.assertTrue(lfs.exists(\n+          new Path(Path.getPathWithoutSchemeAndAuthority(localTragetPath),\n+              localDir.getName()))); // should be in local fs.\n+    } finally {\n+      lfs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows.\n+   * hdfs://localhost:xxx/HDFSUser --> nonexistent://NonExistent/User/\n+   * It should fail to add non existent fs link.\n+   */\n+  @Test(expected = IOException.class, timeout = 30000)\n+  public void testMountLinkWithNonExistentLink() throws Exception {\n+    final String userFolder = \"/User\";\n+    final Path nonExistTargetPath =\n+        new Path(\"nonexistent://NonExistent\" + userFolder);\n+\n+    /**\n+     * Below addLink will create following mount points\n+     * hdfs://localhost:xxx/User --> nonexistent://NonExistent/User/\n+     */\n+    ConfigUtil.addLink(conf, defaultFSURI.getAuthority(), userFolder,\n+        nonExistTargetPath.toUri());\n+    FileSystem.get(conf);\n+    Assert.fail(\"Expected to fail with non existent link\");\n+  }\n+\n+  /**\n+   * Create mount links as follows.\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   * ListStatus on / should list the mount links.\n+   */\n+  @Test(timeout = 30000)\n+  public void testListStatusOnRootShouldListAllMountLinks() throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(false, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      FileStatus[] ls = fs.listStatus(new Path(\"/\"));\n+      Assert.assertEquals(2, ls.length);\n+      String lsPath1 =\n+          Path.getPathWithoutSchemeAndAuthority(ls[0].getPath()).toString();\n+      String lsPath2 =\n+          Path.getPathWithoutSchemeAndAuthority(ls[1].getPath()).toString();\n+      Assert.assertTrue(\n+          HDFS_USER_FOLDER.equals(lsPath1) || LOCAL_FOLDER.equals(lsPath1));\n+      Assert.assertTrue(\n+          HDFS_USER_FOLDER.equals(lsPath2) || LOCAL_FOLDER.equals(lsPath2));\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   * ListStatus non mount directory should fail.\n+   */\n+  @Test(expected = IOException.class, timeout = 30000)\n+  public void testListStatusOnNonMountedPath() throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(false, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      fs.listStatus(new Path(\"/nonMount\"));\n+      Assert.fail(\"It should fail as no mount link with /nonMount\");\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows hdfs://localhost:xxx/HDFSUser -->\n+   * hdfs://localhost:xxx/HDFSUser/ hdfs://localhost:xxx/local -->\n+   * file://TEST_ROOT_DIR/root/ fallback --> hdfs://localhost:xxx/HDFSUser/\n+   * Creating file or directory at non root level should succeed with fallback\n+   * links.\n+   */\n+  @Test(timeout = 30000)\n+  public void testWithLinkFallBack() throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(true, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      fs.create(new Path(\"/nonMount/myfile\"));\n+      FileStatus[] ls = fs.listStatus(new Path(\"/nonMount\"));\n+      Assert.assertEquals(1, ls.length);\n+      Assert.assertEquals(\n+          Path.getPathWithoutSchemeAndAuthority(ls[0].getPath()).getName(),\n+          \"myfile\");\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   *\n+   * It can not find any mount link. ViewFS expects a mount point from root.\n+   */\n+  @Test(expected = NotInMountpointException.class, timeout = 30000)\n+  public void testCreateOnRootShouldFailWhenMountLinkConfigured()\n+      throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(false, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      fs.create(new Path(\"/newFileOnRoot\"));\n+      Assert.fail(\"It should fail as root is read only in viewFS.\");\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   * fallback --> hdfs://localhost:xxx/HDFSUser/\n+   *\n+   * It will find fallback link, but root is not accessible and read only.\n+   */\n+  @Test(expected = AccessControlException.class, timeout = 30000)\n+  public void testCreateOnRootShouldFailEvenFallBackMountLinkConfigured()\n+      throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(true, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      fs.create(new Path(\"/onRootWhenFallBack\"));\n+      Assert.fail(\n+          \"It should fail as root is read only in viewFS, even when configured\"\n+              + \" with fallback.\");\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   * fallback --> hdfs://localhost:xxx/HDFSUser/\n+   *\n+   * It will find fallback link, but root is not accessible and read only.\n+   */\n+  @Test(expected = UnsupportedFileSystemException.class, timeout = 30000)\n+  public void testInvalidOverloadSchemeTargetFS() throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+    conf = new Configuration();\n+    createLinks(true, hdfsTargetPath, localTragetPath);\n+    conf.set(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY,\n+        defaultFSURI.toString());\n+    conf.set(String.format(FS_IMPL_PATTERN_KEY, HDFS_SCHEME),\n+        ViewFsOverloadScheme.class.getName());\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      fs.create(new Path(\"/onRootWhenFallBack\"));\n+      Assert.fail(\"OverloadScheme target fs should be valid.\");\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   *\n+   * It should be able to create file using ViewFsOverloadScheme.\n+   */\n+  @Test(timeout = 30000)\n+  public void testViewFsOverloadSchemeWhenInnerCacheDisabled()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0NTEyNQ=="}, "originalCommit": {"oid": "c1525f808f02ed40ca7451d4d71b7fa12708e106"}, "originalPosition": 336}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTE3OTYxMA==", "bodyText": "Added Tests.", "url": "https://github.com/apache/hadoop/pull/1988#discussion_r419179610", "createdAt": "2020-05-04T00:24:31Z", "author": {"login": "umamaheswararao"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeWithHdfsScheme.java", "diffHunk": "@@ -0,0 +1,351 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.CommonConfigurationKeys;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.RawLocalFileSystem;\n+import org.apache.hadoop.fs.UnsupportedFileSystemException;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.security.AccessControlException;\n+import org.apache.hadoop.test.PathUtils;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+/**\n+ * Tests ViewFsOverloadScheme with configured mount links.\n+ */\n+public class TestViewFsOverloadSchemeWithHdfsScheme {\n+  private static final String FS_IMPL_PATTERN_KEY = \"fs.%s.impl\";\n+  private static final String HDFS_SCHEME = \"hdfs\";\n+  private Configuration conf = null;\n+  private MiniDFSCluster cluster = null;\n+  private URI defaultFSURI;\n+  private File localTargetDir;\n+  private static final String TEST_ROOT_DIR =\n+      PathUtils.getTestDirName(TestViewFsOverloadSchemeWithHdfsScheme.class);\n+  private static final String HDFS_USER_FOLDER = \"/HDFSUser\";\n+  private static final String LOCAL_FOLDER = \"/local\";\n+\n+  @Before\n+  public void startCluster() throws IOException {\n+    conf = new Configuration();\n+    conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_DELEGATION_TOKEN_ALWAYS_USE_KEY,\n+        true);\n+    conf.set(String.format(FS_IMPL_PATTERN_KEY, HDFS_SCHEME),\n+        ViewFsOverloadScheme.class.getName());\n+    conf.set(String.format(\n+        FsConstants.FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN,\n+        HDFS_SCHEME), DistributedFileSystem.class.getName());\n+    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(2).build();\n+    cluster.waitClusterUp();\n+    defaultFSURI =\n+        URI.create(conf.get(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY));\n+    localTargetDir = new File(TEST_ROOT_DIR, \"/root/\");\n+    Assert.assertEquals(HDFS_SCHEME, defaultFSURI.getScheme()); // hdfs scheme.\n+  }\n+\n+  @After\n+  public void tearDown() throws IOException {\n+    if (cluster != null) {\n+      FileSystem.closeAll();\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  private void createLinks(boolean needFalbackLink, Path hdfsTargetPath,\n+      Path localTragetPath) {\n+    ConfigUtil.addLink(conf, defaultFSURI.getAuthority(), HDFS_USER_FOLDER,\n+        hdfsTargetPath.toUri());\n+    ConfigUtil.addLink(conf, defaultFSURI.getAuthority(), LOCAL_FOLDER,\n+        localTragetPath.toUri());\n+    if (needFalbackLink) {\n+      ConfigUtil.addLinkFallback(conf, defaultFSURI.getAuthority(),\n+          hdfsTargetPath.toUri());\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows.\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   *\n+   * create file /HDFSUser/testfile should create in hdfs\n+   * create file /local/test should create directory in local fs\n+   */\n+  @Test(timeout = 30000)\n+  public void testMountLinkWithLocalAndHDFS() throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(false, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    Assert.assertEquals(2, fs.getMountPoints().length);\n+\n+    // /HDFSUser/testfile\n+    Path hdfsFile = new Path(HDFS_USER_FOLDER + \"/testfile\");\n+    // /local/test\n+    Path localDir = new Path(LOCAL_FOLDER + \"/test\");\n+\n+    fs.create(hdfsFile); // /HDFSUser/testfile\n+    fs.mkdirs(localDir); // /local/test\n+\n+    // Initialize HDFS and test files exist in ls or not\n+    DistributedFileSystem dfs = new DistributedFileSystem();\n+    dfs.initialize(defaultFSURI, conf);\n+    try {\n+      Assert.assertTrue(dfs.exists(\n+          new Path(Path.getPathWithoutSchemeAndAuthority(hdfsTargetPath),\n+              hdfsFile.getName()))); // should be in hdfs.\n+      Assert.assertFalse(dfs.exists(\n+          new Path(Path.getPathWithoutSchemeAndAuthority(localTragetPath),\n+              localDir.getName()))); // should not be in local fs.\n+    } finally {\n+      dfs.close();\n+    }\n+\n+    RawLocalFileSystem lfs = new RawLocalFileSystem();\n+    lfs.initialize(localTragetPath.toUri(), conf);\n+    try {\n+      Assert.assertFalse(lfs.exists(\n+          new Path(Path.getPathWithoutSchemeAndAuthority(hdfsTargetPath),\n+              hdfsFile.getName()))); // should not be in hdfs.\n+      Assert.assertTrue(lfs.exists(\n+          new Path(Path.getPathWithoutSchemeAndAuthority(localTragetPath),\n+              localDir.getName()))); // should be in local fs.\n+    } finally {\n+      lfs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows.\n+   * hdfs://localhost:xxx/HDFSUser --> nonexistent://NonExistent/User/\n+   * It should fail to add non existent fs link.\n+   */\n+  @Test(expected = IOException.class, timeout = 30000)\n+  public void testMountLinkWithNonExistentLink() throws Exception {\n+    final String userFolder = \"/User\";\n+    final Path nonExistTargetPath =\n+        new Path(\"nonexistent://NonExistent\" + userFolder);\n+\n+    /**\n+     * Below addLink will create following mount points\n+     * hdfs://localhost:xxx/User --> nonexistent://NonExistent/User/\n+     */\n+    ConfigUtil.addLink(conf, defaultFSURI.getAuthority(), userFolder,\n+        nonExistTargetPath.toUri());\n+    FileSystem.get(conf);\n+    Assert.fail(\"Expected to fail with non existent link\");\n+  }\n+\n+  /**\n+   * Create mount links as follows.\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   * ListStatus on / should list the mount links.\n+   */\n+  @Test(timeout = 30000)\n+  public void testListStatusOnRootShouldListAllMountLinks() throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(false, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      FileStatus[] ls = fs.listStatus(new Path(\"/\"));\n+      Assert.assertEquals(2, ls.length);\n+      String lsPath1 =\n+          Path.getPathWithoutSchemeAndAuthority(ls[0].getPath()).toString();\n+      String lsPath2 =\n+          Path.getPathWithoutSchemeAndAuthority(ls[1].getPath()).toString();\n+      Assert.assertTrue(\n+          HDFS_USER_FOLDER.equals(lsPath1) || LOCAL_FOLDER.equals(lsPath1));\n+      Assert.assertTrue(\n+          HDFS_USER_FOLDER.equals(lsPath2) || LOCAL_FOLDER.equals(lsPath2));\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   * ListStatus non mount directory should fail.\n+   */\n+  @Test(expected = IOException.class, timeout = 30000)\n+  public void testListStatusOnNonMountedPath() throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(false, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      fs.listStatus(new Path(\"/nonMount\"));\n+      Assert.fail(\"It should fail as no mount link with /nonMount\");\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows hdfs://localhost:xxx/HDFSUser -->\n+   * hdfs://localhost:xxx/HDFSUser/ hdfs://localhost:xxx/local -->\n+   * file://TEST_ROOT_DIR/root/ fallback --> hdfs://localhost:xxx/HDFSUser/\n+   * Creating file or directory at non root level should succeed with fallback\n+   * links.\n+   */\n+  @Test(timeout = 30000)\n+  public void testWithLinkFallBack() throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(true, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      fs.create(new Path(\"/nonMount/myfile\"));\n+      FileStatus[] ls = fs.listStatus(new Path(\"/nonMount\"));\n+      Assert.assertEquals(1, ls.length);\n+      Assert.assertEquals(\n+          Path.getPathWithoutSchemeAndAuthority(ls[0].getPath()).getName(),\n+          \"myfile\");\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   *\n+   * It can not find any mount link. ViewFS expects a mount point from root.\n+   */\n+  @Test(expected = NotInMountpointException.class, timeout = 30000)\n+  public void testCreateOnRootShouldFailWhenMountLinkConfigured()\n+      throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(false, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      fs.create(new Path(\"/newFileOnRoot\"));\n+      Assert.fail(\"It should fail as root is read only in viewFS.\");\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   * fallback --> hdfs://localhost:xxx/HDFSUser/\n+   *\n+   * It will find fallback link, but root is not accessible and read only.\n+   */\n+  @Test(expected = AccessControlException.class, timeout = 30000)\n+  public void testCreateOnRootShouldFailEvenFallBackMountLinkConfigured()\n+      throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+\n+    createLinks(true, hdfsTargetPath, localTragetPath);\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      fs.create(new Path(\"/onRootWhenFallBack\"));\n+      Assert.fail(\n+          \"It should fail as root is read only in viewFS, even when configured\"\n+              + \" with fallback.\");\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   * fallback --> hdfs://localhost:xxx/HDFSUser/\n+   *\n+   * It will find fallback link, but root is not accessible and read only.\n+   */\n+  @Test(expected = UnsupportedFileSystemException.class, timeout = 30000)\n+  public void testInvalidOverloadSchemeTargetFS() throws Exception {\n+    final Path hdfsTargetPath = new Path(defaultFSURI + HDFS_USER_FOLDER);\n+    final Path localTragetPath = new Path(localTargetDir.toURI());\n+    conf = new Configuration();\n+    createLinks(true, hdfsTargetPath, localTragetPath);\n+    conf.set(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY,\n+        defaultFSURI.toString());\n+    conf.set(String.format(FS_IMPL_PATTERN_KEY, HDFS_SCHEME),\n+        ViewFsOverloadScheme.class.getName());\n+\n+    ViewFsOverloadScheme fs = (ViewFsOverloadScheme) FileSystem.get(conf);\n+    try {\n+      fs.create(new Path(\"/onRootWhenFallBack\"));\n+      Assert.fail(\"OverloadScheme target fs should be valid.\");\n+    } finally {\n+      fs.close();\n+    }\n+  }\n+\n+  /**\n+   * Create mount links as follows\n+   * hdfs://localhost:xxx/HDFSUser --> hdfs://localhost:xxx/HDFSUser/\n+   * hdfs://localhost:xxx/local --> file://TEST_ROOT_DIR/root/\n+   *\n+   * It should be able to create file using ViewFsOverloadScheme.\n+   */\n+  @Test(timeout = 30000)\n+  public void testViewFsOverloadSchemeWhenInnerCacheDisabled()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0NTEyNQ=="}, "originalCommit": {"oid": "c1525f808f02ed40ca7451d4d71b7fa12708e106"}, "originalPosition": 336}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3538, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}