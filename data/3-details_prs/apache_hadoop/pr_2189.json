{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDYyNTA0MjY4", "number": 2189, "title": "HDFS-15025. Applying NVDIMM storage media to HDFS", "bodyText": "The non-volatile memory NVDIMM is faster than SSD,\nit can be used simultaneously with RAM, DISK and SSD.\nStoring the data of HDFS on NVDIMM directly will get\nbetter response rate and reliability.", "createdAt": "2020-08-04T03:23:29Z", "url": "https://github.com/apache/hadoop/pull/2189", "merged": true, "mergeCommit": {"oid": "ff59fbb8b04a20a85208ad8921d97ee71abd991c"}, "closed": true, "closedAt": "2020-09-24T08:57:05Z", "author": {"login": "huangtianhua"}, "timelineItems": {"totalCount": 23, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc7i1_5ABqjM2MTkyODUyNDg=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdL57HvAFqTQ5NTIyMDExNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "231c2dea3a64c4e74d06d5868d3120679f7bf78b", "author": {"user": {"login": "huangtianhua", "name": "huangtianhua"}}, "url": "https://github.com/apache/hadoop/commit/231c2dea3a64c4e74d06d5868d3120679f7bf78b", "committedDate": "2020-08-04T03:10:42Z", "message": "HDFS-15025. Applying NVDIMM storage media to HDFS\n\nThe non-volatile memory NVDIMM is faster than SSD,\nit can be used simultaneously with RAM, DISK and SSD.\nStoring the data of HDFS on NVDIMM directly will get\nbetter response rate and reliability."}, "afterCommit": {"oid": "9f6dca326d6887ddcd2368a40a8f689edbb74153", "author": {"user": {"login": "huangtianhua", "name": "huangtianhua"}}, "url": "https://github.com/apache/hadoop/commit/9f6dca326d6887ddcd2368a40a8f689edbb74153", "committedDate": "2020-08-04T09:02:39Z", "message": "HDFS-15025. Applying NVDIMM storage media to HDFS\n\nThe non-volatile memory NVDIMM is faster than SSD,\nit can be used simultaneously with RAM, DISK and SSD.\nStoring the data of HDFS on NVDIMM directly will get\nbetter response rate and reliability."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9f6dca326d6887ddcd2368a40a8f689edbb74153", "author": {"user": {"login": "huangtianhua", "name": "huangtianhua"}}, "url": "https://github.com/apache/hadoop/commit/9f6dca326d6887ddcd2368a40a8f689edbb74153", "committedDate": "2020-08-04T09:02:39Z", "message": "HDFS-15025. Applying NVDIMM storage media to HDFS\n\nThe non-volatile memory NVDIMM is faster than SSD,\nit can be used simultaneously with RAM, DISK and SSD.\nStoring the data of HDFS on NVDIMM directly will get\nbetter response rate and reliability."}, "afterCommit": {"oid": "a1caf08b54a05c7891d6f2b4806965cee01c2cd5", "author": {"user": {"login": "huangtianhua", "name": "huangtianhua"}}, "url": "https://github.com/apache/hadoop/commit/a1caf08b54a05c7891d6f2b4806965cee01c2cd5", "committedDate": "2020-08-04T09:24:30Z", "message": "HDFS-15025. Applying NVDIMM storage media to HDFS\n\nThe non-volatile memory NVDIMM is faster than SSD,\nit can be used simultaneously with RAM, DISK and SSD.\nStoring the data of HDFS on NVDIMM directly will get\nbetter response rate and reliability.\n\nCo-authored-by: YaYun-Wang yywangyayun@163.com"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a1caf08b54a05c7891d6f2b4806965cee01c2cd5", "author": {"user": {"login": "huangtianhua", "name": "huangtianhua"}}, "url": "https://github.com/apache/hadoop/commit/a1caf08b54a05c7891d6f2b4806965cee01c2cd5", "committedDate": "2020-08-04T09:24:30Z", "message": "HDFS-15025. Applying NVDIMM storage media to HDFS\n\nThe non-volatile memory NVDIMM is faster than SSD,\nit can be used simultaneously with RAM, DISK and SSD.\nStoring the data of HDFS on NVDIMM directly will get\nbetter response rate and reliability.\n\nCo-authored-by: YaYun-Wang yywangyayun@163.com"}, "afterCommit": {"oid": "b8286dc50be8f0335a7a68e937d598758d47335a", "author": {"user": {"login": "huangtianhua", "name": "huangtianhua"}}, "url": "https://github.com/apache/hadoop/commit/b8286dc50be8f0335a7a68e937d598758d47335a", "committedDate": "2020-08-04T09:39:57Z", "message": "HDFS-15025. Applying NVDIMM storage media to HDFS\n\nThe non-volatile memory NVDIMM is faster than SSD,\nit can be used simultaneously with RAM, DISK and SSD.\nStoring the data of HDFS on NVDIMM directly will get\nbetter response rate and reliability.\n\nCo-authored-by: YaYun-Wang <yywangyayun@163.com>"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "b8286dc50be8f0335a7a68e937d598758d47335a", "author": {"user": {"login": "huangtianhua", "name": "huangtianhua"}}, "url": "https://github.com/apache/hadoop/commit/b8286dc50be8f0335a7a68e937d598758d47335a", "committedDate": "2020-08-04T09:39:57Z", "message": "HDFS-15025. Applying NVDIMM storage media to HDFS\n\nThe non-volatile memory NVDIMM is faster than SSD,\nit can be used simultaneously with RAM, DISK and SSD.\nStoring the data of HDFS on NVDIMM directly will get\nbetter response rate and reliability.\n\nCo-authored-by: YaYun-Wang <yywangyayun@163.com>"}, "afterCommit": {"oid": "bb02bd8718d81805aeba6b758772c326037bd77a", "author": {"user": {"login": "huangtianhua", "name": "huangtianhua"}}, "url": "https://github.com/apache/hadoop/commit/bb02bd8718d81805aeba6b758772c326037bd77a", "committedDate": "2020-08-20T03:38:38Z", "message": "HDFS-15025. Applying NVDIMM storage media to HDFS\n\nThe non-volatile memory NVDIMM is faster than SSD,\nit can be used simultaneously with RAM, DISK and SSD.\nStoring the data of HDFS on NVDIMM directly will get\nbetter response rate and reliability.\n\nCo-authored-by: YaYun-Wang <yywangyayun@163.com>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDczMTU1OTMw", "url": "https://github.com/apache/hadoop/pull/2189#pullrequestreview-473155930", "createdAt": "2020-08-24T06:40:28Z", "commit": {"oid": "bb02bd8718d81805aeba6b758772c326037bd77a"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQwNjo0MDoyOFrOHFWdfg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQwNzowMToxNlrOHFW9tg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTM3MjkyNg==", "bodyText": "nit: I see other variables are using lower case, could we change this name to allnvdimmId", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r475372926", "createdAt": "2020-08-24T06:40:28Z", "author": {"login": "liuml07"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockStoragePolicySuite.java", "diffHunk": "@@ -63,6 +63,12 @@ public static BlockStoragePolicySuite createDefaultSuite(\n         new StorageType[]{StorageType.DISK},\n         new StorageType[]{StorageType.DISK},\n         true);    // Cannot be changed on regular files, but inherited.\n+    final byte allNVDIMMId = HdfsConstants.StoragePolicy.ALL_NVDIMM.value();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bb02bd8718d81805aeba6b758772c326037bd77a"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTM4MTE3NA==", "bodyText": "is added for supporting archival storage. What is the main purpose of this use case? I don't believe this is for \"archival storage\".\nAlso, this sentence is a bit verbose to me. Do you think we can make it concise, something like this?\n\nFrom Hadoop 3.4, a new storage type *NVDIMM* is added for supporting writing replica files in non-volatile \nmemory that has the capability to hold saved data even if the power is turned off.\n\nIf you instead prefer keeping the current phrase, correct the syntax error by s/retains/retain/ and s/datastored/data stored.", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r475381174", "createdAt": "2020-08-24T07:01:16Z", "author": {"login": "liuml07"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/site/markdown/ArchivalStorage.md", "diffHunk": "@@ -27,15 +27,17 @@ The frameworks provided by Heterogeneous Storage and Archival Storage generalize\n Storage Types and Storage Policies\n ----------------------------------\n \n-### Storage Types: ARCHIVE, DISK, SSD and RAM\\_DISK\n+### Storage Types: ARCHIVE, DISK, SSD, NVDIMM and RAM\\_DISK\n \n The first phase of [Heterogeneous Storage (HDFS-2832)](https://issues.apache.org/jira/browse/HDFS-2832) changed datanode storage model from a single storage, which may correspond to multiple physical storage medias, to a collection of storages with each storage corresponding to a physical storage media. It also added the notion of storage types, DISK and SSD, where DISK is the default storage type.\n \n A new storage type *ARCHIVE*, which has high storage density (petabyte of storage) but little compute power, is added for supporting archival storage.\n \n Another new storage type *RAM\\_DISK* is added for supporting writing single replica files in memory.\n \n-### Storage Policies: Hot, Warm, Cold, All\\_SSD, One\\_SSD, Lazy\\_Persist and Provided\n+And a new storage type *NVDIMM*, which is a non-volatile memory that will retains the datastored on the memory when the computer is powered down or system crashes and restore the data when the machine powered on, is added for supporting archival storage.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bb02bd8718d81805aeba6b758772c326037bd77a"}, "originalPosition": 14}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9e1f06d4c41d5704a6b673ed3eccb4fb05d1bb32", "author": {"user": {"login": "YaYun-Wang", "name": null}}, "url": "https://github.com/apache/hadoop/commit/9e1f06d4c41d5704a6b673ed3eccb4fb05d1bb32", "committedDate": "2020-08-25T03:38:45Z", "message": "Update ArchivalStorage.md"}, "afterCommit": {"oid": "84c30785e64d6419d702c468a0a6b22b6c855bc6", "author": {"user": {"login": "huangtianhua", "name": "huangtianhua"}}, "url": "https://github.com/apache/hadoop/commit/84c30785e64d6419d702c468a0a6b22b6c855bc6", "committedDate": "2020-08-25T08:22:08Z", "message": "HDFS-15025. Applying NVDIMM storage media to HDFS\n\nThe non-volatile memory NVDIMM is faster than SSD,\nit can be used simultaneously with RAM, DISK and SSD.\nStoring the data of HDFS on NVDIMM directly will get\nbetter response rate and reliability.\n\nCo-authored-by: YaYun-Wang <yywangyayun@163.com>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc0NzM0NjMz", "url": "https://github.com/apache/hadoop/pull/2189#pullrequestreview-474734633", "createdAt": "2020-08-25T18:13:14Z", "commit": {"oid": "84c30785e64d6419d702c468a0a6b22b6c855bc6"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQxODoxMzoxNFrOHGkF7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQxODoxNjowNVrOHGkL6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NDg0Ng==", "bodyText": "will RAM_DISK and NVDIMM co-exist..? if co-exist's, why can't we name NVDIM itself..?", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r476644846", "createdAt": "2020-08-25T18:13:14Z", "author": {"login": "brahmareddybattula"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StorageType.java", "diffHunk": "@@ -34,28 +34,35 @@\n @InterfaceStability.Unstable\n public enum StorageType {\n   // sorted by the speed of the storage types, from fast to slow\n-  RAM_DISK(true),\n-  SSD(false),\n-  DISK(false),\n-  ARCHIVE(false),\n-  PROVIDED(false);\n+  RAM_DISK(true, true),\n+  NVDIMM(false, true),\n+  SSD(false, false),\n+  DISK(false, false),\n+  ARCHIVE(false, false),\n+  PROVIDED(false, false);\n \n   private final boolean isTransient;\n+  private final boolean isRAM;\n \n   public static final StorageType DEFAULT = DISK;\n \n   public static final StorageType[] EMPTY_ARRAY = {};\n \n   private static final StorageType[] VALUES = values();\n \n-  StorageType(boolean isTransient) {\n+  StorageType(boolean isTransient, boolean isRAM) {\n     this.isTransient = isTransient;\n+    this.isRAM = isRAM;\n   }\n \n   public boolean isTransient() {\n     return isTransient;\n   }\n \n+  public boolean isRAM() {\n+    return isRAM;\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84c30785e64d6419d702c468a0a6b22b6c855bc6"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NTA4OQ==", "bodyText": "Let's add only this line", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r476645089", "createdAt": "2020-08-25T18:13:39Z", "author": {"login": "brahmareddybattula"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StorageType.java", "diffHunk": "@@ -34,28 +34,35 @@\n @InterfaceStability.Unstable\n public enum StorageType {\n   // sorted by the speed of the storage types, from fast to slow\n-  RAM_DISK(true),\n-  SSD(false),\n-  DISK(false),\n-  ARCHIVE(false),\n-  PROVIDED(false);\n+  RAM_DISK(true, true),\n+  NVDIMM(false, true),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84c30785e64d6419d702c468a0a6b22b6c855bc6"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NTYzNA==", "bodyText": "is this will not valid anymore.?", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r476645634", "createdAt": "2020-08-25T18:14:45Z", "author": {"login": "brahmareddybattula"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsVolumeSpi.java", "diffHunk": "@@ -77,6 +77,9 @@\n   /** Returns true if the volume is NOT backed by persistent storage. */\n   boolean isTransientStorage();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84c30785e64d6419d702c468a0a6b22b6c855bc6"}, "originalPosition": 2}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NjM3Ng==", "bodyText": "can we maintain same order here also..?", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r476646376", "createdAt": "2020-08-25T18:16:05Z", "author": {"login": "brahmareddybattula"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/site/markdown/ArchivalStorage.md", "diffHunk": "@@ -88,13 +92,14 @@ The effective storage policy can be retrieved by the \"[`storagepolicies -getStor\n ### Configuration\n \n * **dfs.storage.policy.enabled** - for enabling/disabling the storage policy feature. The default value is `true`.\n-* **dfs.storage.default.policy** - Set the default storage policy with the policy name. The default value is `HOT`.  All possible policies are defined in enum StoragePolicy, including `LAZY_PERSIST` `ALL_SSD` `ONE_SSD` `HOT` `WARM` `COLD` and `PROVIDED`.\n+* **dfs.storage.default.policy** - Set the default storage policy with the policy name. The default value is `HOT`.  All possible policies are defined in enum StoragePolicy, including `LAZY_PERSIST` `ALL_SSD` `ONE_SSD` `HOT` `WARM` `COLD` `ALL_NVDIMM` and `PROVIDED`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84c30785e64d6419d702c468a0a6b22b6c855bc6"}, "originalPosition": 50}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg3MjE2MzUw", "url": "https://github.com/apache/hadoop/pull/2189#pullrequestreview-487216350", "createdAt": "2020-09-12T08:57:15Z", "commit": {"oid": "dea6c63a91a7d0815dbdb405f8cf1c219365aeca"}, "state": "DISMISSED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMlQwODo1NzoxNVrOHQzyIw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMFQwNzozOToxN1rOHU4y_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzM4NzY4Mw==", "bodyText": "nit: we can make it clear what the volume is.\n LOG.warn(\"Caching not supported on block with id {} since the volume \"\n    + \"is backed by {} which is RAM.\", blockId, volume);", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r487387683", "createdAt": "2020-09-12T08:57:15Z", "author": {"login": "liuml07"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java", "diffHunk": "@@ -2298,9 +2298,9 @@ private void cacheBlock(String bpid, long blockId) {\n               \": volume was not an instance of FsVolumeImpl.\");\n           return;\n         }\n-        if (volume.isTransientStorage()) {\n+        if (volume.isRAMStorage()) {\n           LOG.warn(\"Caching not supported on block with id \" + blockId +\n-              \" since the volume is backed by RAM.\");\n+              \" since the volume is backed by RAM_DISK or NVDIMM.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dea6c63a91a7d0815dbdb405f8cf1c219365aeca"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY2MTQyMg==", "bodyText": "nit: Add a blank line before every new rack, aka\n  \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r1\",\n  \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r2\",\n  \"/l3/d5/r1\", \"/l3/d5/r1\", \"/l3/d5/r2\"};\n\ncan be\n  \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r1\",\n  \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r2\",\n  \n  \"/l3/d5/r1\", \"/l3/d5/r1\", \"/l3/d5/r2\"};\n\nSame to the hosts, please  make the last three hosts in a new line so that racks, hosts, and types can be easily read with eyeballs.", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r491661422", "createdAt": "2020-09-20T07:03:08Z", "author": {"login": "liuml07"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java", "diffHunk": "@@ -64,37 +64,42 @@ public void setupDatanodes() {\n     final String[] racks = {\n         \"/l1/d1/r1\", \"/l1/d1/r1\", \"/l1/d1/r2\", \"/l1/d1/r2\", \"/l1/d1/r2\",\n \n-        \"/l1/d2/r3\", \"/l1/d2/r3\", \"/l1/d2/r3\",\n+        \"/l1/d2/r3\", \"/l1/d2/r3\", \"/l1/d2/r3\", \"/l1/d2/r3\",\n \n         \"/l2/d3/r1\", \"/l2/d3/r2\", \"/l2/d3/r3\", \"/l2/d3/r4\", \"/l2/d3/r5\",\n \n         \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r1\",\n-        \"/l2/d4/r1\", \"/l2/d4/r1\"};\n+        \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r2\",\n+        \"/l3/d5/r1\", \"/l3/d5/r1\", \"/l3/d5/r2\"};", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dea6c63a91a7d0815dbdb405f8cf1c219365aeca"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY2MTU3Mg==", "bodyText": "nit: replace with assertEquals(4, d2info.get(\"r3\").size());", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r491661572", "createdAt": "2020-09-20T07:05:17Z", "author": {"login": "liuml07"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java", "diffHunk": "@@ -120,10 +125,11 @@ public void testGetStorageTypeInfo() throws Exception {\n     HashMap<String, EnumMap<StorageType, Integer>> d2info =\n         d2.getChildrenStorageInfo();\n     assertEquals(1, d2info.keySet().size());\n-    assertTrue(d2info.get(\"r3\").size() == 3);\n+    assertTrue(d2info.get(\"r3\").size() == 4);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dea6c63a91a7d0815dbdb405f8cf1c219365aeca"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY2MzI0NQ==", "bodyText": "nit: why not use 34.34.34.34?", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r491663245", "createdAt": "2020-09-20T07:27:27Z", "author": {"login": "liuml07"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java", "diffHunk": "@@ -182,15 +212,16 @@ public void testGetStorageTypeInfo() throws Exception {\n    */\n   @Test\n   public void testAddAndRemoveTopology() throws Exception {\n-    String[] newRack = {\"/l1/d1/r1\", \"/l1/d1/r3\", \"/l1/d3/r3\", \"/l1/d3/r3\"};\n-    String[] newHost = {\"nhost1\", \"nhost2\", \"nhost3\", \"nhost4\"};\n+    String[] newRack = {\"/l1/d1/r1\", \"/l1/d1/r3\", \"/l1/d3/r3\", \"/l1/d3/r3\",\n+        \"/l1/d3/r4\"};\n+    String[] newHost = {\"nhost1\", \"nhost2\", \"nhost3\", \"nhost4\", \"nhost5\"};\n     String[] newips = {\"30.30.30.30\", \"31.31.31.31\", \"32.32.32.32\",\n-        \"33.33.33.33\"};\n+        \"33.33.33.33\", \"33.33.33.34\"};", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dea6c63a91a7d0815dbdb405f8cf1c219365aeca"}, "originalPosition": 142}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY2MzI4MA==", "bodyText": "nit: let's have space before and after < operator", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r491663280", "createdAt": "2020-09-20T07:27:57Z", "author": {"login": "liuml07"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java", "diffHunk": "@@ -182,15 +212,16 @@ public void testGetStorageTypeInfo() throws Exception {\n    */\n   @Test\n   public void testAddAndRemoveTopology() throws Exception {\n-    String[] newRack = {\"/l1/d1/r1\", \"/l1/d1/r3\", \"/l1/d3/r3\", \"/l1/d3/r3\"};\n-    String[] newHost = {\"nhost1\", \"nhost2\", \"nhost3\", \"nhost4\"};\n+    String[] newRack = {\"/l1/d1/r1\", \"/l1/d1/r3\", \"/l1/d3/r3\", \"/l1/d3/r3\",\n+        \"/l1/d3/r4\"};\n+    String[] newHost = {\"nhost1\", \"nhost2\", \"nhost3\", \"nhost4\", \"nhost5\"};\n     String[] newips = {\"30.30.30.30\", \"31.31.31.31\", \"32.32.32.32\",\n-        \"33.33.33.33\"};\n+        \"33.33.33.33\", \"33.33.33.34\"};\n     StorageType[] newTypes = {StorageType.DISK, StorageType.SSD,\n-        StorageType.SSD, StorageType.SSD};\n-    DatanodeDescriptor[] newDD = new DatanodeDescriptor[4];\n+        StorageType.SSD, StorageType.SSD, StorageType.NVDIMM};\n+    DatanodeDescriptor[] newDD = new DatanodeDescriptor[5];\n \n-    for (int i = 0; i<4; i++) {\n+    for (int i = 0; i<5; i++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dea6c63a91a7d0815dbdb405f8cf1c219365aeca"}, "originalPosition": 150}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY2MzU0OQ==", "bodyText": "As this if-else if-else if-else getting longer, let's use switch case?", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r491663549", "createdAt": "2020-09-20T07:31:21Z", "author": {"login": "liuml07"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockStatsMXBean.java", "diffHunk": "@@ -145,9 +150,11 @@ public void testStorageTypeStatsJMX() throws Exception {\n       Map<String,Object> storageTypeStats = (Map<String,Object>)entry.get(\"value\");\n       typesPresent.add(storageType);\n       if (storageType.equals(\"ARCHIVE\") || storageType.equals(\"DISK\") ) {\n-        assertEquals(3l, storageTypeStats.get(\"nodesInService\"));\n+        assertEquals(3L, storageTypeStats.get(\"nodesInService\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dea6c63a91a7d0815dbdb405f8cf1c219365aeca"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY2MzgxMQ==", "bodyText": "The invalid 8th URI has ending space deliberately for testing. Let's keep it, aka\n\"[ram_disk]/dir4,[disk]/dir5, [disk] /dir6, [disk] , [nvdimm]/dir7\";", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r491663811", "createdAt": "2020-09-20T07:34:55Z", "author": {"login": "liuml07"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataDirs.java", "diffHunk": "@@ -43,14 +43,15 @@ public void testDataDirParsing() throws Throwable {\n \n     File dir5 = new File(\"/dir5\");\n     File dir6 = new File(\"/dir6\");\n+    File dir7 = new File(\"/dir7\");\n     // Verify that a valid string is correctly parsed, and that storage\n     // type is not case-sensitive and we are able to handle white-space between\n     // storage type and URI.\n     String locations1 = \"[disk]/dir0,[DISK]/dir1,[sSd]/dir2,[disK]/dir3,\" +\n-            \"[ram_disk]/dir4,[disk]/dir5, [disk] /dir6, [disk] \";\n+            \"[ram_disk]/dir4,[disk]/dir5, [disk] /dir6, [disk], [nvdimm]/dir7\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dea6c63a91a7d0815dbdb405f8cf1c219365aeca"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY2MzkxMA==", "bodyText": "nit: Let's use better assertion statement assertEquals(3L, volume5.getReserved());", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r491663910", "createdAt": "2020-09-20T07:35:59Z", "author": {"login": "liuml07"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsVolumeList.java", "diffHunk": "@@ -202,6 +205,14 @@ public void testDfsReservedForDifferentStorageTypes() throws IOException {\n         .setConf(conf)\n         .build();\n     assertEquals(\"\", 100L, volume4.getReserved());\n+    FsVolumeImpl volume5 = new FsVolumeImplBuilder().setDataset(dataset)\n+        .setStorageDirectory(\n+            new StorageDirectory(\n+                StorageLocation.parse(\"[NVDIMM]\"+volDir.getPath())))\n+        .setStorageID(\"storage-id\")\n+        .setConf(conf)\n+        .build();\n+    assertEquals(\"\", 3L, volume5.getReserved());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dea6c63a91a7d0815dbdb405f8cf1c219365aeca"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY2NDEyNg==", "bodyText": "Also add a few happy test?", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r491664126", "createdAt": "2020-09-20T07:39:17Z", "author": {"login": "liuml07"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHDFS.java", "diffHunk": "@@ -1103,6 +1103,8 @@ public void testSetQuota() throws Exception {\n         () -> webHdfs.setQuotaByStorageType(path, StorageType.SSD, -100));\n     LambdaTestUtils.intercept(IllegalArgumentException.class,\n         () -> webHdfs.setQuotaByStorageType(path, StorageType.RAM_DISK, 100));\n+    LambdaTestUtils.intercept(IllegalArgumentException.class,\n+        () -> webHdfs.setQuotaByStorageType(path, StorageType.NVDIMM, -100));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dea6c63a91a7d0815dbdb405f8cf1c219365aeca"}, "originalPosition": 5}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk1MDM2OTUy", "url": "https://github.com/apache/hadoop/pull/2189#pullrequestreview-495036952", "createdAt": "2020-09-23T20:49:10Z", "commit": {"oid": "82464cb4e6f4c57cb5de118b47033f72ead8096a"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "43bf6723c7c47e13be801781ae99724f1e6725e5", "author": {"user": {"login": "huangtianhua", "name": "huangtianhua"}}, "url": "https://github.com/apache/hadoop/commit/43bf6723c7c47e13be801781ae99724f1e6725e5", "committedDate": "2020-09-24T01:23:35Z", "message": "HDFS-15025. Applying NVDIMM storage media to HDFS\n\nThe non-volatile memory NVDIMM is faster than SSD,\nit can be used simultaneously with RAM, DISK and SSD.\nStoring the data of HDFS on NVDIMM directly will get\nbetter response rate and reliability.\n\nCo-authored-by: YaYun-Wang <yywangyayun@163.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "17fd047e62c287fbf605916f6676561df1d6e8ca", "author": {"user": {"login": "YaYun-Wang", "name": null}}, "url": "https://github.com/apache/hadoop/commit/17fd047e62c287fbf605916f6676561df1d6e8ca", "committedDate": "2020-09-24T01:23:35Z", "message": "Update ArchivalStorage.md"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "76d2716777e822a3c975879f4c28b2c6d9c298c2", "author": {"user": {"login": "YaYun-Wang", "name": null}}, "url": "https://github.com/apache/hadoop/commit/76d2716777e822a3c975879f4c28b2c6d9c298c2", "committedDate": "2020-09-24T01:23:35Z", "message": "Update ArchivalStorage.md\n\nput NVDIMM to the end of all storage types"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "075ee8a07ec7c1f994e478824cdee17b93dad959", "author": {"user": {"login": "YaYun-Wang", "name": null}}, "url": "https://github.com/apache/hadoop/commit/075ee8a07ec7c1f994e478824cdee17b93dad959", "committedDate": "2020-09-24T01:23:35Z", "message": "Update FsDatasetImpl.java"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "180cc036a21a4263eb030ca19f7347e5718f0a8b", "author": {"user": {"login": "YaYun-Wang", "name": null}}, "url": "https://github.com/apache/hadoop/commit/180cc036a21a4263eb030ca19f7347e5718f0a8b", "committedDate": "2020-09-24T01:23:35Z", "message": "Update TestDFSNetworkTopology.java\n\nadd a blank line before every new rack"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0d753adac011ddf898b8565515998bfe775dd35f", "author": {"user": {"login": "YaYun-Wang", "name": null}}, "url": "https://github.com/apache/hadoop/commit/0d753adac011ddf898b8565515998bfe775dd35f", "committedDate": "2020-09-24T01:23:35Z", "message": "Update TestDFSNetworkTopology.java"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8f9ace07121f69aa55f426169474ab5c33f4cb5e", "author": {"user": {"login": "YaYun-Wang", "name": null}}, "url": "https://github.com/apache/hadoop/commit/8f9ace07121f69aa55f426169474ab5c33f4cb5e", "committedDate": "2020-09-24T01:23:35Z", "message": "Update TestDFSNetworkTopology.java"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "842f8c72d17b55ebf318a0d19f7d4a3b710f4f0c", "author": {"user": {"login": "YaYun-Wang", "name": null}}, "url": "https://github.com/apache/hadoop/commit/842f8c72d17b55ebf318a0d19f7d4a3b710f4f0c", "committedDate": "2020-09-24T01:23:35Z", "message": "Update TestBlockStatsMXBean.java"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8b75c799db9157ae3263d07b678878cad69560ef", "author": {"user": {"login": "YaYun-Wang", "name": null}}, "url": "https://github.com/apache/hadoop/commit/8b75c799db9157ae3263d07b678878cad69560ef", "committedDate": "2020-09-24T01:23:35Z", "message": "Update TestDataDirs.java\n\nadd one space after 'disk'"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "241faf4eb72b6870607f6a6fe9e2a1d9a9c4752d", "author": {"user": {"login": "YaYun-Wang", "name": null}}, "url": "https://github.com/apache/hadoop/commit/241faf4eb72b6870607f6a6fe9e2a1d9a9c4752d", "committedDate": "2020-09-24T01:23:35Z", "message": "Update TestWebHDFS.java\n\nadd  NVDIMM test for setQuotaByStorageType method"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7771df7d4968641fac6f2652717cce3d8cb6a0e5", "author": {"user": {"login": "YaYun-Wang", "name": null}}, "url": "https://github.com/apache/hadoop/commit/7771df7d4968641fac6f2652717cce3d8cb6a0e5", "committedDate": "2020-09-24T01:23:35Z", "message": "Update TestFsVolumeList.java\n\nupdate assertEquals() with two parameters"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0a87e83673394ded5f0104d4ec295262a1b673a8", "author": {"user": {"login": "huangtianhua", "name": "huangtianhua"}}, "url": "https://github.com/apache/hadoop/commit/0a87e83673394ded5f0104d4ec295262a1b673a8", "committedDate": "2020-09-24T01:44:54Z", "message": "Fix checkstyle errors"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "82464cb4e6f4c57cb5de118b47033f72ead8096a", "author": {"user": {"login": "YaYun-Wang", "name": null}}, "url": "https://github.com/apache/hadoop/commit/82464cb4e6f4c57cb5de118b47033f72ead8096a", "committedDate": "2020-09-23T01:09:01Z", "message": "Update TestFsVolumeList.java\n\nupdate assertEquals() with two parameters"}, "afterCommit": {"oid": "0a87e83673394ded5f0104d4ec295262a1b673a8", "author": {"user": {"login": "huangtianhua", "name": "huangtianhua"}}, "url": "https://github.com/apache/hadoop/commit/0a87e83673394ded5f0104d4ec295262a1b673a8", "committedDate": "2020-09-24T01:44:54Z", "message": "Fix checkstyle errors"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk1MjIwMTE2", "url": "https://github.com/apache/hadoop/pull/2189#pullrequestreview-495220116", "createdAt": "2020-09-24T04:59:34Z", "commit": {"oid": "0a87e83673394ded5f0104d4ec295262a1b673a8"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3846, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}