{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE0Mjk2NjQ4", "number": 1999, "title": "HADOOP-14566. Add seek support for SFTP FileSystem.", "bodyText": "This PR implements seek method for SFTP File System which supports both forward and backward lazy seeks.\nNormally subsequent reads proceed reading from the position where the previous read finished, meaning we can avoid making seek operations in this case. We only need to seek when the current FsDataInputstream#getPos() doesn't equal a requested position to read from.\nWhenever seek is called it's deferred until the next read operation. If a consecutive read happens at the same position where the previous read finished then it merely proceeds reading. If a consecutive read happens at the position greater than the position where the previous read finished we seek forward to the requested position. And finally, if a consecutive read happens at the position smaller than the current inputstream position then we close the current inputstream, reopen a new one and seek to the requested position.", "createdAt": "2020-05-06T19:44:50Z", "url": "https://github.com/apache/hadoop/pull/1999", "merged": true, "mergeCommit": {"oid": "97c98ce531ccb27581cbb10260d7307b0ccd199c"}, "closed": true, "closedAt": "2020-06-03T10:37:41Z", "author": {"login": "mpryahin"}, "timelineItems": {"totalCount": 22, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcesSLGgH2gAyNDE0Mjk2NjQ4OjdmOTVlMjQ2ODczMDQ5NzNjZDg5OTZmZmFlYjhmMzVmNDRjZDFlNzc=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcnVMY4AH2gAyNDE0Mjk2NjQ4Ojg2YmY4OTZhZmUyMGM0ZDM5YWY5YTAzOWRhYjhjNjk1Zjk4ZmViNTI=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "7f95e24687304973cd8996ffaeb8f35f44cd1e77", "author": {"user": {"login": "mpryahin", "name": "Mike"}}, "url": "https://github.com/apache/hadoop/commit/7f95e24687304973cd8996ffaeb8f35f44cd1e77", "committedDate": "2020-05-06T17:39:29Z", "message": "seek method implementation"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e0fa916714bc4421ba8b1cba7ad3d96272401bfc", "author": {"user": {"login": "mpryahin", "name": "Mike"}}, "url": "https://github.com/apache/hadoop/commit/e0fa916714bc4421ba8b1cba7ad3d96272401bfc", "committedDate": "2020-05-06T18:49:58Z", "message": "fixed code style issues"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9fa1307d554efc04ab20f190c541224937324d43", "author": {"user": {"login": "mpryahin", "name": "Mike"}}, "url": "https://github.com/apache/hadoop/commit/9fa1307d554efc04ab20f190c541224937324d43", "committedDate": "2020-05-06T19:31:51Z", "message": "close ssh server only in case it is initialised"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9896efd6855b6552c1bce20cfac4e4641c31dd6d", "author": {"user": {"login": "mpryahin", "name": "Mike"}}, "url": "https://github.com/apache/hadoop/commit/9896efd6855b6552c1bce20cfac4e4641c31dd6d", "committedDate": "2020-05-07T06:44:45Z", "message": "fixed broken thread safety"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "46a2d51ee5ec0c8162154bf3ecfc0f9bd151f057", "author": {"user": {"login": "mpryahin", "name": "Mike"}}, "url": "https://github.com/apache/hadoop/commit/46a2d51ee5ec0c8162154bf3ecfc0f9bd151f057", "committedDate": "2020-05-07T11:27:58Z", "message": "Fixed broken tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fb098c73e68b04d8054f0cd695ab0194d0ddb425", "author": {"user": {"login": "mpryahin", "name": "Mike"}}, "url": "https://github.com/apache/hadoop/commit/fb098c73e68b04d8054f0cd695ab0194d0ddb425", "committedDate": "2020-05-11T07:58:22Z", "message": "Merge branch 'trunk' of https://github.com/apache/hadoop into HADOOP-14566"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6bad39285e95114e7b39cac186f536d06f784b29", "author": {"user": {"login": "mpryahin", "name": "Mike"}}, "url": "https://github.com/apache/hadoop/commit/6bad39285e95114e7b39cac186f536d06f784b29", "committedDate": "2020-05-14T17:52:26Z", "message": "Merge branch 'trunk' of https://github.com/apache/hadoop into HADOOP-14566"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a9bc169c98a1f9ef4e3c438a86445106af42bf0b", "author": {"user": {"login": "mpryahin", "name": "Mike"}}, "url": "https://github.com/apache/hadoop/commit/a9bc169c98a1f9ef4e3c438a86445106af42bf0b", "committedDate": "2020-05-14T18:26:22Z", "message": "Merge branch 'trunk' of https://github.com/apache/hadoop into HADOOP-14566"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "773284d78979ed5eede930e95865769ed830a14f", "author": {"user": {"login": "mpryahin", "name": "Mike"}}, "url": "https://github.com/apache/hadoop/commit/773284d78979ed5eede930e95865769ed830a14f", "committedDate": "2020-05-15T10:32:29Z", "message": "Merge branch 'trunk' of https://github.com/apache/hadoop into HADOOP-14566"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5bf201c250705f27f22fb5456a521d55b5f6c54d", "author": {"user": {"login": "mpryahin", "name": "Mike"}}, "url": "https://github.com/apache/hadoop/commit/5bf201c250705f27f22fb5456a521d55b5f6c54d", "committedDate": "2020-05-18T11:49:20Z", "message": "Merge branch 'trunk' of https://github.com/apache/hadoop into HADOOP-14566"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bd5f4dfc4a8e69a35260dd6abf49ed9ce429d681", "author": {"user": {"login": "mpryahin", "name": "Mike"}}, "url": "https://github.com/apache/hadoop/commit/bd5f4dfc4a8e69a35260dd6abf49ed9ce429d681", "committedDate": "2020-05-18T14:31:41Z", "message": "Merge branch 'trunk' of https://github.com/apache/hadoop into HADOOP-14566"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "576093738eff65b69a4ddae340f8a25ef4ced4d6", "author": {"user": {"login": "mpryahin", "name": "Mike"}}, "url": "https://github.com/apache/hadoop/commit/576093738eff65b69a4ddae340f8a25ef4ced4d6", "committedDate": "2020-05-18T16:08:54Z", "message": "Merge branch 'trunk' of https://github.com/apache/hadoop into HADOOP-14566"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "86bdff3d89be5324a213654947a2ba7c4fd6891b", "author": {"user": {"login": "mpryahin", "name": "Mike"}}, "url": "https://github.com/apache/hadoop/commit/86bdff3d89be5324a213654947a2ba7c4fd6891b", "committedDate": "2020-05-18T17:21:43Z", "message": "Merge branch 'trunk' of https://github.com/apache/hadoop into HADOOP-14566"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE0NjE3MjM1", "url": "https://github.com/apache/hadoop/pull/1999#pullrequestreview-414617235", "createdAt": "2020-05-19T16:24:24Z", "commit": {"oid": "86bdff3d89be5324a213654947a2ba7c4fd6891b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNjoyNDoyNFrOGXoewA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNjoyNDoyNFrOGXoewA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQzMzY2NA==", "bodyText": "nit: add a gap between org.apache imports and the others", "url": "https://github.com/apache/hadoop/pull/1999#discussion_r427433664", "createdAt": "2020-05-19T16:24:24Z", "author": {"login": "steveloughran"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java", "diffHunk": "@@ -15,86 +15,113 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n+\n package org.apache.hadoop.fs.sftp;\n \n+import java.io.EOFException;\n import java.io.IOException;\n import java.io.InputStream;\n+import java.io.UncheckedIOException;\n \n+import com.jcraft.jsch.ChannelSftp;\n+import com.jcraft.jsch.SftpATTRS;\n+import com.jcraft.jsch.SftpException;\n+import org.apache.hadoop.fs.FSExceptionMessages;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "86bdff3d89be5324a213654947a2ba7c4fd6891b"}, "originalPosition": 15}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE0NjE3ODU1", "url": "https://github.com/apache/hadoop/pull/1999#pullrequestreview-414617855", "createdAt": "2020-05-19T16:25:08Z", "commit": {"oid": "86bdff3d89be5324a213654947a2ba7c4fd6891b"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNjoyNTowOFrOGXogug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNjozNjowN1rOGXo-mg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQzNDE3MA==", "bodyText": "why the double wrap?", "url": "https://github.com/apache/hadoop/pull/1999#discussion_r427434170", "createdAt": "2020-05-19T16:25:08Z", "author": {"login": "steveloughran"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java", "diffHunk": "@@ -15,86 +15,113 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n+\n package org.apache.hadoop.fs.sftp;\n \n+import java.io.EOFException;\n import java.io.IOException;\n import java.io.InputStream;\n+import java.io.UncheckedIOException;\n \n+import com.jcraft.jsch.ChannelSftp;\n+import com.jcraft.jsch.SftpATTRS;\n+import com.jcraft.jsch.SftpException;\n+import org.apache.hadoop.fs.FSExceptionMessages;\n import org.apache.hadoop.fs.FSInputStream;\n import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n \n /** SFTP FileSystem input stream. */\n class SFTPInputStream extends FSInputStream {\n \n-  public static final String E_SEEK_NOTSUPPORTED = \"Seek not supported\";\n-  public static final String E_NULL_INPUTSTREAM = \"Null InputStream\";\n-  public static final String E_STREAM_CLOSED = \"Stream closed\";\n-\n+  private final ChannelSftp channel;\n+  private final Path path;\n   private InputStream wrappedStream;\n   private FileSystem.Statistics stats;\n   private boolean closed;\n   private long pos;\n-\n-  SFTPInputStream(InputStream stream,  FileSystem.Statistics stats) {\n-\n-    if (stream == null) {\n-      throw new IllegalArgumentException(E_NULL_INPUTSTREAM);\n+  private long nextPos;\n+  private long contentLength;\n+\n+  SFTPInputStream(ChannelSftp channel, Path path, FileSystem.Statistics stats) {\n+    try {\n+      this.channel = channel;\n+      this.path = path;\n+      this.stats = stats;\n+      this.wrappedStream = channel.get(path.toUri().getPath());\n+      SftpATTRS stat = channel.lstat(path.toString());\n+      this.contentLength = stat.getSize();\n+    } catch (SftpException e) {\n+      throw new UncheckedIOException(new IOException(e));\n     }\n-    this.wrappedStream = stream;\n-    this.stats = stats;\n+  }\n \n-    this.pos = 0;\n-    this.closed = false;\n+  @Override\n+  public synchronized void seek(long position) throws IOException {\n+    checkNotClosed();\n+    if (position < 0) {\n+      throw new EOFException(FSExceptionMessages.NEGATIVE_SEEK);\n+    }\n+    nextPos = position;\n   }\n \n   @Override\n-  public void seek(long position) throws IOException {\n-    throw new IOException(E_SEEK_NOTSUPPORTED);\n+  public synchronized int available() throws IOException {\n+    checkNotClosed();\n+    long remaining = contentLength - nextPos;\n+    if (remaining > Integer.MAX_VALUE) {\n+      return Integer.MAX_VALUE;\n+    }\n+    return (int) remaining;\n+  }\n+\n+  private void seekInternal() throws IOException {\n+    if (pos == nextPos) {\n+      return;\n+    }\n+    if (nextPos > pos) {\n+      long skipped = wrappedStream.skip(nextPos - pos);\n+      pos = pos + skipped;\n+    }\n+    if (nextPos < pos) {\n+      wrappedStream.close();\n+      try {\n+        wrappedStream = channel.get(path.toUri().getPath());\n+        pos = wrappedStream.skip(nextPos);\n+      } catch (SftpException e) {\n+        throw new UncheckedIOException(new IOException(e));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "86bdff3d89be5324a213654947a2ba7c4fd6891b"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQzNjQ1MQ==", "bodyText": "why the double wrap?", "url": "https://github.com/apache/hadoop/pull/1999#discussion_r427436451", "createdAt": "2020-05-19T16:28:30Z", "author": {"login": "steveloughran"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java", "diffHunk": "@@ -15,86 +15,113 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n+\n package org.apache.hadoop.fs.sftp;\n \n+import java.io.EOFException;\n import java.io.IOException;\n import java.io.InputStream;\n+import java.io.UncheckedIOException;\n \n+import com.jcraft.jsch.ChannelSftp;\n+import com.jcraft.jsch.SftpATTRS;\n+import com.jcraft.jsch.SftpException;\n+import org.apache.hadoop.fs.FSExceptionMessages;\n import org.apache.hadoop.fs.FSInputStream;\n import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n \n /** SFTP FileSystem input stream. */\n class SFTPInputStream extends FSInputStream {\n \n-  public static final String E_SEEK_NOTSUPPORTED = \"Seek not supported\";\n-  public static final String E_NULL_INPUTSTREAM = \"Null InputStream\";\n-  public static final String E_STREAM_CLOSED = \"Stream closed\";\n-\n+  private final ChannelSftp channel;\n+  private final Path path;\n   private InputStream wrappedStream;\n   private FileSystem.Statistics stats;\n   private boolean closed;\n   private long pos;\n-\n-  SFTPInputStream(InputStream stream,  FileSystem.Statistics stats) {\n-\n-    if (stream == null) {\n-      throw new IllegalArgumentException(E_NULL_INPUTSTREAM);\n+  private long nextPos;\n+  private long contentLength;\n+\n+  SFTPInputStream(ChannelSftp channel, Path path, FileSystem.Statistics stats) {\n+    try {\n+      this.channel = channel;\n+      this.path = path;\n+      this.stats = stats;\n+      this.wrappedStream = channel.get(path.toUri().getPath());\n+      SftpATTRS stat = channel.lstat(path.toString());\n+      this.contentLength = stat.getSize();\n+    } catch (SftpException e) {\n+      throw new UncheckedIOException(new IOException(e));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "86bdff3d89be5324a213654947a2ba7c4fd6891b"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQzNzY2MA==", "bodyText": "shouldn't this be set in core-default.xml already?\nif not, sftp:// urls would break. (yes, i know every stack overflow spark example does this, but that is just superstition)", "url": "https://github.com/apache/hadoop/pull/1999#discussion_r427437660", "createdAt": "2020-05-19T16:30:16Z", "author": {"login": "steveloughran"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/sftp/SFTPContract.java", "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *       http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.contract.sftp;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.contract.AbstractFSContract;\n+import org.apache.hadoop.fs.sftp.SFTPFileSystem;\n+import org.apache.sshd.common.NamedFactory;\n+import org.apache.sshd.server.SshServer;\n+import org.apache.sshd.server.auth.UserAuth;\n+import org.apache.sshd.server.auth.password.UserAuthPasswordFactory;\n+import org.apache.sshd.server.keyprovider.SimpleGeneratorHostKeyProvider;\n+import org.apache.sshd.server.subsystem.sftp.SftpSubsystemFactory;\n+\n+public class SFTPContract extends AbstractFSContract {\n+\n+  private String testDataDir = new FileSystemTestHelper().getTestRootDir();\n+  private Configuration conf;\n+  public static final String CONTRACT_XML = \"contract/sftp.xml\";\n+  private SshServer sshd;\n+\n+  public SFTPContract(Configuration conf) {\n+    super(conf);\n+    addConfResource(CONTRACT_XML);\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void init() throws IOException {\n+    sshd = SshServer.setUpDefaultServer();\n+    // ask OS to assign a port\n+    sshd.setPort(0);\n+    sshd.setKeyPairProvider(new SimpleGeneratorHostKeyProvider());\n+\n+    List<NamedFactory<UserAuth>> userAuthFactories = new ArrayList<>();\n+    userAuthFactories.add(new UserAuthPasswordFactory());\n+\n+    sshd.setUserAuthFactories(userAuthFactories);\n+    sshd.setPasswordAuthenticator((username, password, session) ->\n+        username.equals(\"user\") && password.equals(\"password\")\n+    );\n+\n+    sshd.setSubsystemFactories(\n+        Collections.singletonList(new SftpSubsystemFactory()));\n+\n+    sshd.start();\n+    int port = sshd.getPort();\n+\n+    conf.setClass(\"fs.sftp.impl\", SFTPFileSystem.class, FileSystem.class);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "86bdff3d89be5324a213654947a2ba7c4fd6891b"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQzODE3MQ==", "bodyText": "nit: make a variable and reuse", "url": "https://github.com/apache/hadoop/pull/1999#discussion_r427438171", "createdAt": "2020-05-19T16:30:59Z", "author": {"login": "steveloughran"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/sftp/SFTPContract.java", "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *       http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.contract.sftp;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.contract.AbstractFSContract;\n+import org.apache.hadoop.fs.sftp.SFTPFileSystem;\n+import org.apache.sshd.common.NamedFactory;\n+import org.apache.sshd.server.SshServer;\n+import org.apache.sshd.server.auth.UserAuth;\n+import org.apache.sshd.server.auth.password.UserAuthPasswordFactory;\n+import org.apache.sshd.server.keyprovider.SimpleGeneratorHostKeyProvider;\n+import org.apache.sshd.server.subsystem.sftp.SftpSubsystemFactory;\n+\n+public class SFTPContract extends AbstractFSContract {\n+\n+  private String testDataDir = new FileSystemTestHelper().getTestRootDir();\n+  private Configuration conf;\n+  public static final String CONTRACT_XML = \"contract/sftp.xml\";\n+  private SshServer sshd;\n+\n+  public SFTPContract(Configuration conf) {\n+    super(conf);\n+    addConfResource(CONTRACT_XML);\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void init() throws IOException {\n+    sshd = SshServer.setUpDefaultServer();\n+    // ask OS to assign a port\n+    sshd.setPort(0);\n+    sshd.setKeyPairProvider(new SimpleGeneratorHostKeyProvider());\n+\n+    List<NamedFactory<UserAuth>> userAuthFactories = new ArrayList<>();\n+    userAuthFactories.add(new UserAuthPasswordFactory());\n+\n+    sshd.setUserAuthFactories(userAuthFactories);\n+    sshd.setPasswordAuthenticator((username, password, session) ->\n+        username.equals(\"user\") && password.equals(\"password\")\n+    );\n+\n+    sshd.setSubsystemFactories(\n+        Collections.singletonList(new SftpSubsystemFactory()));\n+\n+    sshd.start();\n+    int port = sshd.getPort();\n+\n+    conf.setClass(\"fs.sftp.impl\", SFTPFileSystem.class, FileSystem.class);\n+    conf.setInt(\"fs.sftp.host.port\", port);\n+    conf.setBoolean(\"fs.sftp.impl.disable.cache\", true);\n+  }\n+\n+  @Override\n+  public void teardown() throws IOException {\n+    if (sshd != null) {\n+      sshd.stop();\n+    }\n+  }\n+\n+  @Override\n+  public FileSystem getTestFileSystem() throws IOException {\n+    return FileSystem.get(URI.create(\"sftp://user:password@localhost\"), conf);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "86bdff3d89be5324a213654947a2ba7c4fd6891b"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ0MDMwNA==", "bodyText": "shame we didn't declare this as raising an ioe; probably too late now", "url": "https://github.com/apache/hadoop/pull/1999#discussion_r427440304", "createdAt": "2020-05-19T16:33:46Z", "author": {"login": "steveloughran"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/sftp/SFTPContract.java", "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *       http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.contract.sftp;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.contract.AbstractFSContract;\n+import org.apache.hadoop.fs.sftp.SFTPFileSystem;\n+import org.apache.sshd.common.NamedFactory;\n+import org.apache.sshd.server.SshServer;\n+import org.apache.sshd.server.auth.UserAuth;\n+import org.apache.sshd.server.auth.password.UserAuthPasswordFactory;\n+import org.apache.sshd.server.keyprovider.SimpleGeneratorHostKeyProvider;\n+import org.apache.sshd.server.subsystem.sftp.SftpSubsystemFactory;\n+\n+public class SFTPContract extends AbstractFSContract {\n+\n+  private String testDataDir = new FileSystemTestHelper().getTestRootDir();\n+  private Configuration conf;\n+  public static final String CONTRACT_XML = \"contract/sftp.xml\";\n+  private SshServer sshd;\n+\n+  public SFTPContract(Configuration conf) {\n+    super(conf);\n+    addConfResource(CONTRACT_XML);\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void init() throws IOException {\n+    sshd = SshServer.setUpDefaultServer();\n+    // ask OS to assign a port\n+    sshd.setPort(0);\n+    sshd.setKeyPairProvider(new SimpleGeneratorHostKeyProvider());\n+\n+    List<NamedFactory<UserAuth>> userAuthFactories = new ArrayList<>();\n+    userAuthFactories.add(new UserAuthPasswordFactory());\n+\n+    sshd.setUserAuthFactories(userAuthFactories);\n+    sshd.setPasswordAuthenticator((username, password, session) ->\n+        username.equals(\"user\") && password.equals(\"password\")\n+    );\n+\n+    sshd.setSubsystemFactories(\n+        Collections.singletonList(new SftpSubsystemFactory()));\n+\n+    sshd.start();\n+    int port = sshd.getPort();\n+\n+    conf.setClass(\"fs.sftp.impl\", SFTPFileSystem.class, FileSystem.class);\n+    conf.setInt(\"fs.sftp.host.port\", port);\n+    conf.setBoolean(\"fs.sftp.impl.disable.cache\", true);\n+  }\n+\n+  @Override\n+  public void teardown() throws IOException {\n+    if (sshd != null) {\n+      sshd.stop();\n+    }\n+  }\n+\n+  @Override\n+  public FileSystem getTestFileSystem() throws IOException {\n+    return FileSystem.get(URI.create(\"sftp://user:password@localhost\"), conf);\n+  }\n+\n+  @Override\n+  public String getScheme() {\n+    return \"sftp\";\n+  }\n+\n+  @Override\n+  public Path getTestPath() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "86bdff3d89be5324a213654947a2ba7c4fd6891b"}, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ0MDUzMg==", "bodyText": "nit: sftp", "url": "https://github.com/apache/hadoop/pull/1999#discussion_r427440532", "createdAt": "2020-05-19T16:34:08Z", "author": {"login": "steveloughran"}, "path": "hadoop-common-project/hadoop-common/src/test/resources/contract/sftp.xml", "diffHunk": "@@ -0,0 +1,79 @@\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one\n+  ~  or more contributor license agreements.  See the NOTICE file\n+  ~  distributed with this work for additional information\n+  ~  regarding copyright ownership.  The ASF licenses this file\n+  ~  to you under the Apache License, Version 2.0 (the\n+  ~  \"License\"); you may not use this file except in compliance\n+  ~  with the License.  You may obtain a copy of the License at\n+  ~\n+  ~       http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~  Unless required by applicable law or agreed to in writing, software\n+  ~  distributed under the License is distributed on an \"AS IS\" BASIS,\n+  ~  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  ~  See the License for the specific language governing permissions and\n+  ~  limitations under the License.\n+  -->\n+\n+<configuration>\n+  <!--\n+  FTP -these options are for testing against a remote unix filesystem.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "86bdff3d89be5324a213654947a2ba7c4fd6891b"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ0MTgxOA==", "bodyText": "wrap super.close() with a try/finally so the channel is always diconnected", "url": "https://github.com/apache/hadoop/pull/1999#discussion_r427441818", "createdAt": "2020-05-19T16:36:07Z", "author": {"login": "steveloughran"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java", "diffHunk": "@@ -516,16 +515,14 @@ public FSDataInputStream open(Path f, int bufferSize) throws IOException {\n       disconnect(channel);\n       throw new IOException(String.format(E_PATH_DIR, f));\n     }\n-    InputStream is;\n     try {\n       // the path could be a symbolic link, so get the real path\n       absolute = new Path(\"/\", channel.realpath(absolute.toUri().getPath()));\n-\n-      is = channel.get(absolute.toUri().getPath());\n     } catch (SftpException e) {\n       throw new IOException(e);\n     }\n-    return new FSDataInputStream(new SFTPInputStream(is, statistics)){\n+    return new FSDataInputStream(\n+        new SFTPInputStream(channel, absolute, statistics)){\n       @Override\n       public void close() throws IOException {\n         super.close();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "86bdff3d89be5324a213654947a2ba7c4fd6891b"}, "originalPosition": 26}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4e823fdc32eeb209c7306eda32284653a3225b51", "author": {"user": {"login": "mpryahin", "name": "Mike"}}, "url": "https://github.com/apache/hadoop/commit/4e823fdc32eeb209c7306eda32284653a3225b51", "committedDate": "2020-05-19T17:50:46Z", "message": "code review improvements"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3bc3e90b22045739f8b367ebd2f6fd575cd613e1", "author": {"user": {"login": "mpryahin", "name": "Mike"}}, "url": "https://github.com/apache/hadoop/commit/3bc3e90b22045739f8b367ebd2f6fd575cd613e1", "committedDate": "2020-05-19T17:51:48Z", "message": "Merge branch 'trunk' of https://github.com/apache/hadoop into HADOOP-14566"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fb49a212116103c81e62b50cda6ee1bce80b15d5", "author": {"user": {"login": "mpryahin", "name": "Mike"}}, "url": "https://github.com/apache/hadoop/commit/fb49a212116103c81e62b50cda6ee1bce80b15d5", "committedDate": "2020-05-19T20:51:02Z", "message": "codestyle improvements"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE0NjU1ODMz", "url": "https://github.com/apache/hadoop/pull/1999#pullrequestreview-414655833", "createdAt": "2020-05-19T17:11:47Z", "commit": {"oid": "86bdff3d89be5324a213654947a2ba7c4fd6891b"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNzoxMTo0OFrOGXqWnQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNzozOTozMFrOGXrcAg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ2NDM0OQ==", "bodyText": "I tried to keep the contract unchanged but it seems I dropped the ball:\nPreviously an inputstream instance was created in the SFTPFileSystem#open(Path f, int bufferSize) method which threw IOExpetion in case of failure and its clients might deeply rely on this behaviour. Now as an underlying inputstream creation was moved from\nSFTPFileSystem into FSDataInputStream constructor I should've added IOExpetion to the FSDataInputStream constructor signature to keep things unchanged and backward compatible. Thanks a lot for pointing out.", "url": "https://github.com/apache/hadoop/pull/1999#discussion_r427464349", "createdAt": "2020-05-19T17:11:48Z", "author": {"login": "mpryahin"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java", "diffHunk": "@@ -15,86 +15,113 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n+\n package org.apache.hadoop.fs.sftp;\n \n+import java.io.EOFException;\n import java.io.IOException;\n import java.io.InputStream;\n+import java.io.UncheckedIOException;\n \n+import com.jcraft.jsch.ChannelSftp;\n+import com.jcraft.jsch.SftpATTRS;\n+import com.jcraft.jsch.SftpException;\n+import org.apache.hadoop.fs.FSExceptionMessages;\n import org.apache.hadoop.fs.FSInputStream;\n import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n \n /** SFTP FileSystem input stream. */\n class SFTPInputStream extends FSInputStream {\n \n-  public static final String E_SEEK_NOTSUPPORTED = \"Seek not supported\";\n-  public static final String E_NULL_INPUTSTREAM = \"Null InputStream\";\n-  public static final String E_STREAM_CLOSED = \"Stream closed\";\n-\n+  private final ChannelSftp channel;\n+  private final Path path;\n   private InputStream wrappedStream;\n   private FileSystem.Statistics stats;\n   private boolean closed;\n   private long pos;\n-\n-  SFTPInputStream(InputStream stream,  FileSystem.Statistics stats) {\n-\n-    if (stream == null) {\n-      throw new IllegalArgumentException(E_NULL_INPUTSTREAM);\n+  private long nextPos;\n+  private long contentLength;\n+\n+  SFTPInputStream(ChannelSftp channel, Path path, FileSystem.Statistics stats) {\n+    try {\n+      this.channel = channel;\n+      this.path = path;\n+      this.stats = stats;\n+      this.wrappedStream = channel.get(path.toUri().getPath());\n+      SftpATTRS stat = channel.lstat(path.toString());\n+      this.contentLength = stat.getSize();\n+    } catch (SftpException e) {\n+      throw new UncheckedIOException(new IOException(e));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQzNjQ1MQ=="}, "originalCommit": {"oid": "86bdff3d89be5324a213654947a2ba7c4fd6891b"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ2NTUzOA==", "bodyText": "my bad, i'll unwrap it. Thank you!", "url": "https://github.com/apache/hadoop/pull/1999#discussion_r427465538", "createdAt": "2020-05-19T17:13:47Z", "author": {"login": "mpryahin"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java", "diffHunk": "@@ -15,86 +15,113 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n+\n package org.apache.hadoop.fs.sftp;\n \n+import java.io.EOFException;\n import java.io.IOException;\n import java.io.InputStream;\n+import java.io.UncheckedIOException;\n \n+import com.jcraft.jsch.ChannelSftp;\n+import com.jcraft.jsch.SftpATTRS;\n+import com.jcraft.jsch.SftpException;\n+import org.apache.hadoop.fs.FSExceptionMessages;\n import org.apache.hadoop.fs.FSInputStream;\n import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n \n /** SFTP FileSystem input stream. */\n class SFTPInputStream extends FSInputStream {\n \n-  public static final String E_SEEK_NOTSUPPORTED = \"Seek not supported\";\n-  public static final String E_NULL_INPUTSTREAM = \"Null InputStream\";\n-  public static final String E_STREAM_CLOSED = \"Stream closed\";\n-\n+  private final ChannelSftp channel;\n+  private final Path path;\n   private InputStream wrappedStream;\n   private FileSystem.Statistics stats;\n   private boolean closed;\n   private long pos;\n-\n-  SFTPInputStream(InputStream stream,  FileSystem.Statistics stats) {\n-\n-    if (stream == null) {\n-      throw new IllegalArgumentException(E_NULL_INPUTSTREAM);\n+  private long nextPos;\n+  private long contentLength;\n+\n+  SFTPInputStream(ChannelSftp channel, Path path, FileSystem.Statistics stats) {\n+    try {\n+      this.channel = channel;\n+      this.path = path;\n+      this.stats = stats;\n+      this.wrappedStream = channel.get(path.toUri().getPath());\n+      SftpATTRS stat = channel.lstat(path.toString());\n+      this.contentLength = stat.getSize();\n+    } catch (SftpException e) {\n+      throw new UncheckedIOException(new IOException(e));\n     }\n-    this.wrappedStream = stream;\n-    this.stats = stats;\n+  }\n \n-    this.pos = 0;\n-    this.closed = false;\n+  @Override\n+  public synchronized void seek(long position) throws IOException {\n+    checkNotClosed();\n+    if (position < 0) {\n+      throw new EOFException(FSExceptionMessages.NEGATIVE_SEEK);\n+    }\n+    nextPos = position;\n   }\n \n   @Override\n-  public void seek(long position) throws IOException {\n-    throw new IOException(E_SEEK_NOTSUPPORTED);\n+  public synchronized int available() throws IOException {\n+    checkNotClosed();\n+    long remaining = contentLength - nextPos;\n+    if (remaining > Integer.MAX_VALUE) {\n+      return Integer.MAX_VALUE;\n+    }\n+    return (int) remaining;\n+  }\n+\n+  private void seekInternal() throws IOException {\n+    if (pos == nextPos) {\n+      return;\n+    }\n+    if (nextPos > pos) {\n+      long skipped = wrappedStream.skip(nextPos - pos);\n+      pos = pos + skipped;\n+    }\n+    if (nextPos < pos) {\n+      wrappedStream.close();\n+      try {\n+        wrappedStream = channel.get(path.toUri().getPath());\n+        pos = wrappedStream.skip(nextPos);\n+      } catch (SftpException e) {\n+        throw new UncheckedIOException(new IOException(e));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQzNDE3MA=="}, "originalCommit": {"oid": "86bdff3d89be5324a213654947a2ba7c4fd6891b"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ2ODU0MA==", "bodyText": "thanks a lot will do", "url": "https://github.com/apache/hadoop/pull/1999#discussion_r427468540", "createdAt": "2020-05-19T17:18:38Z", "author": {"login": "mpryahin"}, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java", "diffHunk": "@@ -516,16 +515,14 @@ public FSDataInputStream open(Path f, int bufferSize) throws IOException {\n       disconnect(channel);\n       throw new IOException(String.format(E_PATH_DIR, f));\n     }\n-    InputStream is;\n     try {\n       // the path could be a symbolic link, so get the real path\n       absolute = new Path(\"/\", channel.realpath(absolute.toUri().getPath()));\n-\n-      is = channel.get(absolute.toUri().getPath());\n     } catch (SftpException e) {\n       throw new IOException(e);\n     }\n-    return new FSDataInputStream(new SFTPInputStream(is, statistics)){\n+    return new FSDataInputStream(\n+        new SFTPInputStream(channel, absolute, statistics)){\n       @Override\n       public void close() throws IOException {\n         super.close();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ0MTgxOA=="}, "originalCommit": {"oid": "86bdff3d89be5324a213654947a2ba7c4fd6891b"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ3MjU3OA==", "bodyText": "will do, thank you!", "url": "https://github.com/apache/hadoop/pull/1999#discussion_r427472578", "createdAt": "2020-05-19T17:25:01Z", "author": {"login": "mpryahin"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/sftp/SFTPContract.java", "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *       http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.contract.sftp;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.contract.AbstractFSContract;\n+import org.apache.hadoop.fs.sftp.SFTPFileSystem;\n+import org.apache.sshd.common.NamedFactory;\n+import org.apache.sshd.server.SshServer;\n+import org.apache.sshd.server.auth.UserAuth;\n+import org.apache.sshd.server.auth.password.UserAuthPasswordFactory;\n+import org.apache.sshd.server.keyprovider.SimpleGeneratorHostKeyProvider;\n+import org.apache.sshd.server.subsystem.sftp.SftpSubsystemFactory;\n+\n+public class SFTPContract extends AbstractFSContract {\n+\n+  private String testDataDir = new FileSystemTestHelper().getTestRootDir();\n+  private Configuration conf;\n+  public static final String CONTRACT_XML = \"contract/sftp.xml\";\n+  private SshServer sshd;\n+\n+  public SFTPContract(Configuration conf) {\n+    super(conf);\n+    addConfResource(CONTRACT_XML);\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void init() throws IOException {\n+    sshd = SshServer.setUpDefaultServer();\n+    // ask OS to assign a port\n+    sshd.setPort(0);\n+    sshd.setKeyPairProvider(new SimpleGeneratorHostKeyProvider());\n+\n+    List<NamedFactory<UserAuth>> userAuthFactories = new ArrayList<>();\n+    userAuthFactories.add(new UserAuthPasswordFactory());\n+\n+    sshd.setUserAuthFactories(userAuthFactories);\n+    sshd.setPasswordAuthenticator((username, password, session) ->\n+        username.equals(\"user\") && password.equals(\"password\")\n+    );\n+\n+    sshd.setSubsystemFactories(\n+        Collections.singletonList(new SftpSubsystemFactory()));\n+\n+    sshd.start();\n+    int port = sshd.getPort();\n+\n+    conf.setClass(\"fs.sftp.impl\", SFTPFileSystem.class, FileSystem.class);\n+    conf.setInt(\"fs.sftp.host.port\", port);\n+    conf.setBoolean(\"fs.sftp.impl.disable.cache\", true);\n+  }\n+\n+  @Override\n+  public void teardown() throws IOException {\n+    if (sshd != null) {\n+      sshd.stop();\n+    }\n+  }\n+\n+  @Override\n+  public FileSystem getTestFileSystem() throws IOException {\n+    return FileSystem.get(URI.create(\"sftp://user:password@localhost\"), conf);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQzODE3MQ=="}, "originalCommit": {"oid": "86bdff3d89be5324a213654947a2ba7c4fd6891b"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ3Mjg3Mg==", "bodyText": "yep, thanks a lot.", "url": "https://github.com/apache/hadoop/pull/1999#discussion_r427472872", "createdAt": "2020-05-19T17:25:27Z", "author": {"login": "mpryahin"}, "path": "hadoop-common-project/hadoop-common/src/test/resources/contract/sftp.xml", "diffHunk": "@@ -0,0 +1,79 @@\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one\n+  ~  or more contributor license agreements.  See the NOTICE file\n+  ~  distributed with this work for additional information\n+  ~  regarding copyright ownership.  The ASF licenses this file\n+  ~  to you under the Apache License, Version 2.0 (the\n+  ~  \"License\"); you may not use this file except in compliance\n+  ~  with the License.  You may obtain a copy of the License at\n+  ~\n+  ~       http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~  Unless required by applicable law or agreed to in writing, software\n+  ~  distributed under the License is distributed on an \"AS IS\" BASIS,\n+  ~  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  ~  See the License for the specific language governing permissions and\n+  ~  limitations under the License.\n+  -->\n+\n+<configuration>\n+  <!--\n+  FTP -these options are for testing against a remote unix filesystem.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ0MDUzMg=="}, "originalCommit": {"oid": "86bdff3d89be5324a213654947a2ba7c4fd6891b"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ3NTY0OQ==", "bodyText": "it's not set in core-default.xml, and if not specified here sftp urls won't be resolved by sftp schema. Could you please clarify a bit what exactly you mean here? Thank you!", "url": "https://github.com/apache/hadoop/pull/1999#discussion_r427475649", "createdAt": "2020-05-19T17:29:38Z", "author": {"login": "mpryahin"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/sftp/SFTPContract.java", "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *       http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.contract.sftp;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.contract.AbstractFSContract;\n+import org.apache.hadoop.fs.sftp.SFTPFileSystem;\n+import org.apache.sshd.common.NamedFactory;\n+import org.apache.sshd.server.SshServer;\n+import org.apache.sshd.server.auth.UserAuth;\n+import org.apache.sshd.server.auth.password.UserAuthPasswordFactory;\n+import org.apache.sshd.server.keyprovider.SimpleGeneratorHostKeyProvider;\n+import org.apache.sshd.server.subsystem.sftp.SftpSubsystemFactory;\n+\n+public class SFTPContract extends AbstractFSContract {\n+\n+  private String testDataDir = new FileSystemTestHelper().getTestRootDir();\n+  private Configuration conf;\n+  public static final String CONTRACT_XML = \"contract/sftp.xml\";\n+  private SshServer sshd;\n+\n+  public SFTPContract(Configuration conf) {\n+    super(conf);\n+    addConfResource(CONTRACT_XML);\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void init() throws IOException {\n+    sshd = SshServer.setUpDefaultServer();\n+    // ask OS to assign a port\n+    sshd.setPort(0);\n+    sshd.setKeyPairProvider(new SimpleGeneratorHostKeyProvider());\n+\n+    List<NamedFactory<UserAuth>> userAuthFactories = new ArrayList<>();\n+    userAuthFactories.add(new UserAuthPasswordFactory());\n+\n+    sshd.setUserAuthFactories(userAuthFactories);\n+    sshd.setPasswordAuthenticator((username, password, session) ->\n+        username.equals(\"user\") && password.equals(\"password\")\n+    );\n+\n+    sshd.setSubsystemFactories(\n+        Collections.singletonList(new SftpSubsystemFactory()));\n+\n+    sshd.start();\n+    int port = sshd.getPort();\n+\n+    conf.setClass(\"fs.sftp.impl\", SFTPFileSystem.class, FileSystem.class);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQzNzY2MA=="}, "originalCommit": {"oid": "86bdff3d89be5324a213654947a2ba7c4fd6891b"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ4MjExNA==", "bodyText": "it's not too late I suppose, would you like me to try to declare it as throwing IOE? I can issue a new Jira and fix it there.", "url": "https://github.com/apache/hadoop/pull/1999#discussion_r427482114", "createdAt": "2020-05-19T17:39:30Z", "author": {"login": "mpryahin"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/sftp/SFTPContract.java", "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *       http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.contract.sftp;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.contract.AbstractFSContract;\n+import org.apache.hadoop.fs.sftp.SFTPFileSystem;\n+import org.apache.sshd.common.NamedFactory;\n+import org.apache.sshd.server.SshServer;\n+import org.apache.sshd.server.auth.UserAuth;\n+import org.apache.sshd.server.auth.password.UserAuthPasswordFactory;\n+import org.apache.sshd.server.keyprovider.SimpleGeneratorHostKeyProvider;\n+import org.apache.sshd.server.subsystem.sftp.SftpSubsystemFactory;\n+\n+public class SFTPContract extends AbstractFSContract {\n+\n+  private String testDataDir = new FileSystemTestHelper().getTestRootDir();\n+  private Configuration conf;\n+  public static final String CONTRACT_XML = \"contract/sftp.xml\";\n+  private SshServer sshd;\n+\n+  public SFTPContract(Configuration conf) {\n+    super(conf);\n+    addConfResource(CONTRACT_XML);\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void init() throws IOException {\n+    sshd = SshServer.setUpDefaultServer();\n+    // ask OS to assign a port\n+    sshd.setPort(0);\n+    sshd.setKeyPairProvider(new SimpleGeneratorHostKeyProvider());\n+\n+    List<NamedFactory<UserAuth>> userAuthFactories = new ArrayList<>();\n+    userAuthFactories.add(new UserAuthPasswordFactory());\n+\n+    sshd.setUserAuthFactories(userAuthFactories);\n+    sshd.setPasswordAuthenticator((username, password, session) ->\n+        username.equals(\"user\") && password.equals(\"password\")\n+    );\n+\n+    sshd.setSubsystemFactories(\n+        Collections.singletonList(new SftpSubsystemFactory()));\n+\n+    sshd.start();\n+    int port = sshd.getPort();\n+\n+    conf.setClass(\"fs.sftp.impl\", SFTPFileSystem.class, FileSystem.class);\n+    conf.setInt(\"fs.sftp.host.port\", port);\n+    conf.setBoolean(\"fs.sftp.impl.disable.cache\", true);\n+  }\n+\n+  @Override\n+  public void teardown() throws IOException {\n+    if (sshd != null) {\n+      sshd.stop();\n+    }\n+  }\n+\n+  @Override\n+  public FileSystem getTestFileSystem() throws IOException {\n+    return FileSystem.get(URI.create(\"sftp://user:password@localhost\"), conf);\n+  }\n+\n+  @Override\n+  public String getScheme() {\n+    return \"sftp\";\n+  }\n+\n+  @Override\n+  public Path getTestPath() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ0MDMwNA=="}, "originalCommit": {"oid": "86bdff3d89be5324a213654947a2ba7c4fd6891b"}, "originalPosition": 98}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b874cdf5b10d61184b6602147aa6a9ae1e6f9929", "author": {"user": {"login": "mpryahin", "name": "Mike"}}, "url": "https://github.com/apache/hadoop/commit/b874cdf5b10d61184b6602147aa6a9ae1e6f9929", "committedDate": "2020-05-19T21:19:21Z", "message": "make sure an underlying channel gets closed"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIyMDg3MDE4", "url": "https://github.com/apache/hadoop/pull/1999#pullrequestreview-422087018", "createdAt": "2020-06-01T18:38:27Z", "commit": {"oid": "b874cdf5b10d61184b6602147aa6a9ae1e6f9929"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMVQxODozODoyN1rOGdVqWg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMVQxODozODoyN1rOGdVqWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzQxNjc5NA==", "bodyText": "add . so javadoc is happy", "url": "https://github.com/apache/hadoop/pull/1999#discussion_r433416794", "createdAt": "2020-06-01T18:38:27Z", "author": {"login": "steveloughran"}, "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/AbstractFSContract.java", "diffHunk": "@@ -69,6 +69,14 @@ public void init() throws IOException {\n \n   }\n \n+  /**\n+   * Any teardown logic can go here", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b874cdf5b10d61184b6602147aa6a9ae1e6f9929"}, "originalPosition": 5}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "86bf896afe20c4d39af9a039dab8c695f98feb52", "author": {"user": {"login": "mpryahin", "name": "Mike"}}, "url": "https://github.com/apache/hadoop/commit/86bf896afe20c4d39af9a039dab8c695f98feb52", "committedDate": "2020-06-02T13:50:40Z", "message": "fixed missing persiod in javadoc"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4281, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}