{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDgzNTI1NDg2", "number": 2296, "title": "HDFS-15568. namenode start failed to start when dfs.namenode.max.snapshot.limit set.", "bodyText": "please see https://issues.apache.org/jira/browse/HDFS-15568", "createdAt": "2020-09-10T08:19:17Z", "url": "https://github.com/apache/hadoop/pull/2296", "merged": true, "mergeCommit": {"oid": "425f48799c0666ef6acd6dab0d5299eb86a0ed44"}, "closed": true, "closedAt": "2020-09-17T09:20:09Z", "author": {"login": "bshashikant"}, "timelineItems": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdHcXKmgH2gAyNDgzNTI1NDg2OmM3ODZjMjcwOTA3MDExZWNhZWY4NzlhOThkOTMxZDI3NWFiOWExNWI=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdI3MNwgH2gAyNDgzNTI1NDg2OjlmMDgxMThjMWZmNTI4ZWNiMjJhNTA0NDM2ZGJlNTM0NDFjOWI3MzI=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "c786c270907011ecaef879a98d931d275ab9a15b", "author": {"user": {"login": "bshashikant", "name": null}}, "url": "https://github.com/apache/hadoop/commit/c786c270907011ecaef879a98d931d275ab9a15b", "committedDate": "2020-09-10T08:17:21Z", "message": "HDFS-15568. namenode start failed to start when dfs.namenode.snapshot.max.limit set."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg2MjQwMzEx", "url": "https://github.com/apache/hadoop/pull/2296#pullrequestreview-486240311", "createdAt": "2020-09-10T19:19:09Z", "commit": {"oid": "c786c270907011ecaef879a98d931d275ab9a15b"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxOToxOTowOVrOHQCclA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxOToyMDozNlrOHQCflg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU3OTM0OA==", "bodyText": "Add javadoc", "url": "https://github.com/apache/hadoop/pull/2296#discussion_r486579348", "createdAt": "2020-09-10T19:19:09Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java", "diffHunk": "@@ -508,6 +508,10 @@ FSNamesystem getFSNamesystem() {\n     return namesystem;\n   }\n \n+  public boolean isImageLoaded() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c786c270907011ecaef879a98d931d275ab9a15b"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU3OTQzOA==", "bodyText": "javadoc", "url": "https://github.com/apache/hadoop/pull/2296#discussion_r486579438", "createdAt": "2020-09-10T19:19:21Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "diffHunk": "@@ -368,6 +368,13 @@ void assertFirstSnapshot(INodeDirectory dir,\n     }\n   }\n \n+  boolean captureOpenFiles() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c786c270907011ecaef879a98d931d275ab9a15b"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU3OTYxMQ==", "bodyText": "VisibleForTesting", "url": "https://github.com/apache/hadoop/pull/2296#discussion_r486579611", "createdAt": "2020-09-10T19:19:37Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "diffHunk": "@@ -368,6 +368,13 @@ void assertFirstSnapshot(INodeDirectory dir,\n     }\n   }\n \n+  boolean captureOpenFiles() {\n+    return captureOpenFiles;\n+  }\n+\n+  int getMaxSnapshotLimit() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c786c270907011ecaef879a98d931d275ab9a15b"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU3OTgxNQ==", "bodyText": "Use LambdaTestUtils#intercept", "url": "https://github.com/apache/hadoop/pull/2296#discussion_r486579815", "createdAt": "2020-09-10T19:19:58Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotManager.java", "diffHunk": "@@ -133,4 +137,68 @@ public void testValidateSnapshotIDWidth() throws Exception {\n         getMaxSnapshotID() < Snapshot.CURRENT_STATE_ID);\n   }\n \n+  @Test\n+  public void SnapshotLimitOnRestart() throws Exception {\n+    final Configuration conf = new Configuration();\n+    final Path snapshottableDir\n+        = new Path(\"/\" + getClass().getSimpleName());\n+    int numSnapshots = 5;\n+    conf.setInt(DFSConfigKeys.\n+            DFS_NAMENODE_SNAPSHOT_MAX_LIMIT, numSnapshots);\n+    conf.setInt(DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_FILESYSTEM_LIMIT,\n+        numSnapshots * 2);\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).\n+        numDataNodes(0).build();\n+    cluster.waitActive();\n+    DistributedFileSystem hdfs = cluster.getFileSystem();\n+    hdfs.mkdirs(snapshottableDir);\n+    hdfs.allowSnapshot(snapshottableDir);\n+    int i = 0;\n+    for (; i < numSnapshots; i++) {\n+      hdfs.createSnapshot(snapshottableDir, \"s\" + i);\n+    }\n+    try {\n+      hdfs.createSnapshot(snapshottableDir, \"s\" + i);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c786c270907011ecaef879a98d931d275ab9a15b"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU3OTk0MQ==", "bodyText": "Extract the getSnapshotManager()", "url": "https://github.com/apache/hadoop/pull/2296#discussion_r486579941", "createdAt": "2020-09-10T19:20:15Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotManager.java", "diffHunk": "@@ -133,4 +137,68 @@ public void testValidateSnapshotIDWidth() throws Exception {\n         getMaxSnapshotID() < Snapshot.CURRENT_STATE_ID);\n   }\n \n+  @Test\n+  public void SnapshotLimitOnRestart() throws Exception {\n+    final Configuration conf = new Configuration();\n+    final Path snapshottableDir\n+        = new Path(\"/\" + getClass().getSimpleName());\n+    int numSnapshots = 5;\n+    conf.setInt(DFSConfigKeys.\n+            DFS_NAMENODE_SNAPSHOT_MAX_LIMIT, numSnapshots);\n+    conf.setInt(DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_FILESYSTEM_LIMIT,\n+        numSnapshots * 2);\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).\n+        numDataNodes(0).build();\n+    cluster.waitActive();\n+    DistributedFileSystem hdfs = cluster.getFileSystem();\n+    hdfs.mkdirs(snapshottableDir);\n+    hdfs.allowSnapshot(snapshottableDir);\n+    int i = 0;\n+    for (; i < numSnapshots; i++) {\n+      hdfs.createSnapshot(snapshottableDir, \"s\" + i);\n+    }\n+    try {\n+      hdfs.createSnapshot(snapshottableDir, \"s\" + i);\n+      Assert.fail(\"Expected SnapshotException not thrown\");\n+    } catch (SnapshotException se) {\n+      Assert.assertTrue(\n+          StringUtils.toLowerCase(se.getMessage()).contains(\n+              \"max snapshot limit\"));\n+    }\n+\n+    // now change max snapshot directory limit to 2 and restart namenode\n+    cluster.getNameNode().getConf().setInt(DFSConfigKeys.\n+        DFS_NAMENODE_SNAPSHOT_MAX_LIMIT, 2);\n+    cluster.restartNameNodes();\n+\n+    // make sure edits of all previous 5 create snapshots are replayed\n+    Assert.assertEquals(numSnapshots, cluster.getNamesystem().", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c786c270907011ecaef879a98d931d275ab9a15b"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU4MDExOA==", "bodyText": "LambdaTestUtils", "url": "https://github.com/apache/hadoop/pull/2296#discussion_r486580118", "createdAt": "2020-09-10T19:20:36Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotManager.java", "diffHunk": "@@ -133,4 +137,68 @@ public void testValidateSnapshotIDWidth() throws Exception {\n         getMaxSnapshotID() < Snapshot.CURRENT_STATE_ID);\n   }\n \n+  @Test\n+  public void SnapshotLimitOnRestart() throws Exception {\n+    final Configuration conf = new Configuration();\n+    final Path snapshottableDir\n+        = new Path(\"/\" + getClass().getSimpleName());\n+    int numSnapshots = 5;\n+    conf.setInt(DFSConfigKeys.\n+            DFS_NAMENODE_SNAPSHOT_MAX_LIMIT, numSnapshots);\n+    conf.setInt(DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_FILESYSTEM_LIMIT,\n+        numSnapshots * 2);\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).\n+        numDataNodes(0).build();\n+    cluster.waitActive();\n+    DistributedFileSystem hdfs = cluster.getFileSystem();\n+    hdfs.mkdirs(snapshottableDir);\n+    hdfs.allowSnapshot(snapshottableDir);\n+    int i = 0;\n+    for (; i < numSnapshots; i++) {\n+      hdfs.createSnapshot(snapshottableDir, \"s\" + i);\n+    }\n+    try {\n+      hdfs.createSnapshot(snapshottableDir, \"s\" + i);\n+      Assert.fail(\"Expected SnapshotException not thrown\");\n+    } catch (SnapshotException se) {\n+      Assert.assertTrue(\n+          StringUtils.toLowerCase(se.getMessage()).contains(\n+              \"max snapshot limit\"));\n+    }\n+\n+    // now change max snapshot directory limit to 2 and restart namenode\n+    cluster.getNameNode().getConf().setInt(DFSConfigKeys.\n+        DFS_NAMENODE_SNAPSHOT_MAX_LIMIT, 2);\n+    cluster.restartNameNodes();\n+\n+    // make sure edits of all previous 5 create snapshots are replayed\n+    Assert.assertEquals(numSnapshots, cluster.getNamesystem().\n+        getSnapshotManager().getNumSnapshots());\n+\n+    // make sure namenode has the new snapshot limit configured as 2\n+    Assert.assertEquals(2,\n+        cluster.getNamesystem().getSnapshotManager().getMaxSnapshotLimit());\n+\n+    // Any new snapshot creation should still fail\n+    try {\n+      hdfs.createSnapshot(snapshottableDir, \"s\" + i);\n+      Assert.fail(\"Expected SnapshotException not thrown\");\n+    } catch (SnapshotException se) {\n+      Assert.assertTrue(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c786c270907011ecaef879a98d931d275ab9a15b"}, "originalPosition": 69}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg2MjU5OTc5", "url": "https://github.com/apache/hadoop/pull/2296#pullrequestreview-486259979", "createdAt": "2020-09-10T19:49:15Z", "commit": {"oid": "c786c270907011ecaef879a98d931d275ab9a15b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxOTo0OToxNVrOHQDXrg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxOTo0OToxNVrOHQDXrg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU5NDQ3OA==", "bodyText": "I suggest to add limit type to the error message as below.\ndiff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/DirectorySnapshottableFeature.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/DirectorySnapshottableFeature.java\nindex 266c0a71241..7a47ab4000d 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/DirectorySnapshottableFeature.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/DirectorySnapshottableFeature.java\n@@ -190,8 +190,7 @@ public Snapshot addSnapshot(INodeDirectory snapshotRoot,\n           + n + \" snapshot(s) and the snapshot quota is \"\n           + snapshotQuota);\n     }\n-    snapshotManager.checkSnapshotLimit(snapshotManager.\n-        getMaxSnapshotLimit(), n);\n+    snapshotManager.checkPerDirectorySnapshotLimit(n);\n     final Snapshot s = new Snapshot(id, name, snapshotRoot);\n     final byte[] nameBytes = s.getRoot().getLocalNameBytes();\n     final int i = searchSnapshot(nameBytes);\ndiff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java\nindex 0a2e18c3dc3..7c482074486 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java\n@@ -455,7 +455,7 @@ public String createSnapshot(final LeaseManager leaseManager,\n           \"snapshot IDs and ID rollover is not supported.\");\n     }\n     int n = numSnapshots.get();\n-    checkSnapshotLimit(maxSnapshotFSLimit, n);\n+    checkFileSystemSnapshotLimit(n);\n     srcRoot.addSnapshot(this, snapshotName, leaseManager, mtime);\n       \n     //create success, update id\n@@ -464,12 +464,19 @@ public String createSnapshot(final LeaseManager leaseManager,\n     return Snapshot.getSnapshotPath(snapshotRoot, snapshotName);\n   }\n \n-  void checkSnapshotLimit(int limit, int numSnapshots)\n-      throws SnapshotException {\n+  void checkFileSystemSnapshotLimit(int n) throws SnapshotException {\n+    checkSnapshotLimit(maxSnapshotFSLimit, n, \"file system\");\n+  }\n+\n+  void checkPerDirectorySnapshotLimit(int n) throws SnapshotException {\n+    checkSnapshotLimit(maxSnapshotLimit, n, \"per directory\");\n+  }\n+\n+  private void checkSnapshotLimit(int limit, int numSnapshots,\n+      String type) throws SnapshotException {\n     if (numSnapshots >= limit) {\n-      String msg = \"there are already \" + (numSnapshots + 1)\n-          + \" snapshot(s) and the max snapshot limit is \"\n-          + limit;\n+      String msg = \"There are already \" + (numSnapshots + 1)\n+          + \" snapshot(s) and the \" + type + \" snapshot limit is \" + limit;\n       if (fsdir.isImageLoaded()) {\n         // We have reached the maximum snapshot limit\n         throw new SnapshotException(", "url": "https://github.com/apache/hadoop/pull/2296#discussion_r486594478", "createdAt": "2020-09-10T19:49:15Z", "author": {"login": "szetszwo"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "diffHunk": "@@ -448,22 +455,31 @@ public String createSnapshot(final LeaseManager leaseManager,\n           \"snapshot IDs and ID rollover is not supported.\");\n     }\n     int n = numSnapshots.get();\n-    if (n >= maxSnapshotFSLimit) {\n-      // We have reached the maximum snapshot limit\n-      throw new SnapshotException(\n-          \"Failed to create snapshot: there are already \" + (n + 1)\n-              + \" snapshot(s) and the max snapshot limit is \"\n-              + maxSnapshotFSLimit);\n-    }\n-\n-    srcRoot.addSnapshot(snapshotCounter, snapshotName, leaseManager,\n-        this.captureOpenFiles, maxSnapshotLimit, mtime);\n+    checkSnapshotLimit(maxSnapshotFSLimit, n);\n+    srcRoot.addSnapshot(this, snapshotName, leaseManager, mtime);\n       \n     //create success, update id\n     snapshotCounter++;\n     numSnapshots.getAndIncrement();\n     return Snapshot.getSnapshotPath(snapshotRoot, snapshotName);\n   }\n+\n+  void checkSnapshotLimit(int limit, int numSnapshots)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c786c270907011ecaef879a98d931d275ab9a15b"}, "originalPosition": 37}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg2MjYwNzEy", "url": "https://github.com/apache/hadoop/pull/2296#pullrequestreview-486260712", "createdAt": "2020-09-10T19:50:22Z", "commit": {"oid": "c786c270907011ecaef879a98d931d275ab9a15b"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "943166ec78a55f558375796ad61d8dd954393ee4", "author": {"user": {"login": "bshashikant", "name": null}}, "url": "https://github.com/apache/hadoop/commit/943166ec78a55f558375796ad61d8dd954393ee4", "committedDate": "2020-09-11T09:09:44Z", "message": "Addressed review comments."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ca83922fe5e11535aee53c07495cd03113144669", "author": {"user": {"login": "bshashikant", "name": null}}, "url": "https://github.com/apache/hadoop/commit/ca83922fe5e11535aee53c07495cd03113144669", "committedDate": "2020-09-11T13:12:13Z", "message": "Addressed Checkstyle issues."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg3MDU4NDAy", "url": "https://github.com/apache/hadoop/pull/2296#pullrequestreview-487058402", "createdAt": "2020-09-11T19:12:02Z", "commit": {"oid": "ca83922fe5e11535aee53c07495cd03113144669"}, "state": "APPROVED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMVQxOToxMjowMlrOHQqprw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMVQxOToxMjo0NVrOHQqrBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzIzODA2Mw==", "bodyText": "Who uses this \"i\" later? in for loop def should be good.", "url": "https://github.com/apache/hadoop/pull/2296#discussion_r487238063", "createdAt": "2020-09-11T19:12:02Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotManager.java", "diffHunk": "@@ -133,4 +137,58 @@ public void testValidateSnapshotIDWidth() throws Exception {\n         getMaxSnapshotID() < Snapshot.CURRENT_STATE_ID);\n   }\n \n+  @Test\n+  public void testSnapshotLimitOnRestart() throws Exception {\n+    final Configuration conf = new Configuration();\n+    final Path snapshottableDir\n+        = new Path(\"/\" + getClass().getSimpleName());\n+    int numSnapshots = 5;\n+    conf.setInt(DFSConfigKeys.\n+            DFS_NAMENODE_SNAPSHOT_MAX_LIMIT, numSnapshots);\n+    conf.setInt(DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_FILESYSTEM_LIMIT,\n+        numSnapshots * 2);\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).\n+        numDataNodes(0).build();\n+    cluster.waitActive();\n+    DistributedFileSystem hdfs = cluster.getFileSystem();\n+    hdfs.mkdirs(snapshottableDir);\n+    hdfs.allowSnapshot(snapshottableDir);\n+    int i = 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca83922fe5e11535aee53c07495cd03113144669"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzIzODE0NA==", "bodyText": "1 line?", "url": "https://github.com/apache/hadoop/pull/2296#discussion_r487238144", "createdAt": "2020-09-11T19:12:12Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotManager.java", "diffHunk": "@@ -133,4 +137,58 @@ public void testValidateSnapshotIDWidth() throws Exception {\n         getMaxSnapshotID() < Snapshot.CURRENT_STATE_ID);\n   }\n \n+  @Test\n+  public void testSnapshotLimitOnRestart() throws Exception {\n+    final Configuration conf = new Configuration();\n+    final Path snapshottableDir\n+        = new Path(\"/\" + getClass().getSimpleName());\n+    int numSnapshots = 5;\n+    conf.setInt(DFSConfigKeys.\n+            DFS_NAMENODE_SNAPSHOT_MAX_LIMIT, numSnapshots);\n+    conf.setInt(DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_FILESYSTEM_LIMIT,\n+        numSnapshots * 2);\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).\n+        numDataNodes(0).build();\n+    cluster.waitActive();\n+    DistributedFileSystem hdfs = cluster.getFileSystem();\n+    hdfs.mkdirs(snapshottableDir);\n+    hdfs.allowSnapshot(snapshottableDir);\n+    int i = 0;\n+    for (; i < numSnapshots; i++) {\n+      hdfs.createSnapshot(snapshottableDir, \"s\" + i);\n+    }\n+    LambdaTestUtils.intercept(SnapshotException.class,\n+        \"snapshot limit\",\n+        () -> hdfs.createSnapshot(snapshottableDir, \"s5\"));\n+\n+    // now change max snapshot directory limit to 2 and restart namenode\n+    cluster.getNameNode().getConf().setInt(DFSConfigKeys.\n+        DFS_NAMENODE_SNAPSHOT_MAX_LIMIT, 2);\n+    cluster.restartNameNodes();\n+    SnapshotManager snapshotManager = cluster.getNamesystem().\n+        getSnapshotManager();\n+\n+    // make sure edits of all previous 5 create snapshots are replayed\n+    Assert.assertEquals(numSnapshots, snapshotManager.getNumSnapshots());\n+\n+    // make sure namenode has the new snapshot limit configured as 2\n+    Assert.assertEquals(2, snapshotManager.getMaxSnapshotLimit());\n+\n+    // Any new snapshot creation should still fail\n+    LambdaTestUtils.intercept(SnapshotException.class,\n+        \"snapshot limit\",\n+        () -> hdfs.createSnapshot(snapshottableDir, \"s5\"));\n+    // now change max snapshot FS limit to 2 and restart namenode\n+    cluster.getNameNode().getConf().setInt(DFSConfigKeys.\n+        DFS_NAMENODE_SNAPSHOT_FILESYSTEM_LIMIT, 2);\n+    cluster.restartNameNodes();\n+    snapshotManager = cluster.getNamesystem().", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca83922fe5e11535aee53c07495cd03113144669"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzIzODQwNw==", "bodyText": "should we clean this cluster?", "url": "https://github.com/apache/hadoop/pull/2296#discussion_r487238407", "createdAt": "2020-09-11T19:12:45Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotManager.java", "diffHunk": "@@ -133,4 +137,58 @@ public void testValidateSnapshotIDWidth() throws Exception {\n         getMaxSnapshotID() < Snapshot.CURRENT_STATE_ID);\n   }\n \n+  @Test\n+  public void testSnapshotLimitOnRestart() throws Exception {\n+    final Configuration conf = new Configuration();\n+    final Path snapshottableDir\n+        = new Path(\"/\" + getClass().getSimpleName());\n+    int numSnapshots = 5;\n+    conf.setInt(DFSConfigKeys.\n+            DFS_NAMENODE_SNAPSHOT_MAX_LIMIT, numSnapshots);\n+    conf.setInt(DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_FILESYSTEM_LIMIT,\n+        numSnapshots * 2);\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca83922fe5e11535aee53c07495cd03113144669"}, "originalPosition": 32}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg3MTk0NzA1", "url": "https://github.com/apache/hadoop/pull/2296#pullrequestreview-487194705", "createdAt": "2020-09-12T02:12:00Z", "commit": {"oid": "ca83922fe5e11535aee53c07495cd03113144669"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "66b21abbd9a7d7915e8b12f171d25cd3f3e28c93", "author": {"user": {"login": "bshashikant", "name": null}}, "url": "https://github.com/apache/hadoop/commit/66b21abbd9a7d7915e8b12f171d25cd3f3e28c93", "committedDate": "2020-09-14T16:38:39Z", "message": "Addressed review comments."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9db4fbce5f2eb888be5baceb10358cffa20f79bb", "author": {"user": {"login": "bshashikant", "name": null}}, "url": "https://github.com/apache/hadoop/commit/9db4fbce5f2eb888be5baceb10358cffa20f79bb", "committedDate": "2020-09-14T16:48:23Z", "message": "Addressed TestSnapshotManager test failure."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg4MDAwOTg3", "url": "https://github.com/apache/hadoop/pull/2296#pullrequestreview-488000987", "createdAt": "2020-09-14T17:39:15Z", "commit": {"oid": "9db4fbce5f2eb888be5baceb10358cffa20f79bb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQxNzozOToxNVrOHRf1eA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQxNzozOToxNVrOHRf1eA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODEwOTQzMg==", "bodyText": "We may need to do it in a finally to make sure we always clean it. Also check for null.", "url": "https://github.com/apache/hadoop/pull/2296#discussion_r488109432", "createdAt": "2020-09-14T17:39:15Z", "author": {"login": "goiri"}, "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotManager.java", "diffHunk": "@@ -133,4 +138,57 @@ public void testValidateSnapshotIDWidth() throws Exception {\n         getMaxSnapshotID() < Snapshot.CURRENT_STATE_ID);\n   }\n \n+  @Test\n+  public void testSnapshotLimitOnRestart() throws Exception {\n+    final Configuration conf = new Configuration();\n+    final Path snapshottableDir\n+        = new Path(\"/\" + getClass().getSimpleName());\n+    int numSnapshots = 5;\n+    conf.setInt(DFSConfigKeys.\n+            DFS_NAMENODE_SNAPSHOT_MAX_LIMIT, numSnapshots);\n+    conf.setInt(DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_FILESYSTEM_LIMIT,\n+        numSnapshots * 2);\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).\n+        numDataNodes(0).build();\n+    cluster.waitActive();\n+    DistributedFileSystem hdfs = cluster.getFileSystem();\n+    hdfs.mkdirs(snapshottableDir);\n+    hdfs.allowSnapshot(snapshottableDir);\n+    for (int i = 0; i < numSnapshots; i++) {\n+      hdfs.createSnapshot(snapshottableDir, \"s\" + i);\n+    }\n+    LambdaTestUtils.intercept(SnapshotException.class,\n+        \"snapshot limit\",\n+        () -> hdfs.createSnapshot(snapshottableDir, \"s5\"));\n+\n+    // now change max snapshot directory limit to 2 and restart namenode\n+    cluster.getNameNode().getConf().setInt(DFSConfigKeys.\n+        DFS_NAMENODE_SNAPSHOT_MAX_LIMIT, 2);\n+    cluster.restartNameNodes();\n+    SnapshotManager snapshotManager = cluster.getNamesystem().\n+        getSnapshotManager();\n+\n+    // make sure edits of all previous 5 create snapshots are replayed\n+    Assert.assertEquals(numSnapshots, snapshotManager.getNumSnapshots());\n+\n+    // make sure namenode has the new snapshot limit configured as 2\n+    Assert.assertEquals(2, snapshotManager.getMaxSnapshotLimit());\n+\n+    // Any new snapshot creation should still fail\n+    LambdaTestUtils.intercept(SnapshotException.class,\n+        \"snapshot limit\", () -> hdfs.createSnapshot(snapshottableDir, \"s5\"));\n+    // now change max snapshot FS limit to 2 and restart namenode\n+    cluster.getNameNode().getConf().setInt(DFSConfigKeys.\n+        DFS_NAMENODE_SNAPSHOT_FILESYSTEM_LIMIT, 2);\n+    cluster.restartNameNodes();\n+    snapshotManager = cluster.getNamesystem().\n+        getSnapshotManager();\n+    // make sure edits of all previous 5 create snapshots are replayed\n+    Assert.assertEquals(numSnapshots, snapshotManager.getNumSnapshots());\n+\n+    // make sure namenode has the new snapshot limit configured as 2\n+    Assert.assertEquals(2, snapshotManager.getMaxSnapshotLimit());\n+    cluster.shutdown();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9db4fbce5f2eb888be5baceb10358cffa20f79bb"}, "originalPosition": 89}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9f08118c1ff528ecb22a504436dbe53441c9b732", "author": {"user": {"login": "bshashikant", "name": null}}, "url": "https://github.com/apache/hadoop/commit/9f08118c1ff528ecb22a504436dbe53441c9b732", "committedDate": "2020-09-14T18:06:45Z", "message": "Addressed review comments."}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3637, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}