{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTA5MTk1NTQ5", "number": 182, "reviewThreads": {"totalCount": 22, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxNzo0NTo0OFrOExrulA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQwMjoxMDo0OFrOE1d1QA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNTMyMTE2OnYy", "diffSide": "RIGHT", "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxNzo0NTo0OFrOHn7Png==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwNDoyMzo0NFrOHoAfKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYyNzE2Ng==", "bodyText": "without changing the configuration interface could we optionally swap out this load_batches for creating a GCS to BQ Data Transfer Service config and submitting a new data transfer? This would have the advantage of relying on DTS's existing mechanics to orchestrate multiple loads to a staging table and atomically commit partitions to the destination table with a copy job. This has the advantage of atomicity of loading a partition but the disadvantage of a requiring a call to bigquerydatatransfer.googleapis.com which may prohibit use in environments that can't route this address due to lack of restricted VIP.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r511627166", "createdAt": "2020-10-25T17:45:48Z", "author": {"login": "jaketf"}, "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -0,0 +1,378 @@\n+# Copyright 2018 Google LLC.\n+# This software is provided as-is, without warranty or representation\n+# for any use or purpose.\n+# Your use of it is subject to your agreement with Google.\n+\n+# Licensed under the Apache License, Version 2.0 (the 'License');\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+# http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an 'AS IS' BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Background Cloud Function for loading data to BigQuery.\n+\"\"\"\n+import json\n+from collections import deque\n+from os import getenv\n+from pathlib import Path\n+from typing import Any\n+from typing import Dict, List, Optional, Tuple\n+from urllib.parse import urlparse\n+import logging\n+from time import monotonic, sleep\n+\n+from google.api_core.client_info import ClientInfo\n+from google.cloud import bigquery, storage\n+from google.cloud.exceptions import NotFound\n+\n+# 15TB per BQ load job.\n+MAX_BATCH_BYTES = 15 * 10 ** 12\n+MAX_URIS_PER_LOAD = 10 ** 4\n+\n+BASE_LOAD_JOB_CONFIG = {\n+    \"sourceFormat\": \"CSV\",\n+    \"fieldDelimiter\": \"|\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+}\n+\n+DEFAULT_BQ_LOAD_STATE_TABLE = \"bigquery_loads.serverless_bq_loads\"\n+\n+DEFAULT_EXTERNAL_TABLE_DEFINITION = {\n+    \"sourceFormat\": \"PARQUET\"\n+}\n+\n+DEFAULT_JOB_LABELS = {\n+    \"component\": \"event-based-gcs-ingest\",\n+    \"cloud-function-name\": getenv(\"FUNCTION_NAME\"),\n+}\n+\n+BASE_LOAD_JOB_CONFIG = {\n+    \"sourceFormat\": \"CSV\",\n+    \"fieldDelimiter\": \"|\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+    \"labels\": DEFAULT_JOB_LABELS,\n+}\n+\n+# Will wait up to this polling for errors before exiting\n+# This is to check if job fail quickly, not to assert the succeed\n+# This may not be honored if longer than cloud function timeout\n+# https://cloud.google.com/functions/docs/concepts/exec#timeout\n+WAIT_FOR_JOB_SECONDS = 5\n+\n+\n+def main(event: Dict, context):\n+    # Set by Cloud Function Execution Environment\n+    # https://cloud.google.com/functions/docs/env-var\n+    project = getenv(\"GCP_PROJECT\")\n+\n+\n+    bucket_id, object_id = parse_notification(event)\n+\n+    success_filename = getenv(\"SUCCESS_FILENAME\", \"_SUCCESS\")\n+\n+    # Exit eagerly if not a success file.\n+    # TODO we can improve this with pub/sub message filtering once it supports\n+    # a hasSuffix filter function (we can filter on hasSuffix successfile name)\n+    #  https://cloud.google.com/pubsub/docs/filtering\n+    if not object_id.endswith(f\"/{success_filename}\"):\n+        logging.debug(f\"No-op. This notification was not for a {success_filename} file.\")\n+        print(f\"No-op. This notification was not for a {success_filename} file.\")\n+        return\n+\n+    prefix_to_load = removesuffix(object_id, success_filename)\n+    parts = object_id.split(\"/\")\n+    dataset, table = parts[0:2]\n+    dest_table_ref = bigquery.TableReference.from_string(\n+        f\"{dataset}.{table}\", default_project=project\n+    )\n+\n+    client_info = ClientInfo(user_agent=\"google-pso-tool/bq-severless-loader\")\n+    gcs = storage.Client(client_info=client_info)\n+    default_query_config = bigquery.QueryJobConfig()\n+    default_query_config.use_legacy_sql = False\n+    default_query_config.labels = DEFAULT_JOB_LABELS\n+    bq = bigquery.Client(\n+        client_info=client_info, default_query_job_config=default_query_config\n+    )\n+\n+    gsurl = f\"gs://{bucket_id}/{prefix_to_load}\"\n+\n+    logging.debug(f\"looking for {gsurl}_config/bq_transform.sql\")\n+    print(f\"looking for {gsurl}_config/bq_transform.sql\")\n+    external_query_sql = read_gcs_file_if_exists(gcs, f\"{gsurl}_config/bq_transform.sql\")\n+    logging.debug(f\"external_query_sql = '{external_query_sql}'\")\n+    print(f\"external_query_sql = {external_query_sql}\")\n+    if external_query_sql:\n+        logging.debug(\"EXTERNAL QUERY\")\n+        print(\"EXTERNAL QUERY\")\n+        external_query(gcs, bq, gsurl, external_query_sql, dest_table_ref)\n+        return\n+\n+    logging.debug(\"LOAD_JOB\")\n+    print(\"LOAD_JOB\")\n+    load_batches(gcs, bq, gsurl, dest_table_ref)\n+\n+\n+def external_query(gcs, bq, gsurl, query, dest_table_ref):\n+    \"\"\"Load from query over external table from GCS.\n+\n+    This hinges on a SQL query defined in GCS at _config/bq_transform.sql and\n+    an external table definition _config/external.json (otherwise will assume\n+    parquet external table)\n+    \"\"\"\n+    external_table_config = read_gcs_file_if_exists(gcs, f\"{gsurl}_config/external.json\")\n+    logging.debug(\"reading external table config\")\n+    if external_table_config:\n+        external_table_def = json.loads(external_table_config)\n+    else:\n+        print(f\"Falling back to default parquet external table.\"\n+              f\" {gsurl}/_config/external.json not found.\")\n+        external_table_def = DEFAULT_EXTERNAL_TABLE_DEFINITION\n+\n+    external_table_def[\"sourceUris\"] = flatten(get_batches_for_prefix(gcs, gsurl))\n+    external_config = bigquery.ExternalConfig.from_api_repr(\n+        external_table_def)\n+    job_config = bigquery.QueryJobConfig(\n+        table_definitions={\"temp_ext\": external_config},\n+        use_legacy_sql=False\n+    )\n+    # for some reason string literal wrapped in b''\n+    rendered_query = str(str(query).format(\n+        dest_dataset=dest_table_ref.dataset_id,\n+        dest_table=dest_table_ref.table_id))[2:-1]\n+\n+    job: bigquery.QueryJob = bq.query(\n+        rendered_query,\n+        job_config=job_config)\n+\n+    print(f\"started asynchronous query job: {job.job_id}\")\n+\n+    start_poll_for_errors = monotonic()\n+    # Check if job failed quickly\n+    while monotonic() - start_poll_for_errors < WAIT_FOR_JOB_SECONDS:\n+        job.reload()\n+        if job.errors:\n+            raise RuntimeError(\n+                f\"query job {job.job_id} failed quickly: {job.errors}\")\n+        sleep(1)\n+\n+\n+def flatten(arr: List[List[Any]]) -> List[Any]:\n+    \"\"\"Flatten list of lists to flat list of elements\"\"\"\n+    return [j for i in arr for j in i]\n+\n+\n+def load_batches(gcs, bq, gsurl, dest_table_ref):", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18970ad78f262c24980cee43f5de47bfde2a0a4b"}, "originalPosition": 170}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcxMzA2Nw==", "bodyText": "I think you should still be able to use BigQuery DTS GCS to BQ inside the VPC-SC. Not sure where the gaps are between transfer services but other customers have been running these types of jobs inside the VPC.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r511713067", "createdAt": "2020-10-26T04:23:44Z", "author": {"login": "ryanmcdowell"}, "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -0,0 +1,378 @@\n+# Copyright 2018 Google LLC.\n+# This software is provided as-is, without warranty or representation\n+# for any use or purpose.\n+# Your use of it is subject to your agreement with Google.\n+\n+# Licensed under the Apache License, Version 2.0 (the 'License');\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+# http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an 'AS IS' BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Background Cloud Function for loading data to BigQuery.\n+\"\"\"\n+import json\n+from collections import deque\n+from os import getenv\n+from pathlib import Path\n+from typing import Any\n+from typing import Dict, List, Optional, Tuple\n+from urllib.parse import urlparse\n+import logging\n+from time import monotonic, sleep\n+\n+from google.api_core.client_info import ClientInfo\n+from google.cloud import bigquery, storage\n+from google.cloud.exceptions import NotFound\n+\n+# 15TB per BQ load job.\n+MAX_BATCH_BYTES = 15 * 10 ** 12\n+MAX_URIS_PER_LOAD = 10 ** 4\n+\n+BASE_LOAD_JOB_CONFIG = {\n+    \"sourceFormat\": \"CSV\",\n+    \"fieldDelimiter\": \"|\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+}\n+\n+DEFAULT_BQ_LOAD_STATE_TABLE = \"bigquery_loads.serverless_bq_loads\"\n+\n+DEFAULT_EXTERNAL_TABLE_DEFINITION = {\n+    \"sourceFormat\": \"PARQUET\"\n+}\n+\n+DEFAULT_JOB_LABELS = {\n+    \"component\": \"event-based-gcs-ingest\",\n+    \"cloud-function-name\": getenv(\"FUNCTION_NAME\"),\n+}\n+\n+BASE_LOAD_JOB_CONFIG = {\n+    \"sourceFormat\": \"CSV\",\n+    \"fieldDelimiter\": \"|\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+    \"labels\": DEFAULT_JOB_LABELS,\n+}\n+\n+# Will wait up to this polling for errors before exiting\n+# This is to check if job fail quickly, not to assert the succeed\n+# This may not be honored if longer than cloud function timeout\n+# https://cloud.google.com/functions/docs/concepts/exec#timeout\n+WAIT_FOR_JOB_SECONDS = 5\n+\n+\n+def main(event: Dict, context):\n+    # Set by Cloud Function Execution Environment\n+    # https://cloud.google.com/functions/docs/env-var\n+    project = getenv(\"GCP_PROJECT\")\n+\n+\n+    bucket_id, object_id = parse_notification(event)\n+\n+    success_filename = getenv(\"SUCCESS_FILENAME\", \"_SUCCESS\")\n+\n+    # Exit eagerly if not a success file.\n+    # TODO we can improve this with pub/sub message filtering once it supports\n+    # a hasSuffix filter function (we can filter on hasSuffix successfile name)\n+    #  https://cloud.google.com/pubsub/docs/filtering\n+    if not object_id.endswith(f\"/{success_filename}\"):\n+        logging.debug(f\"No-op. This notification was not for a {success_filename} file.\")\n+        print(f\"No-op. This notification was not for a {success_filename} file.\")\n+        return\n+\n+    prefix_to_load = removesuffix(object_id, success_filename)\n+    parts = object_id.split(\"/\")\n+    dataset, table = parts[0:2]\n+    dest_table_ref = bigquery.TableReference.from_string(\n+        f\"{dataset}.{table}\", default_project=project\n+    )\n+\n+    client_info = ClientInfo(user_agent=\"google-pso-tool/bq-severless-loader\")\n+    gcs = storage.Client(client_info=client_info)\n+    default_query_config = bigquery.QueryJobConfig()\n+    default_query_config.use_legacy_sql = False\n+    default_query_config.labels = DEFAULT_JOB_LABELS\n+    bq = bigquery.Client(\n+        client_info=client_info, default_query_job_config=default_query_config\n+    )\n+\n+    gsurl = f\"gs://{bucket_id}/{prefix_to_load}\"\n+\n+    logging.debug(f\"looking for {gsurl}_config/bq_transform.sql\")\n+    print(f\"looking for {gsurl}_config/bq_transform.sql\")\n+    external_query_sql = read_gcs_file_if_exists(gcs, f\"{gsurl}_config/bq_transform.sql\")\n+    logging.debug(f\"external_query_sql = '{external_query_sql}'\")\n+    print(f\"external_query_sql = {external_query_sql}\")\n+    if external_query_sql:\n+        logging.debug(\"EXTERNAL QUERY\")\n+        print(\"EXTERNAL QUERY\")\n+        external_query(gcs, bq, gsurl, external_query_sql, dest_table_ref)\n+        return\n+\n+    logging.debug(\"LOAD_JOB\")\n+    print(\"LOAD_JOB\")\n+    load_batches(gcs, bq, gsurl, dest_table_ref)\n+\n+\n+def external_query(gcs, bq, gsurl, query, dest_table_ref):\n+    \"\"\"Load from query over external table from GCS.\n+\n+    This hinges on a SQL query defined in GCS at _config/bq_transform.sql and\n+    an external table definition _config/external.json (otherwise will assume\n+    parquet external table)\n+    \"\"\"\n+    external_table_config = read_gcs_file_if_exists(gcs, f\"{gsurl}_config/external.json\")\n+    logging.debug(\"reading external table config\")\n+    if external_table_config:\n+        external_table_def = json.loads(external_table_config)\n+    else:\n+        print(f\"Falling back to default parquet external table.\"\n+              f\" {gsurl}/_config/external.json not found.\")\n+        external_table_def = DEFAULT_EXTERNAL_TABLE_DEFINITION\n+\n+    external_table_def[\"sourceUris\"] = flatten(get_batches_for_prefix(gcs, gsurl))\n+    external_config = bigquery.ExternalConfig.from_api_repr(\n+        external_table_def)\n+    job_config = bigquery.QueryJobConfig(\n+        table_definitions={\"temp_ext\": external_config},\n+        use_legacy_sql=False\n+    )\n+    # for some reason string literal wrapped in b''\n+    rendered_query = str(str(query).format(\n+        dest_dataset=dest_table_ref.dataset_id,\n+        dest_table=dest_table_ref.table_id))[2:-1]\n+\n+    job: bigquery.QueryJob = bq.query(\n+        rendered_query,\n+        job_config=job_config)\n+\n+    print(f\"started asynchronous query job: {job.job_id}\")\n+\n+    start_poll_for_errors = monotonic()\n+    # Check if job failed quickly\n+    while monotonic() - start_poll_for_errors < WAIT_FOR_JOB_SECONDS:\n+        job.reload()\n+        if job.errors:\n+            raise RuntimeError(\n+                f\"query job {job.job_id} failed quickly: {job.errors}\")\n+        sleep(1)\n+\n+\n+def flatten(arr: List[List[Any]]) -> List[Any]:\n+    \"\"\"Flatten list of lists to flat list of elements\"\"\"\n+    return [j for i in arr for j in i]\n+\n+\n+def load_batches(gcs, bq, gsurl, dest_table_ref):", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYyNzE2Ng=="}, "originalCommit": {"oid": "18970ad78f262c24980cee43f5de47bfde2a0a4b"}, "originalPosition": 170}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNTMyMTk3OnYy", "diffSide": "RIGHT", "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxNzo0Njo0M1rOHn7P_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxNzo0Njo0M1rOHn7P_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYyNzI2Mw==", "bodyText": "this should be configurable w/ environment variable for customers who have negotiated a higher per load job bytes limit.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r511627263", "createdAt": "2020-10-25T17:46:43Z", "author": {"login": "jaketf"}, "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -0,0 +1,378 @@\n+# Copyright 2018 Google LLC.\n+# This software is provided as-is, without warranty or representation\n+# for any use or purpose.\n+# Your use of it is subject to your agreement with Google.\n+\n+# Licensed under the Apache License, Version 2.0 (the 'License');\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+# http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an 'AS IS' BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Background Cloud Function for loading data to BigQuery.\n+\"\"\"\n+import json\n+from collections import deque\n+from os import getenv\n+from pathlib import Path\n+from typing import Any\n+from typing import Dict, List, Optional, Tuple\n+from urllib.parse import urlparse\n+import logging\n+from time import monotonic, sleep\n+\n+from google.api_core.client_info import ClientInfo\n+from google.cloud import bigquery, storage\n+from google.cloud.exceptions import NotFound\n+\n+# 15TB per BQ load job.\n+MAX_BATCH_BYTES = 15 * 10 ** 12", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18970ad78f262c24980cee43f5de47bfde2a0a4b"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNTk4NTQ3OnYy", "diffSide": "RIGHT", "path": "cloud_functions/gcs_event_based_ingest/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwNDoyOToyMVrOHoAjvg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwNDoyOToyMVrOHoAjvg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcxNDIzOA==", "bodyText": "Perhaps a notice that this will utilize the Query slots as opposed to the free load slot tier to execute would be good.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r511714238", "createdAt": "2020-10-26T04:29:21Z", "author": {"login": "ryanmcdowell"}, "path": "cloud_functions/gcs_event_based_ingest/README.md", "diffHunk": "@@ -0,0 +1,116 @@\n+<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n+<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n+**Table of Contents**\n+\n+- [BigQuery Serverless Ingest](#bigquery-serverless-ingest)\n+  - [Tracking Table](#tracking-table)\n+  - [Environment Variables](#environment-variables)\n+  - [GCS Object Naming Convention](#gcs-object-naming-convention)\n+  - [Triggers](#triggers)\n+\n+<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n+\n+# BigQuery Serverless Ingest\n+\n+Flexible service for performing  BigQuery file loads to existing tables.\n+This service handles splitting load jobs when the data volume exceeds\n+the BigQuery 15TB load job limit. The goal of the service is to orchestrate\n+BigQuery Load Jobs to many bigquery datasets / tables from a single bucket\n+providing transparent configuration that is overridable at any level.\n+\n+![architecture](img/bq-ingest.png)\n+\n+## Environment Variables\n+- `SUCCESS_FILENAME`: Filename to trigger a load (defaults to `_SUCCESS`).\n+\n+## GCS Object Naming Convention\n+### Data Files\n+Data should be ingested to a prefix containing destination dataset and table\n+like so:\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/*`\n+Note, the table prefix can contain multiple sub-prefixes for handling partitions\n+or for configuring historical / incremental loads differently.\n+\n+### Configuration Files\n+The Ingestion has many optional configuration files that should live in\n+a special `_config/` prefix at the root of the bucket and/or under the dataset\n+and/or table and/or under the partition prefixes.\n+\n+For example if you have the following files:\n+```text\n+gs://${INGESTION_BUCKET}/_config/load.json`\n+gs://${INGESTION_BUCKET}/${BQ_DATASET}/_config/load.json`\n+gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/_config/load.json\n+```\n+The json objects will be merged where key conflicts are resolved by config in\n+the closest directory to the data.\n+If the files contents were like this:\n+`gs://${INGESTION_BUCKET}/_config/load.json`:\n+```json\n+{\n+    \"sourceFormat\": \"CSV\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+    \"schemaUpdateOptions\": [\"ALLOW_FILED_RELAXATION\"]\n+}\n+```\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/_config/load.json`:\n+```json\n+{\n+    \"sourceFormat\": \"AVRO\",\n+}\n+```\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/_config/load.json`:\n+```json\n+{\n+    \"writeDisposition\": \"WRTITE_TRUNCATE\",\n+}\n+```\n+\n+The result of merging these would be:\n+```json\n+{\n+    \"sourceFormat\": \"AVRO\",\n+    \"writeDisposition\": \"WRITE_TRUNCATE\",\n+    \"schemaUpdateOptions\": [\"ALLOW_FILED_RELAXATION\"]\n+}\n+```\n+This configuration system gives us the ability to DRY up common defaults but\n+override them at whatever level is appropriate as new cases come up.\n+\n+#### Transformation SQL", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18970ad78f262c24980cee43f5de47bfde2a0a4b"}, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNTk4OTczOnYy", "diffSide": "RIGHT", "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/README.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwNDozMTozOFrOHoAl1A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwMDowMjozOFrOHomtyA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcxNDc3Mg==", "bodyText": "I'm curious how you would suggest we track what data has been ingested given this limitation?", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r511714772", "createdAt": "2020-10-26T04:31:38Z", "author": {"login": "ryanmcdowell"}, "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/README.md", "diffHunk": "@@ -0,0 +1,73 @@\n+<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n+<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n+**Table of Contents**\n+\n+- [Event Driven BigQuery Ingest with External Table Query](#event-driven-bigquery-ingest-with-external-table-query)\n+  - [Orchestration](#orchestration)\n+  - [Ingestion Mechanics](#ingestion-mechanics)\n+  - [Deployment](#deployment)\n+  - [Implementation notes](#implementation-notes)\n+  - [Tests](#tests)\n+  - [Limitations](#limitations)\n+  - [Future work](#future-work)\n+\n+<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n+\n+# Event Driven BigQuery Ingest \n+This directory defines a reusable [Background Cloud Function](https://cloud.google.com/functions/docs/writing/background)\n+for ingesting any new file at a GCS prefix with a file name containing a\n+timestamp to be used as the partitioning and clustering column in a partitioned\n+BigQuery Table.\n+\n+![architecture](img/arch.png)\n+\n+## Orchestration\n+1. Files pulled from on-prem to gcs bucket.\n+1. [Pub/Sub Notification](https://cloud.google.com/storage/docs/pubsub-notifications)\n+object finalize.\n+1. Cloud Function subscribes to notifications and ingests all the data into \n+BigQuery a directory once a `_SUCCESS` file arrives.\n+\n+\n+## Deployment\n+The source for this Cloud Function can easily be reused to repeat this pattern\n+for many tables by using the accompanying terraform module (TODO).\n+\n+This way we can reuse the tested source code for the Cloud Function.\n+\n+### Environment Variables\n+To configure each deployement of the Cloud Function we will use\n+[Environment Variables](https://cloud.google.com/functions/docs/env-var)\n+\n+\n+#### Optional\n+| Variable                      | Description                           | Default                                      |\n+|-------------------------------|---------------------------------------|----------------------------------------------|\n+| `BQ_LOAD_STATE_TABLE` | BigQuery table to log load state to           | \"bigquery_loads.serverless_bq_loads\" (in same project as cloud function) |\n+\n+\n+## Implementation notes\n+1. To support notifications based on a GCS prefix\n+(rather than every object in the bucket), we chose to use manually\n+configure Pub/Sub Notifications manually and use a Pub/Sub triggered\n+Cloud Function.\n+\n+## Tests\n+From the `gcs_ocn_bq_ingest` dir simply run\n+```bash\n+pytest\n+```\n+\n+## Limitations\n+1. Cloud Functions have a 10 minute timeout. If the BQ load job takes too long\n+ the data will still be ingested but the function may be marked in timeout state\n+ rather than success and the function may not have the opportunity to produce\n+ all the expected logs.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18970ad78f262c24980cee43f5de47bfde2a0a4b"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjMzOTQwMA==", "bodyText": "I should update these docs.\nEssentially the cloud function should exit quickly always (does not block on load job completion just wait n ~= 5 seconds to make sure it didn't fail quickly).\nTracking of \"what data has been ingested\" is all captured in INFORMATION_SCHEMA / Audit logs.\nAll jobs submitted will follow a naming convention and be labelled.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r512339400", "createdAt": "2020-10-27T00:02:38Z", "author": {"login": "jaketf"}, "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/README.md", "diffHunk": "@@ -0,0 +1,73 @@\n+<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n+<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n+**Table of Contents**\n+\n+- [Event Driven BigQuery Ingest with External Table Query](#event-driven-bigquery-ingest-with-external-table-query)\n+  - [Orchestration](#orchestration)\n+  - [Ingestion Mechanics](#ingestion-mechanics)\n+  - [Deployment](#deployment)\n+  - [Implementation notes](#implementation-notes)\n+  - [Tests](#tests)\n+  - [Limitations](#limitations)\n+  - [Future work](#future-work)\n+\n+<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n+\n+# Event Driven BigQuery Ingest \n+This directory defines a reusable [Background Cloud Function](https://cloud.google.com/functions/docs/writing/background)\n+for ingesting any new file at a GCS prefix with a file name containing a\n+timestamp to be used as the partitioning and clustering column in a partitioned\n+BigQuery Table.\n+\n+![architecture](img/arch.png)\n+\n+## Orchestration\n+1. Files pulled from on-prem to gcs bucket.\n+1. [Pub/Sub Notification](https://cloud.google.com/storage/docs/pubsub-notifications)\n+object finalize.\n+1. Cloud Function subscribes to notifications and ingests all the data into \n+BigQuery a directory once a `_SUCCESS` file arrives.\n+\n+\n+## Deployment\n+The source for this Cloud Function can easily be reused to repeat this pattern\n+for many tables by using the accompanying terraform module (TODO).\n+\n+This way we can reuse the tested source code for the Cloud Function.\n+\n+### Environment Variables\n+To configure each deployement of the Cloud Function we will use\n+[Environment Variables](https://cloud.google.com/functions/docs/env-var)\n+\n+\n+#### Optional\n+| Variable                      | Description                           | Default                                      |\n+|-------------------------------|---------------------------------------|----------------------------------------------|\n+| `BQ_LOAD_STATE_TABLE` | BigQuery table to log load state to           | \"bigquery_loads.serverless_bq_loads\" (in same project as cloud function) |\n+\n+\n+## Implementation notes\n+1. To support notifications based on a GCS prefix\n+(rather than every object in the bucket), we chose to use manually\n+configure Pub/Sub Notifications manually and use a Pub/Sub triggered\n+Cloud Function.\n+\n+## Tests\n+From the `gcs_ocn_bq_ingest` dir simply run\n+```bash\n+pytest\n+```\n+\n+## Limitations\n+1. Cloud Functions have a 10 minute timeout. If the BQ load job takes too long\n+ the data will still be ingested but the function may be marked in timeout state\n+ rather than success and the function may not have the opportunity to produce\n+ all the expected logs.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcxNDc3Mg=="}, "originalCommit": {"oid": "18970ad78f262c24980cee43f5de47bfde2a0a4b"}, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNTk4OTk3OnYy", "diffSide": "RIGHT", "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwNDozMTo0NlrOHoAl8A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwNDozMTo0NlrOHoAl8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcxNDgwMA==", "bodyText": "2020", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r511714800", "createdAt": "2020-10-26T04:31:46Z", "author": {"login": "ryanmcdowell"}, "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -0,0 +1,378 @@\n+# Copyright 2018 Google LLC.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18970ad78f262c24980cee43f5de47bfde2a0a4b"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNTk5MTkyOnYy", "diffSide": "RIGHT", "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwNDozMzoxMVrOHoAm-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwNDozMzoxMVrOHoAm-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcxNTA2Nw==", "bodyText": "Remove extra blank line", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r511715067", "createdAt": "2020-10-26T04:33:11Z", "author": {"login": "ryanmcdowell"}, "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -0,0 +1,378 @@\n+# Copyright 2018 Google LLC.\n+# This software is provided as-is, without warranty or representation\n+# for any use or purpose.\n+# Your use of it is subject to your agreement with Google.\n+\n+# Licensed under the Apache License, Version 2.0 (the 'License');\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+# http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an 'AS IS' BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Background Cloud Function for loading data to BigQuery.\n+\"\"\"\n+import json\n+from collections import deque\n+from os import getenv\n+from pathlib import Path\n+from typing import Any\n+from typing import Dict, List, Optional, Tuple\n+from urllib.parse import urlparse\n+import logging\n+from time import monotonic, sleep\n+\n+from google.api_core.client_info import ClientInfo\n+from google.cloud import bigquery, storage\n+from google.cloud.exceptions import NotFound\n+\n+# 15TB per BQ load job.\n+MAX_BATCH_BYTES = 15 * 10 ** 12\n+MAX_URIS_PER_LOAD = 10 ** 4\n+\n+BASE_LOAD_JOB_CONFIG = {\n+    \"sourceFormat\": \"CSV\",\n+    \"fieldDelimiter\": \"|\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+}\n+\n+DEFAULT_BQ_LOAD_STATE_TABLE = \"bigquery_loads.serverless_bq_loads\"\n+\n+DEFAULT_EXTERNAL_TABLE_DEFINITION = {\n+    \"sourceFormat\": \"PARQUET\"\n+}\n+\n+DEFAULT_JOB_LABELS = {\n+    \"component\": \"event-based-gcs-ingest\",\n+    \"cloud-function-name\": getenv(\"FUNCTION_NAME\"),\n+}\n+\n+BASE_LOAD_JOB_CONFIG = {\n+    \"sourceFormat\": \"CSV\",\n+    \"fieldDelimiter\": \"|\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+    \"labels\": DEFAULT_JOB_LABELS,\n+}\n+\n+# Will wait up to this polling for errors before exiting\n+# This is to check if job fail quickly, not to assert the succeed\n+# This may not be honored if longer than cloud function timeout\n+# https://cloud.google.com/functions/docs/concepts/exec#timeout\n+WAIT_FOR_JOB_SECONDS = 5\n+\n+\n+def main(event: Dict, context):\n+    # Set by Cloud Function Execution Environment\n+    # https://cloud.google.com/functions/docs/env-var\n+    project = getenv(\"GCP_PROJECT\")\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18970ad78f262c24980cee43f5de47bfde2a0a4b"}, "originalPosition": 72}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNTk5Nzk5OnYy", "diffSide": "RIGHT", "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwNDozNzoyNVrOHoAqYA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwNDozNzoyNVrOHoAqYA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcxNTkzNg==", "bodyText": "Seems like this could be a variable defined above given get_batches_for_prefix depends on the same default", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r511715936", "createdAt": "2020-10-26T04:37:25Z", "author": {"login": "ryanmcdowell"}, "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -0,0 +1,378 @@\n+# Copyright 2018 Google LLC.\n+# This software is provided as-is, without warranty or representation\n+# for any use or purpose.\n+# Your use of it is subject to your agreement with Google.\n+\n+# Licensed under the Apache License, Version 2.0 (the 'License');\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+# http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an 'AS IS' BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Background Cloud Function for loading data to BigQuery.\n+\"\"\"\n+import json\n+from collections import deque\n+from os import getenv\n+from pathlib import Path\n+from typing import Any\n+from typing import Dict, List, Optional, Tuple\n+from urllib.parse import urlparse\n+import logging\n+from time import monotonic, sleep\n+\n+from google.api_core.client_info import ClientInfo\n+from google.cloud import bigquery, storage\n+from google.cloud.exceptions import NotFound\n+\n+# 15TB per BQ load job.\n+MAX_BATCH_BYTES = 15 * 10 ** 12\n+MAX_URIS_PER_LOAD = 10 ** 4\n+\n+BASE_LOAD_JOB_CONFIG = {\n+    \"sourceFormat\": \"CSV\",\n+    \"fieldDelimiter\": \"|\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+}\n+\n+DEFAULT_BQ_LOAD_STATE_TABLE = \"bigquery_loads.serverless_bq_loads\"\n+\n+DEFAULT_EXTERNAL_TABLE_DEFINITION = {\n+    \"sourceFormat\": \"PARQUET\"\n+}\n+\n+DEFAULT_JOB_LABELS = {\n+    \"component\": \"event-based-gcs-ingest\",\n+    \"cloud-function-name\": getenv(\"FUNCTION_NAME\"),\n+}\n+\n+BASE_LOAD_JOB_CONFIG = {\n+    \"sourceFormat\": \"CSV\",\n+    \"fieldDelimiter\": \"|\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+    \"labels\": DEFAULT_JOB_LABELS,\n+}\n+\n+# Will wait up to this polling for errors before exiting\n+# This is to check if job fail quickly, not to assert the succeed\n+# This may not be honored if longer than cloud function timeout\n+# https://cloud.google.com/functions/docs/concepts/exec#timeout\n+WAIT_FOR_JOB_SECONDS = 5\n+\n+\n+def main(event: Dict, context):\n+    # Set by Cloud Function Execution Environment\n+    # https://cloud.google.com/functions/docs/env-var\n+    project = getenv(\"GCP_PROJECT\")\n+\n+\n+    bucket_id, object_id = parse_notification(event)\n+\n+    success_filename = getenv(\"SUCCESS_FILENAME\", \"_SUCCESS\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18970ad78f262c24980cee43f5de47bfde2a0a4b"}, "originalPosition": 76}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNjAwNDkxOnYy", "diffSide": "RIGHT", "path": "cloud_functions/gcs_event_based_ingest/tests/test_gcs_ocn_bq_ingest_it.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwNDo0MjowNVrOHoAuGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwNDo0MjowNVrOHoAuGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcxNjg4OQ==", "bodyText": "2020", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r511716889", "createdAt": "2020-10-26T04:42:05Z", "author": {"login": "ryanmcdowell"}, "path": "cloud_functions/gcs_event_based_ingest/tests/test_gcs_ocn_bq_ingest_it.py", "diffHunk": "@@ -0,0 +1,235 @@\n+# Copyright 2019 Google LLC", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18970ad78f262c24980cee43f5de47bfde2a0a4b"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNjAwOTQ4OnYy", "diffSide": "RIGHT", "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwNDo0NToxOVrOHoAwlA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQxODoxMzozN1rOHp3daA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcxNzUyNA==", "bodyText": "If a directory which has been previously loaded receives new data and a new _SUCCESS file, it seems like entire table will be reloaded. Is it safe to say this does not handle incremental loads? If so, perhaps the default disposition should be WRITE_TRUNCTATE", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r511717524", "createdAt": "2020-10-26T04:45:19Z", "author": {"login": "ryanmcdowell"}, "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -0,0 +1,378 @@\n+# Copyright 2018 Google LLC.\n+# This software is provided as-is, without warranty or representation\n+# for any use or purpose.\n+# Your use of it is subject to your agreement with Google.\n+\n+# Licensed under the Apache License, Version 2.0 (the 'License');\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+# http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an 'AS IS' BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Background Cloud Function for loading data to BigQuery.\n+\"\"\"\n+import json\n+from collections import deque\n+from os import getenv\n+from pathlib import Path\n+from typing import Any\n+from typing import Dict, List, Optional, Tuple\n+from urllib.parse import urlparse\n+import logging\n+from time import monotonic, sleep\n+\n+from google.api_core.client_info import ClientInfo\n+from google.cloud import bigquery, storage\n+from google.cloud.exceptions import NotFound\n+\n+# 15TB per BQ load job.\n+MAX_BATCH_BYTES = 15 * 10 ** 12\n+MAX_URIS_PER_LOAD = 10 ** 4\n+\n+BASE_LOAD_JOB_CONFIG = {\n+    \"sourceFormat\": \"CSV\",\n+    \"fieldDelimiter\": \"|\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+}\n+\n+DEFAULT_BQ_LOAD_STATE_TABLE = \"bigquery_loads.serverless_bq_loads\"\n+\n+DEFAULT_EXTERNAL_TABLE_DEFINITION = {\n+    \"sourceFormat\": \"PARQUET\"\n+}\n+\n+DEFAULT_JOB_LABELS = {\n+    \"component\": \"event-based-gcs-ingest\",\n+    \"cloud-function-name\": getenv(\"FUNCTION_NAME\"),\n+}\n+\n+BASE_LOAD_JOB_CONFIG = {\n+    \"sourceFormat\": \"CSV\",\n+    \"fieldDelimiter\": \"|\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+    \"labels\": DEFAULT_JOB_LABELS,\n+}\n+\n+# Will wait up to this polling for errors before exiting\n+# This is to check if job fail quickly, not to assert the succeed\n+# This may not be honored if longer than cloud function timeout\n+# https://cloud.google.com/functions/docs/concepts/exec#timeout\n+WAIT_FOR_JOB_SECONDS = 5\n+\n+\n+def main(event: Dict, context):\n+    # Set by Cloud Function Execution Environment\n+    # https://cloud.google.com/functions/docs/env-var\n+    project = getenv(\"GCP_PROJECT\")\n+\n+\n+    bucket_id, object_id = parse_notification(event)\n+\n+    success_filename = getenv(\"SUCCESS_FILENAME\", \"_SUCCESS\")\n+\n+    # Exit eagerly if not a success file.\n+    # TODO we can improve this with pub/sub message filtering once it supports\n+    # a hasSuffix filter function (we can filter on hasSuffix successfile name)\n+    #  https://cloud.google.com/pubsub/docs/filtering\n+    if not object_id.endswith(f\"/{success_filename}\"):", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18970ad78f262c24980cee43f5de47bfde2a0a4b"}, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjM0NTEzNg==", "bodyText": "I definitely want to handle incrementals.\nI will add a note to the docs and a test for this as this will be a common use case.\nIncrementals should always happen in a prefix under the table prefix (e.g. a partition prefix) so that their behavior can be properly overwritten in a load.json .\ne.g.\ngs://bucket/dataset/table/_config/load.json          # load.json dictates WRITE_APPEND for historical\ngs://bucket/dataset/table/parts/_config/load.json # load.json WRITE_TRUNCATE for all prefixes under parts/\ngs://bucket/dataset/table/parts/yyyy/mm/dd/hh/*  # All files uploaded to dirs like this would use WRITE_TRUNCATE\n\nHowever, if willing to pay for query slots for incrementals IMHO the most general way to solve incrementals should happen via external query as this provides richer DML semantics (e.g. MERGE into partition) than load job write disposition and access to the _FILE_NAME which can be used to.\nIn the future as GCS > BQ DTS ongoing \"mirroring\" is supported, this asset would mostly be useful for orchestrating external query ingest.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r512345136", "createdAt": "2020-10-27T00:23:01Z", "author": {"login": "jaketf"}, "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -0,0 +1,378 @@\n+# Copyright 2018 Google LLC.\n+# This software is provided as-is, without warranty or representation\n+# for any use or purpose.\n+# Your use of it is subject to your agreement with Google.\n+\n+# Licensed under the Apache License, Version 2.0 (the 'License');\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+# http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an 'AS IS' BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Background Cloud Function for loading data to BigQuery.\n+\"\"\"\n+import json\n+from collections import deque\n+from os import getenv\n+from pathlib import Path\n+from typing import Any\n+from typing import Dict, List, Optional, Tuple\n+from urllib.parse import urlparse\n+import logging\n+from time import monotonic, sleep\n+\n+from google.api_core.client_info import ClientInfo\n+from google.cloud import bigquery, storage\n+from google.cloud.exceptions import NotFound\n+\n+# 15TB per BQ load job.\n+MAX_BATCH_BYTES = 15 * 10 ** 12\n+MAX_URIS_PER_LOAD = 10 ** 4\n+\n+BASE_LOAD_JOB_CONFIG = {\n+    \"sourceFormat\": \"CSV\",\n+    \"fieldDelimiter\": \"|\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+}\n+\n+DEFAULT_BQ_LOAD_STATE_TABLE = \"bigquery_loads.serverless_bq_loads\"\n+\n+DEFAULT_EXTERNAL_TABLE_DEFINITION = {\n+    \"sourceFormat\": \"PARQUET\"\n+}\n+\n+DEFAULT_JOB_LABELS = {\n+    \"component\": \"event-based-gcs-ingest\",\n+    \"cloud-function-name\": getenv(\"FUNCTION_NAME\"),\n+}\n+\n+BASE_LOAD_JOB_CONFIG = {\n+    \"sourceFormat\": \"CSV\",\n+    \"fieldDelimiter\": \"|\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+    \"labels\": DEFAULT_JOB_LABELS,\n+}\n+\n+# Will wait up to this polling for errors before exiting\n+# This is to check if job fail quickly, not to assert the succeed\n+# This may not be honored if longer than cloud function timeout\n+# https://cloud.google.com/functions/docs/concepts/exec#timeout\n+WAIT_FOR_JOB_SECONDS = 5\n+\n+\n+def main(event: Dict, context):\n+    # Set by Cloud Function Execution Environment\n+    # https://cloud.google.com/functions/docs/env-var\n+    project = getenv(\"GCP_PROJECT\")\n+\n+\n+    bucket_id, object_id = parse_notification(event)\n+\n+    success_filename = getenv(\"SUCCESS_FILENAME\", \"_SUCCESS\")\n+\n+    # Exit eagerly if not a success file.\n+    # TODO we can improve this with pub/sub message filtering once it supports\n+    # a hasSuffix filter function (we can filter on hasSuffix successfile name)\n+    #  https://cloud.google.com/pubsub/docs/filtering\n+    if not object_id.endswith(f\"/{success_filename}\"):", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcxNzUyNA=="}, "originalCommit": {"oid": "18970ad78f262c24980cee43f5de47bfde2a0a4b"}, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY2MjMxMg==", "bodyText": "@ryanmcdowell can you see if the docs and regex system I've added in latest commits addresses your incremental loads use case? (of course it will be better once I've added integration test for this pattern)", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r513662312", "createdAt": "2020-10-28T18:13:37Z", "author": {"login": "jaketf"}, "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -0,0 +1,378 @@\n+# Copyright 2018 Google LLC.\n+# This software is provided as-is, without warranty or representation\n+# for any use or purpose.\n+# Your use of it is subject to your agreement with Google.\n+\n+# Licensed under the Apache License, Version 2.0 (the 'License');\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+# http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an 'AS IS' BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Background Cloud Function for loading data to BigQuery.\n+\"\"\"\n+import json\n+from collections import deque\n+from os import getenv\n+from pathlib import Path\n+from typing import Any\n+from typing import Dict, List, Optional, Tuple\n+from urllib.parse import urlparse\n+import logging\n+from time import monotonic, sleep\n+\n+from google.api_core.client_info import ClientInfo\n+from google.cloud import bigquery, storage\n+from google.cloud.exceptions import NotFound\n+\n+# 15TB per BQ load job.\n+MAX_BATCH_BYTES = 15 * 10 ** 12\n+MAX_URIS_PER_LOAD = 10 ** 4\n+\n+BASE_LOAD_JOB_CONFIG = {\n+    \"sourceFormat\": \"CSV\",\n+    \"fieldDelimiter\": \"|\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+}\n+\n+DEFAULT_BQ_LOAD_STATE_TABLE = \"bigquery_loads.serverless_bq_loads\"\n+\n+DEFAULT_EXTERNAL_TABLE_DEFINITION = {\n+    \"sourceFormat\": \"PARQUET\"\n+}\n+\n+DEFAULT_JOB_LABELS = {\n+    \"component\": \"event-based-gcs-ingest\",\n+    \"cloud-function-name\": getenv(\"FUNCTION_NAME\"),\n+}\n+\n+BASE_LOAD_JOB_CONFIG = {\n+    \"sourceFormat\": \"CSV\",\n+    \"fieldDelimiter\": \"|\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+    \"labels\": DEFAULT_JOB_LABELS,\n+}\n+\n+# Will wait up to this polling for errors before exiting\n+# This is to check if job fail quickly, not to assert the succeed\n+# This may not be honored if longer than cloud function timeout\n+# https://cloud.google.com/functions/docs/concepts/exec#timeout\n+WAIT_FOR_JOB_SECONDS = 5\n+\n+\n+def main(event: Dict, context):\n+    # Set by Cloud Function Execution Environment\n+    # https://cloud.google.com/functions/docs/env-var\n+    project = getenv(\"GCP_PROJECT\")\n+\n+\n+    bucket_id, object_id = parse_notification(event)\n+\n+    success_filename = getenv(\"SUCCESS_FILENAME\", \"_SUCCESS\")\n+\n+    # Exit eagerly if not a success file.\n+    # TODO we can improve this with pub/sub message filtering once it supports\n+    # a hasSuffix filter function (we can filter on hasSuffix successfile name)\n+    #  https://cloud.google.com/pubsub/docs/filtering\n+    if not object_id.endswith(f\"/{success_filename}\"):", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcxNzUyNA=="}, "originalCommit": {"oid": "18970ad78f262c24980cee43f5de47bfde2a0a4b"}, "originalPosition": 82}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMTQ5MjQ5OnYy", "diffSide": "RIGHT", "path": "cloud_functions/gcs_event_based_ingest/Dockerfile.ci", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMVQyMDoxMjo0MFrOHrxxjg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQyMDowMTowNFrOHsTsng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2NjMxOA==", "bodyText": "Trailing newline", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r515666318", "createdAt": "2020-11-01T20:12:40Z", "author": {"login": "ryanmcdowell"}, "path": "cloud_functions/gcs_event_based_ingest/Dockerfile.ci", "diffHunk": "@@ -0,0 +1,6 @@\n+FROM python:3.8-slim\n+COPY requirements.txt requirements-dev.txt .\n+RUN pip3 install -r requirements-dev.txt\n+WORKDIR /ci\n+COPY . /ci/\n+ENTRYPOINT [\"pytest\"]", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4a345c8d3aa4068c1c9631cb052cd316f5039c85"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIyMjExMA==", "bodyText": "fixed also a good cue that this dockerfile should also be linted :) I've also added a hadolint step to cloudbuild.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r516222110", "createdAt": "2020-11-02T20:01:04Z", "author": {"login": "jaketf"}, "path": "cloud_functions/gcs_event_based_ingest/Dockerfile.ci", "diffHunk": "@@ -0,0 +1,6 @@\n+FROM python:3.8-slim\n+COPY requirements.txt requirements-dev.txt .\n+RUN pip3 install -r requirements-dev.txt\n+WORKDIR /ci\n+COPY . /ci/\n+ENTRYPOINT [\"pytest\"]", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2NjMxOA=="}, "originalCommit": {"oid": "4a345c8d3aa4068c1c9631cb052cd316f5039c85"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMTQ5NjcyOnYy", "diffSide": "RIGHT", "path": "cloud_functions/gcs_event_based_ingest/README.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMVQyMDoxNzozNlrOHrxzjg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxNTo1NjowMlrOHsKfhA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2NjgzMA==", "bodyText": "Why not just default this to the hive partitioning layout?", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r515666830", "createdAt": "2020-11-01T20:17:36Z", "author": {"login": "ryanmcdowell"}, "path": "cloud_functions/gcs_event_based_ingest/README.md", "diffHunk": "@@ -0,0 +1,248 @@\n+<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n+<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n+**Table of Contents**\n+\n+- [BigQuery Serverless Ingest](#bigquery-serverless-ingest)\n+  - [Tracking Table](#tracking-table)\n+  - [Environment Variables](#environment-variables)\n+  - [GCS Object Naming Convention](#gcs-object-naming-convention)\n+  - [Triggers](#triggers)\n+\n+<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n+\n+# BigQuery Serverless Ingest\n+\n+Flexible service for performing  BigQuery file loads to existing tables.\n+This service handles splitting load jobs when the data volume exceeds\n+the BigQuery 15TB load job limit. The goal of the service is to orchestrate\n+BigQuery Load Jobs to many bigquery datasets / tables from a single bucket\n+providing transparent configuration that is overridable at any level.\n+\n+![architecture](img/bq-ingest.png)\n+\n+## Environment Variables\n+- `SUCCESS_FILENAME`: Filename to trigger a load (defaults to `_SUCCESS`).\n+- `DESTINATION_REGEX`:  A [Python Regex with named capturing groups](https://docs.python.org/3/howto/regex.html#non-capturing-and-named-groups)\n+for (defaults to `(?P<dataset>[\\w\\-_0-9]+)/(?P<table>[\\w\\-_0-9]+)/?(?P<partition>\\$[\\w\\-_0-9]+)?/?(?P<batch>[\\w\\-_0-9]+)?/`): \n+  - `dataset`: destintaion BigQuery Dataset\n+  - `table`: destination BigQuery Table\n+  - `partition`: (optional) destination BigQuery [partition decorator](https://cloud.google.com/bigquery/docs/creating-partitioned-tables#creating_an_ingestion-time_partitioned_table_when_loading_data)\n+    (For example $)\n+  - `batch`: (optional) indicates an incremental load from an upstream system (see [Handling Incremental Loads](#handling-incremental-loads))\n+- `MAX_BATCH_BYTES`: Max bytes for BigQuery Load job. (default 15 TB)\n+\n+## GCS Object Naming Convention\n+### Data Files\n+Data should be ingested to a prefix containing destination dataset and table\n+like so:\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/*`\n+Note, the table prefix can contain multiple sub-prefixes for handling partitions\n+or for configuring historical / incremental loads differently.\n+\n+### Configuration Files\n+The Ingestion has many optional configuration files that should live in\n+a special `_config/` prefix at the root of the bucket and/or under the dataset\n+and/or table and/or under the partition prefixes.\n+\n+For example if you have the following files:\n+```text\n+gs://${INGESTION_BUCKET}/_config/load.json`\n+gs://${INGESTION_BUCKET}/${BQ_DATASET}/_config/load.json`\n+gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/_config/load.json\n+```\n+The json objects will be merged where key conflicts are resolved by config in\n+the closest directory to the data.\n+If the files contents were like this:\n+`gs://${INGESTION_BUCKET}/_config/load.json`:\n+```json\n+{\n+    \"sourceFormat\": \"CSV\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+    \"schemaUpdateOptions\": [\"ALLOW_FILED_RELAXATION\"]\n+}\n+```\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/_config/load.json`:\n+```json\n+{\n+    \"sourceFormat\": \"AVRO\"\n+}\n+```\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/_config/load.json`:\n+```json\n+{\n+    \"writeDisposition\": \"WRTITE_TRUNCATE\"\n+}\n+```\n+\n+The result of merging these would be:\n+```json\n+{\n+    \"sourceFormat\": \"AVRO\",\n+    \"writeDisposition\": \"WRITE_TRUNCATE\",\n+    \"schemaUpdateOptions\": [\"ALLOW_FILED_RELAXATION\"]\n+}\n+```\n+This configuration system gives us the ability to DRY up common defaults but\n+override them at whatever level is appropriate as new cases come up.\n+\n+#### Transformation SQL\n+In some cases we may need to perform transformations on the files in GCS\n+before they can be loaded to BigQuery. This is handled by query on an\n+temporary external table over the GCS objects as a proxy for load job.\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/_config/bq_transform.sql`\n+\n+Note, external queries will consume query slots from this project's reservation\n+or count towards your on-demand billing. They will _not_ use free tie load slots.\n+\n+Note, that the query should select from a `temp_ext` which will be a temporary\n+external table configured on the fly by the Cloud Function.\n+The query must handle the logic for inserting into the destination table.\n+This means it should use BigQuery DML to either `INSERT` or `MERGE` into the\n+destination table.\n+For example:\n+```sql\n+INSERT {dest_dataset}.{dest_table}\n+SELECT * FROM temp_ext\n+```\n+Note that `{dest_dataset}` and `{dest_table}` can be used to inject the dataset\n+and table inferred from the GCS path.\n+\n+\n+The query will be run with the appropriate [external table definitions](https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#ExternalDataConfiguration)\n+defined in:\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/_config/external.json`\n+If this file is missing the external table will be assumed to be `PARQUET` format.\n+\n+### Partitions\n+\n+#### Partition Table Decorators\n+Note that if the directory immediately before the triggering successfile starts with\n+a `$` it will be treated as a BigQuery Partition decorator for the destination table.\n+\n+This means for:\n+```text\n+gs://${BUCKET}/foo/bar/$20201026/_SUCCESS", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4a345c8d3aa4068c1c9631cb052cd316f5039c85"}, "originalPosition": 124}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjA3MTMwMA==", "bodyText": "My thought is this default supports date, timestamp and integer range partition decorators and is explicit / matches BQ target syntax.\nThe hive partitioning is useful only to those following a supported folder structure.\nIn a future PR I can add more detailed docs and an integration test for how we'd suggest using this function with native BQ support for hive partitioning by specifying it in load.json at the table level.\nThe way the docs for BQ hive partitioning reads is that you can provide dt=/val= however most often in the wild (e.g. my current customer) I see data lakes of partitioned data in table/yyyy/MM/dd/hh/ which is unsupported by bq hive partitioning because partition key name is not encoded in the path. Even for yr=yyyy/m=MM/d=dd/hr=hh (ala my first ad tech customer) this is \"supported\" but all of a sudden you can only partition on fields yr, m, d, or  h rather than the whole date time (which defeats the purpose). Often times there are other ETL process / consumers who need the data in this exact layout so the customer is not that flexible to change.\nFor these customers I'd recommend using external query loads and define a small udf to parse your org's partitioning / naming convention and constructing the timestamp / date time field you want to partition on. Alternatively they can fork this function for those use cases and add some extra parsing step on the partition extracted by regex to produce the partition decorator they desire for their destination table ref.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r516071300", "createdAt": "2020-11-02T15:56:02Z", "author": {"login": "jaketf"}, "path": "cloud_functions/gcs_event_based_ingest/README.md", "diffHunk": "@@ -0,0 +1,248 @@\n+<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n+<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n+**Table of Contents**\n+\n+- [BigQuery Serverless Ingest](#bigquery-serverless-ingest)\n+  - [Tracking Table](#tracking-table)\n+  - [Environment Variables](#environment-variables)\n+  - [GCS Object Naming Convention](#gcs-object-naming-convention)\n+  - [Triggers](#triggers)\n+\n+<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n+\n+# BigQuery Serverless Ingest\n+\n+Flexible service for performing  BigQuery file loads to existing tables.\n+This service handles splitting load jobs when the data volume exceeds\n+the BigQuery 15TB load job limit. The goal of the service is to orchestrate\n+BigQuery Load Jobs to many bigquery datasets / tables from a single bucket\n+providing transparent configuration that is overridable at any level.\n+\n+![architecture](img/bq-ingest.png)\n+\n+## Environment Variables\n+- `SUCCESS_FILENAME`: Filename to trigger a load (defaults to `_SUCCESS`).\n+- `DESTINATION_REGEX`:  A [Python Regex with named capturing groups](https://docs.python.org/3/howto/regex.html#non-capturing-and-named-groups)\n+for (defaults to `(?P<dataset>[\\w\\-_0-9]+)/(?P<table>[\\w\\-_0-9]+)/?(?P<partition>\\$[\\w\\-_0-9]+)?/?(?P<batch>[\\w\\-_0-9]+)?/`): \n+  - `dataset`: destintaion BigQuery Dataset\n+  - `table`: destination BigQuery Table\n+  - `partition`: (optional) destination BigQuery [partition decorator](https://cloud.google.com/bigquery/docs/creating-partitioned-tables#creating_an_ingestion-time_partitioned_table_when_loading_data)\n+    (For example $)\n+  - `batch`: (optional) indicates an incremental load from an upstream system (see [Handling Incremental Loads](#handling-incremental-loads))\n+- `MAX_BATCH_BYTES`: Max bytes for BigQuery Load job. (default 15 TB)\n+\n+## GCS Object Naming Convention\n+### Data Files\n+Data should be ingested to a prefix containing destination dataset and table\n+like so:\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/*`\n+Note, the table prefix can contain multiple sub-prefixes for handling partitions\n+or for configuring historical / incremental loads differently.\n+\n+### Configuration Files\n+The Ingestion has many optional configuration files that should live in\n+a special `_config/` prefix at the root of the bucket and/or under the dataset\n+and/or table and/or under the partition prefixes.\n+\n+For example if you have the following files:\n+```text\n+gs://${INGESTION_BUCKET}/_config/load.json`\n+gs://${INGESTION_BUCKET}/${BQ_DATASET}/_config/load.json`\n+gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/_config/load.json\n+```\n+The json objects will be merged where key conflicts are resolved by config in\n+the closest directory to the data.\n+If the files contents were like this:\n+`gs://${INGESTION_BUCKET}/_config/load.json`:\n+```json\n+{\n+    \"sourceFormat\": \"CSV\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+    \"schemaUpdateOptions\": [\"ALLOW_FILED_RELAXATION\"]\n+}\n+```\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/_config/load.json`:\n+```json\n+{\n+    \"sourceFormat\": \"AVRO\"\n+}\n+```\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/_config/load.json`:\n+```json\n+{\n+    \"writeDisposition\": \"WRTITE_TRUNCATE\"\n+}\n+```\n+\n+The result of merging these would be:\n+```json\n+{\n+    \"sourceFormat\": \"AVRO\",\n+    \"writeDisposition\": \"WRITE_TRUNCATE\",\n+    \"schemaUpdateOptions\": [\"ALLOW_FILED_RELAXATION\"]\n+}\n+```\n+This configuration system gives us the ability to DRY up common defaults but\n+override them at whatever level is appropriate as new cases come up.\n+\n+#### Transformation SQL\n+In some cases we may need to perform transformations on the files in GCS\n+before they can be loaded to BigQuery. This is handled by query on an\n+temporary external table over the GCS objects as a proxy for load job.\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/_config/bq_transform.sql`\n+\n+Note, external queries will consume query slots from this project's reservation\n+or count towards your on-demand billing. They will _not_ use free tie load slots.\n+\n+Note, that the query should select from a `temp_ext` which will be a temporary\n+external table configured on the fly by the Cloud Function.\n+The query must handle the logic for inserting into the destination table.\n+This means it should use BigQuery DML to either `INSERT` or `MERGE` into the\n+destination table.\n+For example:\n+```sql\n+INSERT {dest_dataset}.{dest_table}\n+SELECT * FROM temp_ext\n+```\n+Note that `{dest_dataset}` and `{dest_table}` can be used to inject the dataset\n+and table inferred from the GCS path.\n+\n+\n+The query will be run with the appropriate [external table definitions](https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#ExternalDataConfiguration)\n+defined in:\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/_config/external.json`\n+If this file is missing the external table will be assumed to be `PARQUET` format.\n+\n+### Partitions\n+\n+#### Partition Table Decorators\n+Note that if the directory immediately before the triggering successfile starts with\n+a `$` it will be treated as a BigQuery Partition decorator for the destination table.\n+\n+This means for:\n+```text\n+gs://${BUCKET}/foo/bar/$20201026/_SUCCESS", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2NjgzMA=="}, "originalCommit": {"oid": "4a345c8d3aa4068c1c9631cb052cd316f5039c85"}, "originalPosition": 124}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMTQ5NzgxOnYy", "diffSide": "RIGHT", "path": "cloud_functions/gcs_event_based_ingest/README.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMVQyMDoxOToxNFrOHrx0IA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxNTo1NjoyNlrOHsKgow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2Njk3Ng==", "bodyText": "should but is not required correct?", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r515666976", "createdAt": "2020-11-01T20:19:14Z", "author": {"login": "ryanmcdowell"}, "path": "cloud_functions/gcs_event_based_ingest/README.md", "diffHunk": "@@ -0,0 +1,248 @@\n+<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n+<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n+**Table of Contents**\n+\n+- [BigQuery Serverless Ingest](#bigquery-serverless-ingest)\n+  - [Tracking Table](#tracking-table)\n+  - [Environment Variables](#environment-variables)\n+  - [GCS Object Naming Convention](#gcs-object-naming-convention)\n+  - [Triggers](#triggers)\n+\n+<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n+\n+# BigQuery Serverless Ingest\n+\n+Flexible service for performing  BigQuery file loads to existing tables.\n+This service handles splitting load jobs when the data volume exceeds\n+the BigQuery 15TB load job limit. The goal of the service is to orchestrate\n+BigQuery Load Jobs to many bigquery datasets / tables from a single bucket\n+providing transparent configuration that is overridable at any level.\n+\n+![architecture](img/bq-ingest.png)\n+\n+## Environment Variables\n+- `SUCCESS_FILENAME`: Filename to trigger a load (defaults to `_SUCCESS`).\n+- `DESTINATION_REGEX`:  A [Python Regex with named capturing groups](https://docs.python.org/3/howto/regex.html#non-capturing-and-named-groups)\n+for (defaults to `(?P<dataset>[\\w\\-_0-9]+)/(?P<table>[\\w\\-_0-9]+)/?(?P<partition>\\$[\\w\\-_0-9]+)?/?(?P<batch>[\\w\\-_0-9]+)?/`): \n+  - `dataset`: destintaion BigQuery Dataset\n+  - `table`: destination BigQuery Table\n+  - `partition`: (optional) destination BigQuery [partition decorator](https://cloud.google.com/bigquery/docs/creating-partitioned-tables#creating_an_ingestion-time_partitioned_table_when_loading_data)\n+    (For example $)\n+  - `batch`: (optional) indicates an incremental load from an upstream system (see [Handling Incremental Loads](#handling-incremental-loads))\n+- `MAX_BATCH_BYTES`: Max bytes for BigQuery Load job. (default 15 TB)\n+\n+## GCS Object Naming Convention\n+### Data Files\n+Data should be ingested to a prefix containing destination dataset and table\n+like so:\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/*`\n+Note, the table prefix can contain multiple sub-prefixes for handling partitions\n+or for configuring historical / incremental loads differently.\n+\n+### Configuration Files\n+The Ingestion has many optional configuration files that should live in\n+a special `_config/` prefix at the root of the bucket and/or under the dataset\n+and/or table and/or under the partition prefixes.\n+\n+For example if you have the following files:\n+```text\n+gs://${INGESTION_BUCKET}/_config/load.json`\n+gs://${INGESTION_BUCKET}/${BQ_DATASET}/_config/load.json`\n+gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/_config/load.json\n+```\n+The json objects will be merged where key conflicts are resolved by config in\n+the closest directory to the data.\n+If the files contents were like this:\n+`gs://${INGESTION_BUCKET}/_config/load.json`:\n+```json\n+{\n+    \"sourceFormat\": \"CSV\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+    \"schemaUpdateOptions\": [\"ALLOW_FILED_RELAXATION\"]\n+}\n+```\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/_config/load.json`:\n+```json\n+{\n+    \"sourceFormat\": \"AVRO\"\n+}\n+```\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/_config/load.json`:\n+```json\n+{\n+    \"writeDisposition\": \"WRTITE_TRUNCATE\"\n+}\n+```\n+\n+The result of merging these would be:\n+```json\n+{\n+    \"sourceFormat\": \"AVRO\",\n+    \"writeDisposition\": \"WRITE_TRUNCATE\",\n+    \"schemaUpdateOptions\": [\"ALLOW_FILED_RELAXATION\"]\n+}\n+```\n+This configuration system gives us the ability to DRY up common defaults but\n+override them at whatever level is appropriate as new cases come up.\n+\n+#### Transformation SQL\n+In some cases we may need to perform transformations on the files in GCS\n+before they can be loaded to BigQuery. This is handled by query on an\n+temporary external table over the GCS objects as a proxy for load job.\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/_config/bq_transform.sql`\n+\n+Note, external queries will consume query slots from this project's reservation\n+or count towards your on-demand billing. They will _not_ use free tie load slots.\n+\n+Note, that the query should select from a `temp_ext` which will be a temporary\n+external table configured on the fly by the Cloud Function.\n+The query must handle the logic for inserting into the destination table.\n+This means it should use BigQuery DML to either `INSERT` or `MERGE` into the\n+destination table.\n+For example:\n+```sql\n+INSERT {dest_dataset}.{dest_table}\n+SELECT * FROM temp_ext\n+```\n+Note that `{dest_dataset}` and `{dest_table}` can be used to inject the dataset\n+and table inferred from the GCS path.\n+\n+\n+The query will be run with the appropriate [external table definitions](https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#ExternalDataConfiguration)\n+defined in:\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/_config/external.json`\n+If this file is missing the external table will be assumed to be `PARQUET` format.\n+\n+### Partitions\n+\n+#### Partition Table Decorators\n+Note that if the directory immediately before the triggering successfile starts with\n+a `$` it will be treated as a BigQuery Partition decorator for the destination table.\n+\n+This means for:\n+```text\n+gs://${BUCKET}/foo/bar/$20201026/_SUCCESS\n+```\n+will trigger a load job with a destination table of `foo.bar$20201026`\n+This allows you to specify write disposition at the partition level.\n+This can be helpful in reprocessing scenarios where you'd want to `WRITE_TRUNCATE`\n+a partition that had some data quality issue.\n+\n+#### Hive Partitioning\n+If your data will be uploaded to GCS from a hadoop system that uses the \n+[supported default hive partitioning](https://cloud.google.com/bigquery/docs/hive-partitioned-loads-gcs#supported_data_layouts)\n+you can specify this in the [`hivePartitioningOptions`](https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#hivepartitioningoptions)\n+key of `load.json` for that table.\n+\n+Any non-trivial incremental loading to partitions should usually use the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4a345c8d3aa4068c1c9631cb052cd316f5039c85"}, "originalPosition": 137}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjA3MTU4Nw==", "bodyText": "correct.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r516071587", "createdAt": "2020-11-02T15:56:26Z", "author": {"login": "jaketf"}, "path": "cloud_functions/gcs_event_based_ingest/README.md", "diffHunk": "@@ -0,0 +1,248 @@\n+<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n+<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n+**Table of Contents**\n+\n+- [BigQuery Serverless Ingest](#bigquery-serverless-ingest)\n+  - [Tracking Table](#tracking-table)\n+  - [Environment Variables](#environment-variables)\n+  - [GCS Object Naming Convention](#gcs-object-naming-convention)\n+  - [Triggers](#triggers)\n+\n+<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n+\n+# BigQuery Serverless Ingest\n+\n+Flexible service for performing  BigQuery file loads to existing tables.\n+This service handles splitting load jobs when the data volume exceeds\n+the BigQuery 15TB load job limit. The goal of the service is to orchestrate\n+BigQuery Load Jobs to many bigquery datasets / tables from a single bucket\n+providing transparent configuration that is overridable at any level.\n+\n+![architecture](img/bq-ingest.png)\n+\n+## Environment Variables\n+- `SUCCESS_FILENAME`: Filename to trigger a load (defaults to `_SUCCESS`).\n+- `DESTINATION_REGEX`:  A [Python Regex with named capturing groups](https://docs.python.org/3/howto/regex.html#non-capturing-and-named-groups)\n+for (defaults to `(?P<dataset>[\\w\\-_0-9]+)/(?P<table>[\\w\\-_0-9]+)/?(?P<partition>\\$[\\w\\-_0-9]+)?/?(?P<batch>[\\w\\-_0-9]+)?/`): \n+  - `dataset`: destintaion BigQuery Dataset\n+  - `table`: destination BigQuery Table\n+  - `partition`: (optional) destination BigQuery [partition decorator](https://cloud.google.com/bigquery/docs/creating-partitioned-tables#creating_an_ingestion-time_partitioned_table_when_loading_data)\n+    (For example $)\n+  - `batch`: (optional) indicates an incremental load from an upstream system (see [Handling Incremental Loads](#handling-incremental-loads))\n+- `MAX_BATCH_BYTES`: Max bytes for BigQuery Load job. (default 15 TB)\n+\n+## GCS Object Naming Convention\n+### Data Files\n+Data should be ingested to a prefix containing destination dataset and table\n+like so:\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/*`\n+Note, the table prefix can contain multiple sub-prefixes for handling partitions\n+or for configuring historical / incremental loads differently.\n+\n+### Configuration Files\n+The Ingestion has many optional configuration files that should live in\n+a special `_config/` prefix at the root of the bucket and/or under the dataset\n+and/or table and/or under the partition prefixes.\n+\n+For example if you have the following files:\n+```text\n+gs://${INGESTION_BUCKET}/_config/load.json`\n+gs://${INGESTION_BUCKET}/${BQ_DATASET}/_config/load.json`\n+gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/_config/load.json\n+```\n+The json objects will be merged where key conflicts are resolved by config in\n+the closest directory to the data.\n+If the files contents were like this:\n+`gs://${INGESTION_BUCKET}/_config/load.json`:\n+```json\n+{\n+    \"sourceFormat\": \"CSV\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+    \"schemaUpdateOptions\": [\"ALLOW_FILED_RELAXATION\"]\n+}\n+```\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/_config/load.json`:\n+```json\n+{\n+    \"sourceFormat\": \"AVRO\"\n+}\n+```\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/_config/load.json`:\n+```json\n+{\n+    \"writeDisposition\": \"WRTITE_TRUNCATE\"\n+}\n+```\n+\n+The result of merging these would be:\n+```json\n+{\n+    \"sourceFormat\": \"AVRO\",\n+    \"writeDisposition\": \"WRITE_TRUNCATE\",\n+    \"schemaUpdateOptions\": [\"ALLOW_FILED_RELAXATION\"]\n+}\n+```\n+This configuration system gives us the ability to DRY up common defaults but\n+override them at whatever level is appropriate as new cases come up.\n+\n+#### Transformation SQL\n+In some cases we may need to perform transformations on the files in GCS\n+before they can be loaded to BigQuery. This is handled by query on an\n+temporary external table over the GCS objects as a proxy for load job.\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/_config/bq_transform.sql`\n+\n+Note, external queries will consume query slots from this project's reservation\n+or count towards your on-demand billing. They will _not_ use free tie load slots.\n+\n+Note, that the query should select from a `temp_ext` which will be a temporary\n+external table configured on the fly by the Cloud Function.\n+The query must handle the logic for inserting into the destination table.\n+This means it should use BigQuery DML to either `INSERT` or `MERGE` into the\n+destination table.\n+For example:\n+```sql\n+INSERT {dest_dataset}.{dest_table}\n+SELECT * FROM temp_ext\n+```\n+Note that `{dest_dataset}` and `{dest_table}` can be used to inject the dataset\n+and table inferred from the GCS path.\n+\n+\n+The query will be run with the appropriate [external table definitions](https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#ExternalDataConfiguration)\n+defined in:\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/_config/external.json`\n+If this file is missing the external table will be assumed to be `PARQUET` format.\n+\n+### Partitions\n+\n+#### Partition Table Decorators\n+Note that if the directory immediately before the triggering successfile starts with\n+a `$` it will be treated as a BigQuery Partition decorator for the destination table.\n+\n+This means for:\n+```text\n+gs://${BUCKET}/foo/bar/$20201026/_SUCCESS\n+```\n+will trigger a load job with a destination table of `foo.bar$20201026`\n+This allows you to specify write disposition at the partition level.\n+This can be helpful in reprocessing scenarios where you'd want to `WRITE_TRUNCATE`\n+a partition that had some data quality issue.\n+\n+#### Hive Partitioning\n+If your data will be uploaded to GCS from a hadoop system that uses the \n+[supported default hive partitioning](https://cloud.google.com/bigquery/docs/hive-partitioned-loads-gcs#supported_data_layouts)\n+you can specify this in the [`hivePartitioningOptions`](https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#hivepartitioningoptions)\n+key of `load.json` for that table.\n+\n+Any non-trivial incremental loading to partitions should usually use the", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2Njk3Ng=="}, "originalCommit": {"oid": "4a345c8d3aa4068c1c9631cb052cd316f5039c85"}, "originalPosition": 137}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMTUwMTQ5OnYy", "diffSide": "RIGHT", "path": "cloud_functions/gcs_event_based_ingest/README.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMVQyMDoyMzozOFrOHrx19Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwNDoxODo0OVrOHr12KQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2NzQ0NQ==", "bodyText": "I'm curious if there's an alternative way to implement this. For instance I see for incrementals we include batch id. But alternatively these semantics could be specified as:\nBatch load if a success file is specified, otherwise load files as they land into GCS. If the user wants transactional semantics, they can use the success file method. The WRITE_DISPOSITION can be separately defined in the config such that either method could technically be incremental.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r515667445", "createdAt": "2020-11-01T20:23:38Z", "author": {"login": "ryanmcdowell"}, "path": "cloud_functions/gcs_event_based_ingest/README.md", "diffHunk": "@@ -0,0 +1,248 @@\n+<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n+<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n+**Table of Contents**\n+\n+- [BigQuery Serverless Ingest](#bigquery-serverless-ingest)\n+  - [Tracking Table](#tracking-table)\n+  - [Environment Variables](#environment-variables)\n+  - [GCS Object Naming Convention](#gcs-object-naming-convention)\n+  - [Triggers](#triggers)\n+\n+<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n+\n+# BigQuery Serverless Ingest\n+\n+Flexible service for performing  BigQuery file loads to existing tables.\n+This service handles splitting load jobs when the data volume exceeds\n+the BigQuery 15TB load job limit. The goal of the service is to orchestrate\n+BigQuery Load Jobs to many bigquery datasets / tables from a single bucket\n+providing transparent configuration that is overridable at any level.\n+\n+![architecture](img/bq-ingest.png)\n+\n+## Environment Variables\n+- `SUCCESS_FILENAME`: Filename to trigger a load (defaults to `_SUCCESS`).\n+- `DESTINATION_REGEX`:  A [Python Regex with named capturing groups](https://docs.python.org/3/howto/regex.html#non-capturing-and-named-groups)\n+for (defaults to `(?P<dataset>[\\w\\-_0-9]+)/(?P<table>[\\w\\-_0-9]+)/?(?P<partition>\\$[\\w\\-_0-9]+)?/?(?P<batch>[\\w\\-_0-9]+)?/`): \n+  - `dataset`: destintaion BigQuery Dataset\n+  - `table`: destination BigQuery Table\n+  - `partition`: (optional) destination BigQuery [partition decorator](https://cloud.google.com/bigquery/docs/creating-partitioned-tables#creating_an_ingestion-time_partitioned_table_when_loading_data)\n+    (For example $)\n+  - `batch`: (optional) indicates an incremental load from an upstream system (see [Handling Incremental Loads](#handling-incremental-loads))\n+- `MAX_BATCH_BYTES`: Max bytes for BigQuery Load job. (default 15 TB)\n+\n+## GCS Object Naming Convention\n+### Data Files\n+Data should be ingested to a prefix containing destination dataset and table\n+like so:\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/*`\n+Note, the table prefix can contain multiple sub-prefixes for handling partitions\n+or for configuring historical / incremental loads differently.\n+\n+### Configuration Files\n+The Ingestion has many optional configuration files that should live in\n+a special `_config/` prefix at the root of the bucket and/or under the dataset\n+and/or table and/or under the partition prefixes.\n+\n+For example if you have the following files:\n+```text\n+gs://${INGESTION_BUCKET}/_config/load.json`\n+gs://${INGESTION_BUCKET}/${BQ_DATASET}/_config/load.json`\n+gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/_config/load.json\n+```\n+The json objects will be merged where key conflicts are resolved by config in\n+the closest directory to the data.\n+If the files contents were like this:\n+`gs://${INGESTION_BUCKET}/_config/load.json`:\n+```json\n+{\n+    \"sourceFormat\": \"CSV\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+    \"schemaUpdateOptions\": [\"ALLOW_FILED_RELAXATION\"]\n+}\n+```\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/_config/load.json`:\n+```json\n+{\n+    \"sourceFormat\": \"AVRO\"\n+}\n+```\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/_config/load.json`:\n+```json\n+{\n+    \"writeDisposition\": \"WRTITE_TRUNCATE\"\n+}\n+```\n+\n+The result of merging these would be:\n+```json\n+{\n+    \"sourceFormat\": \"AVRO\",\n+    \"writeDisposition\": \"WRITE_TRUNCATE\",\n+    \"schemaUpdateOptions\": [\"ALLOW_FILED_RELAXATION\"]\n+}\n+```\n+This configuration system gives us the ability to DRY up common defaults but\n+override them at whatever level is appropriate as new cases come up.\n+\n+#### Transformation SQL\n+In some cases we may need to perform transformations on the files in GCS\n+before they can be loaded to BigQuery. This is handled by query on an\n+temporary external table over the GCS objects as a proxy for load job.\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/_config/bq_transform.sql`\n+\n+Note, external queries will consume query slots from this project's reservation\n+or count towards your on-demand billing. They will _not_ use free tie load slots.\n+\n+Note, that the query should select from a `temp_ext` which will be a temporary\n+external table configured on the fly by the Cloud Function.\n+The query must handle the logic for inserting into the destination table.\n+This means it should use BigQuery DML to either `INSERT` or `MERGE` into the\n+destination table.\n+For example:\n+```sql\n+INSERT {dest_dataset}.{dest_table}\n+SELECT * FROM temp_ext\n+```\n+Note that `{dest_dataset}` and `{dest_table}` can be used to inject the dataset\n+and table inferred from the GCS path.\n+\n+\n+The query will be run with the appropriate [external table definitions](https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#ExternalDataConfiguration)\n+defined in:\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/_config/external.json`\n+If this file is missing the external table will be assumed to be `PARQUET` format.\n+\n+### Partitions\n+\n+#### Partition Table Decorators\n+Note that if the directory immediately before the triggering successfile starts with\n+a `$` it will be treated as a BigQuery Partition decorator for the destination table.\n+\n+This means for:\n+```text\n+gs://${BUCKET}/foo/bar/$20201026/_SUCCESS\n+```\n+will trigger a load job with a destination table of `foo.bar$20201026`\n+This allows you to specify write disposition at the partition level.\n+This can be helpful in reprocessing scenarios where you'd want to `WRITE_TRUNCATE`\n+a partition that had some data quality issue.\n+\n+#### Hive Partitioning\n+If your data will be uploaded to GCS from a hadoop system that uses the \n+[supported default hive partitioning](https://cloud.google.com/bigquery/docs/hive-partitioned-loads-gcs#supported_data_layouts)\n+you can specify this in the [`hivePartitioningOptions`](https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#hivepartitioningoptions)\n+key of `load.json` for that table.\n+\n+Any non-trivial incremental loading to partitions should usually use the\n+Transformation SQL to define the `INSERT / MERGE / UPDATE / DELETE` logic into\n+the target BQ table as these DML semantics are much more flexible thant the load\n+job write dispositions.\n+Furthermore, using external query has the added benefit of circumventing the \n+per load job bytes limits (default 15 TB) and commiting large partitions\n+atomically.\n+\n+## Handling Incremental Loads\n+This solution introduces the concept of `batch_id` which uniquely identifies \n+a batch of data committed by an upstream system that needs to be picked up as an\n+incremental load. You can again set the load job or external query configuration\n+at any parent folders `_config` prefix. This allows you dictate\n+\"for this table any new batch should `WRITE_TRUNCATE` it's parent partition/table\"\n+or \"for that table any new batch should `WRITE_APPEND` to it's parent partition/table\".\n+\n+## Monitoring\n+Monitoring what data has been loaded by this solution should be done with the\n+BigQuery [`INFORMATION_SCHEMA` jobs metadata](https://cloud.google.com/bigquery/docs/information-schema-jobs)\n+If more granular data is needed about a particular job id \n+\n+### Job Naming Convention\n+All load or external query jobs will have a job id witha  prefix following this convention:\n+```python3\n+job_id_prefix=f\"gcf-ingest-{dest_table_ref.dataset_id}-{dest_table_ref.table_id}-{1}-of-{1}-\"\n+```\n+\n+### Job Labels\n+All load or external query jobs are labelled with functional component and cloud function name.\n+```python3\n+DEFAULT_JOB_LABELS = {\n+    \"component\": \"event-based-gcs-ingest\",\n+    \"cloud-function-name\": getenv(\"FUNCTION_NAME\"),\n+    \"gcs-prefix\": gs://bucket/prefix/for/this/ingest,\n+}\n+```\n+If the destination regex matches a batch group, there will be a `batch-id` label.\n+\n+### Example INFROMATION SCHEMA Query\n+```sql\n+SELECT\n+   job_id,\n+   job_type,\n+   start_time,\n+   end_time,\n+   query,\n+   total_bytes_processed,\n+   total_slot_ms,\n+   destination_table\n+   state,\n+   (SELECT value FROM UNNEST(labels) WHERE key = \"component\") as component,\n+   (SELECT value FROM UNNEST(labels) WHERE key = \"cloud-function-name\") as cloud_function_name,\n+   (SELECT value FROM UNNEST(labels) WHERE key = \"batch-id\") as batch_id,\n+FROM\n+   `region-us`.INFORMATION_SCHEMA.JOBS_BY_PROJECT\n+WHERE\n+   (SELECT value FROM UNNEST(labels) WHERE key = \"component\") = \"event-based-gcs-ingest\"\n+```\n+\n+## Triggers\n+\n+### Pub/Sub Storage Notifications `_SUCCESS`\n+1. Trigger on `_SUCCESS` File to load all other files in that directory.\n+1. Trigger on non-`_SUCCESS` File will no-op", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4a345c8d3aa4068c1c9631cb052cd316f5039c85"}, "originalPosition": 200}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTczMzAzMw==", "bodyText": "Interesting proposal.\ntl;dr this should be addressed w/ another tool and if we want to support it in this function it should be a separate PR IMHO.\nI had a similar thought but decided not to implement it this way. Do you have a customer with actual requirements like this? I'm open to thinking about it more but specific requirements would be helpful.\ne.g.\n\nfreshness requirements (aka why not use scheduled loads every 2-5 mins? could even use this solution and drop a success file and write truncate the partition every few mins / or use transnational external query for more specificity)\nquery performance requirements (aka why not use external table?)\nduplicate loading of same file tolerance\nhow many files per day / hour\n\"files not loaded sequentially\" tolerance\n\nnot sure what kind of queries don't care about the above things except speculative results which could usually be served w/ external table.\nIMHO this use cases of \"load each file as it lands\" are 90%  of the time either better served with external query the data directly OR pivoting publishers to streaming inserts.\nI've originally avoided this because it will create an inefficient number of load jobs (resulting in inefficient number of storage sets IIUC which may damage performance of queries until some compaction of storage sets occurs) and I've been focused on scalability.\nIt will become much more likely to hit max load jobs per project / table https://cloud.google.com/bigquery/quotas#load_jobs.\nThis also makes the system harder to monitor / reason about.\nAlso this would complicate handling duplicate OCN pubsub messages as we'd have to use a more sophisticated de-duplication strategy than the one currently implemented (based on a single lock file). This will likely require a transactional database (cloud SQL, firestore, memorystore, bigtable). In my experience customers doing BQ migrations don't have a lot of GCP services approved by their infosec teams yet making additional complexity in the architecture a blocker to many orgs, not to mention adding cost / maintenance overhead of this metadata db to an otherwise cheap / managed solution.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r515733033", "createdAt": "2020-11-02T04:18:49Z", "author": {"login": "jaketf"}, "path": "cloud_functions/gcs_event_based_ingest/README.md", "diffHunk": "@@ -0,0 +1,248 @@\n+<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n+<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n+**Table of Contents**\n+\n+- [BigQuery Serverless Ingest](#bigquery-serverless-ingest)\n+  - [Tracking Table](#tracking-table)\n+  - [Environment Variables](#environment-variables)\n+  - [GCS Object Naming Convention](#gcs-object-naming-convention)\n+  - [Triggers](#triggers)\n+\n+<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n+\n+# BigQuery Serverless Ingest\n+\n+Flexible service for performing  BigQuery file loads to existing tables.\n+This service handles splitting load jobs when the data volume exceeds\n+the BigQuery 15TB load job limit. The goal of the service is to orchestrate\n+BigQuery Load Jobs to many bigquery datasets / tables from a single bucket\n+providing transparent configuration that is overridable at any level.\n+\n+![architecture](img/bq-ingest.png)\n+\n+## Environment Variables\n+- `SUCCESS_FILENAME`: Filename to trigger a load (defaults to `_SUCCESS`).\n+- `DESTINATION_REGEX`:  A [Python Regex with named capturing groups](https://docs.python.org/3/howto/regex.html#non-capturing-and-named-groups)\n+for (defaults to `(?P<dataset>[\\w\\-_0-9]+)/(?P<table>[\\w\\-_0-9]+)/?(?P<partition>\\$[\\w\\-_0-9]+)?/?(?P<batch>[\\w\\-_0-9]+)?/`): \n+  - `dataset`: destintaion BigQuery Dataset\n+  - `table`: destination BigQuery Table\n+  - `partition`: (optional) destination BigQuery [partition decorator](https://cloud.google.com/bigquery/docs/creating-partitioned-tables#creating_an_ingestion-time_partitioned_table_when_loading_data)\n+    (For example $)\n+  - `batch`: (optional) indicates an incremental load from an upstream system (see [Handling Incremental Loads](#handling-incremental-loads))\n+- `MAX_BATCH_BYTES`: Max bytes for BigQuery Load job. (default 15 TB)\n+\n+## GCS Object Naming Convention\n+### Data Files\n+Data should be ingested to a prefix containing destination dataset and table\n+like so:\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/*`\n+Note, the table prefix can contain multiple sub-prefixes for handling partitions\n+or for configuring historical / incremental loads differently.\n+\n+### Configuration Files\n+The Ingestion has many optional configuration files that should live in\n+a special `_config/` prefix at the root of the bucket and/or under the dataset\n+and/or table and/or under the partition prefixes.\n+\n+For example if you have the following files:\n+```text\n+gs://${INGESTION_BUCKET}/_config/load.json`\n+gs://${INGESTION_BUCKET}/${BQ_DATASET}/_config/load.json`\n+gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/_config/load.json\n+```\n+The json objects will be merged where key conflicts are resolved by config in\n+the closest directory to the data.\n+If the files contents were like this:\n+`gs://${INGESTION_BUCKET}/_config/load.json`:\n+```json\n+{\n+    \"sourceFormat\": \"CSV\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+    \"schemaUpdateOptions\": [\"ALLOW_FILED_RELAXATION\"]\n+}\n+```\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/_config/load.json`:\n+```json\n+{\n+    \"sourceFormat\": \"AVRO\"\n+}\n+```\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/_config/load.json`:\n+```json\n+{\n+    \"writeDisposition\": \"WRTITE_TRUNCATE\"\n+}\n+```\n+\n+The result of merging these would be:\n+```json\n+{\n+    \"sourceFormat\": \"AVRO\",\n+    \"writeDisposition\": \"WRITE_TRUNCATE\",\n+    \"schemaUpdateOptions\": [\"ALLOW_FILED_RELAXATION\"]\n+}\n+```\n+This configuration system gives us the ability to DRY up common defaults but\n+override them at whatever level is appropriate as new cases come up.\n+\n+#### Transformation SQL\n+In some cases we may need to perform transformations on the files in GCS\n+before they can be loaded to BigQuery. This is handled by query on an\n+temporary external table over the GCS objects as a proxy for load job.\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/_config/bq_transform.sql`\n+\n+Note, external queries will consume query slots from this project's reservation\n+or count towards your on-demand billing. They will _not_ use free tie load slots.\n+\n+Note, that the query should select from a `temp_ext` which will be a temporary\n+external table configured on the fly by the Cloud Function.\n+The query must handle the logic for inserting into the destination table.\n+This means it should use BigQuery DML to either `INSERT` or `MERGE` into the\n+destination table.\n+For example:\n+```sql\n+INSERT {dest_dataset}.{dest_table}\n+SELECT * FROM temp_ext\n+```\n+Note that `{dest_dataset}` and `{dest_table}` can be used to inject the dataset\n+and table inferred from the GCS path.\n+\n+\n+The query will be run with the appropriate [external table definitions](https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#ExternalDataConfiguration)\n+defined in:\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/_config/external.json`\n+If this file is missing the external table will be assumed to be `PARQUET` format.\n+\n+### Partitions\n+\n+#### Partition Table Decorators\n+Note that if the directory immediately before the triggering successfile starts with\n+a `$` it will be treated as a BigQuery Partition decorator for the destination table.\n+\n+This means for:\n+```text\n+gs://${BUCKET}/foo/bar/$20201026/_SUCCESS\n+```\n+will trigger a load job with a destination table of `foo.bar$20201026`\n+This allows you to specify write disposition at the partition level.\n+This can be helpful in reprocessing scenarios where you'd want to `WRITE_TRUNCATE`\n+a partition that had some data quality issue.\n+\n+#### Hive Partitioning\n+If your data will be uploaded to GCS from a hadoop system that uses the \n+[supported default hive partitioning](https://cloud.google.com/bigquery/docs/hive-partitioned-loads-gcs#supported_data_layouts)\n+you can specify this in the [`hivePartitioningOptions`](https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#hivepartitioningoptions)\n+key of `load.json` for that table.\n+\n+Any non-trivial incremental loading to partitions should usually use the\n+Transformation SQL to define the `INSERT / MERGE / UPDATE / DELETE` logic into\n+the target BQ table as these DML semantics are much more flexible thant the load\n+job write dispositions.\n+Furthermore, using external query has the added benefit of circumventing the \n+per load job bytes limits (default 15 TB) and commiting large partitions\n+atomically.\n+\n+## Handling Incremental Loads\n+This solution introduces the concept of `batch_id` which uniquely identifies \n+a batch of data committed by an upstream system that needs to be picked up as an\n+incremental load. You can again set the load job or external query configuration\n+at any parent folders `_config` prefix. This allows you dictate\n+\"for this table any new batch should `WRITE_TRUNCATE` it's parent partition/table\"\n+or \"for that table any new batch should `WRITE_APPEND` to it's parent partition/table\".\n+\n+## Monitoring\n+Monitoring what data has been loaded by this solution should be done with the\n+BigQuery [`INFORMATION_SCHEMA` jobs metadata](https://cloud.google.com/bigquery/docs/information-schema-jobs)\n+If more granular data is needed about a particular job id \n+\n+### Job Naming Convention\n+All load or external query jobs will have a job id witha  prefix following this convention:\n+```python3\n+job_id_prefix=f\"gcf-ingest-{dest_table_ref.dataset_id}-{dest_table_ref.table_id}-{1}-of-{1}-\"\n+```\n+\n+### Job Labels\n+All load or external query jobs are labelled with functional component and cloud function name.\n+```python3\n+DEFAULT_JOB_LABELS = {\n+    \"component\": \"event-based-gcs-ingest\",\n+    \"cloud-function-name\": getenv(\"FUNCTION_NAME\"),\n+    \"gcs-prefix\": gs://bucket/prefix/for/this/ingest,\n+}\n+```\n+If the destination regex matches a batch group, there will be a `batch-id` label.\n+\n+### Example INFROMATION SCHEMA Query\n+```sql\n+SELECT\n+   job_id,\n+   job_type,\n+   start_time,\n+   end_time,\n+   query,\n+   total_bytes_processed,\n+   total_slot_ms,\n+   destination_table\n+   state,\n+   (SELECT value FROM UNNEST(labels) WHERE key = \"component\") as component,\n+   (SELECT value FROM UNNEST(labels) WHERE key = \"cloud-function-name\") as cloud_function_name,\n+   (SELECT value FROM UNNEST(labels) WHERE key = \"batch-id\") as batch_id,\n+FROM\n+   `region-us`.INFORMATION_SCHEMA.JOBS_BY_PROJECT\n+WHERE\n+   (SELECT value FROM UNNEST(labels) WHERE key = \"component\") = \"event-based-gcs-ingest\"\n+```\n+\n+## Triggers\n+\n+### Pub/Sub Storage Notifications `_SUCCESS`\n+1. Trigger on `_SUCCESS` File to load all other files in that directory.\n+1. Trigger on non-`_SUCCESS` File will no-op", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2NzQ0NQ=="}, "originalCommit": {"oid": "4a345c8d3aa4068c1c9631cb052cd316f5039c85"}, "originalPosition": 200}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMTUwNDQxOnYy", "diffSide": "RIGHT", "path": "cloud_functions/gcs_event_based_ingest/tests/test_gcs_ocn_bq_ingest_it.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMVQyMDoyNjozOFrOHrx3VQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwNDoyMDozN1rOHr13mQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2Nzc5Nw==", "bodyText": "Where is this referenced?", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r515667797", "createdAt": "2020-11-01T20:26:38Z", "author": {"login": "ryanmcdowell"}, "path": "cloud_functions/gcs_event_based_ingest/tests/test_gcs_ocn_bq_ingest_it.py", "diffHunk": "@@ -0,0 +1,388 @@\n+# Copyright 2020 Google LLC\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"unit tests for gcs_ocn_bq_ingest\"\"\"\n+import json\n+import logging\n+import os\n+import sys\n+import uuid\n+from time import sleep\n+from typing import List\n+\n+import google.cloud.storage as storage\n+import pytest\n+from google.cloud import bigquery\n+from google.cloud.exceptions import NotFound\n+\n+sys.path.append(os.path.realpath(os.path.dirname(__file__) + \"/..\"))\n+from gcs_ocn_bq_ingest import main\n+\n+TEST_DIR = os.path.dirname(os.path.abspath(__file__))\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def bq() -> bigquery.Client:\n+    \"\"\"BigQuery Client\"\"\"\n+    return bigquery.Client(location=\"US\")\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def gcs() -> storage.Client:\n+    \"\"\"GCS Client\"\"\"\n+    return storage.Client()\n+\n+\n+@pytest.fixture(scope=\"module\")\n+@pytest.mark.usefixtures(\"gcs\")\n+def gcs_bucket(request, gcs) -> storage.bucket.Bucket:\n+    \"\"\"GCS bucket for test artifacts\"\"\"\n+    bucket = gcs.create_bucket(str(uuid.uuid4()))\n+\n+    def teardown():\n+        bucket.delete(force=True)\n+\n+    request.addfinalizer(teardown)\n+\n+    return bucket\n+\n+\n+@pytest.mark.usefixtures(\"gcs_bucket\")\n+@pytest.fixture\n+def mock_env(gcs, monkeypatch):\n+    \"\"\"environment variable mocks\"\"\"\n+    # Infer project from ADC of gcs client.\n+    monkeypatch.setenv(\"GCP_PROJECT\", gcs.project)\n+    monkeypatch.setenv(\"FUNCTION_NAME\", \"integration-test\")\n+\n+\n+@pytest.mark.usefixtures(\"bq\", \"mock_env\")\n+@pytest.fixture\n+def dest_dataset(request, bq, mock_env, monkeypatch):\n+    random_dataset = f\"test_bq_ingest_gcf_{str(uuid.uuid4())[:8].replace('-','_')}\"\n+    dataset = bigquery.Dataset(f\"{os.getenv('GCP_PROJECT')}\"\n+                               f\".{random_dataset}\")\n+    dataset.location = \"US\"\n+    bq.create_dataset(dataset)\n+    monkeypatch.setenv(\"BQ_LOAD_STATE_TABLE\",\n+                       f\"{dataset.dataset_id}.serverless_bq_loads\")\n+    print(f\"created dataset {dataset.dataset_id}\")\n+\n+    def teardown():\n+        bq.delete_dataset(dataset, delete_contents=True, not_found_ok=True)\n+\n+    request.addfinalizer(teardown)\n+    return dataset\n+\n+\n+@pytest.mark.usefixtures(\"bq\", \"mock_env\", \"dest_dataset\")\n+@pytest.fixture\n+def dest_table(request, bq, mock_env, dest_dataset):\n+    with open(os.path.join(TEST_DIR, \"resources\",\n+                           \"schema.json\")) as schema_file:\n+        schema = main.dict_to_bq_schema(json.load(schema_file))\n+\n+    table = bigquery.Table(\n+        f\"{os.environ.get('GCP_PROJECT')}.{dest_dataset.dataset_id}.cf_test_nation\",\n+        schema=schema,\n+    )\n+\n+    table = bq.create_table(table)\n+\n+    def teardown():\n+        bq.delete_table(table, not_found_ok=True)\n+\n+    request.addfinalizer(teardown)\n+    return table\n+\n+\n+@pytest.fixture(scope=\"function\")\n+@pytest.mark.usefixtures(\"gcs_bucket\", \"dest_dataset\", \"dest_table\")\n+def gcs_data(request, gcs_bucket, dest_dataset,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4a345c8d3aa4068c1c9631cb052cd316f5039c85"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTczMzQwMQ==", "bodyText": "It's a pytest fixture (sort of confusing dynamic syntax) used by these tests:\nhttps://github.com/GoogleCloudPlatform/bigquery-utils/pull/182/files/4a345c8d3aa4068c1c9631cb052cd316f5039c85#diff-0e25c24817556a4a7ba570e46eebf9adf31ad1c091130b4045220a5c54c532f3R132\nhttps://github.com/GoogleCloudPlatform/bigquery-utils/pull/182/files/4a345c8d3aa4068c1c9631cb052cd316f5039c85#diff-0e25c24817556a4a7ba570e46eebf9adf31ad1c091130b4045220a5c54c532f3R157", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r515733401", "createdAt": "2020-11-02T04:20:37Z", "author": {"login": "jaketf"}, "path": "cloud_functions/gcs_event_based_ingest/tests/test_gcs_ocn_bq_ingest_it.py", "diffHunk": "@@ -0,0 +1,388 @@\n+# Copyright 2020 Google LLC\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"unit tests for gcs_ocn_bq_ingest\"\"\"\n+import json\n+import logging\n+import os\n+import sys\n+import uuid\n+from time import sleep\n+from typing import List\n+\n+import google.cloud.storage as storage\n+import pytest\n+from google.cloud import bigquery\n+from google.cloud.exceptions import NotFound\n+\n+sys.path.append(os.path.realpath(os.path.dirname(__file__) + \"/..\"))\n+from gcs_ocn_bq_ingest import main\n+\n+TEST_DIR = os.path.dirname(os.path.abspath(__file__))\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def bq() -> bigquery.Client:\n+    \"\"\"BigQuery Client\"\"\"\n+    return bigquery.Client(location=\"US\")\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def gcs() -> storage.Client:\n+    \"\"\"GCS Client\"\"\"\n+    return storage.Client()\n+\n+\n+@pytest.fixture(scope=\"module\")\n+@pytest.mark.usefixtures(\"gcs\")\n+def gcs_bucket(request, gcs) -> storage.bucket.Bucket:\n+    \"\"\"GCS bucket for test artifacts\"\"\"\n+    bucket = gcs.create_bucket(str(uuid.uuid4()))\n+\n+    def teardown():\n+        bucket.delete(force=True)\n+\n+    request.addfinalizer(teardown)\n+\n+    return bucket\n+\n+\n+@pytest.mark.usefixtures(\"gcs_bucket\")\n+@pytest.fixture\n+def mock_env(gcs, monkeypatch):\n+    \"\"\"environment variable mocks\"\"\"\n+    # Infer project from ADC of gcs client.\n+    monkeypatch.setenv(\"GCP_PROJECT\", gcs.project)\n+    monkeypatch.setenv(\"FUNCTION_NAME\", \"integration-test\")\n+\n+\n+@pytest.mark.usefixtures(\"bq\", \"mock_env\")\n+@pytest.fixture\n+def dest_dataset(request, bq, mock_env, monkeypatch):\n+    random_dataset = f\"test_bq_ingest_gcf_{str(uuid.uuid4())[:8].replace('-','_')}\"\n+    dataset = bigquery.Dataset(f\"{os.getenv('GCP_PROJECT')}\"\n+                               f\".{random_dataset}\")\n+    dataset.location = \"US\"\n+    bq.create_dataset(dataset)\n+    monkeypatch.setenv(\"BQ_LOAD_STATE_TABLE\",\n+                       f\"{dataset.dataset_id}.serverless_bq_loads\")\n+    print(f\"created dataset {dataset.dataset_id}\")\n+\n+    def teardown():\n+        bq.delete_dataset(dataset, delete_contents=True, not_found_ok=True)\n+\n+    request.addfinalizer(teardown)\n+    return dataset\n+\n+\n+@pytest.mark.usefixtures(\"bq\", \"mock_env\", \"dest_dataset\")\n+@pytest.fixture\n+def dest_table(request, bq, mock_env, dest_dataset):\n+    with open(os.path.join(TEST_DIR, \"resources\",\n+                           \"schema.json\")) as schema_file:\n+        schema = main.dict_to_bq_schema(json.load(schema_file))\n+\n+    table = bigquery.Table(\n+        f\"{os.environ.get('GCP_PROJECT')}.{dest_dataset.dataset_id}.cf_test_nation\",\n+        schema=schema,\n+    )\n+\n+    table = bq.create_table(table)\n+\n+    def teardown():\n+        bq.delete_table(table, not_found_ok=True)\n+\n+    request.addfinalizer(teardown)\n+    return table\n+\n+\n+@pytest.fixture(scope=\"function\")\n+@pytest.mark.usefixtures(\"gcs_bucket\", \"dest_dataset\", \"dest_table\")\n+def gcs_data(request, gcs_bucket, dest_dataset,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2Nzc5Nw=="}, "originalCommit": {"oid": "4a345c8d3aa4068c1c9631cb052cd316f5039c85"}, "originalPosition": 111}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzODYwNjE2OnYy", "diffSide": "RIGHT", "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNToyNTo0MlrOHsz9UA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNToyNTo0MlrOHsz9UA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjc1MDY3Mg==", "bodyText": "Let's default this to CSV to align with the load job default", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r516750672", "createdAt": "2020-11-03T15:25:42Z", "author": {"login": "danieldeleo"}, "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -0,0 +1,459 @@\n+# Copyright 2020 Google LLC.\n+# This software is provided as-is, without warranty or representation\n+# for any use or purpose.\n+# Your use of it is subject to your agreement with Google.\n+\n+# Licensed under the Apache License, Version 2.0 (the 'License');\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+# http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an 'AS IS' BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Background Cloud Function for loading data from GCS to BigQuery.\n+\"\"\"\n+import json\n+import re\n+from collections import deque\n+from os import getenv\n+from pathlib import Path\n+from time import monotonic, sleep\n+from typing import Any, Deque, Dict, List, Optional, Tuple\n+from urllib.parse import urlparse\n+\n+from google.api_core.client_info import ClientInfo\n+from google.api_core.exceptions import PreconditionFailed\n+from google.cloud import bigquery, storage\n+from google.cloud.exceptions import NotFound\n+\n+# 15TB per BQ load job.\n+DEFAULT_MAX_BATCH_BYTES = str(15 * 10**12)\n+MAX_URIS_PER_LOAD = 10**4\n+\n+DEFAULT_EXTERNAL_TABLE_DEFINITION = {\"sourceFormat\": \"PARQUET\"}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bfc9622038f1ba8d1006e9057b69990fcedb5981"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzODY3OTE2OnYy", "diffSide": "RIGHT", "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNTo0MDozMVrOHs0qtQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNTo0MDozMVrOHs0qtQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjc2MjI5Mw==", "bodyText": "Nit, but perhaps rename to MAX_SOURCE_URIS_PER_LOAD and/or add to above comment the following link: https://cloud.google.com/bigquery/quotas#load_jobs", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r516762293", "createdAt": "2020-11-03T15:40:31Z", "author": {"login": "danieldeleo"}, "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -0,0 +1,459 @@\n+# Copyright 2020 Google LLC.\n+# This software is provided as-is, without warranty or representation\n+# for any use or purpose.\n+# Your use of it is subject to your agreement with Google.\n+\n+# Licensed under the Apache License, Version 2.0 (the 'License');\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+# http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an 'AS IS' BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Background Cloud Function for loading data from GCS to BigQuery.\n+\"\"\"\n+import json\n+import re\n+from collections import deque\n+from os import getenv\n+from pathlib import Path\n+from time import monotonic, sleep\n+from typing import Any, Deque, Dict, List, Optional, Tuple\n+from urllib.parse import urlparse\n+\n+from google.api_core.client_info import ClientInfo\n+from google.api_core.exceptions import PreconditionFailed\n+from google.cloud import bigquery, storage\n+from google.cloud.exceptions import NotFound\n+\n+# 15TB per BQ load job.\n+DEFAULT_MAX_BATCH_BYTES = str(15 * 10**12)\n+MAX_URIS_PER_LOAD = 10**4", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bfc9622038f1ba8d1006e9057b69990fcedb5981"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzODY4ODcyOnYy", "diffSide": "RIGHT", "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNTo0Mjo0MlrOHs0wzA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNTo0Mjo0MlrOHs0wzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjc2Mzg1Mg==", "bodyText": "bq tool uses comma as default field delimiter, let's align with that tool", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r516763852", "createdAt": "2020-11-03T15:42:42Z", "author": {"login": "danieldeleo"}, "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -0,0 +1,459 @@\n+# Copyright 2020 Google LLC.\n+# This software is provided as-is, without warranty or representation\n+# for any use or purpose.\n+# Your use of it is subject to your agreement with Google.\n+\n+# Licensed under the Apache License, Version 2.0 (the 'License');\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+# http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an 'AS IS' BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Background Cloud Function for loading data from GCS to BigQuery.\n+\"\"\"\n+import json\n+import re\n+from collections import deque\n+from os import getenv\n+from pathlib import Path\n+from time import monotonic, sleep\n+from typing import Any, Deque, Dict, List, Optional, Tuple\n+from urllib.parse import urlparse\n+\n+from google.api_core.client_info import ClientInfo\n+from google.api_core.exceptions import PreconditionFailed\n+from google.cloud import bigquery, storage\n+from google.cloud.exceptions import NotFound\n+\n+# 15TB per BQ load job.\n+DEFAULT_MAX_BATCH_BYTES = str(15 * 10**12)\n+MAX_URIS_PER_LOAD = 10**4\n+\n+DEFAULT_EXTERNAL_TABLE_DEFINITION = {\"sourceFormat\": \"PARQUET\"}\n+\n+DEFAULT_JOB_LABELS = {\n+    \"component\": \"event-based-gcs-ingest\",\n+    \"cloud-function-name\": getenv(\"FUNCTION_NAME\"),\n+}\n+\n+BASE_LOAD_JOB_CONFIG = {\n+    \"sourceFormat\": \"CSV\",\n+    \"fieldDelimiter\": \"|\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bfc9622038f1ba8d1006e9057b69990fcedb5981"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzODcyNTcxOnYy", "diffSide": "RIGHT", "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNTo1MTowMFrOHs1IpQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNTo1MTowMFrOHs1IpQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjc2OTk1Nw==", "bodyText": "Update message to default CSV external table", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r516769957", "createdAt": "2020-11-03T15:51:00Z", "author": {"login": "danieldeleo"}, "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -0,0 +1,459 @@\n+# Copyright 2020 Google LLC.\n+# This software is provided as-is, without warranty or representation\n+# for any use or purpose.\n+# Your use of it is subject to your agreement with Google.\n+\n+# Licensed under the Apache License, Version 2.0 (the 'License');\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+# http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an 'AS IS' BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Background Cloud Function for loading data from GCS to BigQuery.\n+\"\"\"\n+import json\n+import re\n+from collections import deque\n+from os import getenv\n+from pathlib import Path\n+from time import monotonic, sleep\n+from typing import Any, Deque, Dict, List, Optional, Tuple\n+from urllib.parse import urlparse\n+\n+from google.api_core.client_info import ClientInfo\n+from google.api_core.exceptions import PreconditionFailed\n+from google.cloud import bigquery, storage\n+from google.cloud.exceptions import NotFound\n+\n+# 15TB per BQ load job.\n+DEFAULT_MAX_BATCH_BYTES = str(15 * 10**12)\n+MAX_URIS_PER_LOAD = 10**4\n+\n+DEFAULT_EXTERNAL_TABLE_DEFINITION = {\"sourceFormat\": \"PARQUET\"}\n+\n+DEFAULT_JOB_LABELS = {\n+    \"component\": \"event-based-gcs-ingest\",\n+    \"cloud-function-name\": getenv(\"FUNCTION_NAME\"),\n+}\n+\n+BASE_LOAD_JOB_CONFIG = {\n+    \"sourceFormat\": \"CSV\",\n+    \"fieldDelimiter\": \"|\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+    \"labels\": DEFAULT_JOB_LABELS,\n+}\n+\n+DEFAULT_DESTINATION_REGEX = r\"(?P<dataset>[\\w\\-_0-9]+)/\" \\\n+                            r\"(?P<table>[\\w\\-_0-9]+)/\" \\\n+                            r\"?(?P<partition>\\$[0-9]+)?/\" \\\n+                            r\"?(?P<batch>[\\w\\-_0-9]+)?/\"\n+\n+# Will wait up to this polling for errors before exiting\n+# This is to check if job fail quickly, not to assert the succeed\n+# This may not be honored if longer than cloud function timeout\n+# https://cloud.google.com/functions/docs/concepts/exec#timeout\n+WAIT_FOR_JOB_SECONDS = 5\n+SUCCESS_FILENAME = getenv(\"SUCCESS_FILENAME\", \"_SUCCESS\")\n+\n+CLIENT_INFO = ClientInfo(user_agent=\"google-pso-tool/bq-severless-loader\")\n+\n+\n+def main(event: Dict, context):    # pylint: disable=unused-argument\n+    \"\"\"entry point for background cloud function for event driven GCS to\n+    BigQuery ingest.\"\"\"\n+    # pylint: disable=too-many-locals\n+    # Set by Cloud Function Execution Environment\n+    # https://cloud.google.com/functions/docs/env-var\n+    project = getenv(\"GCP_PROJECT\")\n+    destination_regex = getenv(\"DESTINATION_REGEX\", DEFAULT_DESTINATION_REGEX)\n+    dest_re = re.compile(destination_regex)\n+\n+    bucket_id, object_id = parse_notification(event)\n+\n+    # Exit eagerly if not a success file.\n+    # we can improve this with pub/sub message filtering once it supports\n+    # a hasSuffix filter function (we can filter on hasSuffix successfile name)\n+    #  https://cloud.google.com/pubsub/docs/filtering\n+    if not object_id.endswith(f\"/{SUCCESS_FILENAME}\"):\n+        print(\n+            f\"No-op. This notification was not for a {SUCCESS_FILENAME} file.\")\n+        return\n+\n+    prefix_to_load = removesuffix(object_id, SUCCESS_FILENAME)\n+    gsurl = f\"gs://{bucket_id}/{prefix_to_load}\"\n+    gcs_client = storage.Client(client_info=CLIENT_INFO)\n+    bkt = gcs_client.lookup_bucket(bucket_id)\n+    success_blob: storage.Blob = bkt.blob(object_id)\n+    handle_duplicate_notification(bkt, success_blob, gsurl)\n+\n+    destination_match = dest_re.match(object_id)\n+    if not destination_match:\n+        raise RuntimeError(f\"Object ID {object_id} did not match regex:\"\n+                           f\" {destination_regex}\")\n+    destination_details = destination_match.groupdict()\n+    try:\n+        dataset = destination_details['dataset']\n+        table = destination_details['table']\n+    except KeyError:\n+        raise RuntimeError(\n+            f\"Object ID {object_id} did not match dataset and table in regex:\"\n+            f\" {destination_regex}\") from KeyError\n+    partition = destination_details.get('partition')\n+    batch_id = destination_details.get('batch')\n+    labels = DEFAULT_JOB_LABELS\n+    labels[\"bucket\"] = bucket_id\n+\n+    if batch_id:\n+        labels[\"batch-id\"] = batch_id\n+\n+    if partition:\n+        dest_table_ref = bigquery.TableReference.from_string(\n+            f\"{dataset}.{table}{partition}\", default_project=project)\n+    else:\n+        dest_table_ref = bigquery.TableReference.from_string(\n+            f\"{dataset}.{table}\", default_project=project)\n+\n+    default_query_config = bigquery.QueryJobConfig()\n+    default_query_config.use_legacy_sql = False\n+    default_query_config.labels = labels\n+    bq_client = bigquery.Client(client_info=CLIENT_INFO,\n+                                default_query_job_config=default_query_config)\n+\n+    print(f\"looking for {gsurl}_config/bq_transform.sql\")\n+    external_query_sql = read_gcs_file_if_exists(\n+        gcs_client, f\"{gsurl}_config/bq_transform.sql\")\n+    print(f\"external_query_sql = {external_query_sql}\")\n+    if not external_query_sql:\n+        external_query_sql = look_for_transform_sql(gcs_client, gsurl)\n+    if external_query_sql:\n+        print(\"EXTERNAL QUERY\")\n+        external_query(gcs_client, bq_client, gsurl, external_query_sql,\n+                       dest_table_ref)\n+        return\n+\n+    print(\"LOAD_JOB\")\n+    load_batches(gcs_client, bq_client, gsurl, dest_table_ref)\n+\n+\n+def handle_duplicate_notification(\n+    bkt: storage.Bucket,\n+    success_blob: storage.Blob,\n+    gsurl: str\n+):\n+    \"\"\"\n+    Need to handle potential duplicate Pub/Sub notifications.\n+    To achieve this we will drop an empty \"claimed\" file that indicates\n+    an invocation of this cloud function has picked up the success file\n+    with a certain creation timestamp. This will support republishing the\n+    success file as a mechanism of re-running the ingestion while avoiding\n+    duplicate ingestion due to multiple Pub/Sub messages for a success file\n+    with the same creation time.\n+    \"\"\"\n+    success_blob.reload()\n+    success_created_unix_timestamp = success_blob.time_created.timestamp()\n+\n+    claim_blob: storage.Blob = bkt.blob(\n+        f\"_claimed_{success_created_unix_timestamp}\")\n+    try:\n+        claim_blob.upload_from_string(\n+            \"\",\n+            if_generation_match=0)\n+    except PreconditionFailed as err:\n+        raise RuntimeError(\n+            f\"The prefix {gsurl} appears to already have been claimed for \"\n+            f\"{gsurl}{SUCCESS_FILENAME} with created timestamp\"\n+            f\"{success_created_unix_timestamp}.\"\n+            \"This means that another invocation of this cloud function has\"\n+            \"claimed the ingestion of this batch.\"\n+            \"This may be due to a rare duplicate delivery of the Pub/Sub \"\n+            \"storage notification.\") from err\n+\n+\n+def external_query(gcs_client, bq_client, gsurl, query, dest_table_ref):\n+    \"\"\"Load from query over external table from GCS.\n+\n+    This hinges on a SQL query defined in GCS at _config/bq_transform.sql and\n+    an external table definition _config/external.json (otherwise will assume\n+    parquet external table)\n+    \"\"\"\n+    external_table_config = read_gcs_file_if_exists(\n+        gcs_client, f\"{gsurl}_config/external.json\")\n+    if external_table_config:\n+        external_table_def = json.loads(external_table_config)\n+    else:\n+        print(f\"Falling back to default parquet external table.\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bfc9622038f1ba8d1006e9057b69990fcedb5981"}, "originalPosition": 189}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzODg0MTk4OnYy", "diffSide": "RIGHT", "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNjoxNzoxN1rOHs2S3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxOToxMzoyN1rOHs861Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjc4ODk1Ng==", "bodyText": "query is wrapped in b'' because it's a bytestring. Instead of [2:-1] just call --> query.decode('UTF-8')", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r516788956", "createdAt": "2020-11-03T16:17:17Z", "author": {"login": "danieldeleo"}, "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -0,0 +1,459 @@\n+# Copyright 2020 Google LLC.\n+# This software is provided as-is, without warranty or representation\n+# for any use or purpose.\n+# Your use of it is subject to your agreement with Google.\n+\n+# Licensed under the Apache License, Version 2.0 (the 'License');\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+# http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an 'AS IS' BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Background Cloud Function for loading data from GCS to BigQuery.\n+\"\"\"\n+import json\n+import re\n+from collections import deque\n+from os import getenv\n+from pathlib import Path\n+from time import monotonic, sleep\n+from typing import Any, Deque, Dict, List, Optional, Tuple\n+from urllib.parse import urlparse\n+\n+from google.api_core.client_info import ClientInfo\n+from google.api_core.exceptions import PreconditionFailed\n+from google.cloud import bigquery, storage\n+from google.cloud.exceptions import NotFound\n+\n+# 15TB per BQ load job.\n+DEFAULT_MAX_BATCH_BYTES = str(15 * 10**12)\n+MAX_URIS_PER_LOAD = 10**4\n+\n+DEFAULT_EXTERNAL_TABLE_DEFINITION = {\"sourceFormat\": \"PARQUET\"}\n+\n+DEFAULT_JOB_LABELS = {\n+    \"component\": \"event-based-gcs-ingest\",\n+    \"cloud-function-name\": getenv(\"FUNCTION_NAME\"),\n+}\n+\n+BASE_LOAD_JOB_CONFIG = {\n+    \"sourceFormat\": \"CSV\",\n+    \"fieldDelimiter\": \"|\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+    \"labels\": DEFAULT_JOB_LABELS,\n+}\n+\n+DEFAULT_DESTINATION_REGEX = r\"(?P<dataset>[\\w\\-_0-9]+)/\" \\\n+                            r\"(?P<table>[\\w\\-_0-9]+)/\" \\\n+                            r\"?(?P<partition>\\$[0-9]+)?/\" \\\n+                            r\"?(?P<batch>[\\w\\-_0-9]+)?/\"\n+\n+# Will wait up to this polling for errors before exiting\n+# This is to check if job fail quickly, not to assert the succeed\n+# This may not be honored if longer than cloud function timeout\n+# https://cloud.google.com/functions/docs/concepts/exec#timeout\n+WAIT_FOR_JOB_SECONDS = 5\n+SUCCESS_FILENAME = getenv(\"SUCCESS_FILENAME\", \"_SUCCESS\")\n+\n+CLIENT_INFO = ClientInfo(user_agent=\"google-pso-tool/bq-severless-loader\")\n+\n+\n+def main(event: Dict, context):    # pylint: disable=unused-argument\n+    \"\"\"entry point for background cloud function for event driven GCS to\n+    BigQuery ingest.\"\"\"\n+    # pylint: disable=too-many-locals\n+    # Set by Cloud Function Execution Environment\n+    # https://cloud.google.com/functions/docs/env-var\n+    project = getenv(\"GCP_PROJECT\")\n+    destination_regex = getenv(\"DESTINATION_REGEX\", DEFAULT_DESTINATION_REGEX)\n+    dest_re = re.compile(destination_regex)\n+\n+    bucket_id, object_id = parse_notification(event)\n+\n+    # Exit eagerly if not a success file.\n+    # we can improve this with pub/sub message filtering once it supports\n+    # a hasSuffix filter function (we can filter on hasSuffix successfile name)\n+    #  https://cloud.google.com/pubsub/docs/filtering\n+    if not object_id.endswith(f\"/{SUCCESS_FILENAME}\"):\n+        print(\n+            f\"No-op. This notification was not for a {SUCCESS_FILENAME} file.\")\n+        return\n+\n+    prefix_to_load = removesuffix(object_id, SUCCESS_FILENAME)\n+    gsurl = f\"gs://{bucket_id}/{prefix_to_load}\"\n+    gcs_client = storage.Client(client_info=CLIENT_INFO)\n+    bkt = gcs_client.lookup_bucket(bucket_id)\n+    success_blob: storage.Blob = bkt.blob(object_id)\n+    handle_duplicate_notification(bkt, success_blob, gsurl)\n+\n+    destination_match = dest_re.match(object_id)\n+    if not destination_match:\n+        raise RuntimeError(f\"Object ID {object_id} did not match regex:\"\n+                           f\" {destination_regex}\")\n+    destination_details = destination_match.groupdict()\n+    try:\n+        dataset = destination_details['dataset']\n+        table = destination_details['table']\n+    except KeyError:\n+        raise RuntimeError(\n+            f\"Object ID {object_id} did not match dataset and table in regex:\"\n+            f\" {destination_regex}\") from KeyError\n+    partition = destination_details.get('partition')\n+    batch_id = destination_details.get('batch')\n+    labels = DEFAULT_JOB_LABELS\n+    labels[\"bucket\"] = bucket_id\n+\n+    if batch_id:\n+        labels[\"batch-id\"] = batch_id\n+\n+    if partition:\n+        dest_table_ref = bigquery.TableReference.from_string(\n+            f\"{dataset}.{table}{partition}\", default_project=project)\n+    else:\n+        dest_table_ref = bigquery.TableReference.from_string(\n+            f\"{dataset}.{table}\", default_project=project)\n+\n+    default_query_config = bigquery.QueryJobConfig()\n+    default_query_config.use_legacy_sql = False\n+    default_query_config.labels = labels\n+    bq_client = bigquery.Client(client_info=CLIENT_INFO,\n+                                default_query_job_config=default_query_config)\n+\n+    print(f\"looking for {gsurl}_config/bq_transform.sql\")\n+    external_query_sql = read_gcs_file_if_exists(\n+        gcs_client, f\"{gsurl}_config/bq_transform.sql\")\n+    print(f\"external_query_sql = {external_query_sql}\")\n+    if not external_query_sql:\n+        external_query_sql = look_for_transform_sql(gcs_client, gsurl)\n+    if external_query_sql:\n+        print(\"EXTERNAL QUERY\")\n+        external_query(gcs_client, bq_client, gsurl, external_query_sql,\n+                       dest_table_ref)\n+        return\n+\n+    print(\"LOAD_JOB\")\n+    load_batches(gcs_client, bq_client, gsurl, dest_table_ref)\n+\n+\n+def handle_duplicate_notification(\n+    bkt: storage.Bucket,\n+    success_blob: storage.Blob,\n+    gsurl: str\n+):\n+    \"\"\"\n+    Need to handle potential duplicate Pub/Sub notifications.\n+    To achieve this we will drop an empty \"claimed\" file that indicates\n+    an invocation of this cloud function has picked up the success file\n+    with a certain creation timestamp. This will support republishing the\n+    success file as a mechanism of re-running the ingestion while avoiding\n+    duplicate ingestion due to multiple Pub/Sub messages for a success file\n+    with the same creation time.\n+    \"\"\"\n+    success_blob.reload()\n+    success_created_unix_timestamp = success_blob.time_created.timestamp()\n+\n+    claim_blob: storage.Blob = bkt.blob(\n+        f\"_claimed_{success_created_unix_timestamp}\")\n+    try:\n+        claim_blob.upload_from_string(\n+            \"\",\n+            if_generation_match=0)\n+    except PreconditionFailed as err:\n+        raise RuntimeError(\n+            f\"The prefix {gsurl} appears to already have been claimed for \"\n+            f\"{gsurl}{SUCCESS_FILENAME} with created timestamp\"\n+            f\"{success_created_unix_timestamp}.\"\n+            \"This means that another invocation of this cloud function has\"\n+            \"claimed the ingestion of this batch.\"\n+            \"This may be due to a rare duplicate delivery of the Pub/Sub \"\n+            \"storage notification.\") from err\n+\n+\n+def external_query(gcs_client, bq_client, gsurl, query, dest_table_ref):\n+    \"\"\"Load from query over external table from GCS.\n+\n+    This hinges on a SQL query defined in GCS at _config/bq_transform.sql and\n+    an external table definition _config/external.json (otherwise will assume\n+    parquet external table)\n+    \"\"\"\n+    external_table_config = read_gcs_file_if_exists(\n+        gcs_client, f\"{gsurl}_config/external.json\")\n+    if external_table_config:\n+        external_table_def = json.loads(external_table_config)\n+    else:\n+        print(f\"Falling back to default parquet external table.\"\n+              f\" {gsurl}/_config/external.json not found.\")\n+        external_table_def = DEFAULT_EXTERNAL_TABLE_DEFINITION\n+\n+    external_table_def[\"sourceUris\"] = flatten(\n+        get_batches_for_prefix(gcs_client, gsurl))\n+    external_config = bigquery.ExternalConfig.from_api_repr(external_table_def)\n+    job_config = bigquery.QueryJobConfig(\n+        table_definitions={\"temp_ext\": external_config}, use_legacy_sql=False)\n+    # for some reason query string literal wrapped in b''\n+    rendered_query = str(\n+        str(query).format(dest_dataset=dest_table_ref.dataset_id,\n+                          dest_table=dest_table_ref.table_id))[2:-1]", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bfc9622038f1ba8d1006e9057b69990fcedb5981"}, "originalPosition": 201}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjg5NzQ5Mw==", "bodyText": "TIL thank you.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r516897493", "createdAt": "2020-11-03T19:13:27Z", "author": {"login": "jaketf"}, "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -0,0 +1,459 @@\n+# Copyright 2020 Google LLC.\n+# This software is provided as-is, without warranty or representation\n+# for any use or purpose.\n+# Your use of it is subject to your agreement with Google.\n+\n+# Licensed under the Apache License, Version 2.0 (the 'License');\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+# http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an 'AS IS' BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Background Cloud Function for loading data from GCS to BigQuery.\n+\"\"\"\n+import json\n+import re\n+from collections import deque\n+from os import getenv\n+from pathlib import Path\n+from time import monotonic, sleep\n+from typing import Any, Deque, Dict, List, Optional, Tuple\n+from urllib.parse import urlparse\n+\n+from google.api_core.client_info import ClientInfo\n+from google.api_core.exceptions import PreconditionFailed\n+from google.cloud import bigquery, storage\n+from google.cloud.exceptions import NotFound\n+\n+# 15TB per BQ load job.\n+DEFAULT_MAX_BATCH_BYTES = str(15 * 10**12)\n+MAX_URIS_PER_LOAD = 10**4\n+\n+DEFAULT_EXTERNAL_TABLE_DEFINITION = {\"sourceFormat\": \"PARQUET\"}\n+\n+DEFAULT_JOB_LABELS = {\n+    \"component\": \"event-based-gcs-ingest\",\n+    \"cloud-function-name\": getenv(\"FUNCTION_NAME\"),\n+}\n+\n+BASE_LOAD_JOB_CONFIG = {\n+    \"sourceFormat\": \"CSV\",\n+    \"fieldDelimiter\": \"|\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+    \"labels\": DEFAULT_JOB_LABELS,\n+}\n+\n+DEFAULT_DESTINATION_REGEX = r\"(?P<dataset>[\\w\\-_0-9]+)/\" \\\n+                            r\"(?P<table>[\\w\\-_0-9]+)/\" \\\n+                            r\"?(?P<partition>\\$[0-9]+)?/\" \\\n+                            r\"?(?P<batch>[\\w\\-_0-9]+)?/\"\n+\n+# Will wait up to this polling for errors before exiting\n+# This is to check if job fail quickly, not to assert the succeed\n+# This may not be honored if longer than cloud function timeout\n+# https://cloud.google.com/functions/docs/concepts/exec#timeout\n+WAIT_FOR_JOB_SECONDS = 5\n+SUCCESS_FILENAME = getenv(\"SUCCESS_FILENAME\", \"_SUCCESS\")\n+\n+CLIENT_INFO = ClientInfo(user_agent=\"google-pso-tool/bq-severless-loader\")\n+\n+\n+def main(event: Dict, context):    # pylint: disable=unused-argument\n+    \"\"\"entry point for background cloud function for event driven GCS to\n+    BigQuery ingest.\"\"\"\n+    # pylint: disable=too-many-locals\n+    # Set by Cloud Function Execution Environment\n+    # https://cloud.google.com/functions/docs/env-var\n+    project = getenv(\"GCP_PROJECT\")\n+    destination_regex = getenv(\"DESTINATION_REGEX\", DEFAULT_DESTINATION_REGEX)\n+    dest_re = re.compile(destination_regex)\n+\n+    bucket_id, object_id = parse_notification(event)\n+\n+    # Exit eagerly if not a success file.\n+    # we can improve this with pub/sub message filtering once it supports\n+    # a hasSuffix filter function (we can filter on hasSuffix successfile name)\n+    #  https://cloud.google.com/pubsub/docs/filtering\n+    if not object_id.endswith(f\"/{SUCCESS_FILENAME}\"):\n+        print(\n+            f\"No-op. This notification was not for a {SUCCESS_FILENAME} file.\")\n+        return\n+\n+    prefix_to_load = removesuffix(object_id, SUCCESS_FILENAME)\n+    gsurl = f\"gs://{bucket_id}/{prefix_to_load}\"\n+    gcs_client = storage.Client(client_info=CLIENT_INFO)\n+    bkt = gcs_client.lookup_bucket(bucket_id)\n+    success_blob: storage.Blob = bkt.blob(object_id)\n+    handle_duplicate_notification(bkt, success_blob, gsurl)\n+\n+    destination_match = dest_re.match(object_id)\n+    if not destination_match:\n+        raise RuntimeError(f\"Object ID {object_id} did not match regex:\"\n+                           f\" {destination_regex}\")\n+    destination_details = destination_match.groupdict()\n+    try:\n+        dataset = destination_details['dataset']\n+        table = destination_details['table']\n+    except KeyError:\n+        raise RuntimeError(\n+            f\"Object ID {object_id} did not match dataset and table in regex:\"\n+            f\" {destination_regex}\") from KeyError\n+    partition = destination_details.get('partition')\n+    batch_id = destination_details.get('batch')\n+    labels = DEFAULT_JOB_LABELS\n+    labels[\"bucket\"] = bucket_id\n+\n+    if batch_id:\n+        labels[\"batch-id\"] = batch_id\n+\n+    if partition:\n+        dest_table_ref = bigquery.TableReference.from_string(\n+            f\"{dataset}.{table}{partition}\", default_project=project)\n+    else:\n+        dest_table_ref = bigquery.TableReference.from_string(\n+            f\"{dataset}.{table}\", default_project=project)\n+\n+    default_query_config = bigquery.QueryJobConfig()\n+    default_query_config.use_legacy_sql = False\n+    default_query_config.labels = labels\n+    bq_client = bigquery.Client(client_info=CLIENT_INFO,\n+                                default_query_job_config=default_query_config)\n+\n+    print(f\"looking for {gsurl}_config/bq_transform.sql\")\n+    external_query_sql = read_gcs_file_if_exists(\n+        gcs_client, f\"{gsurl}_config/bq_transform.sql\")\n+    print(f\"external_query_sql = {external_query_sql}\")\n+    if not external_query_sql:\n+        external_query_sql = look_for_transform_sql(gcs_client, gsurl)\n+    if external_query_sql:\n+        print(\"EXTERNAL QUERY\")\n+        external_query(gcs_client, bq_client, gsurl, external_query_sql,\n+                       dest_table_ref)\n+        return\n+\n+    print(\"LOAD_JOB\")\n+    load_batches(gcs_client, bq_client, gsurl, dest_table_ref)\n+\n+\n+def handle_duplicate_notification(\n+    bkt: storage.Bucket,\n+    success_blob: storage.Blob,\n+    gsurl: str\n+):\n+    \"\"\"\n+    Need to handle potential duplicate Pub/Sub notifications.\n+    To achieve this we will drop an empty \"claimed\" file that indicates\n+    an invocation of this cloud function has picked up the success file\n+    with a certain creation timestamp. This will support republishing the\n+    success file as a mechanism of re-running the ingestion while avoiding\n+    duplicate ingestion due to multiple Pub/Sub messages for a success file\n+    with the same creation time.\n+    \"\"\"\n+    success_blob.reload()\n+    success_created_unix_timestamp = success_blob.time_created.timestamp()\n+\n+    claim_blob: storage.Blob = bkt.blob(\n+        f\"_claimed_{success_created_unix_timestamp}\")\n+    try:\n+        claim_blob.upload_from_string(\n+            \"\",\n+            if_generation_match=0)\n+    except PreconditionFailed as err:\n+        raise RuntimeError(\n+            f\"The prefix {gsurl} appears to already have been claimed for \"\n+            f\"{gsurl}{SUCCESS_FILENAME} with created timestamp\"\n+            f\"{success_created_unix_timestamp}.\"\n+            \"This means that another invocation of this cloud function has\"\n+            \"claimed the ingestion of this batch.\"\n+            \"This may be due to a rare duplicate delivery of the Pub/Sub \"\n+            \"storage notification.\") from err\n+\n+\n+def external_query(gcs_client, bq_client, gsurl, query, dest_table_ref):\n+    \"\"\"Load from query over external table from GCS.\n+\n+    This hinges on a SQL query defined in GCS at _config/bq_transform.sql and\n+    an external table definition _config/external.json (otherwise will assume\n+    parquet external table)\n+    \"\"\"\n+    external_table_config = read_gcs_file_if_exists(\n+        gcs_client, f\"{gsurl}_config/external.json\")\n+    if external_table_config:\n+        external_table_def = json.loads(external_table_config)\n+    else:\n+        print(f\"Falling back to default parquet external table.\"\n+              f\" {gsurl}/_config/external.json not found.\")\n+        external_table_def = DEFAULT_EXTERNAL_TABLE_DEFINITION\n+\n+    external_table_def[\"sourceUris\"] = flatten(\n+        get_batches_for_prefix(gcs_client, gsurl))\n+    external_config = bigquery.ExternalConfig.from_api_repr(external_table_def)\n+    job_config = bigquery.QueryJobConfig(\n+        table_definitions={\"temp_ext\": external_config}, use_legacy_sql=False)\n+    # for some reason query string literal wrapped in b''\n+    rendered_query = str(\n+        str(query).format(dest_dataset=dest_table_ref.dataset_id,\n+                          dest_table=dest_table_ref.table_id))[2:-1]", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjc4ODk1Ng=="}, "originalCommit": {"oid": "bfc9622038f1ba8d1006e9057b69990fcedb5981"}, "originalPosition": 201}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzOTgxNDc2OnYy", "diffSide": "LEFT", "path": "cloud_functions/gcs_event_based_ingest/Dockerfile.ci", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMDozOTo1NVrOHs_qXw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQxNzowNTowN1rOHxWLpQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk0MjQzMQ==", "bodyText": "cc: @danieldeleo FYI because we know cloud build we have the source cloned locally we can remove this step speeding up image build time significantly as the ci container doesn't need the source inside the container.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r516942431", "createdAt": "2020-11-03T20:39:55Z", "author": {"login": "jaketf"}, "path": "cloud_functions/gcs_event_based_ingest/Dockerfile.ci", "diffHunk": "@@ -1,6 +1,4 @@\n FROM python:3.8-slim\n COPY requirements.txt requirements-dev.txt ./\n RUN pip3 install -r requirements-dev.txt\n-WORKDIR /ci", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "41dc1cd6abc540cecbab21223e49e7060695d5f7"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTUwNTcwMQ==", "bodyText": "I was wondering about this! Thanks for the fyi", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r521505701", "createdAt": "2020-11-11T17:05:07Z", "author": {"login": "danieldeleo"}, "path": "cloud_functions/gcs_event_based_ingest/Dockerfile.ci", "diffHunk": "@@ -1,6 +1,4 @@\n FROM python:3.8-slim\n COPY requirements.txt requirements-dev.txt ./\n RUN pip3 install -r requirements-dev.txt\n-WORKDIR /ci", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk0MjQzMQ=="}, "originalCommit": {"oid": "41dc1cd6abc540cecbab21223e49e7060695d5f7"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzOTgyMDMyOnYy", "diffSide": "RIGHT", "path": "cloud_functions/gcs_event_based_ingest/tests/test_gcs_ocn_bq_ingest_it.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMDo0MTo0OFrOHs_trg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMDo0MTo0OFrOHs_trg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk0MzI3OA==", "bodyText": "Pivoting to this approach has two benefits:\n\ntable.get requests are free (v.s SELECT COUNT(*) queries which cost / consume slots)\nspeeds up tests ~50%  on average (depending on what the bq load slot pool latency was).", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r516943278", "createdAt": "2020-11-03T20:41:48Z", "author": {"login": "jaketf"}, "path": "cloud_functions/gcs_event_based_ingest/tests/test_gcs_ocn_bq_ingest_it.py", "diffHunk": "@@ -374,15 +357,54 @@ def test_external_query_IT(bq, gcs_data, gcs_external_config, dest_dataset,\n         }\n     }\n     main.main(test_event, None)\n-    sleep(3)    # Need to wait on async query job\n-    validation_query_job = bq.query(f\"\"\"\n-        SELECT\n-            COUNT(*) as count\n-        FROM\n-          `{os.environ.get('GCP_PROJECT')}.{dest_dataset.dataset_id}.{dest_table.table_id}`\n-    \"\"\")\n-\n     test_data_file = os.path.join(TEST_DIR, \"resources\", \"test-data\", \"nation\",\n                                   \"part-m-00001\")\n-    for row in validation_query_job.result():\n-        assert row[\"count\"] == sum(1 for _ in open(test_data_file))\n+    expected_num_rows = sum(1 for _ in open(test_data_file))\n+    try:\n+        bq_wait_for_rows(bq, dest_table, expected_num_rows)\n+    except TimeoutError as err:\n+        raise AssertionError from err\n+\n+\n+def bq_wait_for_rows(bq_client: bigquery.Client, table: bigquery.Table,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "41dc1cd6abc540cecbab21223e49e7060695d5f7"}, "originalPosition": 200}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0NDk4NzUyOnYy", "diffSide": "RIGHT", "path": "cloud_functions/gcs_event_based_ingest/tests/resources/.gitignore", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQwMjoxMDo0OFrOHtwYTw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQwMjoxMDo0OFrOHtwYTw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc0MDYyMw==", "bodyText": "this doesn't match any files in source tree remove.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r517740623", "createdAt": "2020-11-05T02:10:48Z", "author": {"login": "jaketf"}, "path": "cloud_functions/gcs_event_based_ingest/tests/resources/.gitignore", "diffHunk": "@@ -0,0 +1 @@\n+hit_data_2020-06-25_09.tsv", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0bafe3e4ddcdfa5b4de35912b81d7e69677d89e"}, "originalPosition": 1}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2901, "cost": 1, "resetAt": "2021-11-12T19:05:54Z"}}}