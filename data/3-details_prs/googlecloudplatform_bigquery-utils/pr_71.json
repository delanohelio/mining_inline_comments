{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM4MjI5OTEx", "number": 71, "title": "SQL Crawler: Added crawler architecture and files", "bodyText": "", "createdAt": "2020-06-22T23:34:12Z", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71", "merged": true, "mergeCommit": {"oid": "ff2125daec1d2e8d22ed2568268baa868888a700"}, "closed": true, "closedAt": "2020-06-25T18:01:40Z", "author": {"login": "noah-kuo"}, "timelineItems": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABct5elQAH2gAyNDM4MjI5OTExOjUyNzUwZThmODMzMDAxNmE0YzljNDUzMTA0MTNjNzdhYjY2NTQxM2Y=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcuygTSAH2gAyNDM4MjI5OTExOmFiMDA2MTJiY2JjOGZhOTBmMjNjNDQ3ZWZlM2I4OWFkMzIzYjE2NDA=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "52750e8f8330016a4c9c45310413c77ab665413f", "author": {"user": {"login": "noah-kuo", "name": "Noah Kuo"}}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/52750e8f8330016a4c9c45310413c77ab665413f", "committedDate": "2020-06-22T23:30:40Z", "message": "SQL Crawler: Added crawler architecture and files"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM1NDUxNDA4", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#pullrequestreview-435451408", "createdAt": "2020-06-23T05:27:25Z", "commit": {"oid": "52750e8f8330016a4c9c45310413c77ab665413f"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwNToyNzoyNVrOGnZyBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwNTo0NzowM1rOGnaIbg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk3MDA1NQ==", "bodyText": "This can just be set()", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#discussion_r443970055", "createdAt": "2020-06-23T05:27:25Z", "author": {"login": "yzhvictor"}, "path": "tools/unsupervised_dataset/Crawler/Crawler.py", "diffHunk": "@@ -0,0 +1,101 @@\n+import requests\n+import queue\n+import CQNode\n+import Extractor\n+import CrawlerLog\n+\n+class Crawler:\n+    \"\"\" Contains the functions and logic to run and coordinate the crawling process. Given\n+        initial starting URLs and maximum size, the crawler will explore websites to look\n+        for SQL queries.\n+    \"\"\"\n+    \n+    def __init__(self, links, maxDepth=3, maxSize=500):\n+        \"\"\" Initializes the crawler and instance variables.\n+        \n+        Args:\n+            links: The root URLs to begin crawling from. Can be one or more.\n+            maxDepth: The maximum depth for the crawler to explore.\n+            maxSize: THe maximum number of links the crawler should explore.\n+    \n+        \"\"\"\n+        self.linkQueue = queue.Queue()\n+        self.seen = set([])", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52750e8f8330016a4c9c45310413c77ab665413f"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk3MjE5Ng==", "bodyText": "not self.linkQueue.empty()", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#discussion_r443972196", "createdAt": "2020-06-23T05:34:57Z", "author": {"login": "yzhvictor"}, "path": "tools/unsupervised_dataset/Crawler/Crawler.py", "diffHunk": "@@ -0,0 +1,101 @@\n+import requests\n+import queue\n+import CQNode\n+import Extractor\n+import CrawlerLog\n+\n+class Crawler:\n+    \"\"\" Contains the functions and logic to run and coordinate the crawling process. Given\n+        initial starting URLs and maximum size, the crawler will explore websites to look\n+        for SQL queries.\n+    \"\"\"\n+    \n+    def __init__(self, links, maxDepth=3, maxSize=500):\n+        \"\"\" Initializes the crawler and instance variables.\n+        \n+        Args:\n+            links: The root URLs to begin crawling from. Can be one or more.\n+            maxDepth: The maximum depth for the crawler to explore.\n+            maxSize: THe maximum number of links the crawler should explore.\n+    \n+        \"\"\"\n+        self.linkQueue = queue.Queue()\n+        self.seen = set([])\n+        self.maxDepth = maxDepth\n+        self.maxSize = maxSize\n+        self.log = CrawlerLog.CrawlerLog()\n+        self.count = 0\n+        \n+        for link in links:\n+            self.linkQueue.put(CQNode.CQNode(link, 0))\n+            self.seen.add(link)\n+        \n+    def crawl(self):\n+        \"\"\" Begins the crawling process using variables set earlier. Extracts\n+            queries by locating website-specific HTML tags or searching for\n+            common expression patterns. Writes queries to output after finishing\n+            each site.\n+        \"\"\"\n+        \n+        while (self.linkQueue.empty() == False):", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52750e8f8330016a4c9c45310413c77ab665413f"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk3MjYwNw==", "bodyText": "Instead of printing to console, try to print to some logger, such as https://docs.python.org/2/library/logging.html", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#discussion_r443972607", "createdAt": "2020-06-23T05:36:20Z", "author": {"login": "yzhvictor"}, "path": "tools/unsupervised_dataset/Crawler/Crawler.py", "diffHunk": "@@ -0,0 +1,101 @@\n+import requests\n+import queue\n+import CQNode\n+import Extractor\n+import CrawlerLog\n+\n+class Crawler:\n+    \"\"\" Contains the functions and logic to run and coordinate the crawling process. Given\n+        initial starting URLs and maximum size, the crawler will explore websites to look\n+        for SQL queries.\n+    \"\"\"\n+    \n+    def __init__(self, links, maxDepth=3, maxSize=500):\n+        \"\"\" Initializes the crawler and instance variables.\n+        \n+        Args:\n+            links: The root URLs to begin crawling from. Can be one or more.\n+            maxDepth: The maximum depth for the crawler to explore.\n+            maxSize: THe maximum number of links the crawler should explore.\n+    \n+        \"\"\"\n+        self.linkQueue = queue.Queue()\n+        self.seen = set([])\n+        self.maxDepth = maxDepth\n+        self.maxSize = maxSize\n+        self.log = CrawlerLog.CrawlerLog()\n+        self.count = 0\n+        \n+        for link in links:\n+            self.linkQueue.put(CQNode.CQNode(link, 0))\n+            self.seen.add(link)\n+        \n+    def crawl(self):\n+        \"\"\" Begins the crawling process using variables set earlier. Extracts\n+            queries by locating website-specific HTML tags or searching for\n+            common expression patterns. Writes queries to output after finishing\n+            each site.\n+        \"\"\"\n+        \n+        while (self.linkQueue.empty() == False):\n+            # Retrieve the next link in the queue\n+            nextNode = self.linkQueue.get()\n+            nodeURL = nextNode.getURL()\n+            nodeDepth = nextNode.getDepth()\n+            \n+            # Check if crawler has exceeded maximum depth or maximum count\n+            if nodeDepth >= self.maxDepth or self.count >= self.maxSize:\n+                print(\"Crawled {0} websites.\".format(self.count))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52750e8f8330016a4c9c45310413c77ab665413f"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk3MzcwOQ==", "bodyText": "Instead of creating customized log mechanism, why not use python's logging library to log messages?", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#discussion_r443973709", "createdAt": "2020-06-23T05:40:15Z", "author": {"login": "yzhvictor"}, "path": "tools/unsupervised_dataset/Crawler/CrawlerLog.py", "diffHunk": "@@ -0,0 +1,71 @@\n+import datetime\n+import csv\n+import os\n+\n+class CrawlerLog:\n+    \"\"\" The CrawlerLog keeps track of which websites were explored, how many queries were found,\n+        and creates a CSV with all the queries. It also logs any errors encountered. The log is\n+        saved into Logs subdirectory with name based on start time. Queries are saved into Queries\n+        subdirectory.\n+    \"\"\"\n+    \n+    def __init__(self):\n+        \"\"\" Initializes crawler log to keep track of crawler progress and instantiates\n+            instance variables.\n+        \"\"\"\n+        \n+        self.startTime = datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M\")\n+        \n+        # Create directory for logs if it does not already exists\n+        try:\n+            os.mkdir(\"Logs\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52750e8f8330016a4c9c45310413c77ab665413f"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk3NDk3Mw==", "bodyText": "I think we need to handle this case more carefully. Imagine people mistakenly run the tool with an old path, it may unexpectedly overwrite some existing crawled data?", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#discussion_r443974973", "createdAt": "2020-06-23T05:44:24Z", "author": {"login": "yzhvictor"}, "path": "tools/unsupervised_dataset/Crawler/CrawlerLog.py", "diffHunk": "@@ -0,0 +1,71 @@\n+import datetime\n+import csv\n+import os\n+\n+class CrawlerLog:\n+    \"\"\" The CrawlerLog keeps track of which websites were explored, how many queries were found,\n+        and creates a CSV with all the queries. It also logs any errors encountered. The log is\n+        saved into Logs subdirectory with name based on start time. Queries are saved into Queries\n+        subdirectory.\n+    \"\"\"\n+    \n+    def __init__(self):\n+        \"\"\" Initializes crawler log to keep track of crawler progress and instantiates\n+            instance variables.\n+        \"\"\"\n+        \n+        self.startTime = datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M\")\n+        \n+        # Create directory for logs if it does not already exists\n+        try:\n+            os.mkdir(\"Logs\")\n+        except FileExistsError as e:\n+            pass\n+        \n+        self.log = open(\"Logs/log-\" + self.startTime + \".txt\", \"a\")\n+        self.log.write(\"Beginning crawl at time {0}.\\n\".format(self.startTime))\n+        \n+        try:\n+            os.mkdir(\"Queries\")\n+        except FileExistsError as e:\n+            pass", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52750e8f8330016a4c9c45310413c77ab665413f"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk3NTc5MA==", "bodyText": "In general, it's good to have tests covering the actual logic. So maybe you can consider adding some corresponding tests for each source files.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#discussion_r443975790", "createdAt": "2020-06-23T05:47:03Z", "author": {"login": "yzhvictor"}, "path": "tools/unsupervised_dataset/Crawler/CQNode.py", "diffHunk": "@@ -0,0 +1,32 @@\n+class CQNode:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52750e8f8330016a4c9c45310413c77ab665413f"}, "originalPosition": 1}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM2MTA2NTM0", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#pullrequestreview-436106534", "createdAt": "2020-06-23T19:53:36Z", "commit": {"oid": "52750e8f8330016a4c9c45310413c77ab665413f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxOTo1MzozNlrOGn4R4g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxOTo1MzozNlrOGn4R4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ2OTczMA==", "bodyText": "I am not expert in Python, is this recommended practice?\nI think we usually check for existence before making directories, instead of ignoring exceptions?", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#discussion_r444469730", "createdAt": "2020-06-23T19:53:36Z", "author": {"login": "kikkyo"}, "path": "tools/unsupervised_dataset/Crawler/CrawlerLog.py", "diffHunk": "@@ -0,0 +1,71 @@\n+import datetime\n+import csv\n+import os\n+\n+class CrawlerLog:\n+    \"\"\" The CrawlerLog keeps track of which websites were explored, how many queries were found,\n+        and creates a CSV with all the queries. It also logs any errors encountered. The log is\n+        saved into Logs subdirectory with name based on start time. Queries are saved into Queries\n+        subdirectory.\n+    \"\"\"\n+    \n+    def __init__(self):\n+        \"\"\" Initializes crawler log to keep track of crawler progress and instantiates\n+            instance variables.\n+        \"\"\"\n+        \n+        self.startTime = datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M\")\n+        \n+        # Create directory for logs if it does not already exists\n+        try:\n+            os.mkdir(\"Logs\")\n+        except FileExistsError as e:\n+            pass", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52750e8f8330016a4c9c45310413c77ab665413f"}, "originalPosition": 23}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM2MTA2NzMy", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#pullrequestreview-436106732", "createdAt": "2020-06-23T19:53:53Z", "commit": {"oid": "52750e8f8330016a4c9c45310413c77ab665413f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxOTo1Mzo1NFrOGn4ScQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxOTo1Mzo1NFrOGn4ScQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ2OTg3Mw==", "bodyText": "same here", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#discussion_r444469873", "createdAt": "2020-06-23T19:53:54Z", "author": {"login": "kikkyo"}, "path": "tools/unsupervised_dataset/Crawler/CrawlerLog.py", "diffHunk": "@@ -0,0 +1,71 @@\n+import datetime\n+import csv\n+import os\n+\n+class CrawlerLog:\n+    \"\"\" The CrawlerLog keeps track of which websites were explored, how many queries were found,\n+        and creates a CSV with all the queries. It also logs any errors encountered. The log is\n+        saved into Logs subdirectory with name based on start time. Queries are saved into Queries\n+        subdirectory.\n+    \"\"\"\n+    \n+    def __init__(self):\n+        \"\"\" Initializes crawler log to keep track of crawler progress and instantiates\n+            instance variables.\n+        \"\"\"\n+        \n+        self.startTime = datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M\")\n+        \n+        # Create directory for logs if it does not already exists\n+        try:\n+            os.mkdir(\"Logs\")\n+        except FileExistsError as e:\n+            pass\n+        \n+        self.log = open(\"Logs/log-\" + self.startTime + \".txt\", \"a\")\n+        self.log.write(\"Beginning crawl at time {0}.\\n\".format(self.startTime))\n+        \n+        try:\n+            os.mkdir(\"Queries\")\n+        except FileExistsError as e:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52750e8f8330016a4c9c45310413c77ab665413f"}, "originalPosition": 30}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM2MTExNjA3", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#pullrequestreview-436111607", "createdAt": "2020-06-23T20:00:47Z", "commit": {"oid": "52750e8f8330016a4c9c45310413c77ab665413f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QyMDowMDo0OFrOGn4hPg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QyMDowMDo0OFrOGn4hPg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ3MzY2Mg==", "bodyText": "nit: seems style guide recommends us using: # TODO(kl@gmail.com): Use a \"*\" here for string repetition.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#discussion_r444473662", "createdAt": "2020-06-23T20:00:48Z", "author": {"login": "kikkyo"}, "path": "tools/unsupervised_dataset/Crawler/run_crawler.py", "diffHunk": "@@ -0,0 +1,16 @@\n+import requests\n+import sys\n+import Crawler\n+\n+# Initializes a crawler and starts the crawling process using command line arguments\n+def start_crawler():\n+    # TO-DO: Allow user to optionally define max size or max depth", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52750e8f8330016a4c9c45310413c77ab665413f"}, "originalPosition": 7}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM2MTEzMTE2", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#pullrequestreview-436113116", "createdAt": "2020-06-23T20:03:06Z", "commit": {"oid": "52750e8f8330016a4c9c45310413c77ab665413f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QyMDowMzowNlrOGn4lhg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QyMDowMzowNlrOGn4lhg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ3NDc1OA==", "bodyText": "nit: add a comment on \"URL should be  separated by comma or space/something else\" ?", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#discussion_r444474758", "createdAt": "2020-06-23T20:03:06Z", "author": {"login": "kikkyo"}, "path": "tools/unsupervised_dataset/Crawler/README.md", "diffHunk": "@@ -0,0 +1,11 @@\n+# SQL Crawler\n+\n+This directory contains the code to run a universal, unsupervised SQL web crawler. The user provides a starting target URL from which to begin crawling, and has the option to set the maximum depth or size of the crawler.\n+\n+## Usage\n+To run the crawler, run the following command:\n+\n+```\n+python3 run_crawler.py <starting URLs>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52750e8f8330016a4c9c45310413c77ab665413f"}, "originalPosition": 9}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9f93dc65b95a7a165f7a1e2b4df9610b82f2a9f9", "author": {"user": {"login": "noah-kuo", "name": "Noah Kuo"}}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/9f93dc65b95a7a165f7a1e2b4df9610b82f2a9f9", "committedDate": "2020-06-23T23:47:49Z", "message": "Edited formatting and made fixes, set up testing architecture"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cb6e33705d656dfaace43675e9d5f29521dd5fc3", "author": {"user": {"login": "noah-kuo", "name": "Noah Kuo"}}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/cb6e33705d656dfaace43675e9d5f29521dd5fc3", "committedDate": "2020-06-24T23:49:18Z", "message": "Renamed directory structure, added additional tests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM3MDg3ODI0", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#pullrequestreview-437087824", "createdAt": "2020-06-25T00:12:01Z", "commit": {"oid": "cb6e33705d656dfaace43675e9d5f29521dd5fc3"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM3MTg5Njc0", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#pullrequestreview-437189674", "createdAt": "2020-06-25T06:01:12Z", "commit": {"oid": "cb6e33705d656dfaace43675e9d5f29521dd5fc3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQwNjowMToxMlrOGoscvw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQwNjowMToxMlrOGoscvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTMyNDQ3OQ==", "bodyText": "just a second thought on the output file format (not necessarily to be resolved in the cl:)), if we make it as CSV, later would it be problematic to read the query for later processing? For example, if a multi-line query is not well formatted.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#discussion_r445324479", "createdAt": "2020-06-25T06:01:12Z", "author": {"login": "kikkyo"}, "path": "tools/unsupervised_dataset/SQLCrawler/CrawlerLog.py", "diffHunk": "@@ -0,0 +1,76 @@\n+import datetime\n+import csv\n+import os\n+import logging\n+import pathlib\n+\n+class CrawlerLog(object):\n+    \"\"\" Logs the status of the SQL crawler, including websites and queries.\n+        \n+        The CrawlerLog keeps track of which websites were explored, how many\n+        queries were found, and creates a CSV with all the queries. It also", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cb6e33705d656dfaace43675e9d5f29521dd5fc3"}, "originalPosition": 11}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM3MTk1MzM2", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#pullrequestreview-437195336", "createdAt": "2020-06-25T06:15:05Z", "commit": {"oid": "cb6e33705d656dfaace43675e9d5f29521dd5fc3"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM3MTk1Njg0", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#pullrequestreview-437195684", "createdAt": "2020-06-25T06:15:59Z", "commit": {"oid": "cb6e33705d656dfaace43675e9d5f29521dd5fc3"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ab00612bcbc8fa90f23c447efe3b89ad323b1640", "author": {"user": {"login": "noah-kuo", "name": "Noah Kuo"}}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/ab00612bcbc8fa90f23c447efe3b89ad323b1640", "committedDate": "2020-06-25T17:57:08Z", "message": "Made style and formatting changes to file and variable naming"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 582, "cost": 1, "resetAt": "2021-11-01T14:20:25Z"}}}