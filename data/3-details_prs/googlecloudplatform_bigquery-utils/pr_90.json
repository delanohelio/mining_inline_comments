{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ3MDg5ODcw", "number": 90, "title": "SQL Crawler: Support for command-line arguments and more SQL expressions, additional test cases", "bodyText": "", "createdAt": "2020-07-09T20:57:00Z", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/90", "merged": true, "mergeCommit": {"oid": "c94dfcb08277ad89dfbada8b9da376ee3e3f207e"}, "closed": true, "closedAt": "2020-07-20T21:40:46Z", "author": {"login": "noah-kuo"}, "timelineItems": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcyaXG4AH2gAyNDQ3MDg5ODcwOjRlY2U3NmRiNmUxMTMzNmM3MDkyMzYyY2JmM2I0OTAyYjllZWUzZjU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABc24rpYgH2gAyNDQ3MDg5ODcwOjY4NzhkNzIyNjgzYjg0OTVlOTVmNDQyNDAxZTc2NjI2MDY2MTI3YTY=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "4ece76db6e11336c7092362cbf3b4902b9eee3f5", "author": {"user": {"login": "noah-kuo", "name": "Noah Kuo"}}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/4ece76db6e11336c7092362cbf3b4902b9eee3f5", "committedDate": "2020-07-07T00:05:04Z", "message": "Simplified generic extraction by making regex pattern stricter"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d71580d5ec2c2598fdda3e4eb949721c9be25df0", "author": {"user": {"login": "noah-kuo", "name": "Noah Kuo"}}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/d71580d5ec2c2598fdda3e4eb949721c9be25df0", "committedDate": "2020-07-07T00:06:28Z", "message": "Added more command line args and adjusted defaults"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d5d44cb64260fde4b40b3f6308313a7c5d50c40f", "author": {"user": {"login": "noah-kuo", "name": "Noah Kuo"}}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/d5d44cb64260fde4b40b3f6308313a7c5d50c40f", "committedDate": "2020-07-09T20:55:24Z", "message": "Added additional SQL extraction keywords and further tests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ1OTkzMzkw", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/90#pullrequestreview-445993390", "createdAt": "2020-07-09T21:50:48Z", "commit": {"oid": "d5d44cb64260fde4b40b3f6308313a7c5d50c40f"}, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQyMTo1MDo0OFrOGvjCSg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQyMTo1MTo0OFrOGvjEFw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjUxMDI4Mg==", "bodyText": "Is the list separated by comma or something? maybe add that as well in description", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/90#discussion_r452510282", "createdAt": "2020-07-09T21:50:48Z", "author": {"login": "yzhvictor"}, "path": "tools/unsupervised_dataset/run_crawler.py", "diffHunk": "@@ -1,11 +1,16 @@\n \"\"\" Script to initialize the SQL crawler on a website of the user's choice \"\"\"\n \n import sys\n+import argparse\n from sql_crawler import crawler\n \n def start_crawler():\n-    urls = sys.argv[1:]\n-    new_crawler = crawler.Crawler(urls, max_size=50)\n+    parser = argparse.ArgumentParser(description=\"SQL Web Crawler\")\n+    parser.add_argument(\"urls\", help=\"A list of URLs to be crawled\", nargs='+')", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d5d44cb64260fde4b40b3f6308313a7c5d50c40f"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjUxMDc0Mw==", "bodyText": "Is this the standard? looks like an array, but it should be an int I think.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/90#discussion_r452510743", "createdAt": "2020-07-09T21:51:48Z", "author": {"login": "yzhvictor"}, "path": "tools/unsupervised_dataset/run_crawler.py", "diffHunk": "@@ -1,11 +1,16 @@\n \"\"\" Script to initialize the SQL crawler on a website of the user's choice \"\"\"\n \n import sys\n+import argparse\n from sql_crawler import crawler\n \n def start_crawler():\n-    urls = sys.argv[1:]\n-    new_crawler = crawler.Crawler(urls, max_size=50)\n+    parser = argparse.ArgumentParser(description=\"SQL Web Crawler\")\n+    parser.add_argument(\"urls\", help=\"A list of URLs to be crawled\", nargs='+')\n+    parser.add_argument(\"--max_depth\", help=\"The max depth of the crawler (default=3)\", type=int, nargs=1, default=[3])\n+    parser.add_argument(\"--max_size\", help=\"The maximum number of links to be crawled (default=100)\", type=int, nargs=1, default=[100])\n+    args = parser.parse_args()\n+    new_crawler = crawler.Crawler(args.urls, max_size=args.max_size[0], max_depth=args.max_depth[0])", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d5d44cb64260fde4b40b3f6308313a7c5d50c40f"}, "originalPosition": 15}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0e6f164af159ef8383c030f20958260fc8ac4854", "author": {"user": {"login": "noah-kuo", "name": "Noah Kuo"}}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/0e6f164af159ef8383c030f20958260fc8ac4854", "committedDate": "2020-07-09T22:34:33Z", "message": "Fixed command-line args and updated README"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bcc2fde0767ecc57b9d1e6e61d7976c95343d585", "author": {"user": {"login": "noah-kuo", "name": "Noah Kuo"}}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/bcc2fde0767ecc57b9d1e6e61d7976c95343d585", "committedDate": "2020-07-16T18:45:09Z", "message": "Added cloud output integration and tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d41c07830266e40768ae5e7f16183de1472c188a", "author": {"user": {"login": "noah-kuo", "name": "Noah Kuo"}}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/d41c07830266e40768ae5e7f16183de1472c188a", "committedDate": "2020-07-16T18:50:21Z", "message": "Merge branch 'master' into crawler"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUwMTI4MTM3", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/90#pullrequestreview-450128137", "createdAt": "2020-07-16T18:58:46Z", "commit": {"oid": "d41c07830266e40768ae5e7f16183de1472c188a"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQxODo1ODo0N1rOGy4WXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQxOTowMzoxMVrOGy4f_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjAwNTIxMg==", "bodyText": "Will there be some exceptions such as permission issue, not found issue, should we handle them?", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/90#discussion_r456005212", "createdAt": "2020-07-16T18:58:47Z", "author": {"login": "yzhvictor"}, "path": "tools/unsupervised_dataset/sql_crawler/cloud_integration.py", "diffHunk": "@@ -0,0 +1,61 @@\n+\"\"\" Contains functions to upload data to Google Cloud Storage and Google\n+Bigquery. These functions take input specifying where to store the file\n+and return a message denoting success or error.\n+\"\"\"\n+\n+from google.cloud import bigquery, storage\n+\n+def load_bigquery_table(project_id, dataset_id, table_id, filename):\n+    \"\"\" Uploads a file to Google BigQuery.\n+    \n+    Args:\n+        project_id: Google Cloud Project ID for destination.\n+        dataset_id: Name of destination dataset.\n+        table_id: Name of destination table.\n+        filename: Name of CSV file to be uploaded.\n+    \n+    Returns:\n+        A log message depending on success or failure.\n+    \"\"\"\n+    \n+    client = bigquery.Client(project=project_id)\n+    \n+    dataset_ref = client.dataset(dataset_id)\n+    table_ref = dataset_ref.table(table_id)\n+    job_config = bigquery.LoadJobConfig()\n+    job_config.source_format = bigquery.SourceFormat.CSV\n+    job_config.skip_leading_rows = 1\n+    job_config.autodetect = True\n+\n+    job_config.schema = [\n+        bigquery.SchemaField(\"query\", \"STRING\", mode=\"NULLABLE\"),\n+        bigquery.SchemaField(\"url\", \"STRING\", mode=\"NULLABLE\"),\n+    ]\n+    \n+    with open(filename, \"rb\") as source_file:\n+        job = client.load_table_from_file(source_file, table_ref, job_config=job_config)\n+    \n+    job.result()\n+    \n+    return \"Loaded {} rows into {}:{}.\".format(job.output_rows, dataset_id, table_id)\n+\n+def upload_gcs_file(project_id, bucket_id, destination_blob_name, filename):\n+    \"\"\" Uploads a file to Google Cloud Storage.\n+    \n+    Args:\n+        project_id: Google Cloud Project ID for destination.\n+        bucket_id: Name of destination bucket.\n+        destination_blob_name: Name of destination file.\n+        filename: Name of file to be uploaded.\n+\n+    Returns:\n+        A log message depending on success or failure.\n+    \"\"\"\n+\n+    storage_client = storage.Client(project=project_id)\n+    bucket = storage_client.bucket(bucket_id)\n+    blob = bucket.blob(destination_blob_name)\n+\n+    blob.upload_from_filename(filename)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d41c07830266e40768ae5e7f16183de1472c188a"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjAwNTg2MQ==", "bodyText": "Do we get anything from the return of this method call? Or it's just to make sure thread is blocked.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/90#discussion_r456005861", "createdAt": "2020-07-16T18:59:52Z", "author": {"login": "yzhvictor"}, "path": "tools/unsupervised_dataset/sql_crawler/cloud_integration.py", "diffHunk": "@@ -0,0 +1,61 @@\n+\"\"\" Contains functions to upload data to Google Cloud Storage and Google\n+Bigquery. These functions take input specifying where to store the file\n+and return a message denoting success or error.\n+\"\"\"\n+\n+from google.cloud import bigquery, storage\n+\n+def load_bigquery_table(project_id, dataset_id, table_id, filename):\n+    \"\"\" Uploads a file to Google BigQuery.\n+    \n+    Args:\n+        project_id: Google Cloud Project ID for destination.\n+        dataset_id: Name of destination dataset.\n+        table_id: Name of destination table.\n+        filename: Name of CSV file to be uploaded.\n+    \n+    Returns:\n+        A log message depending on success or failure.\n+    \"\"\"\n+    \n+    client = bigquery.Client(project=project_id)\n+    \n+    dataset_ref = client.dataset(dataset_id)\n+    table_ref = dataset_ref.table(table_id)\n+    job_config = bigquery.LoadJobConfig()\n+    job_config.source_format = bigquery.SourceFormat.CSV\n+    job_config.skip_leading_rows = 1\n+    job_config.autodetect = True\n+\n+    job_config.schema = [\n+        bigquery.SchemaField(\"query\", \"STRING\", mode=\"NULLABLE\"),\n+        bigquery.SchemaField(\"url\", \"STRING\", mode=\"NULLABLE\"),\n+    ]\n+    \n+    with open(filename, \"rb\") as source_file:\n+        job = client.load_table_from_file(source_file, table_ref, job_config=job_config)\n+    \n+    job.result()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d41c07830266e40768ae5e7f16183de1472c188a"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjAwNzY3Ng==", "bodyText": "Do we also need to check gcs_bucket, and bq_dataset for the below method?", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/90#discussion_r456007676", "createdAt": "2020-07-16T19:03:11Z", "author": {"login": "yzhvictor"}, "path": "tools/unsupervised_dataset/sql_crawler/crawler_log.py", "diffHunk": "@@ -66,11 +71,68 @@ def log_error(self, errorMessage):\n         Args:\n             str: Error message to be logged.\n         \"\"\"\n-\n+        \n+        self.error_log_count += 1\n         logging.error(\"ERROR: %s\", errorMessage)\n+        \n+    def parse_location_arg(self, location):\n+        \"\"\" Validates and splits location argument for cloud upload\n+        into two parts. Should be formatted as project_id.dataset.\n+        \n+        Args:\n+            location: String with name of project ID and dataset.\n+            \n+        Returns\n+            List of separate strings after splitting location.\n+        \"\"\"\n+        if location.count(\".\") != 1:\n+            self.log_error(\"Argument not formatted correctly: {0}\".format(location))\n+            return None, None\n+        \n+        return location.split(\".\")\n+        \n+    def set_gcs(self, location):\n+        \"\"\" Sets variables for uploading data to Google Cloud Storage.\n+            \n+        Args:\n+            location: String with name of project ID and bucket name,\n+            separated by a period.\n+        \"\"\"\n+\n+        self.gcs_project, self.gcs_bucket = self.parse_location_arg(location)\n+        if self.gcs_project:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d41c07830266e40768ae5e7f16183de1472c188a"}, "originalPosition": 77}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "641e69949e055897527fc874dca797c88a13a759", "author": {"user": {"login": "noah-kuo", "name": "Noah Kuo"}}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/641e69949e055897527fc874dca797c88a13a759", "committedDate": "2020-07-16T20:02:51Z", "message": "Added additional error handling"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8b285a472db9e7abb25b0b8778caf3323aa7c20c", "author": {"user": {"login": "noah-kuo", "name": "Noah Kuo"}}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/8b285a472db9e7abb25b0b8778caf3323aa7c20c", "committedDate": "2020-07-16T20:43:22Z", "message": "Merge branch 'crawler' of https://github.com/noah-kuo/bigquery-utils into crawler"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6878d722683b8495e95f442401e76626066127a6", "author": {"user": {"login": "noah-kuo", "name": "Noah Kuo"}}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/6878d722683b8495e95f442401e76626066127a6", "committedDate": "2020-07-20T21:40:21Z", "message": "Merge branch 'master' into crawler"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 641, "cost": 1, "resetAt": "2021-11-01T14:20:25Z"}}}