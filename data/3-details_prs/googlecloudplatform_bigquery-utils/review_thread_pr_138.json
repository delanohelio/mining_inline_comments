{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDYzNjk0NzY1", "number": 138, "reviewThreads": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxNjoyNTo0N1rOEV5r2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxNjozMzoxMVrOEV52CA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDAwNjY1OnYy", "diffSide": "RIGHT", "path": "tools/unsupervised_dataset/sql_classifier/parserc/src/main/java/Parserc.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxNjoyNTo0N1rOG86-_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxNjoyNTo0N1rOG86-_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjUzNDE0Mw==", "bodyText": "Why it's called like this? Should be Classifier or something similar?", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/138#discussion_r466534143", "createdAt": "2020-08-06T16:25:47Z", "author": {"login": "yzhvictor"}, "path": "tools/unsupervised_dataset/sql_classifier/parserc/src/main/java/Parserc.java", "diffHunk": "@@ -0,0 +1,156 @@\n+import com.opencsv.*;\n+import org.apache.calcite.sql.parser.SqlParseException;\n+import org.apache.calcite.sql.parser.SqlParser;\n+\n+import org.apache.calcite.sql.parser.dialect1.Dialect1ParserImpl;\n+import org.apache.calcite.sql.parser.bigquery.BigQueryParserImpl;\n+import org.apache.calcite.sql.parser.defaultdialect.DefaultDialectParserImpl;\n+import org.apache.calcite.sql.parser.postgresql.PostgreSQLParserImpl;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.FileWriter;\n+import java.text.SimpleDateFormat;\n+import java.util.List;\n+import java.util.Date;\n+\n+public class Parserc {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8523b45c397cf1120c9bf380ebeb532c9836f40e"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDAyMjgzOnYy", "diffSide": "RIGHT", "path": "tools/unsupervised_dataset/sql_classifier/parserc/src/main/java/Parserc.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxNjozMDoxN1rOG87JPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMTo0OTozNVrOG9FaVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjUzNjc2NQ==", "bodyText": "Instead of adding it one by one, is it possible to do it in a loop to add all dialects Calcite has. Therefore, when Calcite has new dialect, it will be automatically included?", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/138#discussion_r466536765", "createdAt": "2020-08-06T16:30:17Z", "author": {"login": "yzhvictor"}, "path": "tools/unsupervised_dataset/sql_classifier/parserc/src/main/java/Parserc.java", "diffHunk": "@@ -0,0 +1,156 @@\n+import com.opencsv.*;\n+import org.apache.calcite.sql.parser.SqlParseException;\n+import org.apache.calcite.sql.parser.SqlParser;\n+\n+import org.apache.calcite.sql.parser.dialect1.Dialect1ParserImpl;\n+import org.apache.calcite.sql.parser.bigquery.BigQueryParserImpl;\n+import org.apache.calcite.sql.parser.defaultdialect.DefaultDialectParserImpl;\n+import org.apache.calcite.sql.parser.postgresql.PostgreSQLParserImpl;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.FileWriter;\n+import java.text.SimpleDateFormat;\n+import java.util.List;\n+import java.util.Date;\n+\n+public class Parserc {\n+    /*\n+     * Runs the classification tool. Takes a CSV file of queries, classifies queries based on dialect, and creates\n+     * several subdirectories to store the queries by dialect.\n+     *\n+     * @param args Command line arguments\n+     */\n+    public static void main(String[] args) {\n+        if (args.length == 0) {\n+            System.out.println(\"Please provide a CSV file.\");\n+            return;\n+        }\n+        List<String[]> allData = readCSV(args[0]);\n+        if (allData == null) {\n+            return;\n+        }\n+        SqlParser.Config[] parserConfigs = new SqlParser.Config[4];\n+        parserConfigs[0] = SqlParser.configBuilder().setParserFactory(DefaultDialectParserImpl.FACTORY).build();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8523b45c397cf1120c9bf380ebeb532c9836f40e"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjcwNDk4MA==", "bodyText": "I'm not sure if this is possible, especially with the workaround we are using since we cannot get some of the dependencies online yet? I think this would definitely make the most sense when we have the capabilities, so this can be a change once it is available.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/138#discussion_r466704980", "createdAt": "2020-08-06T21:49:35Z", "author": {"login": "noah-kuo"}, "path": "tools/unsupervised_dataset/sql_classifier/parserc/src/main/java/Parserc.java", "diffHunk": "@@ -0,0 +1,156 @@\n+import com.opencsv.*;\n+import org.apache.calcite.sql.parser.SqlParseException;\n+import org.apache.calcite.sql.parser.SqlParser;\n+\n+import org.apache.calcite.sql.parser.dialect1.Dialect1ParserImpl;\n+import org.apache.calcite.sql.parser.bigquery.BigQueryParserImpl;\n+import org.apache.calcite.sql.parser.defaultdialect.DefaultDialectParserImpl;\n+import org.apache.calcite.sql.parser.postgresql.PostgreSQLParserImpl;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.FileWriter;\n+import java.text.SimpleDateFormat;\n+import java.util.List;\n+import java.util.Date;\n+\n+public class Parserc {\n+    /*\n+     * Runs the classification tool. Takes a CSV file of queries, classifies queries based on dialect, and creates\n+     * several subdirectories to store the queries by dialect.\n+     *\n+     * @param args Command line arguments\n+     */\n+    public static void main(String[] args) {\n+        if (args.length == 0) {\n+            System.out.println(\"Please provide a CSV file.\");\n+            return;\n+        }\n+        List<String[]> allData = readCSV(args[0]);\n+        if (allData == null) {\n+            return;\n+        }\n+        SqlParser.Config[] parserConfigs = new SqlParser.Config[4];\n+        parserConfigs[0] = SqlParser.configBuilder().setParserFactory(DefaultDialectParserImpl.FACTORY).build();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjUzNjc2NQ=="}, "originalCommit": {"oid": "8523b45c397cf1120c9bf380ebeb532c9836f40e"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDAyNjMwOnYy", "diffSide": "RIGHT", "path": "tools/unsupervised_dataset/sql_classifier/parserc/src/main/java/Parserc.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxNjozMToxMlrOG87LUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxNjozMToxMlrOG87LUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjUzNzI5Ng==", "bodyText": "consider using logger instead of system.out.print", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/138#discussion_r466537296", "createdAt": "2020-08-06T16:31:12Z", "author": {"login": "yzhvictor"}, "path": "tools/unsupervised_dataset/sql_classifier/parserc/src/main/java/Parserc.java", "diffHunk": "@@ -0,0 +1,156 @@\n+import com.opencsv.*;\n+import org.apache.calcite.sql.parser.SqlParseException;\n+import org.apache.calcite.sql.parser.SqlParser;\n+\n+import org.apache.calcite.sql.parser.dialect1.Dialect1ParserImpl;\n+import org.apache.calcite.sql.parser.bigquery.BigQueryParserImpl;\n+import org.apache.calcite.sql.parser.defaultdialect.DefaultDialectParserImpl;\n+import org.apache.calcite.sql.parser.postgresql.PostgreSQLParserImpl;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.FileWriter;\n+import java.text.SimpleDateFormat;\n+import java.util.List;\n+import java.util.Date;\n+\n+public class Parserc {\n+    /*\n+     * Runs the classification tool. Takes a CSV file of queries, classifies queries based on dialect, and creates\n+     * several subdirectories to store the queries by dialect.\n+     *\n+     * @param args Command line arguments\n+     */\n+    public static void main(String[] args) {\n+        if (args.length == 0) {\n+            System.out.println(\"Please provide a CSV file.\");\n+            return;\n+        }\n+        List<String[]> allData = readCSV(args[0]);\n+        if (allData == null) {\n+            return;\n+        }\n+        SqlParser.Config[] parserConfigs = new SqlParser.Config[4];\n+        parserConfigs[0] = SqlParser.configBuilder().setParserFactory(DefaultDialectParserImpl.FACTORY).build();\n+        parserConfigs[1] = SqlParser.configBuilder().setParserFactory(Dialect1ParserImpl.FACTORY).build();\n+        parserConfigs[2] = SqlParser.configBuilder().setParserFactory(BigQueryParserImpl.FACTORY).build();\n+        parserConfigs[3] = SqlParser.configBuilder().setParserFactory(PostgreSQLParserImpl.FACTORY).build();\n+\n+        CSVWriter[] writers = setupOutput();\n+\n+        for (String[] data : allData) {\n+            boolean[] results = classifyQuery(cleanQuery(data[0]), parserConfigs);\n+            boolean unclassified = true;\n+            String[] nextLine = {data[0], data[1]};\n+            for (int i = 0; i < results.length; i++) {\n+                if (results[i]) {\n+                    unclassified = false;\n+                    writers[i].writeNext(nextLine);\n+                }\n+            }\n+            if (unclassified) {\n+                writers[writers.length-1].writeNext(nextLine);\n+            }\n+        }\n+        try {\n+            for (CSVWriter writer : writers) {\n+                writer.close();\n+            }\n+        } catch (Exception e) {\n+            e.printStackTrace();\n+        }\n+\n+    }\n+\n+    /*\n+     * Reads a CSV file and returns the data as a list of String arrays.\n+     *\n+     * @param filename Path to a CSV file\n+     * @return Contents of the CSV file\n+     */\n+    static List<String[]> readCSV(String filename) {\n+        try {\n+            FileReader filereader = new FileReader(filename);\n+            CSVReader csvReader = new CSVReaderBuilder(filereader).withSkipLines(1).build();\n+            return csvReader.readAll();\n+        }\n+        catch (Exception e) {\n+            System.out.println(e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8523b45c397cf1120c9bf380ebeb532c9836f40e"}, "originalPosition": 78}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDAzMDI1OnYy", "diffSide": "RIGHT", "path": "tools/unsupervised_dataset/sql_classifier/parserc/src/main/java/Parserc.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxNjozMjoyNlrOG87N8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxNjozMjoyNlrOG87N8w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjUzNzk3MQ==", "bodyText": "null may cause issues if other parts didn't handle correctly, consider using Optional<List<String[]>> as return type.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/138#discussion_r466537971", "createdAt": "2020-08-06T16:32:26Z", "author": {"login": "yzhvictor"}, "path": "tools/unsupervised_dataset/sql_classifier/parserc/src/main/java/Parserc.java", "diffHunk": "@@ -0,0 +1,156 @@\n+import com.opencsv.*;\n+import org.apache.calcite.sql.parser.SqlParseException;\n+import org.apache.calcite.sql.parser.SqlParser;\n+\n+import org.apache.calcite.sql.parser.dialect1.Dialect1ParserImpl;\n+import org.apache.calcite.sql.parser.bigquery.BigQueryParserImpl;\n+import org.apache.calcite.sql.parser.defaultdialect.DefaultDialectParserImpl;\n+import org.apache.calcite.sql.parser.postgresql.PostgreSQLParserImpl;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.FileWriter;\n+import java.text.SimpleDateFormat;\n+import java.util.List;\n+import java.util.Date;\n+\n+public class Parserc {\n+    /*\n+     * Runs the classification tool. Takes a CSV file of queries, classifies queries based on dialect, and creates\n+     * several subdirectories to store the queries by dialect.\n+     *\n+     * @param args Command line arguments\n+     */\n+    public static void main(String[] args) {\n+        if (args.length == 0) {\n+            System.out.println(\"Please provide a CSV file.\");\n+            return;\n+        }\n+        List<String[]> allData = readCSV(args[0]);\n+        if (allData == null) {\n+            return;\n+        }\n+        SqlParser.Config[] parserConfigs = new SqlParser.Config[4];\n+        parserConfigs[0] = SqlParser.configBuilder().setParserFactory(DefaultDialectParserImpl.FACTORY).build();\n+        parserConfigs[1] = SqlParser.configBuilder().setParserFactory(Dialect1ParserImpl.FACTORY).build();\n+        parserConfigs[2] = SqlParser.configBuilder().setParserFactory(BigQueryParserImpl.FACTORY).build();\n+        parserConfigs[3] = SqlParser.configBuilder().setParserFactory(PostgreSQLParserImpl.FACTORY).build();\n+\n+        CSVWriter[] writers = setupOutput();\n+\n+        for (String[] data : allData) {\n+            boolean[] results = classifyQuery(cleanQuery(data[0]), parserConfigs);\n+            boolean unclassified = true;\n+            String[] nextLine = {data[0], data[1]};\n+            for (int i = 0; i < results.length; i++) {\n+                if (results[i]) {\n+                    unclassified = false;\n+                    writers[i].writeNext(nextLine);\n+                }\n+            }\n+            if (unclassified) {\n+                writers[writers.length-1].writeNext(nextLine);\n+            }\n+        }\n+        try {\n+            for (CSVWriter writer : writers) {\n+                writer.close();\n+            }\n+        } catch (Exception e) {\n+            e.printStackTrace();\n+        }\n+\n+    }\n+\n+    /*\n+     * Reads a CSV file and returns the data as a list of String arrays.\n+     *\n+     * @param filename Path to a CSV file\n+     * @return Contents of the CSV file\n+     */\n+    static List<String[]> readCSV(String filename) {\n+        try {\n+            FileReader filereader = new FileReader(filename);\n+            CSVReader csvReader = new CSVReaderBuilder(filereader).withSkipLines(1).build();\n+            return csvReader.readAll();\n+        }\n+        catch (Exception e) {\n+            System.out.println(e);\n+            return null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8523b45c397cf1120c9bf380ebeb532c9836f40e"}, "originalPosition": 79}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDAzMjcyOnYy", "diffSide": "RIGHT", "path": "tools/unsupervised_dataset/sql_classifier/parserc/src/main/java/Parserc.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxNjozMzoxMVrOG87PiA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxNjozMzoxMVrOG87PiA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjUzODM3Ng==", "bodyText": "will a map be more explicit? key can be dialect, value is true or false?", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/138#discussion_r466538376", "createdAt": "2020-08-06T16:33:11Z", "author": {"login": "yzhvictor"}, "path": "tools/unsupervised_dataset/sql_classifier/parserc/src/main/java/Parserc.java", "diffHunk": "@@ -0,0 +1,156 @@\n+import com.opencsv.*;\n+import org.apache.calcite.sql.parser.SqlParseException;\n+import org.apache.calcite.sql.parser.SqlParser;\n+\n+import org.apache.calcite.sql.parser.dialect1.Dialect1ParserImpl;\n+import org.apache.calcite.sql.parser.bigquery.BigQueryParserImpl;\n+import org.apache.calcite.sql.parser.defaultdialect.DefaultDialectParserImpl;\n+import org.apache.calcite.sql.parser.postgresql.PostgreSQLParserImpl;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.FileWriter;\n+import java.text.SimpleDateFormat;\n+import java.util.List;\n+import java.util.Date;\n+\n+public class Parserc {\n+    /*\n+     * Runs the classification tool. Takes a CSV file of queries, classifies queries based on dialect, and creates\n+     * several subdirectories to store the queries by dialect.\n+     *\n+     * @param args Command line arguments\n+     */\n+    public static void main(String[] args) {\n+        if (args.length == 0) {\n+            System.out.println(\"Please provide a CSV file.\");\n+            return;\n+        }\n+        List<String[]> allData = readCSV(args[0]);\n+        if (allData == null) {\n+            return;\n+        }\n+        SqlParser.Config[] parserConfigs = new SqlParser.Config[4];\n+        parserConfigs[0] = SqlParser.configBuilder().setParserFactory(DefaultDialectParserImpl.FACTORY).build();\n+        parserConfigs[1] = SqlParser.configBuilder().setParserFactory(Dialect1ParserImpl.FACTORY).build();\n+        parserConfigs[2] = SqlParser.configBuilder().setParserFactory(BigQueryParserImpl.FACTORY).build();\n+        parserConfigs[3] = SqlParser.configBuilder().setParserFactory(PostgreSQLParserImpl.FACTORY).build();\n+\n+        CSVWriter[] writers = setupOutput();\n+\n+        for (String[] data : allData) {\n+            boolean[] results = classifyQuery(cleanQuery(data[0]), parserConfigs);\n+            boolean unclassified = true;\n+            String[] nextLine = {data[0], data[1]};\n+            for (int i = 0; i < results.length; i++) {\n+                if (results[i]) {\n+                    unclassified = false;\n+                    writers[i].writeNext(nextLine);\n+                }\n+            }\n+            if (unclassified) {\n+                writers[writers.length-1].writeNext(nextLine);\n+            }\n+        }\n+        try {\n+            for (CSVWriter writer : writers) {\n+                writer.close();\n+            }\n+        } catch (Exception e) {\n+            e.printStackTrace();\n+        }\n+\n+    }\n+\n+    /*\n+     * Reads a CSV file and returns the data as a list of String arrays.\n+     *\n+     * @param filename Path to a CSV file\n+     * @return Contents of the CSV file\n+     */\n+    static List<String[]> readCSV(String filename) {\n+        try {\n+            FileReader filereader = new FileReader(filename);\n+            CSVReader csvReader = new CSVReaderBuilder(filereader).withSkipLines(1).build();\n+            return csvReader.readAll();\n+        }\n+        catch (Exception e) {\n+            System.out.println(e);\n+            return null;\n+        }\n+    }\n+\n+    /*\n+     * Classifies a single query using different SQL parsers.\n+     *\n+     * @param query The query to be classified\n+     * @param parserConfigs The parsers for each of the different dialects\n+     * @return A boolean array, with true values if the query can be classified in a dialect and false otherwise\n+     */\n+    static boolean[] classifyQuery(String query, SqlParser.Config[] parserConfigs) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8523b45c397cf1120c9bf380ebeb532c9836f40e"}, "originalPosition": 90}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3092, "cost": 1, "resetAt": "2021-11-12T19:05:54Z"}}}