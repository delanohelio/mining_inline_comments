{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTA5MTk1NTQ5", "number": 182, "title": "gcs ingest cloud function", "bodyText": "Adds cloud function to ingest batches of files from GCS with an opinionated storage layout, flexible configuration system and supporting load jobs and external queries as a method of ingest (see README for details).\nPlease let me know if adding BQ utility cloud functions like this would be an appropriate use of this repo and how best to hook my integration tests into CI on PRs.", "createdAt": "2020-10-23T20:13:00Z", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182", "merged": true, "mergeCommit": {"oid": "0df81ee5a7ebd329280e5485dc03133b67fc5d51"}, "closed": true, "closedAt": "2020-11-11T16:17:40Z", "author": {"login": "jaketf"}, "timelineItems": {"totalCount": 41, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdVcTSLgH2gAyNTA5MTk1NTQ5OjE4OTcwYWQ3OGYyNjJjMjQ5ODBjZWU0M2Y1ZGU0N2JmZGUyYTBhNGI=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdbgY-fAFqTUyODMyMTc1OA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "18970ad78f262c24980cee43f5de47bfde2a0a4b", "author": {"user": null}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/18970ad78f262c24980cee43f5de47bfde2a0a4b", "committedDate": "2020-10-23T20:08:03Z", "message": "gcs ingest cloud function"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE2MzgxOTc2", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#pullrequestreview-516381976", "createdAt": "2020-10-25T17:45:48Z", "commit": {"oid": "18970ad78f262c24980cee43f5de47bfde2a0a4b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxNzo0NTo0OFrOHn7Png==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxNzo0NTo0OFrOHn7Png==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYyNzE2Ng==", "bodyText": "without changing the configuration interface could we optionally swap out this load_batches for creating a GCS to BQ Data Transfer Service config and submitting a new data transfer? This would have the advantage of relying on DTS's existing mechanics to orchestrate multiple loads to a staging table and atomically commit partitions to the destination table with a copy job. This has the advantage of atomicity of loading a partition but the disadvantage of a requiring a call to bigquerydatatransfer.googleapis.com which may prohibit use in environments that can't route this address due to lack of restricted VIP.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r511627166", "createdAt": "2020-10-25T17:45:48Z", "author": {"login": "jaketf"}, "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -0,0 +1,378 @@\n+# Copyright 2018 Google LLC.\n+# This software is provided as-is, without warranty or representation\n+# for any use or purpose.\n+# Your use of it is subject to your agreement with Google.\n+\n+# Licensed under the Apache License, Version 2.0 (the 'License');\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+# http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an 'AS IS' BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Background Cloud Function for loading data to BigQuery.\n+\"\"\"\n+import json\n+from collections import deque\n+from os import getenv\n+from pathlib import Path\n+from typing import Any\n+from typing import Dict, List, Optional, Tuple\n+from urllib.parse import urlparse\n+import logging\n+from time import monotonic, sleep\n+\n+from google.api_core.client_info import ClientInfo\n+from google.cloud import bigquery, storage\n+from google.cloud.exceptions import NotFound\n+\n+# 15TB per BQ load job.\n+MAX_BATCH_BYTES = 15 * 10 ** 12\n+MAX_URIS_PER_LOAD = 10 ** 4\n+\n+BASE_LOAD_JOB_CONFIG = {\n+    \"sourceFormat\": \"CSV\",\n+    \"fieldDelimiter\": \"|\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+}\n+\n+DEFAULT_BQ_LOAD_STATE_TABLE = \"bigquery_loads.serverless_bq_loads\"\n+\n+DEFAULT_EXTERNAL_TABLE_DEFINITION = {\n+    \"sourceFormat\": \"PARQUET\"\n+}\n+\n+DEFAULT_JOB_LABELS = {\n+    \"component\": \"event-based-gcs-ingest\",\n+    \"cloud-function-name\": getenv(\"FUNCTION_NAME\"),\n+}\n+\n+BASE_LOAD_JOB_CONFIG = {\n+    \"sourceFormat\": \"CSV\",\n+    \"fieldDelimiter\": \"|\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+    \"labels\": DEFAULT_JOB_LABELS,\n+}\n+\n+# Will wait up to this polling for errors before exiting\n+# This is to check if job fail quickly, not to assert the succeed\n+# This may not be honored if longer than cloud function timeout\n+# https://cloud.google.com/functions/docs/concepts/exec#timeout\n+WAIT_FOR_JOB_SECONDS = 5\n+\n+\n+def main(event: Dict, context):\n+    # Set by Cloud Function Execution Environment\n+    # https://cloud.google.com/functions/docs/env-var\n+    project = getenv(\"GCP_PROJECT\")\n+\n+\n+    bucket_id, object_id = parse_notification(event)\n+\n+    success_filename = getenv(\"SUCCESS_FILENAME\", \"_SUCCESS\")\n+\n+    # Exit eagerly if not a success file.\n+    # TODO we can improve this with pub/sub message filtering once it supports\n+    # a hasSuffix filter function (we can filter on hasSuffix successfile name)\n+    #  https://cloud.google.com/pubsub/docs/filtering\n+    if not object_id.endswith(f\"/{success_filename}\"):\n+        logging.debug(f\"No-op. This notification was not for a {success_filename} file.\")\n+        print(f\"No-op. This notification was not for a {success_filename} file.\")\n+        return\n+\n+    prefix_to_load = removesuffix(object_id, success_filename)\n+    parts = object_id.split(\"/\")\n+    dataset, table = parts[0:2]\n+    dest_table_ref = bigquery.TableReference.from_string(\n+        f\"{dataset}.{table}\", default_project=project\n+    )\n+\n+    client_info = ClientInfo(user_agent=\"google-pso-tool/bq-severless-loader\")\n+    gcs = storage.Client(client_info=client_info)\n+    default_query_config = bigquery.QueryJobConfig()\n+    default_query_config.use_legacy_sql = False\n+    default_query_config.labels = DEFAULT_JOB_LABELS\n+    bq = bigquery.Client(\n+        client_info=client_info, default_query_job_config=default_query_config\n+    )\n+\n+    gsurl = f\"gs://{bucket_id}/{prefix_to_load}\"\n+\n+    logging.debug(f\"looking for {gsurl}_config/bq_transform.sql\")\n+    print(f\"looking for {gsurl}_config/bq_transform.sql\")\n+    external_query_sql = read_gcs_file_if_exists(gcs, f\"{gsurl}_config/bq_transform.sql\")\n+    logging.debug(f\"external_query_sql = '{external_query_sql}'\")\n+    print(f\"external_query_sql = {external_query_sql}\")\n+    if external_query_sql:\n+        logging.debug(\"EXTERNAL QUERY\")\n+        print(\"EXTERNAL QUERY\")\n+        external_query(gcs, bq, gsurl, external_query_sql, dest_table_ref)\n+        return\n+\n+    logging.debug(\"LOAD_JOB\")\n+    print(\"LOAD_JOB\")\n+    load_batches(gcs, bq, gsurl, dest_table_ref)\n+\n+\n+def external_query(gcs, bq, gsurl, query, dest_table_ref):\n+    \"\"\"Load from query over external table from GCS.\n+\n+    This hinges on a SQL query defined in GCS at _config/bq_transform.sql and\n+    an external table definition _config/external.json (otherwise will assume\n+    parquet external table)\n+    \"\"\"\n+    external_table_config = read_gcs_file_if_exists(gcs, f\"{gsurl}_config/external.json\")\n+    logging.debug(\"reading external table config\")\n+    if external_table_config:\n+        external_table_def = json.loads(external_table_config)\n+    else:\n+        print(f\"Falling back to default parquet external table.\"\n+              f\" {gsurl}/_config/external.json not found.\")\n+        external_table_def = DEFAULT_EXTERNAL_TABLE_DEFINITION\n+\n+    external_table_def[\"sourceUris\"] = flatten(get_batches_for_prefix(gcs, gsurl))\n+    external_config = bigquery.ExternalConfig.from_api_repr(\n+        external_table_def)\n+    job_config = bigquery.QueryJobConfig(\n+        table_definitions={\"temp_ext\": external_config},\n+        use_legacy_sql=False\n+    )\n+    # for some reason string literal wrapped in b''\n+    rendered_query = str(str(query).format(\n+        dest_dataset=dest_table_ref.dataset_id,\n+        dest_table=dest_table_ref.table_id))[2:-1]\n+\n+    job: bigquery.QueryJob = bq.query(\n+        rendered_query,\n+        job_config=job_config)\n+\n+    print(f\"started asynchronous query job: {job.job_id}\")\n+\n+    start_poll_for_errors = monotonic()\n+    # Check if job failed quickly\n+    while monotonic() - start_poll_for_errors < WAIT_FOR_JOB_SECONDS:\n+        job.reload()\n+        if job.errors:\n+            raise RuntimeError(\n+                f\"query job {job.job_id} failed quickly: {job.errors}\")\n+        sleep(1)\n+\n+\n+def flatten(arr: List[List[Any]]) -> List[Any]:\n+    \"\"\"Flatten list of lists to flat list of elements\"\"\"\n+    return [j for i in arr for j in i]\n+\n+\n+def load_batches(gcs, bq, gsurl, dest_table_ref):", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18970ad78f262c24980cee43f5de47bfde2a0a4b"}, "originalPosition": 170}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE2MzgyMDUx", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#pullrequestreview-516382051", "createdAt": "2020-10-25T17:46:43Z", "commit": {"oid": "18970ad78f262c24980cee43f5de47bfde2a0a4b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxNzo0Njo0M1rOHn7P_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxNzo0Njo0M1rOHn7P_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYyNzI2Mw==", "bodyText": "this should be configurable w/ environment variable for customers who have negotiated a higher per load job bytes limit.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r511627263", "createdAt": "2020-10-25T17:46:43Z", "author": {"login": "jaketf"}, "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -0,0 +1,378 @@\n+# Copyright 2018 Google LLC.\n+# This software is provided as-is, without warranty or representation\n+# for any use or purpose.\n+# Your use of it is subject to your agreement with Google.\n+\n+# Licensed under the Apache License, Version 2.0 (the 'License');\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+# http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an 'AS IS' BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Background Cloud Function for loading data to BigQuery.\n+\"\"\"\n+import json\n+from collections import deque\n+from os import getenv\n+from pathlib import Path\n+from typing import Any\n+from typing import Dict, List, Optional, Tuple\n+from urllib.parse import urlparse\n+import logging\n+from time import monotonic, sleep\n+\n+from google.api_core.client_info import ClientInfo\n+from google.cloud import bigquery, storage\n+from google.cloud.exceptions import NotFound\n+\n+# 15TB per BQ load job.\n+MAX_BATCH_BYTES = 15 * 10 ** 12", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18970ad78f262c24980cee43f5de47bfde2a0a4b"}, "originalPosition": 34}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE2NDYyMzE5", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#pullrequestreview-516462319", "createdAt": "2020-10-26T04:29:20Z", "commit": {"oid": "18970ad78f262c24980cee43f5de47bfde2a0a4b"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwNDoyOToyMVrOHoAjvg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwNDo0NToxOVrOHoAwlA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcxNDIzOA==", "bodyText": "Perhaps a notice that this will utilize the Query slots as opposed to the free load slot tier to execute would be good.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r511714238", "createdAt": "2020-10-26T04:29:21Z", "author": {"login": "ryanmcdowell"}, "path": "cloud_functions/gcs_event_based_ingest/README.md", "diffHunk": "@@ -0,0 +1,116 @@\n+<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n+<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n+**Table of Contents**\n+\n+- [BigQuery Serverless Ingest](#bigquery-serverless-ingest)\n+  - [Tracking Table](#tracking-table)\n+  - [Environment Variables](#environment-variables)\n+  - [GCS Object Naming Convention](#gcs-object-naming-convention)\n+  - [Triggers](#triggers)\n+\n+<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n+\n+# BigQuery Serverless Ingest\n+\n+Flexible service for performing  BigQuery file loads to existing tables.\n+This service handles splitting load jobs when the data volume exceeds\n+the BigQuery 15TB load job limit. The goal of the service is to orchestrate\n+BigQuery Load Jobs to many bigquery datasets / tables from a single bucket\n+providing transparent configuration that is overridable at any level.\n+\n+![architecture](img/bq-ingest.png)\n+\n+## Environment Variables\n+- `SUCCESS_FILENAME`: Filename to trigger a load (defaults to `_SUCCESS`).\n+\n+## GCS Object Naming Convention\n+### Data Files\n+Data should be ingested to a prefix containing destination dataset and table\n+like so:\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/*`\n+Note, the table prefix can contain multiple sub-prefixes for handling partitions\n+or for configuring historical / incremental loads differently.\n+\n+### Configuration Files\n+The Ingestion has many optional configuration files that should live in\n+a special `_config/` prefix at the root of the bucket and/or under the dataset\n+and/or table and/or under the partition prefixes.\n+\n+For example if you have the following files:\n+```text\n+gs://${INGESTION_BUCKET}/_config/load.json`\n+gs://${INGESTION_BUCKET}/${BQ_DATASET}/_config/load.json`\n+gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/_config/load.json\n+```\n+The json objects will be merged where key conflicts are resolved by config in\n+the closest directory to the data.\n+If the files contents were like this:\n+`gs://${INGESTION_BUCKET}/_config/load.json`:\n+```json\n+{\n+    \"sourceFormat\": \"CSV\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+    \"schemaUpdateOptions\": [\"ALLOW_FILED_RELAXATION\"]\n+}\n+```\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/_config/load.json`:\n+```json\n+{\n+    \"sourceFormat\": \"AVRO\",\n+}\n+```\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/_config/load.json`:\n+```json\n+{\n+    \"writeDisposition\": \"WRTITE_TRUNCATE\",\n+}\n+```\n+\n+The result of merging these would be:\n+```json\n+{\n+    \"sourceFormat\": \"AVRO\",\n+    \"writeDisposition\": \"WRITE_TRUNCATE\",\n+    \"schemaUpdateOptions\": [\"ALLOW_FILED_RELAXATION\"]\n+}\n+```\n+This configuration system gives us the ability to DRY up common defaults but\n+override them at whatever level is appropriate as new cases come up.\n+\n+#### Transformation SQL", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18970ad78f262c24980cee43f5de47bfde2a0a4b"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcxNDc3Mg==", "bodyText": "I'm curious how you would suggest we track what data has been ingested given this limitation?", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r511714772", "createdAt": "2020-10-26T04:31:38Z", "author": {"login": "ryanmcdowell"}, "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/README.md", "diffHunk": "@@ -0,0 +1,73 @@\n+<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n+<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n+**Table of Contents**\n+\n+- [Event Driven BigQuery Ingest with External Table Query](#event-driven-bigquery-ingest-with-external-table-query)\n+  - [Orchestration](#orchestration)\n+  - [Ingestion Mechanics](#ingestion-mechanics)\n+  - [Deployment](#deployment)\n+  - [Implementation notes](#implementation-notes)\n+  - [Tests](#tests)\n+  - [Limitations](#limitations)\n+  - [Future work](#future-work)\n+\n+<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n+\n+# Event Driven BigQuery Ingest \n+This directory defines a reusable [Background Cloud Function](https://cloud.google.com/functions/docs/writing/background)\n+for ingesting any new file at a GCS prefix with a file name containing a\n+timestamp to be used as the partitioning and clustering column in a partitioned\n+BigQuery Table.\n+\n+![architecture](img/arch.png)\n+\n+## Orchestration\n+1. Files pulled from on-prem to gcs bucket.\n+1. [Pub/Sub Notification](https://cloud.google.com/storage/docs/pubsub-notifications)\n+object finalize.\n+1. Cloud Function subscribes to notifications and ingests all the data into \n+BigQuery a directory once a `_SUCCESS` file arrives.\n+\n+\n+## Deployment\n+The source for this Cloud Function can easily be reused to repeat this pattern\n+for many tables by using the accompanying terraform module (TODO).\n+\n+This way we can reuse the tested source code for the Cloud Function.\n+\n+### Environment Variables\n+To configure each deployement of the Cloud Function we will use\n+[Environment Variables](https://cloud.google.com/functions/docs/env-var)\n+\n+\n+#### Optional\n+| Variable                      | Description                           | Default                                      |\n+|-------------------------------|---------------------------------------|----------------------------------------------|\n+| `BQ_LOAD_STATE_TABLE` | BigQuery table to log load state to           | \"bigquery_loads.serverless_bq_loads\" (in same project as cloud function) |\n+\n+\n+## Implementation notes\n+1. To support notifications based on a GCS prefix\n+(rather than every object in the bucket), we chose to use manually\n+configure Pub/Sub Notifications manually and use a Pub/Sub triggered\n+Cloud Function.\n+\n+## Tests\n+From the `gcs_ocn_bq_ingest` dir simply run\n+```bash\n+pytest\n+```\n+\n+## Limitations\n+1. Cloud Functions have a 10 minute timeout. If the BQ load job takes too long\n+ the data will still be ingested but the function may be marked in timeout state\n+ rather than success and the function may not have the opportunity to produce\n+ all the expected logs.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18970ad78f262c24980cee43f5de47bfde2a0a4b"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcxNDgwMA==", "bodyText": "2020", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r511714800", "createdAt": "2020-10-26T04:31:46Z", "author": {"login": "ryanmcdowell"}, "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -0,0 +1,378 @@\n+# Copyright 2018 Google LLC.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18970ad78f262c24980cee43f5de47bfde2a0a4b"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcxNTA2Nw==", "bodyText": "Remove extra blank line", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r511715067", "createdAt": "2020-10-26T04:33:11Z", "author": {"login": "ryanmcdowell"}, "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -0,0 +1,378 @@\n+# Copyright 2018 Google LLC.\n+# This software is provided as-is, without warranty or representation\n+# for any use or purpose.\n+# Your use of it is subject to your agreement with Google.\n+\n+# Licensed under the Apache License, Version 2.0 (the 'License');\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+# http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an 'AS IS' BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Background Cloud Function for loading data to BigQuery.\n+\"\"\"\n+import json\n+from collections import deque\n+from os import getenv\n+from pathlib import Path\n+from typing import Any\n+from typing import Dict, List, Optional, Tuple\n+from urllib.parse import urlparse\n+import logging\n+from time import monotonic, sleep\n+\n+from google.api_core.client_info import ClientInfo\n+from google.cloud import bigquery, storage\n+from google.cloud.exceptions import NotFound\n+\n+# 15TB per BQ load job.\n+MAX_BATCH_BYTES = 15 * 10 ** 12\n+MAX_URIS_PER_LOAD = 10 ** 4\n+\n+BASE_LOAD_JOB_CONFIG = {\n+    \"sourceFormat\": \"CSV\",\n+    \"fieldDelimiter\": \"|\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+}\n+\n+DEFAULT_BQ_LOAD_STATE_TABLE = \"bigquery_loads.serverless_bq_loads\"\n+\n+DEFAULT_EXTERNAL_TABLE_DEFINITION = {\n+    \"sourceFormat\": \"PARQUET\"\n+}\n+\n+DEFAULT_JOB_LABELS = {\n+    \"component\": \"event-based-gcs-ingest\",\n+    \"cloud-function-name\": getenv(\"FUNCTION_NAME\"),\n+}\n+\n+BASE_LOAD_JOB_CONFIG = {\n+    \"sourceFormat\": \"CSV\",\n+    \"fieldDelimiter\": \"|\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+    \"labels\": DEFAULT_JOB_LABELS,\n+}\n+\n+# Will wait up to this polling for errors before exiting\n+# This is to check if job fail quickly, not to assert the succeed\n+# This may not be honored if longer than cloud function timeout\n+# https://cloud.google.com/functions/docs/concepts/exec#timeout\n+WAIT_FOR_JOB_SECONDS = 5\n+\n+\n+def main(event: Dict, context):\n+    # Set by Cloud Function Execution Environment\n+    # https://cloud.google.com/functions/docs/env-var\n+    project = getenv(\"GCP_PROJECT\")\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18970ad78f262c24980cee43f5de47bfde2a0a4b"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcxNTkzNg==", "bodyText": "Seems like this could be a variable defined above given get_batches_for_prefix depends on the same default", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r511715936", "createdAt": "2020-10-26T04:37:25Z", "author": {"login": "ryanmcdowell"}, "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -0,0 +1,378 @@\n+# Copyright 2018 Google LLC.\n+# This software is provided as-is, without warranty or representation\n+# for any use or purpose.\n+# Your use of it is subject to your agreement with Google.\n+\n+# Licensed under the Apache License, Version 2.0 (the 'License');\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+# http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an 'AS IS' BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Background Cloud Function for loading data to BigQuery.\n+\"\"\"\n+import json\n+from collections import deque\n+from os import getenv\n+from pathlib import Path\n+from typing import Any\n+from typing import Dict, List, Optional, Tuple\n+from urllib.parse import urlparse\n+import logging\n+from time import monotonic, sleep\n+\n+from google.api_core.client_info import ClientInfo\n+from google.cloud import bigquery, storage\n+from google.cloud.exceptions import NotFound\n+\n+# 15TB per BQ load job.\n+MAX_BATCH_BYTES = 15 * 10 ** 12\n+MAX_URIS_PER_LOAD = 10 ** 4\n+\n+BASE_LOAD_JOB_CONFIG = {\n+    \"sourceFormat\": \"CSV\",\n+    \"fieldDelimiter\": \"|\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+}\n+\n+DEFAULT_BQ_LOAD_STATE_TABLE = \"bigquery_loads.serverless_bq_loads\"\n+\n+DEFAULT_EXTERNAL_TABLE_DEFINITION = {\n+    \"sourceFormat\": \"PARQUET\"\n+}\n+\n+DEFAULT_JOB_LABELS = {\n+    \"component\": \"event-based-gcs-ingest\",\n+    \"cloud-function-name\": getenv(\"FUNCTION_NAME\"),\n+}\n+\n+BASE_LOAD_JOB_CONFIG = {\n+    \"sourceFormat\": \"CSV\",\n+    \"fieldDelimiter\": \"|\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+    \"labels\": DEFAULT_JOB_LABELS,\n+}\n+\n+# Will wait up to this polling for errors before exiting\n+# This is to check if job fail quickly, not to assert the succeed\n+# This may not be honored if longer than cloud function timeout\n+# https://cloud.google.com/functions/docs/concepts/exec#timeout\n+WAIT_FOR_JOB_SECONDS = 5\n+\n+\n+def main(event: Dict, context):\n+    # Set by Cloud Function Execution Environment\n+    # https://cloud.google.com/functions/docs/env-var\n+    project = getenv(\"GCP_PROJECT\")\n+\n+\n+    bucket_id, object_id = parse_notification(event)\n+\n+    success_filename = getenv(\"SUCCESS_FILENAME\", \"_SUCCESS\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18970ad78f262c24980cee43f5de47bfde2a0a4b"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcxNjg4OQ==", "bodyText": "2020", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r511716889", "createdAt": "2020-10-26T04:42:05Z", "author": {"login": "ryanmcdowell"}, "path": "cloud_functions/gcs_event_based_ingest/tests/test_gcs_ocn_bq_ingest_it.py", "diffHunk": "@@ -0,0 +1,235 @@\n+# Copyright 2019 Google LLC", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18970ad78f262c24980cee43f5de47bfde2a0a4b"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcxNzUyNA==", "bodyText": "If a directory which has been previously loaded receives new data and a new _SUCCESS file, it seems like entire table will be reloaded. Is it safe to say this does not handle incremental loads? If so, perhaps the default disposition should be WRITE_TRUNCTATE", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r511717524", "createdAt": "2020-10-26T04:45:19Z", "author": {"login": "ryanmcdowell"}, "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -0,0 +1,378 @@\n+# Copyright 2018 Google LLC.\n+# This software is provided as-is, without warranty or representation\n+# for any use or purpose.\n+# Your use of it is subject to your agreement with Google.\n+\n+# Licensed under the Apache License, Version 2.0 (the 'License');\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+# http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an 'AS IS' BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Background Cloud Function for loading data to BigQuery.\n+\"\"\"\n+import json\n+from collections import deque\n+from os import getenv\n+from pathlib import Path\n+from typing import Any\n+from typing import Dict, List, Optional, Tuple\n+from urllib.parse import urlparse\n+import logging\n+from time import monotonic, sleep\n+\n+from google.api_core.client_info import ClientInfo\n+from google.cloud import bigquery, storage\n+from google.cloud.exceptions import NotFound\n+\n+# 15TB per BQ load job.\n+MAX_BATCH_BYTES = 15 * 10 ** 12\n+MAX_URIS_PER_LOAD = 10 ** 4\n+\n+BASE_LOAD_JOB_CONFIG = {\n+    \"sourceFormat\": \"CSV\",\n+    \"fieldDelimiter\": \"|\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+}\n+\n+DEFAULT_BQ_LOAD_STATE_TABLE = \"bigquery_loads.serverless_bq_loads\"\n+\n+DEFAULT_EXTERNAL_TABLE_DEFINITION = {\n+    \"sourceFormat\": \"PARQUET\"\n+}\n+\n+DEFAULT_JOB_LABELS = {\n+    \"component\": \"event-based-gcs-ingest\",\n+    \"cloud-function-name\": getenv(\"FUNCTION_NAME\"),\n+}\n+\n+BASE_LOAD_JOB_CONFIG = {\n+    \"sourceFormat\": \"CSV\",\n+    \"fieldDelimiter\": \"|\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+    \"labels\": DEFAULT_JOB_LABELS,\n+}\n+\n+# Will wait up to this polling for errors before exiting\n+# This is to check if job fail quickly, not to assert the succeed\n+# This may not be honored if longer than cloud function timeout\n+# https://cloud.google.com/functions/docs/concepts/exec#timeout\n+WAIT_FOR_JOB_SECONDS = 5\n+\n+\n+def main(event: Dict, context):\n+    # Set by Cloud Function Execution Environment\n+    # https://cloud.google.com/functions/docs/env-var\n+    project = getenv(\"GCP_PROJECT\")\n+\n+\n+    bucket_id, object_id = parse_notification(event)\n+\n+    success_filename = getenv(\"SUCCESS_FILENAME\", \"_SUCCESS\")\n+\n+    # Exit eagerly if not a success file.\n+    # TODO we can improve this with pub/sub message filtering once it supports\n+    # a hasSuffix filter function (we can filter on hasSuffix successfile name)\n+    #  https://cloud.google.com/pubsub/docs/filtering\n+    if not object_id.endswith(f\"/{success_filename}\"):", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18970ad78f262c24980cee43f5de47bfde2a0a4b"}, "originalPosition": 82}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e9dd912856fea330b05ccbab2234bf40c3e087ef", "author": {"user": null}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/e9dd912856fea330b05ccbab2234bf40c3e087ef", "committedDate": "2020-10-27T02:17:50Z", "message": "address review feedback"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "843f8714b1bc4623b19b011474f86e14f495ab09", "author": {"user": null}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/843f8714b1bc4623b19b011474f86e14f495ab09", "committedDate": "2020-10-27T02:18:51Z", "message": "remove extra newline"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bf1fd011b27fbb9c2d3e011840de0b482e0255d6", "author": {"user": null}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/bf1fd011b27fbb9c2d3e011840de0b482e0255d6", "committedDate": "2020-10-27T02:19:28Z", "message": "remove project files"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1e831070123750e853b5ab36e9f271aa1a734eaa", "author": {"user": null}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/1e831070123750e853b5ab36e9f271aa1a734eaa", "committedDate": "2020-10-28T17:46:19Z", "message": "regex destination details env var"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "64c026c5e5dfc5413d0bfd9a6c0b303e0537d709", "author": {"user": null}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/64c026c5e5dfc5413d0bfd9a6c0b303e0537d709", "committedDate": "2020-10-28T17:47:02Z", "message": "ignore intellij .idea/ configs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "242283f78d21bff8c638b37dee7a9ac20e8260df", "author": {"user": null}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/242283f78d21bff8c638b37dee7a9ac20e8260df", "committedDate": "2020-10-28T18:07:28Z", "message": "incremental loads docs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "152f4b777dedb18c1e29bb42b7c6a8a85227bed9", "author": {"user": null}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/152f4b777dedb18c1e29bb42b7c6a8a85227bed9", "committedDate": "2020-10-28T18:09:45Z", "message": "update TODOs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "db1f2bf1fefa7d7a6da7c1d2f00e7ee0ecc034b2", "author": {"user": null}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/db1f2bf1fefa7d7a6da7c1d2f00e7ee0ecc034b2", "committedDate": "2020-10-28T18:12:13Z", "message": "Merge branch 'master' into cloud-functions-ingest"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "899203dd95157c513c3abf3dc0907528fc4c860a", "author": {"user": null}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/899203dd95157c513c3abf3dc0907528fc4c860a", "committedDate": "2020-10-28T21:05:28Z", "message": "remove bad labels"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "79665f8563f375fe1616f5e2ca7371385061fea5", "author": {"user": null}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/79665f8563f375fe1616f5e2ca7371385061fea5", "committedDate": "2020-10-28T21:07:39Z", "message": "Merge branch 'cloud-functions-ingest' of github.com:jaketf/bigquery-utils into cloud-functions-ingest"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6335be9ae8cbc309841383a9522c136020b0912d", "author": {"user": null}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/6335be9ae8cbc309841383a9522c136020b0912d", "committedDate": "2020-10-28T21:12:47Z", "message": "move external query recursive search todo"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "36eb155bdccfaa6eed00f93b4b655f326ec05618", "author": {"user": null}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/36eb155bdccfaa6eed00f93b4b655f326ec05618", "committedDate": "2020-10-30T17:59:52Z", "message": "passing integration tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d2d4dc9e7e4579eca0495f5115a555c9e06b6dd1", "author": {"user": null}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/d2d4dc9e7e4579eca0495f5115a555c9e06b6dd1", "committedDate": "2020-10-31T00:03:04Z", "message": "add ci for gcs_event_based_ingest cloud function"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4a345c8d3aa4068c1c9631cb052cd316f5039c85", "author": {"user": null}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/4a345c8d3aa4068c1c9631cb052cd316f5039c85", "committedDate": "2020-10-31T01:25:34Z", "message": "handle duplicate pubsub notification"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIxMjU2Nzk1", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#pullrequestreview-521256795", "createdAt": "2020-11-01T20:12:40Z", "commit": {"oid": "4a345c8d3aa4068c1c9631cb052cd316f5039c85"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMVQyMDoxMjo0MFrOHrxxjg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMVQyMDoyNjozOFrOHrx3VQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2NjMxOA==", "bodyText": "Trailing newline", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r515666318", "createdAt": "2020-11-01T20:12:40Z", "author": {"login": "ryanmcdowell"}, "path": "cloud_functions/gcs_event_based_ingest/Dockerfile.ci", "diffHunk": "@@ -0,0 +1,6 @@\n+FROM python:3.8-slim\n+COPY requirements.txt requirements-dev.txt .\n+RUN pip3 install -r requirements-dev.txt\n+WORKDIR /ci\n+COPY . /ci/\n+ENTRYPOINT [\"pytest\"]", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4a345c8d3aa4068c1c9631cb052cd316f5039c85"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2NjgzMA==", "bodyText": "Why not just default this to the hive partitioning layout?", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r515666830", "createdAt": "2020-11-01T20:17:36Z", "author": {"login": "ryanmcdowell"}, "path": "cloud_functions/gcs_event_based_ingest/README.md", "diffHunk": "@@ -0,0 +1,248 @@\n+<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n+<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n+**Table of Contents**\n+\n+- [BigQuery Serverless Ingest](#bigquery-serverless-ingest)\n+  - [Tracking Table](#tracking-table)\n+  - [Environment Variables](#environment-variables)\n+  - [GCS Object Naming Convention](#gcs-object-naming-convention)\n+  - [Triggers](#triggers)\n+\n+<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n+\n+# BigQuery Serverless Ingest\n+\n+Flexible service for performing  BigQuery file loads to existing tables.\n+This service handles splitting load jobs when the data volume exceeds\n+the BigQuery 15TB load job limit. The goal of the service is to orchestrate\n+BigQuery Load Jobs to many bigquery datasets / tables from a single bucket\n+providing transparent configuration that is overridable at any level.\n+\n+![architecture](img/bq-ingest.png)\n+\n+## Environment Variables\n+- `SUCCESS_FILENAME`: Filename to trigger a load (defaults to `_SUCCESS`).\n+- `DESTINATION_REGEX`:  A [Python Regex with named capturing groups](https://docs.python.org/3/howto/regex.html#non-capturing-and-named-groups)\n+for (defaults to `(?P<dataset>[\\w\\-_0-9]+)/(?P<table>[\\w\\-_0-9]+)/?(?P<partition>\\$[\\w\\-_0-9]+)?/?(?P<batch>[\\w\\-_0-9]+)?/`): \n+  - `dataset`: destintaion BigQuery Dataset\n+  - `table`: destination BigQuery Table\n+  - `partition`: (optional) destination BigQuery [partition decorator](https://cloud.google.com/bigquery/docs/creating-partitioned-tables#creating_an_ingestion-time_partitioned_table_when_loading_data)\n+    (For example $)\n+  - `batch`: (optional) indicates an incremental load from an upstream system (see [Handling Incremental Loads](#handling-incremental-loads))\n+- `MAX_BATCH_BYTES`: Max bytes for BigQuery Load job. (default 15 TB)\n+\n+## GCS Object Naming Convention\n+### Data Files\n+Data should be ingested to a prefix containing destination dataset and table\n+like so:\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/*`\n+Note, the table prefix can contain multiple sub-prefixes for handling partitions\n+or for configuring historical / incremental loads differently.\n+\n+### Configuration Files\n+The Ingestion has many optional configuration files that should live in\n+a special `_config/` prefix at the root of the bucket and/or under the dataset\n+and/or table and/or under the partition prefixes.\n+\n+For example if you have the following files:\n+```text\n+gs://${INGESTION_BUCKET}/_config/load.json`\n+gs://${INGESTION_BUCKET}/${BQ_DATASET}/_config/load.json`\n+gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/_config/load.json\n+```\n+The json objects will be merged where key conflicts are resolved by config in\n+the closest directory to the data.\n+If the files contents were like this:\n+`gs://${INGESTION_BUCKET}/_config/load.json`:\n+```json\n+{\n+    \"sourceFormat\": \"CSV\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+    \"schemaUpdateOptions\": [\"ALLOW_FILED_RELAXATION\"]\n+}\n+```\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/_config/load.json`:\n+```json\n+{\n+    \"sourceFormat\": \"AVRO\"\n+}\n+```\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/_config/load.json`:\n+```json\n+{\n+    \"writeDisposition\": \"WRTITE_TRUNCATE\"\n+}\n+```\n+\n+The result of merging these would be:\n+```json\n+{\n+    \"sourceFormat\": \"AVRO\",\n+    \"writeDisposition\": \"WRITE_TRUNCATE\",\n+    \"schemaUpdateOptions\": [\"ALLOW_FILED_RELAXATION\"]\n+}\n+```\n+This configuration system gives us the ability to DRY up common defaults but\n+override them at whatever level is appropriate as new cases come up.\n+\n+#### Transformation SQL\n+In some cases we may need to perform transformations on the files in GCS\n+before they can be loaded to BigQuery. This is handled by query on an\n+temporary external table over the GCS objects as a proxy for load job.\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/_config/bq_transform.sql`\n+\n+Note, external queries will consume query slots from this project's reservation\n+or count towards your on-demand billing. They will _not_ use free tie load slots.\n+\n+Note, that the query should select from a `temp_ext` which will be a temporary\n+external table configured on the fly by the Cloud Function.\n+The query must handle the logic for inserting into the destination table.\n+This means it should use BigQuery DML to either `INSERT` or `MERGE` into the\n+destination table.\n+For example:\n+```sql\n+INSERT {dest_dataset}.{dest_table}\n+SELECT * FROM temp_ext\n+```\n+Note that `{dest_dataset}` and `{dest_table}` can be used to inject the dataset\n+and table inferred from the GCS path.\n+\n+\n+The query will be run with the appropriate [external table definitions](https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#ExternalDataConfiguration)\n+defined in:\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/_config/external.json`\n+If this file is missing the external table will be assumed to be `PARQUET` format.\n+\n+### Partitions\n+\n+#### Partition Table Decorators\n+Note that if the directory immediately before the triggering successfile starts with\n+a `$` it will be treated as a BigQuery Partition decorator for the destination table.\n+\n+This means for:\n+```text\n+gs://${BUCKET}/foo/bar/$20201026/_SUCCESS", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4a345c8d3aa4068c1c9631cb052cd316f5039c85"}, "originalPosition": 124}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2Njk3Ng==", "bodyText": "should but is not required correct?", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r515666976", "createdAt": "2020-11-01T20:19:14Z", "author": {"login": "ryanmcdowell"}, "path": "cloud_functions/gcs_event_based_ingest/README.md", "diffHunk": "@@ -0,0 +1,248 @@\n+<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n+<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n+**Table of Contents**\n+\n+- [BigQuery Serverless Ingest](#bigquery-serverless-ingest)\n+  - [Tracking Table](#tracking-table)\n+  - [Environment Variables](#environment-variables)\n+  - [GCS Object Naming Convention](#gcs-object-naming-convention)\n+  - [Triggers](#triggers)\n+\n+<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n+\n+# BigQuery Serverless Ingest\n+\n+Flexible service for performing  BigQuery file loads to existing tables.\n+This service handles splitting load jobs when the data volume exceeds\n+the BigQuery 15TB load job limit. The goal of the service is to orchestrate\n+BigQuery Load Jobs to many bigquery datasets / tables from a single bucket\n+providing transparent configuration that is overridable at any level.\n+\n+![architecture](img/bq-ingest.png)\n+\n+## Environment Variables\n+- `SUCCESS_FILENAME`: Filename to trigger a load (defaults to `_SUCCESS`).\n+- `DESTINATION_REGEX`:  A [Python Regex with named capturing groups](https://docs.python.org/3/howto/regex.html#non-capturing-and-named-groups)\n+for (defaults to `(?P<dataset>[\\w\\-_0-9]+)/(?P<table>[\\w\\-_0-9]+)/?(?P<partition>\\$[\\w\\-_0-9]+)?/?(?P<batch>[\\w\\-_0-9]+)?/`): \n+  - `dataset`: destintaion BigQuery Dataset\n+  - `table`: destination BigQuery Table\n+  - `partition`: (optional) destination BigQuery [partition decorator](https://cloud.google.com/bigquery/docs/creating-partitioned-tables#creating_an_ingestion-time_partitioned_table_when_loading_data)\n+    (For example $)\n+  - `batch`: (optional) indicates an incremental load from an upstream system (see [Handling Incremental Loads](#handling-incremental-loads))\n+- `MAX_BATCH_BYTES`: Max bytes for BigQuery Load job. (default 15 TB)\n+\n+## GCS Object Naming Convention\n+### Data Files\n+Data should be ingested to a prefix containing destination dataset and table\n+like so:\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/*`\n+Note, the table prefix can contain multiple sub-prefixes for handling partitions\n+or for configuring historical / incremental loads differently.\n+\n+### Configuration Files\n+The Ingestion has many optional configuration files that should live in\n+a special `_config/` prefix at the root of the bucket and/or under the dataset\n+and/or table and/or under the partition prefixes.\n+\n+For example if you have the following files:\n+```text\n+gs://${INGESTION_BUCKET}/_config/load.json`\n+gs://${INGESTION_BUCKET}/${BQ_DATASET}/_config/load.json`\n+gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/_config/load.json\n+```\n+The json objects will be merged where key conflicts are resolved by config in\n+the closest directory to the data.\n+If the files contents were like this:\n+`gs://${INGESTION_BUCKET}/_config/load.json`:\n+```json\n+{\n+    \"sourceFormat\": \"CSV\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+    \"schemaUpdateOptions\": [\"ALLOW_FILED_RELAXATION\"]\n+}\n+```\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/_config/load.json`:\n+```json\n+{\n+    \"sourceFormat\": \"AVRO\"\n+}\n+```\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/_config/load.json`:\n+```json\n+{\n+    \"writeDisposition\": \"WRTITE_TRUNCATE\"\n+}\n+```\n+\n+The result of merging these would be:\n+```json\n+{\n+    \"sourceFormat\": \"AVRO\",\n+    \"writeDisposition\": \"WRITE_TRUNCATE\",\n+    \"schemaUpdateOptions\": [\"ALLOW_FILED_RELAXATION\"]\n+}\n+```\n+This configuration system gives us the ability to DRY up common defaults but\n+override them at whatever level is appropriate as new cases come up.\n+\n+#### Transformation SQL\n+In some cases we may need to perform transformations on the files in GCS\n+before they can be loaded to BigQuery. This is handled by query on an\n+temporary external table over the GCS objects as a proxy for load job.\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/_config/bq_transform.sql`\n+\n+Note, external queries will consume query slots from this project's reservation\n+or count towards your on-demand billing. They will _not_ use free tie load slots.\n+\n+Note, that the query should select from a `temp_ext` which will be a temporary\n+external table configured on the fly by the Cloud Function.\n+The query must handle the logic for inserting into the destination table.\n+This means it should use BigQuery DML to either `INSERT` or `MERGE` into the\n+destination table.\n+For example:\n+```sql\n+INSERT {dest_dataset}.{dest_table}\n+SELECT * FROM temp_ext\n+```\n+Note that `{dest_dataset}` and `{dest_table}` can be used to inject the dataset\n+and table inferred from the GCS path.\n+\n+\n+The query will be run with the appropriate [external table definitions](https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#ExternalDataConfiguration)\n+defined in:\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/_config/external.json`\n+If this file is missing the external table will be assumed to be `PARQUET` format.\n+\n+### Partitions\n+\n+#### Partition Table Decorators\n+Note that if the directory immediately before the triggering successfile starts with\n+a `$` it will be treated as a BigQuery Partition decorator for the destination table.\n+\n+This means for:\n+```text\n+gs://${BUCKET}/foo/bar/$20201026/_SUCCESS\n+```\n+will trigger a load job with a destination table of `foo.bar$20201026`\n+This allows you to specify write disposition at the partition level.\n+This can be helpful in reprocessing scenarios where you'd want to `WRITE_TRUNCATE`\n+a partition that had some data quality issue.\n+\n+#### Hive Partitioning\n+If your data will be uploaded to GCS from a hadoop system that uses the \n+[supported default hive partitioning](https://cloud.google.com/bigquery/docs/hive-partitioned-loads-gcs#supported_data_layouts)\n+you can specify this in the [`hivePartitioningOptions`](https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#hivepartitioningoptions)\n+key of `load.json` for that table.\n+\n+Any non-trivial incremental loading to partitions should usually use the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4a345c8d3aa4068c1c9631cb052cd316f5039c85"}, "originalPosition": 137}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2NzQ0NQ==", "bodyText": "I'm curious if there's an alternative way to implement this. For instance I see for incrementals we include batch id. But alternatively these semantics could be specified as:\nBatch load if a success file is specified, otherwise load files as they land into GCS. If the user wants transactional semantics, they can use the success file method. The WRITE_DISPOSITION can be separately defined in the config such that either method could technically be incremental.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r515667445", "createdAt": "2020-11-01T20:23:38Z", "author": {"login": "ryanmcdowell"}, "path": "cloud_functions/gcs_event_based_ingest/README.md", "diffHunk": "@@ -0,0 +1,248 @@\n+<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n+<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n+**Table of Contents**\n+\n+- [BigQuery Serverless Ingest](#bigquery-serverless-ingest)\n+  - [Tracking Table](#tracking-table)\n+  - [Environment Variables](#environment-variables)\n+  - [GCS Object Naming Convention](#gcs-object-naming-convention)\n+  - [Triggers](#triggers)\n+\n+<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n+\n+# BigQuery Serverless Ingest\n+\n+Flexible service for performing  BigQuery file loads to existing tables.\n+This service handles splitting load jobs when the data volume exceeds\n+the BigQuery 15TB load job limit. The goal of the service is to orchestrate\n+BigQuery Load Jobs to many bigquery datasets / tables from a single bucket\n+providing transparent configuration that is overridable at any level.\n+\n+![architecture](img/bq-ingest.png)\n+\n+## Environment Variables\n+- `SUCCESS_FILENAME`: Filename to trigger a load (defaults to `_SUCCESS`).\n+- `DESTINATION_REGEX`:  A [Python Regex with named capturing groups](https://docs.python.org/3/howto/regex.html#non-capturing-and-named-groups)\n+for (defaults to `(?P<dataset>[\\w\\-_0-9]+)/(?P<table>[\\w\\-_0-9]+)/?(?P<partition>\\$[\\w\\-_0-9]+)?/?(?P<batch>[\\w\\-_0-9]+)?/`): \n+  - `dataset`: destintaion BigQuery Dataset\n+  - `table`: destination BigQuery Table\n+  - `partition`: (optional) destination BigQuery [partition decorator](https://cloud.google.com/bigquery/docs/creating-partitioned-tables#creating_an_ingestion-time_partitioned_table_when_loading_data)\n+    (For example $)\n+  - `batch`: (optional) indicates an incremental load from an upstream system (see [Handling Incremental Loads](#handling-incremental-loads))\n+- `MAX_BATCH_BYTES`: Max bytes for BigQuery Load job. (default 15 TB)\n+\n+## GCS Object Naming Convention\n+### Data Files\n+Data should be ingested to a prefix containing destination dataset and table\n+like so:\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/*`\n+Note, the table prefix can contain multiple sub-prefixes for handling partitions\n+or for configuring historical / incremental loads differently.\n+\n+### Configuration Files\n+The Ingestion has many optional configuration files that should live in\n+a special `_config/` prefix at the root of the bucket and/or under the dataset\n+and/or table and/or under the partition prefixes.\n+\n+For example if you have the following files:\n+```text\n+gs://${INGESTION_BUCKET}/_config/load.json`\n+gs://${INGESTION_BUCKET}/${BQ_DATASET}/_config/load.json`\n+gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/_config/load.json\n+```\n+The json objects will be merged where key conflicts are resolved by config in\n+the closest directory to the data.\n+If the files contents were like this:\n+`gs://${INGESTION_BUCKET}/_config/load.json`:\n+```json\n+{\n+    \"sourceFormat\": \"CSV\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+    \"schemaUpdateOptions\": [\"ALLOW_FILED_RELAXATION\"]\n+}\n+```\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/_config/load.json`:\n+```json\n+{\n+    \"sourceFormat\": \"AVRO\"\n+}\n+```\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/_config/load.json`:\n+```json\n+{\n+    \"writeDisposition\": \"WRTITE_TRUNCATE\"\n+}\n+```\n+\n+The result of merging these would be:\n+```json\n+{\n+    \"sourceFormat\": \"AVRO\",\n+    \"writeDisposition\": \"WRITE_TRUNCATE\",\n+    \"schemaUpdateOptions\": [\"ALLOW_FILED_RELAXATION\"]\n+}\n+```\n+This configuration system gives us the ability to DRY up common defaults but\n+override them at whatever level is appropriate as new cases come up.\n+\n+#### Transformation SQL\n+In some cases we may need to perform transformations on the files in GCS\n+before they can be loaded to BigQuery. This is handled by query on an\n+temporary external table over the GCS objects as a proxy for load job.\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/_config/bq_transform.sql`\n+\n+Note, external queries will consume query slots from this project's reservation\n+or count towards your on-demand billing. They will _not_ use free tie load slots.\n+\n+Note, that the query should select from a `temp_ext` which will be a temporary\n+external table configured on the fly by the Cloud Function.\n+The query must handle the logic for inserting into the destination table.\n+This means it should use BigQuery DML to either `INSERT` or `MERGE` into the\n+destination table.\n+For example:\n+```sql\n+INSERT {dest_dataset}.{dest_table}\n+SELECT * FROM temp_ext\n+```\n+Note that `{dest_dataset}` and `{dest_table}` can be used to inject the dataset\n+and table inferred from the GCS path.\n+\n+\n+The query will be run with the appropriate [external table definitions](https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#ExternalDataConfiguration)\n+defined in:\n+`gs://${INGESTION_BUCKET}/${BQ_DATASET}/${BQ_TABLE_NAME}/_config/external.json`\n+If this file is missing the external table will be assumed to be `PARQUET` format.\n+\n+### Partitions\n+\n+#### Partition Table Decorators\n+Note that if the directory immediately before the triggering successfile starts with\n+a `$` it will be treated as a BigQuery Partition decorator for the destination table.\n+\n+This means for:\n+```text\n+gs://${BUCKET}/foo/bar/$20201026/_SUCCESS\n+```\n+will trigger a load job with a destination table of `foo.bar$20201026`\n+This allows you to specify write disposition at the partition level.\n+This can be helpful in reprocessing scenarios where you'd want to `WRITE_TRUNCATE`\n+a partition that had some data quality issue.\n+\n+#### Hive Partitioning\n+If your data will be uploaded to GCS from a hadoop system that uses the \n+[supported default hive partitioning](https://cloud.google.com/bigquery/docs/hive-partitioned-loads-gcs#supported_data_layouts)\n+you can specify this in the [`hivePartitioningOptions`](https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#hivepartitioningoptions)\n+key of `load.json` for that table.\n+\n+Any non-trivial incremental loading to partitions should usually use the\n+Transformation SQL to define the `INSERT / MERGE / UPDATE / DELETE` logic into\n+the target BQ table as these DML semantics are much more flexible thant the load\n+job write dispositions.\n+Furthermore, using external query has the added benefit of circumventing the \n+per load job bytes limits (default 15 TB) and commiting large partitions\n+atomically.\n+\n+## Handling Incremental Loads\n+This solution introduces the concept of `batch_id` which uniquely identifies \n+a batch of data committed by an upstream system that needs to be picked up as an\n+incremental load. You can again set the load job or external query configuration\n+at any parent folders `_config` prefix. This allows you dictate\n+\"for this table any new batch should `WRITE_TRUNCATE` it's parent partition/table\"\n+or \"for that table any new batch should `WRITE_APPEND` to it's parent partition/table\".\n+\n+## Monitoring\n+Monitoring what data has been loaded by this solution should be done with the\n+BigQuery [`INFORMATION_SCHEMA` jobs metadata](https://cloud.google.com/bigquery/docs/information-schema-jobs)\n+If more granular data is needed about a particular job id \n+\n+### Job Naming Convention\n+All load or external query jobs will have a job id witha  prefix following this convention:\n+```python3\n+job_id_prefix=f\"gcf-ingest-{dest_table_ref.dataset_id}-{dest_table_ref.table_id}-{1}-of-{1}-\"\n+```\n+\n+### Job Labels\n+All load or external query jobs are labelled with functional component and cloud function name.\n+```python3\n+DEFAULT_JOB_LABELS = {\n+    \"component\": \"event-based-gcs-ingest\",\n+    \"cloud-function-name\": getenv(\"FUNCTION_NAME\"),\n+    \"gcs-prefix\": gs://bucket/prefix/for/this/ingest,\n+}\n+```\n+If the destination regex matches a batch group, there will be a `batch-id` label.\n+\n+### Example INFROMATION SCHEMA Query\n+```sql\n+SELECT\n+   job_id,\n+   job_type,\n+   start_time,\n+   end_time,\n+   query,\n+   total_bytes_processed,\n+   total_slot_ms,\n+   destination_table\n+   state,\n+   (SELECT value FROM UNNEST(labels) WHERE key = \"component\") as component,\n+   (SELECT value FROM UNNEST(labels) WHERE key = \"cloud-function-name\") as cloud_function_name,\n+   (SELECT value FROM UNNEST(labels) WHERE key = \"batch-id\") as batch_id,\n+FROM\n+   `region-us`.INFORMATION_SCHEMA.JOBS_BY_PROJECT\n+WHERE\n+   (SELECT value FROM UNNEST(labels) WHERE key = \"component\") = \"event-based-gcs-ingest\"\n+```\n+\n+## Triggers\n+\n+### Pub/Sub Storage Notifications `_SUCCESS`\n+1. Trigger on `_SUCCESS` File to load all other files in that directory.\n+1. Trigger on non-`_SUCCESS` File will no-op", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4a345c8d3aa4068c1c9631cb052cd316f5039c85"}, "originalPosition": 200}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2Nzc5Nw==", "bodyText": "Where is this referenced?", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r515667797", "createdAt": "2020-11-01T20:26:38Z", "author": {"login": "ryanmcdowell"}, "path": "cloud_functions/gcs_event_based_ingest/tests/test_gcs_ocn_bq_ingest_it.py", "diffHunk": "@@ -0,0 +1,388 @@\n+# Copyright 2020 Google LLC\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"unit tests for gcs_ocn_bq_ingest\"\"\"\n+import json\n+import logging\n+import os\n+import sys\n+import uuid\n+from time import sleep\n+from typing import List\n+\n+import google.cloud.storage as storage\n+import pytest\n+from google.cloud import bigquery\n+from google.cloud.exceptions import NotFound\n+\n+sys.path.append(os.path.realpath(os.path.dirname(__file__) + \"/..\"))\n+from gcs_ocn_bq_ingest import main\n+\n+TEST_DIR = os.path.dirname(os.path.abspath(__file__))\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def bq() -> bigquery.Client:\n+    \"\"\"BigQuery Client\"\"\"\n+    return bigquery.Client(location=\"US\")\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def gcs() -> storage.Client:\n+    \"\"\"GCS Client\"\"\"\n+    return storage.Client()\n+\n+\n+@pytest.fixture(scope=\"module\")\n+@pytest.mark.usefixtures(\"gcs\")\n+def gcs_bucket(request, gcs) -> storage.bucket.Bucket:\n+    \"\"\"GCS bucket for test artifacts\"\"\"\n+    bucket = gcs.create_bucket(str(uuid.uuid4()))\n+\n+    def teardown():\n+        bucket.delete(force=True)\n+\n+    request.addfinalizer(teardown)\n+\n+    return bucket\n+\n+\n+@pytest.mark.usefixtures(\"gcs_bucket\")\n+@pytest.fixture\n+def mock_env(gcs, monkeypatch):\n+    \"\"\"environment variable mocks\"\"\"\n+    # Infer project from ADC of gcs client.\n+    monkeypatch.setenv(\"GCP_PROJECT\", gcs.project)\n+    monkeypatch.setenv(\"FUNCTION_NAME\", \"integration-test\")\n+\n+\n+@pytest.mark.usefixtures(\"bq\", \"mock_env\")\n+@pytest.fixture\n+def dest_dataset(request, bq, mock_env, monkeypatch):\n+    random_dataset = f\"test_bq_ingest_gcf_{str(uuid.uuid4())[:8].replace('-','_')}\"\n+    dataset = bigquery.Dataset(f\"{os.getenv('GCP_PROJECT')}\"\n+                               f\".{random_dataset}\")\n+    dataset.location = \"US\"\n+    bq.create_dataset(dataset)\n+    monkeypatch.setenv(\"BQ_LOAD_STATE_TABLE\",\n+                       f\"{dataset.dataset_id}.serverless_bq_loads\")\n+    print(f\"created dataset {dataset.dataset_id}\")\n+\n+    def teardown():\n+        bq.delete_dataset(dataset, delete_contents=True, not_found_ok=True)\n+\n+    request.addfinalizer(teardown)\n+    return dataset\n+\n+\n+@pytest.mark.usefixtures(\"bq\", \"mock_env\", \"dest_dataset\")\n+@pytest.fixture\n+def dest_table(request, bq, mock_env, dest_dataset):\n+    with open(os.path.join(TEST_DIR, \"resources\",\n+                           \"schema.json\")) as schema_file:\n+        schema = main.dict_to_bq_schema(json.load(schema_file))\n+\n+    table = bigquery.Table(\n+        f\"{os.environ.get('GCP_PROJECT')}.{dest_dataset.dataset_id}.cf_test_nation\",\n+        schema=schema,\n+    )\n+\n+    table = bq.create_table(table)\n+\n+    def teardown():\n+        bq.delete_table(table, not_found_ok=True)\n+\n+    request.addfinalizer(teardown)\n+    return table\n+\n+\n+@pytest.fixture(scope=\"function\")\n+@pytest.mark.usefixtures(\"gcs_bucket\", \"dest_dataset\", \"dest_table\")\n+def gcs_data(request, gcs_bucket, dest_dataset,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4a345c8d3aa4068c1c9631cb052cd316f5039c85"}, "originalPosition": 111}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0d6fdaf2b2e01f5ccf54ab2204d1ee6e2aca5d2f", "author": {"user": {"login": "ryanmcdowell", "name": "Ryan McDowell"}}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/0d6fdaf2b2e01f5ccf54ab2204d1ee6e2aca5d2f", "committedDate": "2020-11-02T14:08:11Z", "message": "Merge branch 'master' into cloud-functions-ingest"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a542f385f6bce2e07d65b9d0a6e214523517570d", "author": {"user": null}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/a542f385f6bce2e07d65b9d0a6e214523517570d", "committedDate": "2020-11-02T19:34:59Z", "message": "fixup ci dockerfile"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c4da1e34622ee6afa332195dcb74fcb88153ce5e", "author": {"user": null}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/c4da1e34622ee6afa332195dcb74fcb88153ce5e", "committedDate": "2020-11-02T19:59:01Z", "message": "more ci"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bfc9622038f1ba8d1006e9057b69990fcedb5981", "author": {"user": null}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/bfc9622038f1ba8d1006e9057b69990fcedb5981", "committedDate": "2020-11-02T20:01:59Z", "message": "Merge branch 'cloud-functions-ingest' of github.com:jaketf/bigquery-utils into cloud-functions-ingest"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIyNjA4OTAx", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#pullrequestreview-522608901", "createdAt": "2020-11-03T15:25:42Z", "commit": {"oid": "bfc9622038f1ba8d1006e9057b69990fcedb5981"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNToyNTo0MlrOHsz9UA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNToyNTo0MlrOHsz9UA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjc1MDY3Mg==", "bodyText": "Let's default this to CSV to align with the load job default", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r516750672", "createdAt": "2020-11-03T15:25:42Z", "author": {"login": "danieldeleo"}, "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -0,0 +1,459 @@\n+# Copyright 2020 Google LLC.\n+# This software is provided as-is, without warranty or representation\n+# for any use or purpose.\n+# Your use of it is subject to your agreement with Google.\n+\n+# Licensed under the Apache License, Version 2.0 (the 'License');\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+# http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an 'AS IS' BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Background Cloud Function for loading data from GCS to BigQuery.\n+\"\"\"\n+import json\n+import re\n+from collections import deque\n+from os import getenv\n+from pathlib import Path\n+from time import monotonic, sleep\n+from typing import Any, Deque, Dict, List, Optional, Tuple\n+from urllib.parse import urlparse\n+\n+from google.api_core.client_info import ClientInfo\n+from google.api_core.exceptions import PreconditionFailed\n+from google.cloud import bigquery, storage\n+from google.cloud.exceptions import NotFound\n+\n+# 15TB per BQ load job.\n+DEFAULT_MAX_BATCH_BYTES = str(15 * 10**12)\n+MAX_URIS_PER_LOAD = 10**4\n+\n+DEFAULT_EXTERNAL_TABLE_DEFINITION = {\"sourceFormat\": \"PARQUET\"}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bfc9622038f1ba8d1006e9057b69990fcedb5981"}, "originalPosition": 37}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIyNjI0NDgy", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#pullrequestreview-522624482", "createdAt": "2020-11-03T15:40:31Z", "commit": {"oid": "bfc9622038f1ba8d1006e9057b69990fcedb5981"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNTo0MDozMVrOHs0qtQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNTo0MDozMVrOHs0qtQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjc2MjI5Mw==", "bodyText": "Nit, but perhaps rename to MAX_SOURCE_URIS_PER_LOAD and/or add to above comment the following link: https://cloud.google.com/bigquery/quotas#load_jobs", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r516762293", "createdAt": "2020-11-03T15:40:31Z", "author": {"login": "danieldeleo"}, "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -0,0 +1,459 @@\n+# Copyright 2020 Google LLC.\n+# This software is provided as-is, without warranty or representation\n+# for any use or purpose.\n+# Your use of it is subject to your agreement with Google.\n+\n+# Licensed under the Apache License, Version 2.0 (the 'License');\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+# http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an 'AS IS' BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Background Cloud Function for loading data from GCS to BigQuery.\n+\"\"\"\n+import json\n+import re\n+from collections import deque\n+from os import getenv\n+from pathlib import Path\n+from time import monotonic, sleep\n+from typing import Any, Deque, Dict, List, Optional, Tuple\n+from urllib.parse import urlparse\n+\n+from google.api_core.client_info import ClientInfo\n+from google.api_core.exceptions import PreconditionFailed\n+from google.cloud import bigquery, storage\n+from google.cloud.exceptions import NotFound\n+\n+# 15TB per BQ load job.\n+DEFAULT_MAX_BATCH_BYTES = str(15 * 10**12)\n+MAX_URIS_PER_LOAD = 10**4", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bfc9622038f1ba8d1006e9057b69990fcedb5981"}, "originalPosition": 35}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIyNjI2NjEw", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#pullrequestreview-522626610", "createdAt": "2020-11-03T15:42:42Z", "commit": {"oid": "bfc9622038f1ba8d1006e9057b69990fcedb5981"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNTo0Mjo0MlrOHs0wzA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNTo0Mjo0MlrOHs0wzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjc2Mzg1Mg==", "bodyText": "bq tool uses comma as default field delimiter, let's align with that tool", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r516763852", "createdAt": "2020-11-03T15:42:42Z", "author": {"login": "danieldeleo"}, "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -0,0 +1,459 @@\n+# Copyright 2020 Google LLC.\n+# This software is provided as-is, without warranty or representation\n+# for any use or purpose.\n+# Your use of it is subject to your agreement with Google.\n+\n+# Licensed under the Apache License, Version 2.0 (the 'License');\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+# http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an 'AS IS' BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Background Cloud Function for loading data from GCS to BigQuery.\n+\"\"\"\n+import json\n+import re\n+from collections import deque\n+from os import getenv\n+from pathlib import Path\n+from time import monotonic, sleep\n+from typing import Any, Deque, Dict, List, Optional, Tuple\n+from urllib.parse import urlparse\n+\n+from google.api_core.client_info import ClientInfo\n+from google.api_core.exceptions import PreconditionFailed\n+from google.cloud import bigquery, storage\n+from google.cloud.exceptions import NotFound\n+\n+# 15TB per BQ load job.\n+DEFAULT_MAX_BATCH_BYTES = str(15 * 10**12)\n+MAX_URIS_PER_LOAD = 10**4\n+\n+DEFAULT_EXTERNAL_TABLE_DEFINITION = {\"sourceFormat\": \"PARQUET\"}\n+\n+DEFAULT_JOB_LABELS = {\n+    \"component\": \"event-based-gcs-ingest\",\n+    \"cloud-function-name\": getenv(\"FUNCTION_NAME\"),\n+}\n+\n+BASE_LOAD_JOB_CONFIG = {\n+    \"sourceFormat\": \"CSV\",\n+    \"fieldDelimiter\": \"|\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bfc9622038f1ba8d1006e9057b69990fcedb5981"}, "originalPosition": 46}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIyNjM0Nzcx", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#pullrequestreview-522634771", "createdAt": "2020-11-03T15:51:00Z", "commit": {"oid": "bfc9622038f1ba8d1006e9057b69990fcedb5981"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNTo1MTowMFrOHs1IpQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNTo1MTowMFrOHs1IpQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjc2OTk1Nw==", "bodyText": "Update message to default CSV external table", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r516769957", "createdAt": "2020-11-03T15:51:00Z", "author": {"login": "danieldeleo"}, "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -0,0 +1,459 @@\n+# Copyright 2020 Google LLC.\n+# This software is provided as-is, without warranty or representation\n+# for any use or purpose.\n+# Your use of it is subject to your agreement with Google.\n+\n+# Licensed under the Apache License, Version 2.0 (the 'License');\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+# http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an 'AS IS' BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Background Cloud Function for loading data from GCS to BigQuery.\n+\"\"\"\n+import json\n+import re\n+from collections import deque\n+from os import getenv\n+from pathlib import Path\n+from time import monotonic, sleep\n+from typing import Any, Deque, Dict, List, Optional, Tuple\n+from urllib.parse import urlparse\n+\n+from google.api_core.client_info import ClientInfo\n+from google.api_core.exceptions import PreconditionFailed\n+from google.cloud import bigquery, storage\n+from google.cloud.exceptions import NotFound\n+\n+# 15TB per BQ load job.\n+DEFAULT_MAX_BATCH_BYTES = str(15 * 10**12)\n+MAX_URIS_PER_LOAD = 10**4\n+\n+DEFAULT_EXTERNAL_TABLE_DEFINITION = {\"sourceFormat\": \"PARQUET\"}\n+\n+DEFAULT_JOB_LABELS = {\n+    \"component\": \"event-based-gcs-ingest\",\n+    \"cloud-function-name\": getenv(\"FUNCTION_NAME\"),\n+}\n+\n+BASE_LOAD_JOB_CONFIG = {\n+    \"sourceFormat\": \"CSV\",\n+    \"fieldDelimiter\": \"|\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+    \"labels\": DEFAULT_JOB_LABELS,\n+}\n+\n+DEFAULT_DESTINATION_REGEX = r\"(?P<dataset>[\\w\\-_0-9]+)/\" \\\n+                            r\"(?P<table>[\\w\\-_0-9]+)/\" \\\n+                            r\"?(?P<partition>\\$[0-9]+)?/\" \\\n+                            r\"?(?P<batch>[\\w\\-_0-9]+)?/\"\n+\n+# Will wait up to this polling for errors before exiting\n+# This is to check if job fail quickly, not to assert the succeed\n+# This may not be honored if longer than cloud function timeout\n+# https://cloud.google.com/functions/docs/concepts/exec#timeout\n+WAIT_FOR_JOB_SECONDS = 5\n+SUCCESS_FILENAME = getenv(\"SUCCESS_FILENAME\", \"_SUCCESS\")\n+\n+CLIENT_INFO = ClientInfo(user_agent=\"google-pso-tool/bq-severless-loader\")\n+\n+\n+def main(event: Dict, context):    # pylint: disable=unused-argument\n+    \"\"\"entry point for background cloud function for event driven GCS to\n+    BigQuery ingest.\"\"\"\n+    # pylint: disable=too-many-locals\n+    # Set by Cloud Function Execution Environment\n+    # https://cloud.google.com/functions/docs/env-var\n+    project = getenv(\"GCP_PROJECT\")\n+    destination_regex = getenv(\"DESTINATION_REGEX\", DEFAULT_DESTINATION_REGEX)\n+    dest_re = re.compile(destination_regex)\n+\n+    bucket_id, object_id = parse_notification(event)\n+\n+    # Exit eagerly if not a success file.\n+    # we can improve this with pub/sub message filtering once it supports\n+    # a hasSuffix filter function (we can filter on hasSuffix successfile name)\n+    #  https://cloud.google.com/pubsub/docs/filtering\n+    if not object_id.endswith(f\"/{SUCCESS_FILENAME}\"):\n+        print(\n+            f\"No-op. This notification was not for a {SUCCESS_FILENAME} file.\")\n+        return\n+\n+    prefix_to_load = removesuffix(object_id, SUCCESS_FILENAME)\n+    gsurl = f\"gs://{bucket_id}/{prefix_to_load}\"\n+    gcs_client = storage.Client(client_info=CLIENT_INFO)\n+    bkt = gcs_client.lookup_bucket(bucket_id)\n+    success_blob: storage.Blob = bkt.blob(object_id)\n+    handle_duplicate_notification(bkt, success_blob, gsurl)\n+\n+    destination_match = dest_re.match(object_id)\n+    if not destination_match:\n+        raise RuntimeError(f\"Object ID {object_id} did not match regex:\"\n+                           f\" {destination_regex}\")\n+    destination_details = destination_match.groupdict()\n+    try:\n+        dataset = destination_details['dataset']\n+        table = destination_details['table']\n+    except KeyError:\n+        raise RuntimeError(\n+            f\"Object ID {object_id} did not match dataset and table in regex:\"\n+            f\" {destination_regex}\") from KeyError\n+    partition = destination_details.get('partition')\n+    batch_id = destination_details.get('batch')\n+    labels = DEFAULT_JOB_LABELS\n+    labels[\"bucket\"] = bucket_id\n+\n+    if batch_id:\n+        labels[\"batch-id\"] = batch_id\n+\n+    if partition:\n+        dest_table_ref = bigquery.TableReference.from_string(\n+            f\"{dataset}.{table}{partition}\", default_project=project)\n+    else:\n+        dest_table_ref = bigquery.TableReference.from_string(\n+            f\"{dataset}.{table}\", default_project=project)\n+\n+    default_query_config = bigquery.QueryJobConfig()\n+    default_query_config.use_legacy_sql = False\n+    default_query_config.labels = labels\n+    bq_client = bigquery.Client(client_info=CLIENT_INFO,\n+                                default_query_job_config=default_query_config)\n+\n+    print(f\"looking for {gsurl}_config/bq_transform.sql\")\n+    external_query_sql = read_gcs_file_if_exists(\n+        gcs_client, f\"{gsurl}_config/bq_transform.sql\")\n+    print(f\"external_query_sql = {external_query_sql}\")\n+    if not external_query_sql:\n+        external_query_sql = look_for_transform_sql(gcs_client, gsurl)\n+    if external_query_sql:\n+        print(\"EXTERNAL QUERY\")\n+        external_query(gcs_client, bq_client, gsurl, external_query_sql,\n+                       dest_table_ref)\n+        return\n+\n+    print(\"LOAD_JOB\")\n+    load_batches(gcs_client, bq_client, gsurl, dest_table_ref)\n+\n+\n+def handle_duplicate_notification(\n+    bkt: storage.Bucket,\n+    success_blob: storage.Blob,\n+    gsurl: str\n+):\n+    \"\"\"\n+    Need to handle potential duplicate Pub/Sub notifications.\n+    To achieve this we will drop an empty \"claimed\" file that indicates\n+    an invocation of this cloud function has picked up the success file\n+    with a certain creation timestamp. This will support republishing the\n+    success file as a mechanism of re-running the ingestion while avoiding\n+    duplicate ingestion due to multiple Pub/Sub messages for a success file\n+    with the same creation time.\n+    \"\"\"\n+    success_blob.reload()\n+    success_created_unix_timestamp = success_blob.time_created.timestamp()\n+\n+    claim_blob: storage.Blob = bkt.blob(\n+        f\"_claimed_{success_created_unix_timestamp}\")\n+    try:\n+        claim_blob.upload_from_string(\n+            \"\",\n+            if_generation_match=0)\n+    except PreconditionFailed as err:\n+        raise RuntimeError(\n+            f\"The prefix {gsurl} appears to already have been claimed for \"\n+            f\"{gsurl}{SUCCESS_FILENAME} with created timestamp\"\n+            f\"{success_created_unix_timestamp}.\"\n+            \"This means that another invocation of this cloud function has\"\n+            \"claimed the ingestion of this batch.\"\n+            \"This may be due to a rare duplicate delivery of the Pub/Sub \"\n+            \"storage notification.\") from err\n+\n+\n+def external_query(gcs_client, bq_client, gsurl, query, dest_table_ref):\n+    \"\"\"Load from query over external table from GCS.\n+\n+    This hinges on a SQL query defined in GCS at _config/bq_transform.sql and\n+    an external table definition _config/external.json (otherwise will assume\n+    parquet external table)\n+    \"\"\"\n+    external_table_config = read_gcs_file_if_exists(\n+        gcs_client, f\"{gsurl}_config/external.json\")\n+    if external_table_config:\n+        external_table_def = json.loads(external_table_config)\n+    else:\n+        print(f\"Falling back to default parquet external table.\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bfc9622038f1ba8d1006e9057b69990fcedb5981"}, "originalPosition": 189}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIyNjYwNTI1", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#pullrequestreview-522660525", "createdAt": "2020-11-03T16:17:17Z", "commit": {"oid": "bfc9622038f1ba8d1006e9057b69990fcedb5981"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNjoxNzoxN1rOHs2S3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNjoxNzoxN1rOHs2S3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjc4ODk1Ng==", "bodyText": "query is wrapped in b'' because it's a bytestring. Instead of [2:-1] just call --> query.decode('UTF-8')", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r516788956", "createdAt": "2020-11-03T16:17:17Z", "author": {"login": "danieldeleo"}, "path": "cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -0,0 +1,459 @@\n+# Copyright 2020 Google LLC.\n+# This software is provided as-is, without warranty or representation\n+# for any use or purpose.\n+# Your use of it is subject to your agreement with Google.\n+\n+# Licensed under the Apache License, Version 2.0 (the 'License');\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+# http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an 'AS IS' BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Background Cloud Function for loading data from GCS to BigQuery.\n+\"\"\"\n+import json\n+import re\n+from collections import deque\n+from os import getenv\n+from pathlib import Path\n+from time import monotonic, sleep\n+from typing import Any, Deque, Dict, List, Optional, Tuple\n+from urllib.parse import urlparse\n+\n+from google.api_core.client_info import ClientInfo\n+from google.api_core.exceptions import PreconditionFailed\n+from google.cloud import bigquery, storage\n+from google.cloud.exceptions import NotFound\n+\n+# 15TB per BQ load job.\n+DEFAULT_MAX_BATCH_BYTES = str(15 * 10**12)\n+MAX_URIS_PER_LOAD = 10**4\n+\n+DEFAULT_EXTERNAL_TABLE_DEFINITION = {\"sourceFormat\": \"PARQUET\"}\n+\n+DEFAULT_JOB_LABELS = {\n+    \"component\": \"event-based-gcs-ingest\",\n+    \"cloud-function-name\": getenv(\"FUNCTION_NAME\"),\n+}\n+\n+BASE_LOAD_JOB_CONFIG = {\n+    \"sourceFormat\": \"CSV\",\n+    \"fieldDelimiter\": \"|\",\n+    \"writeDisposition\": \"WRITE_APPEND\",\n+    \"labels\": DEFAULT_JOB_LABELS,\n+}\n+\n+DEFAULT_DESTINATION_REGEX = r\"(?P<dataset>[\\w\\-_0-9]+)/\" \\\n+                            r\"(?P<table>[\\w\\-_0-9]+)/\" \\\n+                            r\"?(?P<partition>\\$[0-9]+)?/\" \\\n+                            r\"?(?P<batch>[\\w\\-_0-9]+)?/\"\n+\n+# Will wait up to this polling for errors before exiting\n+# This is to check if job fail quickly, not to assert the succeed\n+# This may not be honored if longer than cloud function timeout\n+# https://cloud.google.com/functions/docs/concepts/exec#timeout\n+WAIT_FOR_JOB_SECONDS = 5\n+SUCCESS_FILENAME = getenv(\"SUCCESS_FILENAME\", \"_SUCCESS\")\n+\n+CLIENT_INFO = ClientInfo(user_agent=\"google-pso-tool/bq-severless-loader\")\n+\n+\n+def main(event: Dict, context):    # pylint: disable=unused-argument\n+    \"\"\"entry point for background cloud function for event driven GCS to\n+    BigQuery ingest.\"\"\"\n+    # pylint: disable=too-many-locals\n+    # Set by Cloud Function Execution Environment\n+    # https://cloud.google.com/functions/docs/env-var\n+    project = getenv(\"GCP_PROJECT\")\n+    destination_regex = getenv(\"DESTINATION_REGEX\", DEFAULT_DESTINATION_REGEX)\n+    dest_re = re.compile(destination_regex)\n+\n+    bucket_id, object_id = parse_notification(event)\n+\n+    # Exit eagerly if not a success file.\n+    # we can improve this with pub/sub message filtering once it supports\n+    # a hasSuffix filter function (we can filter on hasSuffix successfile name)\n+    #  https://cloud.google.com/pubsub/docs/filtering\n+    if not object_id.endswith(f\"/{SUCCESS_FILENAME}\"):\n+        print(\n+            f\"No-op. This notification was not for a {SUCCESS_FILENAME} file.\")\n+        return\n+\n+    prefix_to_load = removesuffix(object_id, SUCCESS_FILENAME)\n+    gsurl = f\"gs://{bucket_id}/{prefix_to_load}\"\n+    gcs_client = storage.Client(client_info=CLIENT_INFO)\n+    bkt = gcs_client.lookup_bucket(bucket_id)\n+    success_blob: storage.Blob = bkt.blob(object_id)\n+    handle_duplicate_notification(bkt, success_blob, gsurl)\n+\n+    destination_match = dest_re.match(object_id)\n+    if not destination_match:\n+        raise RuntimeError(f\"Object ID {object_id} did not match regex:\"\n+                           f\" {destination_regex}\")\n+    destination_details = destination_match.groupdict()\n+    try:\n+        dataset = destination_details['dataset']\n+        table = destination_details['table']\n+    except KeyError:\n+        raise RuntimeError(\n+            f\"Object ID {object_id} did not match dataset and table in regex:\"\n+            f\" {destination_regex}\") from KeyError\n+    partition = destination_details.get('partition')\n+    batch_id = destination_details.get('batch')\n+    labels = DEFAULT_JOB_LABELS\n+    labels[\"bucket\"] = bucket_id\n+\n+    if batch_id:\n+        labels[\"batch-id\"] = batch_id\n+\n+    if partition:\n+        dest_table_ref = bigquery.TableReference.from_string(\n+            f\"{dataset}.{table}{partition}\", default_project=project)\n+    else:\n+        dest_table_ref = bigquery.TableReference.from_string(\n+            f\"{dataset}.{table}\", default_project=project)\n+\n+    default_query_config = bigquery.QueryJobConfig()\n+    default_query_config.use_legacy_sql = False\n+    default_query_config.labels = labels\n+    bq_client = bigquery.Client(client_info=CLIENT_INFO,\n+                                default_query_job_config=default_query_config)\n+\n+    print(f\"looking for {gsurl}_config/bq_transform.sql\")\n+    external_query_sql = read_gcs_file_if_exists(\n+        gcs_client, f\"{gsurl}_config/bq_transform.sql\")\n+    print(f\"external_query_sql = {external_query_sql}\")\n+    if not external_query_sql:\n+        external_query_sql = look_for_transform_sql(gcs_client, gsurl)\n+    if external_query_sql:\n+        print(\"EXTERNAL QUERY\")\n+        external_query(gcs_client, bq_client, gsurl, external_query_sql,\n+                       dest_table_ref)\n+        return\n+\n+    print(\"LOAD_JOB\")\n+    load_batches(gcs_client, bq_client, gsurl, dest_table_ref)\n+\n+\n+def handle_duplicate_notification(\n+    bkt: storage.Bucket,\n+    success_blob: storage.Blob,\n+    gsurl: str\n+):\n+    \"\"\"\n+    Need to handle potential duplicate Pub/Sub notifications.\n+    To achieve this we will drop an empty \"claimed\" file that indicates\n+    an invocation of this cloud function has picked up the success file\n+    with a certain creation timestamp. This will support republishing the\n+    success file as a mechanism of re-running the ingestion while avoiding\n+    duplicate ingestion due to multiple Pub/Sub messages for a success file\n+    with the same creation time.\n+    \"\"\"\n+    success_blob.reload()\n+    success_created_unix_timestamp = success_blob.time_created.timestamp()\n+\n+    claim_blob: storage.Blob = bkt.blob(\n+        f\"_claimed_{success_created_unix_timestamp}\")\n+    try:\n+        claim_blob.upload_from_string(\n+            \"\",\n+            if_generation_match=0)\n+    except PreconditionFailed as err:\n+        raise RuntimeError(\n+            f\"The prefix {gsurl} appears to already have been claimed for \"\n+            f\"{gsurl}{SUCCESS_FILENAME} with created timestamp\"\n+            f\"{success_created_unix_timestamp}.\"\n+            \"This means that another invocation of this cloud function has\"\n+            \"claimed the ingestion of this batch.\"\n+            \"This may be due to a rare duplicate delivery of the Pub/Sub \"\n+            \"storage notification.\") from err\n+\n+\n+def external_query(gcs_client, bq_client, gsurl, query, dest_table_ref):\n+    \"\"\"Load from query over external table from GCS.\n+\n+    This hinges on a SQL query defined in GCS at _config/bq_transform.sql and\n+    an external table definition _config/external.json (otherwise will assume\n+    parquet external table)\n+    \"\"\"\n+    external_table_config = read_gcs_file_if_exists(\n+        gcs_client, f\"{gsurl}_config/external.json\")\n+    if external_table_config:\n+        external_table_def = json.loads(external_table_config)\n+    else:\n+        print(f\"Falling back to default parquet external table.\"\n+              f\" {gsurl}/_config/external.json not found.\")\n+        external_table_def = DEFAULT_EXTERNAL_TABLE_DEFINITION\n+\n+    external_table_def[\"sourceUris\"] = flatten(\n+        get_batches_for_prefix(gcs_client, gsurl))\n+    external_config = bigquery.ExternalConfig.from_api_repr(external_table_def)\n+    job_config = bigquery.QueryJobConfig(\n+        table_definitions={\"temp_ext\": external_config}, use_legacy_sql=False)\n+    # for some reason query string literal wrapped in b''\n+    rendered_query = str(\n+        str(query).format(dest_dataset=dest_table_ref.dataset_id,\n+                          dest_table=dest_table_ref.table_id))[2:-1]", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bfc9622038f1ba8d1006e9057b69990fcedb5981"}, "originalPosition": 201}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "41dc1cd6abc540cecbab21223e49e7060695d5f7", "author": {"user": null}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/41dc1cd6abc540cecbab21223e49e7060695d5f7", "committedDate": "2020-11-03T20:36:08Z", "message": "better defaults, faster ci"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIyODU3Nzg4", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#pullrequestreview-522857788", "createdAt": "2020-11-03T20:39:55Z", "commit": {"oid": "41dc1cd6abc540cecbab21223e49e7060695d5f7"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMDozOTo1NVrOHs_qXw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMDozOTo1NVrOHs_qXw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk0MjQzMQ==", "bodyText": "cc: @danieldeleo FYI because we know cloud build we have the source cloned locally we can remove this step speeding up image build time significantly as the ci container doesn't need the source inside the container.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r516942431", "createdAt": "2020-11-03T20:39:55Z", "author": {"login": "jaketf"}, "path": "cloud_functions/gcs_event_based_ingest/Dockerfile.ci", "diffHunk": "@@ -1,6 +1,4 @@\n FROM python:3.8-slim\n COPY requirements.txt requirements-dev.txt ./\n RUN pip3 install -r requirements-dev.txt\n-WORKDIR /ci", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "41dc1cd6abc540cecbab21223e49e7060695d5f7"}, "originalPosition": 4}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIyODU4ODcx", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#pullrequestreview-522858871", "createdAt": "2020-11-03T20:41:48Z", "commit": {"oid": "41dc1cd6abc540cecbab21223e49e7060695d5f7"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMDo0MTo0OFrOHs_trg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMDo0MTo0OFrOHs_trg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk0MzI3OA==", "bodyText": "Pivoting to this approach has two benefits:\n\ntable.get requests are free (v.s SELECT COUNT(*) queries which cost / consume slots)\nspeeds up tests ~50%  on average (depending on what the bq load slot pool latency was).", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r516943278", "createdAt": "2020-11-03T20:41:48Z", "author": {"login": "jaketf"}, "path": "cloud_functions/gcs_event_based_ingest/tests/test_gcs_ocn_bq_ingest_it.py", "diffHunk": "@@ -374,15 +357,54 @@ def test_external_query_IT(bq, gcs_data, gcs_external_config, dest_dataset,\n         }\n     }\n     main.main(test_event, None)\n-    sleep(3)    # Need to wait on async query job\n-    validation_query_job = bq.query(f\"\"\"\n-        SELECT\n-            COUNT(*) as count\n-        FROM\n-          `{os.environ.get('GCP_PROJECT')}.{dest_dataset.dataset_id}.{dest_table.table_id}`\n-    \"\"\")\n-\n     test_data_file = os.path.join(TEST_DIR, \"resources\", \"test-data\", \"nation\",\n                                   \"part-m-00001\")\n-    for row in validation_query_job.result():\n-        assert row[\"count\"] == sum(1 for _ in open(test_data_file))\n+    expected_num_rows = sum(1 for _ in open(test_data_file))\n+    try:\n+        bq_wait_for_rows(bq, dest_table, expected_num_rows)\n+    except TimeoutError as err:\n+        raise AssertionError from err\n+\n+\n+def bq_wait_for_rows(bq_client: bigquery.Client, table: bigquery.Table,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "41dc1cd6abc540cecbab21223e49e7060695d5f7"}, "originalPosition": 200}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "54390f6198e25cdc4ddb2a5486979e3f7bd4e32c", "author": {"user": null}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/54390f6198e25cdc4ddb2a5486979e3f7bd4e32c", "committedDate": "2020-11-03T23:14:19Z", "message": "add integration test for partitioned tables / data"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7e7272db359c6b30ebfbb05870f522a9f0695aed", "author": {"user": null}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/7e7272db359c6b30ebfbb05870f522a9f0695aed", "committedDate": "2020-11-04T19:17:50Z", "message": "parallelize tests 4x+ speed up in ci"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ddc265615275c72a983c5030ff9b08e5b296860b", "author": {"user": null}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/ddc265615275c72a983c5030ff9b08e5b296860b", "committedDate": "2020-11-04T19:46:31Z", "message": "fixup docs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b0bafe3e4ddcdfa5b4de35912b81d7e69677d89e", "author": {"user": null}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/b0bafe3e4ddcdfa5b4de35912b81d7e69677d89e", "committedDate": "2020-11-05T02:07:31Z", "message": "add terraform module for deployment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3da292d49b236f94d8ac2328b3d2cb8aa323b06b", "author": {"user": null}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/3da292d49b236f94d8ac2328b3d2cb8aa323b06b", "committedDate": "2020-11-05T02:10:33Z", "message": "remove useless gitignore"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIzODc0MzU0", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#pullrequestreview-523874354", "createdAt": "2020-11-05T02:10:48Z", "commit": {"oid": "b0bafe3e4ddcdfa5b4de35912b81d7e69677d89e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQwMjoxMDo0OFrOHtwYTw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQwMjoxMDo0OFrOHtwYTw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc0MDYyMw==", "bodyText": "this doesn't match any files in source tree remove.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#discussion_r517740623", "createdAt": "2020-11-05T02:10:48Z", "author": {"login": "jaketf"}, "path": "cloud_functions/gcs_event_based_ingest/tests/resources/.gitignore", "diffHunk": "@@ -0,0 +1 @@\n+hit_data_2020-06-25_09.tsv", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0bafe3e4ddcdfa5b4de35912b81d7e69677d89e"}, "originalPosition": 1}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f1b5659f3b4b5cccd1277b55ea0f95fa99e6d6a2", "author": {"user": null}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/f1b5659f3b4b5cccd1277b55ea0f95fa99e6d6a2", "committedDate": "2020-11-05T22:26:18Z", "message": "add pytest.mark.IT"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a88939b1d8d5b9f4bbc05ceecdee661345d44b25", "author": {"user": null}, "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/a88939b1d8d5b9f4bbc05ceecdee661345d44b25", "committedDate": "2020-11-09T21:50:15Z", "message": "add note on alternatives"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI4MzE2OTY5", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#pullrequestreview-528316969", "createdAt": "2020-11-11T16:12:03Z", "commit": {"oid": "a88939b1d8d5b9f4bbc05ceecdee661345d44b25"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI4MzIxNzU4", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/182#pullrequestreview-528321758", "createdAt": "2020-11-11T16:17:26Z", "commit": {"oid": "a88939b1d8d5b9f4bbc05ceecdee661345d44b25"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 530, "cost": 1, "resetAt": "2021-11-01T14:20:25Z"}}}