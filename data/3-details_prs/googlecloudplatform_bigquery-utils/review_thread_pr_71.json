{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM4MjI5OTEx", "number": 71, "reviewThreads": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwNToyNzoyNVrOEH0WZw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQwNjowMToxMlrOEInzVQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NjMzMTkxOnYy", "diffSide": "RIGHT", "path": "tools/unsupervised_dataset/Crawler/Crawler.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwNToyNzoyNVrOGnZyBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwNToyNzoyNVrOGnZyBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk3MDA1NQ==", "bodyText": "This can just be set()", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#discussion_r443970055", "createdAt": "2020-06-23T05:27:25Z", "author": {"login": "yzhvictor"}, "path": "tools/unsupervised_dataset/Crawler/Crawler.py", "diffHunk": "@@ -0,0 +1,101 @@\n+import requests\n+import queue\n+import CQNode\n+import Extractor\n+import CrawlerLog\n+\n+class Crawler:\n+    \"\"\" Contains the functions and logic to run and coordinate the crawling process. Given\n+        initial starting URLs and maximum size, the crawler will explore websites to look\n+        for SQL queries.\n+    \"\"\"\n+    \n+    def __init__(self, links, maxDepth=3, maxSize=500):\n+        \"\"\" Initializes the crawler and instance variables.\n+        \n+        Args:\n+            links: The root URLs to begin crawling from. Can be one or more.\n+            maxDepth: The maximum depth for the crawler to explore.\n+            maxSize: THe maximum number of links the crawler should explore.\n+    \n+        \"\"\"\n+        self.linkQueue = queue.Queue()\n+        self.seen = set([])", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52750e8f8330016a4c9c45310413c77ab665413f"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NjM0NTYzOnYy", "diffSide": "RIGHT", "path": "tools/unsupervised_dataset/Crawler/Crawler.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwNTozNDo1N1rOGnZ6ZA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwNTozNDo1N1rOGnZ6ZA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk3MjE5Ng==", "bodyText": "not self.linkQueue.empty()", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#discussion_r443972196", "createdAt": "2020-06-23T05:34:57Z", "author": {"login": "yzhvictor"}, "path": "tools/unsupervised_dataset/Crawler/Crawler.py", "diffHunk": "@@ -0,0 +1,101 @@\n+import requests\n+import queue\n+import CQNode\n+import Extractor\n+import CrawlerLog\n+\n+class Crawler:\n+    \"\"\" Contains the functions and logic to run and coordinate the crawling process. Given\n+        initial starting URLs and maximum size, the crawler will explore websites to look\n+        for SQL queries.\n+    \"\"\"\n+    \n+    def __init__(self, links, maxDepth=3, maxSize=500):\n+        \"\"\" Initializes the crawler and instance variables.\n+        \n+        Args:\n+            links: The root URLs to begin crawling from. Can be one or more.\n+            maxDepth: The maximum depth for the crawler to explore.\n+            maxSize: THe maximum number of links the crawler should explore.\n+    \n+        \"\"\"\n+        self.linkQueue = queue.Queue()\n+        self.seen = set([])\n+        self.maxDepth = maxDepth\n+        self.maxSize = maxSize\n+        self.log = CrawlerLog.CrawlerLog()\n+        self.count = 0\n+        \n+        for link in links:\n+            self.linkQueue.put(CQNode.CQNode(link, 0))\n+            self.seen.add(link)\n+        \n+    def crawl(self):\n+        \"\"\" Begins the crawling process using variables set earlier. Extracts\n+            queries by locating website-specific HTML tags or searching for\n+            common expression patterns. Writes queries to output after finishing\n+            each site.\n+        \"\"\"\n+        \n+        while (self.linkQueue.empty() == False):", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52750e8f8330016a4c9c45310413c77ab665413f"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NjM0ODM4OnYy", "diffSide": "RIGHT", "path": "tools/unsupervised_dataset/Crawler/Crawler.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwNTozNjoyMFrOGnZ7_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwNTozNjoyMFrOGnZ7_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk3MjYwNw==", "bodyText": "Instead of printing to console, try to print to some logger, such as https://docs.python.org/2/library/logging.html", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#discussion_r443972607", "createdAt": "2020-06-23T05:36:20Z", "author": {"login": "yzhvictor"}, "path": "tools/unsupervised_dataset/Crawler/Crawler.py", "diffHunk": "@@ -0,0 +1,101 @@\n+import requests\n+import queue\n+import CQNode\n+import Extractor\n+import CrawlerLog\n+\n+class Crawler:\n+    \"\"\" Contains the functions and logic to run and coordinate the crawling process. Given\n+        initial starting URLs and maximum size, the crawler will explore websites to look\n+        for SQL queries.\n+    \"\"\"\n+    \n+    def __init__(self, links, maxDepth=3, maxSize=500):\n+        \"\"\" Initializes the crawler and instance variables.\n+        \n+        Args:\n+            links: The root URLs to begin crawling from. Can be one or more.\n+            maxDepth: The maximum depth for the crawler to explore.\n+            maxSize: THe maximum number of links the crawler should explore.\n+    \n+        \"\"\"\n+        self.linkQueue = queue.Queue()\n+        self.seen = set([])\n+        self.maxDepth = maxDepth\n+        self.maxSize = maxSize\n+        self.log = CrawlerLog.CrawlerLog()\n+        self.count = 0\n+        \n+        for link in links:\n+            self.linkQueue.put(CQNode.CQNode(link, 0))\n+            self.seen.add(link)\n+        \n+    def crawl(self):\n+        \"\"\" Begins the crawling process using variables set earlier. Extracts\n+            queries by locating website-specific HTML tags or searching for\n+            common expression patterns. Writes queries to output after finishing\n+            each site.\n+        \"\"\"\n+        \n+        while (self.linkQueue.empty() == False):\n+            # Retrieve the next link in the queue\n+            nextNode = self.linkQueue.get()\n+            nodeURL = nextNode.getURL()\n+            nodeDepth = nextNode.getDepth()\n+            \n+            # Check if crawler has exceeded maximum depth or maximum count\n+            if nodeDepth >= self.maxDepth or self.count >= self.maxSize:\n+                print(\"Crawled {0} websites.\".format(self.count))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52750e8f8330016a4c9c45310413c77ab665413f"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NjM1NTM0OnYy", "diffSide": "RIGHT", "path": "tools/unsupervised_dataset/Crawler/CrawlerLog.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwNTo0MDoxNVrOGnaATQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwNTo0MDoxNVrOGnaATQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk3MzcwOQ==", "bodyText": "Instead of creating customized log mechanism, why not use python's logging library to log messages?", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#discussion_r443973709", "createdAt": "2020-06-23T05:40:15Z", "author": {"login": "yzhvictor"}, "path": "tools/unsupervised_dataset/Crawler/CrawlerLog.py", "diffHunk": "@@ -0,0 +1,71 @@\n+import datetime\n+import csv\n+import os\n+\n+class CrawlerLog:\n+    \"\"\" The CrawlerLog keeps track of which websites were explored, how many queries were found,\n+        and creates a CSV with all the queries. It also logs any errors encountered. The log is\n+        saved into Logs subdirectory with name based on start time. Queries are saved into Queries\n+        subdirectory.\n+    \"\"\"\n+    \n+    def __init__(self):\n+        \"\"\" Initializes crawler log to keep track of crawler progress and instantiates\n+            instance variables.\n+        \"\"\"\n+        \n+        self.startTime = datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M\")\n+        \n+        # Create directory for logs if it does not already exists\n+        try:\n+            os.mkdir(\"Logs\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52750e8f8330016a4c9c45310413c77ab665413f"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NjM2MzMzOnYy", "diffSide": "RIGHT", "path": "tools/unsupervised_dataset/Crawler/CrawlerLog.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwNTo0NDoyNFrOGnaFPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwNTo0NDoyNFrOGnaFPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk3NDk3Mw==", "bodyText": "I think we need to handle this case more carefully. Imagine people mistakenly run the tool with an old path, it may unexpectedly overwrite some existing crawled data?", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#discussion_r443974973", "createdAt": "2020-06-23T05:44:24Z", "author": {"login": "yzhvictor"}, "path": "tools/unsupervised_dataset/Crawler/CrawlerLog.py", "diffHunk": "@@ -0,0 +1,71 @@\n+import datetime\n+import csv\n+import os\n+\n+class CrawlerLog:\n+    \"\"\" The CrawlerLog keeps track of which websites were explored, how many queries were found,\n+        and creates a CSV with all the queries. It also logs any errors encountered. The log is\n+        saved into Logs subdirectory with name based on start time. Queries are saved into Queries\n+        subdirectory.\n+    \"\"\"\n+    \n+    def __init__(self):\n+        \"\"\" Initializes crawler log to keep track of crawler progress and instantiates\n+            instance variables.\n+        \"\"\"\n+        \n+        self.startTime = datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M\")\n+        \n+        # Create directory for logs if it does not already exists\n+        try:\n+            os.mkdir(\"Logs\")\n+        except FileExistsError as e:\n+            pass\n+        \n+        self.log = open(\"Logs/log-\" + self.startTime + \".txt\", \"a\")\n+        self.log.write(\"Beginning crawl at time {0}.\\n\".format(self.startTime))\n+        \n+        try:\n+            os.mkdir(\"Queries\")\n+        except FileExistsError as e:\n+            pass", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52750e8f8330016a4c9c45310413c77ab665413f"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NjM2ODU5OnYy", "diffSide": "RIGHT", "path": "tools/unsupervised_dataset/Crawler/CQNode.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwNTo0NzowM1rOGnaIbg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QyMDowNjo0OFrOGn4sew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk3NTc5MA==", "bodyText": "In general, it's good to have tests covering the actual logic. So maybe you can consider adding some corresponding tests for each source files.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#discussion_r443975790", "createdAt": "2020-06-23T05:47:03Z", "author": {"login": "yzhvictor"}, "path": "tools/unsupervised_dataset/Crawler/CQNode.py", "diffHunk": "@@ -0,0 +1,32 @@\n+class CQNode:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52750e8f8330016a4c9c45310413c77ab665413f"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ3NjUzOQ==", "bodyText": "+1 Please add tests0\nAssume pylint has been run on this?\nAlso do we already have some format checker integrated, it is easier to have people's code more consistent.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#discussion_r444476539", "createdAt": "2020-06-23T20:06:48Z", "author": {"login": "kikkyo"}, "path": "tools/unsupervised_dataset/Crawler/CQNode.py", "diffHunk": "@@ -0,0 +1,32 @@\n+class CQNode:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk3NTc5MA=="}, "originalCommit": {"oid": "52750e8f8330016a4c9c45310413c77ab665413f"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2OTQxODM0OnYy", "diffSide": "RIGHT", "path": "tools/unsupervised_dataset/Crawler/CrawlerLog.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxOTo1MzozNlrOGn4R4g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxOTo1MzozNlrOGn4R4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ2OTczMA==", "bodyText": "I am not expert in Python, is this recommended practice?\nI think we usually check for existence before making directories, instead of ignoring exceptions?", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#discussion_r444469730", "createdAt": "2020-06-23T19:53:36Z", "author": {"login": "kikkyo"}, "path": "tools/unsupervised_dataset/Crawler/CrawlerLog.py", "diffHunk": "@@ -0,0 +1,71 @@\n+import datetime\n+import csv\n+import os\n+\n+class CrawlerLog:\n+    \"\"\" The CrawlerLog keeps track of which websites were explored, how many queries were found,\n+        and creates a CSV with all the queries. It also logs any errors encountered. The log is\n+        saved into Logs subdirectory with name based on start time. Queries are saved into Queries\n+        subdirectory.\n+    \"\"\"\n+    \n+    def __init__(self):\n+        \"\"\" Initializes crawler log to keep track of crawler progress and instantiates\n+            instance variables.\n+        \"\"\"\n+        \n+        self.startTime = datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M\")\n+        \n+        # Create directory for logs if it does not already exists\n+        try:\n+            os.mkdir(\"Logs\")\n+        except FileExistsError as e:\n+            pass", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52750e8f8330016a4c9c45310413c77ab665413f"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2OTQxOTE4OnYy", "diffSide": "RIGHT", "path": "tools/unsupervised_dataset/Crawler/CrawlerLog.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxOTo1Mzo1NFrOGn4ScQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxOTo1Mzo1NFrOGn4ScQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ2OTg3Mw==", "bodyText": "same here", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#discussion_r444469873", "createdAt": "2020-06-23T19:53:54Z", "author": {"login": "kikkyo"}, "path": "tools/unsupervised_dataset/Crawler/CrawlerLog.py", "diffHunk": "@@ -0,0 +1,71 @@\n+import datetime\n+import csv\n+import os\n+\n+class CrawlerLog:\n+    \"\"\" The CrawlerLog keeps track of which websites were explored, how many queries were found,\n+        and creates a CSV with all the queries. It also logs any errors encountered. The log is\n+        saved into Logs subdirectory with name based on start time. Queries are saved into Queries\n+        subdirectory.\n+    \"\"\"\n+    \n+    def __init__(self):\n+        \"\"\" Initializes crawler log to keep track of crawler progress and instantiates\n+            instance variables.\n+        \"\"\"\n+        \n+        self.startTime = datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M\")\n+        \n+        # Create directory for logs if it does not already exists\n+        try:\n+            os.mkdir(\"Logs\")\n+        except FileExistsError as e:\n+            pass\n+        \n+        self.log = open(\"Logs/log-\" + self.startTime + \".txt\", \"a\")\n+        self.log.write(\"Beginning crawl at time {0}.\\n\".format(self.startTime))\n+        \n+        try:\n+            os.mkdir(\"Queries\")\n+        except FileExistsError as e:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52750e8f8330016a4c9c45310413c77ab665413f"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2OTQ0MjAxOnYy", "diffSide": "RIGHT", "path": "tools/unsupervised_dataset/Crawler/run_crawler.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QyMDowMDo0OFrOGn4hPg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QyMDowMDo0OFrOGn4hPg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ3MzY2Mg==", "bodyText": "nit: seems style guide recommends us using: # TODO(kl@gmail.com): Use a \"*\" here for string repetition.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#discussion_r444473662", "createdAt": "2020-06-23T20:00:48Z", "author": {"login": "kikkyo"}, "path": "tools/unsupervised_dataset/Crawler/run_crawler.py", "diffHunk": "@@ -0,0 +1,16 @@\n+import requests\n+import sys\n+import Crawler\n+\n+# Initializes a crawler and starts the crawling process using command line arguments\n+def start_crawler():\n+    # TO-DO: Allow user to optionally define max size or max depth", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52750e8f8330016a4c9c45310413c77ab665413f"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2OTQ0ODQ3OnYy", "diffSide": "RIGHT", "path": "tools/unsupervised_dataset/Crawler/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QyMDowMzowNlrOGn4lhg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QyMDowMzowNlrOGn4lhg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ3NDc1OA==", "bodyText": "nit: add a comment on \"URL should be  separated by comma or space/something else\" ?", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#discussion_r444474758", "createdAt": "2020-06-23T20:03:06Z", "author": {"login": "kikkyo"}, "path": "tools/unsupervised_dataset/Crawler/README.md", "diffHunk": "@@ -0,0 +1,11 @@\n+# SQL Crawler\n+\n+This directory contains the code to run a universal, unsupervised SQL web crawler. The user provides a starting target URL from which to begin crawling, and has the option to set the maximum depth or size of the crawler.\n+\n+## Usage\n+To run the crawler, run the following command:\n+\n+```\n+python3 run_crawler.py <starting URLs>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52750e8f8330016a4c9c45310413c77ab665413f"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3NDc2MTgxOnYy", "diffSide": "RIGHT", "path": "tools/unsupervised_dataset/SQLCrawler/CrawlerLog.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQwNjowMToxMlrOGoscvw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQwNjowMToxMlrOGoscvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTMyNDQ3OQ==", "bodyText": "just a second thought on the output file format (not necessarily to be resolved in the cl:)), if we make it as CSV, later would it be problematic to read the query for later processing? For example, if a multi-line query is not well formatted.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#discussion_r445324479", "createdAt": "2020-06-25T06:01:12Z", "author": {"login": "kikkyo"}, "path": "tools/unsupervised_dataset/SQLCrawler/CrawlerLog.py", "diffHunk": "@@ -0,0 +1,76 @@\n+import datetime\n+import csv\n+import os\n+import logging\n+import pathlib\n+\n+class CrawlerLog(object):\n+    \"\"\" Logs the status of the SQL crawler, including websites and queries.\n+        \n+        The CrawlerLog keeps track of which websites were explored, how many\n+        queries were found, and creates a CSV with all the queries. It also", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cb6e33705d656dfaace43675e9d5f29521dd5fc3"}, "originalPosition": 11}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2957, "cost": 1, "resetAt": "2021-11-12T19:05:54Z"}}}