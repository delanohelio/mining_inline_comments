{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE5NDY5MTMy", "number": 190, "reviewThreads": {"totalCount": 18, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQyMzo0NToxN1rOE36u7A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QyMjo0ODozM1rOE6RIng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3MDY5NDIwOnYy", "diffSide": "RIGHT", "path": "tools/cloud_functions/gcs_event_based_ingest/tests/cli/test_backfill.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQyMzo0NToxN1rOHxijWw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQyMzo0NToxN1rOHxijWw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTcwODM3OQ==", "bodyText": "these fixtures are all duplicated from test_gcs_ocn_bq_ingest_it.py should DRY them up into a common fixtures module.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r521708379", "createdAt": "2020-11-11T23:45:17Z", "author": {"login": "jaketf"}, "path": "tools/cloud_functions/gcs_event_based_ingest/tests/cli/test_backfill.py", "diffHunk": "@@ -0,0 +1,257 @@\n+# Copyright 2020 Google LLC\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"integrtion tests for gcs_ocn_bq_ingest\"\"\"\n+import json\n+import os\n+import uuid\n+from time import monotonic\n+from typing import List\n+\n+import backfill\n+import google.cloud.storage as storage\n+import pytest\n+from google.cloud import bigquery\n+\n+from gcs_ocn_bq_ingest import main\n+\n+TEST_DIR = os.path.realpath(os.path.dirname(__file__) + \"/..\")\n+LOAD_JOB_POLLING_TIMEOUT = 20  # seconds\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def bq() -> bigquery.Client:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f617f91e39dc19c380a5741d7e92afa2689a4629"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3NTE4NDYyOnYy", "diffSide": "RIGHT", "path": "tools/cloud_functions/gcs_event_based_ingest/backfill.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMDo0MDoyNVrOHyNiOA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMDo0MDoyNVrOHyNiOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQxMjYwMA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    \"--success_filename\",\n          \n          \n            \n                    \"--success-filename\",\n          \n      \n    \n    \n  \n\nThis is the common convention.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r522412600", "createdAt": "2020-11-12T20:40:25Z", "author": {"login": "mik-laj"}, "path": "tools/cloud_functions/gcs_event_based_ingest/backfill.py", "diffHunk": "@@ -0,0 +1,162 @@\n+# Copyright 2020 Google LLC\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\" Command Line utility for backfilling gcs_ocn_bq_ingest cloud function\n+\"\"\"\n+import concurrent.futures\n+import logging\n+import os\n+import pprint\n+import sys\n+from argparse import ArgumentParser, Namespace\n+from typing import Dict, Iterator, List\n+\n+from google.api_core.client_info import ClientInfo\n+from google.cloud import storage\n+\n+import gcs_ocn_bq_ingest  # pylint: disable=import-error\n+\n+CLIENT_INFO = ClientInfo(user_agent=\"google-pso-tool/bq-severless-loader\")\n+\n+os.environ[\"FUNCTION_NAME\"] = \"backfill-cli\"\n+\n+\n+def find_blobs_with_suffix(\n+    gcs_cli: storage.Client,\n+    prefix: str,\n+    suffix: str = \"_SUCCESS\",\n+) -> Iterator[storage.Blob]:\n+    \"\"\"\n+    Find GCS blobs with a given suffix.\n+\n+    :param gcs_cli:  storage.Client\n+    :param prefix: A GCS prefix to search i.e. gs://bucket/prefix/to/search\n+    :param suffix: A suffix in blob name to match\n+    :return:  Iterable of blobs matching the suffix.\n+    \"\"\"\n+    bucket_name, prefix = gcs_ocn_bq_ingest.main.parse_gcs_url(prefix)  # noqa\n+    bucket: storage.Bucket = gcs_cli.lookup_bucket(bucket_name)\n+    # filter passes on scalability / laziness advantages of iterator.\n+    return filter(lambda blob: blob.name.endswith(suffix),\n+                  bucket.list_blobs(prefix=prefix))\n+\n+\n+def main(args: Namespace):\n+    \"\"\"main entry point for backfill CLI.\"\"\"\n+    gcs_cli: storage.Client = storage.Client(client_info=CLIENT_INFO)\n+    ps_cli = None\n+    suffix = args.success_filename\n+    if args.mode == \"NOTIFICATIONS\":\n+        if not args.pubsub_topic:\n+            raise ValueError(\"when passing mode=NOTIFICATIONS\"\n+                             \"you must also pass pubsub_topic.\")\n+        # import is here because this utility can be used without\n+        # google-cloud-pubsub dependency in LOCAL mode.\n+        # pylint: disable=import-outside-toplevel\n+        from google.cloud import pubsub\n+        ps_cli = pubsub.PublisherClient()\n+\n+    # These are all I/O bound tasks so use Thread Pool concurrency for speed.\n+    with concurrent.futures.ThreadPoolExecutor() as executor:\n+        future_to_gsurl = {}\n+        for blob in find_blobs_with_suffix(gcs_cli, args.gcs_path, suffix):\n+            if ps_cli:\n+                # kwargs are message attributes\n+                # https://googleapis.dev/python/pubsub/latest/publisher/index.html#publish-a-message\n+                logging.info(\"sending pubsub message for: %s\",\n+                             f\"gs://{blob.bucket.name}/{blob.name}\")\n+                future_to_gsurl[executor.submit(\n+                    ps_cli.publish,\n+                    args.pubsub_topic,\n+                    b'',    # cloud function ignores message body\n+                    bucketId=blob.bucket.name,\n+                    objectId=blob.name,\n+                    _metaInfo=\"this message was submitted with \"\n+                    \"gcs_ocn_bq_ingest backfill.py utility\"\n+                )] = f\"gs://{blob.bucket.name}/{blob.name}\"\n+            else:\n+                logging.info(\"running  cloud function locally for: %s\",\n+                             f\"gs://{blob.bucket.name}/{blob.name}\")\n+                future_to_gsurl[executor.submit(\n+                    gcs_ocn_bq_ingest.main.main,\n+                    {\n+                        \"attributes\": {\n+                            \"bucketId\": blob.bucket.name,\n+                            \"objectId\": blob.name\n+                        }\n+                    },\n+                    None,\n+                )] = f\"gs://{blob.bucket.name}/{blob.name}\"\n+        exceptions: Dict[str, Exception] = dict()\n+        for future in concurrent.futures.as_completed(future_to_gsurl):\n+            gsurl = future_to_gsurl[future]\n+            try:\n+                future.result()\n+            except Exception as err:  # pylint: disable=broad-except\n+                logging.error(\"Error processing %s: %s\", gsurl, err)\n+                exceptions[gsurl] = err\n+        if exceptions:\n+            raise RuntimeError(\"The following errors were encountered:\\n\" +\n+                               pprint.pformat(exceptions))\n+\n+\n+def parse_args(args: List[str]) -> Namespace:\n+    \"\"\"argument parser for backfill CLI\"\"\"\n+    parser = ArgumentParser(\n+        description=\"utility to backfill success file notifications \"\n+                    \"or run the cloud function locally in concurrent threads.\")\n+\n+    parser.add_argument(\n+        \"--gcs_path\",\n+        \"-p\",\n+        help=\"GCS path (e.g. gs://bucket/prefix/to/search/)to search for \"\n+        \"existing _SUCCESS files\",\n+        required=True,\n+    )\n+\n+    parser.add_argument(\n+        \"--mode\",\n+        \"-m\",\n+        help=\"How to perform the backfill: LOCAL run cloud function main\"\n+        \" method locally (in concurrent threads) or NOTIFICATIONS just push\"\n+        \" notifications to Pub/Sub for a deployed version of the cloud function\"\n+        \" to pick up. Default is NOTIFICATIONS.\",\n+        required=False,\n+        type=str.upper,\n+        choices=[\"LOCAL\", \"NOTIFICATIONS\"],\n+        default=\"NOTIFICATIONS\",\n+    )\n+\n+    parser.add_argument(\n+        \"--pubsub_topic\",\n+        \"--topic\",\n+        \"-t\",\n+        help=\"Pub/Sub notifications topic to post notifications for. \"\n+        \"i.e. projects/{PROJECT_ID}/topics/{TOPIC_ID} \"\n+        \"Required if using NOTIFICATIONS mode.\",\n+        required=False,\n+        default=None,\n+    )\n+\n+    parser.add_argument(\n+        \"--success_filename\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "33ff20192980fc1d0bfd999ce920a36cba0382c4"}, "originalPosition": 152}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3NTE4NTA4OnYy", "diffSide": "RIGHT", "path": "tools/cloud_functions/gcs_event_based_ingest/backfill.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMDo0MDozNFrOHyNifQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMDo0MDozNFrOHyNifQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQxMjY2OQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    \"--pubsub_topic\",\n          \n          \n            \n                    \"--pubsub-topic\",", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r522412669", "createdAt": "2020-11-12T20:40:34Z", "author": {"login": "mik-laj"}, "path": "tools/cloud_functions/gcs_event_based_ingest/backfill.py", "diffHunk": "@@ -0,0 +1,162 @@\n+# Copyright 2020 Google LLC\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\" Command Line utility for backfilling gcs_ocn_bq_ingest cloud function\n+\"\"\"\n+import concurrent.futures\n+import logging\n+import os\n+import pprint\n+import sys\n+from argparse import ArgumentParser, Namespace\n+from typing import Dict, Iterator, List\n+\n+from google.api_core.client_info import ClientInfo\n+from google.cloud import storage\n+\n+import gcs_ocn_bq_ingest  # pylint: disable=import-error\n+\n+CLIENT_INFO = ClientInfo(user_agent=\"google-pso-tool/bq-severless-loader\")\n+\n+os.environ[\"FUNCTION_NAME\"] = \"backfill-cli\"\n+\n+\n+def find_blobs_with_suffix(\n+    gcs_cli: storage.Client,\n+    prefix: str,\n+    suffix: str = \"_SUCCESS\",\n+) -> Iterator[storage.Blob]:\n+    \"\"\"\n+    Find GCS blobs with a given suffix.\n+\n+    :param gcs_cli:  storage.Client\n+    :param prefix: A GCS prefix to search i.e. gs://bucket/prefix/to/search\n+    :param suffix: A suffix in blob name to match\n+    :return:  Iterable of blobs matching the suffix.\n+    \"\"\"\n+    bucket_name, prefix = gcs_ocn_bq_ingest.main.parse_gcs_url(prefix)  # noqa\n+    bucket: storage.Bucket = gcs_cli.lookup_bucket(bucket_name)\n+    # filter passes on scalability / laziness advantages of iterator.\n+    return filter(lambda blob: blob.name.endswith(suffix),\n+                  bucket.list_blobs(prefix=prefix))\n+\n+\n+def main(args: Namespace):\n+    \"\"\"main entry point for backfill CLI.\"\"\"\n+    gcs_cli: storage.Client = storage.Client(client_info=CLIENT_INFO)\n+    ps_cli = None\n+    suffix = args.success_filename\n+    if args.mode == \"NOTIFICATIONS\":\n+        if not args.pubsub_topic:\n+            raise ValueError(\"when passing mode=NOTIFICATIONS\"\n+                             \"you must also pass pubsub_topic.\")\n+        # import is here because this utility can be used without\n+        # google-cloud-pubsub dependency in LOCAL mode.\n+        # pylint: disable=import-outside-toplevel\n+        from google.cloud import pubsub\n+        ps_cli = pubsub.PublisherClient()\n+\n+    # These are all I/O bound tasks so use Thread Pool concurrency for speed.\n+    with concurrent.futures.ThreadPoolExecutor() as executor:\n+        future_to_gsurl = {}\n+        for blob in find_blobs_with_suffix(gcs_cli, args.gcs_path, suffix):\n+            if ps_cli:\n+                # kwargs are message attributes\n+                # https://googleapis.dev/python/pubsub/latest/publisher/index.html#publish-a-message\n+                logging.info(\"sending pubsub message for: %s\",\n+                             f\"gs://{blob.bucket.name}/{blob.name}\")\n+                future_to_gsurl[executor.submit(\n+                    ps_cli.publish,\n+                    args.pubsub_topic,\n+                    b'',    # cloud function ignores message body\n+                    bucketId=blob.bucket.name,\n+                    objectId=blob.name,\n+                    _metaInfo=\"this message was submitted with \"\n+                    \"gcs_ocn_bq_ingest backfill.py utility\"\n+                )] = f\"gs://{blob.bucket.name}/{blob.name}\"\n+            else:\n+                logging.info(\"running  cloud function locally for: %s\",\n+                             f\"gs://{blob.bucket.name}/{blob.name}\")\n+                future_to_gsurl[executor.submit(\n+                    gcs_ocn_bq_ingest.main.main,\n+                    {\n+                        \"attributes\": {\n+                            \"bucketId\": blob.bucket.name,\n+                            \"objectId\": blob.name\n+                        }\n+                    },\n+                    None,\n+                )] = f\"gs://{blob.bucket.name}/{blob.name}\"\n+        exceptions: Dict[str, Exception] = dict()\n+        for future in concurrent.futures.as_completed(future_to_gsurl):\n+            gsurl = future_to_gsurl[future]\n+            try:\n+                future.result()\n+            except Exception as err:  # pylint: disable=broad-except\n+                logging.error(\"Error processing %s: %s\", gsurl, err)\n+                exceptions[gsurl] = err\n+        if exceptions:\n+            raise RuntimeError(\"The following errors were encountered:\\n\" +\n+                               pprint.pformat(exceptions))\n+\n+\n+def parse_args(args: List[str]) -> Namespace:\n+    \"\"\"argument parser for backfill CLI\"\"\"\n+    parser = ArgumentParser(\n+        description=\"utility to backfill success file notifications \"\n+                    \"or run the cloud function locally in concurrent threads.\")\n+\n+    parser.add_argument(\n+        \"--gcs_path\",\n+        \"-p\",\n+        help=\"GCS path (e.g. gs://bucket/prefix/to/search/)to search for \"\n+        \"existing _SUCCESS files\",\n+        required=True,\n+    )\n+\n+    parser.add_argument(\n+        \"--mode\",\n+        \"-m\",\n+        help=\"How to perform the backfill: LOCAL run cloud function main\"\n+        \" method locally (in concurrent threads) or NOTIFICATIONS just push\"\n+        \" notifications to Pub/Sub for a deployed version of the cloud function\"\n+        \" to pick up. Default is NOTIFICATIONS.\",\n+        required=False,\n+        type=str.upper,\n+        choices=[\"LOCAL\", \"NOTIFICATIONS\"],\n+        default=\"NOTIFICATIONS\",\n+    )\n+\n+    parser.add_argument(\n+        \"--pubsub_topic\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "33ff20192980fc1d0bfd999ce920a36cba0382c4"}, "originalPosition": 141}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3NTE4NTI1OnYy", "diffSide": "RIGHT", "path": "tools/cloud_functions/gcs_event_based_ingest/backfill.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMDo0MDo0MVrOHyNipg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMDo0MDo0MVrOHyNipg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQxMjcxMA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    \"--gcs_path\",\n          \n          \n            \n                    \"--gcs-path\",", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r522412710", "createdAt": "2020-11-12T20:40:41Z", "author": {"login": "mik-laj"}, "path": "tools/cloud_functions/gcs_event_based_ingest/backfill.py", "diffHunk": "@@ -0,0 +1,162 @@\n+# Copyright 2020 Google LLC\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\" Command Line utility for backfilling gcs_ocn_bq_ingest cloud function\n+\"\"\"\n+import concurrent.futures\n+import logging\n+import os\n+import pprint\n+import sys\n+from argparse import ArgumentParser, Namespace\n+from typing import Dict, Iterator, List\n+\n+from google.api_core.client_info import ClientInfo\n+from google.cloud import storage\n+\n+import gcs_ocn_bq_ingest  # pylint: disable=import-error\n+\n+CLIENT_INFO = ClientInfo(user_agent=\"google-pso-tool/bq-severless-loader\")\n+\n+os.environ[\"FUNCTION_NAME\"] = \"backfill-cli\"\n+\n+\n+def find_blobs_with_suffix(\n+    gcs_cli: storage.Client,\n+    prefix: str,\n+    suffix: str = \"_SUCCESS\",\n+) -> Iterator[storage.Blob]:\n+    \"\"\"\n+    Find GCS blobs with a given suffix.\n+\n+    :param gcs_cli:  storage.Client\n+    :param prefix: A GCS prefix to search i.e. gs://bucket/prefix/to/search\n+    :param suffix: A suffix in blob name to match\n+    :return:  Iterable of blobs matching the suffix.\n+    \"\"\"\n+    bucket_name, prefix = gcs_ocn_bq_ingest.main.parse_gcs_url(prefix)  # noqa\n+    bucket: storage.Bucket = gcs_cli.lookup_bucket(bucket_name)\n+    # filter passes on scalability / laziness advantages of iterator.\n+    return filter(lambda blob: blob.name.endswith(suffix),\n+                  bucket.list_blobs(prefix=prefix))\n+\n+\n+def main(args: Namespace):\n+    \"\"\"main entry point for backfill CLI.\"\"\"\n+    gcs_cli: storage.Client = storage.Client(client_info=CLIENT_INFO)\n+    ps_cli = None\n+    suffix = args.success_filename\n+    if args.mode == \"NOTIFICATIONS\":\n+        if not args.pubsub_topic:\n+            raise ValueError(\"when passing mode=NOTIFICATIONS\"\n+                             \"you must also pass pubsub_topic.\")\n+        # import is here because this utility can be used without\n+        # google-cloud-pubsub dependency in LOCAL mode.\n+        # pylint: disable=import-outside-toplevel\n+        from google.cloud import pubsub\n+        ps_cli = pubsub.PublisherClient()\n+\n+    # These are all I/O bound tasks so use Thread Pool concurrency for speed.\n+    with concurrent.futures.ThreadPoolExecutor() as executor:\n+        future_to_gsurl = {}\n+        for blob in find_blobs_with_suffix(gcs_cli, args.gcs_path, suffix):\n+            if ps_cli:\n+                # kwargs are message attributes\n+                # https://googleapis.dev/python/pubsub/latest/publisher/index.html#publish-a-message\n+                logging.info(\"sending pubsub message for: %s\",\n+                             f\"gs://{blob.bucket.name}/{blob.name}\")\n+                future_to_gsurl[executor.submit(\n+                    ps_cli.publish,\n+                    args.pubsub_topic,\n+                    b'',    # cloud function ignores message body\n+                    bucketId=blob.bucket.name,\n+                    objectId=blob.name,\n+                    _metaInfo=\"this message was submitted with \"\n+                    \"gcs_ocn_bq_ingest backfill.py utility\"\n+                )] = f\"gs://{blob.bucket.name}/{blob.name}\"\n+            else:\n+                logging.info(\"running  cloud function locally for: %s\",\n+                             f\"gs://{blob.bucket.name}/{blob.name}\")\n+                future_to_gsurl[executor.submit(\n+                    gcs_ocn_bq_ingest.main.main,\n+                    {\n+                        \"attributes\": {\n+                            \"bucketId\": blob.bucket.name,\n+                            \"objectId\": blob.name\n+                        }\n+                    },\n+                    None,\n+                )] = f\"gs://{blob.bucket.name}/{blob.name}\"\n+        exceptions: Dict[str, Exception] = dict()\n+        for future in concurrent.futures.as_completed(future_to_gsurl):\n+            gsurl = future_to_gsurl[future]\n+            try:\n+                future.result()\n+            except Exception as err:  # pylint: disable=broad-except\n+                logging.error(\"Error processing %s: %s\", gsurl, err)\n+                exceptions[gsurl] = err\n+        if exceptions:\n+            raise RuntimeError(\"The following errors were encountered:\\n\" +\n+                               pprint.pformat(exceptions))\n+\n+\n+def parse_args(args: List[str]) -> Namespace:\n+    \"\"\"argument parser for backfill CLI\"\"\"\n+    parser = ArgumentParser(\n+        description=\"utility to backfill success file notifications \"\n+                    \"or run the cloud function locally in concurrent threads.\")\n+\n+    parser.add_argument(\n+        \"--gcs_path\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "33ff20192980fc1d0bfd999ce920a36cba0382c4"}, "originalPosition": 120}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5MDc4ODY1OnYy", "diffSide": "RIGHT", "path": "tools/cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwMzoyNTowMFrOH0i8pg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwMzoyNTowMFrOH0i8pg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg2MDU4Mg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                r\"(?P<partition>\\$[0-9]+)?/?\"    # partition decortator (optional)\n          \n          \n            \n                r\"(?P<partition>\\$[0-9]+)?/?\"    # partition decorator (optional)", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r524860582", "createdAt": "2020-11-17T03:25:00Z", "author": {"login": "danieldeleo"}, "path": "tools/cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -52,10 +52,18 @@\n     \"labels\": DEFAULT_JOB_LABELS,\n }\n \n-DEFAULT_DESTINATION_REGEX = r\"(?P<dataset>[\\w\\-_0-9]+)/\" \\\n-                            r\"(?P<table>[\\w\\-_0-9]+)/\" \\\n-                            r\"?(?P<partition>\\$[0-9]+)?/\" \\\n-                            r\"?(?P<batch>[\\w\\-_0-9]+)?/\"\n+# yapf: disable\n+DEFAULT_DESTINATION_REGEX = (\n+    r\"^(?P<dataset>[\\w\\-\\._0-9]+)/\"  # dataset (required)\n+    r\"(?P<table>[\\w\\-_0-9]+)/?\"      # table name (required)\n+    r\"(?P<partition>\\$[0-9]+)?/?\"    # partition decortator (optional)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9e3e963fba00e1757c2dc8b7cf212565bab582d3"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5MDc5MDY1OnYy", "diffSide": "RIGHT", "path": "tools/cloud_functions/gcs_event_based_ingest/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwMzoyNjowM1rOH0i9vg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwMzoyNjowM1rOH0i9vg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg2MDg2Mg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            an hourly partition might have mutliple directories that upload to it.\n          \n          \n            \n            an hourly partition might have multiple directories that upload to it.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r524860862", "createdAt": "2020-11-17T03:26:03Z", "author": {"login": "danieldeleo"}, "path": "tools/cloud_functions/gcs_event_based_ingest/README.md", "diffHunk": "@@ -16,6 +16,77 @@ like so:\n Note, the table prefix can contain multiple sub-prefixes for handling partitions\n or for configuring historical / incremental loads differently.\n \n+### Configurable Naming Convention with Regex\n+By Default we try to read dataset, table, partition (or yyyy/mm/dd/hh) and\n+batch id using the following python regex:\n+```python3\n+DEFAULT_DESTINATION_REGEX = (\n+    r\"^(?P<dataset>[\\w\\-\\._0-9]+)/\"  # dataset (required)\n+    r\"(?P<table>[\\w\\-_0-9]+)/?\"      # table name (required)\n+    r\"(?P<partition>\\$[0-9]+)?/?\"    # partition decortator (optional)\n+    r\"(?P<yyyy>[0-9]{4})?/?\"         # partition year (yyyy) (optional)\n+    r\"(?P<mm>[0-9]{2})?/?\"           # partition month (mm) (optional)\n+    r\"(?P<dd>[0-9]{2})?/?\"           # partition day (dd)  (optional)\n+    r\"(?P<hh>[0-9]{2})?/?\"           # partition hour (hh) (optional)\n+    r\"(?P<batch>[\\w\\-_0-9]+)?/\"      # batch id (optional)\n+)\n+```\n+you can see if this meets your needs in this [regex playground](https://regex101.com/r/5Y9TDh/2)\n+Otherwise you can override the regex by setting the `DESTINATION_REGEX` to\n+better fit your naming convention on GCS. Your regex must include\n+[Python Regex with named capturing groups](https://docs.python.org/3/howto/regex.html#non-capturing-and-named-groups)\n+for destination `dataset`, and `table`.\n+Note, that `dataset` can optionally, explicitly specify destination project\n+(i.e. `gs://${BUCKET}/project_id.dataset_id/table/....`) otherwise the default\n+project will be inferred from Application Default Credential (the project in\n+which the Cloud Function is running, or the ADC configured in Google Cloud SDK\n+if invoked locally). This is useful in scenarios where a single deployment of\n+the Cloud Function is responsible for ingesting data into BigQuery tables in\n+projects other than the one it is deployed in. In these cases it is crucial to\n+ensure the service account that Cloud Functions is impersonating has the correct\n+permissions on all destination projects.\n+\n+Your regex can optionally include  for \n+- `partition` must be BigQuery Partition decorator with leading `$`\n+- `yyyy`, `mm`, `dd`, `hr` partition year, month, day, and hour\n+(depending on your partition granularity)\n+- `batch` an optional batch id to indicate multiple uploads for this partition.\n+\n+For example, if your datafiles were laid out like this:\n+```text\n+gs://${BUCKET}/${SOURCE_SYSTEM}/${DATASET}/${TABLE}/region=${LOCATION}/yyyy=${YEAR}/mm=${MONTH}/dd=${DAY}/hh=${HOUR}\n+```\n+i.e.\n+```text\n+gs://my-bucket/on-prem-edw/my_product/transactions/region=US/yyyy=2020/mm=01/dd=02/hh=03/_SUCCESS\n+```\n+Then you could use [this regex](https://regex101.com/r/OLpmg4/2):\n+```text\n+DESTINATION_REGEX='(?:[\\w\\-_0-9]+)/(?P<dataset>[\\w\\-_0-9]+)/(?P<table>[\\w\\-_0-9]+)/region=(?P<batch>[\\w]+)/yyyy=(?P<yyyy>[0-9]{4})/mm=(?P<mm>[0-9]{2})/dd=(?P<dd>[0-9]{2})/hh=(?P<hh>[0-9]{2})/'\n+```\n+In this case we can take advantage of a more known rigid structure so our regex \n+is simpler (no optional capturing groups, optional slashes).\n+Note, we can use the `region=` string (which may have been partitioned on\n+in an  upstream system such as Hive) as a batch ID because we might expect that\n+an hourly partition might have mutliple directories that upload to it.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9e3e963fba00e1757c2dc8b7cf212565bab582d3"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5MDc5MjkwOnYy", "diffSide": "RIGHT", "path": "tools/cloud_functions/gcs_event_based_ingest/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwMzoyNzoyMFrOH0i_Eg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwMzoyNzoyMFrOH0i_Eg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg2MTIwMg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            In most cases, it would be recommeneded to have separate buckets / deployment \n          \n          \n            \n            In most cases, it would be recommended to have separate buckets / deployment", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r524861202", "createdAt": "2020-11-17T03:27:20Z", "author": {"login": "danieldeleo"}, "path": "tools/cloud_functions/gcs_event_based_ingest/README.md", "diffHunk": "@@ -16,6 +16,77 @@ like so:\n Note, the table prefix can contain multiple sub-prefixes for handling partitions\n or for configuring historical / incremental loads differently.\n \n+### Configurable Naming Convention with Regex\n+By Default we try to read dataset, table, partition (or yyyy/mm/dd/hh) and\n+batch id using the following python regex:\n+```python3\n+DEFAULT_DESTINATION_REGEX = (\n+    r\"^(?P<dataset>[\\w\\-\\._0-9]+)/\"  # dataset (required)\n+    r\"(?P<table>[\\w\\-_0-9]+)/?\"      # table name (required)\n+    r\"(?P<partition>\\$[0-9]+)?/?\"    # partition decortator (optional)\n+    r\"(?P<yyyy>[0-9]{4})?/?\"         # partition year (yyyy) (optional)\n+    r\"(?P<mm>[0-9]{2})?/?\"           # partition month (mm) (optional)\n+    r\"(?P<dd>[0-9]{2})?/?\"           # partition day (dd)  (optional)\n+    r\"(?P<hh>[0-9]{2})?/?\"           # partition hour (hh) (optional)\n+    r\"(?P<batch>[\\w\\-_0-9]+)?/\"      # batch id (optional)\n+)\n+```\n+you can see if this meets your needs in this [regex playground](https://regex101.com/r/5Y9TDh/2)\n+Otherwise you can override the regex by setting the `DESTINATION_REGEX` to\n+better fit your naming convention on GCS. Your regex must include\n+[Python Regex with named capturing groups](https://docs.python.org/3/howto/regex.html#non-capturing-and-named-groups)\n+for destination `dataset`, and `table`.\n+Note, that `dataset` can optionally, explicitly specify destination project\n+(i.e. `gs://${BUCKET}/project_id.dataset_id/table/....`) otherwise the default\n+project will be inferred from Application Default Credential (the project in\n+which the Cloud Function is running, or the ADC configured in Google Cloud SDK\n+if invoked locally). This is useful in scenarios where a single deployment of\n+the Cloud Function is responsible for ingesting data into BigQuery tables in\n+projects other than the one it is deployed in. In these cases it is crucial to\n+ensure the service account that Cloud Functions is impersonating has the correct\n+permissions on all destination projects.\n+\n+Your regex can optionally include  for \n+- `partition` must be BigQuery Partition decorator with leading `$`\n+- `yyyy`, `mm`, `dd`, `hr` partition year, month, day, and hour\n+(depending on your partition granularity)\n+- `batch` an optional batch id to indicate multiple uploads for this partition.\n+\n+For example, if your datafiles were laid out like this:\n+```text\n+gs://${BUCKET}/${SOURCE_SYSTEM}/${DATASET}/${TABLE}/region=${LOCATION}/yyyy=${YEAR}/mm=${MONTH}/dd=${DAY}/hh=${HOUR}\n+```\n+i.e.\n+```text\n+gs://my-bucket/on-prem-edw/my_product/transactions/region=US/yyyy=2020/mm=01/dd=02/hh=03/_SUCCESS\n+```\n+Then you could use [this regex](https://regex101.com/r/OLpmg4/2):\n+```text\n+DESTINATION_REGEX='(?:[\\w\\-_0-9]+)/(?P<dataset>[\\w\\-_0-9]+)/(?P<table>[\\w\\-_0-9]+)/region=(?P<batch>[\\w]+)/yyyy=(?P<yyyy>[0-9]{4})/mm=(?P<mm>[0-9]{2})/dd=(?P<dd>[0-9]{2})/hh=(?P<hh>[0-9]{2})/'\n+```\n+In this case we can take advantage of a more known rigid structure so our regex \n+is simpler (no optional capturing groups, optional slashes).\n+Note, we can use the `region=` string (which may have been partitioned on\n+in an  upstream system such as Hive) as a batch ID because we might expect that\n+an hourly partition might have mutliple directories that upload to it.\n+(e.g. US, GB, etc). Because it is all named capturing groups we don't have any\n+strict ordering restrictions about batch id appearing before / after partition\n+information.\n+\n+### Dealing with Different Naming Conventions in the Same Bucket\n+In most cases, it would be recommeneded to have separate buckets / deployment ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9e3e963fba00e1757c2dc8b7cf212565bab582d3"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5MDc5OTE3OnYy", "diffSide": "RIGHT", "path": "tools/cloud_functions/gcs_event_based_ingest/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwMzozMTowNFrOH0jCrA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwMzozMTowNFrOH0jCrA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg2MjEyNA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            All load or external query jobs will have a job id witha  prefix following this convention:\n          \n          \n            \n            All load or external query jobs will have a job id with a  prefix following this convention:", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r524862124", "createdAt": "2020-11-17T03:31:04Z", "author": {"login": "danieldeleo"}, "path": "tools/cloud_functions/gcs_event_based_ingest/README.md", "diffHunk": "@@ -135,16 +206,19 @@ If more granular data is needed about a particular job id\n ### Job Naming Convention\n All load or external query jobs will have a job id witha  prefix following this convention:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9e3e963fba00e1757c2dc8b7cf212565bab582d3"}, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5MDgwMTIxOnYy", "diffSide": "RIGHT", "path": "tools/cloud_functions/gcs_event_based_ingest/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwMzozMjowOVrOH0jDzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwMzozMjowOVrOH0jDzQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg2MjQxMw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            and the integrationt tests for the backfill CLI.\n          \n          \n            \n            and the integration tests for the backfill CLI.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r524862413", "createdAt": "2020-11-17T03:32:09Z", "author": {"login": "danieldeleo"}, "path": "tools/cloud_functions/gcs_event_based_ingest/README.md", "diffHunk": "@@ -225,6 +306,12 @@ Note that integration tests will spin up / tear down cloud resources that can\n incur a small cost. These resources will be spun up based on your Google Cloud SDK\n [Application Default Credentials](https://cloud.google.com/sdk/gcloud/reference/auth/application-default)\n \n+#### Pytest Fixtures\n+All Pytest fixtures are DRY-ed up into `tests/conftest.py`\n+This is mostly to share fixtures between the main integration test for the cloud function\n+and the integrationt tests for the backfill CLI.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9e3e963fba00e1757c2dc8b7cf212565bab582d3"}, "originalPosition": 126}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5MDgxMTE0OnYy", "diffSide": "RIGHT", "path": "tools/cloud_functions/gcs_event_based_ingest/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwMzozNzo0NlrOH0jJnA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwMzozNzo0NlrOH0jJnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg2MzkwMA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                    Override the default destination regex for determining BigQuerydestination based on information encoded in the GCS path of thesuccess file\n          \n          \n            \n                                    Override the default destination regex for determining BigQuery destination based on information encoded in the GCS path of the success file", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r524863900", "createdAt": "2020-11-17T03:37:46Z", "author": {"login": "danieldeleo"}, "path": "tools/cloud_functions/gcs_event_based_ingest/README.md", "diffHunk": "@@ -240,7 +327,85 @@ pytest -m IT\n ```\n \n ## Deployment\n-It is suggested to deploy this Cloud Function with the [accompanying terraform module](terraform_module/gcs_ocn_bq_ingest_function/README.md)\n+It is suggested to deploy this Cloud Function with the\n+[accompanying terraform module](terraform_module/gcs_ocn_bq_ingest_function/README.md)\n+\n+### Google Cloud SDK \n+Alternatively, you can deploy with Google Cloud SDK:\n+\n+#### Pub/Sub Notifications\n+```bash\n+PROJECT_ID=your-project-id\n+TOPIC_ID=test-gcs-ocn\n+PUBSUB_TOPIC=projects/${PROJECT_ID/topics/${TOPIC_ID}\n+\n+# Create Pub/Sub Object Change Notifications\n+gsutil notification create -f json -t ${PUBSUB_TOPIC} -e OBJECT_FINALIZE gs://${INGESTION_BUCKET}\n+\n+# Deploy Cloud Function\n+gcloud functions deploy test-gcs-bq-ingest \\\n+  --region=us-west4 \\\n+  --source=gcs_ocn_bq_ingest \\\n+  --entrypoint=main \\\n+  --runtime=python38 \\\n+  --trigger-topic=${PUBSUB_TOPIC} \\\n+  --service-account=${SERVICE_ACCOUNT_EMAIL} \\\n+  --timeout=540 \\\n+  --set-env-vars='DESTINATION_REGEX=^(?:[\\w\\-0-9]+)/(?P<dataset>[\\w\\-_0-9]+)/(?P<table>[\\w\\-_0-9]+)/?(?:incremental|history)?/?(?P<yyyy>[0-9]{4})?/?(?P<mm>[0-9]{2})?/?(?P<dd>[0-9]{2})?/?(?P<hh>[0-9]{2})?/?(?P<batch>[0-9]+)?/?'\n+```\n+\n+#### Cloud Functions Events\n+```bash\n+PROJECT_ID=your-project-id\n+\n+# Deploy Cloud Function\n+gcloud functions deploy test-gcs-bq-ingest \\\n+  --region=us-west4 \\\n+  --source=gcs_ocn_bq_ingest \\\n+  --entrypoint=main \\\n+  --runtime=python38 \\\n+  --trigger-resource ${INGESTION_BUCKET} \\\n+  --trigger-event google.storage.object.finalize\n+  --service-account=${SERVICE_ACCOUNT_EMAIL} \\\n+  --timeout=540 \\\n+  --set-env-vars='DESTINATION_REGEX=^(?:[\\w\\-0-9]+)/(?P<dataset>[\\w\\-_0-9]+)/(?P<table>[\\w\\-_0-9]+)/?(?:incremental|history)?/?(?P<yyyy>[0-9]{4})?/?(?P<mm>[0-9]{2})?/?(?P<dd>[0-9]{2})?/?(?P<hh>[0-9]{2})?/?(?P<batch>[0-9]+)?/?'\n+```\n+\n+In theory, one could set up Pub/Sub notifications from multiple GCS Buckets \n+(owned by different teams but following a common naming convention) to the same\n+Pub/Sub topic so that data uploaded to any of these buckets could get\n+automatically loaded to BigQuery by a single deployment of the Cloud Function.\n+\n+## Backfill\n+There are some cases where you may have data already copied to GCS according to\n+the naming convention / with success files before the Object Change\n+Notifications or Cloud Function have been set up. In these cases, you can use\n+the `backfill.py` CLI utility to crawl an existing bucket searching for success\n+files. The utility supports either invoking the Cloud Function main method\n+locally (in concurrent threads) or publishing notifications for the success \n+files (for a deployed Cloud Function to pick up).\n+\n+### Usage\n+```\n+python3 -m backfill -h\n+usage: backfill.py [-h] --gcs-path GCS_PATH [--mode {LOCAL,NOTIFICATIONS}] [--pubsub-topic PUBSUB_TOPIC] [--success-filename SUCCESS_FILENAME] [--destination-regex DESTINATION_REGEX]\n+\n+utility to backfill success file notifications or run the cloud function locally in concurrent threads.\n+\n+optional arguments:\n+  -h, --help            show this help message and exit\n+  --gcs-path GCS_PATH, -p GCS_PATH\n+                        GCS path (e.g. gs://bucket/prefix/to/search/)to search for existing _SUCCESS files\n+  --mode {LOCAL,NOTIFICATIONS}, -m {LOCAL,NOTIFICATIONS}\n+                        How to perform the backfill: LOCAL run cloud function main method locally (in concurrent threads) or NOTIFICATIONS just push notifications to Pub/Sub for a deployed\n+                        version of the cloud function to pick up. Default is NOTIFICATIONS.\n+  --pubsub-topic PUBSUB_TOPIC, --topic PUBSUB_TOPIC, -t PUBSUB_TOPIC\n+                        Pub/Sub notifications topic to post notifications for. i.e. projects/{PROJECT_ID}/topics/{TOPIC_ID} Required if using NOTIFICATIONS mode.\n+  --success-filename SUCCESS_FILENAME, -f SUCCESS_FILENAME\n+                        Override the default success filename '_SUCCESS'\n+  --destination-regex DESTINATION_REGEX, -r DESTINATION_REGEX\n+                        Override the default destination regex for determining BigQuerydestination based on information encoded in the GCS path of thesuccess file", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9e3e963fba00e1757c2dc8b7cf212565bab582d3"}, "originalPosition": 214}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5MDgyMjE0OnYy", "diffSide": "RIGHT", "path": "tools/cloud_functions/gcs_event_based_ingest/backfill.py", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwMzo0MzozOFrOH0jP3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QyMToxNToyNFrOH1LwHw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg2NTUwMw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            from argparse import ArgumentParser, Namespace\n          \n          \n            \n            from argparse import ArgumentParser\n          \n          \n            \n            from argparse import Namespace", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r524865503", "createdAt": "2020-11-17T03:43:38Z", "author": {"login": "danieldeleo"}, "path": "tools/cloud_functions/gcs_event_based_ingest/backfill.py", "diffHunk": "@@ -0,0 +1,175 @@\n+# Copyright 2020 Google LLC\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\" Command Line utility for backfilling gcs_ocn_bq_ingest cloud function\n+\"\"\"\n+import concurrent.futures\n+import logging\n+import os\n+import pprint\n+import sys\n+from argparse import ArgumentParser, Namespace", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9e3e963fba00e1757c2dc8b7cf212565bab582d3"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTM4MzQ2NA==", "bodyText": "reasoning? this looks like java style suggestion on a python import. Many python communities will import several things from a single module in a single statement (under the hood it is slightly more performant IIUC).\nExample in airflow even splitting on to multiple lines:\nhttps://github.com/apache/airflow/blob/master/airflow/operators/check_operator.py#L23-L28", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r525383464", "createdAt": "2020-11-17T18:17:18Z", "author": {"login": "jaketf"}, "path": "tools/cloud_functions/gcs_event_based_ingest/backfill.py", "diffHunk": "@@ -0,0 +1,175 @@\n+# Copyright 2020 Google LLC\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\" Command Line utility for backfilling gcs_ocn_bq_ingest cloud function\n+\"\"\"\n+import concurrent.futures\n+import logging\n+import os\n+import pprint\n+import sys\n+from argparse import ArgumentParser, Namespace", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg2NTUwMw=="}, "originalCommit": {"oid": "9e3e963fba00e1757c2dc8b7cf212565bab582d3"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTUxMzEwNQ==", "bodyText": "tl;dr I'll refactor imports to just import full modules directly to be more explicit and change yapf to style google.\nAlright I did some research:\nIt seems PEP-8 says this style fine (link)\nInterestingly Google style says always import entire module for namespacing reasons (https://google.github.io/styleguide/pyguide.html#22-imports)\nFor posterity I was totally wrong about any performance penalty of importing a whole module according to this SO post", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r525513105", "createdAt": "2020-11-17T20:45:34Z", "author": {"login": "jaketf"}, "path": "tools/cloud_functions/gcs_event_based_ingest/backfill.py", "diffHunk": "@@ -0,0 +1,175 @@\n+# Copyright 2020 Google LLC\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\" Command Line utility for backfilling gcs_ocn_bq_ingest cloud function\n+\"\"\"\n+import concurrent.futures\n+import logging\n+import os\n+import pprint\n+import sys\n+from argparse import ArgumentParser, Namespace", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg2NTUwMw=="}, "originalCommit": {"oid": "9e3e963fba00e1757c2dc8b7cf212565bab582d3"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTUyOTExOQ==", "bodyText": "refactored imports to match google style recommendation in 4ae77a2", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r525529119", "createdAt": "2020-11-17T21:15:24Z", "author": {"login": "jaketf"}, "path": "tools/cloud_functions/gcs_event_based_ingest/backfill.py", "diffHunk": "@@ -0,0 +1,175 @@\n+# Copyright 2020 Google LLC\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\" Command Line utility for backfilling gcs_ocn_bq_ingest cloud function\n+\"\"\"\n+import concurrent.futures\n+import logging\n+import os\n+import pprint\n+import sys\n+from argparse import ArgumentParser, Namespace", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg2NTUwMw=="}, "originalCommit": {"oid": "9e3e963fba00e1757c2dc8b7cf212565bab582d3"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5MDg0ODQwOnYy", "diffSide": "RIGHT", "path": "tools/cloud_functions/gcs_event_based_ingest/backfill.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwMzo1ODowOVrOH0jemA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwMzo1ODowOVrOH0jemA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg2OTI3Mg==", "bodyText": "Rename to a more descriptive variable name like \"pubsub_client\".\ncli is more commonly understood as \"command line interface\"", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r524869272", "createdAt": "2020-11-17T03:58:09Z", "author": {"login": "danieldeleo"}, "path": "tools/cloud_functions/gcs_event_based_ingest/backfill.py", "diffHunk": "@@ -0,0 +1,175 @@\n+# Copyright 2020 Google LLC\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\" Command Line utility for backfilling gcs_ocn_bq_ingest cloud function\n+\"\"\"\n+import concurrent.futures\n+import logging\n+import os\n+import pprint\n+import sys\n+from argparse import ArgumentParser, Namespace\n+from typing import Dict, Iterator, List\n+\n+from google.api_core.client_info import ClientInfo\n+from google.cloud import storage\n+\n+import gcs_ocn_bq_ingest  # pylint: disable=import-error\n+\n+CLIENT_INFO = ClientInfo(user_agent=\"google-pso-tool/bq-severless-loader\")\n+\n+os.environ[\"FUNCTION_NAME\"] = \"backfill-cli\"\n+\n+\n+def find_blobs_with_suffix(\n+    gcs_cli: storage.Client,\n+    prefix: str,\n+    suffix: str = \"_SUCCESS\",\n+) -> Iterator[storage.Blob]:\n+    \"\"\"\n+    Find GCS blobs with a given suffix.\n+\n+    :param gcs_cli:  storage.Client\n+    :param prefix: A GCS prefix to search i.e. gs://bucket/prefix/to/search\n+    :param suffix: A suffix in blob name to match\n+    :return:  Iterable of blobs matching the suffix.\n+    \"\"\"\n+    bucket_name, prefix = gcs_ocn_bq_ingest.main.parse_gcs_url(\n+        prefix)    # noqa\n+    bucket: storage.Bucket = gcs_cli.lookup_bucket(bucket_name)\n+    # filter passes on scalability / laziness advantages of iterator.\n+    return filter(lambda blob: blob.name.endswith(suffix),\n+                  bucket.list_blobs(prefix=prefix))\n+\n+\n+def main(args: Namespace):\n+    \"\"\"main entry point for backfill CLI.\"\"\"\n+    gcs_cli: storage.Client = storage.Client(client_info=CLIENT_INFO)\n+    ps_cli = None", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9e3e963fba00e1757c2dc8b7cf212565bab582d3"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5MzI0NDc1OnYy", "diffSide": "RIGHT", "path": "tools/cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QxNToyMToxOVrOH06Esw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QyMDozNDoyMFrOH1KVzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTIzOTQ3NQ==", "bodyText": "Check and raise error if bkt is None before calling blob()", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r525239475", "createdAt": "2020-11-17T15:21:19Z", "author": {"login": "danieldeleo"}, "path": "tools/cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -103,6 +110,7 @@ def main(event: Dict, context):    # pylint: disable=unused-argument\n     prefix_to_load = removesuffix(object_id, SUCCESS_FILENAME)\n     gsurl = f\"gs://{bucket_id}/{prefix_to_load}\"\n     gcs_client = storage.Client(client_info=CLIENT_INFO)\n+    project = gcs_client.project\n     bkt = gcs_client.lookup_bucket(bucket_id)\n     success_blob: storage.Blob = bkt.blob(object_id)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9e3e963fba00e1757c2dc8b7cf212565bab582d3"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTM4NTIwNQ==", "bodyText": "goooood point mypy should have caught this if there was a proper typehint lookup_bucket() -> Optional[storage.Bucket].", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r525385205", "createdAt": "2020-11-17T18:19:54Z", "author": {"login": "jaketf"}, "path": "tools/cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -103,6 +110,7 @@ def main(event: Dict, context):    # pylint: disable=unused-argument\n     prefix_to_load = removesuffix(object_id, SUCCESS_FILENAME)\n     gsurl = f\"gs://{bucket_id}/{prefix_to_load}\"\n     gcs_client = storage.Client(client_info=CLIENT_INFO)\n+    project = gcs_client.project\n     bkt = gcs_client.lookup_bucket(bucket_id)\n     success_blob: storage.Blob = bkt.blob(object_id)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTIzOTQ3NQ=="}, "originalCommit": {"oid": "9e3e963fba00e1757c2dc8b7cf212565bab582d3"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTM5Mjg2NA==", "bodyText": "Opened googleapis/python-storage#318", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r525392864", "createdAt": "2020-11-17T18:31:39Z", "author": {"login": "jaketf"}, "path": "tools/cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -103,6 +110,7 @@ def main(event: Dict, context):    # pylint: disable=unused-argument\n     prefix_to_load = removesuffix(object_id, SUCCESS_FILENAME)\n     gsurl = f\"gs://{bucket_id}/{prefix_to_load}\"\n     gcs_client = storage.Client(client_info=CLIENT_INFO)\n+    project = gcs_client.project\n     bkt = gcs_client.lookup_bucket(bucket_id)\n     success_blob: storage.Blob = bkt.blob(object_id)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTIzOTQ3NQ=="}, "originalCommit": {"oid": "9e3e963fba00e1757c2dc8b7cf212565bab582d3"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTUwNTk5Ng==", "bodyText": "implemented this in a wrapper function and added local type hint.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r525505996", "createdAt": "2020-11-17T20:34:20Z", "author": {"login": "jaketf"}, "path": "tools/cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -103,6 +110,7 @@ def main(event: Dict, context):    # pylint: disable=unused-argument\n     prefix_to_load = removesuffix(object_id, SUCCESS_FILENAME)\n     gsurl = f\"gs://{bucket_id}/{prefix_to_load}\"\n     gcs_client = storage.Client(client_info=CLIENT_INFO)\n+    project = gcs_client.project\n     bkt = gcs_client.lookup_bucket(bucket_id)\n     success_blob: storage.Blob = bkt.blob(object_id)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTIzOTQ3NQ=="}, "originalCommit": {"oid": "9e3e963fba00e1757c2dc8b7cf212565bab582d3"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5MzM1ODk0OnYy", "diffSide": "RIGHT", "path": "tools/cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QxNTozNjoxNFrOH07OeQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QxODoyMDozMVrOH1C_mw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTI1ODM2MQ==", "bodyText": "I'd recommend using as much of the cloud storage python client as possible. You can get rid of your parsing function and use:\nhttps://googleapis.dev/python/storage/latest/blobs.html#google.cloud.storage.blob.Blob.from_string", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r525258361", "createdAt": "2020-11-17T15:36:14Z", "author": {"login": "danieldeleo"}, "path": "tools/cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -376,7 +389,7 @@ def get_batches_for_prefix(storage_client,\n     (one batch has an array of multiple GCS uris)\n     \"\"\"\n     batches = []\n-    bucket_name, prefix_name = _parse_gcs_url(prefix_path)\n+    bucket_name, prefix_name = parse_gcs_url(prefix_path)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9e3e963fba00e1757c2dc8b7cf212565bab582d3"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTM4NTYyNw==", "bodyText": "TIL this existed thanks <3", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r525385627", "createdAt": "2020-11-17T18:20:31Z", "author": {"login": "jaketf"}, "path": "tools/cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -376,7 +389,7 @@ def get_batches_for_prefix(storage_client,\n     (one batch has an array of multiple GCS uris)\n     \"\"\"\n     batches = []\n-    bucket_name, prefix_name = _parse_gcs_url(prefix_path)\n+    bucket_name, prefix_name = parse_gcs_url(prefix_path)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTI1ODM2MQ=="}, "originalCommit": {"oid": "9e3e963fba00e1757c2dc8b7cf212565bab582d3"}, "originalPosition": 74}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5MzQ1MjE4OnYy", "diffSide": "RIGHT", "path": "tools/cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QxNTo1NDoyNlrOH08JIQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QxODo0Mjo1N1rOH1D37w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTI3MzM3Nw==", "bodyText": "Cleaner to check for:\nnotification.get(\"kind\") == \"storage#object\"", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r525273377", "createdAt": "2020-11-17T15:54:26Z", "author": {"login": "danieldeleo"}, "path": "tools/cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -426,19 +439,38 @@ def parse_notification(notification: dict) -> Tuple[str, str]:\n     Args:\n         notification(dict): Pub/Sub Storage Notification\n         https://cloud.google.com/storage/docs/pubsub-notifications\n+        Or Cloud Functions direct trigger\n+        https://cloud.google.com/functions/docs/tutorials/storage\n+        with notification schema\n+        https://cloud.google.com/storage/docs/json_api/v1/objects#resource\n     Returns:\n         tuple of bucketId and objectId attributes\n     Raises:\n         KeyError if the input notification does not contain the expected\n         attributes.\n     \"\"\"\n-    try:\n-        attributes = notification[\"attributes\"]\n-        return attributes[\"bucketId\"], attributes[\"objectId\"]\n-    except KeyError:\n-        raise RuntimeError(\n-            \"Issue with payload, did not contain expected attributes\"\n-            f\"'bucketId' and 'objectId': {notification}\") from KeyError\n+    if {\"bucket\", \"name\", } <= notification.keys():", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9e3e963fba00e1757c2dc8b7cf212565bab582d3"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTQwMDA0Nw==", "bodyText": "so much more explicit, love it.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r525400047", "createdAt": "2020-11-17T18:42:57Z", "author": {"login": "jaketf"}, "path": "tools/cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -426,19 +439,38 @@ def parse_notification(notification: dict) -> Tuple[str, str]:\n     Args:\n         notification(dict): Pub/Sub Storage Notification\n         https://cloud.google.com/storage/docs/pubsub-notifications\n+        Or Cloud Functions direct trigger\n+        https://cloud.google.com/functions/docs/tutorials/storage\n+        with notification schema\n+        https://cloud.google.com/storage/docs/json_api/v1/objects#resource\n     Returns:\n         tuple of bucketId and objectId attributes\n     Raises:\n         KeyError if the input notification does not contain the expected\n         attributes.\n     \"\"\"\n-    try:\n-        attributes = notification[\"attributes\"]\n-        return attributes[\"bucketId\"], attributes[\"objectId\"]\n-    except KeyError:\n-        raise RuntimeError(\n-            \"Issue with payload, did not contain expected attributes\"\n-            f\"'bucketId' and 'objectId': {notification}\") from KeyError\n+    if {\"bucket\", \"name\", } <= notification.keys():", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTI3MzM3Nw=="}, "originalCommit": {"oid": "9e3e963fba00e1757c2dc8b7cf212565bab582d3"}, "originalPosition": 99}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5MzQ4ODE5OnYy", "diffSide": "RIGHT", "path": "tools/cloud_functions/gcs_event_based_ingest/tests/cli/test_backfill.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QxNjowMToyMFrOH08fcw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QxNjowMToyMFrOH08fcw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTI3OTA5MQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                This is an adapatation of test_load_job_partitioned but instead uses the\n          \n          \n            \n                This is an adaptation of test_load_job_partitioned but instead uses the", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r525279091", "createdAt": "2020-11-17T16:01:20Z", "author": {"login": "danieldeleo"}, "path": "tools/cloud_functions/gcs_event_based_ingest/tests/cli/test_backfill.py", "diffHunk": "@@ -0,0 +1,90 @@\n+# Copyright 2020 Google LLC\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"integrtion tests for gcs_ocn_bq_ingest\"\"\"\n+import os\n+from time import monotonic\n+\n+import backfill\n+import pytest\n+from google.cloud import bigquery\n+\n+TEST_DIR = os.path.realpath(os.path.dirname(__file__) + \"/..\")\n+LOAD_JOB_POLLING_TIMEOUT = 20    # seconds\n+\n+\n+@pytest.mark.IT\n+@pytest.mark.CLI\n+def test_backfill(bq, gcs_partitioned_data, gcs_truncating_load_config,\n+                  gcs_bucket, dest_dataset, dest_partitioned_table, mock_env):\n+    \"\"\"\n+    This is an adapatation of test_load_job_partitioned but instead uses the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9e3e963fba00e1757c2dc8b7cf212565bab582d3"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5MzQ5NzI5OnYy", "diffSide": "RIGHT", "path": "tools/cloud_functions/gcs_event_based_ingest/tests/conftest.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QxNjowMzoxN1rOH08lKA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QxNjowMzoxN1rOH08lKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTI4MDU1Mg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \"\"\"integrtion tests for gcs_ocn_bq_ingest\"\"\"\n          \n          \n            \n            \"\"\"Integration tests for gcs_ocn_bq_ingest\"\"\"", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r525280552", "createdAt": "2020-11-17T16:03:17Z", "author": {"login": "danieldeleo"}, "path": "tools/cloud_functions/gcs_event_based_ingest/tests/conftest.py", "diffHunk": "@@ -11,23 +11,20 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\"\"\"unit tests for gcs_ocn_bq_ingest\"\"\"\n+\"\"\"integrtion tests for gcs_ocn_bq_ingest\"\"\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9e3e963fba00e1757c2dc8b7cf212565bab582d3"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5NTMzNTk4OnYy", "diffSide": "RIGHT", "path": "tools/cloud_functions/gcs_event_based_ingest/tests/conftest.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QyMjo0ODozM1rOH1OqHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQxNzo0NjoyNlrOH16nJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTU3NjczNA==", "bodyText": "Imports should be on separate lines\nhttps://google.github.io/styleguide/pyguide.html#313-imports-formatting", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r525576734", "createdAt": "2020-11-17T22:48:33Z", "author": {"login": "danieldeleo"}, "path": "tools/cloud_functions/gcs_event_based_ingest/tests/conftest.py", "diffHunk": "@@ -11,24 +11,20 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\"\"\"unit tests for gcs_ocn_bq_ingest\"\"\"\n+\"\"\"Integration tests for gcs_ocn_bq_ingest\"\"\"\n import json\n import os\n-import sys\n+import time\n import uuid\n-from time import monotonic\n from typing import List\n \n-import google.cloud.storage as storage\n import pytest\n-from google.cloud import bigquery\n-from google.cloud.exceptions import NotFound\n+from google.cloud import bigquery, storage", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8ccc387de0242f7dc4b94f3d535544fa660eb691"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjI5Njg3MA==", "bodyText": "Doh! we were just talking about this too!\nFixed here and in main.py in 348287f\nI will look into if there is a pylint or isort plugin / configuration we can use to check this import style.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r526296870", "createdAt": "2020-11-18T17:46:26Z", "author": {"login": "jaketf"}, "path": "tools/cloud_functions/gcs_event_based_ingest/tests/conftest.py", "diffHunk": "@@ -11,24 +11,20 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\"\"\"unit tests for gcs_ocn_bq_ingest\"\"\"\n+\"\"\"Integration tests for gcs_ocn_bq_ingest\"\"\"\n import json\n import os\n-import sys\n+import time\n import uuid\n-from time import monotonic\n from typing import List\n \n-import google.cloud.storage as storage\n import pytest\n-from google.cloud import bigquery\n-from google.cloud.exceptions import NotFound\n+from google.cloud import bigquery, storage", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTU3NjczNA=="}, "originalCommit": {"oid": "8ccc387de0242f7dc4b94f3d535544fa660eb691"}, "originalPosition": 18}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2921, "cost": 1, "resetAt": "2021-11-12T19:05:54Z"}}}