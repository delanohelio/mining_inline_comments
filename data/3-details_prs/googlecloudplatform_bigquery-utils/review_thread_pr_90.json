{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ3MDg5ODcw", "number": 90, "reviewThreads": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQyMTo1MDo0OFrOENEZFw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQxOTowMzoxMVrOEPQ1iA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMTM4OTAzOnYy", "diffSide": "RIGHT", "path": "tools/unsupervised_dataset/run_crawler.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQyMTo1MDo0OFrOGvjCSg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQyMTo1MDo0OFrOGvjCSg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjUxMDI4Mg==", "bodyText": "Is the list separated by comma or something? maybe add that as well in description", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/90#discussion_r452510282", "createdAt": "2020-07-09T21:50:48Z", "author": {"login": "yzhvictor"}, "path": "tools/unsupervised_dataset/run_crawler.py", "diffHunk": "@@ -1,11 +1,16 @@\n \"\"\" Script to initialize the SQL crawler on a website of the user's choice \"\"\"\n \n import sys\n+import argparse\n from sql_crawler import crawler\n \n def start_crawler():\n-    urls = sys.argv[1:]\n-    new_crawler = crawler.Crawler(urls, max_size=50)\n+    parser = argparse.ArgumentParser(description=\"SQL Web Crawler\")\n+    parser.add_argument(\"urls\", help=\"A list of URLs to be crawled\", nargs='+')", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d5d44cb64260fde4b40b3f6308313a7c5d50c40f"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMTM5MjE2OnYy", "diffSide": "RIGHT", "path": "tools/unsupervised_dataset/run_crawler.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQyMTo1MTo0OFrOGvjEFw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQyMTo1MTo0OFrOGvjEFw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjUxMDc0Mw==", "bodyText": "Is this the standard? looks like an array, but it should be an int I think.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/90#discussion_r452510743", "createdAt": "2020-07-09T21:51:48Z", "author": {"login": "yzhvictor"}, "path": "tools/unsupervised_dataset/run_crawler.py", "diffHunk": "@@ -1,11 +1,16 @@\n \"\"\" Script to initialize the SQL crawler on a website of the user's choice \"\"\"\n \n import sys\n+import argparse\n from sql_crawler import crawler\n \n def start_crawler():\n-    urls = sys.argv[1:]\n-    new_crawler = crawler.Crawler(urls, max_size=50)\n+    parser = argparse.ArgumentParser(description=\"SQL Web Crawler\")\n+    parser.add_argument(\"urls\", help=\"A list of URLs to be crawled\", nargs='+')\n+    parser.add_argument(\"--max_depth\", help=\"The max depth of the crawler (default=3)\", type=int, nargs=1, default=[3])\n+    parser.add_argument(\"--max_size\", help=\"The maximum number of links to be crawled (default=100)\", type=int, nargs=1, default=[100])\n+    args = parser.parse_args()\n+    new_crawler = crawler.Crawler(args.urls, max_size=args.max_size[0], max_depth=args.max_depth[0])", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d5d44cb64260fde4b40b3f6308313a7c5d50c40f"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0NDM4Mzg0OnYy", "diffSide": "RIGHT", "path": "tools/unsupervised_dataset/sql_crawler/cloud_integration.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQxODo1ODo0N1rOGy4WXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQxODo1ODo0N1rOGy4WXA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjAwNTIxMg==", "bodyText": "Will there be some exceptions such as permission issue, not found issue, should we handle them?", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/90#discussion_r456005212", "createdAt": "2020-07-16T18:58:47Z", "author": {"login": "yzhvictor"}, "path": "tools/unsupervised_dataset/sql_crawler/cloud_integration.py", "diffHunk": "@@ -0,0 +1,61 @@\n+\"\"\" Contains functions to upload data to Google Cloud Storage and Google\n+Bigquery. These functions take input specifying where to store the file\n+and return a message denoting success or error.\n+\"\"\"\n+\n+from google.cloud import bigquery, storage\n+\n+def load_bigquery_table(project_id, dataset_id, table_id, filename):\n+    \"\"\" Uploads a file to Google BigQuery.\n+    \n+    Args:\n+        project_id: Google Cloud Project ID for destination.\n+        dataset_id: Name of destination dataset.\n+        table_id: Name of destination table.\n+        filename: Name of CSV file to be uploaded.\n+    \n+    Returns:\n+        A log message depending on success or failure.\n+    \"\"\"\n+    \n+    client = bigquery.Client(project=project_id)\n+    \n+    dataset_ref = client.dataset(dataset_id)\n+    table_ref = dataset_ref.table(table_id)\n+    job_config = bigquery.LoadJobConfig()\n+    job_config.source_format = bigquery.SourceFormat.CSV\n+    job_config.skip_leading_rows = 1\n+    job_config.autodetect = True\n+\n+    job_config.schema = [\n+        bigquery.SchemaField(\"query\", \"STRING\", mode=\"NULLABLE\"),\n+        bigquery.SchemaField(\"url\", \"STRING\", mode=\"NULLABLE\"),\n+    ]\n+    \n+    with open(filename, \"rb\") as source_file:\n+        job = client.load_table_from_file(source_file, table_ref, job_config=job_config)\n+    \n+    job.result()\n+    \n+    return \"Loaded {} rows into {}:{}.\".format(job.output_rows, dataset_id, table_id)\n+\n+def upload_gcs_file(project_id, bucket_id, destination_blob_name, filename):\n+    \"\"\" Uploads a file to Google Cloud Storage.\n+    \n+    Args:\n+        project_id: Google Cloud Project ID for destination.\n+        bucket_id: Name of destination bucket.\n+        destination_blob_name: Name of destination file.\n+        filename: Name of file to be uploaded.\n+\n+    Returns:\n+        A log message depending on success or failure.\n+    \"\"\"\n+\n+    storage_client = storage.Client(project=project_id)\n+    bucket = storage_client.bucket(bucket_id)\n+    blob = bucket.blob(destination_blob_name)\n+\n+    blob.upload_from_filename(filename)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d41c07830266e40768ae5e7f16183de1472c188a"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0NDM4ODExOnYy", "diffSide": "RIGHT", "path": "tools/unsupervised_dataset/sql_crawler/cloud_integration.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQxODo1OTo1MlrOGy4Y5Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQxODo1OTo1MlrOGy4Y5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjAwNTg2MQ==", "bodyText": "Do we get anything from the return of this method call? Or it's just to make sure thread is blocked.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/90#discussion_r456005861", "createdAt": "2020-07-16T18:59:52Z", "author": {"login": "yzhvictor"}, "path": "tools/unsupervised_dataset/sql_crawler/cloud_integration.py", "diffHunk": "@@ -0,0 +1,61 @@\n+\"\"\" Contains functions to upload data to Google Cloud Storage and Google\n+Bigquery. These functions take input specifying where to store the file\n+and return a message denoting success or error.\n+\"\"\"\n+\n+from google.cloud import bigquery, storage\n+\n+def load_bigquery_table(project_id, dataset_id, table_id, filename):\n+    \"\"\" Uploads a file to Google BigQuery.\n+    \n+    Args:\n+        project_id: Google Cloud Project ID for destination.\n+        dataset_id: Name of destination dataset.\n+        table_id: Name of destination table.\n+        filename: Name of CSV file to be uploaded.\n+    \n+    Returns:\n+        A log message depending on success or failure.\n+    \"\"\"\n+    \n+    client = bigquery.Client(project=project_id)\n+    \n+    dataset_ref = client.dataset(dataset_id)\n+    table_ref = dataset_ref.table(table_id)\n+    job_config = bigquery.LoadJobConfig()\n+    job_config.source_format = bigquery.SourceFormat.CSV\n+    job_config.skip_leading_rows = 1\n+    job_config.autodetect = True\n+\n+    job_config.schema = [\n+        bigquery.SchemaField(\"query\", \"STRING\", mode=\"NULLABLE\"),\n+        bigquery.SchemaField(\"url\", \"STRING\", mode=\"NULLABLE\"),\n+    ]\n+    \n+    with open(filename, \"rb\") as source_file:\n+        job = client.load_table_from_file(source_file, table_ref, job_config=job_config)\n+    \n+    job.result()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d41c07830266e40768ae5e7f16183de1472c188a"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0NDM5OTQ0OnYy", "diffSide": "RIGHT", "path": "tools/unsupervised_dataset/sql_crawler/crawler_log.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQxOTowMzoxMVrOGy4f_A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQxOTowMzoxMVrOGy4f_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjAwNzY3Ng==", "bodyText": "Do we also need to check gcs_bucket, and bq_dataset for the below method?", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/90#discussion_r456007676", "createdAt": "2020-07-16T19:03:11Z", "author": {"login": "yzhvictor"}, "path": "tools/unsupervised_dataset/sql_crawler/crawler_log.py", "diffHunk": "@@ -66,11 +71,68 @@ def log_error(self, errorMessage):\n         Args:\n             str: Error message to be logged.\n         \"\"\"\n-\n+        \n+        self.error_log_count += 1\n         logging.error(\"ERROR: %s\", errorMessage)\n+        \n+    def parse_location_arg(self, location):\n+        \"\"\" Validates and splits location argument for cloud upload\n+        into two parts. Should be formatted as project_id.dataset.\n+        \n+        Args:\n+            location: String with name of project ID and dataset.\n+            \n+        Returns\n+            List of separate strings after splitting location.\n+        \"\"\"\n+        if location.count(\".\") != 1:\n+            self.log_error(\"Argument not formatted correctly: {0}\".format(location))\n+            return None, None\n+        \n+        return location.split(\".\")\n+        \n+    def set_gcs(self, location):\n+        \"\"\" Sets variables for uploading data to Google Cloud Storage.\n+            \n+        Args:\n+            location: String with name of project ID and bucket name,\n+            separated by a period.\n+        \"\"\"\n+\n+        self.gcs_project, self.gcs_bucket = self.parse_location_arg(location)\n+        if self.gcs_project:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d41c07830266e40768ae5e7f16183de1472c188a"}, "originalPosition": 77}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3009, "cost": 1, "resetAt": "2021-11-12T19:05:54Z"}}}