{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTM1NjI4Mjgz", "number": 205, "reviewThreads": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwMzozMzoxM1rOFDzAEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxODoyMzo0M1rOFEHAGw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NTI1NjQ4OnYy", "diffSide": "RIGHT", "path": "tools/cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/ordering.py", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwMzozMzoxM1rOIDnmHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQyMToxNDozNVrOIEOPfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDY2NTM3Mg==", "bodyText": "What would happen if this code never runs? Say due to platform failure, unexpected network hiccup, etc.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/205#discussion_r540665372", "createdAt": "2020-12-11T03:33:13Z", "author": {"login": "ryanmcdowell"}, "path": "tools/cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/ordering.py", "diffHunk": "@@ -0,0 +1,273 @@\n+# Copyright 2020 Google LLC.\n+# This software is provided as-is, without warranty or representation\n+# for any use or purpose.\n+# Your use of it is subject to your agreement with Google.\n+\n+# Licensed under the Apache License, Version 2.0 (the 'License');\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+# http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an 'AS IS' BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Background Cloud Function for loading data from GCS to BigQuery.\n+\"\"\"\n+import os\n+import time\n+import traceback\n+from typing import Optional, Tuple\n+\n+import google.api_core\n+import google.api_core.exceptions\n+# pylint in cloud build is being flaky about this import discovery.\n+# pylint: disable=no-name-in-module\n+from google.cloud import bigquery\n+from google.cloud import storage\n+\n+from . import constants\n+from . import exceptions\n+from . import utils\n+\n+\n+def backlog_publisher(\n+    gcs_client: storage.Client,\n+    event_blob: storage.Blob,\n+) -> Optional[storage.Blob]:\n+    \"\"\"add success files to the the backlog and trigger backfill if necessary\"\"\"\n+    bkt = event_blob.bucket\n+\n+    # Create an entry in _backlog for this table for this batch / success file\n+    backlog_blob = success_blob_to_backlog_blob(event_blob)\n+    backlog_blob.upload_from_string(\"\", client=gcs_client)\n+    print(f\"added gs://{backlog_blob.bucket.name}/{backlog_blob.name} \"\n+          \"to the backlog.\")\n+\n+    table_prefix = utils.get_table_prefix(event_blob.name)\n+    return start_backfill_subscriber_if_not_running(gcs_client, bkt,\n+                                                    table_prefix)\n+\n+\n+# pylint: disable=too-many-arguments,too-many-locals\n+def backlog_subscriber(gcs_client: Optional[storage.Client],\n+                       bq_client: Optional[bigquery.Client],\n+                       backfill_blob: storage.Blob, function_start_time: float):\n+    \"\"\"Pick up the table lock, poll BQ job id until completion and process next\n+    item in the backlog.\n+    \"\"\"\n+    gcs_client, bq_client = _get_clients_if_none(gcs_client, bq_client)\n+    # We need to retrigger the backfill loop before the Cloud Functions Timeout.\n+    restart_time = function_start_time + (\n+        float(os.getenv(\"FUNCTION_TIMEOUT_SEC\", \"60\")) -\n+        constants.RESTART_BUFFER_SECONDS)\n+    bkt = backfill_blob.bucket\n+    utils.handle_duplicate_notification(gcs_client, backfill_blob)\n+    table_prefix = utils.get_table_prefix(backfill_blob.name)\n+    last_job_done = False\n+    # we will poll for job completion this long in an individual iteration of\n+    # the while loop.\n+    polling_timeout = 5  # seconds\n+    lock_blob: storage.Blob = bkt.blob(f\"{table_prefix}/_bqlock\")\n+    if restart_time - polling_timeout < time.monotonic():\n+        raise EnvironmentError(\n+            \"The Cloud Function timeout is too short for \"\n+            \"backlog subscriber to do it's job. We recommend \"\n+            \"setting the timeout to 540 seconds or at least \"\n+            \"1 minute (Cloud Functions default).\")\n+    while time.monotonic() < restart_time - polling_timeout:\n+        lock_contents = utils.read_gcs_file_if_exists(\n+            gcs_client, f\"gs://{bkt.name}/{lock_blob.name}\")\n+        if lock_contents:\n+            if lock_contents.startswith(\n+                    os.getenv('JOB_PREFIX', constants.DEFAULT_JOB_PREFIX)):\n+                job_id = lock_contents\n+                try:\n+                    last_job_done = utils.wait_on_bq_job_id(\n+                        bq_client, job_id, polling_timeout)\n+                except (exceptions.BigQueryJobFailure,\n+                        google.api_core.exceptions.NotFound) as err:\n+                    raise exceptions.BigQueryJobFailure(\n+                        f\"previous BigQuery job: {job_id} failed or could not \"\n+                        \"be found. This will kill the backfill subscriber for \"\n+                        f\"the table prefix: {table_prefix}.\"\n+                        \"Once the issue is dealt with by a human, the lock \"\n+                        \"file at: \"\n+                        f\"gs://{lock_blob.bucket.name}/{lock_blob.name} \"\n+                        \"should be manually removed and a new empty \"\n+                        f\"{constants.BACKFILL_FILENAME} \"\n+                        \"file uploaded to: \"\n+                        f\"gs://{backfill_blob.bucket.name}/{table_prefix}\"\n+                        \"/_BACKFILL \"\n+                        f\"to resume the backfill subscriber so it can \"\n+                        \"continue with the next item in the backlog.\\n\"\n+                        \"Original Exception:\\n\"\n+                        f\"{traceback.format_exc()}\") from err\n+            else:\n+                print(f\"sleeping for {polling_timeout} seconds because\"\n+                      f\"found manual lock gs://{bkt.name}/{lock_blob.name} with\"\n+                      f\"contents:\\n {lock_contents}. This will be an infinite\"\n+                      \"loop until the manual lock is released.\")\n+                time.sleep(polling_timeout)\n+                continue\n+        if last_job_done:\n+            utils.remove_oldest_backlog_item(gcs_client, bkt, table_prefix)\n+            last_job_done = False\n+\n+        check_backlog_time = time.monotonic()\n+        next_backlog_file = utils.get_next_backlog_item(gcs_client, bkt,\n+                                                        table_prefix)\n+        if not next_backlog_file:\n+            backfill_blob.delete(if_generation_match=backfill_blob.generation,\n+                                 client=gcs_client)\n+            if (check_backlog_time + constants.ENSURE_SUBSCRIBER_SECONDS <\n+                    time.monotonic()):\n+                print(\n+                    \"checking if the backlog is still empty for \"\n+                    f\"gs://${bkt.name}/{table_prefix}/_backlog/\"\n+                    f\"There was more than {constants.ENSURE_SUBSCRIBER_SECONDS}\"\n+                    \" seconds between listing items on the backlog and \"\n+                    f\"attempting to delete the {constants.BACKFILL_FILENAME}. \"\n+                    \"This should not happen often but is meant to alleviate a \"\n+                    \"race condition in the event that something caused the \"\n+                    \"delete operation was delayed or had to be retried for a \"\n+                    \"long time.\")\n+                next_backlog_file = utils.get_next_backlog_item(\n+                    gcs_client, bkt, table_prefix)\n+                if next_backlog_file:\n+                    # The backfill file may have been deleted but the backlog is\n+                    # not empty. Re-trigger the backfill subscriber loop by\n+                    # dropping a new backfill file.\n+                    start_backfill_subscriber_if_not_running(\n+                        gcs_client, bkt, table_prefix)\n+                    return\n+            utils.handle_bq_lock(gcs_client, lock_blob, None)\n+            print(f\"backlog is empty for gs://{bkt.name}/{table_prefix}. \"\n+                  \"backlog subscriber exiting.\")\n+            return\n+        next_success_file: storage.Blob = bkt.blob(\n+            next_backlog_file.name.replace(\"/_backlog/\", \"/\"))\n+        table_ref, batch = utils.gcs_path_to_table_ref_and_batch(\n+            next_success_file.name)\n+        if not next_success_file.exists(client=gcs_client):\n+            raise exceptions.BacklogException(\n+                \"backlog contains\"\n+                f\"gs://{next_backlog_file.bucket}/{next_backlog_file.name}\"\n+                \"but the corresponding success file does not exist at:\"\n+                f\"gs://{next_success_file.bucket}/{next_success_file.name}\")\n+        utils.apply(gcs_client, bq_client, next_success_file, lock_blob,\n+                    utils.create_job_id(table_ref, batch))\n+    # retrigger the subscriber loop by reposting the _BACKFILL file", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2218212d585eb078b5872e110608ad4cbf6aa5ba"}, "originalPosition": 162}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTI5ODU1Nw==", "bodyText": "Great Question!\nBasically if this code never runs, no more BQ jobs will be submitted for this table until this _BACKFILL file is replaced.\nIf other triggers of the function were still able to run the publisher, backlog would accrue but the table would not make progress until this _BACKFILL was replaced.\nWe can improve this situation by making the subscriber_monitor check that the age of the _BACKFILL object is not greater than the function timeout. This will mean the system will recover from the abandoned _BACKFILL the next time a file is added to the backlog. (I'll prepare this in my next commit)\na temporary network \"hiccup\" shouldn't matter much as client library will retry requests and we build in some healthy buffer before the function actually times out however a true network failure might cause these retries to be futile.\nIf cloud functions platform goes down in the region, we won't really be able to recover w/ in this cloud function. It could be possible to run this function in multiple regions, in this case one should silence the logs about duplicate notifications as this would be \"by design\" as you'd expect a race between the functions running in each region to claim the work for a particular event.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/205#discussion_r541298557", "createdAt": "2020-12-11T21:14:35Z", "author": {"login": "jaketf"}, "path": "tools/cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/ordering.py", "diffHunk": "@@ -0,0 +1,273 @@\n+# Copyright 2020 Google LLC.\n+# This software is provided as-is, without warranty or representation\n+# for any use or purpose.\n+# Your use of it is subject to your agreement with Google.\n+\n+# Licensed under the Apache License, Version 2.0 (the 'License');\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+# http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an 'AS IS' BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Background Cloud Function for loading data from GCS to BigQuery.\n+\"\"\"\n+import os\n+import time\n+import traceback\n+from typing import Optional, Tuple\n+\n+import google.api_core\n+import google.api_core.exceptions\n+# pylint in cloud build is being flaky about this import discovery.\n+# pylint: disable=no-name-in-module\n+from google.cloud import bigquery\n+from google.cloud import storage\n+\n+from . import constants\n+from . import exceptions\n+from . import utils\n+\n+\n+def backlog_publisher(\n+    gcs_client: storage.Client,\n+    event_blob: storage.Blob,\n+) -> Optional[storage.Blob]:\n+    \"\"\"add success files to the the backlog and trigger backfill if necessary\"\"\"\n+    bkt = event_blob.bucket\n+\n+    # Create an entry in _backlog for this table for this batch / success file\n+    backlog_blob = success_blob_to_backlog_blob(event_blob)\n+    backlog_blob.upload_from_string(\"\", client=gcs_client)\n+    print(f\"added gs://{backlog_blob.bucket.name}/{backlog_blob.name} \"\n+          \"to the backlog.\")\n+\n+    table_prefix = utils.get_table_prefix(event_blob.name)\n+    return start_backfill_subscriber_if_not_running(gcs_client, bkt,\n+                                                    table_prefix)\n+\n+\n+# pylint: disable=too-many-arguments,too-many-locals\n+def backlog_subscriber(gcs_client: Optional[storage.Client],\n+                       bq_client: Optional[bigquery.Client],\n+                       backfill_blob: storage.Blob, function_start_time: float):\n+    \"\"\"Pick up the table lock, poll BQ job id until completion and process next\n+    item in the backlog.\n+    \"\"\"\n+    gcs_client, bq_client = _get_clients_if_none(gcs_client, bq_client)\n+    # We need to retrigger the backfill loop before the Cloud Functions Timeout.\n+    restart_time = function_start_time + (\n+        float(os.getenv(\"FUNCTION_TIMEOUT_SEC\", \"60\")) -\n+        constants.RESTART_BUFFER_SECONDS)\n+    bkt = backfill_blob.bucket\n+    utils.handle_duplicate_notification(gcs_client, backfill_blob)\n+    table_prefix = utils.get_table_prefix(backfill_blob.name)\n+    last_job_done = False\n+    # we will poll for job completion this long in an individual iteration of\n+    # the while loop.\n+    polling_timeout = 5  # seconds\n+    lock_blob: storage.Blob = bkt.blob(f\"{table_prefix}/_bqlock\")\n+    if restart_time - polling_timeout < time.monotonic():\n+        raise EnvironmentError(\n+            \"The Cloud Function timeout is too short for \"\n+            \"backlog subscriber to do it's job. We recommend \"\n+            \"setting the timeout to 540 seconds or at least \"\n+            \"1 minute (Cloud Functions default).\")\n+    while time.monotonic() < restart_time - polling_timeout:\n+        lock_contents = utils.read_gcs_file_if_exists(\n+            gcs_client, f\"gs://{bkt.name}/{lock_blob.name}\")\n+        if lock_contents:\n+            if lock_contents.startswith(\n+                    os.getenv('JOB_PREFIX', constants.DEFAULT_JOB_PREFIX)):\n+                job_id = lock_contents\n+                try:\n+                    last_job_done = utils.wait_on_bq_job_id(\n+                        bq_client, job_id, polling_timeout)\n+                except (exceptions.BigQueryJobFailure,\n+                        google.api_core.exceptions.NotFound) as err:\n+                    raise exceptions.BigQueryJobFailure(\n+                        f\"previous BigQuery job: {job_id} failed or could not \"\n+                        \"be found. This will kill the backfill subscriber for \"\n+                        f\"the table prefix: {table_prefix}.\"\n+                        \"Once the issue is dealt with by a human, the lock \"\n+                        \"file at: \"\n+                        f\"gs://{lock_blob.bucket.name}/{lock_blob.name} \"\n+                        \"should be manually removed and a new empty \"\n+                        f\"{constants.BACKFILL_FILENAME} \"\n+                        \"file uploaded to: \"\n+                        f\"gs://{backfill_blob.bucket.name}/{table_prefix}\"\n+                        \"/_BACKFILL \"\n+                        f\"to resume the backfill subscriber so it can \"\n+                        \"continue with the next item in the backlog.\\n\"\n+                        \"Original Exception:\\n\"\n+                        f\"{traceback.format_exc()}\") from err\n+            else:\n+                print(f\"sleeping for {polling_timeout} seconds because\"\n+                      f\"found manual lock gs://{bkt.name}/{lock_blob.name} with\"\n+                      f\"contents:\\n {lock_contents}. This will be an infinite\"\n+                      \"loop until the manual lock is released.\")\n+                time.sleep(polling_timeout)\n+                continue\n+        if last_job_done:\n+            utils.remove_oldest_backlog_item(gcs_client, bkt, table_prefix)\n+            last_job_done = False\n+\n+        check_backlog_time = time.monotonic()\n+        next_backlog_file = utils.get_next_backlog_item(gcs_client, bkt,\n+                                                        table_prefix)\n+        if not next_backlog_file:\n+            backfill_blob.delete(if_generation_match=backfill_blob.generation,\n+                                 client=gcs_client)\n+            if (check_backlog_time + constants.ENSURE_SUBSCRIBER_SECONDS <\n+                    time.monotonic()):\n+                print(\n+                    \"checking if the backlog is still empty for \"\n+                    f\"gs://${bkt.name}/{table_prefix}/_backlog/\"\n+                    f\"There was more than {constants.ENSURE_SUBSCRIBER_SECONDS}\"\n+                    \" seconds between listing items on the backlog and \"\n+                    f\"attempting to delete the {constants.BACKFILL_FILENAME}. \"\n+                    \"This should not happen often but is meant to alleviate a \"\n+                    \"race condition in the event that something caused the \"\n+                    \"delete operation was delayed or had to be retried for a \"\n+                    \"long time.\")\n+                next_backlog_file = utils.get_next_backlog_item(\n+                    gcs_client, bkt, table_prefix)\n+                if next_backlog_file:\n+                    # The backfill file may have been deleted but the backlog is\n+                    # not empty. Re-trigger the backfill subscriber loop by\n+                    # dropping a new backfill file.\n+                    start_backfill_subscriber_if_not_running(\n+                        gcs_client, bkt, table_prefix)\n+                    return\n+            utils.handle_bq_lock(gcs_client, lock_blob, None)\n+            print(f\"backlog is empty for gs://{bkt.name}/{table_prefix}. \"\n+                  \"backlog subscriber exiting.\")\n+            return\n+        next_success_file: storage.Blob = bkt.blob(\n+            next_backlog_file.name.replace(\"/_backlog/\", \"/\"))\n+        table_ref, batch = utils.gcs_path_to_table_ref_and_batch(\n+            next_success_file.name)\n+        if not next_success_file.exists(client=gcs_client):\n+            raise exceptions.BacklogException(\n+                \"backlog contains\"\n+                f\"gs://{next_backlog_file.bucket}/{next_backlog_file.name}\"\n+                \"but the corresponding success file does not exist at:\"\n+                f\"gs://{next_success_file.bucket}/{next_success_file.name}\")\n+        utils.apply(gcs_client, bq_client, next_success_file, lock_blob,\n+                    utils.create_job_id(table_ref, batch))\n+    # retrigger the subscriber loop by reposting the _BACKFILL file", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDY2NTM3Mg=="}, "originalCommit": {"oid": "2218212d585eb078b5872e110608ad4cbf6aa5ba"}, "originalPosition": 162}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5Nzc2OTAxOnYy", "diffSide": "RIGHT", "path": "tools/cloud_functions/gcs_event_based_ingest/ordered_backfill.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxNToyNjo0MFrOID9pgQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQyMDo0NjozOVrOIEMmqg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTAyNjY4OQ==", "bodyText": "Could we refer to the CLIENT_INFO in constants?", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/205#discussion_r541026689", "createdAt": "2020-12-11T15:26:40Z", "author": {"login": "ryanmcdowell"}, "path": "tools/cloud_functions/gcs_event_based_ingest/ordered_backfill.py", "diffHunk": "@@ -0,0 +1,177 @@\n+# Copyright 2020 Google LLC\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\" Command Line utility for backfilling gcs_ocn_bq_ingest cloud function when\n+ordering of incrementals is required\n+\"\"\"\n+import argparse\n+import concurrent.futures\n+import logging\n+import os\n+import pprint\n+import sys\n+from typing import Dict, Iterator, List\n+\n+import google.api_core.client_info\n+from google.cloud import storage\n+\n+import gcs_ocn_bq_ingest.main  # pylint: disable=import-error\n+\n+CLIENT_INFO = google.api_core.client_info.ClientInfo(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2218212d585eb078b5872e110608ad4cbf6aa5ba"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTI3MTcyMg==", "bodyText": "I'm actually going to remove this file from this PR it's the same as backfill.py and I'll add an ordering compliant backfill utility in a future PR.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/205#discussion_r541271722", "createdAt": "2020-12-11T20:46:39Z", "author": {"login": "jaketf"}, "path": "tools/cloud_functions/gcs_event_based_ingest/ordered_backfill.py", "diffHunk": "@@ -0,0 +1,177 @@\n+# Copyright 2020 Google LLC\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\" Command Line utility for backfilling gcs_ocn_bq_ingest cloud function when\n+ordering of incrementals is required\n+\"\"\"\n+import argparse\n+import concurrent.futures\n+import logging\n+import os\n+import pprint\n+import sys\n+from typing import Dict, Iterator, List\n+\n+import google.api_core.client_info\n+from google.cloud import storage\n+\n+import gcs_ocn_bq_ingest.main  # pylint: disable=import-error\n+\n+CLIENT_INFO = google.api_core.client_info.ClientInfo(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTAyNjY4OQ=="}, "originalCommit": {"oid": "2218212d585eb078b5872e110608ad4cbf6aa5ba"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5ODExMjE3OnYy", "diffSide": "RIGHT", "path": "tools/cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxNjo0MDoyOFrOIEAxjw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxNjo0MDoyOFrOIEAxjw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTA3NzkwMw==", "bodyText": "Add unit test for the triage logic done w/ this main method.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/205#discussion_r541077903", "createdAt": "2020-12-11T16:40:28Z", "author": {"login": "jaketf"}, "path": "tools/cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -16,536 +16,151 @@\n # limitations under the License.\n \"\"\"Background Cloud Function for loading data from GCS to BigQuery.\n \"\"\"\n-import collections\n-import json\n import os\n-import pathlib\n-import re\n import time\n-from typing import Any, Deque, Dict, List, Optional, Tuple\n+from typing import Dict\n \n-import cachetools\n-import google.api_core.client_info\n-import google.api_core.exceptions\n-import google.cloud.exceptions\n-from google.cloud import bigquery, storage\n+# pylint in cloud build is being flaky about this import discovery.\n+# pylint: disable=no-name-in-module\n+from google.cloud import bigquery\n+from google.cloud import error_reporting\n+from google.cloud import storage\n \n-# https://cloud.google.com/bigquery/quotas#load_jobs\n-# 15TB per BQ load job (soft limit).\n-DEFAULT_MAX_BATCH_BYTES = str(15 * 10**12)\n-# 10,000 GCS URIs per BQ load job.\n-MAX_SOURCE_URIS_PER_LOAD = 10**4\n+from . import constants\n+from . import exceptions\n+from . import ordering\n+from . import utils\n \n-DEFAULT_EXTERNAL_TABLE_DEFINITION = {\n-    \"sourceFormat\": \"CSV\",\n-}\n+# Reuse GCP Clients across function invocations using globbals\n+# https://cloud.google.com/functions/docs/bestpractices/tips#use_global_variables_to_reuse_objects_in_future_invocations\n+# pylint: disable=global-statement\n \n-DEFAULT_JOB_LABELS = {\n-    \"component\": \"event-based-gcs-ingest\",\n-    \"cloud-function-name\": os.getenv(\"FUNCTION_NAME\"),\n-}\n+ERROR_REPORTING_CLIENT = None\n \n-BASE_LOAD_JOB_CONFIG = {\n-    \"sourceFormat\": \"CSV\",\n-    \"fieldDelimiter\": \",\",\n-    \"writeDisposition\": \"WRITE_APPEND\",\n-    \"labels\": DEFAULT_JOB_LABELS,\n-}\n+BQ_CLIENT = None\n \n-# yapf: disable\n-DEFAULT_DESTINATION_REGEX = (\n-    r\"^(?P<dataset>[\\w\\-\\._0-9]+)/\"  # dataset (required)\n-    r\"(?P<table>[\\w\\-_0-9]+)/?\"      # table name (required)\n-    r\"(?P<partition>\\$[0-9]+)?/?\"    # partition decorator (optional)\n-    r\"(?P<yyyy>[0-9]{4})?/?\"         # partition year (yyyy) (optional)\n-    r\"(?P<mm>[0-9]{2})?/?\"           # partition month (mm) (optional)\n-    r\"(?P<dd>[0-9]{2})?/?\"           # partition day (dd)  (optional)\n-    r\"(?P<hh>[0-9]{2})?/?\"           # partition hour (hh) (optional)\n-    r\"(?P<batch>[\\w\\-_0-9]+)?/\"      # batch id (optional)\n-)\n-# yapf: enable\n-\n-# Will wait up to this polling for errors before exiting\n-# This is to check if job fail quickly, not to assert it succeed.\n-# This may not be honored if longer than cloud function timeout.\n-# https://cloud.google.com/functions/docs/concepts/exec#timeout\n-# One might consider lowering this to 1-2 seconds to lower the\n-# upper bound of expected execution time to stay within the free tier.\n-# https://cloud.google.com/functions/pricing#free_tier\n-WAIT_FOR_JOB_SECONDS = int(os.getenv(\"WAIT_FOR_JOB_SECONDS\", \"5\"))\n-\n-# Use caution when lowering the job polling rate.\n-# Keep in mind that many concurrent executions of this cloud function should not\n-# violate the 300 concurrent requests or 100 request per second.\n-# https://cloud.google.com/bigquery/quotas#all_api_requests\n-JOB_POLL_INTERVAL_SECONDS = 1\n-\n-SUCCESS_FILENAME = os.getenv(\"SUCCESS_FILENAME\", \"_SUCCESS\")\n-\n-CLIENT_INFO = google.api_core.client_info.ClientInfo(\n-    user_agent=\"google-pso-tool/bq-severless-loader\")\n-\n-DEFAULT_JOB_PREFIX = \"gcf-ingest-\"\n+GCS_CLIENT = None\n \n \n def main(event: Dict, context):  # pylint: disable=unused-argument\n     \"\"\"entry point for background cloud function for event driven GCS to\n     BigQuery ingest.\"\"\"\n-    # pylint: disable=too-many-locals\n-    # Set by Cloud Function Execution Environment\n-    # https://cloud.google.com/functions/docs/env-var\n-    destination_regex = os.getenv(\"DESTINATION_REGEX\",\n-                                  DEFAULT_DESTINATION_REGEX)\n-    dest_re = re.compile(destination_regex)\n-\n-    bucket_id, object_id = parse_notification(event)\n-\n-    # Exit eagerly if not a success file.\n-    # we can improve this with pub/sub message filtering once it supports\n-    # a hasSuffix filter function (we can filter on hasSuffix successfile name)\n-    #  https://cloud.google.com/pubsub/docs/filtering\n-    if not object_id.endswith(f\"/{SUCCESS_FILENAME}\"):\n-        print(\n-            f\"No-op. This notification was not for a {SUCCESS_FILENAME} file.\")\n-        return\n-\n-    prefix_to_load = removesuffix(object_id, SUCCESS_FILENAME)\n-    gsurl = f\"gs://{bucket_id}/{prefix_to_load}\"\n-    gcs_client = storage.Client(client_info=CLIENT_INFO)\n-    project = gcs_client.project\n-    bkt = cached_get_bucket(gcs_client, bucket_id)\n-    success_blob: storage.Blob = bkt.blob(object_id)\n-    handle_duplicate_notification(bkt, success_blob, gsurl)\n-\n-    destination_match = dest_re.match(object_id)\n-    if not destination_match:\n-        raise RuntimeError(f\"Object ID {object_id} did not match regex:\"\n-                           f\" {destination_regex}\")\n-    destination_details = destination_match.groupdict()\n-    try:\n-        dataset = destination_details['dataset']\n-        table = destination_details['table']\n-    except KeyError:\n-        raise RuntimeError(\n-            f\"Object ID {object_id} did not match dataset and table in regex:\"\n-            f\" {destination_regex}\") from KeyError\n-    partition = destination_details.get('partition')\n-    year, month, day, hour = (\n-        destination_details.get(key, \"\") for key in ('yyyy', 'mm', 'dd', 'hh'))\n-    part_list = (year, month, day, hour)\n-    if not partition and any(part_list):\n-        partition = '$' + ''.join(part_list)\n-    batch_id = destination_details.get('batch')\n-    labels = DEFAULT_JOB_LABELS\n-    labels[\"bucket\"] = bucket_id\n-\n-    if batch_id:\n-        labels[\"batch-id\"] = batch_id\n-\n-    if partition:\n-        dest_table_ref = bigquery.TableReference.from_string(\n-            f\"{dataset}.{table}{partition}\", default_project=project)\n-    else:\n-        dest_table_ref = bigquery.TableReference.from_string(\n-            f\"{dataset}.{table}\", default_project=project)\n-\n-    default_query_config = bigquery.QueryJobConfig()\n-    default_query_config.use_legacy_sql = False\n-    default_query_config.labels = labels\n-    bq_client = bigquery.Client(\n-        client_info=CLIENT_INFO,\n-        default_query_job_config=default_query_config)\n-\n-    print(f\"looking for {gsurl}_config/bq_transform.sql\")\n-    external_query_sql = read_gcs_file_if_exists(\n-        gcs_client, f\"{gsurl}_config/bq_transform.sql\")\n-    print(f\"external_query_sql = {external_query_sql}\")\n-    if not external_query_sql:\n-        external_query_sql = look_for_transform_sql(gcs_client, gsurl)\n-    if external_query_sql:\n-        print(\"EXTERNAL QUERY\")\n-        external_query(gcs_client, bq_client, gsurl, external_query_sql,\n-                       dest_table_ref,\n-                       create_job_id_prefix(dest_table_ref, batch_id))\n-        return\n-\n-    print(\"LOAD_JOB\")\n-    load_batches(gcs_client, bq_client, gsurl, dest_table_ref,\n-                 create_job_id_prefix(dest_table_ref, batch_id))\n-\n-\n-def create_job_id_prefix(dest_table_ref: bigquery.TableReference,\n-                         batch_id: Optional[str]):\n-    \"\"\"Create job id prefix with a consistent naming convention.\n-    The naming conventions is as follows:\n-    gcf-ingest-<dataset_id>-<table_id>-<partition_num>-<batch_id>-\n-    Parts that are not inferrable from the GCS path with have a 'None'\n-    placeholder. This naming convention is crucial for monitoring the system.\n-    Note, gcf-ingest- can be overridden with environment variable JOB_PREFIX\n-\n-    Examples:\n-\n-    Non-partitioned Non batched tables:\n-      - gs://${BUCKET}/tpch/lineitem/_SUCCESS\n-      - gcf-ingest-tpch-lineitem-None-None-\n-    Non-partitioned batched tables:\n-      - gs://${BUCKET}/tpch/lineitem/batch000/_SUCCESS\n-      - gcf-ingest-tpch-lineitem-None-batch000-\n-    Partitioned Batched tables:\n-      - gs://${BUCKET}/tpch/lineitem/$20201031/batch000/_SUCCESS\n-      - gcf-ingest-tpch-lineitem-20201031-batch000-\n-    \"\"\"\n-    table_partition = dest_table_ref.table_id.split(\"$\")\n-    if len(table_partition) < 2:\n-        # If there is no partition put a None placeholder\n-        table_partition.append(\"None\")\n-    return f\"{os.getenv('JOB_PREFIX', DEFAULT_JOB_PREFIX)}\" \\\n-        f\"{dest_table_ref.dataset_id}-\" \\\n-        f\"{'-'.join(table_partition)}-\" \\\n-        f\"{batch_id}-\"\n-\n-\n-def external_query(  # pylint: disable=too-many-arguments\n-        gcs_client: storage.Client, bq_client: bigquery.Client, gsurl: str,\n-        query: str, dest_table_ref: bigquery.TableReference,\n-        job_id_prefix: str):\n-    \"\"\"Load from query over external table from GCS.\n-\n-    This hinges on a SQL query defined in GCS at _config/bq_transform.sql and\n-    an external table definition _config/external.json (otherwise will assume\n-    CSV external table)\n-    \"\"\"\n-    external_table_config = read_gcs_file_if_exists(\n-        gcs_client, f\"{gsurl}_config/external.json\")\n-    if external_table_config:\n-        external_table_def = json.loads(external_table_config)\n-    else:\n-        print(f\"Falling back to default CSV external table.\"\n-              f\" {gsurl}/_config/external.json not found.\")\n-        external_table_def = DEFAULT_EXTERNAL_TABLE_DEFINITION\n-\n-    external_table_def[\"sourceUris\"] = flatten2dlist(\n-        get_batches_for_prefix(gcs_client, gsurl))\n-    external_config = bigquery.ExternalConfig.from_api_repr(external_table_def)\n-    job_config = bigquery.QueryJobConfig(\n-        table_definitions={\"temp_ext\": external_config}, use_legacy_sql=False)\n-\n-    # Note, dest_table might include a partition decorator.\n-    rendered_query = query.format(\n-        dest_dataset=dest_table_ref.dataset_id,\n-        dest_table=dest_table_ref.table_id,\n-    )\n-\n-    job: bigquery.QueryJob = bq_client.query(\n-        rendered_query,\n-        job_config=job_config,\n-        job_id_prefix=job_id_prefix,\n-    )\n-\n-    print(f\"started asynchronous query job: {job.job_id}\")\n-\n-    start_poll_for_errors = time.monotonic()\n-    # Check if job failed quickly\n-    while time.monotonic() - start_poll_for_errors < WAIT_FOR_JOB_SECONDS:\n-        job.reload()\n-        if job.errors:\n-            raise RuntimeError(\n-                f\"query job {job.job_id} failed quickly: {job.errors}\")\n-        time.sleep(JOB_POLL_INTERVAL_SECONDS)\n-\n-\n-def flatten2dlist(arr: List[List[Any]]) -> List[Any]:\n-    \"\"\"Flatten list of lists to flat list of elements\"\"\"\n-    return [j for i in arr for j in i]\n-\n-\n-def load_batches(gcs_client, bq_client, gsurl, dest_table_ref, job_id_prefix):\n-    \"\"\"orchestrate 1 or more load jobs based on number of URIs and total byte\n-    size of objects at gsurl\"\"\"\n-    batches = get_batches_for_prefix(gcs_client, gsurl)\n-    load_config = construct_load_job_config(gcs_client, gsurl)\n-    load_config.labels = DEFAULT_JOB_LABELS\n-    batch_count = len(batches)\n-\n-    jobs: List[bigquery.LoadJob] = []\n-    for batch_num, batch in enumerate(batches):\n-        print(load_config.to_api_repr())\n-        job: bigquery.LoadJob = bq_client.load_table_from_uri(\n-            batch,\n-            dest_table_ref,\n-            job_config=load_config,\n-            job_id_prefix=f\"{job_id_prefix}{batch_num}-of-{batch_count}-\",\n-        )\n-\n-        print(f\"started asyncronous bigquery load job with id: {job.job_id} for\"\n-              f\" {gsurl}\")\n-        jobs.append(job)\n-\n-    start_poll_for_errors = time.monotonic()\n-    # Check if job failed quickly\n-    while time.monotonic() - start_poll_for_errors < WAIT_FOR_JOB_SECONDS:\n-        # Check if job failed quickly\n-        for job in jobs:\n-            job.reload()\n-            if job.errors:\n-                raise RuntimeError(\n-                    f\"load job {job.job_id} failed quickly: {job.errors}\")\n-        time.sleep(JOB_POLL_INTERVAL_SECONDS)\n-\n-\n-def handle_duplicate_notification(bkt: storage.Bucket,\n-                                  success_blob: storage.Blob, gsurl: str):\n-    \"\"\"\n-    Need to handle potential duplicate Pub/Sub notifications.\n-    To achieve this we will drop an empty \"claimed\" file that indicates\n-    an invocation of this cloud function has picked up the success file\n-    with a certain creation timestamp. This will support republishing the\n-    success file as a mechanism of re-running the ingestion while avoiding\n-    duplicate ingestion due to multiple Pub/Sub messages for a success file\n-    with the same creation time.\n-    \"\"\"\n-    success_blob.reload()\n-    success_created_unix_timestamp = success_blob.time_created.timestamp()\n-\n-    claim_blob: storage.Blob = bkt.blob(\n-        success_blob.name.replace(\n-            SUCCESS_FILENAME,\n-            f\"_claimed_{success_created_unix_timestamp}\")\n-    )\n     try:\n-        claim_blob.upload_from_string(\"\", if_generation_match=0)\n-    except google.api_core.exceptions.PreconditionFailed as err:\n-        raise RuntimeError(\n-            f\"The prefix {gsurl} appears to already have been claimed for \"\n-            f\"{gsurl}{SUCCESS_FILENAME} with created timestamp\"\n-            f\"{success_created_unix_timestamp}.\"\n-            \"This means that another invocation of this cloud function has\"\n-            \"claimed the ingestion of this batch.\"\n-            \"This may be due to a rare duplicate delivery of the Pub/Sub \"\n-            \"storage notification.\") from err\n-\n-\n-def _get_parent_config_file(storage_client, config_filename, bucket, path):\n-    config_dir_name = \"_config\"\n-    parent_path = pathlib.Path(path).parent\n-    config_path = parent_path / config_dir_name / config_filename\n-    return read_gcs_file_if_exists(storage_client,\n-                                   f\"gs://{bucket}/{config_path}\")\n-\n-\n-def look_for_transform_sql(storage_client: storage.Client,\n-                           gsurl: str) -> Optional[str]:\n-    \"\"\"look in parent directories for _config/bq_transform.sql\"\"\"\n-    config_filename = \"bq_transform.sql\"\n-    blob: storage.Blob = storage.Blob.from_string(gsurl)\n-    bucket_name = blob.bucket.name\n-    obj_path = blob.name\n-    parts = removesuffix(obj_path, \"/\").split(\"/\")\n-\n-    def _get_parent_query(path):\n-        return _get_parent_config_file(storage_client, config_filename,\n-                                       bucket_name, path)\n-\n-    config = None\n-    while parts:\n-        if config:\n-            return config\n-        config = _get_parent_query(\"/\".join(parts))\n-        parts.pop()\n-    return config\n+        function_start_time = time.monotonic()\n+        # pylint: disable=too-many-locals\n+\n+        bucket_id, object_id = utils.parse_notification(event)\n+\n+        basename_object_id = os.path.basename(object_id)\n+\n+        # Exit eagerly if this is not a file to take action on\n+        # (e.g. a data, config, or lock file)\n+        if basename_object_id not in constants.ACTION_FILENAMES:\n+            action_filenames = constants.ACTION_FILENAMES\n+            if constants.START_BACKFILL_FILENAME is None:\n+                action_filenames.remove(None)\n+            print(f\"No-op. This notification was not for a\"\n+                  f\"{action_filenames} file.\")\n+            return\n+\n+        gcs_client = lazy_gcs_client()\n+        bq_client = lazy_bq_client()\n+        table_ref, batch = utils.gcs_path_to_table_ref_and_batch(object_id)\n+\n+        enforce_ordering = (constants.ORDER_PER_TABLE\n+                            or utils.look_for_config_in_parents(\n+                                gcs_client, f\"gs://{bucket_id}/{object_id}\",\n+                                \"ORDERME\") is not None)\n+\n+        bkt: storage.Bucket = utils.cached_get_bucket(gcs_client, bucket_id)\n+        event_blob: storage.Blob = bkt.blob(object_id)\n+\n+        if enforce_ordering:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2218212d585eb078b5872e110608ad4cbf6aa5ba"}, "originalPosition": 386}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5ODEyNjAxOnYy", "diffSide": "RIGHT", "path": "tools/cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/ordering.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxNjo0MzozOVrOIEA52A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxNjo0MzozOVrOIEA52A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTA4MDAyNA==", "bodyText": "Replace old module doc string", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/205#discussion_r541080024", "createdAt": "2020-12-11T16:43:39Z", "author": {"login": "jaketf"}, "path": "tools/cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/ordering.py", "diffHunk": "@@ -0,0 +1,273 @@\n+# Copyright 2020 Google LLC.\n+# This software is provided as-is, without warranty or representation\n+# for any use or purpose.\n+# Your use of it is subject to your agreement with Google.\n+\n+# Licensed under the Apache License, Version 2.0 (the 'License');\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+# http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an 'AS IS' BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Background Cloud Function for loading data from GCS to BigQuery.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2218212d585eb078b5872e110608ad4cbf6aa5ba"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5ODUxMzEzOnYy", "diffSide": "RIGHT", "path": "tools/cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/utils.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxODoxODoxMlrOIEEcjA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxODoxODoxMlrOIEEcjA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTEzODA2MA==", "bodyText": "This seems like a premature optimization for reading files that might lead to difficult to debug scenarios due to local cache when reading _bqlock.\nI think it's fine to leave the cache for lookup bucket.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/205#discussion_r541138060", "createdAt": "2020-12-11T18:18:12Z", "author": {"login": "jaketf"}, "path": "tools/cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/utils.py", "diffHunk": "@@ -0,0 +1,721 @@\n+# Copyright 2020 Google LLC.\n+# This software is provided as-is, without warranty or representation\n+# for any use or purpose.\n+# Your use of it is subject to your agreement with Google.\n+\n+# Licensed under the Apache License, Version 2.0 (the 'License');\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+# http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an 'AS IS' BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Contains utility methods used by the BQIngest process\n+\"\"\"\n+import collections\n+import collections.abc\n+import copy\n+import fnmatch\n+import json\n+import os\n+import pathlib\n+import time\n+import uuid\n+from typing import Any, Deque, Dict, List, Optional, Tuple, Union\n+\n+import cachetools\n+import google.api_core\n+import google.api_core.client_info\n+import google.api_core.exceptions\n+import google.cloud.exceptions\n+# pylint in cloud build is being flaky about this import discovery.\n+from google.cloud import bigquery\n+from google.cloud import storage\n+\n+from . import constants  # pylint: disable=no-name-in-module\n+from . import exceptions  # pylint: disable=no-name-in-module\n+\n+\n+def external_query(  # pylint: disable=too-many-arguments\n+        gcs_client: storage.Client, bq_client: bigquery.Client, gsurl: str,\n+        query: str, dest_table_ref: bigquery.TableReference, job_id: str):\n+    \"\"\"Load from query over external table from GCS.\n+\n+    This hinges on a SQL query defined in GCS at _config/*.sql and\n+    an external table definition _config/external.json (otherwise will assume\n+    CSV external table)\n+    \"\"\"\n+    external_table_config = read_gcs_file_if_exists(\n+        gcs_client, f\"{gsurl}_config/external.json\")\n+    if not external_table_config:\n+        external_table_config = look_for_config_in_parents(\n+            gcs_client, gsurl, \"external.json\")\n+    if external_table_config:\n+        external_table_def = json.loads(external_table_config)\n+    else:\n+        print(f\" {gsurl}_config/external.json not found in parents of {gsurl}.\"\n+              \"Falling back to default PARQUET external table:\\n\"\n+              f\"{json.dumps(constants.DEFAULT_EXTERNAL_TABLE_DEFINITION)}\")\n+        external_table_def = constants.DEFAULT_EXTERNAL_TABLE_DEFINITION\n+\n+    # This may cause an issue if >10,000 files. however, we\n+    external_table_def[\"sourceUris\"] = flatten2dlist(\n+        get_batches_for_prefix(gcs_client, gsurl))\n+    print(f\"external table def = {json.dumps(external_table_config, indent=2)}\")\n+    external_config = bigquery.ExternalConfig.from_api_repr(external_table_def)\n+    job_config = bigquery.QueryJobConfig(\n+        table_definitions={\"temp_ext\": external_config}, use_legacy_sql=False)\n+\n+    # Note, dest_table might include a partition decorator.\n+    rendered_query = query.format(\n+        dest_dataset=f\"`{dest_table_ref.project}`.{dest_table_ref.dataset_id}\",\n+        dest_table=dest_table_ref.table_id,\n+    )\n+\n+    job: bigquery.QueryJob = bq_client.query(rendered_query,\n+                                             job_config=job_config,\n+                                             job_id=job_id,\n+                                             project=os.getenv(\n+                                                 \"BQ_PROJECT\",\n+                                                 bq_client.project))\n+\n+    print(f\"started asynchronous query job: {job.job_id}\")\n+\n+    start_poll_for_errors = time.monotonic()\n+    # Check if job failed quickly\n+    while time.monotonic(\n+    ) - start_poll_for_errors < constants.WAIT_FOR_JOB_SECONDS:\n+        job.reload(client=bq_client)\n+        if job.errors:\n+            raise exceptions.BigQueryJobFailure(\n+                f\"query job {job.job_id} failed quickly: {job.errors}\")\n+        if job.state == \"DONE\":\n+            return\n+        time.sleep(constants.JOB_POLL_INTERVAL_SECONDS)\n+\n+\n+def flatten2dlist(arr: List[List[Any]]) -> List[Any]:\n+    \"\"\"Flatten list of lists to flat list of elements\"\"\"\n+    return [j for i in arr for j in i]\n+\n+\n+def load_batches(gcs_client, bq_client, gsurl, dest_table_ref, job_id):\n+    \"\"\"orchestrate 1 or more load jobs based on number of URIs and total byte\n+    size of objects at gsurl\"\"\"\n+    batches = get_batches_for_prefix(gcs_client, gsurl)\n+    load_config = construct_load_job_config(gcs_client, gsurl)\n+    load_config.labels = constants.DEFAULT_JOB_LABELS\n+\n+    jobs: List[bigquery.LoadJob] = []\n+    for batch in batches:\n+        print(load_config.to_api_repr())\n+        job: bigquery.LoadJob = bq_client.load_table_from_uri(\n+            batch, dest_table_ref, job_config=load_config, job_id=job_id)\n+\n+        print(f\"started asyncronous bigquery load job with id: {job.job_id} for\"\n+              f\" {gsurl}\")\n+        jobs.append(job)\n+\n+    start_poll_for_errors = time.monotonic()\n+    # Check if job failed quickly\n+    while time.monotonic(\n+    ) - start_poll_for_errors < constants.WAIT_FOR_JOB_SECONDS:\n+        # Check if job failed quickly\n+        for job in jobs:\n+            job.reload(client=bq_client)\n+            if job.errors:\n+                raise exceptions.BigQueryJobFailure(\n+                    f\"load job {job.job_id} failed quickly: {job.errors}\")\n+        time.sleep(constants.JOB_POLL_INTERVAL_SECONDS)\n+\n+\n+def _get_parent_config_file(storage_client, config_filename, bucket, path):\n+    bkt = storage_client.lookup_bucket(bucket)\n+    config_dir_name = \"_config\"\n+    parent_path = pathlib.Path(path).parent\n+    config_path = parent_path / config_dir_name\n+    config_file_path = config_path / config_filename\n+    # Handle wild card (to support bq transform sql with different names).\n+    if \"*\" in config_filename:\n+        matches: List[storage.Blob] = list(\n+            filter(lambda blob: fnmatch.fnmatch(blob.name, config_filename),\n+                   bkt.list_blobs(prefix=config_path)))\n+        if matches:\n+            if len(matches) > 1:\n+                raise RuntimeError(\n+                    f\"Multiple matches for gs://{bucket}/{config_file_path}\")\n+            return read_gcs_file_if_exists(storage_client,\n+                                           f\"gs://{bucket}/{matches[0].name}\")\n+        return None\n+    return read_gcs_file_if_exists(storage_client,\n+                                   f\"gs://{bucket}/{config_file_path}\")\n+\n+\n+def look_for_config_in_parents(storage_client: storage.Client, gsurl: str,\n+                               config_filename: str) -> Optional[str]:\n+    \"\"\"look in parent directories for _config/config_filename\"\"\"\n+    blob: storage.Blob = storage.Blob.from_string(gsurl)\n+    bucket_name = blob.bucket.name\n+    obj_path = blob.name\n+    parts = removesuffix(obj_path, \"/\").split(\"/\")\n+\n+    def _get_parent_config(path):\n+        return _get_parent_config_file(storage_client, config_filename,\n+                                       bucket_name, path)\n+\n+    config = None\n+    while parts:\n+        if config:\n+            return config\n+        config = _get_parent_config(\"/\".join(parts))\n+        parts.pop()\n+    return config\n+\n+\n+def construct_load_job_config(storage_client: storage.Client,\n+                              gsurl: str) -> bigquery.LoadJobConfig:\n+    \"\"\"\n+    merge dictionaries for loadjob.json configs in parent directories.\n+    The configs closest to gsurl should take precedence.\n+    \"\"\"\n+    config_filename = \"load.json\"\n+    blob: storage.Blob = storage.Blob.from_string(gsurl)\n+    bucket_name = blob.bucket.name\n+    obj_path = blob.name\n+    parts = removesuffix(obj_path, \"/\").split(\"/\")\n+\n+    def _get_parent_config(path):\n+        return _get_parent_config_file(storage_client, config_filename,\n+                                       bucket_name, path)\n+\n+    config_q: Deque[Dict[str, Any]] = collections.deque()\n+    config_q.append(constants.BASE_LOAD_JOB_CONFIG)\n+    while parts:\n+        config = _get_parent_config(\"/\".join(parts))\n+        if config:\n+            config_q.append(json.loads(config))\n+        parts.pop()\n+\n+    merged_config: Dict = {}\n+    while config_q:\n+        recursive_update(merged_config, config_q.popleft(), in_place=True)\n+    print(f\"merged_config: {merged_config}\")\n+    return bigquery.LoadJobConfig.from_api_repr({\"load\": merged_config})\n+\n+\n+def get_batches_for_prefix(\n+        gcs_client: storage.Client,\n+        prefix_path: str,\n+        ignore_subprefix=\"_config/\",\n+        ignore_file=constants.SUCCESS_FILENAME) -> List[List[str]]:\n+    \"\"\"\n+    This function creates batches of GCS uris for a given prefix.\n+    This prefix could be a table prefix or a partition prefix inside a\n+    table prefix.\n+    returns an Array of their batches\n+    (one batch has an array of multiple GCS uris)\n+    \"\"\"\n+    batches = []\n+    blob: storage.Blob = storage.Blob.from_string(prefix_path)\n+    bucket_name = blob.bucket.name\n+    prefix_name = blob.name\n+\n+    prefix_filter = f\"{prefix_name}\"\n+    bucket = cached_get_bucket(gcs_client, bucket_name)\n+    blobs = list(bucket.list_blobs(prefix=prefix_filter, delimiter=\"/\"))\n+\n+    cumulative_bytes = 0\n+    max_batch_size = int(\n+        os.getenv(\"MAX_BATCH_BYTES\", constants.DEFAULT_MAX_BATCH_BYTES))\n+    batch: List[str] = []\n+    for blob in blobs:\n+        # API returns root prefix also. Which should be ignored.\n+        # Similarly, the _SUCCESS file should be ignored.\n+        # Finally, anything in the _config/ prefix should be ignored.\n+        if (blob.name\n+                not in {f\"{prefix_name}/\", f\"{prefix_name}/{ignore_file}\"}\n+                or blob.name.startswith(f\"{prefix_name}/{ignore_subprefix}\")):\n+            if blob.size == 0:  # ignore empty files\n+                print(f\"ignoring empty file: gs://{bucket}/{blob.name}\")\n+                continue\n+            cumulative_bytes += blob.size\n+\n+            # keep adding until we reach threshold\n+            if cumulative_bytes <= max_batch_size or len(\n+                    batch) > constants.MAX_SOURCE_URIS_PER_LOAD:\n+                batch.append(f\"gs://{bucket_name}/{blob.name}\")\n+            else:\n+                batches.append(batch.copy())\n+                batch.clear()\n+                batch.append(f\"gs://{bucket_name}/{blob.name}\")\n+                cumulative_bytes = blob.size\n+\n+    # pick up remaining files in the final batch\n+    if len(batch) > 0:\n+        batches.append(batch.copy())\n+        batch.clear()\n+\n+    if len(batches) > 1:\n+        print(f\"split into {len(batches)} batches.\")\n+    elif len(batches) < 1:\n+        raise google.api_core.exceptions.NotFound(\n+            f\"No files to load at {prefix_path}!\")\n+    return batches\n+\n+\n+def parse_notification(notification: dict) -> Tuple[str, str]:\n+    \"\"\"valdiates notification payload\n+    Args:\n+        notification(dict): Pub/Sub Storage Notification\n+        https://cloud.google.com/storage/docs/pubsub-notifications\n+        Or Cloud Functions direct trigger\n+        https://cloud.google.com/functions/docs/tutorials/storage\n+        with notification schema\n+        https://cloud.google.com/storage/docs/json_api/v1/objects#resource\n+    Returns:\n+        tuple of bucketId and objectId attributes\n+    Raises:\n+        KeyError if the input notification does not contain the expected\n+        attributes.\n+    \"\"\"\n+    if notification.get(\"kind\") == \"storage#object\":\n+        # notification is GCS Object reosource from Cloud Functions trigger\n+        # https://cloud.google.com/storage/docs/json_api/v1/objects#resource\n+        return notification[\"bucket\"], notification[\"name\"]\n+    if notification.get(\"attributes\"):\n+        # notification is Pub/Sub message.\n+        try:\n+            attributes = notification[\"attributes\"]\n+            return attributes[\"bucketId\"], attributes[\"objectId\"]\n+        except KeyError:\n+            raise exceptions.UnexpectedTriggerException(\n+                \"Issue with Pub/Sub message, did not contain expected\"\n+                f\"attributes: 'bucketId' and 'objectId': {notification}\"\n+            ) from KeyError\n+    raise exceptions.UnexpectedTriggerException(\n+        \"Cloud Function received unexpected trigger:\\n\"\n+        f\"{notification}\\n\"\n+        \"This function only supports direct Cloud Functions\"\n+        \"Background Triggers or Pub/Sub storage notificaitons\"\n+        \"as described in the following links:\\n\"\n+        \"https://cloud.google.com/storage/docs/pubsub-notifications\\n\"\n+        \"https://cloud.google.com/functions/docs/tutorials/storage\")\n+\n+\n+# cache lookups against GCS API for 1 second as buckets / objects have update\n+# limit of once per second and we might do several of the same lookup during\n+# the functions lifetime. This should improve performance by eliminating\n+# unnecessary API calls. The lookups on bucket and objects in this function\n+# should not be changing during the function's lifetime as this would lead to\n+# non-deterministic results with or without this cache.\n+# https://cloud.google.com/storage/quotas\n+@cachetools.cached(cachetools.TTLCache(maxsize=1024, ttl=1))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2218212d585eb078b5872e110608ad4cbf6aa5ba"}, "originalPosition": 316}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5ODUzMzM5OnYy", "diffSide": "RIGHT", "path": "tools/cloud_functions/gcs_event_based_ingest/ORDERING.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxODoyMzo0M1rOIEEong==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxODoyMzo0M1rOIEEong==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTE0MTE1MA==", "bodyText": "Add test for this behavior.", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/205#discussion_r541141150", "createdAt": "2020-12-11T18:23:43Z", "author": {"login": "jaketf"}, "path": "tools/cloud_functions/gcs_event_based_ingest/ORDERING.md", "diffHunk": "@@ -0,0 +1,139 @@\n+# Ordering Batches\n+There are use cases where it is important for incremental batches get\n+applied in order rather than as soon as they are uploaded to GCS (which is the\n+default behavior of this solution).\n+1. When using External Query that performs DML other than insert only.\n+(e.g. an `UPDATE` assumes that prior batches have already been committed) \n+1. To ensure that there are not time gaps in the data (e.g. ensure that\n+2020/01/02 data is not committed to BigQuery before 2020/01/01, or similarly\n+that 00 hour is ingested before the 01 hour, etc.)\n+\n+This Cloud Function supports serializing the submission of ingestion jobs to \n+BigQuery by using Google Cloud Storage's consistency guarantees to provide a\n+pessimistic lock on a table to prevent concurrent jobs and\n+[GCS Object.list](https://cloud.google.com/storage/docs/json_api/v1/objects/list)\n+lexicographic sorting of results to providing ordering gurantees.\n+The solution involves a table level `_backlog/` directory to keep track\n+of success files whose batches have not yet been committed to BigQuery and\n+a table level `_bqlock` file to keep track of what job is currently ingesting to\n+that table. This way we can make our Cloud Function idempotent by having all the\n+state stored in GCS so we can safely retrigger it to skirt the Cloud Functions\n+timeout.\n+\n+## Assumptions\n+This ordering solution assumes that you want to apply batches in lexicographic\n+order. This is usually the case because path names usually contain some sort of\n+date / hour information.\n+\n+## Enabling Ordering\n+### Environment Variable\n+Ordering can be enabled at the function level by setting the `ORDER_PER_TABLE`\n+environment variable to `\"True\"`.\n+### Config File\n+Ordering can be configured at any level of your naming convention (e.g. dataset\n+table or some sub-path) by placing a `_config/ORDERME` file. This can be helpful\n+in scenarios where your historical load can be processed safely in parallel but\n+incrementals must be ordered.\n+For example:\n+```text\n+gs://${BUCKET}/${DATASET}/${TABLE}/historical/_config/load.json\n+gs://${BUCKET}/${DATASET}/${TABLE}/incremental/_config/external.json\n+gs://${BUCKET}/${DATASET}/${TABLE}/incremental/_config/bq_transform.sql\n+gs://${BUCKET}/${DATASET}/${TABLE}/incremental/_config/ORDERME\n+```\n+\n+## Dealing With Out-of-Order Publishing to GCS During Historical Load", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2218212d585eb078b5872e110608ad4cbf6aa5ba"}, "originalPosition": 45}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2937, "cost": 1, "resetAt": "2021-11-12T19:05:54Z"}}}