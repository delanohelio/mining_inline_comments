{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY4MTE4NjY1", "number": 3883, "title": "[RW-5035] Documenting how to randomize a VCF sample and upload to BQ", "bodyText": "Not meant to merge", "createdAt": "2020-08-14T18:08:53Z", "url": "https://github.com/all-of-us/workbench/pull/3883", "merged": true, "mergeCommit": {"oid": "8bb098887cdbedd18d598f7e5ec016f7d6729009"}, "closed": true, "closedAt": "2020-08-20T15:53:25Z", "author": {"login": "ericsong"}, "timelineItems": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc-4oLEgH2gAyNDY4MTE4NjY1OjJjYWI3YjA2NzZmNzU0NWJiNzIyYzRkNzIwYTNmZTk5MzNkNTA0NWE=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdAxqb0AH2gAyNDY4MTE4NjY1OjljYTY4N2RkNjM3ZGFiNTdmZDU5MTdiMTE5ZjNmMmU2MWI1ZjBlNzQ=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "2cab7b0676f7545bb722c4d720a3fe9933d5045a", "author": {"user": {"login": "ericsong", "name": "Eric Song"}}, "url": "https://github.com/all-of-us/workbench/commit/2cab7b0676f7545bb722c4d720a3fe9933d5045a", "committedDate": "2020-08-14T18:07:57Z", "message": "documenting how to randomize a VCF sample and upload to BQ"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY5ODc4NjI1", "url": "https://github.com/all-of-us/workbench/pull/3883#pullrequestreview-469878625", "createdAt": "2020-08-18T22:41:14Z", "commit": {"oid": "2cab7b0676f7545bb722c4d720a3fe9933d5045a"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cb9bc0940227c0fa49ccded9e1115602ee78ff6e", "author": {"user": {"login": "ericsong", "name": "Eric Song"}}, "url": "https://github.com/all-of-us/workbench/commit/cb9bc0940227c0fa49ccded9e1115602ee78ff6e", "committedDate": "2020-08-19T22:29:21Z", "message": "convert to readme and add profiling"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "20e67aa7fde324073041ae335ac21d49c6525d92", "author": {"user": {"login": "ericsong", "name": "Eric Song"}}, "url": "https://github.com/all-of-us/workbench/commit/20e67aa7fde324073041ae335ac21d49c6525d92", "committedDate": "2020-08-19T22:33:57Z", "message": "typos"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7369aeb03fe6bd4f81ce4e3585c43011ddcc58d8", "author": {"user": {"login": "ericsong", "name": "Eric Song"}}, "url": "https://github.com/all-of-us/workbench/commit/7369aeb03fe6bd4f81ce4e3585c43011ddcc58d8", "committedDate": "2020-08-19T22:37:39Z", "message": "add profiling specs"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDcxMDE0Mjk1", "url": "https://github.com/all-of-us/workbench/pull/3883#pullrequestreview-471014295", "createdAt": "2020-08-19T22:44:32Z", "commit": {"oid": "7369aeb03fe6bd4f81ce4e3585c43011ddcc58d8"}, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQyMjo0NDozMlrOHDeB3Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQyMjo0NDo0OVrOHDeCsw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzM5OTc3Mw==", "bodyText": "Adding a direct link here to the Java file in the gatk branch would be good", "url": "https://github.com/all-of-us/workbench/pull/3883#discussion_r473399773", "createdAt": "2020-08-19T22:44:32Z", "author": {"login": "calbach"}, "path": "api/genomics/docs/uploading_test_microarray_data.md", "diffHunk": "@@ -0,0 +1,73 @@\n+## Creating a randomized microarray dataset and uploading to BigQuery in test environment\n+- The following commands are not meant to be run as is. Treat it more as documentation of what I ran to get the data uploaded.\n+- All profiling was done on a laptop with an Intel Core i7-9850H CPU @ 2.60GHz, 32GB RAM, and an SSD\n+### Generate Randomized VCF samples\n+- Run from workbench/api  \n+`./project.rb randomize-vcf --vcf ~/broad/variantstore/NA12878_204126160130_R01C01.vcf --number-of-copies 1000 --output-path /mnt/genomics/randomized1000.vcf`\n+\n+| Number of Samples | Clock time | Output file size |\n+| ----------------  | ---------- | ---------------- |\n+|1 | 53s | 1.4 GB |\n+|10|1m 26s|2.3 GB|\n+|100|6m 38s| 11 GB|\n+|500|27m|49 GB|\n+|1,000|59m|95 GB|\n+|10,000**|~ 10h|~ 1 TB|\n+|100,000**|~ 100h|~ 10 TB|\n+** Estimates assuming linear growth\n+\n+### Compress VCF file \n+- `bgzip -c randomized1000.vcf > randomized1000.vcf.gz`\n+\n+| Number of Samples | Clock time | Compression |\n+| ----------------- | ---------- | ----------- |\n+|100|52s|11 G -> 544 M|\n+|500|4m 18s|49 G -> 1.3 G|\n+|1000|5m 44s|95G -> 2.1 G|\n+\n+### Index VCF file\n+- `bcftools index randomized1000.vcf.gz`\n+\n+| Number of Samples | Clock time |\n+| ----------------- | ---------- |\n+|100|20s|\n+|500|1m 20s|\n+|1000|2m 40s|\n+\n+### Extracting a single sample VCF from multi-sample VCF\n+- `bcftools view -s 204126160130_R01C01.908 randomized1000.vcf.gz > 1000_sample.vcf`\n+\n+| Number of Sample | Clock time |\n+| ---------------  | ---------- |\n+|100|3m 16s|\n+|500|14m|\n+|1,000|28m|\n+|10,000**|~ 4h 40m|\n+|100,000**|~ 46h 40m|\n+** Estimates assuming linear growth\n+\n+### Generate ingest files and upload to GCS\n+```\n+  # Run from variantstore repo, https://github.com/broadinstitute/variantstore\n+  # rm/cp *.tsv is a hack I used to clean up already uploaded files while running the code in a loop\n+  rm *.tsv\n+  ~/broad/gatk/gatk CreateArrayIngestFiles --sample-id 908 -V 1000_sample.vcf --probe-info-file probe_info.csv --ref-version 37\n+  gsutil cp *.tsv gs://all-of-us-workbench-test-genomics/eric/import/3/ready/\n+```\n+- Every import/# folder can only contain up to 4k samples\n+- CreateArrayIngestFiles takes ~1m 20s and operates 1 sample at a time", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7369aeb03fe6bd4f81ce4e3585c43011ddcc58d8"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzM5OTk4Nw==", "bodyText": "Adding  a link here to this shell script would be great.", "url": "https://github.com/all-of-us/workbench/pull/3883#discussion_r473399987", "createdAt": "2020-08-19T22:44:49Z", "author": {"login": "calbach"}, "path": "api/genomics/docs/uploading_test_microarray_data.md", "diffHunk": "@@ -0,0 +1,73 @@\n+## Creating a randomized microarray dataset and uploading to BigQuery in test environment\n+- The following commands are not meant to be run as is. Treat it more as documentation of what I ran to get the data uploaded.\n+- All profiling was done on a laptop with an Intel Core i7-9850H CPU @ 2.60GHz, 32GB RAM, and an SSD\n+### Generate Randomized VCF samples\n+- Run from workbench/api  \n+`./project.rb randomize-vcf --vcf ~/broad/variantstore/NA12878_204126160130_R01C01.vcf --number-of-copies 1000 --output-path /mnt/genomics/randomized1000.vcf`\n+\n+| Number of Samples | Clock time | Output file size |\n+| ----------------  | ---------- | ---------------- |\n+|1 | 53s | 1.4 GB |\n+|10|1m 26s|2.3 GB|\n+|100|6m 38s| 11 GB|\n+|500|27m|49 GB|\n+|1,000|59m|95 GB|\n+|10,000**|~ 10h|~ 1 TB|\n+|100,000**|~ 100h|~ 10 TB|\n+** Estimates assuming linear growth\n+\n+### Compress VCF file \n+- `bgzip -c randomized1000.vcf > randomized1000.vcf.gz`\n+\n+| Number of Samples | Clock time | Compression |\n+| ----------------- | ---------- | ----------- |\n+|100|52s|11 G -> 544 M|\n+|500|4m 18s|49 G -> 1.3 G|\n+|1000|5m 44s|95G -> 2.1 G|\n+\n+### Index VCF file\n+- `bcftools index randomized1000.vcf.gz`\n+\n+| Number of Samples | Clock time |\n+| ----------------- | ---------- |\n+|100|20s|\n+|500|1m 20s|\n+|1000|2m 40s|\n+\n+### Extracting a single sample VCF from multi-sample VCF\n+- `bcftools view -s 204126160130_R01C01.908 randomized1000.vcf.gz > 1000_sample.vcf`\n+\n+| Number of Sample | Clock time |\n+| ---------------  | ---------- |\n+|100|3m 16s|\n+|500|14m|\n+|1,000|28m|\n+|10,000**|~ 4h 40m|\n+|100,000**|~ 46h 40m|\n+** Estimates assuming linear growth\n+\n+### Generate ingest files and upload to GCS\n+```\n+  # Run from variantstore repo, https://github.com/broadinstitute/variantstore\n+  # rm/cp *.tsv is a hack I used to clean up already uploaded files while running the code in a loop\n+  rm *.tsv\n+  ~/broad/gatk/gatk CreateArrayIngestFiles --sample-id 908 -V 1000_sample.vcf --probe-info-file probe_info.csv --ref-version 37\n+  gsutil cp *.tsv gs://all-of-us-workbench-test-genomics/eric/import/3/ready/\n+```\n+- Every import/# folder can only contain up to 4k samples\n+- CreateArrayIngestFiles takes ~1m 20s and operates 1 sample at a time\n+- gsutil cp depends on upload speed\n+\n+\n+### GCS -> BigQuery \n+```\n+  # Run from variantstore repo\n+  cd ingest\n+  ./bq_ingest_arrays.sh all-of-us-workbench-test microarray_data gs://all-of-us-workbench-test-genomics/eric/import 3\n+```\n+- This will have to be changed for >4k samples since a BigQuery table can only have 4k partitions", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7369aeb03fe6bd4f81ce4e3585c43011ddcc58d8"}, "originalPosition": 68}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9ca687dd637dab57fd5917b119f3f2e61b5f0e74", "author": {"user": {"login": "ericsong", "name": "Eric Song"}}, "url": "https://github.com/all-of-us/workbench/commit/9ca687dd637dab57fd5917b119f3f2e61b5f0e74", "committedDate": "2020-08-20T15:08:56Z", "message": "Add links"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4276, "cost": 1, "resetAt": "2021-10-29T17:30:11Z"}}}