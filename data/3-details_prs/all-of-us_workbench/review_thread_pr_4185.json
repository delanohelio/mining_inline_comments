{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTA2MDUxOTg0", "number": 4185, "reviewThreads": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQyMDo0NjowM1rOEvf3HQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNjo1NzozNVrOEv4y-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4MjQwNTQxOnYy", "diffSide": "RIGHT", "path": "api/src/main/java/org/pmiops/workbench/dataset/DataSetServiceImpl.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQyMDo0NjowM1rOHkhAlQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNjoxNDo0M1rOHlGQcA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODA1MTYwNQ==", "bodyText": ".tsv?", "url": "https://github.com/all-of-us/workbench/pull/4185#discussion_r508051605", "createdAt": "2020-10-19T20:46:03Z", "author": {"login": "calbach"}, "path": "api/src/main/java/org/pmiops/workbench/dataset/DataSetServiceImpl.java", "diffHunk": "@@ -815,6 +815,58 @@ private String getQualifiedColumnName(Domain currentDomain, String columnName) {\n             + \"head results.P2.assoc\");\n   }\n \n+  @Override\n+  public List<String> generateHailDemoCode(String qualifier) {\n+    final String phenotypeFilename = \"phenotypes_annotations_\" + qualifier + \".txt\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d33e62b4a74cef08c417a76bf173b4bd06a63cb"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODY2MTg3Mg==", "bodyText": "The Hail tutorial had it as \".txt\" but tsv makes sense.", "url": "https://github.com/all-of-us/workbench/pull/4185#discussion_r508661872", "createdAt": "2020-10-20T16:14:43Z", "author": {"login": "ericsong"}, "path": "api/src/main/java/org/pmiops/workbench/dataset/DataSetServiceImpl.java", "diffHunk": "@@ -815,6 +815,58 @@ private String getQualifiedColumnName(Domain currentDomain, String columnName) {\n             + \"head results.P2.assoc\");\n   }\n \n+  @Override\n+  public List<String> generateHailDemoCode(String qualifier) {\n+    final String phenotypeFilename = \"phenotypes_annotations_\" + qualifier + \".txt\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODA1MTYwNQ=="}, "originalCommit": {"oid": "5d33e62b4a74cef08c417a76bf173b4bd06a63cb"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4MjQwODUyOnYy", "diffSide": "RIGHT", "path": "api/src/main/java/org/pmiops/workbench/dataset/DataSetServiceImpl.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQyMDo0Njo1OVrOHkhCaw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQyMDo0Njo1OVrOHkhCaw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODA1MjA3NQ==", "bodyText": "Please spell it out in the variable name", "url": "https://github.com/all-of-us/workbench/pull/4185#discussion_r508052075", "createdAt": "2020-10-19T20:46:59Z", "author": {"login": "calbach"}, "path": "api/src/main/java/org/pmiops/workbench/dataset/DataSetServiceImpl.java", "diffHunk": "@@ -815,6 +815,58 @@ private String getQualifiedColumnName(Domain currentDomain, String columnName) {\n             + \"head results.P2.assoc\");\n   }\n \n+  @Override\n+  public List<String> generateHailDemoCode(String qualifier) {\n+    final String phenotypeFilename = \"phenotypes_annotations_\" + qualifier + \".txt\";\n+    final String cohortQualifier = \"cohort_\" + qualifier;\n+    final String cohortVcfFilename = cohortQualifier + \".vcf\";\n+    final String cohortMtFilename = cohortQualifier + \".mt\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d33e62b4a74cef08c417a76bf173b4bd06a63cb"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4MjQxMjA1OnYy", "diffSide": "RIGHT", "path": "api/src/main/java/org/pmiops/workbench/dataset/DataSetServiceImpl.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQyMDo0ODowMVrOHkhEfA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNjoxNDo1MlrOHlGREw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODA1MjYwNA==", "bodyText": "Why?", "url": "https://github.com/all-of-us/workbench/pull/4185#discussion_r508052604", "createdAt": "2020-10-19T20:48:01Z", "author": {"login": "calbach"}, "path": "api/src/main/java/org/pmiops/workbench/dataset/DataSetServiceImpl.java", "diffHunk": "@@ -815,6 +815,58 @@ private String getQualifiedColumnName(Domain currentDomain, String columnName) {\n             + \"head results.P2.assoc\");\n   }\n \n+  @Override\n+  public List<String> generateHailDemoCode(String qualifier) {\n+    final String phenotypeFilename = \"phenotypes_annotations_\" + qualifier + \".txt\";\n+    final String cohortQualifier = \"cohort_\" + qualifier;\n+    final String cohortVcfFilename = cohortQualifier + \".vcf\";\n+    final String cohortMtFilename = cohortQualifier + \".mt\";\n+\n+    return ImmutableList.of(\n+        \"import subprocess, os\\n\"\n+            + \"import random\\n\"\n+            + \"\\n\"\n+            + \"# Creating phenotype annotations file\\n\"\n+            + \"phenotypes_table = []\\n\"\n+            + \"for person_id in person_ids:\\n\"\n+            + \"    person_id = person_id\\n\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d33e62b4a74cef08c417a76bf173b4bd06a63cb"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODY2MjAzNQ==", "bodyText": "oops. leftover from Plink", "url": "https://github.com/all-of-us/workbench/pull/4185#discussion_r508662035", "createdAt": "2020-10-20T16:14:52Z", "author": {"login": "ericsong"}, "path": "api/src/main/java/org/pmiops/workbench/dataset/DataSetServiceImpl.java", "diffHunk": "@@ -815,6 +815,58 @@ private String getQualifiedColumnName(Domain currentDomain, String columnName) {\n             + \"head results.P2.assoc\");\n   }\n \n+  @Override\n+  public List<String> generateHailDemoCode(String qualifier) {\n+    final String phenotypeFilename = \"phenotypes_annotations_\" + qualifier + \".txt\";\n+    final String cohortQualifier = \"cohort_\" + qualifier;\n+    final String cohortVcfFilename = cohortQualifier + \".vcf\";\n+    final String cohortMtFilename = cohortQualifier + \".mt\";\n+\n+    return ImmutableList.of(\n+        \"import subprocess, os\\n\"\n+            + \"import random\\n\"\n+            + \"\\n\"\n+            + \"# Creating phenotype annotations file\\n\"\n+            + \"phenotypes_table = []\\n\"\n+            + \"for person_id in person_ids:\\n\"\n+            + \"    person_id = person_id\\n\"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODA1MjYwNA=="}, "originalCommit": {"oid": "5d33e62b4a74cef08c417a76bf173b4bd06a63cb"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4MjQxNDc4OnYy", "diffSide": "RIGHT", "path": "api/src/main/java/org/pmiops/workbench/dataset/DataSetServiceImpl.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQyMDo0ODo1MVrOHkhGKg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQyMDo0ODo1MVrOHkhGKg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODA1MzAzNA==", "bodyText": "If covariates is something the user should expect to be modifying, then please comment or pull out into a variable", "url": "https://github.com/all-of-us/workbench/pull/4185#discussion_r508053034", "createdAt": "2020-10-19T20:48:51Z", "author": {"login": "calbach"}, "path": "api/src/main/java/org/pmiops/workbench/dataset/DataSetServiceImpl.java", "diffHunk": "@@ -815,6 +815,58 @@ private String getQualifiedColumnName(Domain currentDomain, String columnName) {\n             + \"head results.P2.assoc\");\n   }\n \n+  @Override\n+  public List<String> generateHailDemoCode(String qualifier) {\n+    final String phenotypeFilename = \"phenotypes_annotations_\" + qualifier + \".txt\";\n+    final String cohortQualifier = \"cohort_\" + qualifier;\n+    final String cohortVcfFilename = cohortQualifier + \".vcf\";\n+    final String cohortMtFilename = cohortQualifier + \".mt\";\n+\n+    return ImmutableList.of(\n+        \"import subprocess, os\\n\"\n+            + \"import random\\n\"\n+            + \"\\n\"\n+            + \"# Creating phenotype annotations file\\n\"\n+            + \"phenotypes_table = []\\n\"\n+            + \"for person_id in person_ids:\\n\"\n+            + \"    person_id = person_id\\n\"\n+            + \"    phenotype_1 = random.randint(0, 2) # Change this value to what makes sense for your research by looking through the dataset(s)\\n\"\n+            + \"    phenotype_2 = random.randint(0, 2) # Change this value as well or remove if you are only processing one phenotype \\n\"\n+            + \"    phenotypes_table.append([person_id, phenotype_1, phenotype_2])\\n\"\n+            + \"\\n\"\n+            + \"cohort_phenotypes = pandas.DataFrame(phenotypes_table,columns=[\\\"sample_name\\\", \\\"phenotype1\\\", \\\"phenotype2\\\"]) \\n\"\n+            + \"cohort_phenotypes.to_csv('\"\n+            + phenotypeFilename\n+            + \"', index=False, sep='\\\\t')\\n\"\n+            + \"\\n\"\n+            + \"subprocess.run([\\\"gsutil\\\", \\\"cp\\\", \\\"\"\n+            + phenotypeFilename\n+            + \"\\\", os.environ['WORKSPACE_BUCKET']])\",\n+        \"import hail as hl\\n\"\n+            + \"import os\\n\"\n+            + \"from hail.plot import show\\n\"\n+            + \"\\n\"\n+            + \"hl.plot.output_notebook()\\n\"\n+            + \"bucket = os.environ['WORKSPACE_BUCKET']\\n\"\n+            + \"hl.import_vcf(f'{bucket}/\"\n+            + cohortVcfFilename\n+            + \"').write(f'{bucket}/\"\n+            + cohortMtFilename\n+            + \"')\\n\"\n+            + \"table = hl.import_table(f'{bucket}/\"\n+            + phenotypeFilename\n+            + \"', types={'sample_name': hl.tstr}, impute=True, key='sample_name')\\n\"\n+            + \"\\n\"\n+            + \"mt = hl.read_matrix_table(f'{bucket}/\"\n+            + cohortMtFilename\n+            + \"');\\n\"\n+            + \"mt = mt.annotate_cols(pheno = table[mt.s])\\n\"\n+            + \"\\n\"\n+            + \"gwas = hl.linear_regression_rows(y=mt.pheno.phenotype1, x=mt.GT.n_alt_alleles(), covariates=[1.0])\\n\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d33e62b4a74cef08c417a76bf173b4bd06a63cb"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4NjQ5MDgwOnYy", "diffSide": "RIGHT", "path": "api/src/test/java/org/pmiops/workbench/api/DataSetControllerTest.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNjo1NzozNVrOHlIB2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxOTozODowOFrOHlOD4w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODY5MDkwNQ==", "bodyText": "opt: if you wanted to use streams here, you could use flatMap to expand the inner arrays. Fine as currently written though.", "url": "https://github.com/all-of-us/workbench/pull/4185#discussion_r508690905", "createdAt": "2020-10-20T16:57:35Z", "author": {"login": "calbach"}, "path": "api/src/test/java/org/pmiops/workbench/api/DataSetControllerTest.java", "diffHunk": "@@ -739,4 +732,169 @@ public void testGetValuesFromDomain() {\n         .containsExactly(\n             new DomainValue().value(\"field_one\"), new DomainValue().value(\"field_two\"));\n   }\n+\n+  @Test\n+  public void exportToNotebook_microarrayCodegen_cdrCheck() {\n+    DbCdrVersion cdrVersion =\n+        cdrVersionDao.findByCdrVersionId(Long.parseLong(workspace.getCdrVersionId()));\n+    cdrVersion.setMicroarrayBigqueryDataset(null);\n+    cdrVersionDao.save(cdrVersion);\n+\n+    DataSetExportRequest request =\n+        setUpValidDataSetExportRequest().genomicsDataType(GenomicsDataTypeEnum.MICROARRAY);\n+\n+    FailedPreconditionException e =\n+        assertThrows(\n+            FailedPreconditionException.class,\n+            () ->\n+                dataSetController.exportToNotebook(\n+                    workspace.getNamespace(), WORKSPACE_NAME, request));\n+    assertThat(e)\n+        .hasMessageThat()\n+        .contains(\"The workspace CDR version does not have microarray data\");\n+  }\n+\n+  @Test\n+  public void exportToNotebook_microarrayCodegen_kernelCheck() {\n+    DataSetExportRequest request =\n+        setUpValidDataSetExportRequest()\n+            .kernelType(KernelTypeEnum.R)\n+            .genomicsDataType(GenomicsDataTypeEnum.MICROARRAY);\n+\n+    BadRequestException e =\n+        assertThrows(\n+            BadRequestException.class,\n+            () ->\n+                dataSetController.exportToNotebook(\n+                    workspace.getNamespace(), WORKSPACE_NAME, request));\n+    assertThat(e).hasMessageThat().contains(\"Genomics code generation is only supported in Python\");\n+  }\n+\n+  @Test\n+  public void exportToNotebook_microarrayCodegen_noGenomicsTool() {\n+    DataSetExportRequest request =\n+        setUpValidDataSetExportRequest()\n+            .genomicsDataType(GenomicsDataTypeEnum.MICROARRAY)\n+            .genomicsAnalysisTool(GenomicsAnalysisToolEnum.NONE);\n+\n+    dataSetController.exportToNotebook(workspace.getNamespace(), WORKSPACE_NAME, request);\n+\n+    verify(mockNotebooksService, times(1))\n+        .saveNotebook(\n+            eq(WORKSPACE_BUCKET_NAME),\n+            eq(request.getNotebookName()),\n+            notebookContentsCaptor.capture());\n+\n+    List<String> codeCells = notebookContentsToStrings(notebookContentsCaptor.getValue());\n+\n+    assertThat(codeCells.size()).isEqualTo(3);\n+    assertThat(codeCells.get(2)).contains(\"raw_array_cohort_extract.py\");\n+    assertThat(codeCells.get(2)).contains(\"gatk ArrayExtractCohort\");\n+    assertThat(codeCells.get(2)).contains(\"gsutil cp\");\n+  }\n+\n+  @Test\n+  public void exportToNotebook_microarrayCodegen_plink() {\n+    DataSetExportRequest request =\n+        setUpValidDataSetExportRequest()\n+            .genomicsDataType(GenomicsDataTypeEnum.MICROARRAY)\n+            .genomicsAnalysisTool(GenomicsAnalysisToolEnum.PLINK);\n+\n+    dataSetController.exportToNotebook(workspace.getNamespace(), WORKSPACE_NAME, request);\n+\n+    verify(mockNotebooksService, times(1))\n+        .saveNotebook(\n+            eq(WORKSPACE_BUCKET_NAME),\n+            eq(request.getNotebookName()),\n+            notebookContentsCaptor.capture());\n+\n+    List<String> codeCells = notebookContentsToStrings(notebookContentsCaptor.getValue());\n+\n+    assertThat(codeCells.size()).isEqualTo(5);\n+    assertThat(codeCells.get(2)).contains(\"gatk ArrayExtractCohort\");\n+    assertThat(codeCells.get(3)).contains(\"cohort_phenotypes.to_csv\");\n+    assertThat(codeCells.get(3)).contains(\".phe\");\n+    assertThat(codeCells.get(4)).contains(\"plink\");\n+  }\n+\n+  List<String> notebookContentsToStrings(JSONObject notebookContents) {\n+    List<String> codeCellStrings = new ArrayList<>();\n+\n+    JSONArray cells = notebookContents.getJSONArray(\"cells\");\n+    for (int i = 0; i < cells.length(); i++) {\n+      String cellString = \"\";\n+      JSONArray innerCells = cells.getJSONObject(i).getJSONArray(\"source\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b4580de882fe131020465a6a4013c8e73b719a3f"}, "originalPosition": 181}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODc4NjcxMA==", "bodyText": "I tried using streams but the issue I ran into is that the JSONArray type isn't iterable and it doesn't easily translate to streams. Would flatmap fix that issue?", "url": "https://github.com/all-of-us/workbench/pull/4185#discussion_r508786710", "createdAt": "2020-10-20T19:32:48Z", "author": {"login": "ericsong"}, "path": "api/src/test/java/org/pmiops/workbench/api/DataSetControllerTest.java", "diffHunk": "@@ -739,4 +732,169 @@ public void testGetValuesFromDomain() {\n         .containsExactly(\n             new DomainValue().value(\"field_one\"), new DomainValue().value(\"field_two\"));\n   }\n+\n+  @Test\n+  public void exportToNotebook_microarrayCodegen_cdrCheck() {\n+    DbCdrVersion cdrVersion =\n+        cdrVersionDao.findByCdrVersionId(Long.parseLong(workspace.getCdrVersionId()));\n+    cdrVersion.setMicroarrayBigqueryDataset(null);\n+    cdrVersionDao.save(cdrVersion);\n+\n+    DataSetExportRequest request =\n+        setUpValidDataSetExportRequest().genomicsDataType(GenomicsDataTypeEnum.MICROARRAY);\n+\n+    FailedPreconditionException e =\n+        assertThrows(\n+            FailedPreconditionException.class,\n+            () ->\n+                dataSetController.exportToNotebook(\n+                    workspace.getNamespace(), WORKSPACE_NAME, request));\n+    assertThat(e)\n+        .hasMessageThat()\n+        .contains(\"The workspace CDR version does not have microarray data\");\n+  }\n+\n+  @Test\n+  public void exportToNotebook_microarrayCodegen_kernelCheck() {\n+    DataSetExportRequest request =\n+        setUpValidDataSetExportRequest()\n+            .kernelType(KernelTypeEnum.R)\n+            .genomicsDataType(GenomicsDataTypeEnum.MICROARRAY);\n+\n+    BadRequestException e =\n+        assertThrows(\n+            BadRequestException.class,\n+            () ->\n+                dataSetController.exportToNotebook(\n+                    workspace.getNamespace(), WORKSPACE_NAME, request));\n+    assertThat(e).hasMessageThat().contains(\"Genomics code generation is only supported in Python\");\n+  }\n+\n+  @Test\n+  public void exportToNotebook_microarrayCodegen_noGenomicsTool() {\n+    DataSetExportRequest request =\n+        setUpValidDataSetExportRequest()\n+            .genomicsDataType(GenomicsDataTypeEnum.MICROARRAY)\n+            .genomicsAnalysisTool(GenomicsAnalysisToolEnum.NONE);\n+\n+    dataSetController.exportToNotebook(workspace.getNamespace(), WORKSPACE_NAME, request);\n+\n+    verify(mockNotebooksService, times(1))\n+        .saveNotebook(\n+            eq(WORKSPACE_BUCKET_NAME),\n+            eq(request.getNotebookName()),\n+            notebookContentsCaptor.capture());\n+\n+    List<String> codeCells = notebookContentsToStrings(notebookContentsCaptor.getValue());\n+\n+    assertThat(codeCells.size()).isEqualTo(3);\n+    assertThat(codeCells.get(2)).contains(\"raw_array_cohort_extract.py\");\n+    assertThat(codeCells.get(2)).contains(\"gatk ArrayExtractCohort\");\n+    assertThat(codeCells.get(2)).contains(\"gsutil cp\");\n+  }\n+\n+  @Test\n+  public void exportToNotebook_microarrayCodegen_plink() {\n+    DataSetExportRequest request =\n+        setUpValidDataSetExportRequest()\n+            .genomicsDataType(GenomicsDataTypeEnum.MICROARRAY)\n+            .genomicsAnalysisTool(GenomicsAnalysisToolEnum.PLINK);\n+\n+    dataSetController.exportToNotebook(workspace.getNamespace(), WORKSPACE_NAME, request);\n+\n+    verify(mockNotebooksService, times(1))\n+        .saveNotebook(\n+            eq(WORKSPACE_BUCKET_NAME),\n+            eq(request.getNotebookName()),\n+            notebookContentsCaptor.capture());\n+\n+    List<String> codeCells = notebookContentsToStrings(notebookContentsCaptor.getValue());\n+\n+    assertThat(codeCells.size()).isEqualTo(5);\n+    assertThat(codeCells.get(2)).contains(\"gatk ArrayExtractCohort\");\n+    assertThat(codeCells.get(3)).contains(\"cohort_phenotypes.to_csv\");\n+    assertThat(codeCells.get(3)).contains(\".phe\");\n+    assertThat(codeCells.get(4)).contains(\"plink\");\n+  }\n+\n+  List<String> notebookContentsToStrings(JSONObject notebookContents) {\n+    List<String> codeCellStrings = new ArrayList<>();\n+\n+    JSONArray cells = notebookContents.getJSONArray(\"cells\");\n+    for (int i = 0; i < cells.length(); i++) {\n+      String cellString = \"\";\n+      JSONArray innerCells = cells.getJSONObject(i).getJSONArray(\"source\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODY5MDkwNQ=="}, "originalCommit": {"oid": "b4580de882fe131020465a6a4013c8e73b719a3f"}, "originalPosition": 181}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODc4OTczMQ==", "bodyText": "Ah I see. Probably not. You'd probably need to do something like:\n.flatMap(c -> StreamSupport.stream(\n          Spliterators.spliteratorUnknownSize(c.iterator()));\n\nwhich is already getting pretty hairy. I'd leave it as you have it, in this case", "url": "https://github.com/all-of-us/workbench/pull/4185#discussion_r508789731", "createdAt": "2020-10-20T19:38:08Z", "author": {"login": "calbach"}, "path": "api/src/test/java/org/pmiops/workbench/api/DataSetControllerTest.java", "diffHunk": "@@ -739,4 +732,169 @@ public void testGetValuesFromDomain() {\n         .containsExactly(\n             new DomainValue().value(\"field_one\"), new DomainValue().value(\"field_two\"));\n   }\n+\n+  @Test\n+  public void exportToNotebook_microarrayCodegen_cdrCheck() {\n+    DbCdrVersion cdrVersion =\n+        cdrVersionDao.findByCdrVersionId(Long.parseLong(workspace.getCdrVersionId()));\n+    cdrVersion.setMicroarrayBigqueryDataset(null);\n+    cdrVersionDao.save(cdrVersion);\n+\n+    DataSetExportRequest request =\n+        setUpValidDataSetExportRequest().genomicsDataType(GenomicsDataTypeEnum.MICROARRAY);\n+\n+    FailedPreconditionException e =\n+        assertThrows(\n+            FailedPreconditionException.class,\n+            () ->\n+                dataSetController.exportToNotebook(\n+                    workspace.getNamespace(), WORKSPACE_NAME, request));\n+    assertThat(e)\n+        .hasMessageThat()\n+        .contains(\"The workspace CDR version does not have microarray data\");\n+  }\n+\n+  @Test\n+  public void exportToNotebook_microarrayCodegen_kernelCheck() {\n+    DataSetExportRequest request =\n+        setUpValidDataSetExportRequest()\n+            .kernelType(KernelTypeEnum.R)\n+            .genomicsDataType(GenomicsDataTypeEnum.MICROARRAY);\n+\n+    BadRequestException e =\n+        assertThrows(\n+            BadRequestException.class,\n+            () ->\n+                dataSetController.exportToNotebook(\n+                    workspace.getNamespace(), WORKSPACE_NAME, request));\n+    assertThat(e).hasMessageThat().contains(\"Genomics code generation is only supported in Python\");\n+  }\n+\n+  @Test\n+  public void exportToNotebook_microarrayCodegen_noGenomicsTool() {\n+    DataSetExportRequest request =\n+        setUpValidDataSetExportRequest()\n+            .genomicsDataType(GenomicsDataTypeEnum.MICROARRAY)\n+            .genomicsAnalysisTool(GenomicsAnalysisToolEnum.NONE);\n+\n+    dataSetController.exportToNotebook(workspace.getNamespace(), WORKSPACE_NAME, request);\n+\n+    verify(mockNotebooksService, times(1))\n+        .saveNotebook(\n+            eq(WORKSPACE_BUCKET_NAME),\n+            eq(request.getNotebookName()),\n+            notebookContentsCaptor.capture());\n+\n+    List<String> codeCells = notebookContentsToStrings(notebookContentsCaptor.getValue());\n+\n+    assertThat(codeCells.size()).isEqualTo(3);\n+    assertThat(codeCells.get(2)).contains(\"raw_array_cohort_extract.py\");\n+    assertThat(codeCells.get(2)).contains(\"gatk ArrayExtractCohort\");\n+    assertThat(codeCells.get(2)).contains(\"gsutil cp\");\n+  }\n+\n+  @Test\n+  public void exportToNotebook_microarrayCodegen_plink() {\n+    DataSetExportRequest request =\n+        setUpValidDataSetExportRequest()\n+            .genomicsDataType(GenomicsDataTypeEnum.MICROARRAY)\n+            .genomicsAnalysisTool(GenomicsAnalysisToolEnum.PLINK);\n+\n+    dataSetController.exportToNotebook(workspace.getNamespace(), WORKSPACE_NAME, request);\n+\n+    verify(mockNotebooksService, times(1))\n+        .saveNotebook(\n+            eq(WORKSPACE_BUCKET_NAME),\n+            eq(request.getNotebookName()),\n+            notebookContentsCaptor.capture());\n+\n+    List<String> codeCells = notebookContentsToStrings(notebookContentsCaptor.getValue());\n+\n+    assertThat(codeCells.size()).isEqualTo(5);\n+    assertThat(codeCells.get(2)).contains(\"gatk ArrayExtractCohort\");\n+    assertThat(codeCells.get(3)).contains(\"cohort_phenotypes.to_csv\");\n+    assertThat(codeCells.get(3)).contains(\".phe\");\n+    assertThat(codeCells.get(4)).contains(\"plink\");\n+  }\n+\n+  List<String> notebookContentsToStrings(JSONObject notebookContents) {\n+    List<String> codeCellStrings = new ArrayList<>();\n+\n+    JSONArray cells = notebookContents.getJSONArray(\"cells\");\n+    for (int i = 0; i < cells.length(); i++) {\n+      String cellString = \"\";\n+      JSONArray innerCells = cells.getJSONObject(i).getJSONArray(\"source\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODY5MDkwNQ=="}, "originalCommit": {"oid": "b4580de882fe131020465a6a4013c8e73b719a3f"}, "originalPosition": 181}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3853, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}