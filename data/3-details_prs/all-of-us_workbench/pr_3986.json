{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDg0MDM2NTcz", "number": 3986, "title": "[RW-5267] Tooling for translating table schemas into code", "bodyText": "See api/schemas/REPORTING-SCHEMA-TOOLS.md for detailed description. This Ruby utility generates the following items for each MySQL table:\n\nA BigQuery JSON Schema\nA swagger object definition for a DTO type used in the snapshot & upload services\nA Java/Spring Data @Query annotation\nA matching Java projection interface\n\nThe script is called like ./reporting-codegen.rb workspace ./input/ ~/scratch/reporting-codegen. Some niceties have been omitted such as help text, organization of the code into reusable classes, etc. It's not going to be common to re-run this, but it's very helpful to have when needed.\nA couple of modifications aren't handled in the generation as it assumes tables are 1:1 with BigQuery tables.\n\nIn the case of User, for example, several columns from Address and Institutional Affiliation tables will be de-normalized by hand. It was not (yet) deemed a worthwhile investment to automate this.\nsome columns in the schema won't have a MySQL counterpart at all, and will be added by hand.\nat least one column is begging to be renamed, which is email -> username. Otherwise, I can live with the names we already have.\n\nStill to do is filling in the description attributes and adding an automated way to synchronize these in every output. For example, we can add the description in swagger and BigQuery, and might as well make Javadoc entries for the interfaces while we're at it.\nNote projection interfaces and queries must match exactly in\n\nvariable name\ntype (not just compatible types)\norder\nIt's not possible to query for fewer columns than are present in a projection, for example.\n\nExample outputs, given the invocation\n./reporting-codegen.rb institution ./input/ ~/scratch/reporting3\nGenerate types for institution...\ninstitution Spring BigQuery Schema: /Users/jaycarlton/scratch/reporting3/projection_interface/institution.java\ninstitution DTO Swagger Definition to /Users/jaycarlton/scratch/reporting3/swagger_yaml/institution.yaml\ninstitution Spring Data Projection Interface: /Users/jaycarlton/scratch/reporting3/projection_interface/institution.java\ninstitution Projection Query: /Users/jaycarlton/scratch/reporting3/projection_query/institution.java\n\n[\n  {\n    \"name\": \"institution_id\",\n    \"type\": \"INT64\"\n  },\n  {\n    \"name\": \"short_name\",\n    \"type\": \"STRING\"\n  },\n  {\n    \"name\": \"display_name\",\n    \"type\": \"STRING\"\n  },\n  {\n    \"name\": \"organization_type_enum\",\n    \"type\": \"INT64\"\n  },\n  {\n    \"name\": \"organization_type_other_text\",\n    \"type\": \"STRING\"\n  },\n  {\n    \"name\": \"dua_type_enum\",\n    \"type\": \"INT64\"\n  }\n]\nProjection interface:\ninterface PrjInstitution {\n  long getInstitutionId();\n  String getShortName();\n  String getDisplayName();\n  long getOrganizationTypeEnum();\n  String getOrganizationTypeOtherText();\n  long getDuaTypeEnum();\n}\nProjection Query Annotation:\n@Query(\"SELECT\n+ \"  i.institutionId,\"\n+ \"  i.shortName,\"\n+ \"  i.displayName,\"\n+ \"  i.organizationTypeEnum,\"\n+ \"  i.organizationTypeOtherText,\"\n+ \"  i.duaTypeEnum\"\n+ \"FROM DbInstitution i\")\nSwagger DTO:\n---\nReportingInstitution:\n  type: object\n  properties:\n    institutionId:\n      description: ''\n      type: integer\n      format: int64\n    shortName:\n      description: ''\n      type: string\n    displayName:\n      description: ''\n      type: string\n    organizationTypeEnum:\n      description: ''\n      type: integer\n      format: int64\n    organizationTypeOtherText:\n      description: ''\n      type: string\n    duaTypeEnum:\n      description: ''\n      type: integer\n      format: int64\nTODO:\n\ngenerate unit test code to verify every field. This turns out to be really important to catch Spring Data problems with the projection (which generally only show up at runtime)\nfinalize the actual schemas for BigQuery\nupdate the services to use this new code\n\n\nPR checklist\n\n[ x] This PR meets the Acceptance Criteria in the JIRA story\n The JIRA story has been moved to Dev Review\n This PR includes appropriate unit tests\n[ x] I have run and tested this change locally\n I have run the E2E tests on ths change against my local UI + API server with yarn test:local\n If this includes a UI change, I have taken screen recordings or screenshots of the new behavior and notified the PO and UX designer\n If this includes an API change, I have updated the appropriate Swagger definitions and notified API consumers\n If this includes a new feature flag, I have created and linked new JIRA tickets to (a) turn on the feature flag and (b) remove it later", "createdAt": "2020-09-10T15:55:46Z", "url": "https://github.com/all-of-us/workbench/pull/3986", "merged": true, "mergeCommit": {"oid": "e153e19a94911e05232a1b7dc7d9525ed9073c3f"}, "closed": true, "closedAt": "2020-09-11T22:03:38Z", "author": {"login": "jaycarlton"}, "timelineItems": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdHkohggFqTQ4NjE3NjgxNA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdH7TksgFqTQ4NzA5OTYwOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg2MTc2ODE0", "url": "https://github.com/all-of-us/workbench/pull/3986#pullrequestreview-486176814", "createdAt": "2020-09-10T17:55:33Z", "commit": {"oid": "217f964ebb09e965d1a7aba90ca91ba5a655020b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxNzo1NTozM1rOHP_Yuw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxNzo1NTozM1rOHP_Yuw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjUyOTIxMQ==", "bodyText": "TODO: link here from top-level README.", "url": "https://github.com/all-of-us/workbench/pull/3986#discussion_r486529211", "createdAt": "2020-09-10T17:55:33Z", "author": {"login": "jaycarlton"}, "path": "api/reporting/schemas/REPORTING-SCHEMA-TOOLS.md", "diffHunk": "@@ -0,0 +1,177 @@\n+# Reporting Schema Tools & Process", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "217f964ebb09e965d1a7aba90ca91ba5a655020b"}, "originalPosition": 1}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg2MTg1ODU1", "url": "https://github.com/all-of-us/workbench/pull/3986#pullrequestreview-486185855", "createdAt": "2020-09-10T18:08:04Z", "commit": {"oid": "217f964ebb09e965d1a7aba90ca91ba5a655020b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxODowODowNFrOHP_0lQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxODowODowNFrOHP_0lQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjUzNjM0MQ==", "bodyText": "A prototype example of this is in this PR (changing rapidly).", "url": "https://github.com/all-of-us/workbench/pull/3986#discussion_r486536341", "createdAt": "2020-09-10T18:08:04Z", "author": {"login": "jaycarlton"}, "path": "api/reporting/schemas/REPORTING-SCHEMA-TOOLS.md", "diffHunk": "@@ -0,0 +1,177 @@\n+# Reporting Schema Tools & Process\n+This directory and subdirectories archive schema changes\n+and tools to generate and translate them into code. There\n+are a few different stages, but the grunt work is mostly automated\n+for repeatability and consistency.\n+\n+The ruby script `generate_all_tables.rb` will operate on all of the input CSV files (described below)\n+and produce output files in this directory structure \n+```\n+$ ls ~/scratch/reportingOut5\n+big_query_json       projection_interface projection_query     swagger_yaml\n+```\n+\n+## EXPLAIN Application DB Table\n+The `explain table` MySql [statement](https://dev.mysql.com/doc/refman/8.0/en/explain.html) generates a tabular\n+set of attributes for table. These are then saved as CSV files and placed into the `mysql_describe_csv` directory.\n+\n+For example, the output of the `explain address;` is shown below.\n+\n+| Field | Type | Null | Key | Default | Extra |\n+| :--- | :--- | :--- | :--- | :--- | :--- |\n+| id | bigint\\(20\\) | NO | PRI | NULL | auto\\_increment |\n+| street\\_address\\_1 | varchar\\(95\\) | NO |  | NULL |  |\n+| street\\_address\\_2 | varchar\\(95\\) | YES |  | NULL |  |\n+| zip\\_code | varchar\\(10\\) | YES |  | NULL |  |\n+| city | varchar\\(95\\) | NO |  | NULL |  |\n+| state | varchar\\(95\\) | NO |  | NULL |  |\n+| country | varchar\\(95\\) | NO |  | NULL |  |\n+| user\\_id | bigint\\(20\\) | NO | MUL | NULL |  |\n+\n+## Mapping to BigQuery Schema JSON Format\n+The next task is to translate the MySql table description into the [BigQuery schema format](https://cloud.google.com/bigquery/docs/schemas#specifying_a_json_schema_file). Given this CSV input, we only really care about the field name and types. The\n+relational constraints won't be preserved in BigQuery, so they're basically ignored,\n+but we do want the primary key (which would be renamed to `address_id` in this example,\n+though in this case we actually merge `address` columns into the\n+`user` table). The script generates the for each table, with the example\n+of the address table shown. All fields are marked nullable, as to do otherwise would involve\n+adding constraints on the source data and/or its snapshot & upload processes.\n+\n+```json\n+[\n+  {\n+    \"name\": \"id\",\n+    \"description\": \"\",\n+    \"type\": \"INT64\",\n+    \"mode\": \"NULLABLE\"\n+  },\n+  {\n+    \"name\": \"street_address_1\",\n+    \"description\": \"\",\n+    \"type\": \"STRING\",\n+    \"mode\": \"NULLABLE\"\n+  },\n+  {\n+    \"name\": \"street_address_2\",\n+    \"description\": \"\",\n+    \"type\": \"STRING\",\n+    \"mode\": \"NULLABLE\"\n+  },\n+  {\n+    \"name\": \"zip_code\",\n+    \"description\": \"\",\n+    \"type\": \"STRING\",\n+    \"mode\": \"NULLABLE\"\n+  },\n+  {\n+    \"name\": \"city\",\n+    \"description\": \"\",\n+    \"type\": \"STRING\",\n+    \"mode\": \"NULLABLE\"\n+  },\n+  {\n+    \"name\": \"state\",\n+    \"description\": \"\",\n+    \"type\": \"STRING\",\n+    \"mode\": \"NULLABLE\"\n+  },\n+  {\n+    \"name\": \"country\",\n+    \"description\": \"\",\n+    \"type\": \"STRING\",\n+    \"mode\": \"NULLABLE\"\n+  },\n+  {\n+    \"name\": \"user_id\",\n+    \"description\": \"\",\n+    \"type\": \"INT64\",\n+    \"mode\": \"NULLABLE\"\n+  }\n+]\n+```\n+\n+## Spring Data Projection Interface", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "217f964ebb09e965d1a7aba90ca91ba5a655020b"}, "originalPosition": 93}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a55490889d0741363394bf2e1ffea68f922c04b8", "author": {"user": {"login": "jaycarlton", "name": "Jay Carlton"}}, "url": "https://github.com/all-of-us/workbench/commit/a55490889d0741363394bf2e1ffea68f922c04b8", "committedDate": "2020-09-10T18:13:07Z", "message": "rebased"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "217f964ebb09e965d1a7aba90ca91ba5a655020b", "author": {"user": {"login": "jaycarlton", "name": "Jay Carlton"}}, "url": "https://github.com/all-of-us/workbench/commit/217f964ebb09e965d1a7aba90ca91ba5a655020b", "committedDate": "2020-09-10T17:53:08Z", "message": "fixup doc"}, "afterCommit": {"oid": "a55490889d0741363394bf2e1ffea68f922c04b8", "author": {"user": {"login": "jaycarlton", "name": "Jay Carlton"}}, "url": "https://github.com/all-of-us/workbench/commit/a55490889d0741363394bf2e1ffea68f922c04b8", "committedDate": "2020-09-10T18:13:07Z", "message": "rebased"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg2MTk2NzE0", "url": "https://github.com/all-of-us/workbench/pull/3986#pullrequestreview-486196714", "createdAt": "2020-09-10T18:16:17Z", "commit": {"oid": "a55490889d0741363394bf2e1ffea68f922c04b8"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxODoxNjoxN1rOHQAJGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxODoyNTowNFrOHQAivA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU0MTU5Mg==", "bodyText": "I'm not clear on what is intended by this sentence.  Also, could you rename this script to include \"local\" somewhere?", "url": "https://github.com/all-of-us/workbench/pull/3986#discussion_r486541592", "createdAt": "2020-09-10T18:16:17Z", "author": {"login": "jmthibault79"}, "path": "api/curl/upload-snapshot-cron.sh", "diffHunk": "@@ -0,0 +1,7 @@\n+#!/bin/bash\n+\n+# Start hit the reporting snapshot & upload cron endpoint  locally.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a55490889d0741363394bf2e1ffea68f922c04b8"}, "originalPosition": 3}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU0NTYwMQ==", "bodyText": "I don't like the name \"valid\" here.  Maybe should_include_field?", "url": "https://github.com/all-of-us/workbench/pull/3986#discussion_r486545601", "createdAt": "2020-09-10T18:21:36Z", "author": {"login": "jmthibault79"}, "path": "api/reporting/schemas/reporting-codegen.rb", "diffHunk": "@@ -0,0 +1,178 @@\n+#!/usr/bin/ruby\n+require 'csv'\n+require 'json'\n+require 'yaml'\n+\n+table_name = ARGV[0]\n+input_dir = File.expand_path(ARGV[1])\n+output_dir = File.expand_path(ARGV[2])\n+puts \"Generate types for #{table_name}...\"\n+Dir.mkdir(output_dir) unless Dir.exist?(output_dir)\n+\n+def to_camel_case(snake_case, capitalize_initial)\n+  result =  snake_case.split('_').collect(&:capitalize).join\n+  unless capitalize_initial\n+    result[0] = result[0].downcase\n+  end\n+  result\n+end\n+\n+def to_input_path(dir_name, table_name, suffix)\n+  File.expand_path(File.join(dir_name, \"#{table_name}.#{suffix}\"))\n+end\n+\n+def to_output_path(dir_name, table_name, suffix)\n+  Dir.mkdir(dir_name) unless Dir.exist?(dir_name)\n+  File.expand_path(File.join(dir_name, \"#{table_name}.#{suffix}\"))\n+end\n+\n+dto_class_name = \"Reporting#{to_camel_case(table_name, true)}\"\n+\n+inputs = {\n+    :describe_csv => to_input_path(File.join(input_dir, 'mysql_describe_csv'), table_name,'csv'),\n+    :exclude_columns => to_input_path(File.join(input_dir, 'excluded_columns'), table_name,'txt')\n+}\n+\n+outputs = {\n+    :big_query_json => to_output_path(File.join(output_dir, 'big_query_json'), table_name,'json'),\n+    :swagger_yaml => to_output_path(File.join(output_dir, 'swagger_yaml'), table_name,'yaml'),\n+    :projection_interface => to_output_path(File.join(output_dir, 'projection_interface'), table_name, 'java'),\n+    :projection_query => to_output_path(File.join(output_dir, 'projection_query'), table_name,'java')\n+}\n+\n+MYSQL_TO_BIGQUERY_TYPE = {\n+    'varchar' => 'STRING',\n+    'datetime' => 'TIMESTAMP',\n+    'bigint' => 'INT64',\n+    'smallint' => 'INT64',\n+    'longtext' => 'STRING',\n+    'int' => 'INT64',\n+    'tinyint' => 'INT64',\n+    'bit' => 'BOOLEAN',\n+    'double' => 'FLOAT64',\n+    'text' => 'STRING',\n+    'mediumblob' => 'STRING'\n+}\n+\n+def to_bq_type(mysql_type)\n+  type_pattern = Regexp.new(\"(?<type>\\\\w+)(\\\\(\\\\d+\\\\))?\")\n+  match_data = mysql_type.match(type_pattern)\n+  result = MYSQL_TO_BIGQUERY_TYPE[match_data[:type]]\n+  raise \"MySQL type #{mysql_type} not recognized.\" if result.nil?\n+  result\n+end\n+\n+excluded_fields = File.exist?(inputs[:exclude_columns]) ? File.readlines(inputs[:exclude_columns]) : []\n+\n+def is_valid_field?(excluded_fields, field)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a55490889d0741363394bf2e1ffea68f922c04b8"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU0NzI0NA==", "bodyText": "How do you connect an address to a user without this?\nMore generally, how did you choose which columns to exclude?", "url": "https://github.com/all-of-us/workbench/pull/3986#discussion_r486547244", "createdAt": "2020-09-10T18:23:50Z", "author": {"login": "jmthibault79"}, "path": "api/reporting/schemas/input/excluded_columns/address.txt", "diffHunk": "@@ -0,0 +1,2 @@\n+id\n+user_id", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a55490889d0741363394bf2e1ffea68f922c04b8"}, "originalPosition": 2}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU0ODE1Ng==", "bodyText": "missing end of comment", "url": "https://github.com/all-of-us/workbench/pull/3986#discussion_r486548156", "createdAt": "2020-09-10T18:25:04Z", "author": {"login": "jmthibault79"}, "path": "api/reporting/sql/query-users.sql", "diffHunk": "@@ -0,0 +1,10 @@\n+-- Fetch all users from this environment who are not", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a55490889d0741363394bf2e1ffea68f922c04b8"}, "originalPosition": 1}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6fad502fc7ec6f2b9735c9062cdaa1e7447b2030", "author": {"user": {"login": "jaycarlton", "name": "Jay Carlton"}}, "url": "https://github.com/all-of-us/workbench/commit/6fad502fc7ec6f2b9735c9062cdaa1e7447b2030", "committedDate": "2020-09-10T18:33:29Z", "message": "alphabetize columns and print output from subprorcess"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bf7327da516c8bed19c634bd5e9846866ff82c2d", "author": {"user": {"login": "jaycarlton", "name": "Jay Carlton"}}, "url": "https://github.com/all-of-us/workbench/commit/bf7327da516c8bed19c634bd5e9846866ff82c2d", "committedDate": "2020-09-10T18:39:47Z", "message": "fixup exclude processing"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg2MzA3NTEy", "url": "https://github.com/all-of-us/workbench/pull/3986#pullrequestreview-486307512", "createdAt": "2020-09-10T21:01:39Z", "commit": {"oid": "bf7327da516c8bed19c634bd5e9846866ff82c2d"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQyMTowMTozOVrOHQFnAA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQyMTozNTo1NFrOHQGlxQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjYzMTE2OA==", "bodyText": "Where are these sql files used?", "url": "https://github.com/all-of-us/workbench/pull/3986#discussion_r486631168", "createdAt": "2020-09-10T21:01:39Z", "author": {"login": "freemabd"}, "path": "api/reporting/sql/researcher_stats_view.sql", "diffHunk": "@@ -0,0 +1,4 @@\n+CREATE MATERIALIZED VIEW reporting_local.researcher_stats AS", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bf7327da516c8bed19c634bd5e9846866ff82c2d"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjYzMjY3MA==", "bodyText": "Is the csv generation manual or is there a ruby script that does this? Do the csv file names have to match the table name?", "url": "https://github.com/all-of-us/workbench/pull/3986#discussion_r486632670", "createdAt": "2020-09-10T21:04:41Z", "author": {"login": "freemabd"}, "path": "api/reporting/schemas/REPORTING-SCHEMA-TOOLS.md", "diffHunk": "@@ -0,0 +1,177 @@\n+# Reporting Schema Tools & Process\n+This directory and subdirectories archive schema changes\n+and tools to generate and translate them into code. There\n+are a few different stages, but the grunt work is mostly automated\n+for repeatability and consistency.\n+\n+The ruby script `generate_all_tables.rb` will operate on all of the input CSV files (described below)\n+and produce output files in this directory structure \n+```\n+$ ls ~/scratch/reportingOut5\n+big_query_json       projection_interface projection_query     swagger_yaml\n+```\n+\n+## EXPLAIN Application DB Table\n+The `explain table` MySql [statement](https://dev.mysql.com/doc/refman/8.0/en/explain.html) generates a tabular\n+set of attributes for table. These are then saved as CSV files and placed into the `mysql_describe_csv` directory.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bf7327da516c8bed19c634bd5e9846866ff82c2d"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjY0NTg0NQ==", "bodyText": "Should put something in the readme about how the excluded columns work", "url": "https://github.com/all-of-us/workbench/pull/3986#discussion_r486645845", "createdAt": "2020-09-10T21:32:44Z", "author": {"login": "freemabd"}, "path": "api/reporting/schemas/REPORTING-SCHEMA-TOOLS.md", "diffHunk": "@@ -0,0 +1,177 @@\n+# Reporting Schema Tools & Process\n+This directory and subdirectories archive schema changes\n+and tools to generate and translate them into code. There\n+are a few different stages, but the grunt work is mostly automated\n+for repeatability and consistency.\n+\n+The ruby script `generate_all_tables.rb` will operate on all of the input CSV files (described below)\n+and produce output files in this directory structure \n+```\n+$ ls ~/scratch/reportingOut5\n+big_query_json       projection_interface projection_query     swagger_yaml\n+```\n+\n+## EXPLAIN Application DB Table\n+The `explain table` MySql [statement](https://dev.mysql.com/doc/refman/8.0/en/explain.html) generates a tabular\n+set of attributes for table. These are then saved as CSV files and placed into the `mysql_describe_csv` directory.\n+\n+For example, the output of the `explain address;` is shown below.\n+\n+| Field | Type | Null | Key | Default | Extra |\n+| :--- | :--- | :--- | :--- | :--- | :--- |\n+| id | bigint\\(20\\) | NO | PRI | NULL | auto\\_increment |\n+| street\\_address\\_1 | varchar\\(95\\) | NO |  | NULL |  |\n+| street\\_address\\_2 | varchar\\(95\\) | YES |  | NULL |  |\n+| zip\\_code | varchar\\(10\\) | YES |  | NULL |  |\n+| city | varchar\\(95\\) | NO |  | NULL |  |\n+| state | varchar\\(95\\) | NO |  | NULL |  |\n+| country | varchar\\(95\\) | NO |  | NULL |  |\n+| user\\_id | bigint\\(20\\) | NO | MUL | NULL |  |\n+\n+## Mapping to BigQuery Schema JSON Format\n+The next task is to translate the MySql table description into the [BigQuery schema format](https://cloud.google.com/bigquery/docs/schemas#specifying_a_json_schema_file). Given this CSV input, we only really care about the field name and types. The\n+relational constraints won't be preserved in BigQuery, so they're basically ignored,\n+but we do want the primary key (which would be renamed to `address_id` in this example,\n+though in this case we actually merge `address` columns into the\n+`user` table). The script generates the for each table, with the example\n+of the address table shown. All fields are marked nullable, as to do otherwise would involve\n+adding constraints on the source data and/or its snapshot & upload processes.\n+\n+```json\n+[\n+  {\n+    \"name\": \"id\",\n+    \"description\": \"\",\n+    \"type\": \"INT64\",\n+    \"mode\": \"NULLABLE\"\n+  },\n+  {\n+    \"name\": \"street_address_1\",\n+    \"description\": \"\",\n+    \"type\": \"STRING\",\n+    \"mode\": \"NULLABLE\"\n+  },\n+  {\n+    \"name\": \"street_address_2\",\n+    \"description\": \"\",\n+    \"type\": \"STRING\",\n+    \"mode\": \"NULLABLE\"\n+  },\n+  {\n+    \"name\": \"zip_code\",\n+    \"description\": \"\",\n+    \"type\": \"STRING\",\n+    \"mode\": \"NULLABLE\"\n+  },\n+  {\n+    \"name\": \"city\",\n+    \"description\": \"\",\n+    \"type\": \"STRING\",\n+    \"mode\": \"NULLABLE\"\n+  },\n+  {\n+    \"name\": \"state\",\n+    \"description\": \"\",\n+    \"type\": \"STRING\",\n+    \"mode\": \"NULLABLE\"\n+  },\n+  {\n+    \"name\": \"country\",\n+    \"description\": \"\",\n+    \"type\": \"STRING\",\n+    \"mode\": \"NULLABLE\"\n+  },\n+  {\n+    \"name\": \"user_id\",\n+    \"description\": \"\",\n+    \"type\": \"INT64\",\n+    \"mode\": \"NULLABLE\"\n+  }\n+]\n+```\n+\n+## Spring Data Projection Interface\n+The Ruby script builds projection interface definitions and query\n+annotations for the DAO method to fetch the projection. Since these must match exactly, it makes sense\n+to generate them in the same script.\n+\n+We use a projection for reporting snapshots for a couple of reasons. First, it allows the schema used\n+in reporting to evolve independently of the main application schema (up to a point). Second, when doing\n+very large reads, avoiding the overhead of constructing full-fledged entities is useful. Finally, it\n+allows us to decouple the upload snapshot types (DTOs) from the MySQL return values. MapStruct should\n+allow clean convergence from the projection types to the DTOs, though the reverse isn't possible, as\n+we can't actually instantiate the projection interfaces.\n+\n+### Projection Interface\n+The output for the Address table (assuming it's a stand-alone query) is below. In practice, these fields\n+are appended (by hand) to the `PrjUser` interface and query to denormalize it.\n+```java\n+interface PrjAddress {\n+  String getCity();\n+  String getCountry();\n+  long getId();\n+  String getState();\n+  String getStreetAddress1();\n+  String getStreetAddress2();\n+  long getUserId();\n+  String getZipCode();\n+}\n+```\n+\n+### Projection Query Annotation\n+The projection interface query must match exactly in terms of type, name, and order of columns referenced.\n+Failing to do this seems to lead to runtime exceptions but rarely any compile-time warnings. Thus, it's important\n+to unit test these.\n+\n+The following annotation is generated and should be attached to a DAO method such as `List<PrjAddress> findAllAddresses();`\n+```java\n+@Query(\"SELECT\n++ \"  a.city,\"\n++ \"  a.country,\"\n++ \"  a.id,\"\n++ \"  a.state,\"\n++ \"  a.streetAddress1,\"\n++ \"  a.streetAddress2,\"\n++ \"  a.userId,\"\n++ \"  a.zipCode\"\n++ \"FROM DbAddress a\")\n+```\n+\n+## Swagger DTO Classes\n+Currently the classes for the `ReportingSnapshot` object used\n+buy the upload service are specified in the API Swagger file. This was expedient,\n+but it means we're exposing internal details of the application in the API documentation.\n+\n+The output should look something like this:\n+```yaml\n+---\n+ReportingAddress:\n+  type: object\n+  properties:\n+    id:\n+      description: ''\n+      type: integer\n+      format: int64\n+    streetAddress1:\n+      description: ''\n+      type: string\n+    streetAddress2:\n+      description: ''\n+      type: string\n+    zipCode:\n+      description: ''\n+      type: string\n+    city:\n+      description: ''\n+      type: string\n+    state:\n+      description: ''\n+      type: string\n+    country:\n+      description: ''\n+      type: string\n+    userId:\n+      description: ''\n+      type: integer\n+      format: int64\n+```", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bf7327da516c8bed19c634bd5e9846866ff82c2d"}, "originalPosition": 177}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjY0NzIzNw==", "bodyText": "Should the csv/text files be generated somehow, instead of committing them into the project. Won't the files become stale over time as the schema evolves?", "url": "https://github.com/all-of-us/workbench/pull/3986#discussion_r486647237", "createdAt": "2020-09-10T21:35:54Z", "author": {"login": "freemabd"}, "path": "api/reporting/schemas/input/mysql_describe_csv/user.csv", "diffHunk": "@@ -0,0 +1,47 @@\n+user_id,bigint(20),NO,PRI,,auto_increment", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bf7327da516c8bed19c634bd5e9846866ff82c2d"}, "originalPosition": 1}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1ff4b81feac7cf58005dfc391bad623e425fdd14", "author": {"user": {"login": "jaycarlton", "name": "Jay Carlton"}}, "url": "https://github.com/all-of-us/workbench/commit/1ff4b81feac7cf58005dfc391bad623e425fdd14", "committedDate": "2020-09-10T21:37:42Z", "message": "cleanup output, indent yaml, and make interface public"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "23b9f7394a33cf2c888ca22666f9307b43e31bf0", "author": {"user": {"login": "jaycarlton", "name": "Jay Carlton"}}, "url": "https://github.com/all-of-us/workbench/commit/23b9f7394a33cf2c888ca22666f9307b43e31bf0", "committedDate": "2020-09-10T21:41:26Z", "message": "email -> username at source & use BqDto prefix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "67330d68aeb791423abaeb912b1dd441c6de880b", "author": {"user": {"login": "jaycarlton", "name": "Jay Carlton"}}, "url": "https://github.com/all-of-us/workbench/commit/67330d68aeb791423abaeb912b1dd441c6de880b", "committedDate": "2020-09-10T22:30:07Z", "message": "fix newlines in queries and add projection fucntion decl"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ffef953e334f4967bd7040820d39f80f81548415", "author": {"user": {"login": "jaycarlton", "name": "Jay Carlton"}}, "url": "https://github.com/all-of-us/workbench/commit/ffef953e334f4967bd7040820d39f80f81548415", "committedDate": "2020-09-11T15:50:22Z", "message": "patch for research purpose entity naming convention"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6ba36379bf720f3e7132c3098c63eb0351e11f60", "author": {"user": {"login": "jaycarlton", "name": "Jay Carlton"}}, "url": "https://github.com/all-of-us/workbench/commit/6ba36379bf720f3e7132c3098c63eb0351e11f60", "committedDate": "2020-09-11T18:47:27Z", "message": "fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6791e38b83661e2cb42bffd3fb977bfb5f09c8be", "author": {"user": {"login": "jaycarlton", "name": "Jay Carlton"}}, "url": "https://github.com/all-of-us/workbench/commit/6791e38b83661e2cb42bffd3fb977bfb5f09c8be", "committedDate": "2020-09-11T19:22:12Z", "message": "PR comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg3MDk0MTY4", "url": "https://github.com/all-of-us/workbench/pull/3986#pullrequestreview-487094168", "createdAt": "2020-09-11T20:10:30Z", "commit": {"oid": "6791e38b83661e2cb42bffd3fb977bfb5f09c8be"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg3MDk5NjA4", "url": "https://github.com/all-of-us/workbench/pull/3986#pullrequestreview-487099608", "createdAt": "2020-09-11T20:20:29Z", "commit": {"oid": "6791e38b83661e2cb42bffd3fb977bfb5f09c8be"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4194, "cost": 1, "resetAt": "2021-10-29T17:30:11Z"}}}