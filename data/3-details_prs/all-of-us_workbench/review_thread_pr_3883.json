{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY4MTE4NjY1", "number": 3883, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQyMjo0NDozMlrOEaMgvQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQyMjo0NDo0OVrOEaMhRA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1OTAzNDIxOnYy", "diffSide": "RIGHT", "path": "api/genomics/docs/uploading_test_microarray_data.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQyMjo0NDozMlrOHDeB3Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQyMjo0NDozMlrOHDeB3Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzM5OTc3Mw==", "bodyText": "Adding a direct link here to the Java file in the gatk branch would be good", "url": "https://github.com/all-of-us/workbench/pull/3883#discussion_r473399773", "createdAt": "2020-08-19T22:44:32Z", "author": {"login": "calbach"}, "path": "api/genomics/docs/uploading_test_microarray_data.md", "diffHunk": "@@ -0,0 +1,73 @@\n+## Creating a randomized microarray dataset and uploading to BigQuery in test environment\n+- The following commands are not meant to be run as is. Treat it more as documentation of what I ran to get the data uploaded.\n+- All profiling was done on a laptop with an Intel Core i7-9850H CPU @ 2.60GHz, 32GB RAM, and an SSD\n+### Generate Randomized VCF samples\n+- Run from workbench/api  \n+`./project.rb randomize-vcf --vcf ~/broad/variantstore/NA12878_204126160130_R01C01.vcf --number-of-copies 1000 --output-path /mnt/genomics/randomized1000.vcf`\n+\n+| Number of Samples | Clock time | Output file size |\n+| ----------------  | ---------- | ---------------- |\n+|1 | 53s | 1.4 GB |\n+|10|1m 26s|2.3 GB|\n+|100|6m 38s| 11 GB|\n+|500|27m|49 GB|\n+|1,000|59m|95 GB|\n+|10,000**|~ 10h|~ 1 TB|\n+|100,000**|~ 100h|~ 10 TB|\n+** Estimates assuming linear growth\n+\n+### Compress VCF file \n+- `bgzip -c randomized1000.vcf > randomized1000.vcf.gz`\n+\n+| Number of Samples | Clock time | Compression |\n+| ----------------- | ---------- | ----------- |\n+|100|52s|11 G -> 544 M|\n+|500|4m 18s|49 G -> 1.3 G|\n+|1000|5m 44s|95G -> 2.1 G|\n+\n+### Index VCF file\n+- `bcftools index randomized1000.vcf.gz`\n+\n+| Number of Samples | Clock time |\n+| ----------------- | ---------- |\n+|100|20s|\n+|500|1m 20s|\n+|1000|2m 40s|\n+\n+### Extracting a single sample VCF from multi-sample VCF\n+- `bcftools view -s 204126160130_R01C01.908 randomized1000.vcf.gz > 1000_sample.vcf`\n+\n+| Number of Sample | Clock time |\n+| ---------------  | ---------- |\n+|100|3m 16s|\n+|500|14m|\n+|1,000|28m|\n+|10,000**|~ 4h 40m|\n+|100,000**|~ 46h 40m|\n+** Estimates assuming linear growth\n+\n+### Generate ingest files and upload to GCS\n+```\n+  # Run from variantstore repo, https://github.com/broadinstitute/variantstore\n+  # rm/cp *.tsv is a hack I used to clean up already uploaded files while running the code in a loop\n+  rm *.tsv\n+  ~/broad/gatk/gatk CreateArrayIngestFiles --sample-id 908 -V 1000_sample.vcf --probe-info-file probe_info.csv --ref-version 37\n+  gsutil cp *.tsv gs://all-of-us-workbench-test-genomics/eric/import/3/ready/\n+```\n+- Every import/# folder can only contain up to 4k samples\n+- CreateArrayIngestFiles takes ~1m 20s and operates 1 sample at a time", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7369aeb03fe6bd4f81ce4e3585c43011ddcc58d8"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1OTAzNTU2OnYy", "diffSide": "RIGHT", "path": "api/genomics/docs/uploading_test_microarray_data.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQyMjo0NDo0OVrOHDeCsw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQyMjo0NDo0OVrOHDeCsw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzM5OTk4Nw==", "bodyText": "Adding  a link here to this shell script would be great.", "url": "https://github.com/all-of-us/workbench/pull/3883#discussion_r473399987", "createdAt": "2020-08-19T22:44:49Z", "author": {"login": "calbach"}, "path": "api/genomics/docs/uploading_test_microarray_data.md", "diffHunk": "@@ -0,0 +1,73 @@\n+## Creating a randomized microarray dataset and uploading to BigQuery in test environment\n+- The following commands are not meant to be run as is. Treat it more as documentation of what I ran to get the data uploaded.\n+- All profiling was done on a laptop with an Intel Core i7-9850H CPU @ 2.60GHz, 32GB RAM, and an SSD\n+### Generate Randomized VCF samples\n+- Run from workbench/api  \n+`./project.rb randomize-vcf --vcf ~/broad/variantstore/NA12878_204126160130_R01C01.vcf --number-of-copies 1000 --output-path /mnt/genomics/randomized1000.vcf`\n+\n+| Number of Samples | Clock time | Output file size |\n+| ----------------  | ---------- | ---------------- |\n+|1 | 53s | 1.4 GB |\n+|10|1m 26s|2.3 GB|\n+|100|6m 38s| 11 GB|\n+|500|27m|49 GB|\n+|1,000|59m|95 GB|\n+|10,000**|~ 10h|~ 1 TB|\n+|100,000**|~ 100h|~ 10 TB|\n+** Estimates assuming linear growth\n+\n+### Compress VCF file \n+- `bgzip -c randomized1000.vcf > randomized1000.vcf.gz`\n+\n+| Number of Samples | Clock time | Compression |\n+| ----------------- | ---------- | ----------- |\n+|100|52s|11 G -> 544 M|\n+|500|4m 18s|49 G -> 1.3 G|\n+|1000|5m 44s|95G -> 2.1 G|\n+\n+### Index VCF file\n+- `bcftools index randomized1000.vcf.gz`\n+\n+| Number of Samples | Clock time |\n+| ----------------- | ---------- |\n+|100|20s|\n+|500|1m 20s|\n+|1000|2m 40s|\n+\n+### Extracting a single sample VCF from multi-sample VCF\n+- `bcftools view -s 204126160130_R01C01.908 randomized1000.vcf.gz > 1000_sample.vcf`\n+\n+| Number of Sample | Clock time |\n+| ---------------  | ---------- |\n+|100|3m 16s|\n+|500|14m|\n+|1,000|28m|\n+|10,000**|~ 4h 40m|\n+|100,000**|~ 46h 40m|\n+** Estimates assuming linear growth\n+\n+### Generate ingest files and upload to GCS\n+```\n+  # Run from variantstore repo, https://github.com/broadinstitute/variantstore\n+  # rm/cp *.tsv is a hack I used to clean up already uploaded files while running the code in a loop\n+  rm *.tsv\n+  ~/broad/gatk/gatk CreateArrayIngestFiles --sample-id 908 -V 1000_sample.vcf --probe-info-file probe_info.csv --ref-version 37\n+  gsutil cp *.tsv gs://all-of-us-workbench-test-genomics/eric/import/3/ready/\n+```\n+- Every import/# folder can only contain up to 4k samples\n+- CreateArrayIngestFiles takes ~1m 20s and operates 1 sample at a time\n+- gsutil cp depends on upload speed\n+\n+\n+### GCS -> BigQuery \n+```\n+  # Run from variantstore repo\n+  cd ingest\n+  ./bq_ingest_arrays.sh all-of-us-workbench-test microarray_data gs://all-of-us-workbench-test-genomics/eric/import 3\n+```\n+- This will have to be changed for >4k samples since a BigQuery table can only have 4k partitions", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7369aeb03fe6bd4f81ce4e3585c43011ddcc58d8"}, "originalPosition": 68}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2269, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}