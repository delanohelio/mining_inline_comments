{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDA3MzExMzM1", "number": 7871, "title": "CB-5741 Support For Retrieving VM Size Information", "bodyText": "Changes to retrieve VM Size Information from running cluster.\nAuto-Supports all Cloud Platforms including Yarn.\nAuto-Supports all future Cloud instance types like m6.\nCached VM Details per host-group to avoid unnecessary lookups.\nNo metadata dependency on CB and avoids runtime errors due to missing model labels.\nAlso introduced forced downscaling to match loadalert policy maxResourceValue.\nCloses #CB-5741", "createdAt": "2020-04-22T14:01:46Z", "url": "https://github.com/hortonworks/cloudbreak/pull/7871", "merged": true, "mergeCommit": {"oid": "2c09206d01174db9178b1de5b968a60af342d632"}, "closed": true, "closedAt": "2020-04-25T14:21:28Z", "author": {"login": "smaniraju"}, "timelineItems": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcaTqD1gFqTM5ODcwMDY3NQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcbGf49ABqjMyNzE5NDI5NzU=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk4NzAwNjc1", "url": "https://github.com/hortonworks/cloudbreak/pull/7871#pullrequestreview-398700675", "createdAt": "2020-04-23T01:34:34Z", "commit": {"oid": "a2492e94502bd73f1e5c0528fe18e1c37cdfdefd"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QwMTozNDozNVrOGKSrSQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QwMjozOTo0MFrOGKT_JA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzQ0NDkzNw==", "bodyText": "This is nice. Had forgotten that eventually the instanceType doesn't matter. What we need is the memory assigned for containers on the node. Have the CMCommunicator in place will help with https://jira.cloudera.com/browse/DISTX-410, https://jira.cloudera.com/browse/OPSAPS-54750", "url": "https://github.com/hortonworks/cloudbreak/pull/7871#discussion_r413444937", "createdAt": "2020-04-23T01:34:35Z", "author": {"login": "sidseth"}, "path": "autoscale/src/main/java/com/sequenceiq/periscope/monitor/client/YarnMetricsClient.java", "diffHunk": "@@ -37,21 +37,28 @@\n \n     private static final String DEFAULT_UPSCALE_RESOURCE_TYPE = \"memory-mb\";\n \n+    private static final Integer DEFAULT_CLOUD_VM_NUM_CORES = 8;\n+\n+    private static final Long DEFAULT_CLOUD_VM_MEMORY_MB = 32000L;\n+\n     @Inject\n     private TlsSecurityService tlsSecurityService;\n \n     @Inject\n     private ClusterProxyConfigurationService clusterProxyConfigurationService;\n \n     @Inject\n-    private CloudInstanceTypeService cloudInstanceTypeService;\n+    private ClouderaManagerCommunicator clouderaManagerCommunicator;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a2492e94502bd73f1e5c0528fe18e1c37cdfdefd"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzQ0NTY1NQ==", "bodyText": "If the number of nodes in 'compute' is 0 - will end up using 32GB? That I think is a big problem. Maybe the CM jiras mentioned above help with this. I don't think we can ship with (if 0) assume 32GB though.", "url": "https://github.com/hortonworks/cloudbreak/pull/7871#discussion_r413445655", "createdAt": "2020-04-23T01:36:45Z", "author": {"login": "sidseth"}, "path": "autoscale/src/main/java/com/sequenceiq/periscope/monitor/client/YarnMetricsClient.java", "diffHunk": "@@ -66,13 +73,17 @@ public YarnScalingServiceV1Response getYarnMetricsForCluster(Cluster cluster,\n         String yarnApiUrl = String.format(YARN_API_URL, clusterProxyUrl.get(), cluster.getStackCrn());\n         YarnScalingServiceV1Request yarnScalingServiceV1Request = new YarnScalingServiceV1Request();\n \n-        CloudInstanceType cloudInstanceType = cloudInstanceTypeService.getCloudVMInstanceType(cloudPlatform, hostGroupInstanceType)\n-                .orElseThrow(() -> new RuntimeException(String.format(\"CloudVmType not found for CloudPlatform %s, \" +\n-                        \" InstanceType %s, Cluster %s \", cloudPlatform, hostGroupInstanceType, cluster.getStackCrn())));\n+        CloudInstanceType cloudInstanceType = hostGroupFqdns.size() == 0 ? defaultCloudInstanceType : clouderaManagerCommunicator", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a2492e94502bd73f1e5c0528fe18e1c37cdfdefd"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzQ0NzE2Nw==", "bodyText": "Likewise for the fallback in case of a CM failure.", "url": "https://github.com/hortonworks/cloudbreak/pull/7871#discussion_r413447167", "createdAt": "2020-04-23T01:41:21Z", "author": {"login": "sidseth"}, "path": "autoscale/src/main/java/com/sequenceiq/periscope/monitor/client/YarnMetricsClient.java", "diffHunk": "@@ -66,13 +73,17 @@ public YarnScalingServiceV1Response getYarnMetricsForCluster(Cluster cluster,\n         String yarnApiUrl = String.format(YARN_API_URL, clusterProxyUrl.get(), cluster.getStackCrn());\n         YarnScalingServiceV1Request yarnScalingServiceV1Request = new YarnScalingServiceV1Request();\n \n-        CloudInstanceType cloudInstanceType = cloudInstanceTypeService.getCloudVMInstanceType(cloudPlatform, hostGroupInstanceType)\n-                .orElseThrow(() -> new RuntimeException(String.format(\"CloudVmType not found for CloudPlatform %s, \" +\n-                        \" InstanceType %s, Cluster %s \", cloudPlatform, hostGroupInstanceType, cluster.getStackCrn())));\n+        CloudInstanceType cloudInstanceType = hostGroupFqdns.size() == 0 ? defaultCloudInstanceType : clouderaManagerCommunicator\n+                .getCloudVMDetailsForHostGroup(cluster, hostGroup, hostGroupFqdns)\n+                .orElseGet(() -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a2492e94502bd73f1e5c0528fe18e1c37cdfdefd"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzQ1MDQ2Mw==", "bodyText": "Should this just be a log line instead of an Exception? Does YARN never return an empty ScaleUpCandidateList", "url": "https://github.com/hortonworks/cloudbreak/pull/7871#discussion_r413450463", "createdAt": "2020-04-23T01:51:07Z", "author": {"login": "sidseth"}, "path": "autoscale/src/main/java/com/sequenceiq/periscope/monitor/evaluator/load/YarnLoadEvaluator.java", "diffHunk": "@@ -109,55 +115,50 @@ protected void execute() {\n     }\n \n     protected void pollYarnMetricsAndScaleCluster() throws Exception {\n-\n         StackV4Response stackV4Response = cloudbreakCommunicator.getByCrn(cluster.getStackCrn());\n-\n-        String hostGroupInstanceType =\n-                stackResponseUtils.getHostGroupInstanceType(stackV4Response, loadAlert.getScalingPolicy().getHostGroup());\n+        Map<String, String> hostFqdnsToInstanceId = stackResponseUtils.getCloudInstanceIdsForHostGroup(stackV4Response, policyHostGroup);\n \n         YarnScalingServiceV1Response yarnResponse = yarnMetricsClient\n-                .getYarnMetricsForCluster(cluster, hostGroupInstanceType, stackV4Response.getCloudPlatform());\n+                .getYarnMetricsForCluster(cluster, policyHostGroup, hostFqdnsToInstanceId.keySet());\n \n-        Map<String, String> hostFqdnsToInstanceId = stackResponseUtils\n-                .getCloudInstanceIdsForHostGroup(stackV4Response, loadAlert.getScalingPolicy().getHostGroup());\n         yarnResponse.getScaleUpCandidates().ifPresentOrElse(\n-                scaleUpCandidates -> handleScaleUp(hostGroupInstanceType, scaleUpCandidates, hostFqdnsToInstanceId.size()),\n+                scaleUpCandidates -> handleScaleUp(scaleUpCandidates, hostFqdnsToInstanceId.size()),\n                 () -> {\n-                    yarnResponse.getScaleDownCandidates().ifPresent(\n-                            scaleDownCandidates -> handleScaleDown(scaleDownCandidates, hostFqdnsToInstanceId));\n+                    handleScaleDown(yarnResponse.getScaleDownCandidates().orElse(List.of()), hostFqdnsToInstanceId);\n                 });\n     }\n \n-    protected void handleScaleUp(String hostGroupInstanceType, NewNodeManagerCandidates newNMCandidates, Integer existingHostGroupSize) {\n+    protected void handleScaleUp(NewNodeManagerCandidates newNMCandidates, Integer existingHostGroupSize) {\n         Integer yarnRecommendedHostGroupCount =\n                 newNMCandidates.getCandidates().stream()\n-                        .filter(candidate -> candidate.getModelName().equalsIgnoreCase(hostGroupInstanceType))\n+                        .filter(candidate -> candidate.getModelName().equalsIgnoreCase(policyHostGroup))\n                         .findFirst()\n                         .map(NewNodeManagerCandidates.Candidate::getCount)\n                         .orElseThrow(() -> new RuntimeException(String.format(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a2492e94502bd73f1e5c0528fe18e1c37cdfdefd"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzQ1MzUwMg==", "bodyText": "Shouldn't the check for whether to scale based on config.min / config.max happen before checking what YARN has to say. That decides on whether this will be a scale up or a scale down.\nclusterSize=10, max=8, YARN has pending containers. The scale up event will end up asking for \"-2\" containers from what I can tell. Even if negative values are handled, it's better to get these decommission candidates from YARN itself.", "url": "https://github.com/hortonworks/cloudbreak/pull/7871#discussion_r413453502", "createdAt": "2020-04-23T01:59:50Z", "author": {"login": "sidseth"}, "path": "autoscale/src/main/java/com/sequenceiq/periscope/monitor/evaluator/load/YarnLoadEvaluator.java", "diffHunk": "@@ -109,55 +115,50 @@ protected void execute() {\n     }\n \n     protected void pollYarnMetricsAndScaleCluster() throws Exception {\n-\n         StackV4Response stackV4Response = cloudbreakCommunicator.getByCrn(cluster.getStackCrn());\n-\n-        String hostGroupInstanceType =\n-                stackResponseUtils.getHostGroupInstanceType(stackV4Response, loadAlert.getScalingPolicy().getHostGroup());\n+        Map<String, String> hostFqdnsToInstanceId = stackResponseUtils.getCloudInstanceIdsForHostGroup(stackV4Response, policyHostGroup);\n \n         YarnScalingServiceV1Response yarnResponse = yarnMetricsClient\n-                .getYarnMetricsForCluster(cluster, hostGroupInstanceType, stackV4Response.getCloudPlatform());\n+                .getYarnMetricsForCluster(cluster, policyHostGroup, hostFqdnsToInstanceId.keySet());\n \n-        Map<String, String> hostFqdnsToInstanceId = stackResponseUtils\n-                .getCloudInstanceIdsForHostGroup(stackV4Response, loadAlert.getScalingPolicy().getHostGroup());\n         yarnResponse.getScaleUpCandidates().ifPresentOrElse(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a2492e94502bd73f1e5c0528fe18e1c37cdfdefd"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzQ1Mzc1NQ==", "bodyText": "Similarly here. This check before the actual scale down. Actually the combined check likely needs to happen before processing results from YARN.", "url": "https://github.com/hortonworks/cloudbreak/pull/7871#discussion_r413453755", "createdAt": "2020-04-23T02:00:35Z", "author": {"login": "sidseth"}, "path": "autoscale/src/main/java/com/sequenceiq/periscope/monitor/evaluator/load/YarnLoadEvaluator.java", "diffHunk": "@@ -168,14 +169,23 @@ protected void handleScaleDown(List<DecommissionCandidate> decommissionCandidate\n                 .map(nodeFqdn -> hostGroupFqdnsToInstanceId.get(nodeFqdn))\n                 .collect(Collectors.toList());\n \n-        LOGGER.info(\"ScaleDown NodeCount '{}' for Cluster '{}', HostGroup '{}', NodeIds '{}'\",\n-                decommissionHostGroupNodeIds.size(), cluster.getStackCrn(), loadAlert.getScalingPolicy().getHostGroup(),\n-                decommissionHostGroupNodeIds);\n \n         ScalingEvent scalingEvent = new ScalingEvent(loadAlert);\n         if (!decommissionHostGroupNodeIds.isEmpty()) {\n+            LOGGER.info(\"ScaleDown NodeCount '{}' for Cluster '{}', HostGroup '{}', NodeIds '{}'\",\n+                    decommissionHostGroupNodeIds.size(), cluster.getStackCrn(), policyHostGroup,\n+                    decommissionHostGroupNodeIds);\n+\n             scalingEvent.setDecommissionNodeIds(decommissionHostGroupNodeIds);\n             eventPublisher.publishEvent(scalingEvent);\n+        } else if (existingHostGroupSize > loadAlertConfiguration.getMaxResourceValue()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a2492e94502bd73f1e5c0528fe18e1c37cdfdefd"}, "originalPosition": 120}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzQ2NjQwNA==", "bodyText": "Not really related to this patch\nDug through the CM API after looking at this usage.\nhttps://sseth-hf34092-master0.sseth-en.xcu2-8y8x.dev.cldr.work/api/v32/clusters/sseth-hf34092/services/yarn/roleConfigGroups/yarn-NODEMANAGER-WORKER/config - may provide the actual value allocated.\nThe problem here may be the roleConfigGroup name. Highly doubt that CB stores that in its database. Even if it does, or we could get access to the template to figure this out - determining the roleName may not be trivial.", "url": "https://github.com/hortonworks/cloudbreak/pull/7871#discussion_r413466404", "createdAt": "2020-04-23T02:39:40Z", "author": {"login": "sidseth"}, "path": "autoscale/src/main/java/com/sequenceiq/periscope/monitor/handler/ClouderaManagerCommunicator.java", "diffHunk": "@@ -0,0 +1,72 @@\n+package com.sequenceiq.periscope.monitor.handler;\n+\n+import java.util.Optional;\n+import java.util.Set;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.cache.annotation.Cacheable;\n+import org.springframework.stereotype.Service;\n+\n+import com.cloudera.api.swagger.HostsResourceApi;\n+import com.cloudera.api.swagger.client.ApiClient;\n+import com.sequenceiq.cloudbreak.client.HttpClientConfig;\n+import com.sequenceiq.cloudbreak.cm.DataView;\n+import com.sequenceiq.cloudbreak.cm.client.ClouderaManagerApiClientProvider;\n+import com.sequenceiq.cloudbreak.cm.client.retry.ClouderaManagerApiFactory;\n+import com.sequenceiq.cloudbreak.service.secret.service.SecretService;\n+import com.sequenceiq.periscope.domain.Cluster;\n+import com.sequenceiq.periscope.domain.ClusterManager;\n+import com.sequenceiq.periscope.model.CloudInstanceType;\n+import com.sequenceiq.periscope.service.ClusterService;\n+import com.sequenceiq.periscope.service.security.TlsHttpClientConfigurationService;\n+import com.sequenceiq.periscope.utils.ClusterUtils;\n+\n+@Service\n+public class ClouderaManagerCommunicator {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(ClouderaManagerCommunicator.class);\n+\n+    @Inject\n+    private TlsHttpClientConfigurationService tlsHttpClientConfigurationService;\n+\n+    @Inject\n+    private SecretService secretService;\n+\n+    @Inject\n+    private ClusterService clusterService;\n+\n+    @Inject\n+    private ClouderaManagerApiClientProvider clouderaManagerApiClientProvider;\n+\n+    @Inject\n+    private ClouderaManagerApiFactory clouderaManagerApiFactory;\n+\n+    @Cacheable(cacheNames = \"cloudVMTypeCache\", unless = \"#result == null\", key = \"#cluster.id + #hostGroup\")\n+    public Optional<CloudInstanceType> getCloudVMDetailsForHostGroup(Cluster cluster, String hostGroup, Set<String> hostGroupFqdns) {\n+        try {\n+            LOGGER.debug(\"Retrieving CloudVMType for cluster '{}', hostGroup '{}'\", cluster.getStackCrn(), hostGroupFqdns);\n+            HttpClientConfig httpClientConfig = tlsHttpClientConfigurationService.buildTLSClientConfig(cluster.getStackCrn(),\n+                    cluster.getClusterManager().getHost(), cluster.getTunnel());\n+            ClusterManager cm = cluster.getClusterManager();\n+            String user = secretService.get(cm.getUser());\n+            String pass = secretService.get(cm.getPass());\n+            ApiClient client = clouderaManagerApiClientProvider.getClient(Integer.valueOf(cm.getPort()), user, pass, httpClientConfig);\n+            HostsResourceApi hostsResourceApi = clouderaManagerApiFactory.getHostsResourceApi(client);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a2492e94502bd73f1e5c0528fe18e1c37cdfdefd"}, "originalPosition": 57}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a2492e94502bd73f1e5c0528fe18e1c37cdfdefd", "author": {"user": {"login": "smaniraju", "name": null}}, "url": "https://github.com/hortonworks/cloudbreak/commit/a2492e94502bd73f1e5c0528fe18e1c37cdfdefd", "committedDate": "2020-04-22T11:15:45Z", "message": "CB-5741SupportForRetrievingVMSize"}, "afterCommit": {"oid": "2a9ea3e18ce0504385806badc1535bd3e7c29e0c", "author": {"user": {"login": "smaniraju", "name": null}}, "url": "https://github.com/hortonworks/cloudbreak/commit/2a9ea3e18ce0504385806badc1535bd3e7c29e0c", "committedDate": "2020-04-23T10:48:44Z", "message": "CB-5741SupportForRetrievingVMSize"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk5MDA3ODU0", "url": "https://github.com/hortonworks/cloudbreak/pull/7871#pullrequestreview-399007854", "createdAt": "2020-04-23T11:30:25Z", "commit": {"oid": "2a9ea3e18ce0504385806badc1535bd3e7c29e0c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QxMTozMDoyNVrOGKkh1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QxMTozMDoyNVrOGKkh1Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzczNzQyOQ==", "bodyText": "The number of nodes to be scaled down can be greater than the decommissionCandidates size but that will be forcibly downscaled in the next evaluator run since CB does not support forced downscaling and decommissioning certain node ids simultaneously.", "url": "https://github.com/hortonworks/cloudbreak/pull/7871#discussion_r413737429", "createdAt": "2020-04-23T11:30:25Z", "author": {"login": "smaniraju"}, "path": "autoscale/src/main/java/com/sequenceiq/periscope/monitor/evaluator/load/YarnLoadEvaluator.java", "diffHunk": "@@ -109,73 +114,59 @@ protected void execute() {\n     }\n \n     protected void pollYarnMetricsAndScaleCluster() throws Exception {\n-\n         StackV4Response stackV4Response = cloudbreakCommunicator.getByCrn(cluster.getStackCrn());\n-\n-        String hostGroupInstanceType =\n-                stackResponseUtils.getHostGroupInstanceType(stackV4Response, loadAlert.getScalingPolicy().getHostGroup());\n+        Map<String, String> hostFqdnsToInstanceId = stackResponseUtils.getCloudInstanceIdsForHostGroup(stackV4Response, policyHostGroup);\n+        Set<String> hostGroupFqdns = hostFqdnsToInstanceId.keySet();\n \n         YarnScalingServiceV1Response yarnResponse = yarnMetricsClient\n-                .getYarnMetricsForCluster(cluster, hostGroupInstanceType, stackV4Response.getCloudPlatform());\n-\n-        Map<String, String> hostFqdnsToInstanceId = stackResponseUtils\n-                .getCloudInstanceIdsForHostGroup(stackV4Response, loadAlert.getScalingPolicy().getHostGroup());\n-        yarnResponse.getScaleUpCandidates().ifPresentOrElse(\n-                scaleUpCandidates -> handleScaleUp(hostGroupInstanceType, scaleUpCandidates, hostFqdnsToInstanceId.size()),\n-                () -> {\n-                    yarnResponse.getScaleDownCandidates().ifPresent(\n-                            scaleDownCandidates -> handleScaleDown(scaleDownCandidates, hostFqdnsToInstanceId));\n-                });\n-    }\n-\n-    protected void handleScaleUp(String hostGroupInstanceType, NewNodeManagerCandidates newNMCandidates, Integer existingHostGroupSize) {\n-        Integer yarnRecommendedHostGroupCount =\n-                newNMCandidates.getCandidates().stream()\n-                        .filter(candidate -> candidate.getModelName().equalsIgnoreCase(hostGroupInstanceType))\n-                        .findFirst()\n-                        .map(NewNodeManagerCandidates.Candidate::getCount)\n-                        .orElseThrow(() -> new RuntimeException(String.format(\n-                                \"Yarn Scaling API Response does not contain recommended node count \" +\n-                                        \" for hostGroupInstanceType '%s' in Cluster '%s', Yarn Response '%s'\",\n-                                hostGroupInstanceType, cluster.getStackCrn(), newNMCandidates)));\n-\n-        Integer maxAllowedScaleUp = Math.max(0, loadAlert.getLoadAlertConfiguration().getMaxResourceValue() - existingHostGroupSize);\n-        Integer scaleUpCount = IntStream.of(yarnRecommendedHostGroupCount, DEFAULT_MAX_SCALE_UP_STEP_SIZE, maxAllowedScaleUp)\n-                .min()\n-                .getAsInt();\n-\n-        LOGGER.info(\"ScaleUp NodeCount '{}' for Cluster '{}', HostGroup '{}'\", scaleUpCount,\n-                cluster.getStackCrn(), loadAlert.getScalingPolicy().getHostGroup());\n-\n-        if (scaleUpCount > 0) {\n-            ScalingEvent scalingEvent = new ScalingEvent(loadAlert);\n-            scalingEvent.setHostGroupNodeCount(Optional.of(existingHostGroupSize));\n-            scalingEvent.setScaleUpNodeCount(Optional.of(scaleUpCount));\n-            eventPublisher.publishEvent(scalingEvent);\n-        }\n-    }\n-\n-    protected void handleScaleDown(List<DecommissionCandidate> decommissionCandidates, Map<String, String> hostGroupFqdnsToInstanceId) {\n-        Set<String> hostGroupFqdns = hostGroupFqdnsToInstanceId.keySet();\n-        int maxAllowedScaleDown = Math.max(0, hostGroupFqdns.size() - loadAlert.getLoadAlertConfiguration().getMinResourceValue());\n-\n-        List<String> decommissionHostGroupNodeIds = decommissionCandidates.stream()\n+                .getYarnMetricsForCluster(cluster, policyHostGroup, hostFqdnsToInstanceId.keySet());\n+\n+        Integer existingHostGroupSize = hostFqdnsToInstanceId.size();\n+        Integer maxAllowedScaleUp = loadAlertConfiguration.getMaxResourceValue() - existingHostGroupSize;\n+        Integer maxAllowedScaleDown = Math.max(0, hostGroupFqdns.size() - loadAlertConfiguration.getMinResourceValue());\n+\n+        Integer yarnRecommendedUpscaleCount = yarnResponse.getScaleUpCandidates()\n+                .map(NewNodeManagerCandidates::getCandidates).orElse(List.of()).stream()\n+                .filter(candidate -> candidate.getModelName().equalsIgnoreCase(policyHostGroup))\n+                .findFirst()\n+                .map(NewNodeManagerCandidates.Candidate::getCount)\n+                .map(scaleUpNodeCount -> Math.min(scaleUpNodeCount, DEFAULT_MAX_SCALE_UP_STEP_SIZE))\n+                .map(scaleUpNodeCount -> Math.min(scaleUpNodeCount, maxAllowedScaleUp))\n+                .orElse(0);\n+\n+        List<String> yarnRecommendedDecommissionHosts = yarnResponse.getScaleDownCandidates().orElse(List.of()).stream()\n                 .sorted(Comparator.comparingInt(DecommissionCandidate::getAmCount))\n                 .map(DecommissionCandidate::getNodeId)\n                 .map(nodeFqdn -> nodeFqdn.split(\":\")[0])\n                 .filter(s -> hostGroupFqdns.contains(s))\n                 .limit(maxAllowedScaleDown)\n-                .map(nodeFqdn -> hostGroupFqdnsToInstanceId.get(nodeFqdn))\n+                .map(nodeFqdn -> hostFqdnsToInstanceId.get(nodeFqdn))\n                 .collect(Collectors.toList());\n \n-        LOGGER.info(\"ScaleDown NodeCount '{}' for Cluster '{}', HostGroup '{}', NodeIds '{}'\",\n-                decommissionHostGroupNodeIds.size(), cluster.getStackCrn(), loadAlert.getScalingPolicy().getHostGroup(),\n-                decommissionHostGroupNodeIds);\n+        if (yarnRecommendedUpscaleCount > 0 && yarnRecommendedUpscaleCount <= maxAllowedScaleUp) {\n+            ScalingEvent scalingEvent = new ScalingEvent(loadAlert);\n+            scalingEvent.setHostGroupNodeCount(Optional.of(existingHostGroupSize));\n+            scalingEvent.setScalingNodeCount(Optional.of(yarnRecommendedUpscaleCount));\n+            eventPublisher.publishEvent(scalingEvent);\n+\n+            LOGGER.info(\"ScaleUp NodeCount '{}' for Cluster '{}', HostGroup '{}'\",\n+                    yarnRecommendedUpscaleCount, cluster.getStackCrn(), policyHostGroup);\n+        } else if (maxAllowedScaleDown > 0 && !yarnRecommendedDecommissionHosts.isEmpty()) {\n+            ScalingEvent scalingEvent = new ScalingEvent(loadAlert);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2a9ea3e18ce0504385806badc1535bd3e7c29e0c"}, "originalPosition": 138}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1a0017330a5208245f4e3cb24a10994a09c1d71f", "author": {"user": {"login": "smaniraju", "name": null}}, "url": "https://github.com/hortonworks/cloudbreak/commit/1a0017330a5208245f4e3cb24a10994a09c1d71f", "committedDate": "2020-04-25T13:47:16Z", "message": "CB-5741SupportForRetrievingVMSize"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "2a9ea3e18ce0504385806badc1535bd3e7c29e0c", "author": {"user": {"login": "smaniraju", "name": null}}, "url": "https://github.com/hortonworks/cloudbreak/commit/2a9ea3e18ce0504385806badc1535bd3e7c29e0c", "committedDate": "2020-04-23T10:48:44Z", "message": "CB-5741SupportForRetrievingVMSize"}, "afterCommit": {"oid": "1a0017330a5208245f4e3cb24a10994a09c1d71f", "author": {"user": {"login": "smaniraju", "name": null}}, "url": "https://github.com/hortonworks/cloudbreak/commit/1a0017330a5208245f4e3cb24a10994a09c1d71f", "committedDate": "2020-04-25T13:47:16Z", "message": "CB-5741SupportForRetrievingVMSize"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2232, "cost": 1, "resetAt": "2021-11-01T14:51:55Z"}}}