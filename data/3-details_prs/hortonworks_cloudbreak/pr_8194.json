{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDI2NTkxMzY1", "number": 8194, "title": "CB-7212: Add datalake service changes to perform database backup/restore.", "bodyText": "Changes needed to the datalake service to handle the requests to perform backup and restore of the database backing datalake. This includes exposing API's to perform/restore and also API's to poll the status of those operations. These API internally uses separate flows for backup and restore operations.\nBackup and restore operation return UUID which can be used to track the status of the operation requested.\nI have added tests for the service backing the backup/restore operations.\nPerformed manual testing to make sure the Flow transition is done properly. Here are the state transitions are listed in the sheet below\nhttps://docs.google.com/spreadsheets/d/1rms3b-YGAIeTLf-Pqm4gSnJopoQGQ1O8pyTVRR_86t0/edit?usp=sharing", "createdAt": "2020-06-02T13:39:21Z", "url": "https://github.com/hortonworks/cloudbreak/pull/8194", "merged": true, "mergeCommit": {"oid": "53cfbdf248e4bb4ed35bc0a7dd8c1d274b251168"}, "closed": true, "closedAt": "2020-06-29T16:31:40Z", "author": {"login": "kkalvagadda1"}, "timelineItems": {"totalCount": 30, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcnVHH9gFqTQyMjY5MDUzNA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcwDCrKgFqTQzOTI3NzAzMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIyNjkwNTM0", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#pullrequestreview-422690534", "createdAt": "2020-06-02T13:44:37Z", "commit": {"oid": "eeb0ac24348ecbbc4850b91fd35da885067a3cdd"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxMzo0NDozOFrOGdyTQg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxMzo0NDozOFrOGdyTQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzg4NjAxOA==", "bodyText": "nitpick: This should be DATABASE.\nThe same is true for all the following enums, and for the related @Bean(name = \"\") annotations in the action class.", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r433886018", "createdAt": "2020-06-02T13:44:38Z", "author": {"login": "brycederriso"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseDrState.java", "diffHunk": "@@ -0,0 +1,33 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import com.sequenceiq.flow.core.FlowState;\n+import com.sequenceiq.flow.core.RestartAction;\n+import com.sequenceiq.flow.core.restart.DefaultRestartAction;\n+\n+public enum DatalakeDatabaseDrState implements FlowState {\n+\n+    INIT_STATE,\n+    DATALAKE_DATABSE_BACKUP_START_STATE,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eeb0ac24348ecbbc4850b91fd35da885067a3cdd"}, "originalPosition": 10}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "eeb0ac24348ecbbc4850b91fd35da885067a3cdd", "author": {"user": {"login": "kkalvagadda1", "name": "kalyan kumar kalvagadda"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/eeb0ac24348ecbbc4850b91fd35da885067a3cdd", "committedDate": "2020-06-02T13:38:05Z", "message": "CB-7212: Add datalake service changes to perform database backup/restore."}, "afterCommit": {"oid": "5b53f6a124117c9d051c0bc95aa76a71d4c6f3b7", "author": {"user": {"login": "kkalvagadda1", "name": "kalyan kumar kalvagadda"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/5b53f6a124117c9d051c0bc95aa76a71d4c6f3b7", "committedDate": "2020-06-11T04:10:41Z", "message": "CB-7212: Add datalake service changes to perform database backup/restore."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5b53f6a124117c9d051c0bc95aa76a71d4c6f3b7", "author": {"user": {"login": "kkalvagadda1", "name": "kalyan kumar kalvagadda"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/5b53f6a124117c9d051c0bc95aa76a71d4c6f3b7", "committedDate": "2020-06-11T04:10:41Z", "message": "CB-7212: Add datalake service changes to perform database backup/restore."}, "afterCommit": {"oid": "193405603e46ccc9be38527729240556706c3fe6", "author": {"user": {"login": "kkalvagadda1", "name": "kalyan kumar kalvagadda"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/193405603e46ccc9be38527729240556706c3fe6", "committedDate": "2020-06-15T12:32:59Z", "message": "CB-7212: Add datalake service changes to perform database backup/restore.\n\nAdded changes to accept the backup-id and pass it to CB."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMwODQ4ODIx", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#pullrequestreview-430848821", "createdAt": "2020-06-15T17:27:02Z", "commit": {"oid": "193405603e46ccc9be38527729240556706c3fe6"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQxNzoyNzowMlrOGj7xqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQxODowMDowOFrOGj84MQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDMzMjcxNQ==", "bodyText": "Capitalization and grammer: \"Performs a backup of the database to a provided location\"", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r440332715", "createdAt": "2020-06-15T17:27:02Z", "author": {"login": "hreeve-cloudera"}, "path": "core-api/src/main/java/com/sequenceiq/cloudbreak/doc/OperationDescriptions.java", "diffHunk": "@@ -44,6 +44,8 @@\n         public static final String CHECK_STACK_UPGRADE = \"Checks for upgrade options by name\";\n         public static final String STACK_UPGRADE = \"Upgrades a cluster to the latest CM or CDH version\";\n         public static final String LIST_RETRYABLE_FLOWS = \"List retryable failed flows\";\n+        public static final String DATABASE_BACKUP = \"performs a backup of database to a provided location\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "193405603e46ccc9be38527729240556706c3fe6"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDM1MDc2OQ==", "bodyText": "This is a general comment for the review about how the status works on the cloudbreak side. It might or might not have any affect on what you do with status in your code. But on the cloudbreak side, there is no separate states of requested/started and in-progress. By the time the flow advances to the point where it's updating the stack status, the operation is already in progress. So in my version of these status messages, I had:\nBACKUP_IN_PROGRESS(StatusKind.PROGRESS), BACKUP_FINISHED(StatusKind.FINAL), BACKUP_FAILED(StatusKind.FINAL), RESTORE_IN_PROGRESS(StatusKind.PROGRESS), RESTORE_FINISHED(StatusKind.FINAL), RESTORE_FAILED(StatusKind.FINAL);\nI see you have multiple places where you have a separate start state, and an in-progress state. For the datalake service I think that makes sense, since you can mark it as started as soon as you get the request, and in-progress when you get the flow identifier back from cloudbreak. I just wanted to give you a heads up that cloudbreak won't be using the REQUESTED stattus.", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r440350769", "createdAt": "2020-06-15T18:00:08Z", "author": {"login": "hreeve-cloudera"}, "path": "core-api/src/main/java/com/sequenceiq/cloudbreak/api/endpoint/v4/common/Status.java", "diffHunk": "@@ -12,7 +12,13 @@\n     AVAILABLE(StatusKind.FINAL),\n     UPDATE_IN_PROGRESS(StatusKind.PROGRESS),\n     UPDATE_REQUESTED(StatusKind.PROGRESS),\n+    BACKUP_REQUESTED(StatusKind.PROGRESS),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "193405603e46ccc9be38527729240556706c3fe6"}, "originalPosition": 4}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMwOTQ0NDA3", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#pullrequestreview-430944407", "createdAt": "2020-06-15T19:49:35Z", "commit": {"oid": "193405603e46ccc9be38527729240556706c3fe6"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQxOTo0OTozNVrOGkAaHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQxOTo0OTozNVrOGkAaHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQwODYwNQ==", "bodyText": "To match REST conventions and StackV4Endpoint naming conventions:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                @POST\n          \n          \n            \n                @POST\n          \n          \n            \n                @Path(\"{name}/database_backup\")\n          \n          \n            \n                @Produces(MediaType.APPLICATION_JSON)\n          \n          \n            \n                @ApiOperation(value = DATABASE_BACKUP, nickname = \"databaseBackup\")\n          \n          \n            \n                BackupV4Response backupDatabaseByName(@PathParam(\"workspaceId\") Long workspaceId, @PathParam(\"name\") String name,\n          \n          \n            \n                        @QueryParam(\"location\") String location, @QueryParam(\"backupId\") String backupId);\n          \n          \n            \n            \n          \n          \n            \n                @POST\n          \n          \n            \n                @Path(\"{name}/database_restore\")\n          \n          \n            \n                @Produces(MediaType.APPLICATION_JSON)\n          \n          \n            \n                @ApiOperation(value = DATABASE_RESTORE, nickname = \"databaseRestore\")\n          \n          \n            \n                RestoreV4Response restoreDatabaseByName(@PathParam(\"workspaceId\") Long workspaceId, @PathParam(\"name\") String name,\n          \n          \n            \n                        @QueryParam(\"location\") String location, @QueryParam(\"backupId\") String backupId);", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r440408605", "createdAt": "2020-06-15T19:49:35Z", "author": {"login": "hreeve-cloudera"}, "path": "core-api/src/main/java/com/sequenceiq/cloudbreak/api/endpoint/v4/stacks/StackV4Endpoint.java", "diffHunk": "@@ -269,4 +273,17 @@ FlowIdentifier setClusterMaintenanceMode(@PathParam(\"workspaceId\") Long workspac\n     @ApiOperation(value = UPDATE_SALT, nickname = \"updateSaltByName\")\n     FlowIdentifier updateSaltByName(@PathParam(\"workspaceId\") Long workspaceId, @PathParam(\"name\") String name);\n \n+    @POST", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "193405603e46ccc9be38527729240556706c3fe6"}, "originalPosition": 22}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "293e2940e8ff4fe2597b5e3abdfc779fbab38b8d", "author": {"user": {"login": "kkalvagadda1", "name": "kalyan kumar kalvagadda"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/293e2940e8ff4fe2597b5e3abdfc779fbab38b8d", "committedDate": "2020-06-15T20:38:29Z", "message": "Update core-api/src/main/java/com/sequenceiq/cloudbreak/api/endpoint/v4/stacks/StackV4Endpoint.java\n\nCo-authored-by: hreeve-cloudera <49536206+hreeve-cloudera@users.noreply.github.com>"}, "afterCommit": {"oid": "0d0152ccd99c44623234b376e8701f5ef0328a1b", "author": {"user": {"login": "kkalvagadda1", "name": "kalyan kumar kalvagadda"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/0d0152ccd99c44623234b376e8701f5ef0328a1b", "committedDate": "2020-06-17T12:14:26Z", "message": "CB-7212: Add datalake service changes to perform database backup/restore.\n\nAdded changes to accept the backup-id and pass it to CB."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "0d0152ccd99c44623234b376e8701f5ef0328a1b", "author": {"user": {"login": "kkalvagadda1", "name": "kalyan kumar kalvagadda"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/0d0152ccd99c44623234b376e8701f5ef0328a1b", "committedDate": "2020-06-17T12:14:26Z", "message": "CB-7212: Add datalake service changes to perform database backup/restore.\n\nAdded changes to accept the backup-id and pass it to CB."}, "afterCommit": {"oid": "4f2d9c115f912bb9ba9bc34022900a36c862928b", "author": {"user": {"login": "kkalvagadda1", "name": "kalyan kumar kalvagadda"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/4f2d9c115f912bb9ba9bc34022900a36c862928b", "committedDate": "2020-06-17T15:11:03Z", "message": "CB-7212: Add datalake service changes to perform database backup/restore.\n\nAdded changes to accept the backup-id and pass it to CB."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4f2d9c115f912bb9ba9bc34022900a36c862928b", "author": {"user": {"login": "kkalvagadda1", "name": "kalyan kumar kalvagadda"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/4f2d9c115f912bb9ba9bc34022900a36c862928b", "committedDate": "2020-06-17T15:11:03Z", "message": "CB-7212: Add datalake service changes to perform database backup/restore.\n\nAdded changes to accept the backup-id and pass it to CB."}, "afterCommit": {"oid": "878a27e569cf3d3847035d1e04140228710f8881", "author": {"user": {"login": "kkalvagadda1", "name": "kalyan kumar kalvagadda"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/878a27e569cf3d3847035d1e04140228710f8881", "committedDate": "2020-06-17T15:50:16Z", "message": "CB-7212: Add datalake service changes to perform database backup/restore.\n\nAdded changes to accept the backup-id and pass it to CB."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMyNTY2MDU4", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#pullrequestreview-432566058", "createdAt": "2020-06-17T16:21:55Z", "commit": {"oid": "878a27e569cf3d3847035d1e04140228710f8881"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f632f08e88ae48a78443e3c9b8c9b42641b876da", "author": {"user": {"login": "kkalvagadda1", "name": "kalyan kumar kalvagadda"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/f632f08e88ae48a78443e3c9b8c9b42641b876da", "committedDate": "2020-06-18T14:39:53Z", "message": "fixed a flaky test(CloudStorageValidatorTest)."}, "afterCommit": {"oid": "c2b01428e259a1a1565101b57a98c9371b5fd157", "author": {"user": {"login": "kkalvagadda1", "name": "kalyan kumar kalvagadda"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/c2b01428e259a1a1565101b57a98c9371b5fd157", "committedDate": "2020-06-18T14:51:29Z", "message": "fixed a flaky test(CloudStorageValidatorTest)."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c2b01428e259a1a1565101b57a98c9371b5fd157", "author": {"user": {"login": "kkalvagadda1", "name": "kalyan kumar kalvagadda"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/c2b01428e259a1a1565101b57a98c9371b5fd157", "committedDate": "2020-06-18T14:51:29Z", "message": "fixed a flaky test(CloudStorageValidatorTest)."}, "afterCommit": {"oid": "cc6825762f7475cf94b1e98379c24a3f7b5cd2c8", "author": {"user": {"login": "kkalvagadda1", "name": "kalyan kumar kalvagadda"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/cc6825762f7475cf94b1e98379c24a3f7b5cd2c8", "committedDate": "2020-06-18T14:55:50Z", "message": "CB-7212: Add datalake service changes to perform database backup/restore."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "cc6825762f7475cf94b1e98379c24a3f7b5cd2c8", "author": {"user": {"login": "kkalvagadda1", "name": "kalyan kumar kalvagadda"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/cc6825762f7475cf94b1e98379c24a3f7b5cd2c8", "committedDate": "2020-06-18T14:55:50Z", "message": "CB-7212: Add datalake service changes to perform database backup/restore."}, "afterCommit": {"oid": "e2cf07a07d0db9bd403a8fc69fdb8a98e71270dc", "author": {"user": {"login": "kkalvagadda1", "name": "kalyan kumar kalvagadda"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/e2cf07a07d0db9bd403a8fc69fdb8a98e71270dc", "committedDate": "2020-06-18T14:58:38Z", "message": "CB-7212: Add datalake service changes to perform database backup/restore."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e2cf07a07d0db9bd403a8fc69fdb8a98e71270dc", "author": {"user": {"login": "kkalvagadda1", "name": "kalyan kumar kalvagadda"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/e2cf07a07d0db9bd403a8fc69fdb8a98e71270dc", "committedDate": "2020-06-18T14:58:38Z", "message": "CB-7212: Add datalake service changes to perform database backup/restore."}, "afterCommit": {"oid": "22ed8587748ab6331641c647bad02daf3f52df0d", "author": {"user": {"login": "pdarvasi", "name": null}}, "url": "https://github.com/hortonworks/cloudbreak/commit/22ed8587748ab6331641c647bad02daf3f52df0d", "committedDate": "2020-06-19T11:18:07Z", "message": "Unit test fix"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "22ed8587748ab6331641c647bad02daf3f52df0d", "author": {"user": {"login": "pdarvasi", "name": null}}, "url": "https://github.com/hortonworks/cloudbreak/commit/22ed8587748ab6331641c647bad02daf3f52df0d", "committedDate": "2020-06-19T11:18:07Z", "message": "Unit test fix"}, "afterCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "author": {"user": {"login": "kkalvagadda1", "name": "kalyan kumar kalvagadda"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "committedDate": "2020-06-22T03:24:35Z", "message": "CB-7212: Add datalake service changes to perform database backup/restore."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM1NTg0NTky", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#pullrequestreview-435584592", "createdAt": "2020-06-23T09:02:13Z", "commit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "state": "COMMENTED", "comments": {"totalCount": 22, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwOTowMjoxM1rOGngF9w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxMDowOTowMlrOGnif6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA3MzQ2Mw==", "bodyText": "this should be getBackupDatabaseStatusByName instead", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444073463", "createdAt": "2020-06-23T09:02:13Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/controller/sdx/SdxController.java", "diffHunk": "@@ -246,24 +250,31 @@ public FlowIdentifier stopByCrn(@ResourceCrn String crn) {\n     @Override\n     @CheckPermissionByAccount(action = AuthorizationResourceAction.BACKUP_DATALAKE)\n     public SdxDatabaseBackupResponse backupDatabaseByName(@ResourceName String name, String backupId, String backupLocation) {\n-        return null;\n+        String userCrn = ThreadBasedUserCrnProvider.getUserCrn();\n+        SdxCluster sdxCluster = sdxService.getSdxByNameInAccount(userCrn, name);\n+        return sdxDrService.triggerDatabaseBackup(sdxCluster, backupId, backupLocation);\n     }\n \n     @Override\n     @CheckPermissionByAccount(action = AuthorizationResourceAction.RESTORE_DATALAKE)\n     public SdxDatabaseRestoreResponse restoreDatabaseByName(@ResourceName String name, String backupId, String backupLocation) {\n-        return null;\n+        String userCrn = ThreadBasedUserCrnProvider.getUserCrn();\n+        SdxCluster sdxCluster = sdxService.getSdxByNameInAccount(userCrn, name);\n+        return sdxDrService.triggerDatabaseRestore(sdxCluster, backupId, backupLocation);\n     }\n \n     @Override\n     @CheckPermissionByAccount(action = AuthorizationResourceAction.BACKUP_DATALAKE)\n     public SdxDatabaseBackupStatusResponse backupDatabaseStatusByName(@ResourceName String name, String operationId) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA3Mzc4MA==", "bodyText": "this should be getRestoreDatabaseStatusByName instead", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444073780", "createdAt": "2020-06-23T09:02:42Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/controller/sdx/SdxController.java", "diffHunk": "@@ -246,24 +250,31 @@ public FlowIdentifier stopByCrn(@ResourceCrn String crn) {\n     @Override\n     @CheckPermissionByAccount(action = AuthorizationResourceAction.BACKUP_DATALAKE)\n     public SdxDatabaseBackupResponse backupDatabaseByName(@ResourceName String name, String backupId, String backupLocation) {\n-        return null;\n+        String userCrn = ThreadBasedUserCrnProvider.getUserCrn();\n+        SdxCluster sdxCluster = sdxService.getSdxByNameInAccount(userCrn, name);\n+        return sdxDrService.triggerDatabaseBackup(sdxCluster, backupId, backupLocation);\n     }\n \n     @Override\n     @CheckPermissionByAccount(action = AuthorizationResourceAction.RESTORE_DATALAKE)\n     public SdxDatabaseRestoreResponse restoreDatabaseByName(@ResourceName String name, String backupId, String backupLocation) {\n-        return null;\n+        String userCrn = ThreadBasedUserCrnProvider.getUserCrn();\n+        SdxCluster sdxCluster = sdxService.getSdxByNameInAccount(userCrn, name);\n+        return sdxDrService.triggerDatabaseRestore(sdxCluster, backupId, backupLocation);\n     }\n \n     @Override\n     @CheckPermissionByAccount(action = AuthorizationResourceAction.BACKUP_DATALAKE)\n     public SdxDatabaseBackupStatusResponse backupDatabaseStatusByName(@ResourceName String name, String operationId) {\n-        return null;\n+        String userCrn = ThreadBasedUserCrnProvider.getUserCrn();\n+        SdxCluster sdxCluster = sdxService.getSdxByNameInAccount(userCrn, name);\n+        return sdxDrService.getDatabaseBackupStatus(sdxCluster, operationId);\n     }\n \n     @Override\n     @CheckPermissionByAccount(action = AuthorizationResourceAction.RESTORE_DATALAKE)\n     public SdxDatabaseRestoreStatusResponse restoreDatabaseStatusByName(@ResourceName String name, String operationId) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA3NjcyMg==", "bodyText": "FlowIdentifier should be returned here, the operationId, should be passed as parameter so that notify() can be used instead of notifyDatabaseDrEvent()", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444076722", "createdAt": "2020-06-23T09:07:41Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/SdxReactorFlowManager.java", "diffHunk": "@@ -104,6 +109,18 @@ public FlowIdentifier triggerSdxStopFlow(SdxCluster cluster) {\n         return notify(selector, new SdxStartStopEvent(selector, cluster.getId(), userId));\n     }\n \n+    public String triggerDatalakeDatabaseBackupFlow(Long sdxId, String backupId, String backupLocation) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA3NjgxMg==", "bodyText": "FlowIdentifier should be returned here, the operationId, should be passed as parameter so that notify() can be used instead of notifyDatabaseDrEvent()", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444076812", "createdAt": "2020-06-23T09:07:53Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/SdxReactorFlowManager.java", "diffHunk": "@@ -104,6 +109,18 @@ public FlowIdentifier triggerSdxStopFlow(SdxCluster cluster) {\n         return notify(selector, new SdxStartStopEvent(selector, cluster.getId(), userId));\n     }\n \n+    public String triggerDatalakeDatabaseBackupFlow(Long sdxId, String backupId, String backupLocation) {\n+        String selector = DATALAKE_DATABASE_BACKUP_EVENT.event();\n+        String userId = ThreadBasedUserCrnProvider.getUserCrn();\n+        return notifyDatabaseDrEvent(selector, new DatalakeDatabaseBackupStartEvent(selector, sdxId, userId, backupId, backupLocation));\n+    }\n+\n+    public String triggerDatalakeDatabaseRestoreFlow(Long sdxId, String backupId, String backupLocation) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA3NzA0Mw==", "bodyText": "this is not needed if operationId is passed as parameter", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444077043", "createdAt": "2020-06-23T09:08:19Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/SdxReactorFlowManager.java", "diffHunk": "@@ -132,4 +149,30 @@ private FlowIdentifier notify(String selector, SdxEvent acceptable) {\n         }\n \n     }\n+\n+    String notifyDatabaseDrEvent(String selector,  DatalakeDatabaseDrStartBaseEvent acceptable) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA4NTE0Nw==", "bodyText": "shouldn't we set the status (sdxDrService.updateDatabaseStatusEntry() to INPROGRESS here?", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444085147", "createdAt": "2020-06-23T09:21:46Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseBackupActions.java", "diffHunk": "@@ -0,0 +1,173 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT;\n+\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.statemachine.StateContext;\n+import org.springframework.statemachine.action.Action;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxContext;\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupCouldNotStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupFailedEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupSuccessEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupWaitRequest;\n+import com.sequenceiq.datalake.service.AbstractSdxAction;\n+import com.sequenceiq.datalake.service.sdx.dr.SdxDrService;\n+import com.sequenceiq.flow.core.FlowEvent;\n+import com.sequenceiq.flow.core.FlowParameters;\n+import com.sequenceiq.flow.core.FlowState;\n+\n+@Configuration\n+public class DatalakeDatabaseBackupActions {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(DatalakeDatabaseBackupActions.class);\n+\n+    private static final String OPERATION_ID = \"OPERATION-ID\";\n+\n+    @Inject\n+    private SdxDrService sdxDrService;\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_START_STATE\")\n+    public Action<?, ?> datalakeBackup() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void prepareExecution(DatalakeDatabaseBackupStartEvent payload, Map<Object, Object> variables) {\n+                super.prepareExecution(payload, variables);\n+                variables.put(OPERATION_ID, payload.getDrStatus().getOperationId());\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Datalake database backup has been started for {}\", payload.getResourceId());\n+                sdxDrService.databaseBackup(payload.getDrStatus(),\n+                        payload.getResourceId(),\n+                        payload.getBackupId(),\n+                        payload.getBackupLocation());\n+                sendEvent(context, DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseBackupStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupCouldNotStartEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_IN_PROGRESS_STATE\")\n+    public Action<?, ?> datalakebackupInProgress() {\n+        return new AbstractSdxAction<>(SdxEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext, SdxEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, SdxEvent payload, Map<Object, Object> variables) {\n+                LOGGER.info(\"Datalake database backup is in progress for {} \", payload.getResourceId());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA4OTYzMA==", "bodyText": "this doExecute() is the same as the backupCouldNotStart() one, could you pls. extract it to a common method?", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444089630", "createdAt": "2020-06-23T09:29:00Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseBackupActions.java", "diffHunk": "@@ -0,0 +1,173 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT;\n+\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.statemachine.StateContext;\n+import org.springframework.statemachine.action.Action;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxContext;\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupCouldNotStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupFailedEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupSuccessEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupWaitRequest;\n+import com.sequenceiq.datalake.service.AbstractSdxAction;\n+import com.sequenceiq.datalake.service.sdx.dr.SdxDrService;\n+import com.sequenceiq.flow.core.FlowEvent;\n+import com.sequenceiq.flow.core.FlowParameters;\n+import com.sequenceiq.flow.core.FlowState;\n+\n+@Configuration\n+public class DatalakeDatabaseBackupActions {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(DatalakeDatabaseBackupActions.class);\n+\n+    private static final String OPERATION_ID = \"OPERATION-ID\";\n+\n+    @Inject\n+    private SdxDrService sdxDrService;\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_START_STATE\")\n+    public Action<?, ?> datalakeBackup() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void prepareExecution(DatalakeDatabaseBackupStartEvent payload, Map<Object, Object> variables) {\n+                super.prepareExecution(payload, variables);\n+                variables.put(OPERATION_ID, payload.getDrStatus().getOperationId());\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Datalake database backup has been started for {}\", payload.getResourceId());\n+                sdxDrService.databaseBackup(payload.getDrStatus(),\n+                        payload.getResourceId(),\n+                        payload.getBackupId(),\n+                        payload.getBackupLocation());\n+                sendEvent(context, DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseBackupStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupCouldNotStartEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_IN_PROGRESS_STATE\")\n+    public Action<?, ?> datalakebackupInProgress() {\n+        return new AbstractSdxAction<>(SdxEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext, SdxEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, SdxEvent payload, Map<Object, Object> variables) {\n+                LOGGER.info(\"Datalake database backup is in progress for {} \", payload.getResourceId());\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sendEvent(context, DatalakeDatabaseBackupWaitRequest.from(context, operationId));\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(SdxEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupFailedEvent.from(payload, ex);\n+\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_COULD_NOT_START_STATE\")\n+    public Action<?, ?> backupCouldNotStart() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupCouldNotStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupCouldNotStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupCouldNotStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                Exception exception = payload.getException();\n+                LOGGER.error(\"Datalake database backup could not be started for datalake with id: {}\", payload.getResourceId(), exception);\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sdxDrService.updateDatabaseStatusEntry(operationId, SdxDatabaseDrStatus.Status.FAILED, payload.getException().getMessage());\n+                sendEvent(context, DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseBackupCouldNotStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_FINISHED_STATE\")\n+    public Action<?, ?> finishedBackupAction() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupSuccessEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupSuccessEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupSuccessEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Sdx database backup is finalized with sdx id: {}\", payload.getResourceId());\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sdxDrService.updateDatabaseStatusEntry(operationId, SdxDatabaseDrStatus.Status.SUCCEEDED, null);\n+                sendEvent(context, DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseBackupSuccessEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_FAILED_STATE\")\n+    public Action<?, ?> backupFailed() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupFailedEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupFailedEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupFailedEvent payload, Map<Object, Object> variables) throws Exception {\n+                Exception exception = payload.getException();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 160}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA4OTgzNg==", "bodyText": "Not DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT instead?", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444089836", "createdAt": "2020-06-23T09:29:24Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseBackupActions.java", "diffHunk": "@@ -0,0 +1,173 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT;\n+\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.statemachine.StateContext;\n+import org.springframework.statemachine.action.Action;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxContext;\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupCouldNotStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupFailedEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupSuccessEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupWaitRequest;\n+import com.sequenceiq.datalake.service.AbstractSdxAction;\n+import com.sequenceiq.datalake.service.sdx.dr.SdxDrService;\n+import com.sequenceiq.flow.core.FlowEvent;\n+import com.sequenceiq.flow.core.FlowParameters;\n+import com.sequenceiq.flow.core.FlowState;\n+\n+@Configuration\n+public class DatalakeDatabaseBackupActions {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(DatalakeDatabaseBackupActions.class);\n+\n+    private static final String OPERATION_ID = \"OPERATION-ID\";\n+\n+    @Inject\n+    private SdxDrService sdxDrService;\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_START_STATE\")\n+    public Action<?, ?> datalakeBackup() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void prepareExecution(DatalakeDatabaseBackupStartEvent payload, Map<Object, Object> variables) {\n+                super.prepareExecution(payload, variables);\n+                variables.put(OPERATION_ID, payload.getDrStatus().getOperationId());\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Datalake database backup has been started for {}\", payload.getResourceId());\n+                sdxDrService.databaseBackup(payload.getDrStatus(),\n+                        payload.getResourceId(),\n+                        payload.getBackupId(),\n+                        payload.getBackupLocation());\n+                sendEvent(context, DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseBackupStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupCouldNotStartEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_IN_PROGRESS_STATE\")\n+    public Action<?, ?> datalakebackupInProgress() {\n+        return new AbstractSdxAction<>(SdxEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext, SdxEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, SdxEvent payload, Map<Object, Object> variables) {\n+                LOGGER.info(\"Datalake database backup is in progress for {} \", payload.getResourceId());\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sendEvent(context, DatalakeDatabaseBackupWaitRequest.from(context, operationId));\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(SdxEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupFailedEvent.from(payload, ex);\n+\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_COULD_NOT_START_STATE\")\n+    public Action<?, ?> backupCouldNotStart() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupCouldNotStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupCouldNotStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupCouldNotStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                Exception exception = payload.getException();\n+                LOGGER.error(\"Datalake database backup could not be started for datalake with id: {}\", payload.getResourceId(), exception);\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sdxDrService.updateDatabaseStatusEntry(operationId, SdxDatabaseDrStatus.Status.FAILED, payload.getException().getMessage());\n+                sendEvent(context, DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseBackupCouldNotStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_FINISHED_STATE\")\n+    public Action<?, ?> finishedBackupAction() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupSuccessEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupSuccessEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupSuccessEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Sdx database backup is finalized with sdx id: {}\", payload.getResourceId());\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sdxDrService.updateDatabaseStatusEntry(operationId, SdxDatabaseDrStatus.Status.SUCCEEDED, null);\n+                sendEvent(context, DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseBackupSuccessEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_FAILED_STATE\")\n+    public Action<?, ?> backupFailed() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupFailedEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupFailedEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupFailedEvent payload, Map<Object, Object> variables) throws Exception {\n+                Exception exception = payload.getException();\n+                LOGGER.error(\"Datalake database backup failed for datalake with id: {}\", payload.getResourceId(), exception);\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sdxDrService.updateDatabaseStatusEntry(operationId, SdxDatabaseDrStatus.Status.FAILED, exception.getLocalizedMessage());\n+                sendEvent(context, DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT.event(), payload);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 164}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA5MjczMA==", "bodyText": "Not DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT instead?", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444092730", "createdAt": "2020-06-23T09:34:23Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseBackupFlowConfig.java", "diffHunk": "@@ -0,0 +1,98 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_COULD_NOT_START_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FAILED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_SUCCESS_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.DATALAKE_DATABASE_BACKUP_COULD_NOT_START_STATE;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.DATALAKE_DATABASE_BACKUP_FAILED_STATE;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.DATALAKE_DATABASE_BACKUP_FINISHED_STATE;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.DATALAKE_DATABASE_BACKUP_IN_PROGRESS_STATE;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.DATALAKE_DATABASE_BACKUP_START_STATE;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.FINAL_STATE;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.INIT_STATE;\n+\n+import java.util.List;\n+\n+import org.springframework.stereotype.Component;\n+\n+import com.sequenceiq.flow.core.config.AbstractFlowConfiguration;\n+import com.sequenceiq.flow.core.config.RetryableFlowConfiguration;\n+\n+@Component\n+public class DatalakeDatabaseBackupFlowConfig extends AbstractFlowConfiguration<DatalakeDatabaseDrState, DatalakeDatabaseDrEvent>\n+        implements RetryableFlowConfiguration<DatalakeDatabaseDrEvent> {\n+\n+    private static final List<Transition<DatalakeDatabaseDrState, DatalakeDatabaseDrEvent>> TRANSITIONS =\n+            new Transition.Builder<DatalakeDatabaseDrState, DatalakeDatabaseDrEvent>()\n+                    .defaultFailureEvent(DATALAKE_DATABASE_BACKUP_FAILED_EVENT)\n+\n+                    .from(INIT_STATE)\n+                    .to(DATALAKE_DATABASE_BACKUP_START_STATE)\n+                    .event(DATALAKE_DATABASE_BACKUP_EVENT).noFailureEvent()\n+\n+                    .from(DATALAKE_DATABASE_BACKUP_START_STATE)\n+                    .to(DATALAKE_DATABASE_BACKUP_IN_PROGRESS_STATE)\n+                    .event(DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT)\n+                    .failureState(DATALAKE_DATABASE_BACKUP_COULD_NOT_START_STATE)\n+                    .failureEvent(DATALAKE_DATABASE_BACKUP_COULD_NOT_START_EVENT)\n+\n+                    .from(DATALAKE_DATABASE_BACKUP_IN_PROGRESS_STATE)\n+                    .to(DATALAKE_DATABASE_BACKUP_FINISHED_STATE)\n+                    .event(DATALAKE_DATABASE_BACKUP_SUCCESS_EVENT)\n+                    .failureState(DATALAKE_DATABASE_BACKUP_FAILED_STATE)\n+                    .failureEvent(DATALAKE_DATABASE_BACKUP_FAILED_EVENT)\n+\n+                    .from(DATALAKE_DATABASE_BACKUP_FINISHED_STATE)\n+                    .to(FINAL_STATE)\n+                    .event(DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT).defaultFailureEvent()\n+\n+                    .from(DATALAKE_DATABASE_BACKUP_FAILED_STATE)\n+                    .to(FINAL_STATE)\n+                    .event(DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT).defaultFailureEvent()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA5NTYwMA==", "bodyText": "Shouldn't we upgrade to INPROGRESS here also with updateDatabaseStatusEntry?", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444095600", "createdAt": "2020-06-23T09:39:10Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseRestoreActions.java", "diffHunk": "@@ -0,0 +1,170 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_FAILURE_HANDLED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT;\n+\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.statemachine.StateContext;\n+import org.springframework.statemachine.action.Action;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxContext;\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreCouldNotStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreFailedEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreSuccessEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreWaitRequest;\n+import com.sequenceiq.datalake.service.AbstractSdxAction;\n+import com.sequenceiq.datalake.service.sdx.dr.SdxDrService;\n+import com.sequenceiq.flow.core.FlowEvent;\n+import com.sequenceiq.flow.core.FlowParameters;\n+import com.sequenceiq.flow.core.FlowState;\n+\n+@Configuration\n+public class DatalakeDatabaseRestoreActions {\n+    private static final Logger LOGGER = LoggerFactory.getLogger(DatalakeDatabaseRestoreActions.class);\n+\n+    private static final String OPERATION_ID = \"OPERATION-ID\";\n+\n+    @Inject\n+    private SdxDrService sdxDrService;\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_START_STATE\")\n+    public Action<?, ?> datalakeRestore() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseRestoreStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseRestoreStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void prepareExecution(DatalakeDatabaseRestoreStartEvent payload, Map<Object, Object> variables) {\n+                super.prepareExecution(payload, variables);\n+                variables.put(OPERATION_ID, payload.getDrStatus().getOperationId());\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseRestoreStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Datalake database restore has been started for {}\", payload.getResourceId());\n+                sdxDrService.databaseRestore(payload.getDrStatus(),\n+                        payload.getResourceId(),\n+                        payload.getBackupId(),\n+                        payload.getBackupLocation());\n+                sendEvent(context, DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseRestoreStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseRestoreCouldNotStartEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_IN_PROGRESS_STATE\")\n+    public Action<?, ?> datalakeRestoreInProgress() {\n+        return new AbstractSdxAction<>(SdxEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext, SdxEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, SdxEvent payload, Map<Object, Object> variables) {\n+                LOGGER.info(\"Datalake database restore is in progress for {} \", payload.getResourceId());\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sendEvent(context, DatalakeDatabaseRestoreWaitRequest.from(context, operationId));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA5NjkxMw==", "bodyText": "How about setting the SDX status here to failed as well with sdxStatusService.setStatusForDatalakeAndNotify()?", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444096913", "createdAt": "2020-06-23T09:41:29Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseRestoreActions.java", "diffHunk": "@@ -0,0 +1,170 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_FAILURE_HANDLED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT;\n+\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.statemachine.StateContext;\n+import org.springframework.statemachine.action.Action;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxContext;\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreCouldNotStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreFailedEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreSuccessEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreWaitRequest;\n+import com.sequenceiq.datalake.service.AbstractSdxAction;\n+import com.sequenceiq.datalake.service.sdx.dr.SdxDrService;\n+import com.sequenceiq.flow.core.FlowEvent;\n+import com.sequenceiq.flow.core.FlowParameters;\n+import com.sequenceiq.flow.core.FlowState;\n+\n+@Configuration\n+public class DatalakeDatabaseRestoreActions {\n+    private static final Logger LOGGER = LoggerFactory.getLogger(DatalakeDatabaseRestoreActions.class);\n+\n+    private static final String OPERATION_ID = \"OPERATION-ID\";\n+\n+    @Inject\n+    private SdxDrService sdxDrService;\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_START_STATE\")\n+    public Action<?, ?> datalakeRestore() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseRestoreStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseRestoreStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void prepareExecution(DatalakeDatabaseRestoreStartEvent payload, Map<Object, Object> variables) {\n+                super.prepareExecution(payload, variables);\n+                variables.put(OPERATION_ID, payload.getDrStatus().getOperationId());\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseRestoreStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Datalake database restore has been started for {}\", payload.getResourceId());\n+                sdxDrService.databaseRestore(payload.getDrStatus(),\n+                        payload.getResourceId(),\n+                        payload.getBackupId(),\n+                        payload.getBackupLocation());\n+                sendEvent(context, DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseRestoreStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseRestoreCouldNotStartEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_IN_PROGRESS_STATE\")\n+    public Action<?, ?> datalakeRestoreInProgress() {\n+        return new AbstractSdxAction<>(SdxEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext, SdxEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, SdxEvent payload, Map<Object, Object> variables) {\n+                LOGGER.info(\"Datalake database restore is in progress for {} \", payload.getResourceId());\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sendEvent(context, DatalakeDatabaseRestoreWaitRequest.from(context, operationId));\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(SdxEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseRestoreFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_COULD_NOT_START_STATE\")\n+    public Action<?, ?> restoreCouldNotStart() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseRestoreCouldNotStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseRestoreCouldNotStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseRestoreCouldNotStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                Exception exception = payload.getException();\n+                LOGGER.error(\"Datalake database restore could not be started for datalake with id: {}\", payload.getResourceId(), exception);\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sdxDrService.updateDatabaseStatusEntry(operationId, SdxDatabaseDrStatus.Status.FAILED, exception.getLocalizedMessage());\n+                sendEvent(context, DATALAKE_DATABASE_RESTORE_FAILURE_HANDLED_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseRestoreCouldNotStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseRestoreFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_FINISHED_STATE\")\n+    public Action<?, ?> finishedRestoreAction() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseRestoreSuccessEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseRestoreSuccessEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseRestoreSuccessEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Sdx database restore is finalized with sdx id: {}\", payload.getResourceId());\n+                sdxDrService.updateDatabaseStatusEntry(payload.getOperationId(), SdxDatabaseDrStatus.Status.SUCCEEDED, null);\n+                sendEvent(context, DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseRestoreSuccessEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseRestoreFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_FAILED_STATE\")\n+    public Action<?, ?> restoreFailed() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseRestoreFailedEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseRestoreFailedEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseRestoreFailedEvent payload, Map<Object, Object> variables) throws Exception {\n+                Exception exception = payload.getException();\n+                LOGGER.error(\"Datalake database restore could not be started for datalake with id: {}\", payload.getResourceId(), exception);\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sdxDrService.updateDatabaseStatusEntry(operationId, SdxDatabaseDrStatus.Status.FAILED, exception.getLocalizedMessage());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 160}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA5ODAzNg==", "bodyText": "this should be FAILED_HANDLED_EVENT instead", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444098036", "createdAt": "2020-06-23T09:43:26Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseRestoreActions.java", "diffHunk": "@@ -0,0 +1,170 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_FAILURE_HANDLED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT;\n+\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.statemachine.StateContext;\n+import org.springframework.statemachine.action.Action;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxContext;\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreCouldNotStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreFailedEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreSuccessEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreWaitRequest;\n+import com.sequenceiq.datalake.service.AbstractSdxAction;\n+import com.sequenceiq.datalake.service.sdx.dr.SdxDrService;\n+import com.sequenceiq.flow.core.FlowEvent;\n+import com.sequenceiq.flow.core.FlowParameters;\n+import com.sequenceiq.flow.core.FlowState;\n+\n+@Configuration\n+public class DatalakeDatabaseRestoreActions {\n+    private static final Logger LOGGER = LoggerFactory.getLogger(DatalakeDatabaseRestoreActions.class);\n+\n+    private static final String OPERATION_ID = \"OPERATION-ID\";\n+\n+    @Inject\n+    private SdxDrService sdxDrService;\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_START_STATE\")\n+    public Action<?, ?> datalakeRestore() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseRestoreStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseRestoreStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void prepareExecution(DatalakeDatabaseRestoreStartEvent payload, Map<Object, Object> variables) {\n+                super.prepareExecution(payload, variables);\n+                variables.put(OPERATION_ID, payload.getDrStatus().getOperationId());\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseRestoreStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Datalake database restore has been started for {}\", payload.getResourceId());\n+                sdxDrService.databaseRestore(payload.getDrStatus(),\n+                        payload.getResourceId(),\n+                        payload.getBackupId(),\n+                        payload.getBackupLocation());\n+                sendEvent(context, DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseRestoreStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseRestoreCouldNotStartEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_IN_PROGRESS_STATE\")\n+    public Action<?, ?> datalakeRestoreInProgress() {\n+        return new AbstractSdxAction<>(SdxEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext, SdxEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, SdxEvent payload, Map<Object, Object> variables) {\n+                LOGGER.info(\"Datalake database restore is in progress for {} \", payload.getResourceId());\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sendEvent(context, DatalakeDatabaseRestoreWaitRequest.from(context, operationId));\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(SdxEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseRestoreFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_COULD_NOT_START_STATE\")\n+    public Action<?, ?> restoreCouldNotStart() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseRestoreCouldNotStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseRestoreCouldNotStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseRestoreCouldNotStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                Exception exception = payload.getException();\n+                LOGGER.error(\"Datalake database restore could not be started for datalake with id: {}\", payload.getResourceId(), exception);\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sdxDrService.updateDatabaseStatusEntry(operationId, SdxDatabaseDrStatus.Status.FAILED, exception.getLocalizedMessage());\n+                sendEvent(context, DATALAKE_DATABASE_RESTORE_FAILURE_HANDLED_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseRestoreCouldNotStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseRestoreFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_FINISHED_STATE\")\n+    public Action<?, ?> finishedRestoreAction() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseRestoreSuccessEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseRestoreSuccessEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseRestoreSuccessEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Sdx database restore is finalized with sdx id: {}\", payload.getResourceId());\n+                sdxDrService.updateDatabaseStatusEntry(payload.getOperationId(), SdxDatabaseDrStatus.Status.SUCCEEDED, null);\n+                sendEvent(context, DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseRestoreSuccessEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseRestoreFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_FAILED_STATE\")\n+    public Action<?, ?> restoreFailed() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseRestoreFailedEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseRestoreFailedEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseRestoreFailedEvent payload, Map<Object, Object> variables) throws Exception {\n+                Exception exception = payload.getException();\n+                LOGGER.error(\"Datalake database restore could not be started for datalake with id: {}\", payload.getResourceId(), exception);\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sdxDrService.updateDatabaseStatusEntry(operationId, SdxDatabaseDrStatus.Status.FAILED, exception.getLocalizedMessage());\n+                sendEvent(context, DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT.event(), payload);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 161}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA5ODE4OA==", "bodyText": "this should be FAILED_HANDLED_EVENT instead", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444098188", "createdAt": "2020-06-23T09:43:42Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseRestoreFlowConfig.java", "diffHunk": "@@ -0,0 +1,98 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_COULD_NOT_START_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_FAILED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_FAILURE_HANDLED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_SUCCESS_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.DATALAKE_DATABASE_RESTORE_COULD_NOT_START_STATE;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.DATALAKE_DATABASE_RESTORE_FAILED_STATE;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.DATALAKE_DATABASE_RESTORE_FINISHED_STATE;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.DATALAKE_DATABASE_RESTORE_IN_PROGRESS_STATE;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.DATALAKE_DATABASE_RESTORE_START_STATE;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.FINAL_STATE;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.INIT_STATE;\n+\n+import java.util.List;\n+\n+import org.springframework.stereotype.Component;\n+\n+import com.sequenceiq.flow.core.config.AbstractFlowConfiguration;\n+import com.sequenceiq.flow.core.config.RetryableFlowConfiguration;\n+\n+@Component\n+public class DatalakeDatabaseRestoreFlowConfig extends AbstractFlowConfiguration<DatalakeDatabaseDrState, DatalakeDatabaseDrEvent>\n+        implements RetryableFlowConfiguration<DatalakeDatabaseDrEvent> {\n+\n+    private static final List<Transition<DatalakeDatabaseDrState, DatalakeDatabaseDrEvent>> TRANSITIONS =\n+            new Transition.Builder<DatalakeDatabaseDrState, DatalakeDatabaseDrEvent>()\n+                    .defaultFailureEvent(DATALAKE_DATABASE_RESTORE_FAILED_EVENT)\n+\n+                    .from(INIT_STATE)\n+                    .to(DATALAKE_DATABASE_RESTORE_START_STATE)\n+                    .event(DATALAKE_DATABASE_RESTORE_EVENT).noFailureEvent()\n+\n+                    .from(DATALAKE_DATABASE_RESTORE_START_STATE)\n+                    .to(DATALAKE_DATABASE_RESTORE_IN_PROGRESS_STATE)\n+                    .event(DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT)\n+                    .failureState(DATALAKE_DATABASE_RESTORE_COULD_NOT_START_STATE)\n+                    .failureEvent(DATALAKE_DATABASE_RESTORE_COULD_NOT_START_EVENT)\n+\n+                    .from(DATALAKE_DATABASE_RESTORE_IN_PROGRESS_STATE)\n+                    .to(DATALAKE_DATABASE_RESTORE_FINISHED_STATE)\n+                    .event(DATALAKE_DATABASE_RESTORE_SUCCESS_EVENT)\n+                    .failureState(DATALAKE_DATABASE_RESTORE_FAILED_STATE)\n+                    .failureEvent(DATALAKE_DATABASE_RESTORE_FAILED_EVENT)\n+\n+                    .from(DATALAKE_DATABASE_RESTORE_FINISHED_STATE)\n+                    .to(FINAL_STATE)\n+                    .event(DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT).defaultFailureEvent()\n+\n+                    .from(DATALAKE_DATABASE_RESTORE_FAILED_STATE)\n+                    .to(FINAL_STATE)\n+                    .event(DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT).defaultFailureEvent()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEwMjMxOQ==", "bodyText": "We should include the FlowIdentifier in SdxDatabaseBackupResponse, too", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444102319", "createdAt": "2020-06-23T09:50:43Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEwMjQzNg==", "bodyText": "We should include the FlowIdentifier in SdxDatabaseRestoreResponse, too", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444102436", "createdAt": "2020-06-23T09:50:58Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEwMzM5MA==", "bodyText": "should return FlowIdentifier", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444103390", "createdAt": "2020-06-23T09:52:32Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEwMzUwNw==", "bodyText": "should return FlowIdentifier", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444103507", "createdAt": "2020-06-23T09:52:43Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    private String triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(clusterId, backupId, backupLocation);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEwNTYyNQ==", "bodyText": "We might want to set SDX status to something to indicate the restore with sdxStatusService.setStatusForDatalakeAndNotify(), too", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444105625", "createdAt": "2020-06-23T09:56:13Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    private String triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, backupV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEwNjIyMQ==", "bodyText": "these 3 lines have multiple occurrences pls extract to a separate method.", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444106221", "createdAt": "2020-06-23T09:57:17Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    private String triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, backupV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, restoreV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 123}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEwODY5NA==", "bodyText": "typo: successful + doe -> for", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444108694", "createdAt": "2020-06-23T10:01:29Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    private String triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, backupV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, restoreV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id %d not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void waitCloudbreakFlow(Long id, PollingConfig pollingConfig, String pollingMessage) {\n+        SdxCluster sdxCluster = sdxClusterRepository.findById(id).orElseThrow(notFound(\"SDX cluster\", id));\n+        Polling.waitPeriodly(pollingConfig.getSleepTime(), pollingConfig.getSleepTimeUnit())\n+                .stopIfException(pollingConfig.getStopPollingIfExceptionOccured())\n+                .stopAfterDelay(pollingConfig.getDuration(), pollingConfig.getDurationTimeUnit())\n+                .run(() -> checkDatabaseDrStatus(sdxCluster, pollingMessage));\n+    }\n+\n+    private AttemptResult<StackV4Response> checkDatabaseDrStatus(SdxCluster sdxCluster, String pollingMessage) throws JsonProcessingException {\n+        LOGGER.info(\"{} polling cloudbreak for stack status: '{}' in '{}' env\", pollingMessage, sdxCluster.getClusterName(), sdxCluster.getEnvName());\n+        try {\n+            if (PollGroup.CANCELLED.equals(DatalakeInMemoryStateStore.get(sdxCluster.getId()))) {\n+                LOGGER.info(\"{} polling cancelled in inmemory store, id: {}\", pollingMessage, sdxCluster.getId());\n+                return AttemptResults.breakFor(pollingMessage + \" polling cancelled in inmemory store, id: \" + sdxCluster.getId());\n+            } else {\n+                CloudbreakFlowService.FlowState flowState = cloudbreakFlowService.getLastKnownFlowState(sdxCluster);\n+                if (RUNNING.equals(flowState)) {\n+                    LOGGER.info(\"{} polling will continue, cluster has an active flow in Cloudbreak, id: {}\", pollingMessage, sdxCluster.getId());\n+                    return AttemptResults.justContinue();\n+                } else {\n+                    return getStackResponseAttemptResult(sdxCluster, pollingMessage, flowState);\n+                }\n+            }\n+        } catch (javax.ws.rs.NotFoundException e) {\n+            LOGGER.debug(\"Stack not found on CB side \" + sdxCluster.getClusterName(), e);\n+            return AttemptResults.breakFor(\"Stack not found on CB side \" + sdxCluster.getClusterName());\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> getStackResponseAttemptResult(SdxCluster sdxCluster, String pollingMessage, CloudbreakFlowService.FlowState flowState)\n+            throws JsonProcessingException {\n+        StackV4Response stackV4Response = stackV4Endpoint.get(0L, sdxCluster.getClusterName(), Collections.emptySet());\n+        LOGGER.info(\"Response from cloudbreak: {}\", JsonUtil.writeValueAsString(stackV4Response));\n+        ClusterV4Response cluster = stackV4Response.getCluster();\n+        if (stackAndClusterAvailable(stackV4Response, cluster)) {\n+            return sdxDrSucceeded(stackV4Response);\n+        } else {\n+            if (Status.BACKUP_FAILED.equals(stackV4Response.getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getStatus())) {\n+                LOGGER.info(\"{} failed for Stack {} with status {}\", pollingMessage, stackV4Response.getName(), stackV4Response.getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getStatusReason(), pollingMessage);\n+            } else if (Status.BACKUP_FAILED.equals(stackV4Response.getCluster().getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getCluster().getStatus())) {\n+                LOGGER.info(\"{} failed for Cluster {} status {}\", pollingMessage, stackV4Response.getCluster().getName(),\n+                        stackV4Response.getCluster().getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getCluster().getStatusReason(), pollingMessage);\n+            } else {\n+                if (FINISHED.equals(flowState)) {\n+                    LOGGER.info(\"Flow finished, but Backup/Restore is not complete: {}\", sdxCluster.getClusterName());\n+                    return sdxDrFailed(sdxCluster, \"stack is in improper state\", pollingMessage);\n+                } else {\n+                    return AttemptResults.justContinue();\n+                }\n+            }\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrFailed(SdxCluster sdxCluster, String statusReason, String pollingMessage) {\n+        LOGGER.info(\"{} failed, statusReason: {}\", pollingMessage, statusReason);\n+        return AttemptResults.breakFor(\"SDX \" + pollingMessage + \" failed '\" + sdxCluster.getClusterName() + \"', \" + statusReason);\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrSucceeded(StackV4Response stackV4Response) {\n+        LOGGER.info(\"Database DR operation doe SDX cluster {} is successfull\", stackV4Response.getCluster().getName());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 196}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDExMDUzNg==", "bodyText": "we might log this as it should not happen", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444110536", "createdAt": "2020-06-23T10:04:46Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    private String triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, backupV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, restoreV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id %d not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void waitCloudbreakFlow(Long id, PollingConfig pollingConfig, String pollingMessage) {\n+        SdxCluster sdxCluster = sdxClusterRepository.findById(id).orElseThrow(notFound(\"SDX cluster\", id));\n+        Polling.waitPeriodly(pollingConfig.getSleepTime(), pollingConfig.getSleepTimeUnit())\n+                .stopIfException(pollingConfig.getStopPollingIfExceptionOccured())\n+                .stopAfterDelay(pollingConfig.getDuration(), pollingConfig.getDurationTimeUnit())\n+                .run(() -> checkDatabaseDrStatus(sdxCluster, pollingMessage));\n+    }\n+\n+    private AttemptResult<StackV4Response> checkDatabaseDrStatus(SdxCluster sdxCluster, String pollingMessage) throws JsonProcessingException {\n+        LOGGER.info(\"{} polling cloudbreak for stack status: '{}' in '{}' env\", pollingMessage, sdxCluster.getClusterName(), sdxCluster.getEnvName());\n+        try {\n+            if (PollGroup.CANCELLED.equals(DatalakeInMemoryStateStore.get(sdxCluster.getId()))) {\n+                LOGGER.info(\"{} polling cancelled in inmemory store, id: {}\", pollingMessage, sdxCluster.getId());\n+                return AttemptResults.breakFor(pollingMessage + \" polling cancelled in inmemory store, id: \" + sdxCluster.getId());\n+            } else {\n+                CloudbreakFlowService.FlowState flowState = cloudbreakFlowService.getLastKnownFlowState(sdxCluster);\n+                if (RUNNING.equals(flowState)) {\n+                    LOGGER.info(\"{} polling will continue, cluster has an active flow in Cloudbreak, id: {}\", pollingMessage, sdxCluster.getId());\n+                    return AttemptResults.justContinue();\n+                } else {\n+                    return getStackResponseAttemptResult(sdxCluster, pollingMessage, flowState);\n+                }\n+            }\n+        } catch (javax.ws.rs.NotFoundException e) {\n+            LOGGER.debug(\"Stack not found on CB side \" + sdxCluster.getClusterName(), e);\n+            return AttemptResults.breakFor(\"Stack not found on CB side \" + sdxCluster.getClusterName());\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> getStackResponseAttemptResult(SdxCluster sdxCluster, String pollingMessage, CloudbreakFlowService.FlowState flowState)\n+            throws JsonProcessingException {\n+        StackV4Response stackV4Response = stackV4Endpoint.get(0L, sdxCluster.getClusterName(), Collections.emptySet());\n+        LOGGER.info(\"Response from cloudbreak: {}\", JsonUtil.writeValueAsString(stackV4Response));\n+        ClusterV4Response cluster = stackV4Response.getCluster();\n+        if (stackAndClusterAvailable(stackV4Response, cluster)) {\n+            return sdxDrSucceeded(stackV4Response);\n+        } else {\n+            if (Status.BACKUP_FAILED.equals(stackV4Response.getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getStatus())) {\n+                LOGGER.info(\"{} failed for Stack {} with status {}\", pollingMessage, stackV4Response.getName(), stackV4Response.getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getStatusReason(), pollingMessage);\n+            } else if (Status.BACKUP_FAILED.equals(stackV4Response.getCluster().getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getCluster().getStatus())) {\n+                LOGGER.info(\"{} failed for Cluster {} status {}\", pollingMessage, stackV4Response.getCluster().getName(),\n+                        stackV4Response.getCluster().getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getCluster().getStatusReason(), pollingMessage);\n+            } else {\n+                if (FINISHED.equals(flowState)) {\n+                    LOGGER.info(\"Flow finished, but Backup/Restore is not complete: {}\", sdxCluster.getClusterName());\n+                    return sdxDrFailed(sdxCluster, \"stack is in improper state\", pollingMessage);\n+                } else {\n+                    return AttemptResults.justContinue();\n+                }\n+            }\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrFailed(SdxCluster sdxCluster, String statusReason, String pollingMessage) {\n+        LOGGER.info(\"{} failed, statusReason: {}\", pollingMessage, statusReason);\n+        return AttemptResults.breakFor(\"SDX \" + pollingMessage + \" failed '\" + sdxCluster.getClusterName() + \"', \" + statusReason);\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrSucceeded(StackV4Response stackV4Response) {\n+        LOGGER.info(\"Database DR operation doe SDX cluster {} is successfull\", stackV4Response.getCluster().getName());\n+        return AttemptResults.finishWith(stackV4Response);\n+    }\n+\n+    private boolean stackAndClusterAvailable(StackV4Response stackV4Response, ClusterV4Response cluster) {\n+        return stackV4Response.getStatus().isAvailable()\n+                && cluster != null\n+                && cluster.getStatus() != null\n+                && cluster.getStatus().isAvailable();\n+    }\n+\n+    /**\n+     * Updates the status of the database backup/restore operation.\n+     * @param operationId Operation Id\n+     * @param status Status of the operation\n+     * @param failedReason Failure reason, if any.\n+     */\n+    public void updateDatabaseStatusEntry(String operationId, SdxDatabaseDrStatus.Status status, String failedReason) {\n+        if (Strings.isNullOrEmpty(operationId)) {\n+            return;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 215}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDExMjg3Mw==", "bodyText": "the first 6 lines of getDatabaseBackupStatus and getDatabaseRestoreStatus are almost the same -> should be extracted to a common method with SdxDatabaseDrStatusTypeEnum as parameter", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444112873", "createdAt": "2020-06-23T10:09:02Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    private String triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, backupV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, restoreV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id %d not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void waitCloudbreakFlow(Long id, PollingConfig pollingConfig, String pollingMessage) {\n+        SdxCluster sdxCluster = sdxClusterRepository.findById(id).orElseThrow(notFound(\"SDX cluster\", id));\n+        Polling.waitPeriodly(pollingConfig.getSleepTime(), pollingConfig.getSleepTimeUnit())\n+                .stopIfException(pollingConfig.getStopPollingIfExceptionOccured())\n+                .stopAfterDelay(pollingConfig.getDuration(), pollingConfig.getDurationTimeUnit())\n+                .run(() -> checkDatabaseDrStatus(sdxCluster, pollingMessage));\n+    }\n+\n+    private AttemptResult<StackV4Response> checkDatabaseDrStatus(SdxCluster sdxCluster, String pollingMessage) throws JsonProcessingException {\n+        LOGGER.info(\"{} polling cloudbreak for stack status: '{}' in '{}' env\", pollingMessage, sdxCluster.getClusterName(), sdxCluster.getEnvName());\n+        try {\n+            if (PollGroup.CANCELLED.equals(DatalakeInMemoryStateStore.get(sdxCluster.getId()))) {\n+                LOGGER.info(\"{} polling cancelled in inmemory store, id: {}\", pollingMessage, sdxCluster.getId());\n+                return AttemptResults.breakFor(pollingMessage + \" polling cancelled in inmemory store, id: \" + sdxCluster.getId());\n+            } else {\n+                CloudbreakFlowService.FlowState flowState = cloudbreakFlowService.getLastKnownFlowState(sdxCluster);\n+                if (RUNNING.equals(flowState)) {\n+                    LOGGER.info(\"{} polling will continue, cluster has an active flow in Cloudbreak, id: {}\", pollingMessage, sdxCluster.getId());\n+                    return AttemptResults.justContinue();\n+                } else {\n+                    return getStackResponseAttemptResult(sdxCluster, pollingMessage, flowState);\n+                }\n+            }\n+        } catch (javax.ws.rs.NotFoundException e) {\n+            LOGGER.debug(\"Stack not found on CB side \" + sdxCluster.getClusterName(), e);\n+            return AttemptResults.breakFor(\"Stack not found on CB side \" + sdxCluster.getClusterName());\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> getStackResponseAttemptResult(SdxCluster sdxCluster, String pollingMessage, CloudbreakFlowService.FlowState flowState)\n+            throws JsonProcessingException {\n+        StackV4Response stackV4Response = stackV4Endpoint.get(0L, sdxCluster.getClusterName(), Collections.emptySet());\n+        LOGGER.info(\"Response from cloudbreak: {}\", JsonUtil.writeValueAsString(stackV4Response));\n+        ClusterV4Response cluster = stackV4Response.getCluster();\n+        if (stackAndClusterAvailable(stackV4Response, cluster)) {\n+            return sdxDrSucceeded(stackV4Response);\n+        } else {\n+            if (Status.BACKUP_FAILED.equals(stackV4Response.getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getStatus())) {\n+                LOGGER.info(\"{} failed for Stack {} with status {}\", pollingMessage, stackV4Response.getName(), stackV4Response.getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getStatusReason(), pollingMessage);\n+            } else if (Status.BACKUP_FAILED.equals(stackV4Response.getCluster().getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getCluster().getStatus())) {\n+                LOGGER.info(\"{} failed for Cluster {} status {}\", pollingMessage, stackV4Response.getCluster().getName(),\n+                        stackV4Response.getCluster().getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getCluster().getStatusReason(), pollingMessage);\n+            } else {\n+                if (FINISHED.equals(flowState)) {\n+                    LOGGER.info(\"Flow finished, but Backup/Restore is not complete: {}\", sdxCluster.getClusterName());\n+                    return sdxDrFailed(sdxCluster, \"stack is in improper state\", pollingMessage);\n+                } else {\n+                    return AttemptResults.justContinue();\n+                }\n+            }\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrFailed(SdxCluster sdxCluster, String statusReason, String pollingMessage) {\n+        LOGGER.info(\"{} failed, statusReason: {}\", pollingMessage, statusReason);\n+        return AttemptResults.breakFor(\"SDX \" + pollingMessage + \" failed '\" + sdxCluster.getClusterName() + \"', \" + statusReason);\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrSucceeded(StackV4Response stackV4Response) {\n+        LOGGER.info(\"Database DR operation doe SDX cluster {} is successfull\", stackV4Response.getCluster().getName());\n+        return AttemptResults.finishWith(stackV4Response);\n+    }\n+\n+    private boolean stackAndClusterAvailable(StackV4Response stackV4Response, ClusterV4Response cluster) {\n+        return stackV4Response.getStatus().isAvailable()\n+                && cluster != null\n+                && cluster.getStatus() != null\n+                && cluster.getStatus().isAvailable();\n+    }\n+\n+    /**\n+     * Updates the status of the database backup/restore operation.\n+     * @param operationId Operation Id\n+     * @param status Status of the operation\n+     * @param failedReason Failure reason, if any.\n+     */\n+    public void updateDatabaseStatusEntry(String operationId, SdxDatabaseDrStatus.Status status, String failedReason) {\n+        if (Strings.isNullOrEmpty(operationId)) {\n+            return;\n+        }\n+        SdxDatabaseDrStatus drStatus = sdxDatabaseDrStatusRepository.findSdxDatabaseDrStatusByOperationId(operationId);\n+        drStatus.setStatus(status);\n+        if (!Strings.isNullOrEmpty(failedReason)) {\n+            drStatus.setStatusReason(failedReason);\n+        }\n+        sdxDatabaseDrStatusRepository.save(drStatus);\n+    }\n+\n+    /**\n+     * Gets the status of the database backup operation.\n+     * @param sdxCluster Sdx cluster on which the backup operation is performed.\n+     * @param operationId Operation Id\n+     * @return Backup status\n+     */\n+    public SdxDatabaseBackupStatusResponse getDatabaseBackupStatus(SdxCluster sdxCluster, String operationId) {\n+        SdxDatabaseDrStatus drStatus = sdxDatabaseDrStatusRepository.findSdxDatabaseDrStatusByOperationId(operationId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 232}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM1NjQwNjgx", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#pullrequestreview-435640681", "createdAt": "2020-06-23T10:15:04Z", "commit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 17, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxMDoxNTowNFrOGnitIA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxMDo1Njo1NVrOGnkBwA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDExNjI1Ng==", "bodyText": "you shouldn't specify the name here", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444116256", "createdAt": "2020-06-23T10:15:04Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/SdxDatabaseDrStatus.java", "diffHunk": "@@ -0,0 +1,102 @@\n+package com.sequenceiq.datalake.entity;\n+\n+import java.util.UUID;\n+\n+import javax.persistence.Entity;\n+import javax.persistence.EnumType;\n+import javax.persistence.Enumerated;\n+import javax.persistence.GeneratedValue;\n+import javax.persistence.GenerationType;\n+import javax.persistence.Id;\n+import javax.persistence.SequenceGenerator;\n+import javax.persistence.Table;\n+import javax.persistence.UniqueConstraint;\n+import javax.validation.constraints.NotNull;\n+\n+@Entity\n+@Table(name = \"sdxDatabaseDrstatus\", uniqueConstraints = @UniqueConstraint(columnNames = {\"operationId\"}))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDExNzk3OQ==", "bodyText": "please move them out from the entity", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444117979", "createdAt": "2020-06-23T10:18:09Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/SdxDatabaseDrStatus.java", "diffHunk": "@@ -0,0 +1,102 @@\n+package com.sequenceiq.datalake.entity;\n+\n+import java.util.UUID;\n+\n+import javax.persistence.Entity;\n+import javax.persistence.EnumType;\n+import javax.persistence.Enumerated;\n+import javax.persistence.GeneratedValue;\n+import javax.persistence.GenerationType;\n+import javax.persistence.Id;\n+import javax.persistence.SequenceGenerator;\n+import javax.persistence.Table;\n+import javax.persistence.UniqueConstraint;\n+import javax.validation.constraints.NotNull;\n+\n+@Entity\n+@Table(name = \"sdxDatabaseDrstatus\", uniqueConstraints = @UniqueConstraint(columnNames = {\"operationId\"}))\n+public class SdxDatabaseDrStatus {\n+    public enum SdxDatabaseDrStatusTypeEnum {\n+        BACKUP,\n+        RESTORE\n+    }\n+\n+    public enum Status {\n+        INIT,\n+        TRIGGERRED,\n+        INPROGRESS,\n+        SUCCEEDED,\n+        FAILED\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEyMDI1OQ==", "bodyText": "please use converter", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444120259", "createdAt": "2020-06-23T10:22:20Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/SdxDatabaseDrStatus.java", "diffHunk": "@@ -0,0 +1,102 @@\n+package com.sequenceiq.datalake.entity;\n+\n+import java.util.UUID;\n+\n+import javax.persistence.Entity;\n+import javax.persistence.EnumType;\n+import javax.persistence.Enumerated;\n+import javax.persistence.GeneratedValue;\n+import javax.persistence.GenerationType;\n+import javax.persistence.Id;\n+import javax.persistence.SequenceGenerator;\n+import javax.persistence.Table;\n+import javax.persistence.UniqueConstraint;\n+import javax.validation.constraints.NotNull;\n+\n+@Entity\n+@Table(name = \"sdxDatabaseDrstatus\", uniqueConstraints = @UniqueConstraint(columnNames = {\"operationId\"}))\n+public class SdxDatabaseDrStatus {\n+    public enum SdxDatabaseDrStatusTypeEnum {\n+        BACKUP,\n+        RESTORE\n+    }\n+\n+    public enum Status {\n+        INIT,\n+        TRIGGERRED,\n+        INPROGRESS,\n+        SUCCEEDED,\n+        FAILED\n+    }\n+\n+    @Id\n+    @GeneratedValue(strategy = GenerationType.AUTO, generator = \"sdx_status_generator\")\n+    @SequenceGenerator(name = \"sdx_status_generator\", sequenceName = \"sdxstatus_id_seq\", allocationSize = 1)\n+    private Long id;\n+\n+    @NotNull\n+    @Enumerated(EnumType.STRING)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEyMDMwOQ==", "bodyText": "here too", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444120309", "createdAt": "2020-06-23T10:22:27Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/SdxDatabaseDrStatus.java", "diffHunk": "@@ -0,0 +1,102 @@\n+package com.sequenceiq.datalake.entity;\n+\n+import java.util.UUID;\n+\n+import javax.persistence.Entity;\n+import javax.persistence.EnumType;\n+import javax.persistence.Enumerated;\n+import javax.persistence.GeneratedValue;\n+import javax.persistence.GenerationType;\n+import javax.persistence.Id;\n+import javax.persistence.SequenceGenerator;\n+import javax.persistence.Table;\n+import javax.persistence.UniqueConstraint;\n+import javax.validation.constraints.NotNull;\n+\n+@Entity\n+@Table(name = \"sdxDatabaseDrstatus\", uniqueConstraints = @UniqueConstraint(columnNames = {\"operationId\"}))\n+public class SdxDatabaseDrStatus {\n+    public enum SdxDatabaseDrStatusTypeEnum {\n+        BACKUP,\n+        RESTORE\n+    }\n+\n+    public enum Status {\n+        INIT,\n+        TRIGGERRED,\n+        INPROGRESS,\n+        SUCCEEDED,\n+        FAILED\n+    }\n+\n+    @Id\n+    @GeneratedValue(strategy = GenerationType.AUTO, generator = \"sdx_status_generator\")\n+    @SequenceGenerator(name = \"sdx_status_generator\", sequenceName = \"sdxstatus_id_seq\", allocationSize = 1)\n+    private Long id;\n+\n+    @NotNull\n+    @Enumerated(EnumType.STRING)\n+    private SdxDatabaseDrStatusTypeEnum operationType;\n+\n+    @NotNull\n+    private Long sdxClusterId;\n+\n+    private String operationId;\n+\n+    private String statusReason;\n+\n+    @NotNull\n+    @Enumerated(EnumType.STRING)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEyNjk4OQ==", "bodyText": "throws Exception is not necessary. same for all the doExecute", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444126989", "createdAt": "2020-06-23T10:35:25Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseBackupActions.java", "diffHunk": "@@ -0,0 +1,173 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT;\n+\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.statemachine.StateContext;\n+import org.springframework.statemachine.action.Action;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxContext;\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupCouldNotStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupFailedEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupSuccessEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupWaitRequest;\n+import com.sequenceiq.datalake.service.AbstractSdxAction;\n+import com.sequenceiq.datalake.service.sdx.dr.SdxDrService;\n+import com.sequenceiq.flow.core.FlowEvent;\n+import com.sequenceiq.flow.core.FlowParameters;\n+import com.sequenceiq.flow.core.FlowState;\n+\n+@Configuration\n+public class DatalakeDatabaseBackupActions {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(DatalakeDatabaseBackupActions.class);\n+\n+    private static final String OPERATION_ID = \"OPERATION-ID\";\n+\n+    @Inject\n+    private SdxDrService sdxDrService;\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_START_STATE\")\n+    public Action<?, ?> datalakeBackup() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void prepareExecution(DatalakeDatabaseBackupStartEvent payload, Map<Object, Object> variables) {\n+                super.prepareExecution(payload, variables);\n+                variables.put(OPERATION_ID, payload.getDrStatus().getOperationId());\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupStartEvent payload, Map<Object, Object> variables) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEyOTA0NA==", "bodyText": "I won't go through all of your request, but please check them all.\nThe parent class already has defined this with EventSelectorUtil so you should depend on that instead of overriding it with the same", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444129044", "createdAt": "2020-06-23T10:39:17Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/event/DatalakeDatabaseBackupCouldNotStartEvent.java", "diffHunk": "@@ -0,0 +1,33 @@\n+package com.sequenceiq.datalake.flow.dr.event;\n+\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+\n+public class DatalakeDatabaseBackupCouldNotStartEvent extends SdxEvent {\n+    private final Exception exception;\n+\n+    public DatalakeDatabaseBackupCouldNotStartEvent(Long sdxId, String userId, Exception exception) {\n+        super(sdxId, userId);\n+        this.exception = exception;\n+    }\n+\n+    public static DatalakeDatabaseBackupCouldNotStartEvent from(SdxEvent event, Exception exception) {\n+        return new DatalakeDatabaseBackupCouldNotStartEvent(event.getResourceId(), event.getUserId(), exception);\n+    }\n+\n+    @Override\n+    public String selector() {\n+        return \"DatalakeDatabaseBackupCouldNotStartEvent\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEyOTk2OQ==", "bodyText": "please check my comment on one of your event, and use there and here also EventSelectorUtil instead of duplicating string everywhere", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444129969", "createdAt": "2020-06-23T10:41:03Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseDrEvent.java", "diffHunk": "@@ -0,0 +1,32 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import com.sequenceiq.flow.core.FlowEvent;\n+\n+public enum DatalakeDatabaseDrEvent implements FlowEvent {\n+\n+    DATALAKE_DATABASE_BACKUP_EVENT(\"DATALAKE_DATABASE_BACKUP_EVENT\"),\n+    DATALAKE_DATABASE_BACKUP_COULD_NOT_START_EVENT(\"DATALAKE_DATABASE_BACKUP_COULD_NOT_START_EVENT\"),\n+    DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT(\"DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT\"),\n+    DATALAKE_DATABASE_BACKUP_SUCCESS_EVENT(\"DATALAKE_DATABASE_BACKUP_SUCCESS_EVENT\"),\n+    DATALAKE_DATABASE_BACKUP_FAILED_EVENT(\"DATALAKE_DATABASE_BACKUP_FAILED_EVENT\"),\n+    DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT(\"DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT\"),\n+    DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT(\"DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT\"),\n+    DATALAKE_DATABASE_RESTORE_EVENT(\"DATALAKE_DATABASE_RESTORE_EVENT\"),\n+    DATALAKE_DATABASE_RESTORE_COULD_NOT_START_EVENT(\"DATALAKE_DATABASE_RESTORE_COULD_NOT_START_EVENT\"),\n+    DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT(\"DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT\"),\n+    DATALAKE_DATABASE_RESTORE_SUCCESS_EVENT(\"DATALAKE_DATABASE_RESTORE_SUCCESS_EVENT\"),\n+    DATALAKE_DATABASE_RESTORE_FAILED_EVENT(\"DATALAKE_DATABASE_RESTORE_FAILED_EVENT\"),\n+    DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT(\"DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT\"),\n+    DATALAKE_DATABASE_RESTORE_FAILURE_HANDLED_EVENT(\"DATALAKE_DATABASE_RESTORE_FAILURE_HANDLED_EVENT\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzMTU4OA==", "bodyText": "you should use either plain String with concatenation or StringBuilder.\nalso final is unnecessary", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444131588", "createdAt": "2020-06-23T10:44:19Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/event/DatalakeDatabaseRestoreCouldNotStartEvent.java", "diffHunk": "@@ -0,0 +1,33 @@\n+package com.sequenceiq.datalake.flow.dr.event;\n+\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+\n+public class DatalakeDatabaseRestoreCouldNotStartEvent extends SdxEvent {\n+    private final Exception exception;\n+\n+    public DatalakeDatabaseRestoreCouldNotStartEvent(Long sdxId, String userId, Exception exception) {\n+        super(\"DatalakeDatabaseRestoreCouldNotStartEvent\", sdxId, userId);\n+        this.exception = exception;\n+    }\n+\n+    public static DatalakeDatabaseRestoreCouldNotStartEvent from(SdxEvent event, Exception exception) {\n+        return new DatalakeDatabaseRestoreCouldNotStartEvent(event.getResourceId(), event.getUserId(), exception);\n+    }\n+\n+    @Override\n+    public String selector() {\n+        return \"DatalakeDatabaseRestoreCouldNotStartEvent\";\n+    }\n+\n+    public Exception getException() {\n+        return exception;\n+    }\n+\n+    @Override\n+    public String toString() {\n+        final StringBuffer sb = new StringBuffer(\"DatalakeDatabaseRestoreCouldNotStartEvent{\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzMzQxOQ==", "bodyText": "this class needs some identation fix, idea fixed 8 lines for me", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444133419", "createdAt": "2020-06-23T10:47:45Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzMzYyNg==", "bodyText": "if service is the name I think the annotation should be service", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444133626", "createdAt": "2020-06-23T10:48:09Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzMzkxMQ==", "bodyText": "if we need this comment then the name of the class is wrong", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444133911", "createdAt": "2020-06-23T10:48:35Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzNDE0NQ==", "bodyText": "unused", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444134145", "createdAt": "2020-06-23T10:49:05Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzNDIwMg==", "bodyText": "unused", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444134202", "createdAt": "2020-06-23T10:49:12Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzNTQxMw==", "bodyText": "I think this class should be broken into 3\n\nbackup\nrestore\ncommon part\nwhat do you think?", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444135413", "createdAt": "2020-06-23T10:51:38Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzMzQxOQ=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzNjA3NA==", "bodyText": "this condition is a bit hard to read and also almost the same as in getDatabaseRestoreStatus\nit would worth to refactor them into a method", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444136074", "createdAt": "2020-06-23T10:53:05Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    private String triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, backupV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, restoreV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id %d not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void waitCloudbreakFlow(Long id, PollingConfig pollingConfig, String pollingMessage) {\n+        SdxCluster sdxCluster = sdxClusterRepository.findById(id).orElseThrow(notFound(\"SDX cluster\", id));\n+        Polling.waitPeriodly(pollingConfig.getSleepTime(), pollingConfig.getSleepTimeUnit())\n+                .stopIfException(pollingConfig.getStopPollingIfExceptionOccured())\n+                .stopAfterDelay(pollingConfig.getDuration(), pollingConfig.getDurationTimeUnit())\n+                .run(() -> checkDatabaseDrStatus(sdxCluster, pollingMessage));\n+    }\n+\n+    private AttemptResult<StackV4Response> checkDatabaseDrStatus(SdxCluster sdxCluster, String pollingMessage) throws JsonProcessingException {\n+        LOGGER.info(\"{} polling cloudbreak for stack status: '{}' in '{}' env\", pollingMessage, sdxCluster.getClusterName(), sdxCluster.getEnvName());\n+        try {\n+            if (PollGroup.CANCELLED.equals(DatalakeInMemoryStateStore.get(sdxCluster.getId()))) {\n+                LOGGER.info(\"{} polling cancelled in inmemory store, id: {}\", pollingMessage, sdxCluster.getId());\n+                return AttemptResults.breakFor(pollingMessage + \" polling cancelled in inmemory store, id: \" + sdxCluster.getId());\n+            } else {\n+                CloudbreakFlowService.FlowState flowState = cloudbreakFlowService.getLastKnownFlowState(sdxCluster);\n+                if (RUNNING.equals(flowState)) {\n+                    LOGGER.info(\"{} polling will continue, cluster has an active flow in Cloudbreak, id: {}\", pollingMessage, sdxCluster.getId());\n+                    return AttemptResults.justContinue();\n+                } else {\n+                    return getStackResponseAttemptResult(sdxCluster, pollingMessage, flowState);\n+                }\n+            }\n+        } catch (javax.ws.rs.NotFoundException e) {\n+            LOGGER.debug(\"Stack not found on CB side \" + sdxCluster.getClusterName(), e);\n+            return AttemptResults.breakFor(\"Stack not found on CB side \" + sdxCluster.getClusterName());\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> getStackResponseAttemptResult(SdxCluster sdxCluster, String pollingMessage, CloudbreakFlowService.FlowState flowState)\n+            throws JsonProcessingException {\n+        StackV4Response stackV4Response = stackV4Endpoint.get(0L, sdxCluster.getClusterName(), Collections.emptySet());\n+        LOGGER.info(\"Response from cloudbreak: {}\", JsonUtil.writeValueAsString(stackV4Response));\n+        ClusterV4Response cluster = stackV4Response.getCluster();\n+        if (stackAndClusterAvailable(stackV4Response, cluster)) {\n+            return sdxDrSucceeded(stackV4Response);\n+        } else {\n+            if (Status.BACKUP_FAILED.equals(stackV4Response.getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getStatus())) {\n+                LOGGER.info(\"{} failed for Stack {} with status {}\", pollingMessage, stackV4Response.getName(), stackV4Response.getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getStatusReason(), pollingMessage);\n+            } else if (Status.BACKUP_FAILED.equals(stackV4Response.getCluster().getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getCluster().getStatus())) {\n+                LOGGER.info(\"{} failed for Cluster {} status {}\", pollingMessage, stackV4Response.getCluster().getName(),\n+                        stackV4Response.getCluster().getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getCluster().getStatusReason(), pollingMessage);\n+            } else {\n+                if (FINISHED.equals(flowState)) {\n+                    LOGGER.info(\"Flow finished, but Backup/Restore is not complete: {}\", sdxCluster.getClusterName());\n+                    return sdxDrFailed(sdxCluster, \"stack is in improper state\", pollingMessage);\n+                } else {\n+                    return AttemptResults.justContinue();\n+                }\n+            }\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrFailed(SdxCluster sdxCluster, String statusReason, String pollingMessage) {\n+        LOGGER.info(\"{} failed, statusReason: {}\", pollingMessage, statusReason);\n+        return AttemptResults.breakFor(\"SDX \" + pollingMessage + \" failed '\" + sdxCluster.getClusterName() + \"', \" + statusReason);\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrSucceeded(StackV4Response stackV4Response) {\n+        LOGGER.info(\"Database DR operation doe SDX cluster {} is successfull\", stackV4Response.getCluster().getName());\n+        return AttemptResults.finishWith(stackV4Response);\n+    }\n+\n+    private boolean stackAndClusterAvailable(StackV4Response stackV4Response, ClusterV4Response cluster) {\n+        return stackV4Response.getStatus().isAvailable()\n+                && cluster != null\n+                && cluster.getStatus() != null\n+                && cluster.getStatus().isAvailable();\n+    }\n+\n+    /**\n+     * Updates the status of the database backup/restore operation.\n+     * @param operationId Operation Id\n+     * @param status Status of the operation\n+     * @param failedReason Failure reason, if any.\n+     */\n+    public void updateDatabaseStatusEntry(String operationId, SdxDatabaseDrStatus.Status status, String failedReason) {\n+        if (Strings.isNullOrEmpty(operationId)) {\n+            return;\n+        }\n+        SdxDatabaseDrStatus drStatus = sdxDatabaseDrStatusRepository.findSdxDatabaseDrStatusByOperationId(operationId);\n+        drStatus.setStatus(status);\n+        if (!Strings.isNullOrEmpty(failedReason)) {\n+            drStatus.setStatusReason(failedReason);\n+        }\n+        sdxDatabaseDrStatusRepository.save(drStatus);\n+    }\n+\n+    /**\n+     * Gets the status of the database backup operation.\n+     * @param sdxCluster Sdx cluster on which the backup operation is performed.\n+     * @param operationId Operation Id\n+     * @return Backup status\n+     */\n+    public SdxDatabaseBackupStatusResponse getDatabaseBackupStatus(SdxCluster sdxCluster, String operationId) {\n+        SdxDatabaseDrStatus drStatus = sdxDatabaseDrStatusRepository.findSdxDatabaseDrStatusByOperationId(operationId);\n+        if ((drStatus == null) || (!drStatus.getSdxClusterId().equals(sdxCluster.getId()))\n+                || (!drStatus.getOperationType().equals(SdxDatabaseDrStatus.SdxDatabaseDrStatusTypeEnum.BACKUP))) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 234}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzNzQ1NA==", "bodyText": "this if-else thing got out of hand. please refactor parts of into separate method with meaningful names so it would be easier to follow what happens", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444137454", "createdAt": "2020-06-23T10:55:53Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    private String triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, backupV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, restoreV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id %d not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void waitCloudbreakFlow(Long id, PollingConfig pollingConfig, String pollingMessage) {\n+        SdxCluster sdxCluster = sdxClusterRepository.findById(id).orElseThrow(notFound(\"SDX cluster\", id));\n+        Polling.waitPeriodly(pollingConfig.getSleepTime(), pollingConfig.getSleepTimeUnit())\n+                .stopIfException(pollingConfig.getStopPollingIfExceptionOccured())\n+                .stopAfterDelay(pollingConfig.getDuration(), pollingConfig.getDurationTimeUnit())\n+                .run(() -> checkDatabaseDrStatus(sdxCluster, pollingMessage));\n+    }\n+\n+    private AttemptResult<StackV4Response> checkDatabaseDrStatus(SdxCluster sdxCluster, String pollingMessage) throws JsonProcessingException {\n+        LOGGER.info(\"{} polling cloudbreak for stack status: '{}' in '{}' env\", pollingMessage, sdxCluster.getClusterName(), sdxCluster.getEnvName());\n+        try {\n+            if (PollGroup.CANCELLED.equals(DatalakeInMemoryStateStore.get(sdxCluster.getId()))) {\n+                LOGGER.info(\"{} polling cancelled in inmemory store, id: {}\", pollingMessage, sdxCluster.getId());\n+                return AttemptResults.breakFor(pollingMessage + \" polling cancelled in inmemory store, id: \" + sdxCluster.getId());\n+            } else {\n+                CloudbreakFlowService.FlowState flowState = cloudbreakFlowService.getLastKnownFlowState(sdxCluster);\n+                if (RUNNING.equals(flowState)) {\n+                    LOGGER.info(\"{} polling will continue, cluster has an active flow in Cloudbreak, id: {}\", pollingMessage, sdxCluster.getId());\n+                    return AttemptResults.justContinue();\n+                } else {\n+                    return getStackResponseAttemptResult(sdxCluster, pollingMessage, flowState);\n+                }\n+            }\n+        } catch (javax.ws.rs.NotFoundException e) {\n+            LOGGER.debug(\"Stack not found on CB side \" + sdxCluster.getClusterName(), e);\n+            return AttemptResults.breakFor(\"Stack not found on CB side \" + sdxCluster.getClusterName());\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> getStackResponseAttemptResult(SdxCluster sdxCluster, String pollingMessage, CloudbreakFlowService.FlowState flowState)\n+            throws JsonProcessingException {\n+        StackV4Response stackV4Response = stackV4Endpoint.get(0L, sdxCluster.getClusterName(), Collections.emptySet());\n+        LOGGER.info(\"Response from cloudbreak: {}\", JsonUtil.writeValueAsString(stackV4Response));\n+        ClusterV4Response cluster = stackV4Response.getCluster();\n+        if (stackAndClusterAvailable(stackV4Response, cluster)) {\n+            return sdxDrSucceeded(stackV4Response);\n+        } else {\n+            if (Status.BACKUP_FAILED.equals(stackV4Response.getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getStatus())) {\n+                LOGGER.info(\"{} failed for Stack {} with status {}\", pollingMessage, stackV4Response.getName(), stackV4Response.getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getStatusReason(), pollingMessage);\n+            } else if (Status.BACKUP_FAILED.equals(stackV4Response.getCluster().getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getCluster().getStatus())) {\n+                LOGGER.info(\"{} failed for Cluster {} status {}\", pollingMessage, stackV4Response.getCluster().getName(),\n+                        stackV4Response.getCluster().getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getCluster().getStatusReason(), pollingMessage);\n+            } else {\n+                if (FINISHED.equals(flowState)) {\n+                    LOGGER.info(\"Flow finished, but Backup/Restore is not complete: {}\", sdxCluster.getClusterName());\n+                    return sdxDrFailed(sdxCluster, \"stack is in improper state\", pollingMessage);\n+                } else {\n+                    return AttemptResults.justContinue();\n+                }\n+            }\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 187}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzNzkyMA==", "bodyText": "you might want to use WebApplicationExceptionMessageExtractor here", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444137920", "createdAt": "2020-06-23T10:56:55Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    private String triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, backupV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, restoreV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id %d not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 130}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6a9b83770f2c7ce85ab96a03e060a8a654147c46", "author": {"user": {"login": "kkalvagadda1", "name": "kalyan kumar kalvagadda"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/6a9b83770f2c7ce85ab96a03e060a8a654147c46", "committedDate": "2020-06-23T20:33:19Z", "message": "addressed review comments."}, "afterCommit": {"oid": "7bff4f9ec8d26fb09462b89b93099f341d595cef", "author": {"user": {"login": "kkalvagadda1", "name": "kalyan kumar kalvagadda"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/7bff4f9ec8d26fb09462b89b93099f341d595cef", "committedDate": "2020-06-24T15:37:00Z", "message": "addressed review comments."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ecd24732a9b469a300ba12eeb4349db50e5cd950", "author": {"user": {"login": "kkalvagadda1", "name": "kalyan kumar kalvagadda"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/ecd24732a9b469a300ba12eeb4349db50e5cd950", "committedDate": "2020-06-24T23:11:16Z", "message": "fixed issues observed in e2e testing."}, "afterCommit": {"oid": "5f09eca14c5c14a0d91249d689024d88c8163506", "author": {"user": {"login": "kkalvagadda1", "name": "kalyan kumar kalvagadda"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/5f09eca14c5c14a0d91249d689024d88c8163506", "committedDate": "2020-06-25T12:06:49Z", "message": "fixed issues observed in e2e testing."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "846513d7e307dcf1a6f55f14e4936b6363d2fa33", "author": {"user": {"login": "kkalvagadda1", "name": "kalyan kumar kalvagadda"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/846513d7e307dcf1a6f55f14e4936b6363d2fa33", "committedDate": "2020-06-25T15:50:00Z", "message": "fixed failure observed in e2e testing."}, "afterCommit": {"oid": "5675e74f382ccd9d117a2c32c7a69a8ba9401d04", "author": {"user": {"login": "kkalvagadda1", "name": "kalyan kumar kalvagadda"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/5675e74f382ccd9d117a2c32c7a69a8ba9401d04", "committedDate": "2020-06-25T16:50:01Z", "message": "fixed failure observed in e2e testing."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM4MTY4Mzgz", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#pullrequestreview-438168383", "createdAt": "2020-06-26T09:57:29Z", "commit": {"oid": "6a9b83770f2c7ce85ab96a03e060a8a654147c46"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM4MjM1Njk5", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#pullrequestreview-438235699", "createdAt": "2020-06-26T11:58:04Z", "commit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMTo1ODowNFrOGpeMfw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMzowMzo0N1rOGpgFTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEzOTUxOQ==", "bodyText": "why would you skip check in case of finished state?\nalso it looks like is changed in the other PR too", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446139519", "createdAt": "2020-06-26T11:58:04Z", "author": {"login": "lacikaaa"}, "path": "core/src/main/java/com/sequenceiq/cloudbreak/job/StackStatusCheckerJob.java", "diffHunk": "@@ -142,8 +142,10 @@ protected void executeInternal(JobExecutionContext context) throws JobExecutionE\n                 Status.EXTERNAL_DATABASE_CREATION_IN_PROGRESS,\n                 Status.BACKUP_IN_PROGRESS,\n                 Status.BACKUP_FAILED,\n+                Status.BACKUP_FINISHED,\n                 Status.RESTORE_IN_PROGRESS,\n-                Status.RESTORE_FAILED\n+                Status.RESTORE_FAILED,\n+                Status.RESTORE_FINISHED", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE1MTg3Mg==", "bodyText": "after having another glimpse at your pr, I'm confused why would you need this altogether.\nI mean we have flowid, with that you should be able to create a status on the fly, as this is also generated from the state of the flow.\nSo I think you should drop all the operationid stuff, return with the flowid, and the query endpoint should expect flowid instead of operationid. (right now your response also have flowid in it)\nI know in freeipa we have this, but back then we didn't have this for flow, and also usersync doesn't run in flow. But if we would like to go this way, then we should move from returning flowid on all endpoint and make operation general, but I'm not sure this would worth it right now", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446151872", "createdAt": "2020-06-26T12:25:44Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/SdxDatabaseDrStatus.java", "diffHunk": "@@ -0,0 +1,92 @@\n+package com.sequenceiq.datalake.entity;\n+\n+import java.util.UUID;\n+\n+import javax.persistence.Convert;\n+import javax.persistence.Entity;\n+import javax.persistence.GeneratedValue;\n+import javax.persistence.GenerationType;\n+import javax.persistence.Id;\n+import javax.persistence.SequenceGenerator;\n+import javax.persistence.Table;\n+import javax.persistence.UniqueConstraint;\n+import javax.validation.constraints.NotNull;\n+\n+import com.sequenceiq.datalake.converter.SdxDatabaseDrOperationTypeEnumConverter;\n+import com.sequenceiq.datalake.converter.SdxDatabaseDrStatusTypeEnumConverter;\n+\n+@Entity\n+@Table(uniqueConstraints = @UniqueConstraint(columnNames = {\"operationId\"}))\n+public class SdxDatabaseDrStatus {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE1NzA2Nw==", "bodyText": "before the return line you should build the MDCContext using MdcBuilder", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446157067", "createdAt": "2020-06-26T12:36:59Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDatabaseDrService.java", "diffHunk": "@@ -0,0 +1,309 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_EVENT;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Service;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.auth.ThreadBasedUserCrnProvider;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.exception.WebApplicationExceptionMessageExtractor;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrOperationTypeEnum;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatusTypeEnum;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreStartEvent;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.flow.api.model.FlowIdentifier;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Service\n+public class SdxDatabaseDrService {\n+\n+    private static final int MAX_SIZE_OF_FAILURE_REASON = 254;\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDatabaseDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private WebApplicationExceptionMessageExtractor webApplicationExceptionMessageExtractor;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        return triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        return triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2MzEzNQ==", "bodyText": "as backup and restore 2 separate flow, it should have 2 separate state and event enum", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446163135", "createdAt": "2020-06-26T12:49:14Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseDrState.java", "diffHunk": "@@ -0,0 +1,35 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import com.sequenceiq.flow.core.FlowState;\n+import com.sequenceiq.flow.core.RestartAction;\n+import com.sequenceiq.flow.core.restart.DefaultRestartAction;\n+\n+public enum DatalakeDatabaseDrState implements FlowState {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2NDk5NA==", "bodyText": "could be final", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446164994", "createdAt": "2020-06-26T12:53:07Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/event/DatalakeDatabaseBackupStartEvent.java", "diffHunk": "@@ -0,0 +1,25 @@\n+package com.sequenceiq.datalake.flow.dr.event;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrOperationTypeEnum;\n+\n+public class DatalakeDatabaseBackupStartEvent extends DatalakeDatabaseDrStartBaseEvent {\n+\n+    private String backupId;\n+\n+    private String backupLocation;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2NTExOA==", "bodyText": "could be final", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446165118", "createdAt": "2020-06-26T12:53:21Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/event/DatalakeDatabaseBackupSuccessEvent.java", "diffHunk": "@@ -0,0 +1,17 @@\n+package com.sequenceiq.datalake.flow.dr.event;\n+\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+\n+public class DatalakeDatabaseBackupSuccessEvent extends SdxEvent {\n+    private String operationId;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2NTM5NQ==", "bodyText": "coudl be final", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446165395", "createdAt": "2020-06-26T12:53:58Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/event/DatalakeDatabaseDrStartBaseEvent.java", "diffHunk": "@@ -0,0 +1,19 @@\n+package com.sequenceiq.datalake.flow.dr.event;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrOperationTypeEnum;\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+\n+public class DatalakeDatabaseDrStartBaseEvent extends SdxEvent  {\n+    private SdxDatabaseDrStatus drStatus;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2NTYwMA==", "bodyText": "final", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446165600", "createdAt": "2020-06-26T12:54:23Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/event/DatalakeDatabaseRestoreStartEvent.java", "diffHunk": "@@ -0,0 +1,25 @@\n+package com.sequenceiq.datalake.flow.dr.event;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrOperationTypeEnum;\n+\n+public class DatalakeDatabaseRestoreStartEvent extends DatalakeDatabaseDrStartBaseEvent {\n+    private String backupId;\n+\n+    private String backupLocation;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2NTk1Mg==", "bodyText": "should be configurable", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446165952", "createdAt": "2020-06-26T12:55:02Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/handler/DatalakeDatabaseBackupWaitHandler.java", "diffHunk": "@@ -0,0 +1,69 @@\n+package com.sequenceiq.datalake.flow.dr.handler;\n+\n+import java.util.concurrent.TimeUnit;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.exception.PollerException;\n+import com.dyngr.exception.PollerStoppedException;\n+import com.dyngr.exception.UserBreakException;\n+import com.sequenceiq.cloudbreak.common.event.Selectable;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupFailedEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupSuccessEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupWaitRequest;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.dr.SdxDatabaseDrService;\n+import com.sequenceiq.flow.event.EventSelectorUtil;\n+import com.sequenceiq.flow.reactor.api.handler.ExceptionCatcherEventHandler;\n+\n+@Component\n+public class DatalakeDatabaseBackupWaitHandler extends ExceptionCatcherEventHandler<DatalakeDatabaseBackupWaitRequest> {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(DatalakeDatabaseBackupWaitHandler.class);\n+\n+    private static final int SLEEP_TIME_IN_SEC = 20;\n+\n+    private static final int DURATION_IN_MINUTES = 90;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2NzM1Mg==", "bodyText": "the 2 wait handler is almost the same, might be worth to refactor them into one", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446167352", "createdAt": "2020-06-26T12:57:41Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/handler/DatalakeDatabaseRestoreWaitHandler.java", "diffHunk": "@@ -0,0 +1,69 @@\n+package com.sequenceiq.datalake.flow.dr.handler;\n+\n+import java.util.concurrent.TimeUnit;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.exception.PollerException;\n+import com.dyngr.exception.PollerStoppedException;\n+import com.dyngr.exception.UserBreakException;\n+import com.sequenceiq.cloudbreak.common.event.Selectable;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreFailedEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreSuccessEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreWaitRequest;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.dr.SdxDatabaseDrService;\n+import com.sequenceiq.flow.event.EventSelectorUtil;\n+import com.sequenceiq.flow.reactor.api.handler.ExceptionCatcherEventHandler;\n+\n+@Component\n+public class DatalakeDatabaseRestoreWaitHandler extends ExceptionCatcherEventHandler<DatalakeDatabaseRestoreWaitRequest> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2OTYxOQ==", "bodyText": "return Status.BACKUP_FINISHED.equals(status) ||\n                Status.RESTORE_FINISHED.equals(status);", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446169619", "createdAt": "2020-06-26T13:01:59Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDatabaseDrService.java", "diffHunk": "@@ -0,0 +1,309 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_EVENT;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Service;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.auth.ThreadBasedUserCrnProvider;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.exception.WebApplicationExceptionMessageExtractor;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrOperationTypeEnum;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatusTypeEnum;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreStartEvent;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.flow.api.model.FlowIdentifier;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Service\n+public class SdxDatabaseDrService {\n+\n+    private static final int MAX_SIZE_OF_FAILURE_REASON = 254;\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDatabaseDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private WebApplicationExceptionMessageExtractor webApplicationExceptionMessageExtractor;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        return triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        return triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }\n+\n+    private SdxDatabaseBackupResponse triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        String selector = DATALAKE_DATABASE_BACKUP_EVENT.event();\n+        String userId = ThreadBasedUserCrnProvider.getUserCrn();\n+        DatalakeDatabaseBackupStartEvent startEvent = new DatalakeDatabaseBackupStartEvent(selector, clusterId, userId, backupId, backupLocation);\n+        FlowIdentifier flowIdentifier = sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(startEvent);\n+        return new SdxDatabaseBackupResponse(startEvent.getDrStatus().getOperationId(), flowIdentifier);\n+    }\n+\n+    private SdxDatabaseRestoreResponse triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        String selector = DATALAKE_DATABASE_RESTORE_EVENT.event();\n+        String userId = ThreadBasedUserCrnProvider.getUserCrn();\n+        DatalakeDatabaseRestoreStartEvent startEvent = new DatalakeDatabaseRestoreStartEvent(selector, clusterId, userId, backupId, backupLocation);\n+        FlowIdentifier flowIdentifier = sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(startEvent);\n+        return new SdxDatabaseRestoreResponse(startEvent.getDrStatus().getOperationId(), flowIdentifier);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                updateSuccessStatus(drStatus.getOperationId(), sdxCluster, backupV4Response.getFlowIdentifier(),\n+                        SdxDatabaseDrStatusTypeEnum.TRIGGERRED);\n+            }, () -> {\n+                updateFailureStatus(drStatus.getOperationId(), clusterId, String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+            });\n+        } catch (WebApplicationException e) {\n+            String errorMessage = webApplicationExceptionMessageExtractor.getErrorMessage(e);\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, errorMessage);\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                updateSuccessStatus(drStatus.getOperationId(), sdxCluster, restoreV4Response.getFlowIdentifier(),\n+                        SdxDatabaseDrStatusTypeEnum.TRIGGERRED);\n+            }, () -> {\n+                updateFailureStatus(drStatus.getOperationId(), clusterId, String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+            });\n+        } catch (WebApplicationException e) {\n+            String errorMessage = webApplicationExceptionMessageExtractor.getErrorMessage(e);\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, errorMessage);\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void waitCloudbreakFlow(Long id, PollingConfig pollingConfig, String pollingMessage) {\n+        SdxCluster sdxCluster = sdxClusterRepository.findById(id).orElseThrow(notFound(\"SDX cluster\", id));\n+        Polling.waitPeriodly(pollingConfig.getSleepTime(), pollingConfig.getSleepTimeUnit())\n+                .stopIfException(pollingConfig.getStopPollingIfExceptionOccured())\n+                .stopAfterDelay(pollingConfig.getDuration(), pollingConfig.getDurationTimeUnit())\n+                .run(() -> checkDatabaseDrStatus(sdxCluster, pollingMessage));\n+    }\n+\n+    private AttemptResult<StackV4Response> checkDatabaseDrStatus(SdxCluster sdxCluster, String pollingMessage) throws JsonProcessingException {\n+        LOGGER.info(\"{} polling cloudbreak for stack status: '{}' in '{}' env\", pollingMessage, sdxCluster.getClusterName(), sdxCluster.getEnvName());\n+        try {\n+            if (PollGroup.CANCELLED.equals(DatalakeInMemoryStateStore.get(sdxCluster.getId()))) {\n+                LOGGER.info(\"{} polling cancelled in in-memory store, id: {}\", pollingMessage, sdxCluster.getId());\n+                return AttemptResults.breakFor(pollingMessage + \" polling cancelled in inmemory store, id: \" + sdxCluster.getId());\n+            } else {\n+                CloudbreakFlowService.FlowState flowState = cloudbreakFlowService.getLastKnownFlowState(sdxCluster);\n+                if (RUNNING.equals(flowState)) {\n+                    LOGGER.info(\"{} polling will continue, cluster has an active flow in Cloudbreak, id: {}\", pollingMessage, sdxCluster.getId());\n+                    return AttemptResults.justContinue();\n+                } else {\n+                    return getStackResponseAttemptResult(sdxCluster, pollingMessage, flowState);\n+                }\n+            }\n+        } catch (javax.ws.rs.NotFoundException e) {\n+            LOGGER.debug(\"Stack not found on CB side \" + sdxCluster.getClusterName(), e);\n+            return AttemptResults.breakFor(\"Stack not found on CB side \" + sdxCluster.getClusterName());\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> getStackResponseAttemptResult(SdxCluster sdxCluster, String pollingMessage, CloudbreakFlowService.FlowState flowState)\n+            throws JsonProcessingException {\n+        StackV4Response stackV4Response = stackV4Endpoint.get(0L, sdxCluster.getClusterName(), Collections.emptySet());\n+        LOGGER.info(\"Response from cloudbreak: {}\", JsonUtil.writeValueAsString(stackV4Response));\n+        ClusterV4Response cluster = stackV4Response.getCluster();\n+        if (isStackOrClusterDrStatusComplete(stackV4Response.getStatus()) && isStackOrClusterDrStatusComplete(cluster.getStatus())) {\n+            return sdxDrSucceeded(stackV4Response);\n+        } else if (isStackOrClusterStatusFailed(stackV4Response.getStatus())) {\n+            LOGGER.info(\"{} failed for Stack {} with status {}\", pollingMessage, stackV4Response.getName(), stackV4Response.getStatus());\n+            return sdxDrFailed(sdxCluster, stackV4Response.getStatusReason(), pollingMessage);\n+        } else if (isStackOrClusterStatusFailed(stackV4Response.getCluster().getStatus())) {\n+            LOGGER.info(\"{} failed for Cluster {} status {}\", pollingMessage, stackV4Response.getCluster().getName(),\n+                    stackV4Response.getCluster().getStatus());\n+            return sdxDrFailed(sdxCluster, stackV4Response.getCluster().getStatusReason(), pollingMessage);\n+        } else if (FINISHED.equals(flowState)) {\n+            LOGGER.info(\"Flow finished, but Backup/Restore is not complete: {}\", sdxCluster.getClusterName());\n+            return sdxDrFailed(sdxCluster, \"stack is in improper state\", pollingMessage);\n+        } else {\n+            LOGGER.info(\"Flow is unknown state\");\n+            return sdxDrFailed(sdxCluster, \"Flow is unknown state\", pollingMessage);\n+        }\n+    }\n+\n+    /**\n+     * Checks the status if the backup/restore operation failed.\n+     * @param status It could be stack/cluster status\n+     * @return true if status is failed, false otherwise.\n+     */\n+    private boolean isStackOrClusterStatusFailed(Status status) {\n+        return (Status.BACKUP_FAILED.equals(status) ||\n+                Status.RESTORE_FAILED.equals(status)) ? true : false;\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrFailed(SdxCluster sdxCluster, String statusReason, String pollingMessage) {\n+        LOGGER.info(\"{} failed, statusReason: {}\", pollingMessage, statusReason);\n+        return AttemptResults.breakFor(\"SDX \" + pollingMessage + \" failed '\" + sdxCluster.getClusterName() + \"', \" + statusReason);\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrSucceeded(StackV4Response stackV4Response) {\n+        LOGGER.info(\"Database DR operation does SDX cluster {} is successful\", stackV4Response.getCluster().getName());\n+        return AttemptResults.finishWith(stackV4Response);\n+    }\n+\n+    /**\n+     * Checks the status if the backup/restore operation is successful.\n+     * @param status It could be stack/cluster status\n+     * @return true if status is successful, false otherwise.\n+     */\n+    private boolean isStackOrClusterDrStatusComplete(Status status) {\n+        return (Status.BACKUP_FINISHED.equals(status) ||\n+                Status.RESTORE_FINISHED.equals(status)) ? true : false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 221}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE3MDA2Mg==", "bodyText": "return Status.BACKUP_FAILED.equals(status) ||\n                Status.RESTORE_FAILED.equals(status);", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446170062", "createdAt": "2020-06-26T13:02:57Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDatabaseDrService.java", "diffHunk": "@@ -0,0 +1,309 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_EVENT;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Service;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.auth.ThreadBasedUserCrnProvider;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.exception.WebApplicationExceptionMessageExtractor;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrOperationTypeEnum;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatusTypeEnum;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreStartEvent;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.flow.api.model.FlowIdentifier;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Service\n+public class SdxDatabaseDrService {\n+\n+    private static final int MAX_SIZE_OF_FAILURE_REASON = 254;\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDatabaseDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private WebApplicationExceptionMessageExtractor webApplicationExceptionMessageExtractor;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        return triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        return triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }\n+\n+    private SdxDatabaseBackupResponse triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        String selector = DATALAKE_DATABASE_BACKUP_EVENT.event();\n+        String userId = ThreadBasedUserCrnProvider.getUserCrn();\n+        DatalakeDatabaseBackupStartEvent startEvent = new DatalakeDatabaseBackupStartEvent(selector, clusterId, userId, backupId, backupLocation);\n+        FlowIdentifier flowIdentifier = sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(startEvent);\n+        return new SdxDatabaseBackupResponse(startEvent.getDrStatus().getOperationId(), flowIdentifier);\n+    }\n+\n+    private SdxDatabaseRestoreResponse triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        String selector = DATALAKE_DATABASE_RESTORE_EVENT.event();\n+        String userId = ThreadBasedUserCrnProvider.getUserCrn();\n+        DatalakeDatabaseRestoreStartEvent startEvent = new DatalakeDatabaseRestoreStartEvent(selector, clusterId, userId, backupId, backupLocation);\n+        FlowIdentifier flowIdentifier = sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(startEvent);\n+        return new SdxDatabaseRestoreResponse(startEvent.getDrStatus().getOperationId(), flowIdentifier);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                updateSuccessStatus(drStatus.getOperationId(), sdxCluster, backupV4Response.getFlowIdentifier(),\n+                        SdxDatabaseDrStatusTypeEnum.TRIGGERRED);\n+            }, () -> {\n+                updateFailureStatus(drStatus.getOperationId(), clusterId, String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+            });\n+        } catch (WebApplicationException e) {\n+            String errorMessage = webApplicationExceptionMessageExtractor.getErrorMessage(e);\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, errorMessage);\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                updateSuccessStatus(drStatus.getOperationId(), sdxCluster, restoreV4Response.getFlowIdentifier(),\n+                        SdxDatabaseDrStatusTypeEnum.TRIGGERRED);\n+            }, () -> {\n+                updateFailureStatus(drStatus.getOperationId(), clusterId, String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+            });\n+        } catch (WebApplicationException e) {\n+            String errorMessage = webApplicationExceptionMessageExtractor.getErrorMessage(e);\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, errorMessage);\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void waitCloudbreakFlow(Long id, PollingConfig pollingConfig, String pollingMessage) {\n+        SdxCluster sdxCluster = sdxClusterRepository.findById(id).orElseThrow(notFound(\"SDX cluster\", id));\n+        Polling.waitPeriodly(pollingConfig.getSleepTime(), pollingConfig.getSleepTimeUnit())\n+                .stopIfException(pollingConfig.getStopPollingIfExceptionOccured())\n+                .stopAfterDelay(pollingConfig.getDuration(), pollingConfig.getDurationTimeUnit())\n+                .run(() -> checkDatabaseDrStatus(sdxCluster, pollingMessage));\n+    }\n+\n+    private AttemptResult<StackV4Response> checkDatabaseDrStatus(SdxCluster sdxCluster, String pollingMessage) throws JsonProcessingException {\n+        LOGGER.info(\"{} polling cloudbreak for stack status: '{}' in '{}' env\", pollingMessage, sdxCluster.getClusterName(), sdxCluster.getEnvName());\n+        try {\n+            if (PollGroup.CANCELLED.equals(DatalakeInMemoryStateStore.get(sdxCluster.getId()))) {\n+                LOGGER.info(\"{} polling cancelled in in-memory store, id: {}\", pollingMessage, sdxCluster.getId());\n+                return AttemptResults.breakFor(pollingMessage + \" polling cancelled in inmemory store, id: \" + sdxCluster.getId());\n+            } else {\n+                CloudbreakFlowService.FlowState flowState = cloudbreakFlowService.getLastKnownFlowState(sdxCluster);\n+                if (RUNNING.equals(flowState)) {\n+                    LOGGER.info(\"{} polling will continue, cluster has an active flow in Cloudbreak, id: {}\", pollingMessage, sdxCluster.getId());\n+                    return AttemptResults.justContinue();\n+                } else {\n+                    return getStackResponseAttemptResult(sdxCluster, pollingMessage, flowState);\n+                }\n+            }\n+        } catch (javax.ws.rs.NotFoundException e) {\n+            LOGGER.debug(\"Stack not found on CB side \" + sdxCluster.getClusterName(), e);\n+            return AttemptResults.breakFor(\"Stack not found on CB side \" + sdxCluster.getClusterName());\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> getStackResponseAttemptResult(SdxCluster sdxCluster, String pollingMessage, CloudbreakFlowService.FlowState flowState)\n+            throws JsonProcessingException {\n+        StackV4Response stackV4Response = stackV4Endpoint.get(0L, sdxCluster.getClusterName(), Collections.emptySet());\n+        LOGGER.info(\"Response from cloudbreak: {}\", JsonUtil.writeValueAsString(stackV4Response));\n+        ClusterV4Response cluster = stackV4Response.getCluster();\n+        if (isStackOrClusterDrStatusComplete(stackV4Response.getStatus()) && isStackOrClusterDrStatusComplete(cluster.getStatus())) {\n+            return sdxDrSucceeded(stackV4Response);\n+        } else if (isStackOrClusterStatusFailed(stackV4Response.getStatus())) {\n+            LOGGER.info(\"{} failed for Stack {} with status {}\", pollingMessage, stackV4Response.getName(), stackV4Response.getStatus());\n+            return sdxDrFailed(sdxCluster, stackV4Response.getStatusReason(), pollingMessage);\n+        } else if (isStackOrClusterStatusFailed(stackV4Response.getCluster().getStatus())) {\n+            LOGGER.info(\"{} failed for Cluster {} status {}\", pollingMessage, stackV4Response.getCluster().getName(),\n+                    stackV4Response.getCluster().getStatus());\n+            return sdxDrFailed(sdxCluster, stackV4Response.getCluster().getStatusReason(), pollingMessage);\n+        } else if (FINISHED.equals(flowState)) {\n+            LOGGER.info(\"Flow finished, but Backup/Restore is not complete: {}\", sdxCluster.getClusterName());\n+            return sdxDrFailed(sdxCluster, \"stack is in improper state\", pollingMessage);\n+        } else {\n+            LOGGER.info(\"Flow is unknown state\");\n+            return sdxDrFailed(sdxCluster, \"Flow is unknown state\", pollingMessage);\n+        }\n+    }\n+\n+    /**\n+     * Checks the status if the backup/restore operation failed.\n+     * @param status It could be stack/cluster status\n+     * @return true if status is failed, false otherwise.\n+     */\n+    private boolean isStackOrClusterStatusFailed(Status status) {\n+        return (Status.BACKUP_FAILED.equals(status) ||\n+                Status.RESTORE_FAILED.equals(status)) ? true : false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 201}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE3MDM4Nw==", "bodyText": "drop the comment, method name is self explaining", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446170387", "createdAt": "2020-06-26T13:03:39Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDatabaseDrService.java", "diffHunk": "@@ -0,0 +1,309 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_EVENT;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Service;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.auth.ThreadBasedUserCrnProvider;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.exception.WebApplicationExceptionMessageExtractor;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrOperationTypeEnum;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatusTypeEnum;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreStartEvent;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.flow.api.model.FlowIdentifier;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Service\n+public class SdxDatabaseDrService {\n+\n+    private static final int MAX_SIZE_OF_FAILURE_REASON = 254;\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDatabaseDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private WebApplicationExceptionMessageExtractor webApplicationExceptionMessageExtractor;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        return triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        return triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }\n+\n+    private SdxDatabaseBackupResponse triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        String selector = DATALAKE_DATABASE_BACKUP_EVENT.event();\n+        String userId = ThreadBasedUserCrnProvider.getUserCrn();\n+        DatalakeDatabaseBackupStartEvent startEvent = new DatalakeDatabaseBackupStartEvent(selector, clusterId, userId, backupId, backupLocation);\n+        FlowIdentifier flowIdentifier = sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(startEvent);\n+        return new SdxDatabaseBackupResponse(startEvent.getDrStatus().getOperationId(), flowIdentifier);\n+    }\n+\n+    private SdxDatabaseRestoreResponse triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        String selector = DATALAKE_DATABASE_RESTORE_EVENT.event();\n+        String userId = ThreadBasedUserCrnProvider.getUserCrn();\n+        DatalakeDatabaseRestoreStartEvent startEvent = new DatalakeDatabaseRestoreStartEvent(selector, clusterId, userId, backupId, backupLocation);\n+        FlowIdentifier flowIdentifier = sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(startEvent);\n+        return new SdxDatabaseRestoreResponse(startEvent.getDrStatus().getOperationId(), flowIdentifier);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                updateSuccessStatus(drStatus.getOperationId(), sdxCluster, backupV4Response.getFlowIdentifier(),\n+                        SdxDatabaseDrStatusTypeEnum.TRIGGERRED);\n+            }, () -> {\n+                updateFailureStatus(drStatus.getOperationId(), clusterId, String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+            });\n+        } catch (WebApplicationException e) {\n+            String errorMessage = webApplicationExceptionMessageExtractor.getErrorMessage(e);\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, errorMessage);\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                updateSuccessStatus(drStatus.getOperationId(), sdxCluster, restoreV4Response.getFlowIdentifier(),\n+                        SdxDatabaseDrStatusTypeEnum.TRIGGERRED);\n+            }, () -> {\n+                updateFailureStatus(drStatus.getOperationId(), clusterId, String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+            });\n+        } catch (WebApplicationException e) {\n+            String errorMessage = webApplicationExceptionMessageExtractor.getErrorMessage(e);\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, errorMessage);\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void waitCloudbreakFlow(Long id, PollingConfig pollingConfig, String pollingMessage) {\n+        SdxCluster sdxCluster = sdxClusterRepository.findById(id).orElseThrow(notFound(\"SDX cluster\", id));\n+        Polling.waitPeriodly(pollingConfig.getSleepTime(), pollingConfig.getSleepTimeUnit())\n+                .stopIfException(pollingConfig.getStopPollingIfExceptionOccured())\n+                .stopAfterDelay(pollingConfig.getDuration(), pollingConfig.getDurationTimeUnit())\n+                .run(() -> checkDatabaseDrStatus(sdxCluster, pollingMessage));\n+    }\n+\n+    private AttemptResult<StackV4Response> checkDatabaseDrStatus(SdxCluster sdxCluster, String pollingMessage) throws JsonProcessingException {\n+        LOGGER.info(\"{} polling cloudbreak for stack status: '{}' in '{}' env\", pollingMessage, sdxCluster.getClusterName(), sdxCluster.getEnvName());\n+        try {\n+            if (PollGroup.CANCELLED.equals(DatalakeInMemoryStateStore.get(sdxCluster.getId()))) {\n+                LOGGER.info(\"{} polling cancelled in in-memory store, id: {}\", pollingMessage, sdxCluster.getId());\n+                return AttemptResults.breakFor(pollingMessage + \" polling cancelled in inmemory store, id: \" + sdxCluster.getId());\n+            } else {\n+                CloudbreakFlowService.FlowState flowState = cloudbreakFlowService.getLastKnownFlowState(sdxCluster);\n+                if (RUNNING.equals(flowState)) {\n+                    LOGGER.info(\"{} polling will continue, cluster has an active flow in Cloudbreak, id: {}\", pollingMessage, sdxCluster.getId());\n+                    return AttemptResults.justContinue();\n+                } else {\n+                    return getStackResponseAttemptResult(sdxCluster, pollingMessage, flowState);\n+                }\n+            }\n+        } catch (javax.ws.rs.NotFoundException e) {\n+            LOGGER.debug(\"Stack not found on CB side \" + sdxCluster.getClusterName(), e);\n+            return AttemptResults.breakFor(\"Stack not found on CB side \" + sdxCluster.getClusterName());\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> getStackResponseAttemptResult(SdxCluster sdxCluster, String pollingMessage, CloudbreakFlowService.FlowState flowState)\n+            throws JsonProcessingException {\n+        StackV4Response stackV4Response = stackV4Endpoint.get(0L, sdxCluster.getClusterName(), Collections.emptySet());\n+        LOGGER.info(\"Response from cloudbreak: {}\", JsonUtil.writeValueAsString(stackV4Response));\n+        ClusterV4Response cluster = stackV4Response.getCluster();\n+        if (isStackOrClusterDrStatusComplete(stackV4Response.getStatus()) && isStackOrClusterDrStatusComplete(cluster.getStatus())) {\n+            return sdxDrSucceeded(stackV4Response);\n+        } else if (isStackOrClusterStatusFailed(stackV4Response.getStatus())) {\n+            LOGGER.info(\"{} failed for Stack {} with status {}\", pollingMessage, stackV4Response.getName(), stackV4Response.getStatus());\n+            return sdxDrFailed(sdxCluster, stackV4Response.getStatusReason(), pollingMessage);\n+        } else if (isStackOrClusterStatusFailed(stackV4Response.getCluster().getStatus())) {\n+            LOGGER.info(\"{} failed for Cluster {} status {}\", pollingMessage, stackV4Response.getCluster().getName(),\n+                    stackV4Response.getCluster().getStatus());\n+            return sdxDrFailed(sdxCluster, stackV4Response.getCluster().getStatusReason(), pollingMessage);\n+        } else if (FINISHED.equals(flowState)) {\n+            LOGGER.info(\"Flow finished, but Backup/Restore is not complete: {}\", sdxCluster.getClusterName());\n+            return sdxDrFailed(sdxCluster, \"stack is in improper state\", pollingMessage);\n+        } else {\n+            LOGGER.info(\"Flow is unknown state\");\n+            return sdxDrFailed(sdxCluster, \"Flow is unknown state\", pollingMessage);\n+        }\n+    }\n+\n+    /**\n+     * Checks the status if the backup/restore operation failed.\n+     * @param status It could be stack/cluster status\n+     * @return true if status is failed, false otherwise.\n+     */\n+    private boolean isStackOrClusterStatusFailed(Status status) {\n+        return (Status.BACKUP_FAILED.equals(status) ||\n+                Status.RESTORE_FAILED.equals(status)) ? true : false;\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrFailed(SdxCluster sdxCluster, String statusReason, String pollingMessage) {\n+        LOGGER.info(\"{} failed, statusReason: {}\", pollingMessage, statusReason);\n+        return AttemptResults.breakFor(\"SDX \" + pollingMessage + \" failed '\" + sdxCluster.getClusterName() + \"', \" + statusReason);\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrSucceeded(StackV4Response stackV4Response) {\n+        LOGGER.info(\"Database DR operation does SDX cluster {} is successful\", stackV4Response.getCluster().getName());\n+        return AttemptResults.finishWith(stackV4Response);\n+    }\n+\n+    /**\n+     * Checks the status if the backup/restore operation is successful.\n+     * @param status It could be stack/cluster status\n+     * @return true if status is successful, false otherwise.\n+     */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 218}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE3MDQ0Ng==", "bodyText": "drop the comment", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446170446", "createdAt": "2020-06-26T13:03:47Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDatabaseDrService.java", "diffHunk": "@@ -0,0 +1,309 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_EVENT;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Service;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.auth.ThreadBasedUserCrnProvider;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.exception.WebApplicationExceptionMessageExtractor;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrOperationTypeEnum;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatusTypeEnum;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreStartEvent;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.flow.api.model.FlowIdentifier;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Service\n+public class SdxDatabaseDrService {\n+\n+    private static final int MAX_SIZE_OF_FAILURE_REASON = 254;\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDatabaseDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private WebApplicationExceptionMessageExtractor webApplicationExceptionMessageExtractor;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        return triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        return triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }\n+\n+    private SdxDatabaseBackupResponse triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        String selector = DATALAKE_DATABASE_BACKUP_EVENT.event();\n+        String userId = ThreadBasedUserCrnProvider.getUserCrn();\n+        DatalakeDatabaseBackupStartEvent startEvent = new DatalakeDatabaseBackupStartEvent(selector, clusterId, userId, backupId, backupLocation);\n+        FlowIdentifier flowIdentifier = sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(startEvent);\n+        return new SdxDatabaseBackupResponse(startEvent.getDrStatus().getOperationId(), flowIdentifier);\n+    }\n+\n+    private SdxDatabaseRestoreResponse triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        String selector = DATALAKE_DATABASE_RESTORE_EVENT.event();\n+        String userId = ThreadBasedUserCrnProvider.getUserCrn();\n+        DatalakeDatabaseRestoreStartEvent startEvent = new DatalakeDatabaseRestoreStartEvent(selector, clusterId, userId, backupId, backupLocation);\n+        FlowIdentifier flowIdentifier = sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(startEvent);\n+        return new SdxDatabaseRestoreResponse(startEvent.getDrStatus().getOperationId(), flowIdentifier);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                updateSuccessStatus(drStatus.getOperationId(), sdxCluster, backupV4Response.getFlowIdentifier(),\n+                        SdxDatabaseDrStatusTypeEnum.TRIGGERRED);\n+            }, () -> {\n+                updateFailureStatus(drStatus.getOperationId(), clusterId, String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+            });\n+        } catch (WebApplicationException e) {\n+            String errorMessage = webApplicationExceptionMessageExtractor.getErrorMessage(e);\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, errorMessage);\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                updateSuccessStatus(drStatus.getOperationId(), sdxCluster, restoreV4Response.getFlowIdentifier(),\n+                        SdxDatabaseDrStatusTypeEnum.TRIGGERRED);\n+            }, () -> {\n+                updateFailureStatus(drStatus.getOperationId(), clusterId, String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+            });\n+        } catch (WebApplicationException e) {\n+            String errorMessage = webApplicationExceptionMessageExtractor.getErrorMessage(e);\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, errorMessage);\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void waitCloudbreakFlow(Long id, PollingConfig pollingConfig, String pollingMessage) {\n+        SdxCluster sdxCluster = sdxClusterRepository.findById(id).orElseThrow(notFound(\"SDX cluster\", id));\n+        Polling.waitPeriodly(pollingConfig.getSleepTime(), pollingConfig.getSleepTimeUnit())\n+                .stopIfException(pollingConfig.getStopPollingIfExceptionOccured())\n+                .stopAfterDelay(pollingConfig.getDuration(), pollingConfig.getDurationTimeUnit())\n+                .run(() -> checkDatabaseDrStatus(sdxCluster, pollingMessage));\n+    }\n+\n+    private AttemptResult<StackV4Response> checkDatabaseDrStatus(SdxCluster sdxCluster, String pollingMessage) throws JsonProcessingException {\n+        LOGGER.info(\"{} polling cloudbreak for stack status: '{}' in '{}' env\", pollingMessage, sdxCluster.getClusterName(), sdxCluster.getEnvName());\n+        try {\n+            if (PollGroup.CANCELLED.equals(DatalakeInMemoryStateStore.get(sdxCluster.getId()))) {\n+                LOGGER.info(\"{} polling cancelled in in-memory store, id: {}\", pollingMessage, sdxCluster.getId());\n+                return AttemptResults.breakFor(pollingMessage + \" polling cancelled in inmemory store, id: \" + sdxCluster.getId());\n+            } else {\n+                CloudbreakFlowService.FlowState flowState = cloudbreakFlowService.getLastKnownFlowState(sdxCluster);\n+                if (RUNNING.equals(flowState)) {\n+                    LOGGER.info(\"{} polling will continue, cluster has an active flow in Cloudbreak, id: {}\", pollingMessage, sdxCluster.getId());\n+                    return AttemptResults.justContinue();\n+                } else {\n+                    return getStackResponseAttemptResult(sdxCluster, pollingMessage, flowState);\n+                }\n+            }\n+        } catch (javax.ws.rs.NotFoundException e) {\n+            LOGGER.debug(\"Stack not found on CB side \" + sdxCluster.getClusterName(), e);\n+            return AttemptResults.breakFor(\"Stack not found on CB side \" + sdxCluster.getClusterName());\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> getStackResponseAttemptResult(SdxCluster sdxCluster, String pollingMessage, CloudbreakFlowService.FlowState flowState)\n+            throws JsonProcessingException {\n+        StackV4Response stackV4Response = stackV4Endpoint.get(0L, sdxCluster.getClusterName(), Collections.emptySet());\n+        LOGGER.info(\"Response from cloudbreak: {}\", JsonUtil.writeValueAsString(stackV4Response));\n+        ClusterV4Response cluster = stackV4Response.getCluster();\n+        if (isStackOrClusterDrStatusComplete(stackV4Response.getStatus()) && isStackOrClusterDrStatusComplete(cluster.getStatus())) {\n+            return sdxDrSucceeded(stackV4Response);\n+        } else if (isStackOrClusterStatusFailed(stackV4Response.getStatus())) {\n+            LOGGER.info(\"{} failed for Stack {} with status {}\", pollingMessage, stackV4Response.getName(), stackV4Response.getStatus());\n+            return sdxDrFailed(sdxCluster, stackV4Response.getStatusReason(), pollingMessage);\n+        } else if (isStackOrClusterStatusFailed(stackV4Response.getCluster().getStatus())) {\n+            LOGGER.info(\"{} failed for Cluster {} status {}\", pollingMessage, stackV4Response.getCluster().getName(),\n+                    stackV4Response.getCluster().getStatus());\n+            return sdxDrFailed(sdxCluster, stackV4Response.getCluster().getStatusReason(), pollingMessage);\n+        } else if (FINISHED.equals(flowState)) {\n+            LOGGER.info(\"Flow finished, but Backup/Restore is not complete: {}\", sdxCluster.getClusterName());\n+            return sdxDrFailed(sdxCluster, \"stack is in improper state\", pollingMessage);\n+        } else {\n+            LOGGER.info(\"Flow is unknown state\");\n+            return sdxDrFailed(sdxCluster, \"Flow is unknown state\", pollingMessage);\n+        }\n+    }\n+\n+    /**\n+     * Checks the status if the backup/restore operation failed.\n+     * @param status It could be stack/cluster status\n+     * @return true if status is failed, false otherwise.\n+     */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 198}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a", "author": {"user": {"login": "kkalvagadda1", "name": "kalyan kumar kalvagadda"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/84c0a31f4e95900fb13de42d1db56c55e1c6bd7a", "committedDate": "2020-06-25T17:48:25Z", "message": "fixed failure observed in e2e testing."}, "afterCommit": {"oid": "4592edaca637c73aebd18e9c87b2e62a1143aa11", "author": {"user": {"login": "kkalvagadda1", "name": "kalyan kumar kalvagadda"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/4592edaca637c73aebd18e9c87b2e62a1143aa11", "committedDate": "2020-06-26T15:25:43Z", "message": "CB-7212: Add datalake service changes to perform database backup/restore."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM5MDQwODY0", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#pullrequestreview-439040864", "createdAt": "2020-06-29T11:22:47Z", "commit": {"oid": "4592edaca637c73aebd18e9c87b2e62a1143aa11"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOVQxMToyMjo0N1rOGqMT7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOVQxMTozNDo0OVrOGqMsKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njg5NTA4Ng==", "bodyText": "I think this class should be renamed also", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446895086", "createdAt": "2020-06-29T11:22:47Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/converter/SdxDatabaseDrStatusTypeEnumConverter.java", "diffHunk": "@@ -0,0 +1,12 @@\n+package com.sequenceiq.datalake.converter;\n+\n+import com.sequenceiq.cloudbreak.converter.DefaultEnumConverter;\n+import com.sequenceiq.datalake.entity.operation.SdxOperationStatusTypeEnum;\n+\n+public class SdxDatabaseDrStatusTypeEnumConverter extends DefaultEnumConverter<SdxOperationStatusTypeEnum> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4592edaca637c73aebd18e9c87b2e62a1143aa11"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njg5NjY3MA==", "bodyText": "I think SdxOperation would fit better, there is too much status around this class", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446896670", "createdAt": "2020-06-29T11:25:54Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/operation/SdxOperationStatus.java", "diffHunk": "@@ -0,0 +1,92 @@\n+package com.sequenceiq.datalake.entity.operation;\n+\n+import java.util.UUID;\n+\n+import javax.persistence.Convert;\n+import javax.persistence.Entity;\n+import javax.persistence.GeneratedValue;\n+import javax.persistence.GenerationType;\n+import javax.persistence.Id;\n+import javax.persistence.SequenceGenerator;\n+import javax.persistence.Table;\n+import javax.persistence.UniqueConstraint;\n+import javax.validation.constraints.NotNull;\n+\n+import com.sequenceiq.datalake.converter.SdxOperationTypeEnumConverter;\n+import com.sequenceiq.datalake.converter.SdxDatabaseDrStatusTypeEnumConverter;\n+\n+@Entity\n+@Table(uniqueConstraints = @UniqueConstraint(columnNames = {\"operationId\"}))\n+public class SdxOperationStatus {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4592edaca637c73aebd18e9c87b2e62a1143aa11"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njg5NzUzOQ==", "bodyText": "rename to SdxOperationStatus", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446897539", "createdAt": "2020-06-29T11:27:31Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/operation/SdxOperationStatusTypeEnum.java", "diffHunk": "@@ -0,0 +1,19 @@\n+package com.sequenceiq.datalake.entity.operation;\n+\n+public enum SdxOperationStatusTypeEnum {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4592edaca637c73aebd18e9c87b2e62a1143aa11"}, "originalPosition": 3}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njg5ODIzNA==", "bodyText": "drop the Enum from the end", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446898234", "createdAt": "2020-06-29T11:28:57Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/operation/SdxOperationTypeEnum.java", "diffHunk": "@@ -0,0 +1,17 @@\n+package com.sequenceiq.datalake.entity.operation;\n+\n+public enum SdxOperationTypeEnum {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4592edaca637c73aebd18e9c87b2e62a1143aa11"}, "originalPosition": 3}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjkwMTI5MQ==", "bodyText": "I don't know which pr will contain the final setting for this, but the 2 FAILED statuses should be here also.\ncc @hreeve-cloudera", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446901291", "createdAt": "2020-06-29T11:34:49Z", "author": {"login": "lacikaaa"}, "path": "core/src/main/java/com/sequenceiq/cloudbreak/job/StackStatusCheckerJob.java", "diffHunk": "@@ -156,7 +156,9 @@ protected void executeInternal(JobExecutionContext context) throws JobExecutionE\n                 Status.START_FAILED,\n                 Status.STOPPED,\n                 Status.STOP_FAILED,\n-                Status.AMBIGUOUS\n+                Status.AMBIGUOUS,\n+                Status.BACKUP_FINISHED,\n+                Status.RESTORE_FINISHED", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4592edaca637c73aebd18e9c87b2e62a1143aa11"}, "originalPosition": 7}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4592edaca637c73aebd18e9c87b2e62a1143aa11", "author": {"user": {"login": "kkalvagadda1", "name": "kalyan kumar kalvagadda"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/4592edaca637c73aebd18e9c87b2e62a1143aa11", "committedDate": "2020-06-26T15:25:43Z", "message": "CB-7212: Add datalake service changes to perform database backup/restore."}, "afterCommit": {"oid": "f36e483bae35f9e39f0c958d3b73e7002981153a", "author": {"user": {"login": "kkalvagadda1", "name": "kalyan kumar kalvagadda"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/f36e483bae35f9e39f0c958d3b73e7002981153a", "committedDate": "2020-06-29T13:32:05Z", "message": "CB-7212: Add datalake service changes to perform database backup/restore."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f36e483bae35f9e39f0c958d3b73e7002981153a", "author": {"user": {"login": "kkalvagadda1", "name": "kalyan kumar kalvagadda"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/f36e483bae35f9e39f0c958d3b73e7002981153a", "committedDate": "2020-06-29T13:32:05Z", "message": "CB-7212: Add datalake service changes to perform database backup/restore."}, "afterCommit": {"oid": "cb474bc68b698a1fe3a72cafece971a6000dfa05", "author": {"user": {"login": "kkalvagadda1", "name": "kalyan kumar kalvagadda"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/cb474bc68b698a1fe3a72cafece971a6000dfa05", "committedDate": "2020-06-29T14:31:49Z", "message": "CB-7212: Add datalake service changes to perform database backup/restore."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "cb474bc68b698a1fe3a72cafece971a6000dfa05", "author": {"user": {"login": "kkalvagadda1", "name": "kalyan kumar kalvagadda"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/cb474bc68b698a1fe3a72cafece971a6000dfa05", "committedDate": "2020-06-29T14:31:49Z", "message": "CB-7212: Add datalake service changes to perform database backup/restore."}, "afterCommit": {"oid": "b1a94bc7942abdb0d7e5ea324ba2eddcdba0c80c", "author": {"user": {"login": "kkalvagadda1", "name": "kalyan kumar kalvagadda"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/b1a94bc7942abdb0d7e5ea324ba2eddcdba0c80c", "committedDate": "2020-06-29T14:47:13Z", "message": "CB-7212: Add datalake service changes to perform database backup/restore."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "87ffbcb7be03687ef4eae15f750eb4889195a22f", "author": {"user": {"login": "kkalvagadda1", "name": "kalyan kumar kalvagadda"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/87ffbcb7be03687ef4eae15f750eb4889195a22f", "committedDate": "2020-06-29T15:25:28Z", "message": "CB-7212: Add datalake service changes to perform database backup/restore."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "b1a94bc7942abdb0d7e5ea324ba2eddcdba0c80c", "author": {"user": {"login": "kkalvagadda1", "name": "kalyan kumar kalvagadda"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/b1a94bc7942abdb0d7e5ea324ba2eddcdba0c80c", "committedDate": "2020-06-29T14:47:13Z", "message": "CB-7212: Add datalake service changes to perform database backup/restore."}, "afterCommit": {"oid": "87ffbcb7be03687ef4eae15f750eb4889195a22f", "author": {"user": {"login": "kkalvagadda1", "name": "kalyan kumar kalvagadda"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/87ffbcb7be03687ef4eae15f750eb4889195a22f", "committedDate": "2020-06-29T15:25:28Z", "message": "CB-7212: Add datalake service changes to perform database backup/restore."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM5MjU5NDUw", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#pullrequestreview-439259450", "createdAt": "2020-06-29T15:28:12Z", "commit": {"oid": "87ffbcb7be03687ef4eae15f750eb4889195a22f"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM5Mjc3MDMy", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#pullrequestreview-439277032", "createdAt": "2020-06-29T15:47:05Z", "commit": {"oid": "87ffbcb7be03687ef4eae15f750eb4889195a22f"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1783, "cost": 1, "resetAt": "2021-11-01T14:51:55Z"}}}