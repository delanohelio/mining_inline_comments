{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDI2NTkxMzY1", "number": 8194, "reviewThreads": {"totalCount": 61, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxMzo0NDozOFrOEBwE8g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOVQxMTozNDo0OVrOEJnaag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwMjcxNzMwOnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseDrState.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxMzo0NDozOFrOGdyTQg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxMzo0NDozOFrOGdyTQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzg4NjAxOA==", "bodyText": "nitpick: This should be DATABASE.\nThe same is true for all the following enums, and for the related @Bean(name = \"\") annotations in the action class.", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r433886018", "createdAt": "2020-06-02T13:44:38Z", "author": {"login": "brycederriso"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseDrState.java", "diffHunk": "@@ -0,0 +1,33 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import com.sequenceiq.flow.core.FlowState;\n+import com.sequenceiq.flow.core.RestartAction;\n+import com.sequenceiq.flow.core.restart.DefaultRestartAction;\n+\n+public enum DatalakeDatabaseDrState implements FlowState {\n+\n+    INIT_STATE,\n+    DATALAKE_DATABSE_BACKUP_START_STATE,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eeb0ac24348ecbbc4850b91fd35da885067a3cdd"}, "originalPosition": 10}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0MzM4MDczOnYy", "diffSide": "RIGHT", "path": "core-api/src/main/java/com/sequenceiq/cloudbreak/doc/OperationDescriptions.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQxNzoyNzowMlrOGj7xqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQxNzoyNzowMlrOGj7xqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDMzMjcxNQ==", "bodyText": "Capitalization and grammer: \"Performs a backup of the database to a provided location\"", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r440332715", "createdAt": "2020-06-15T17:27:02Z", "author": {"login": "hreeve-cloudera"}, "path": "core-api/src/main/java/com/sequenceiq/cloudbreak/doc/OperationDescriptions.java", "diffHunk": "@@ -44,6 +44,8 @@\n         public static final String CHECK_STACK_UPGRADE = \"Checks for upgrade options by name\";\n         public static final String STACK_UPGRADE = \"Upgrades a cluster to the latest CM or CDH version\";\n         public static final String LIST_RETRYABLE_FLOWS = \"List retryable failed flows\";\n+        public static final String DATABASE_BACKUP = \"performs a backup of database to a provided location\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "193405603e46ccc9be38527729240556706c3fe6"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0MzQ5MTc1OnYy", "diffSide": "RIGHT", "path": "core-api/src/main/java/com/sequenceiq/cloudbreak/api/endpoint/v4/common/Status.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQxODowMDowOFrOGj84MQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQyMDozMDoxMVrOGkBpDw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDM1MDc2OQ==", "bodyText": "This is a general comment for the review about how the status works on the cloudbreak side. It might or might not have any affect on what you do with status in your code. But on the cloudbreak side, there is no separate states of requested/started and in-progress. By the time the flow advances to the point where it's updating the stack status, the operation is already in progress. So in my version of these status messages, I had:\nBACKUP_IN_PROGRESS(StatusKind.PROGRESS), BACKUP_FINISHED(StatusKind.FINAL), BACKUP_FAILED(StatusKind.FINAL), RESTORE_IN_PROGRESS(StatusKind.PROGRESS), RESTORE_FINISHED(StatusKind.FINAL), RESTORE_FAILED(StatusKind.FINAL);\nI see you have multiple places where you have a separate start state, and an in-progress state. For the datalake service I think that makes sense, since you can mark it as started as soon as you get the request, and in-progress when you get the flow identifier back from cloudbreak. I just wanted to give you a heads up that cloudbreak won't be using the REQUESTED stattus.", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r440350769", "createdAt": "2020-06-15T18:00:08Z", "author": {"login": "hreeve-cloudera"}, "path": "core-api/src/main/java/com/sequenceiq/cloudbreak/api/endpoint/v4/common/Status.java", "diffHunk": "@@ -12,7 +12,13 @@\n     AVAILABLE(StatusKind.FINAL),\n     UPDATE_IN_PROGRESS(StatusKind.PROGRESS),\n     UPDATE_REQUESTED(StatusKind.PROGRESS),\n+    BACKUP_REQUESTED(StatusKind.PROGRESS),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "193405603e46ccc9be38527729240556706c3fe6"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQyNjY3Mw==", "bodyText": "I just followed the states defined for UPDATE. UPDATE has UPDATE_REQUESTED and UPDATE_IN_PROGRESS etc.\nI can update the patch accordingly.", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r440426673", "createdAt": "2020-06-15T20:25:56Z", "author": {"login": "kkalvagadda1"}, "path": "core-api/src/main/java/com/sequenceiq/cloudbreak/api/endpoint/v4/common/Status.java", "diffHunk": "@@ -12,7 +12,13 @@\n     AVAILABLE(StatusKind.FINAL),\n     UPDATE_IN_PROGRESS(StatusKind.PROGRESS),\n     UPDATE_REQUESTED(StatusKind.PROGRESS),\n+    BACKUP_REQUESTED(StatusKind.PROGRESS),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDM1MDc2OQ=="}, "originalCommit": {"oid": "193405603e46ccc9be38527729240556706c3fe6"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQyODgxNQ==", "bodyText": "I just followed the states defined for UPDATE. It has UPDATE_REQUESTED and UPDATE_IN_PROGRESS etc.\nIf your patch is not going to have the REQUESTED state, I will remove it.", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r440428815", "createdAt": "2020-06-15T20:30:11Z", "author": {"login": "kkalvagadda1"}, "path": "core-api/src/main/java/com/sequenceiq/cloudbreak/api/endpoint/v4/common/Status.java", "diffHunk": "@@ -12,7 +12,13 @@\n     AVAILABLE(StatusKind.FINAL),\n     UPDATE_IN_PROGRESS(StatusKind.PROGRESS),\n     UPDATE_REQUESTED(StatusKind.PROGRESS),\n+    BACKUP_REQUESTED(StatusKind.PROGRESS),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDM1MDc2OQ=="}, "originalCommit": {"oid": "193405603e46ccc9be38527729240556706c3fe6"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0Mzg0OTg5OnYy", "diffSide": "RIGHT", "path": "core-api/src/main/java/com/sequenceiq/cloudbreak/api/endpoint/v4/stacks/StackV4Endpoint.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQxOTo0OTozNVrOGkAaHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQyMDozMjoxM1rOGkBtQA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQwODYwNQ==", "bodyText": "To match REST conventions and StackV4Endpoint naming conventions:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                @POST\n          \n          \n            \n                @POST\n          \n          \n            \n                @Path(\"{name}/database_backup\")\n          \n          \n            \n                @Produces(MediaType.APPLICATION_JSON)\n          \n          \n            \n                @ApiOperation(value = DATABASE_BACKUP, nickname = \"databaseBackup\")\n          \n          \n            \n                BackupV4Response backupDatabaseByName(@PathParam(\"workspaceId\") Long workspaceId, @PathParam(\"name\") String name,\n          \n          \n            \n                        @QueryParam(\"location\") String location, @QueryParam(\"backupId\") String backupId);\n          \n          \n            \n            \n          \n          \n            \n                @POST\n          \n          \n            \n                @Path(\"{name}/database_restore\")\n          \n          \n            \n                @Produces(MediaType.APPLICATION_JSON)\n          \n          \n            \n                @ApiOperation(value = DATABASE_RESTORE, nickname = \"databaseRestore\")\n          \n          \n            \n                RestoreV4Response restoreDatabaseByName(@PathParam(\"workspaceId\") Long workspaceId, @PathParam(\"name\") String name,\n          \n          \n            \n                        @QueryParam(\"location\") String location, @QueryParam(\"backupId\") String backupId);", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r440408605", "createdAt": "2020-06-15T19:49:35Z", "author": {"login": "hreeve-cloudera"}, "path": "core-api/src/main/java/com/sequenceiq/cloudbreak/api/endpoint/v4/stacks/StackV4Endpoint.java", "diffHunk": "@@ -269,4 +273,17 @@ FlowIdentifier setClusterMaintenanceMode(@PathParam(\"workspaceId\") Long workspac\n     @ApiOperation(value = UPDATE_SALT, nickname = \"updateSaltByName\")\n     FlowIdentifier updateSaltByName(@PathParam(\"workspaceId\") Long workspaceId, @PathParam(\"name\") String name);\n \n+    @POST", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "193405603e46ccc9be38527729240556706c3fe6"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQyOTg4OA==", "bodyText": "Basically the suggestion is to accepted location and backup-id as query parameters, right.", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r440429888", "createdAt": "2020-06-15T20:32:13Z", "author": {"login": "kkalvagadda1"}, "path": "core-api/src/main/java/com/sequenceiq/cloudbreak/api/endpoint/v4/stacks/StackV4Endpoint.java", "diffHunk": "@@ -269,4 +273,17 @@ FlowIdentifier setClusterMaintenanceMode(@PathParam(\"workspaceId\") Long workspac\n     @ApiOperation(value = UPDATE_SALT, nickname = \"updateSaltByName\")\n     FlowIdentifier updateSaltByName(@PathParam(\"workspaceId\") Long workspaceId, @PathParam(\"name\") String name);\n \n+    @POST", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQwODYwNQ=="}, "originalCommit": {"oid": "193405603e46ccc9be38527729240556706c3fe6"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2Njk4MjQ3OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/controller/sdx/SdxController.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwOTowMjoxM1rOGngF9w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxMzozOTo0OVrOGnpvBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA3MzQ2Mw==", "bodyText": "this should be getBackupDatabaseStatusByName instead", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444073463", "createdAt": "2020-06-23T09:02:13Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/controller/sdx/SdxController.java", "diffHunk": "@@ -246,24 +250,31 @@ public FlowIdentifier stopByCrn(@ResourceCrn String crn) {\n     @Override\n     @CheckPermissionByAccount(action = AuthorizationResourceAction.BACKUP_DATALAKE)\n     public SdxDatabaseBackupResponse backupDatabaseByName(@ResourceName String name, String backupId, String backupLocation) {\n-        return null;\n+        String userCrn = ThreadBasedUserCrnProvider.getUserCrn();\n+        SdxCluster sdxCluster = sdxService.getSdxByNameInAccount(userCrn, name);\n+        return sdxDrService.triggerDatabaseBackup(sdxCluster, backupId, backupLocation);\n     }\n \n     @Override\n     @CheckPermissionByAccount(action = AuthorizationResourceAction.RESTORE_DATALAKE)\n     public SdxDatabaseRestoreResponse restoreDatabaseByName(@ResourceName String name, String backupId, String backupLocation) {\n-        return null;\n+        String userCrn = ThreadBasedUserCrnProvider.getUserCrn();\n+        SdxCluster sdxCluster = sdxService.getSdxByNameInAccount(userCrn, name);\n+        return sdxDrService.triggerDatabaseRestore(sdxCluster, backupId, backupLocation);\n     }\n \n     @Override\n     @CheckPermissionByAccount(action = AuthorizationResourceAction.BACKUP_DATALAKE)\n     public SdxDatabaseBackupStatusResponse backupDatabaseStatusByName(@ResourceName String name, String operationId) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDIzMTQyOQ==", "bodyText": "will fix.", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444231429", "createdAt": "2020-06-23T13:39:49Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/controller/sdx/SdxController.java", "diffHunk": "@@ -246,24 +250,31 @@ public FlowIdentifier stopByCrn(@ResourceCrn String crn) {\n     @Override\n     @CheckPermissionByAccount(action = AuthorizationResourceAction.BACKUP_DATALAKE)\n     public SdxDatabaseBackupResponse backupDatabaseByName(@ResourceName String name, String backupId, String backupLocation) {\n-        return null;\n+        String userCrn = ThreadBasedUserCrnProvider.getUserCrn();\n+        SdxCluster sdxCluster = sdxService.getSdxByNameInAccount(userCrn, name);\n+        return sdxDrService.triggerDatabaseBackup(sdxCluster, backupId, backupLocation);\n     }\n \n     @Override\n     @CheckPermissionByAccount(action = AuthorizationResourceAction.RESTORE_DATALAKE)\n     public SdxDatabaseRestoreResponse restoreDatabaseByName(@ResourceName String name, String backupId, String backupLocation) {\n-        return null;\n+        String userCrn = ThreadBasedUserCrnProvider.getUserCrn();\n+        SdxCluster sdxCluster = sdxService.getSdxByNameInAccount(userCrn, name);\n+        return sdxDrService.triggerDatabaseRestore(sdxCluster, backupId, backupLocation);\n     }\n \n     @Override\n     @CheckPermissionByAccount(action = AuthorizationResourceAction.BACKUP_DATALAKE)\n     public SdxDatabaseBackupStatusResponse backupDatabaseStatusByName(@ResourceName String name, String operationId) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA3MzQ2Mw=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2Njk4NDUxOnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/controller/sdx/SdxController.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwOTowMjo0MlrOGngHNA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxMzozOTo1M1rOGnpvOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA3Mzc4MA==", "bodyText": "this should be getRestoreDatabaseStatusByName instead", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444073780", "createdAt": "2020-06-23T09:02:42Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/controller/sdx/SdxController.java", "diffHunk": "@@ -246,24 +250,31 @@ public FlowIdentifier stopByCrn(@ResourceCrn String crn) {\n     @Override\n     @CheckPermissionByAccount(action = AuthorizationResourceAction.BACKUP_DATALAKE)\n     public SdxDatabaseBackupResponse backupDatabaseByName(@ResourceName String name, String backupId, String backupLocation) {\n-        return null;\n+        String userCrn = ThreadBasedUserCrnProvider.getUserCrn();\n+        SdxCluster sdxCluster = sdxService.getSdxByNameInAccount(userCrn, name);\n+        return sdxDrService.triggerDatabaseBackup(sdxCluster, backupId, backupLocation);\n     }\n \n     @Override\n     @CheckPermissionByAccount(action = AuthorizationResourceAction.RESTORE_DATALAKE)\n     public SdxDatabaseRestoreResponse restoreDatabaseByName(@ResourceName String name, String backupId, String backupLocation) {\n-        return null;\n+        String userCrn = ThreadBasedUserCrnProvider.getUserCrn();\n+        SdxCluster sdxCluster = sdxService.getSdxByNameInAccount(userCrn, name);\n+        return sdxDrService.triggerDatabaseRestore(sdxCluster, backupId, backupLocation);\n     }\n \n     @Override\n     @CheckPermissionByAccount(action = AuthorizationResourceAction.BACKUP_DATALAKE)\n     public SdxDatabaseBackupStatusResponse backupDatabaseStatusByName(@ResourceName String name, String operationId) {\n-        return null;\n+        String userCrn = ThreadBasedUserCrnProvider.getUserCrn();\n+        SdxCluster sdxCluster = sdxService.getSdxByNameInAccount(userCrn, name);\n+        return sdxDrService.getDatabaseBackupStatus(sdxCluster, operationId);\n     }\n \n     @Override\n     @CheckPermissionByAccount(action = AuthorizationResourceAction.RESTORE_DATALAKE)\n     public SdxDatabaseRestoreStatusResponse restoreDatabaseStatusByName(@ResourceName String name, String operationId) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDIzMTQ4MQ==", "bodyText": "will fix.", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444231481", "createdAt": "2020-06-23T13:39:53Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/controller/sdx/SdxController.java", "diffHunk": "@@ -246,24 +250,31 @@ public FlowIdentifier stopByCrn(@ResourceCrn String crn) {\n     @Override\n     @CheckPermissionByAccount(action = AuthorizationResourceAction.BACKUP_DATALAKE)\n     public SdxDatabaseBackupResponse backupDatabaseByName(@ResourceName String name, String backupId, String backupLocation) {\n-        return null;\n+        String userCrn = ThreadBasedUserCrnProvider.getUserCrn();\n+        SdxCluster sdxCluster = sdxService.getSdxByNameInAccount(userCrn, name);\n+        return sdxDrService.triggerDatabaseBackup(sdxCluster, backupId, backupLocation);\n     }\n \n     @Override\n     @CheckPermissionByAccount(action = AuthorizationResourceAction.RESTORE_DATALAKE)\n     public SdxDatabaseRestoreResponse restoreDatabaseByName(@ResourceName String name, String backupId, String backupLocation) {\n-        return null;\n+        String userCrn = ThreadBasedUserCrnProvider.getUserCrn();\n+        SdxCluster sdxCluster = sdxService.getSdxByNameInAccount(userCrn, name);\n+        return sdxDrService.triggerDatabaseRestore(sdxCluster, backupId, backupLocation);\n     }\n \n     @Override\n     @CheckPermissionByAccount(action = AuthorizationResourceAction.BACKUP_DATALAKE)\n     public SdxDatabaseBackupStatusResponse backupDatabaseStatusByName(@ResourceName String name, String operationId) {\n-        return null;\n+        String userCrn = ThreadBasedUserCrnProvider.getUserCrn();\n+        SdxCluster sdxCluster = sdxService.getSdxByNameInAccount(userCrn, name);\n+        return sdxDrService.getDatabaseBackupStatus(sdxCluster, operationId);\n     }\n \n     @Override\n     @CheckPermissionByAccount(action = AuthorizationResourceAction.RESTORE_DATALAKE)\n     public SdxDatabaseRestoreStatusResponse restoreDatabaseStatusByName(@ResourceName String name, String operationId) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA3Mzc4MA=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NzAwMjU4OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/SdxReactorFlowManager.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwOTowNzo0MVrOGngSsg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxNToyNTo0NlrOGoXXfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA3NjcyMg==", "bodyText": "FlowIdentifier should be returned here, the operationId, should be passed as parameter so that notify() can be used instead of notifyDatabaseDrEvent()", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444076722", "createdAt": "2020-06-23T09:07:41Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/SdxReactorFlowManager.java", "diffHunk": "@@ -104,6 +109,18 @@ public FlowIdentifier triggerSdxStopFlow(SdxCluster cluster) {\n         return notify(selector, new SdxStartStopEvent(selector, cluster.getId(), userId));\n     }\n \n+    public String triggerDatalakeDatabaseBackupFlow(Long sdxId, String backupId, String backupLocation) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDIzMjk0MA==", "bodyText": "Why should FlowIdentifier return when it is not used? API invoking triggerDatalakeDatabaseBackupFlow uses operationId instead of flowIdentifier.\nI can make changes to avoid the need for notifyDatabaseDrEvent. Does that address your concern?", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444232940", "createdAt": "2020-06-23T13:41:55Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/SdxReactorFlowManager.java", "diffHunk": "@@ -104,6 +109,18 @@ public FlowIdentifier triggerSdxStopFlow(SdxCluster cluster) {\n         return notify(selector, new SdxStartStopEvent(selector, cluster.getId(), userId));\n     }\n \n+    public String triggerDatalakeDatabaseBackupFlow(Long sdxId, String backupId, String backupLocation) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA3NjcyMg=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ4NDEwNA==", "bodyText": "I have removed method \"notifyDatabaseDrEvent\" and re-used \"notify\".", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444484104", "createdAt": "2020-06-23T20:21:43Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/SdxReactorFlowManager.java", "diffHunk": "@@ -104,6 +109,18 @@ public FlowIdentifier triggerSdxStopFlow(SdxCluster cluster) {\n         return notify(selector, new SdxStartStopEvent(selector, cluster.getId(), userId));\n     }\n \n+    public String triggerDatalakeDatabaseBackupFlow(Long sdxId, String backupId, String backupLocation) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA3NjcyMg=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDk3OTA2OQ==", "bodyText": "will update.", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444979069", "createdAt": "2020-06-24T15:25:46Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/SdxReactorFlowManager.java", "diffHunk": "@@ -104,6 +109,18 @@ public FlowIdentifier triggerSdxStopFlow(SdxCluster cluster) {\n         return notify(selector, new SdxStartStopEvent(selector, cluster.getId(), userId));\n     }\n \n+    public String triggerDatalakeDatabaseBackupFlow(Long sdxId, String backupId, String backupLocation) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA3NjcyMg=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NzAwMzA4OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/SdxReactorFlowManager.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwOTowNzo1M1rOGngTDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxNToyNTo0MFrOGoXXKQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA3NjgxMg==", "bodyText": "FlowIdentifier should be returned here, the operationId, should be passed as parameter so that notify() can be used instead of notifyDatabaseDrEvent()", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444076812", "createdAt": "2020-06-23T09:07:53Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/SdxReactorFlowManager.java", "diffHunk": "@@ -104,6 +109,18 @@ public FlowIdentifier triggerSdxStopFlow(SdxCluster cluster) {\n         return notify(selector, new SdxStartStopEvent(selector, cluster.getId(), userId));\n     }\n \n+    public String triggerDatalakeDatabaseBackupFlow(Long sdxId, String backupId, String backupLocation) {\n+        String selector = DATALAKE_DATABASE_BACKUP_EVENT.event();\n+        String userId = ThreadBasedUserCrnProvider.getUserCrn();\n+        return notifyDatabaseDrEvent(selector, new DatalakeDatabaseBackupStartEvent(selector, sdxId, userId, backupId, backupLocation));\n+    }\n+\n+    public String triggerDatalakeDatabaseRestoreFlow(Long sdxId, String backupId, String backupLocation) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDk3ODk4NQ==", "bodyText": "will update.", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444978985", "createdAt": "2020-06-24T15:25:40Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/SdxReactorFlowManager.java", "diffHunk": "@@ -104,6 +109,18 @@ public FlowIdentifier triggerSdxStopFlow(SdxCluster cluster) {\n         return notify(selector, new SdxStartStopEvent(selector, cluster.getId(), userId));\n     }\n \n+    public String triggerDatalakeDatabaseBackupFlow(Long sdxId, String backupId, String backupLocation) {\n+        String selector = DATALAKE_DATABASE_BACKUP_EVENT.event();\n+        String userId = ThreadBasedUserCrnProvider.getUserCrn();\n+        return notifyDatabaseDrEvent(selector, new DatalakeDatabaseBackupStartEvent(selector, sdxId, userId, backupId, backupLocation));\n+    }\n+\n+    public String triggerDatalakeDatabaseRestoreFlow(Long sdxId, String backupId, String backupLocation) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA3NjgxMg=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NzAwNDQxOnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/SdxReactorFlowManager.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwOTowODoxOVrOGngT8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QyMDoyMjoxMlrOGn5K_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA3NzA0Mw==", "bodyText": "this is not needed if operationId is passed as parameter", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444077043", "createdAt": "2020-06-23T09:08:19Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/SdxReactorFlowManager.java", "diffHunk": "@@ -132,4 +149,30 @@ private FlowIdentifier notify(String selector, SdxEvent acceptable) {\n         }\n \n     }\n+\n+    String notifyDatabaseDrEvent(String selector,  DatalakeDatabaseDrStartBaseEvent acceptable) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ4NDM1MQ==", "bodyText": "will remove the method.", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444484351", "createdAt": "2020-06-23T20:22:12Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/SdxReactorFlowManager.java", "diffHunk": "@@ -132,4 +149,30 @@ private FlowIdentifier notify(String selector, SdxEvent acceptable) {\n         }\n \n     }\n+\n+    String notifyDatabaseDrEvent(String selector,  DatalakeDatabaseDrStartBaseEvent acceptable) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA3NzA0Mw=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NzA1Mjg4OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseBackupActions.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwOToyMTo0NlrOGngzmw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxMzo0NzoyOVrOGnqFEA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA4NTE0Nw==", "bodyText": "shouldn't we set the status (sdxDrService.updateDatabaseStatusEntry() to INPROGRESS here?", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444085147", "createdAt": "2020-06-23T09:21:46Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseBackupActions.java", "diffHunk": "@@ -0,0 +1,173 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT;\n+\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.statemachine.StateContext;\n+import org.springframework.statemachine.action.Action;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxContext;\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupCouldNotStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupFailedEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupSuccessEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupWaitRequest;\n+import com.sequenceiq.datalake.service.AbstractSdxAction;\n+import com.sequenceiq.datalake.service.sdx.dr.SdxDrService;\n+import com.sequenceiq.flow.core.FlowEvent;\n+import com.sequenceiq.flow.core.FlowParameters;\n+import com.sequenceiq.flow.core.FlowState;\n+\n+@Configuration\n+public class DatalakeDatabaseBackupActions {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(DatalakeDatabaseBackupActions.class);\n+\n+    private static final String OPERATION_ID = \"OPERATION-ID\";\n+\n+    @Inject\n+    private SdxDrService sdxDrService;\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_START_STATE\")\n+    public Action<?, ?> datalakeBackup() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void prepareExecution(DatalakeDatabaseBackupStartEvent payload, Map<Object, Object> variables) {\n+                super.prepareExecution(payload, variables);\n+                variables.put(OPERATION_ID, payload.getDrStatus().getOperationId());\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Datalake database backup has been started for {}\", payload.getResourceId());\n+                sdxDrService.databaseBackup(payload.getDrStatus(),\n+                        payload.getResourceId(),\n+                        payload.getBackupId(),\n+                        payload.getBackupLocation());\n+                sendEvent(context, DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseBackupStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupCouldNotStartEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_IN_PROGRESS_STATE\")\n+    public Action<?, ?> datalakebackupInProgress() {\n+        return new AbstractSdxAction<>(SdxEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext, SdxEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, SdxEvent payload, Map<Object, Object> variables) {\n+                LOGGER.info(\"Datalake database backup is in progress for {} \", payload.getResourceId());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDIzNzA3Mg==", "bodyText": "yes, you are right. I will fix it.", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444237072", "createdAt": "2020-06-23T13:47:29Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseBackupActions.java", "diffHunk": "@@ -0,0 +1,173 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT;\n+\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.statemachine.StateContext;\n+import org.springframework.statemachine.action.Action;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxContext;\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupCouldNotStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupFailedEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupSuccessEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupWaitRequest;\n+import com.sequenceiq.datalake.service.AbstractSdxAction;\n+import com.sequenceiq.datalake.service.sdx.dr.SdxDrService;\n+import com.sequenceiq.flow.core.FlowEvent;\n+import com.sequenceiq.flow.core.FlowParameters;\n+import com.sequenceiq.flow.core.FlowState;\n+\n+@Configuration\n+public class DatalakeDatabaseBackupActions {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(DatalakeDatabaseBackupActions.class);\n+\n+    private static final String OPERATION_ID = \"OPERATION-ID\";\n+\n+    @Inject\n+    private SdxDrService sdxDrService;\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_START_STATE\")\n+    public Action<?, ?> datalakeBackup() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void prepareExecution(DatalakeDatabaseBackupStartEvent payload, Map<Object, Object> variables) {\n+                super.prepareExecution(payload, variables);\n+                variables.put(OPERATION_ID, payload.getDrStatus().getOperationId());\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Datalake database backup has been started for {}\", payload.getResourceId());\n+                sdxDrService.databaseBackup(payload.getDrStatus(),\n+                        payload.getResourceId(),\n+                        payload.getBackupId(),\n+                        payload.getBackupLocation());\n+                sendEvent(context, DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseBackupStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupCouldNotStartEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_IN_PROGRESS_STATE\")\n+    public Action<?, ?> datalakebackupInProgress() {\n+        return new AbstractSdxAction<>(SdxEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext, SdxEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, SdxEvent payload, Map<Object, Object> variables) {\n+                LOGGER.info(\"Datalake database backup is in progress for {} \", payload.getResourceId());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA4NTE0Nw=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 86}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NzA4MDM3OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseBackupActions.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwOToyOTowMFrOGnhFHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QyMDozMTowOVrOGn5cZQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA4OTYzMA==", "bodyText": "this doExecute() is the same as the backupCouldNotStart() one, could you pls. extract it to a common method?", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444089630", "createdAt": "2020-06-23T09:29:00Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseBackupActions.java", "diffHunk": "@@ -0,0 +1,173 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT;\n+\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.statemachine.StateContext;\n+import org.springframework.statemachine.action.Action;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxContext;\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupCouldNotStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupFailedEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupSuccessEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupWaitRequest;\n+import com.sequenceiq.datalake.service.AbstractSdxAction;\n+import com.sequenceiq.datalake.service.sdx.dr.SdxDrService;\n+import com.sequenceiq.flow.core.FlowEvent;\n+import com.sequenceiq.flow.core.FlowParameters;\n+import com.sequenceiq.flow.core.FlowState;\n+\n+@Configuration\n+public class DatalakeDatabaseBackupActions {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(DatalakeDatabaseBackupActions.class);\n+\n+    private static final String OPERATION_ID = \"OPERATION-ID\";\n+\n+    @Inject\n+    private SdxDrService sdxDrService;\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_START_STATE\")\n+    public Action<?, ?> datalakeBackup() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void prepareExecution(DatalakeDatabaseBackupStartEvent payload, Map<Object, Object> variables) {\n+                super.prepareExecution(payload, variables);\n+                variables.put(OPERATION_ID, payload.getDrStatus().getOperationId());\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Datalake database backup has been started for {}\", payload.getResourceId());\n+                sdxDrService.databaseBackup(payload.getDrStatus(),\n+                        payload.getResourceId(),\n+                        payload.getBackupId(),\n+                        payload.getBackupLocation());\n+                sendEvent(context, DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseBackupStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupCouldNotStartEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_IN_PROGRESS_STATE\")\n+    public Action<?, ?> datalakebackupInProgress() {\n+        return new AbstractSdxAction<>(SdxEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext, SdxEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, SdxEvent payload, Map<Object, Object> variables) {\n+                LOGGER.info(\"Datalake database backup is in progress for {} \", payload.getResourceId());\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sendEvent(context, DatalakeDatabaseBackupWaitRequest.from(context, operationId));\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(SdxEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupFailedEvent.from(payload, ex);\n+\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_COULD_NOT_START_STATE\")\n+    public Action<?, ?> backupCouldNotStart() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupCouldNotStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupCouldNotStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupCouldNotStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                Exception exception = payload.getException();\n+                LOGGER.error(\"Datalake database backup could not be started for datalake with id: {}\", payload.getResourceId(), exception);\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sdxDrService.updateDatabaseStatusEntry(operationId, SdxDatabaseDrStatus.Status.FAILED, payload.getException().getMessage());\n+                sendEvent(context, DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseBackupCouldNotStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_FINISHED_STATE\")\n+    public Action<?, ?> finishedBackupAction() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupSuccessEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupSuccessEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupSuccessEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Sdx database backup is finalized with sdx id: {}\", payload.getResourceId());\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sdxDrService.updateDatabaseStatusEntry(operationId, SdxDatabaseDrStatus.Status.SUCCEEDED, null);\n+                sendEvent(context, DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseBackupSuccessEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_FAILED_STATE\")\n+    public Action<?, ?> backupFailed() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupFailedEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupFailedEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupFailedEvent payload, Map<Object, Object> variables) throws Exception {\n+                Exception exception = payload.getException();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 160}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ4ODgwNQ==", "bodyText": "I understand that the code is duplicate. In fact, I tried to do it before but there is an issue.\nThis code uses \"sentEvent\" which is an implementation in \"AbstractAction\". It is available in a private API in DatalakeDatabaseBackupActions.", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444488805", "createdAt": "2020-06-23T20:31:09Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseBackupActions.java", "diffHunk": "@@ -0,0 +1,173 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT;\n+\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.statemachine.StateContext;\n+import org.springframework.statemachine.action.Action;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxContext;\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupCouldNotStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupFailedEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupSuccessEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupWaitRequest;\n+import com.sequenceiq.datalake.service.AbstractSdxAction;\n+import com.sequenceiq.datalake.service.sdx.dr.SdxDrService;\n+import com.sequenceiq.flow.core.FlowEvent;\n+import com.sequenceiq.flow.core.FlowParameters;\n+import com.sequenceiq.flow.core.FlowState;\n+\n+@Configuration\n+public class DatalakeDatabaseBackupActions {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(DatalakeDatabaseBackupActions.class);\n+\n+    private static final String OPERATION_ID = \"OPERATION-ID\";\n+\n+    @Inject\n+    private SdxDrService sdxDrService;\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_START_STATE\")\n+    public Action<?, ?> datalakeBackup() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void prepareExecution(DatalakeDatabaseBackupStartEvent payload, Map<Object, Object> variables) {\n+                super.prepareExecution(payload, variables);\n+                variables.put(OPERATION_ID, payload.getDrStatus().getOperationId());\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Datalake database backup has been started for {}\", payload.getResourceId());\n+                sdxDrService.databaseBackup(payload.getDrStatus(),\n+                        payload.getResourceId(),\n+                        payload.getBackupId(),\n+                        payload.getBackupLocation());\n+                sendEvent(context, DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseBackupStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupCouldNotStartEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_IN_PROGRESS_STATE\")\n+    public Action<?, ?> datalakebackupInProgress() {\n+        return new AbstractSdxAction<>(SdxEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext, SdxEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, SdxEvent payload, Map<Object, Object> variables) {\n+                LOGGER.info(\"Datalake database backup is in progress for {} \", payload.getResourceId());\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sendEvent(context, DatalakeDatabaseBackupWaitRequest.from(context, operationId));\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(SdxEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupFailedEvent.from(payload, ex);\n+\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_COULD_NOT_START_STATE\")\n+    public Action<?, ?> backupCouldNotStart() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupCouldNotStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupCouldNotStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupCouldNotStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                Exception exception = payload.getException();\n+                LOGGER.error(\"Datalake database backup could not be started for datalake with id: {}\", payload.getResourceId(), exception);\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sdxDrService.updateDatabaseStatusEntry(operationId, SdxDatabaseDrStatus.Status.FAILED, payload.getException().getMessage());\n+                sendEvent(context, DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseBackupCouldNotStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_FINISHED_STATE\")\n+    public Action<?, ?> finishedBackupAction() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupSuccessEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupSuccessEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupSuccessEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Sdx database backup is finalized with sdx id: {}\", payload.getResourceId());\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sdxDrService.updateDatabaseStatusEntry(operationId, SdxDatabaseDrStatus.Status.SUCCEEDED, null);\n+                sendEvent(context, DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseBackupSuccessEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_FAILED_STATE\")\n+    public Action<?, ?> backupFailed() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupFailedEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupFailedEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupFailedEvent payload, Map<Object, Object> variables) throws Exception {\n+                Exception exception = payload.getException();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA4OTYzMA=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 160}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NzA4MTc5OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseBackupActions.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwOToyOToyNFrOGnhF7A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxMzo0MDoxN1rOGoSmTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA4OTgzNg==", "bodyText": "Not DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT instead?", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444089836", "createdAt": "2020-06-23T09:29:24Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseBackupActions.java", "diffHunk": "@@ -0,0 +1,173 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT;\n+\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.statemachine.StateContext;\n+import org.springframework.statemachine.action.Action;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxContext;\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupCouldNotStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupFailedEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupSuccessEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupWaitRequest;\n+import com.sequenceiq.datalake.service.AbstractSdxAction;\n+import com.sequenceiq.datalake.service.sdx.dr.SdxDrService;\n+import com.sequenceiq.flow.core.FlowEvent;\n+import com.sequenceiq.flow.core.FlowParameters;\n+import com.sequenceiq.flow.core.FlowState;\n+\n+@Configuration\n+public class DatalakeDatabaseBackupActions {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(DatalakeDatabaseBackupActions.class);\n+\n+    private static final String OPERATION_ID = \"OPERATION-ID\";\n+\n+    @Inject\n+    private SdxDrService sdxDrService;\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_START_STATE\")\n+    public Action<?, ?> datalakeBackup() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void prepareExecution(DatalakeDatabaseBackupStartEvent payload, Map<Object, Object> variables) {\n+                super.prepareExecution(payload, variables);\n+                variables.put(OPERATION_ID, payload.getDrStatus().getOperationId());\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Datalake database backup has been started for {}\", payload.getResourceId());\n+                sdxDrService.databaseBackup(payload.getDrStatus(),\n+                        payload.getResourceId(),\n+                        payload.getBackupId(),\n+                        payload.getBackupLocation());\n+                sendEvent(context, DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseBackupStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupCouldNotStartEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_IN_PROGRESS_STATE\")\n+    public Action<?, ?> datalakebackupInProgress() {\n+        return new AbstractSdxAction<>(SdxEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext, SdxEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, SdxEvent payload, Map<Object, Object> variables) {\n+                LOGGER.info(\"Datalake database backup is in progress for {} \", payload.getResourceId());\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sendEvent(context, DatalakeDatabaseBackupWaitRequest.from(context, operationId));\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(SdxEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupFailedEvent.from(payload, ex);\n+\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_COULD_NOT_START_STATE\")\n+    public Action<?, ?> backupCouldNotStart() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupCouldNotStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupCouldNotStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupCouldNotStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                Exception exception = payload.getException();\n+                LOGGER.error(\"Datalake database backup could not be started for datalake with id: {}\", payload.getResourceId(), exception);\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sdxDrService.updateDatabaseStatusEntry(operationId, SdxDatabaseDrStatus.Status.FAILED, payload.getException().getMessage());\n+                sendEvent(context, DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseBackupCouldNotStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_FINISHED_STATE\")\n+    public Action<?, ?> finishedBackupAction() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupSuccessEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupSuccessEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupSuccessEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Sdx database backup is finalized with sdx id: {}\", payload.getResourceId());\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sdxDrService.updateDatabaseStatusEntry(operationId, SdxDatabaseDrStatus.Status.SUCCEEDED, null);\n+                sendEvent(context, DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseBackupSuccessEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_FAILED_STATE\")\n+    public Action<?, ?> backupFailed() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupFailedEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupFailedEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupFailedEvent payload, Map<Object, Object> variables) throws Exception {\n+                Exception exception = payload.getException();\n+                LOGGER.error(\"Datalake database backup failed for datalake with id: {}\", payload.getResourceId(), exception);\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sdxDrService.updateDatabaseStatusEntry(operationId, SdxDatabaseDrStatus.Status.FAILED, exception.getLocalizedMessage());\n+                sendEvent(context, DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT.event(), payload);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 164}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI0Mzc5Mw==", "bodyText": "Just for understanding sake, Is \"DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT\" used only in success cases?", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444243793", "createdAt": "2020-06-23T13:56:14Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseBackupActions.java", "diffHunk": "@@ -0,0 +1,173 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT;\n+\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.statemachine.StateContext;\n+import org.springframework.statemachine.action.Action;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxContext;\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupCouldNotStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupFailedEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupSuccessEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupWaitRequest;\n+import com.sequenceiq.datalake.service.AbstractSdxAction;\n+import com.sequenceiq.datalake.service.sdx.dr.SdxDrService;\n+import com.sequenceiq.flow.core.FlowEvent;\n+import com.sequenceiq.flow.core.FlowParameters;\n+import com.sequenceiq.flow.core.FlowState;\n+\n+@Configuration\n+public class DatalakeDatabaseBackupActions {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(DatalakeDatabaseBackupActions.class);\n+\n+    private static final String OPERATION_ID = \"OPERATION-ID\";\n+\n+    @Inject\n+    private SdxDrService sdxDrService;\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_START_STATE\")\n+    public Action<?, ?> datalakeBackup() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void prepareExecution(DatalakeDatabaseBackupStartEvent payload, Map<Object, Object> variables) {\n+                super.prepareExecution(payload, variables);\n+                variables.put(OPERATION_ID, payload.getDrStatus().getOperationId());\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Datalake database backup has been started for {}\", payload.getResourceId());\n+                sdxDrService.databaseBackup(payload.getDrStatus(),\n+                        payload.getResourceId(),\n+                        payload.getBackupId(),\n+                        payload.getBackupLocation());\n+                sendEvent(context, DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseBackupStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupCouldNotStartEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_IN_PROGRESS_STATE\")\n+    public Action<?, ?> datalakebackupInProgress() {\n+        return new AbstractSdxAction<>(SdxEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext, SdxEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, SdxEvent payload, Map<Object, Object> variables) {\n+                LOGGER.info(\"Datalake database backup is in progress for {} \", payload.getResourceId());\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sendEvent(context, DatalakeDatabaseBackupWaitRequest.from(context, operationId));\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(SdxEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupFailedEvent.from(payload, ex);\n+\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_COULD_NOT_START_STATE\")\n+    public Action<?, ?> backupCouldNotStart() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupCouldNotStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupCouldNotStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupCouldNotStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                Exception exception = payload.getException();\n+                LOGGER.error(\"Datalake database backup could not be started for datalake with id: {}\", payload.getResourceId(), exception);\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sdxDrService.updateDatabaseStatusEntry(operationId, SdxDatabaseDrStatus.Status.FAILED, payload.getException().getMessage());\n+                sendEvent(context, DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseBackupCouldNotStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_FINISHED_STATE\")\n+    public Action<?, ?> finishedBackupAction() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupSuccessEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupSuccessEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupSuccessEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Sdx database backup is finalized with sdx id: {}\", payload.getResourceId());\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sdxDrService.updateDatabaseStatusEntry(operationId, SdxDatabaseDrStatus.Status.SUCCEEDED, null);\n+                sendEvent(context, DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseBackupSuccessEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_FAILED_STATE\")\n+    public Action<?, ?> backupFailed() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupFailedEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupFailedEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupFailedEvent payload, Map<Object, Object> variables) throws Exception {\n+                Exception exception = payload.getException();\n+                LOGGER.error(\"Datalake database backup failed for datalake with id: {}\", payload.getResourceId(), exception);\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sdxDrService.updateDatabaseStatusEntry(operationId, SdxDatabaseDrStatus.Status.FAILED, exception.getLocalizedMessage());\n+                sendEvent(context, DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT.event(), payload);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA4OTgzNg=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 164}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDkwMDk0Mg==", "bodyText": "Yes, we use it like that", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444900942", "createdAt": "2020-06-24T13:40:17Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseBackupActions.java", "diffHunk": "@@ -0,0 +1,173 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT;\n+\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.statemachine.StateContext;\n+import org.springframework.statemachine.action.Action;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxContext;\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupCouldNotStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupFailedEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupSuccessEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupWaitRequest;\n+import com.sequenceiq.datalake.service.AbstractSdxAction;\n+import com.sequenceiq.datalake.service.sdx.dr.SdxDrService;\n+import com.sequenceiq.flow.core.FlowEvent;\n+import com.sequenceiq.flow.core.FlowParameters;\n+import com.sequenceiq.flow.core.FlowState;\n+\n+@Configuration\n+public class DatalakeDatabaseBackupActions {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(DatalakeDatabaseBackupActions.class);\n+\n+    private static final String OPERATION_ID = \"OPERATION-ID\";\n+\n+    @Inject\n+    private SdxDrService sdxDrService;\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_START_STATE\")\n+    public Action<?, ?> datalakeBackup() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void prepareExecution(DatalakeDatabaseBackupStartEvent payload, Map<Object, Object> variables) {\n+                super.prepareExecution(payload, variables);\n+                variables.put(OPERATION_ID, payload.getDrStatus().getOperationId());\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Datalake database backup has been started for {}\", payload.getResourceId());\n+                sdxDrService.databaseBackup(payload.getDrStatus(),\n+                        payload.getResourceId(),\n+                        payload.getBackupId(),\n+                        payload.getBackupLocation());\n+                sendEvent(context, DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseBackupStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupCouldNotStartEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_IN_PROGRESS_STATE\")\n+    public Action<?, ?> datalakebackupInProgress() {\n+        return new AbstractSdxAction<>(SdxEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext, SdxEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, SdxEvent payload, Map<Object, Object> variables) {\n+                LOGGER.info(\"Datalake database backup is in progress for {} \", payload.getResourceId());\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sendEvent(context, DatalakeDatabaseBackupWaitRequest.from(context, operationId));\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(SdxEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupFailedEvent.from(payload, ex);\n+\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_COULD_NOT_START_STATE\")\n+    public Action<?, ?> backupCouldNotStart() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupCouldNotStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupCouldNotStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupCouldNotStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                Exception exception = payload.getException();\n+                LOGGER.error(\"Datalake database backup could not be started for datalake with id: {}\", payload.getResourceId(), exception);\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sdxDrService.updateDatabaseStatusEntry(operationId, SdxDatabaseDrStatus.Status.FAILED, payload.getException().getMessage());\n+                sendEvent(context, DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseBackupCouldNotStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_FINISHED_STATE\")\n+    public Action<?, ?> finishedBackupAction() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupSuccessEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupSuccessEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupSuccessEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Sdx database backup is finalized with sdx id: {}\", payload.getResourceId());\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sdxDrService.updateDatabaseStatusEntry(operationId, SdxDatabaseDrStatus.Status.SUCCEEDED, null);\n+                sendEvent(context, DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseBackupSuccessEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_FAILED_STATE\")\n+    public Action<?, ?> backupFailed() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupFailedEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupFailedEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupFailedEvent payload, Map<Object, Object> variables) throws Exception {\n+                Exception exception = payload.getException();\n+                LOGGER.error(\"Datalake database backup failed for datalake with id: {}\", payload.getResourceId(), exception);\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sdxDrService.updateDatabaseStatusEntry(operationId, SdxDatabaseDrStatus.Status.FAILED, exception.getLocalizedMessage());\n+                sendEvent(context, DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT.event(), payload);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA4OTgzNg=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 164}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NzEwMDE0OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseBackupFlowConfig.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwOTozNDoyM1rOGnhROg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwOTozNDoyM1rOGnhROg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA5MjczMA==", "bodyText": "Not DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT instead?", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444092730", "createdAt": "2020-06-23T09:34:23Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseBackupFlowConfig.java", "diffHunk": "@@ -0,0 +1,98 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_COULD_NOT_START_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FAILED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_SUCCESS_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.DATALAKE_DATABASE_BACKUP_COULD_NOT_START_STATE;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.DATALAKE_DATABASE_BACKUP_FAILED_STATE;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.DATALAKE_DATABASE_BACKUP_FINISHED_STATE;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.DATALAKE_DATABASE_BACKUP_IN_PROGRESS_STATE;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.DATALAKE_DATABASE_BACKUP_START_STATE;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.FINAL_STATE;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.INIT_STATE;\n+\n+import java.util.List;\n+\n+import org.springframework.stereotype.Component;\n+\n+import com.sequenceiq.flow.core.config.AbstractFlowConfiguration;\n+import com.sequenceiq.flow.core.config.RetryableFlowConfiguration;\n+\n+@Component\n+public class DatalakeDatabaseBackupFlowConfig extends AbstractFlowConfiguration<DatalakeDatabaseDrState, DatalakeDatabaseDrEvent>\n+        implements RetryableFlowConfiguration<DatalakeDatabaseDrEvent> {\n+\n+    private static final List<Transition<DatalakeDatabaseDrState, DatalakeDatabaseDrEvent>> TRANSITIONS =\n+            new Transition.Builder<DatalakeDatabaseDrState, DatalakeDatabaseDrEvent>()\n+                    .defaultFailureEvent(DATALAKE_DATABASE_BACKUP_FAILED_EVENT)\n+\n+                    .from(INIT_STATE)\n+                    .to(DATALAKE_DATABASE_BACKUP_START_STATE)\n+                    .event(DATALAKE_DATABASE_BACKUP_EVENT).noFailureEvent()\n+\n+                    .from(DATALAKE_DATABASE_BACKUP_START_STATE)\n+                    .to(DATALAKE_DATABASE_BACKUP_IN_PROGRESS_STATE)\n+                    .event(DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT)\n+                    .failureState(DATALAKE_DATABASE_BACKUP_COULD_NOT_START_STATE)\n+                    .failureEvent(DATALAKE_DATABASE_BACKUP_COULD_NOT_START_EVENT)\n+\n+                    .from(DATALAKE_DATABASE_BACKUP_IN_PROGRESS_STATE)\n+                    .to(DATALAKE_DATABASE_BACKUP_FINISHED_STATE)\n+                    .event(DATALAKE_DATABASE_BACKUP_SUCCESS_EVENT)\n+                    .failureState(DATALAKE_DATABASE_BACKUP_FAILED_STATE)\n+                    .failureEvent(DATALAKE_DATABASE_BACKUP_FAILED_EVENT)\n+\n+                    .from(DATALAKE_DATABASE_BACKUP_FINISHED_STATE)\n+                    .to(FINAL_STATE)\n+                    .event(DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT).defaultFailureEvent()\n+\n+                    .from(DATALAKE_DATABASE_BACKUP_FAILED_STATE)\n+                    .to(FINAL_STATE)\n+                    .event(DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT).defaultFailureEvent()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NzExNzc4OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseRestoreActions.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwOTozOToxMFrOGnhccA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxMzo1NjozNVrOGnqgdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA5NTYwMA==", "bodyText": "Shouldn't we upgrade to INPROGRESS here also with updateDatabaseStatusEntry?", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444095600", "createdAt": "2020-06-23T09:39:10Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseRestoreActions.java", "diffHunk": "@@ -0,0 +1,170 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_FAILURE_HANDLED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT;\n+\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.statemachine.StateContext;\n+import org.springframework.statemachine.action.Action;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxContext;\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreCouldNotStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreFailedEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreSuccessEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreWaitRequest;\n+import com.sequenceiq.datalake.service.AbstractSdxAction;\n+import com.sequenceiq.datalake.service.sdx.dr.SdxDrService;\n+import com.sequenceiq.flow.core.FlowEvent;\n+import com.sequenceiq.flow.core.FlowParameters;\n+import com.sequenceiq.flow.core.FlowState;\n+\n+@Configuration\n+public class DatalakeDatabaseRestoreActions {\n+    private static final Logger LOGGER = LoggerFactory.getLogger(DatalakeDatabaseRestoreActions.class);\n+\n+    private static final String OPERATION_ID = \"OPERATION-ID\";\n+\n+    @Inject\n+    private SdxDrService sdxDrService;\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_START_STATE\")\n+    public Action<?, ?> datalakeRestore() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseRestoreStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseRestoreStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void prepareExecution(DatalakeDatabaseRestoreStartEvent payload, Map<Object, Object> variables) {\n+                super.prepareExecution(payload, variables);\n+                variables.put(OPERATION_ID, payload.getDrStatus().getOperationId());\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseRestoreStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Datalake database restore has been started for {}\", payload.getResourceId());\n+                sdxDrService.databaseRestore(payload.getDrStatus(),\n+                        payload.getResourceId(),\n+                        payload.getBackupId(),\n+                        payload.getBackupLocation());\n+                sendEvent(context, DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseRestoreStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseRestoreCouldNotStartEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_IN_PROGRESS_STATE\")\n+    public Action<?, ?> datalakeRestoreInProgress() {\n+        return new AbstractSdxAction<>(SdxEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext, SdxEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, SdxEvent payload, Map<Object, Object> variables) {\n+                LOGGER.info(\"Datalake database restore is in progress for {} \", payload.getResourceId());\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sendEvent(context, DatalakeDatabaseRestoreWaitRequest.from(context, operationId));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI0NDA4NA==", "bodyText": "yes, you are right. I will fix it.", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444244084", "createdAt": "2020-06-23T13:56:35Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseRestoreActions.java", "diffHunk": "@@ -0,0 +1,170 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_FAILURE_HANDLED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT;\n+\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.statemachine.StateContext;\n+import org.springframework.statemachine.action.Action;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxContext;\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreCouldNotStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreFailedEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreSuccessEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreWaitRequest;\n+import com.sequenceiq.datalake.service.AbstractSdxAction;\n+import com.sequenceiq.datalake.service.sdx.dr.SdxDrService;\n+import com.sequenceiq.flow.core.FlowEvent;\n+import com.sequenceiq.flow.core.FlowParameters;\n+import com.sequenceiq.flow.core.FlowState;\n+\n+@Configuration\n+public class DatalakeDatabaseRestoreActions {\n+    private static final Logger LOGGER = LoggerFactory.getLogger(DatalakeDatabaseRestoreActions.class);\n+\n+    private static final String OPERATION_ID = \"OPERATION-ID\";\n+\n+    @Inject\n+    private SdxDrService sdxDrService;\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_START_STATE\")\n+    public Action<?, ?> datalakeRestore() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseRestoreStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseRestoreStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void prepareExecution(DatalakeDatabaseRestoreStartEvent payload, Map<Object, Object> variables) {\n+                super.prepareExecution(payload, variables);\n+                variables.put(OPERATION_ID, payload.getDrStatus().getOperationId());\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseRestoreStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Datalake database restore has been started for {}\", payload.getResourceId());\n+                sdxDrService.databaseRestore(payload.getDrStatus(),\n+                        payload.getResourceId(),\n+                        payload.getBackupId(),\n+                        payload.getBackupLocation());\n+                sendEvent(context, DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseRestoreStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseRestoreCouldNotStartEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_IN_PROGRESS_STATE\")\n+    public Action<?, ?> datalakeRestoreInProgress() {\n+        return new AbstractSdxAction<>(SdxEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext, SdxEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, SdxEvent payload, Map<Object, Object> variables) {\n+                LOGGER.info(\"Datalake database restore is in progress for {} \", payload.getResourceId());\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sendEvent(context, DatalakeDatabaseRestoreWaitRequest.from(context, operationId));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA5NTYwMA=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 87}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NzEyNTU3OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseRestoreActions.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwOTo0MToyOVrOGnhhkQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNDowMToyNFrOGnqusg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA5NjkxMw==", "bodyText": "How about setting the SDX status here to failed as well with sdxStatusService.setStatusForDatalakeAndNotify()?", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444096913", "createdAt": "2020-06-23T09:41:29Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseRestoreActions.java", "diffHunk": "@@ -0,0 +1,170 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_FAILURE_HANDLED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT;\n+\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.statemachine.StateContext;\n+import org.springframework.statemachine.action.Action;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxContext;\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreCouldNotStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreFailedEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreSuccessEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreWaitRequest;\n+import com.sequenceiq.datalake.service.AbstractSdxAction;\n+import com.sequenceiq.datalake.service.sdx.dr.SdxDrService;\n+import com.sequenceiq.flow.core.FlowEvent;\n+import com.sequenceiq.flow.core.FlowParameters;\n+import com.sequenceiq.flow.core.FlowState;\n+\n+@Configuration\n+public class DatalakeDatabaseRestoreActions {\n+    private static final Logger LOGGER = LoggerFactory.getLogger(DatalakeDatabaseRestoreActions.class);\n+\n+    private static final String OPERATION_ID = \"OPERATION-ID\";\n+\n+    @Inject\n+    private SdxDrService sdxDrService;\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_START_STATE\")\n+    public Action<?, ?> datalakeRestore() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseRestoreStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseRestoreStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void prepareExecution(DatalakeDatabaseRestoreStartEvent payload, Map<Object, Object> variables) {\n+                super.prepareExecution(payload, variables);\n+                variables.put(OPERATION_ID, payload.getDrStatus().getOperationId());\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseRestoreStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Datalake database restore has been started for {}\", payload.getResourceId());\n+                sdxDrService.databaseRestore(payload.getDrStatus(),\n+                        payload.getResourceId(),\n+                        payload.getBackupId(),\n+                        payload.getBackupLocation());\n+                sendEvent(context, DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseRestoreStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseRestoreCouldNotStartEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_IN_PROGRESS_STATE\")\n+    public Action<?, ?> datalakeRestoreInProgress() {\n+        return new AbstractSdxAction<>(SdxEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext, SdxEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, SdxEvent payload, Map<Object, Object> variables) {\n+                LOGGER.info(\"Datalake database restore is in progress for {} \", payload.getResourceId());\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sendEvent(context, DatalakeDatabaseRestoreWaitRequest.from(context, operationId));\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(SdxEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseRestoreFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_COULD_NOT_START_STATE\")\n+    public Action<?, ?> restoreCouldNotStart() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseRestoreCouldNotStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseRestoreCouldNotStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseRestoreCouldNotStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                Exception exception = payload.getException();\n+                LOGGER.error(\"Datalake database restore could not be started for datalake with id: {}\", payload.getResourceId(), exception);\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sdxDrService.updateDatabaseStatusEntry(operationId, SdxDatabaseDrStatus.Status.FAILED, exception.getLocalizedMessage());\n+                sendEvent(context, DATALAKE_DATABASE_RESTORE_FAILURE_HANDLED_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseRestoreCouldNotStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseRestoreFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_FINISHED_STATE\")\n+    public Action<?, ?> finishedRestoreAction() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseRestoreSuccessEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseRestoreSuccessEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseRestoreSuccessEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Sdx database restore is finalized with sdx id: {}\", payload.getResourceId());\n+                sdxDrService.updateDatabaseStatusEntry(payload.getOperationId(), SdxDatabaseDrStatus.Status.SUCCEEDED, null);\n+                sendEvent(context, DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseRestoreSuccessEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseRestoreFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_FAILED_STATE\")\n+    public Action<?, ?> restoreFailed() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseRestoreFailedEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseRestoreFailedEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseRestoreFailedEvent payload, Map<Object, Object> variables) throws Exception {\n+                Exception exception = payload.getException();\n+                LOGGER.error(\"Datalake database restore could not be started for datalake with id: {}\", payload.getResourceId(), exception);\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sdxDrService.updateDatabaseStatusEntry(operationId, SdxDatabaseDrStatus.Status.FAILED, exception.getLocalizedMessage());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 160}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI0NzczMA==", "bodyText": "database backup/restore is one part of the datalake backup restore. There will be a separate patch to update the datalake status using SdxStatusService which will be invoked by datalakedr service which orchestrates the datalake backup and restore.", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444247730", "createdAt": "2020-06-23T14:01:24Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseRestoreActions.java", "diffHunk": "@@ -0,0 +1,170 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_FAILURE_HANDLED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT;\n+\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.statemachine.StateContext;\n+import org.springframework.statemachine.action.Action;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxContext;\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreCouldNotStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreFailedEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreSuccessEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreWaitRequest;\n+import com.sequenceiq.datalake.service.AbstractSdxAction;\n+import com.sequenceiq.datalake.service.sdx.dr.SdxDrService;\n+import com.sequenceiq.flow.core.FlowEvent;\n+import com.sequenceiq.flow.core.FlowParameters;\n+import com.sequenceiq.flow.core.FlowState;\n+\n+@Configuration\n+public class DatalakeDatabaseRestoreActions {\n+    private static final Logger LOGGER = LoggerFactory.getLogger(DatalakeDatabaseRestoreActions.class);\n+\n+    private static final String OPERATION_ID = \"OPERATION-ID\";\n+\n+    @Inject\n+    private SdxDrService sdxDrService;\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_START_STATE\")\n+    public Action<?, ?> datalakeRestore() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseRestoreStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseRestoreStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void prepareExecution(DatalakeDatabaseRestoreStartEvent payload, Map<Object, Object> variables) {\n+                super.prepareExecution(payload, variables);\n+                variables.put(OPERATION_ID, payload.getDrStatus().getOperationId());\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseRestoreStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Datalake database restore has been started for {}\", payload.getResourceId());\n+                sdxDrService.databaseRestore(payload.getDrStatus(),\n+                        payload.getResourceId(),\n+                        payload.getBackupId(),\n+                        payload.getBackupLocation());\n+                sendEvent(context, DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseRestoreStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseRestoreCouldNotStartEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_IN_PROGRESS_STATE\")\n+    public Action<?, ?> datalakeRestoreInProgress() {\n+        return new AbstractSdxAction<>(SdxEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext, SdxEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, SdxEvent payload, Map<Object, Object> variables) {\n+                LOGGER.info(\"Datalake database restore is in progress for {} \", payload.getResourceId());\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sendEvent(context, DatalakeDatabaseRestoreWaitRequest.from(context, operationId));\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(SdxEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseRestoreFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_COULD_NOT_START_STATE\")\n+    public Action<?, ?> restoreCouldNotStart() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseRestoreCouldNotStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseRestoreCouldNotStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseRestoreCouldNotStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                Exception exception = payload.getException();\n+                LOGGER.error(\"Datalake database restore could not be started for datalake with id: {}\", payload.getResourceId(), exception);\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sdxDrService.updateDatabaseStatusEntry(operationId, SdxDatabaseDrStatus.Status.FAILED, exception.getLocalizedMessage());\n+                sendEvent(context, DATALAKE_DATABASE_RESTORE_FAILURE_HANDLED_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseRestoreCouldNotStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseRestoreFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_FINISHED_STATE\")\n+    public Action<?, ?> finishedRestoreAction() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseRestoreSuccessEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseRestoreSuccessEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseRestoreSuccessEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Sdx database restore is finalized with sdx id: {}\", payload.getResourceId());\n+                sdxDrService.updateDatabaseStatusEntry(payload.getOperationId(), SdxDatabaseDrStatus.Status.SUCCEEDED, null);\n+                sendEvent(context, DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseRestoreSuccessEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseRestoreFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_FAILED_STATE\")\n+    public Action<?, ?> restoreFailed() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseRestoreFailedEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseRestoreFailedEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseRestoreFailedEvent payload, Map<Object, Object> variables) throws Exception {\n+                Exception exception = payload.getException();\n+                LOGGER.error(\"Datalake database restore could not be started for datalake with id: {}\", payload.getResourceId(), exception);\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sdxDrService.updateDatabaseStatusEntry(operationId, SdxDatabaseDrStatus.Status.FAILED, exception.getLocalizedMessage());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA5NjkxMw=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 160}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NzEzMjYxOnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseRestoreActions.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwOTo0MzoyNlrOGnhl9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwOTo0MzoyNlrOGnhl9A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA5ODAzNg==", "bodyText": "this should be FAILED_HANDLED_EVENT instead", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444098036", "createdAt": "2020-06-23T09:43:26Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseRestoreActions.java", "diffHunk": "@@ -0,0 +1,170 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_FAILURE_HANDLED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT;\n+\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.statemachine.StateContext;\n+import org.springframework.statemachine.action.Action;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxContext;\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreCouldNotStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreFailedEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreSuccessEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreWaitRequest;\n+import com.sequenceiq.datalake.service.AbstractSdxAction;\n+import com.sequenceiq.datalake.service.sdx.dr.SdxDrService;\n+import com.sequenceiq.flow.core.FlowEvent;\n+import com.sequenceiq.flow.core.FlowParameters;\n+import com.sequenceiq.flow.core.FlowState;\n+\n+@Configuration\n+public class DatalakeDatabaseRestoreActions {\n+    private static final Logger LOGGER = LoggerFactory.getLogger(DatalakeDatabaseRestoreActions.class);\n+\n+    private static final String OPERATION_ID = \"OPERATION-ID\";\n+\n+    @Inject\n+    private SdxDrService sdxDrService;\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_START_STATE\")\n+    public Action<?, ?> datalakeRestore() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseRestoreStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseRestoreStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void prepareExecution(DatalakeDatabaseRestoreStartEvent payload, Map<Object, Object> variables) {\n+                super.prepareExecution(payload, variables);\n+                variables.put(OPERATION_ID, payload.getDrStatus().getOperationId());\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseRestoreStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Datalake database restore has been started for {}\", payload.getResourceId());\n+                sdxDrService.databaseRestore(payload.getDrStatus(),\n+                        payload.getResourceId(),\n+                        payload.getBackupId(),\n+                        payload.getBackupLocation());\n+                sendEvent(context, DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseRestoreStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseRestoreCouldNotStartEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_IN_PROGRESS_STATE\")\n+    public Action<?, ?> datalakeRestoreInProgress() {\n+        return new AbstractSdxAction<>(SdxEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext, SdxEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, SdxEvent payload, Map<Object, Object> variables) {\n+                LOGGER.info(\"Datalake database restore is in progress for {} \", payload.getResourceId());\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sendEvent(context, DatalakeDatabaseRestoreWaitRequest.from(context, operationId));\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(SdxEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseRestoreFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_COULD_NOT_START_STATE\")\n+    public Action<?, ?> restoreCouldNotStart() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseRestoreCouldNotStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseRestoreCouldNotStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseRestoreCouldNotStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                Exception exception = payload.getException();\n+                LOGGER.error(\"Datalake database restore could not be started for datalake with id: {}\", payload.getResourceId(), exception);\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sdxDrService.updateDatabaseStatusEntry(operationId, SdxDatabaseDrStatus.Status.FAILED, exception.getLocalizedMessage());\n+                sendEvent(context, DATALAKE_DATABASE_RESTORE_FAILURE_HANDLED_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseRestoreCouldNotStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseRestoreFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_FINISHED_STATE\")\n+    public Action<?, ?> finishedRestoreAction() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseRestoreSuccessEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseRestoreSuccessEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseRestoreSuccessEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Sdx database restore is finalized with sdx id: {}\", payload.getResourceId());\n+                sdxDrService.updateDatabaseStatusEntry(payload.getOperationId(), SdxDatabaseDrStatus.Status.SUCCEEDED, null);\n+                sendEvent(context, DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseRestoreSuccessEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseRestoreFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_FAILED_STATE\")\n+    public Action<?, ?> restoreFailed() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseRestoreFailedEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseRestoreFailedEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseRestoreFailedEvent payload, Map<Object, Object> variables) throws Exception {\n+                Exception exception = payload.getException();\n+                LOGGER.error(\"Datalake database restore could not be started for datalake with id: {}\", payload.getResourceId(), exception);\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sdxDrService.updateDatabaseStatusEntry(operationId, SdxDatabaseDrStatus.Status.FAILED, exception.getLocalizedMessage());\n+                sendEvent(context, DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT.event(), payload);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 161}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NzEzMzQ2OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseRestoreFlowConfig.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwOTo0Mzo0MlrOGnhmjA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwOTo0Mzo0MlrOGnhmjA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA5ODE4OA==", "bodyText": "this should be FAILED_HANDLED_EVENT instead", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444098188", "createdAt": "2020-06-23T09:43:42Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseRestoreFlowConfig.java", "diffHunk": "@@ -0,0 +1,98 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_COULD_NOT_START_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_FAILED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_FAILURE_HANDLED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_SUCCESS_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.DATALAKE_DATABASE_RESTORE_COULD_NOT_START_STATE;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.DATALAKE_DATABASE_RESTORE_FAILED_STATE;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.DATALAKE_DATABASE_RESTORE_FINISHED_STATE;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.DATALAKE_DATABASE_RESTORE_IN_PROGRESS_STATE;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.DATALAKE_DATABASE_RESTORE_START_STATE;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.FINAL_STATE;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.INIT_STATE;\n+\n+import java.util.List;\n+\n+import org.springframework.stereotype.Component;\n+\n+import com.sequenceiq.flow.core.config.AbstractFlowConfiguration;\n+import com.sequenceiq.flow.core.config.RetryableFlowConfiguration;\n+\n+@Component\n+public class DatalakeDatabaseRestoreFlowConfig extends AbstractFlowConfiguration<DatalakeDatabaseDrState, DatalakeDatabaseDrEvent>\n+        implements RetryableFlowConfiguration<DatalakeDatabaseDrEvent> {\n+\n+    private static final List<Transition<DatalakeDatabaseDrState, DatalakeDatabaseDrEvent>> TRANSITIONS =\n+            new Transition.Builder<DatalakeDatabaseDrState, DatalakeDatabaseDrEvent>()\n+                    .defaultFailureEvent(DATALAKE_DATABASE_RESTORE_FAILED_EVENT)\n+\n+                    .from(INIT_STATE)\n+                    .to(DATALAKE_DATABASE_RESTORE_START_STATE)\n+                    .event(DATALAKE_DATABASE_RESTORE_EVENT).noFailureEvent()\n+\n+                    .from(DATALAKE_DATABASE_RESTORE_START_STATE)\n+                    .to(DATALAKE_DATABASE_RESTORE_IN_PROGRESS_STATE)\n+                    .event(DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT)\n+                    .failureState(DATALAKE_DATABASE_RESTORE_COULD_NOT_START_STATE)\n+                    .failureEvent(DATALAKE_DATABASE_RESTORE_COULD_NOT_START_EVENT)\n+\n+                    .from(DATALAKE_DATABASE_RESTORE_IN_PROGRESS_STATE)\n+                    .to(DATALAKE_DATABASE_RESTORE_FINISHED_STATE)\n+                    .event(DATALAKE_DATABASE_RESTORE_SUCCESS_EVENT)\n+                    .failureState(DATALAKE_DATABASE_RESTORE_FAILED_STATE)\n+                    .failureEvent(DATALAKE_DATABASE_RESTORE_FAILED_EVENT)\n+\n+                    .from(DATALAKE_DATABASE_RESTORE_FINISHED_STATE)\n+                    .to(FINAL_STATE)\n+                    .event(DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT).defaultFailureEvent()\n+\n+                    .from(DATALAKE_DATABASE_RESTORE_FAILED_STATE)\n+                    .to(FINAL_STATE)\n+                    .event(DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT).defaultFailureEvent()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NzE1OTU5OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwOTo1MDo0M1rOGnh2rw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxNToyNToxN1rOGoXV-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEwMjMxOQ==", "bodyText": "We should include the FlowIdentifier in SdxDatabaseBackupResponse, too", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444102319", "createdAt": "2020-06-23T09:50:43Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDk3ODY4MQ==", "bodyText": "will update.", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444978681", "createdAt": "2020-06-24T15:25:17Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEwMjMxOQ=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 75}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NzE2MDM4OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwOTo1MDo1OFrOGnh3JA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxNToyNToxNFrOGoXV2A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEwMjQzNg==", "bodyText": "We should include the FlowIdentifier in SdxDatabaseRestoreResponse, too", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444102436", "createdAt": "2020-06-23T09:50:58Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDk3ODY0OA==", "bodyText": "will update.", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444978648", "createdAt": "2020-06-24T15:25:14Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEwMjQzNg=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 81}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NzE2NjYzOnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwOTo1MjozMlrOGnh63g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwOTo1MjozMlrOGnh63g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEwMzM5MA==", "bodyText": "should return FlowIdentifier", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444103390", "createdAt": "2020-06-23T09:52:32Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 88}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NzE2NzM0OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwOTo1Mjo0M1rOGnh7Uw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwOTo1Mjo0M1rOGnh7Uw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEwMzUwNw==", "bodyText": "should return FlowIdentifier", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444103507", "createdAt": "2020-06-23T09:52:43Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    private String triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(clusterId, backupId, backupLocation);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 92}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NzE4MDM4OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwOTo1NjoxM1rOGniDmQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNDowNToyMVrOGnq5lw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEwNTYyNQ==", "bodyText": "We might want to set SDX status to something to indicate the restore with sdxStatusService.setStatusForDatalakeAndNotify(), too", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444105625", "createdAt": "2020-06-23T09:56:13Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    private String triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, backupV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI1MDUxOQ==", "bodyText": "database backup/restore is one part of the datalake backup restore. There will be a separate patch to update the datalake status using SdxStatusService which will be invoked by datalakedr service which orchestrates the datalake backup and restore.", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444250519", "createdAt": "2020-06-23T14:05:21Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    private String triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, backupV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEwNTYyNQ=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 116}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NzE4Mzg2OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwOTo1NzoxN1rOGniF7Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNDoyMDowMFrOGnrjiw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEwNjIyMQ==", "bodyText": "these 3 lines have multiple occurrences pls extract to a separate method.", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444106221", "createdAt": "2020-06-23T09:57:17Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    private String triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, backupV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, restoreV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 123}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI2MTI1OQ==", "bodyText": "will do.", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444261259", "createdAt": "2020-06-23T14:20:00Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    private String triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, backupV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, restoreV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEwNjIyMQ=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 123}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NzE5OTEwOnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxMDowMToyOVrOGniPlg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNDoyMDoyNFrOGnrkug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEwODY5NA==", "bodyText": "typo: successful + doe -> for", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444108694", "createdAt": "2020-06-23T10:01:29Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    private String triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, backupV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, restoreV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id %d not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void waitCloudbreakFlow(Long id, PollingConfig pollingConfig, String pollingMessage) {\n+        SdxCluster sdxCluster = sdxClusterRepository.findById(id).orElseThrow(notFound(\"SDX cluster\", id));\n+        Polling.waitPeriodly(pollingConfig.getSleepTime(), pollingConfig.getSleepTimeUnit())\n+                .stopIfException(pollingConfig.getStopPollingIfExceptionOccured())\n+                .stopAfterDelay(pollingConfig.getDuration(), pollingConfig.getDurationTimeUnit())\n+                .run(() -> checkDatabaseDrStatus(sdxCluster, pollingMessage));\n+    }\n+\n+    private AttemptResult<StackV4Response> checkDatabaseDrStatus(SdxCluster sdxCluster, String pollingMessage) throws JsonProcessingException {\n+        LOGGER.info(\"{} polling cloudbreak for stack status: '{}' in '{}' env\", pollingMessage, sdxCluster.getClusterName(), sdxCluster.getEnvName());\n+        try {\n+            if (PollGroup.CANCELLED.equals(DatalakeInMemoryStateStore.get(sdxCluster.getId()))) {\n+                LOGGER.info(\"{} polling cancelled in inmemory store, id: {}\", pollingMessage, sdxCluster.getId());\n+                return AttemptResults.breakFor(pollingMessage + \" polling cancelled in inmemory store, id: \" + sdxCluster.getId());\n+            } else {\n+                CloudbreakFlowService.FlowState flowState = cloudbreakFlowService.getLastKnownFlowState(sdxCluster);\n+                if (RUNNING.equals(flowState)) {\n+                    LOGGER.info(\"{} polling will continue, cluster has an active flow in Cloudbreak, id: {}\", pollingMessage, sdxCluster.getId());\n+                    return AttemptResults.justContinue();\n+                } else {\n+                    return getStackResponseAttemptResult(sdxCluster, pollingMessage, flowState);\n+                }\n+            }\n+        } catch (javax.ws.rs.NotFoundException e) {\n+            LOGGER.debug(\"Stack not found on CB side \" + sdxCluster.getClusterName(), e);\n+            return AttemptResults.breakFor(\"Stack not found on CB side \" + sdxCluster.getClusterName());\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> getStackResponseAttemptResult(SdxCluster sdxCluster, String pollingMessage, CloudbreakFlowService.FlowState flowState)\n+            throws JsonProcessingException {\n+        StackV4Response stackV4Response = stackV4Endpoint.get(0L, sdxCluster.getClusterName(), Collections.emptySet());\n+        LOGGER.info(\"Response from cloudbreak: {}\", JsonUtil.writeValueAsString(stackV4Response));\n+        ClusterV4Response cluster = stackV4Response.getCluster();\n+        if (stackAndClusterAvailable(stackV4Response, cluster)) {\n+            return sdxDrSucceeded(stackV4Response);\n+        } else {\n+            if (Status.BACKUP_FAILED.equals(stackV4Response.getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getStatus())) {\n+                LOGGER.info(\"{} failed for Stack {} with status {}\", pollingMessage, stackV4Response.getName(), stackV4Response.getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getStatusReason(), pollingMessage);\n+            } else if (Status.BACKUP_FAILED.equals(stackV4Response.getCluster().getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getCluster().getStatus())) {\n+                LOGGER.info(\"{} failed for Cluster {} status {}\", pollingMessage, stackV4Response.getCluster().getName(),\n+                        stackV4Response.getCluster().getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getCluster().getStatusReason(), pollingMessage);\n+            } else {\n+                if (FINISHED.equals(flowState)) {\n+                    LOGGER.info(\"Flow finished, but Backup/Restore is not complete: {}\", sdxCluster.getClusterName());\n+                    return sdxDrFailed(sdxCluster, \"stack is in improper state\", pollingMessage);\n+                } else {\n+                    return AttemptResults.justContinue();\n+                }\n+            }\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrFailed(SdxCluster sdxCluster, String statusReason, String pollingMessage) {\n+        LOGGER.info(\"{} failed, statusReason: {}\", pollingMessage, statusReason);\n+        return AttemptResults.breakFor(\"SDX \" + pollingMessage + \" failed '\" + sdxCluster.getClusterName() + \"', \" + statusReason);\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrSucceeded(StackV4Response stackV4Response) {\n+        LOGGER.info(\"Database DR operation doe SDX cluster {} is successfull\", stackV4Response.getCluster().getName());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 196}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI2MTU2Mg==", "bodyText": "will do", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444261562", "createdAt": "2020-06-23T14:20:24Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    private String triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, backupV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, restoreV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id %d not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void waitCloudbreakFlow(Long id, PollingConfig pollingConfig, String pollingMessage) {\n+        SdxCluster sdxCluster = sdxClusterRepository.findById(id).orElseThrow(notFound(\"SDX cluster\", id));\n+        Polling.waitPeriodly(pollingConfig.getSleepTime(), pollingConfig.getSleepTimeUnit())\n+                .stopIfException(pollingConfig.getStopPollingIfExceptionOccured())\n+                .stopAfterDelay(pollingConfig.getDuration(), pollingConfig.getDurationTimeUnit())\n+                .run(() -> checkDatabaseDrStatus(sdxCluster, pollingMessage));\n+    }\n+\n+    private AttemptResult<StackV4Response> checkDatabaseDrStatus(SdxCluster sdxCluster, String pollingMessage) throws JsonProcessingException {\n+        LOGGER.info(\"{} polling cloudbreak for stack status: '{}' in '{}' env\", pollingMessage, sdxCluster.getClusterName(), sdxCluster.getEnvName());\n+        try {\n+            if (PollGroup.CANCELLED.equals(DatalakeInMemoryStateStore.get(sdxCluster.getId()))) {\n+                LOGGER.info(\"{} polling cancelled in inmemory store, id: {}\", pollingMessage, sdxCluster.getId());\n+                return AttemptResults.breakFor(pollingMessage + \" polling cancelled in inmemory store, id: \" + sdxCluster.getId());\n+            } else {\n+                CloudbreakFlowService.FlowState flowState = cloudbreakFlowService.getLastKnownFlowState(sdxCluster);\n+                if (RUNNING.equals(flowState)) {\n+                    LOGGER.info(\"{} polling will continue, cluster has an active flow in Cloudbreak, id: {}\", pollingMessage, sdxCluster.getId());\n+                    return AttemptResults.justContinue();\n+                } else {\n+                    return getStackResponseAttemptResult(sdxCluster, pollingMessage, flowState);\n+                }\n+            }\n+        } catch (javax.ws.rs.NotFoundException e) {\n+            LOGGER.debug(\"Stack not found on CB side \" + sdxCluster.getClusterName(), e);\n+            return AttemptResults.breakFor(\"Stack not found on CB side \" + sdxCluster.getClusterName());\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> getStackResponseAttemptResult(SdxCluster sdxCluster, String pollingMessage, CloudbreakFlowService.FlowState flowState)\n+            throws JsonProcessingException {\n+        StackV4Response stackV4Response = stackV4Endpoint.get(0L, sdxCluster.getClusterName(), Collections.emptySet());\n+        LOGGER.info(\"Response from cloudbreak: {}\", JsonUtil.writeValueAsString(stackV4Response));\n+        ClusterV4Response cluster = stackV4Response.getCluster();\n+        if (stackAndClusterAvailable(stackV4Response, cluster)) {\n+            return sdxDrSucceeded(stackV4Response);\n+        } else {\n+            if (Status.BACKUP_FAILED.equals(stackV4Response.getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getStatus())) {\n+                LOGGER.info(\"{} failed for Stack {} with status {}\", pollingMessage, stackV4Response.getName(), stackV4Response.getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getStatusReason(), pollingMessage);\n+            } else if (Status.BACKUP_FAILED.equals(stackV4Response.getCluster().getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getCluster().getStatus())) {\n+                LOGGER.info(\"{} failed for Cluster {} status {}\", pollingMessage, stackV4Response.getCluster().getName(),\n+                        stackV4Response.getCluster().getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getCluster().getStatusReason(), pollingMessage);\n+            } else {\n+                if (FINISHED.equals(flowState)) {\n+                    LOGGER.info(\"Flow finished, but Backup/Restore is not complete: {}\", sdxCluster.getClusterName());\n+                    return sdxDrFailed(sdxCluster, \"stack is in improper state\", pollingMessage);\n+                } else {\n+                    return AttemptResults.justContinue();\n+                }\n+            }\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrFailed(SdxCluster sdxCluster, String statusReason, String pollingMessage) {\n+        LOGGER.info(\"{} failed, statusReason: {}\", pollingMessage, statusReason);\n+        return AttemptResults.breakFor(\"SDX \" + pollingMessage + \" failed '\" + sdxCluster.getClusterName() + \"', \" + statusReason);\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrSucceeded(StackV4Response stackV4Response) {\n+        LOGGER.info(\"Database DR operation doe SDX cluster {} is successfull\", stackV4Response.getCluster().getName());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEwODY5NA=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 196}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NzIxMDY3OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxMDowNDo0NlrOGniWyA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNDoyMTo1OFrOGnrpQA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDExMDUzNg==", "bodyText": "we might log this as it should not happen", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444110536", "createdAt": "2020-06-23T10:04:46Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    private String triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, backupV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, restoreV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id %d not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void waitCloudbreakFlow(Long id, PollingConfig pollingConfig, String pollingMessage) {\n+        SdxCluster sdxCluster = sdxClusterRepository.findById(id).orElseThrow(notFound(\"SDX cluster\", id));\n+        Polling.waitPeriodly(pollingConfig.getSleepTime(), pollingConfig.getSleepTimeUnit())\n+                .stopIfException(pollingConfig.getStopPollingIfExceptionOccured())\n+                .stopAfterDelay(pollingConfig.getDuration(), pollingConfig.getDurationTimeUnit())\n+                .run(() -> checkDatabaseDrStatus(sdxCluster, pollingMessage));\n+    }\n+\n+    private AttemptResult<StackV4Response> checkDatabaseDrStatus(SdxCluster sdxCluster, String pollingMessage) throws JsonProcessingException {\n+        LOGGER.info(\"{} polling cloudbreak for stack status: '{}' in '{}' env\", pollingMessage, sdxCluster.getClusterName(), sdxCluster.getEnvName());\n+        try {\n+            if (PollGroup.CANCELLED.equals(DatalakeInMemoryStateStore.get(sdxCluster.getId()))) {\n+                LOGGER.info(\"{} polling cancelled in inmemory store, id: {}\", pollingMessage, sdxCluster.getId());\n+                return AttemptResults.breakFor(pollingMessage + \" polling cancelled in inmemory store, id: \" + sdxCluster.getId());\n+            } else {\n+                CloudbreakFlowService.FlowState flowState = cloudbreakFlowService.getLastKnownFlowState(sdxCluster);\n+                if (RUNNING.equals(flowState)) {\n+                    LOGGER.info(\"{} polling will continue, cluster has an active flow in Cloudbreak, id: {}\", pollingMessage, sdxCluster.getId());\n+                    return AttemptResults.justContinue();\n+                } else {\n+                    return getStackResponseAttemptResult(sdxCluster, pollingMessage, flowState);\n+                }\n+            }\n+        } catch (javax.ws.rs.NotFoundException e) {\n+            LOGGER.debug(\"Stack not found on CB side \" + sdxCluster.getClusterName(), e);\n+            return AttemptResults.breakFor(\"Stack not found on CB side \" + sdxCluster.getClusterName());\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> getStackResponseAttemptResult(SdxCluster sdxCluster, String pollingMessage, CloudbreakFlowService.FlowState flowState)\n+            throws JsonProcessingException {\n+        StackV4Response stackV4Response = stackV4Endpoint.get(0L, sdxCluster.getClusterName(), Collections.emptySet());\n+        LOGGER.info(\"Response from cloudbreak: {}\", JsonUtil.writeValueAsString(stackV4Response));\n+        ClusterV4Response cluster = stackV4Response.getCluster();\n+        if (stackAndClusterAvailable(stackV4Response, cluster)) {\n+            return sdxDrSucceeded(stackV4Response);\n+        } else {\n+            if (Status.BACKUP_FAILED.equals(stackV4Response.getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getStatus())) {\n+                LOGGER.info(\"{} failed for Stack {} with status {}\", pollingMessage, stackV4Response.getName(), stackV4Response.getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getStatusReason(), pollingMessage);\n+            } else if (Status.BACKUP_FAILED.equals(stackV4Response.getCluster().getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getCluster().getStatus())) {\n+                LOGGER.info(\"{} failed for Cluster {} status {}\", pollingMessage, stackV4Response.getCluster().getName(),\n+                        stackV4Response.getCluster().getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getCluster().getStatusReason(), pollingMessage);\n+            } else {\n+                if (FINISHED.equals(flowState)) {\n+                    LOGGER.info(\"Flow finished, but Backup/Restore is not complete: {}\", sdxCluster.getClusterName());\n+                    return sdxDrFailed(sdxCluster, \"stack is in improper state\", pollingMessage);\n+                } else {\n+                    return AttemptResults.justContinue();\n+                }\n+            }\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrFailed(SdxCluster sdxCluster, String statusReason, String pollingMessage) {\n+        LOGGER.info(\"{} failed, statusReason: {}\", pollingMessage, statusReason);\n+        return AttemptResults.breakFor(\"SDX \" + pollingMessage + \" failed '\" + sdxCluster.getClusterName() + \"', \" + statusReason);\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrSucceeded(StackV4Response stackV4Response) {\n+        LOGGER.info(\"Database DR operation doe SDX cluster {} is successfull\", stackV4Response.getCluster().getName());\n+        return AttemptResults.finishWith(stackV4Response);\n+    }\n+\n+    private boolean stackAndClusterAvailable(StackV4Response stackV4Response, ClusterV4Response cluster) {\n+        return stackV4Response.getStatus().isAvailable()\n+                && cluster != null\n+                && cluster.getStatus() != null\n+                && cluster.getStatus().isAvailable();\n+    }\n+\n+    /**\n+     * Updates the status of the database backup/restore operation.\n+     * @param operationId Operation Id\n+     * @param status Status of the operation\n+     * @param failedReason Failure reason, if any.\n+     */\n+    public void updateDatabaseStatusEntry(String operationId, SdxDatabaseDrStatus.Status status, String failedReason) {\n+        if (Strings.isNullOrEmpty(operationId)) {\n+            return;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 215}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI2MjcyMA==", "bodyText": "will do.", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444262720", "createdAt": "2020-06-23T14:21:58Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    private String triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, backupV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, restoreV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id %d not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void waitCloudbreakFlow(Long id, PollingConfig pollingConfig, String pollingMessage) {\n+        SdxCluster sdxCluster = sdxClusterRepository.findById(id).orElseThrow(notFound(\"SDX cluster\", id));\n+        Polling.waitPeriodly(pollingConfig.getSleepTime(), pollingConfig.getSleepTimeUnit())\n+                .stopIfException(pollingConfig.getStopPollingIfExceptionOccured())\n+                .stopAfterDelay(pollingConfig.getDuration(), pollingConfig.getDurationTimeUnit())\n+                .run(() -> checkDatabaseDrStatus(sdxCluster, pollingMessage));\n+    }\n+\n+    private AttemptResult<StackV4Response> checkDatabaseDrStatus(SdxCluster sdxCluster, String pollingMessage) throws JsonProcessingException {\n+        LOGGER.info(\"{} polling cloudbreak for stack status: '{}' in '{}' env\", pollingMessage, sdxCluster.getClusterName(), sdxCluster.getEnvName());\n+        try {\n+            if (PollGroup.CANCELLED.equals(DatalakeInMemoryStateStore.get(sdxCluster.getId()))) {\n+                LOGGER.info(\"{} polling cancelled in inmemory store, id: {}\", pollingMessage, sdxCluster.getId());\n+                return AttemptResults.breakFor(pollingMessage + \" polling cancelled in inmemory store, id: \" + sdxCluster.getId());\n+            } else {\n+                CloudbreakFlowService.FlowState flowState = cloudbreakFlowService.getLastKnownFlowState(sdxCluster);\n+                if (RUNNING.equals(flowState)) {\n+                    LOGGER.info(\"{} polling will continue, cluster has an active flow in Cloudbreak, id: {}\", pollingMessage, sdxCluster.getId());\n+                    return AttemptResults.justContinue();\n+                } else {\n+                    return getStackResponseAttemptResult(sdxCluster, pollingMessage, flowState);\n+                }\n+            }\n+        } catch (javax.ws.rs.NotFoundException e) {\n+            LOGGER.debug(\"Stack not found on CB side \" + sdxCluster.getClusterName(), e);\n+            return AttemptResults.breakFor(\"Stack not found on CB side \" + sdxCluster.getClusterName());\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> getStackResponseAttemptResult(SdxCluster sdxCluster, String pollingMessage, CloudbreakFlowService.FlowState flowState)\n+            throws JsonProcessingException {\n+        StackV4Response stackV4Response = stackV4Endpoint.get(0L, sdxCluster.getClusterName(), Collections.emptySet());\n+        LOGGER.info(\"Response from cloudbreak: {}\", JsonUtil.writeValueAsString(stackV4Response));\n+        ClusterV4Response cluster = stackV4Response.getCluster();\n+        if (stackAndClusterAvailable(stackV4Response, cluster)) {\n+            return sdxDrSucceeded(stackV4Response);\n+        } else {\n+            if (Status.BACKUP_FAILED.equals(stackV4Response.getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getStatus())) {\n+                LOGGER.info(\"{} failed for Stack {} with status {}\", pollingMessage, stackV4Response.getName(), stackV4Response.getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getStatusReason(), pollingMessage);\n+            } else if (Status.BACKUP_FAILED.equals(stackV4Response.getCluster().getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getCluster().getStatus())) {\n+                LOGGER.info(\"{} failed for Cluster {} status {}\", pollingMessage, stackV4Response.getCluster().getName(),\n+                        stackV4Response.getCluster().getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getCluster().getStatusReason(), pollingMessage);\n+            } else {\n+                if (FINISHED.equals(flowState)) {\n+                    LOGGER.info(\"Flow finished, but Backup/Restore is not complete: {}\", sdxCluster.getClusterName());\n+                    return sdxDrFailed(sdxCluster, \"stack is in improper state\", pollingMessage);\n+                } else {\n+                    return AttemptResults.justContinue();\n+                }\n+            }\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrFailed(SdxCluster sdxCluster, String statusReason, String pollingMessage) {\n+        LOGGER.info(\"{} failed, statusReason: {}\", pollingMessage, statusReason);\n+        return AttemptResults.breakFor(\"SDX \" + pollingMessage + \" failed '\" + sdxCluster.getClusterName() + \"', \" + statusReason);\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrSucceeded(StackV4Response stackV4Response) {\n+        LOGGER.info(\"Database DR operation doe SDX cluster {} is successfull\", stackV4Response.getCluster().getName());\n+        return AttemptResults.finishWith(stackV4Response);\n+    }\n+\n+    private boolean stackAndClusterAvailable(StackV4Response stackV4Response, ClusterV4Response cluster) {\n+        return stackV4Response.getStatus().isAvailable()\n+                && cluster != null\n+                && cluster.getStatus() != null\n+                && cluster.getStatus().isAvailable();\n+    }\n+\n+    /**\n+     * Updates the status of the database backup/restore operation.\n+     * @param operationId Operation Id\n+     * @param status Status of the operation\n+     * @param failedReason Failure reason, if any.\n+     */\n+    public void updateDatabaseStatusEntry(String operationId, SdxDatabaseDrStatus.Status status, String failedReason) {\n+        if (Strings.isNullOrEmpty(operationId)) {\n+            return;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDExMDUzNg=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 215}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NzIyNTMyOnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxMDowOTowMlrOGnif6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNDoyNToyMFrOGnrzqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDExMjg3Mw==", "bodyText": "the first 6 lines of getDatabaseBackupStatus and getDatabaseRestoreStatus are almost the same -> should be extracted to a common method with SdxDatabaseDrStatusTypeEnum as parameter", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444112873", "createdAt": "2020-06-23T10:09:02Z", "author": {"login": "pdarvasi"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    private String triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, backupV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, restoreV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id %d not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void waitCloudbreakFlow(Long id, PollingConfig pollingConfig, String pollingMessage) {\n+        SdxCluster sdxCluster = sdxClusterRepository.findById(id).orElseThrow(notFound(\"SDX cluster\", id));\n+        Polling.waitPeriodly(pollingConfig.getSleepTime(), pollingConfig.getSleepTimeUnit())\n+                .stopIfException(pollingConfig.getStopPollingIfExceptionOccured())\n+                .stopAfterDelay(pollingConfig.getDuration(), pollingConfig.getDurationTimeUnit())\n+                .run(() -> checkDatabaseDrStatus(sdxCluster, pollingMessage));\n+    }\n+\n+    private AttemptResult<StackV4Response> checkDatabaseDrStatus(SdxCluster sdxCluster, String pollingMessage) throws JsonProcessingException {\n+        LOGGER.info(\"{} polling cloudbreak for stack status: '{}' in '{}' env\", pollingMessage, sdxCluster.getClusterName(), sdxCluster.getEnvName());\n+        try {\n+            if (PollGroup.CANCELLED.equals(DatalakeInMemoryStateStore.get(sdxCluster.getId()))) {\n+                LOGGER.info(\"{} polling cancelled in inmemory store, id: {}\", pollingMessage, sdxCluster.getId());\n+                return AttemptResults.breakFor(pollingMessage + \" polling cancelled in inmemory store, id: \" + sdxCluster.getId());\n+            } else {\n+                CloudbreakFlowService.FlowState flowState = cloudbreakFlowService.getLastKnownFlowState(sdxCluster);\n+                if (RUNNING.equals(flowState)) {\n+                    LOGGER.info(\"{} polling will continue, cluster has an active flow in Cloudbreak, id: {}\", pollingMessage, sdxCluster.getId());\n+                    return AttemptResults.justContinue();\n+                } else {\n+                    return getStackResponseAttemptResult(sdxCluster, pollingMessage, flowState);\n+                }\n+            }\n+        } catch (javax.ws.rs.NotFoundException e) {\n+            LOGGER.debug(\"Stack not found on CB side \" + sdxCluster.getClusterName(), e);\n+            return AttemptResults.breakFor(\"Stack not found on CB side \" + sdxCluster.getClusterName());\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> getStackResponseAttemptResult(SdxCluster sdxCluster, String pollingMessage, CloudbreakFlowService.FlowState flowState)\n+            throws JsonProcessingException {\n+        StackV4Response stackV4Response = stackV4Endpoint.get(0L, sdxCluster.getClusterName(), Collections.emptySet());\n+        LOGGER.info(\"Response from cloudbreak: {}\", JsonUtil.writeValueAsString(stackV4Response));\n+        ClusterV4Response cluster = stackV4Response.getCluster();\n+        if (stackAndClusterAvailable(stackV4Response, cluster)) {\n+            return sdxDrSucceeded(stackV4Response);\n+        } else {\n+            if (Status.BACKUP_FAILED.equals(stackV4Response.getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getStatus())) {\n+                LOGGER.info(\"{} failed for Stack {} with status {}\", pollingMessage, stackV4Response.getName(), stackV4Response.getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getStatusReason(), pollingMessage);\n+            } else if (Status.BACKUP_FAILED.equals(stackV4Response.getCluster().getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getCluster().getStatus())) {\n+                LOGGER.info(\"{} failed for Cluster {} status {}\", pollingMessage, stackV4Response.getCluster().getName(),\n+                        stackV4Response.getCluster().getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getCluster().getStatusReason(), pollingMessage);\n+            } else {\n+                if (FINISHED.equals(flowState)) {\n+                    LOGGER.info(\"Flow finished, but Backup/Restore is not complete: {}\", sdxCluster.getClusterName());\n+                    return sdxDrFailed(sdxCluster, \"stack is in improper state\", pollingMessage);\n+                } else {\n+                    return AttemptResults.justContinue();\n+                }\n+            }\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrFailed(SdxCluster sdxCluster, String statusReason, String pollingMessage) {\n+        LOGGER.info(\"{} failed, statusReason: {}\", pollingMessage, statusReason);\n+        return AttemptResults.breakFor(\"SDX \" + pollingMessage + \" failed '\" + sdxCluster.getClusterName() + \"', \" + statusReason);\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrSucceeded(StackV4Response stackV4Response) {\n+        LOGGER.info(\"Database DR operation doe SDX cluster {} is successfull\", stackV4Response.getCluster().getName());\n+        return AttemptResults.finishWith(stackV4Response);\n+    }\n+\n+    private boolean stackAndClusterAvailable(StackV4Response stackV4Response, ClusterV4Response cluster) {\n+        return stackV4Response.getStatus().isAvailable()\n+                && cluster != null\n+                && cluster.getStatus() != null\n+                && cluster.getStatus().isAvailable();\n+    }\n+\n+    /**\n+     * Updates the status of the database backup/restore operation.\n+     * @param operationId Operation Id\n+     * @param status Status of the operation\n+     * @param failedReason Failure reason, if any.\n+     */\n+    public void updateDatabaseStatusEntry(String operationId, SdxDatabaseDrStatus.Status status, String failedReason) {\n+        if (Strings.isNullOrEmpty(operationId)) {\n+            return;\n+        }\n+        SdxDatabaseDrStatus drStatus = sdxDatabaseDrStatusRepository.findSdxDatabaseDrStatusByOperationId(operationId);\n+        drStatus.setStatus(status);\n+        if (!Strings.isNullOrEmpty(failedReason)) {\n+            drStatus.setStatusReason(failedReason);\n+        }\n+        sdxDatabaseDrStatusRepository.save(drStatus);\n+    }\n+\n+    /**\n+     * Gets the status of the database backup operation.\n+     * @param sdxCluster Sdx cluster on which the backup operation is performed.\n+     * @param operationId Operation Id\n+     * @return Backup status\n+     */\n+    public SdxDatabaseBackupStatusResponse getDatabaseBackupStatus(SdxCluster sdxCluster, String operationId) {\n+        SdxDatabaseDrStatus drStatus = sdxDatabaseDrStatusRepository.findSdxDatabaseDrStatusByOperationId(operationId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 232}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI2NTM4NQ==", "bodyText": "will do", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444265385", "createdAt": "2020-06-23T14:25:20Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    private String triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, backupV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, restoreV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id %d not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void waitCloudbreakFlow(Long id, PollingConfig pollingConfig, String pollingMessage) {\n+        SdxCluster sdxCluster = sdxClusterRepository.findById(id).orElseThrow(notFound(\"SDX cluster\", id));\n+        Polling.waitPeriodly(pollingConfig.getSleepTime(), pollingConfig.getSleepTimeUnit())\n+                .stopIfException(pollingConfig.getStopPollingIfExceptionOccured())\n+                .stopAfterDelay(pollingConfig.getDuration(), pollingConfig.getDurationTimeUnit())\n+                .run(() -> checkDatabaseDrStatus(sdxCluster, pollingMessage));\n+    }\n+\n+    private AttemptResult<StackV4Response> checkDatabaseDrStatus(SdxCluster sdxCluster, String pollingMessage) throws JsonProcessingException {\n+        LOGGER.info(\"{} polling cloudbreak for stack status: '{}' in '{}' env\", pollingMessage, sdxCluster.getClusterName(), sdxCluster.getEnvName());\n+        try {\n+            if (PollGroup.CANCELLED.equals(DatalakeInMemoryStateStore.get(sdxCluster.getId()))) {\n+                LOGGER.info(\"{} polling cancelled in inmemory store, id: {}\", pollingMessage, sdxCluster.getId());\n+                return AttemptResults.breakFor(pollingMessage + \" polling cancelled in inmemory store, id: \" + sdxCluster.getId());\n+            } else {\n+                CloudbreakFlowService.FlowState flowState = cloudbreakFlowService.getLastKnownFlowState(sdxCluster);\n+                if (RUNNING.equals(flowState)) {\n+                    LOGGER.info(\"{} polling will continue, cluster has an active flow in Cloudbreak, id: {}\", pollingMessage, sdxCluster.getId());\n+                    return AttemptResults.justContinue();\n+                } else {\n+                    return getStackResponseAttemptResult(sdxCluster, pollingMessage, flowState);\n+                }\n+            }\n+        } catch (javax.ws.rs.NotFoundException e) {\n+            LOGGER.debug(\"Stack not found on CB side \" + sdxCluster.getClusterName(), e);\n+            return AttemptResults.breakFor(\"Stack not found on CB side \" + sdxCluster.getClusterName());\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> getStackResponseAttemptResult(SdxCluster sdxCluster, String pollingMessage, CloudbreakFlowService.FlowState flowState)\n+            throws JsonProcessingException {\n+        StackV4Response stackV4Response = stackV4Endpoint.get(0L, sdxCluster.getClusterName(), Collections.emptySet());\n+        LOGGER.info(\"Response from cloudbreak: {}\", JsonUtil.writeValueAsString(stackV4Response));\n+        ClusterV4Response cluster = stackV4Response.getCluster();\n+        if (stackAndClusterAvailable(stackV4Response, cluster)) {\n+            return sdxDrSucceeded(stackV4Response);\n+        } else {\n+            if (Status.BACKUP_FAILED.equals(stackV4Response.getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getStatus())) {\n+                LOGGER.info(\"{} failed for Stack {} with status {}\", pollingMessage, stackV4Response.getName(), stackV4Response.getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getStatusReason(), pollingMessage);\n+            } else if (Status.BACKUP_FAILED.equals(stackV4Response.getCluster().getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getCluster().getStatus())) {\n+                LOGGER.info(\"{} failed for Cluster {} status {}\", pollingMessage, stackV4Response.getCluster().getName(),\n+                        stackV4Response.getCluster().getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getCluster().getStatusReason(), pollingMessage);\n+            } else {\n+                if (FINISHED.equals(flowState)) {\n+                    LOGGER.info(\"Flow finished, but Backup/Restore is not complete: {}\", sdxCluster.getClusterName());\n+                    return sdxDrFailed(sdxCluster, \"stack is in improper state\", pollingMessage);\n+                } else {\n+                    return AttemptResults.justContinue();\n+                }\n+            }\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrFailed(SdxCluster sdxCluster, String statusReason, String pollingMessage) {\n+        LOGGER.info(\"{} failed, statusReason: {}\", pollingMessage, statusReason);\n+        return AttemptResults.breakFor(\"SDX \" + pollingMessage + \" failed '\" + sdxCluster.getClusterName() + \"', \" + statusReason);\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrSucceeded(StackV4Response stackV4Response) {\n+        LOGGER.info(\"Database DR operation doe SDX cluster {} is successfull\", stackV4Response.getCluster().getName());\n+        return AttemptResults.finishWith(stackV4Response);\n+    }\n+\n+    private boolean stackAndClusterAvailable(StackV4Response stackV4Response, ClusterV4Response cluster) {\n+        return stackV4Response.getStatus().isAvailable()\n+                && cluster != null\n+                && cluster.getStatus() != null\n+                && cluster.getStatus().isAvailable();\n+    }\n+\n+    /**\n+     * Updates the status of the database backup/restore operation.\n+     * @param operationId Operation Id\n+     * @param status Status of the operation\n+     * @param failedReason Failure reason, if any.\n+     */\n+    public void updateDatabaseStatusEntry(String operationId, SdxDatabaseDrStatus.Status status, String failedReason) {\n+        if (Strings.isNullOrEmpty(operationId)) {\n+            return;\n+        }\n+        SdxDatabaseDrStatus drStatus = sdxDatabaseDrStatusRepository.findSdxDatabaseDrStatusByOperationId(operationId);\n+        drStatus.setStatus(status);\n+        if (!Strings.isNullOrEmpty(failedReason)) {\n+            drStatus.setStatusReason(failedReason);\n+        }\n+        sdxDatabaseDrStatusRepository.save(drStatus);\n+    }\n+\n+    /**\n+     * Gets the status of the database backup operation.\n+     * @param sdxCluster Sdx cluster on which the backup operation is performed.\n+     * @param operationId Operation Id\n+     * @return Backup status\n+     */\n+    public SdxDatabaseBackupStatusResponse getDatabaseBackupStatus(SdxCluster sdxCluster, String operationId) {\n+        SdxDatabaseDrStatus drStatus = sdxDatabaseDrStatusRepository.findSdxDatabaseDrStatusByOperationId(operationId);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDExMjg3Mw=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 232}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NzI0NjM2OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/SdxDatabaseDrStatus.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxMDoxNTowNFrOGnitIA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxNToxOTowMFrOGoXEOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDExNjI1Ng==", "bodyText": "you shouldn't specify the name here", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444116256", "createdAt": "2020-06-23T10:15:04Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/SdxDatabaseDrStatus.java", "diffHunk": "@@ -0,0 +1,102 @@\n+package com.sequenceiq.datalake.entity;\n+\n+import java.util.UUID;\n+\n+import javax.persistence.Entity;\n+import javax.persistence.EnumType;\n+import javax.persistence.Enumerated;\n+import javax.persistence.GeneratedValue;\n+import javax.persistence.GenerationType;\n+import javax.persistence.Id;\n+import javax.persistence.SequenceGenerator;\n+import javax.persistence.Table;\n+import javax.persistence.UniqueConstraint;\n+import javax.validation.constraints.NotNull;\n+\n+@Entity\n+@Table(name = \"sdxDatabaseDrstatus\", uniqueConstraints = @UniqueConstraint(columnNames = {\"operationId\"}))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI2NzI2Mw==", "bodyText": "This patch also has the sql file to create table \"sdxDatabaseDrstatus\". Without having the name here, can I assume that the entity would using the table with the name \"sdxDatabaseDrstatus\"?", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444267263", "createdAt": "2020-06-23T14:27:41Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/SdxDatabaseDrStatus.java", "diffHunk": "@@ -0,0 +1,102 @@\n+package com.sequenceiq.datalake.entity;\n+\n+import java.util.UUID;\n+\n+import javax.persistence.Entity;\n+import javax.persistence.EnumType;\n+import javax.persistence.Enumerated;\n+import javax.persistence.GeneratedValue;\n+import javax.persistence.GenerationType;\n+import javax.persistence.Id;\n+import javax.persistence.SequenceGenerator;\n+import javax.persistence.Table;\n+import javax.persistence.UniqueConstraint;\n+import javax.validation.constraints.NotNull;\n+\n+@Entity\n+@Table(name = \"sdxDatabaseDrstatus\", uniqueConstraints = @UniqueConstraint(columnNames = {\"operationId\"}))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDExNjI1Ng=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDk3NDEzNw==", "bodyText": "will do.", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444974137", "createdAt": "2020-06-24T15:19:00Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/SdxDatabaseDrStatus.java", "diffHunk": "@@ -0,0 +1,102 @@\n+package com.sequenceiq.datalake.entity;\n+\n+import java.util.UUID;\n+\n+import javax.persistence.Entity;\n+import javax.persistence.EnumType;\n+import javax.persistence.Enumerated;\n+import javax.persistence.GeneratedValue;\n+import javax.persistence.GenerationType;\n+import javax.persistence.Id;\n+import javax.persistence.SequenceGenerator;\n+import javax.persistence.Table;\n+import javax.persistence.UniqueConstraint;\n+import javax.validation.constraints.NotNull;\n+\n+@Entity\n+@Table(name = \"sdxDatabaseDrstatus\", uniqueConstraints = @UniqueConstraint(columnNames = {\"operationId\"}))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDExNjI1Ng=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NzI1NzIxOnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/SdxDatabaseDrStatus.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxMDoxODowOVrOGniz2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNzowMTo0OVrOGnyb_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDExNzk3OQ==", "bodyText": "please move them out from the entity", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444117979", "createdAt": "2020-06-23T10:18:09Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/SdxDatabaseDrStatus.java", "diffHunk": "@@ -0,0 +1,102 @@\n+package com.sequenceiq.datalake.entity;\n+\n+import java.util.UUID;\n+\n+import javax.persistence.Entity;\n+import javax.persistence.EnumType;\n+import javax.persistence.Enumerated;\n+import javax.persistence.GeneratedValue;\n+import javax.persistence.GenerationType;\n+import javax.persistence.Id;\n+import javax.persistence.SequenceGenerator;\n+import javax.persistence.Table;\n+import javax.persistence.UniqueConstraint;\n+import javax.validation.constraints.NotNull;\n+\n+@Entity\n+@Table(name = \"sdxDatabaseDrstatus\", uniqueConstraints = @UniqueConstraint(columnNames = {\"operationId\"}))\n+public class SdxDatabaseDrStatus {\n+    public enum SdxDatabaseDrStatusTypeEnum {\n+        BACKUP,\n+        RESTORE\n+    }\n+\n+    public enum Status {\n+        INIT,\n+        TRIGGERRED,\n+        INPROGRESS,\n+        SUCCEEDED,\n+        FAILED\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM3NDAxNA==", "bodyText": "will do.", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444374014", "createdAt": "2020-06-23T17:01:49Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/SdxDatabaseDrStatus.java", "diffHunk": "@@ -0,0 +1,102 @@\n+package com.sequenceiq.datalake.entity;\n+\n+import java.util.UUID;\n+\n+import javax.persistence.Entity;\n+import javax.persistence.EnumType;\n+import javax.persistence.Enumerated;\n+import javax.persistence.GeneratedValue;\n+import javax.persistence.GenerationType;\n+import javax.persistence.Id;\n+import javax.persistence.SequenceGenerator;\n+import javax.persistence.Table;\n+import javax.persistence.UniqueConstraint;\n+import javax.validation.constraints.NotNull;\n+\n+@Entity\n+@Table(name = \"sdxDatabaseDrstatus\", uniqueConstraints = @UniqueConstraint(columnNames = {\"operationId\"}))\n+public class SdxDatabaseDrStatus {\n+    public enum SdxDatabaseDrStatusTypeEnum {\n+        BACKUP,\n+        RESTORE\n+    }\n+\n+    public enum Status {\n+        INIT,\n+        TRIGGERRED,\n+        INPROGRESS,\n+        SUCCEEDED,\n+        FAILED\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDExNzk3OQ=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NzI3MTM0OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/SdxDatabaseDrStatus.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxMDoyMjoyMFrOGni8ww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNzoxMzo0NlrOGny3MQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEyMDI1OQ==", "bodyText": "please use converter", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444120259", "createdAt": "2020-06-23T10:22:20Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/SdxDatabaseDrStatus.java", "diffHunk": "@@ -0,0 +1,102 @@\n+package com.sequenceiq.datalake.entity;\n+\n+import java.util.UUID;\n+\n+import javax.persistence.Entity;\n+import javax.persistence.EnumType;\n+import javax.persistence.Enumerated;\n+import javax.persistence.GeneratedValue;\n+import javax.persistence.GenerationType;\n+import javax.persistence.Id;\n+import javax.persistence.SequenceGenerator;\n+import javax.persistence.Table;\n+import javax.persistence.UniqueConstraint;\n+import javax.validation.constraints.NotNull;\n+\n+@Entity\n+@Table(name = \"sdxDatabaseDrstatus\", uniqueConstraints = @UniqueConstraint(columnNames = {\"operationId\"}))\n+public class SdxDatabaseDrStatus {\n+    public enum SdxDatabaseDrStatusTypeEnum {\n+        BACKUP,\n+        RESTORE\n+    }\n+\n+    public enum Status {\n+        INIT,\n+        TRIGGERRED,\n+        INPROGRESS,\n+        SUCCEEDED,\n+        FAILED\n+    }\n+\n+    @Id\n+    @GeneratedValue(strategy = GenerationType.AUTO, generator = \"sdx_status_generator\")\n+    @SequenceGenerator(name = \"sdx_status_generator\", sequenceName = \"sdxstatus_id_seq\", allocationSize = 1)\n+    private Long id;\n+\n+    @NotNull\n+    @Enumerated(EnumType.STRING)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI2ODg2Ng==", "bodyText": "Can you elaborate?", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444268866", "createdAt": "2020-06-23T14:29:32Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/SdxDatabaseDrStatus.java", "diffHunk": "@@ -0,0 +1,102 @@\n+package com.sequenceiq.datalake.entity;\n+\n+import java.util.UUID;\n+\n+import javax.persistence.Entity;\n+import javax.persistence.EnumType;\n+import javax.persistence.Enumerated;\n+import javax.persistence.GeneratedValue;\n+import javax.persistence.GenerationType;\n+import javax.persistence.Id;\n+import javax.persistence.SequenceGenerator;\n+import javax.persistence.Table;\n+import javax.persistence.UniqueConstraint;\n+import javax.validation.constraints.NotNull;\n+\n+@Entity\n+@Table(name = \"sdxDatabaseDrstatus\", uniqueConstraints = @UniqueConstraint(columnNames = {\"operationId\"}))\n+public class SdxDatabaseDrStatus {\n+    public enum SdxDatabaseDrStatusTypeEnum {\n+        BACKUP,\n+        RESTORE\n+    }\n+\n+    public enum Status {\n+        INIT,\n+        TRIGGERRED,\n+        INPROGRESS,\n+        SUCCEEDED,\n+        FAILED\n+    }\n+\n+    @Id\n+    @GeneratedValue(strategy = GenerationType.AUTO, generator = \"sdx_status_generator\")\n+    @SequenceGenerator(name = \"sdx_status_generator\", sequenceName = \"sdxstatus_id_seq\", allocationSize = 1)\n+    private Long id;\n+\n+    @NotNull\n+    @Enumerated(EnumType.STRING)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEyMDI1OQ=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM4MDk3Nw==", "bodyText": "Figured it out. will do.", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444380977", "createdAt": "2020-06-23T17:13:46Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/SdxDatabaseDrStatus.java", "diffHunk": "@@ -0,0 +1,102 @@\n+package com.sequenceiq.datalake.entity;\n+\n+import java.util.UUID;\n+\n+import javax.persistence.Entity;\n+import javax.persistence.EnumType;\n+import javax.persistence.Enumerated;\n+import javax.persistence.GeneratedValue;\n+import javax.persistence.GenerationType;\n+import javax.persistence.Id;\n+import javax.persistence.SequenceGenerator;\n+import javax.persistence.Table;\n+import javax.persistence.UniqueConstraint;\n+import javax.validation.constraints.NotNull;\n+\n+@Entity\n+@Table(name = \"sdxDatabaseDrstatus\", uniqueConstraints = @UniqueConstraint(columnNames = {\"operationId\"}))\n+public class SdxDatabaseDrStatus {\n+    public enum SdxDatabaseDrStatusTypeEnum {\n+        BACKUP,\n+        RESTORE\n+    }\n+\n+    public enum Status {\n+        INIT,\n+        TRIGGERRED,\n+        INPROGRESS,\n+        SUCCEEDED,\n+        FAILED\n+    }\n+\n+    @Id\n+    @GeneratedValue(strategy = GenerationType.AUTO, generator = \"sdx_status_generator\")\n+    @SequenceGenerator(name = \"sdx_status_generator\", sequenceName = \"sdxstatus_id_seq\", allocationSize = 1)\n+    private Long id;\n+\n+    @NotNull\n+    @Enumerated(EnumType.STRING)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEyMDI1OQ=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NzI3MTY3OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/SdxDatabaseDrStatus.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxMDoyMjoyN1rOGni89Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxMDoyMjoyN1rOGni89Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEyMDMwOQ==", "bodyText": "here too", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444120309", "createdAt": "2020-06-23T10:22:27Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/SdxDatabaseDrStatus.java", "diffHunk": "@@ -0,0 +1,102 @@\n+package com.sequenceiq.datalake.entity;\n+\n+import java.util.UUID;\n+\n+import javax.persistence.Entity;\n+import javax.persistence.EnumType;\n+import javax.persistence.Enumerated;\n+import javax.persistence.GeneratedValue;\n+import javax.persistence.GenerationType;\n+import javax.persistence.Id;\n+import javax.persistence.SequenceGenerator;\n+import javax.persistence.Table;\n+import javax.persistence.UniqueConstraint;\n+import javax.validation.constraints.NotNull;\n+\n+@Entity\n+@Table(name = \"sdxDatabaseDrstatus\", uniqueConstraints = @UniqueConstraint(columnNames = {\"operationId\"}))\n+public class SdxDatabaseDrStatus {\n+    public enum SdxDatabaseDrStatusTypeEnum {\n+        BACKUP,\n+        RESTORE\n+    }\n+\n+    public enum Status {\n+        INIT,\n+        TRIGGERRED,\n+        INPROGRESS,\n+        SUCCEEDED,\n+        FAILED\n+    }\n+\n+    @Id\n+    @GeneratedValue(strategy = GenerationType.AUTO, generator = \"sdx_status_generator\")\n+    @SequenceGenerator(name = \"sdx_status_generator\", sequenceName = \"sdxstatus_id_seq\", allocationSize = 1)\n+    private Long id;\n+\n+    @NotNull\n+    @Enumerated(EnumType.STRING)\n+    private SdxDatabaseDrStatusTypeEnum operationType;\n+\n+    @NotNull\n+    private Long sdxClusterId;\n+\n+    private String operationId;\n+\n+    private String statusReason;\n+\n+    @NotNull\n+    @Enumerated(EnumType.STRING)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 49}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NzMxMjM0OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseBackupActions.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxMDozNToyNVrOGnjXDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNDozMToyMFrOGnsGnw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEyNjk4OQ==", "bodyText": "throws Exception is not necessary. same for all the doExecute", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444126989", "createdAt": "2020-06-23T10:35:25Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseBackupActions.java", "diffHunk": "@@ -0,0 +1,173 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT;\n+\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.statemachine.StateContext;\n+import org.springframework.statemachine.action.Action;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxContext;\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupCouldNotStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupFailedEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupSuccessEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupWaitRequest;\n+import com.sequenceiq.datalake.service.AbstractSdxAction;\n+import com.sequenceiq.datalake.service.sdx.dr.SdxDrService;\n+import com.sequenceiq.flow.core.FlowEvent;\n+import com.sequenceiq.flow.core.FlowParameters;\n+import com.sequenceiq.flow.core.FlowState;\n+\n+@Configuration\n+public class DatalakeDatabaseBackupActions {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(DatalakeDatabaseBackupActions.class);\n+\n+    private static final String OPERATION_ID = \"OPERATION-ID\";\n+\n+    @Inject\n+    private SdxDrService sdxDrService;\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_START_STATE\")\n+    public Action<?, ?> datalakeBackup() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void prepareExecution(DatalakeDatabaseBackupStartEvent payload, Map<Object, Object> variables) {\n+                super.prepareExecution(payload, variables);\n+                variables.put(OPERATION_ID, payload.getDrStatus().getOperationId());\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupStartEvent payload, Map<Object, Object> variables) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI3MDIzOQ==", "bodyText": "will do", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444270239", "createdAt": "2020-06-23T14:31:20Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseBackupActions.java", "diffHunk": "@@ -0,0 +1,173 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT;\n+\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.statemachine.StateContext;\n+import org.springframework.statemachine.action.Action;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxContext;\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupCouldNotStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupFailedEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupSuccessEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupWaitRequest;\n+import com.sequenceiq.datalake.service.AbstractSdxAction;\n+import com.sequenceiq.datalake.service.sdx.dr.SdxDrService;\n+import com.sequenceiq.flow.core.FlowEvent;\n+import com.sequenceiq.flow.core.FlowParameters;\n+import com.sequenceiq.flow.core.FlowState;\n+\n+@Configuration\n+public class DatalakeDatabaseBackupActions {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(DatalakeDatabaseBackupActions.class);\n+\n+    private static final String OPERATION_ID = \"OPERATION-ID\";\n+\n+    @Inject\n+    private SdxDrService sdxDrService;\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_START_STATE\")\n+    public Action<?, ?> datalakeBackup() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void prepareExecution(DatalakeDatabaseBackupStartEvent payload, Map<Object, Object> variables) {\n+                super.prepareExecution(payload, variables);\n+                variables.put(OPERATION_ID, payload.getDrStatus().getOperationId());\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupStartEvent payload, Map<Object, Object> variables) throws Exception {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEyNjk4OQ=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NzMyNTQwOnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/event/DatalakeDatabaseBackupCouldNotStartEvent.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxMDozOToxN1rOGnjfFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNDozNTo0MlrOGnsT5g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEyOTA0NA==", "bodyText": "I won't go through all of your request, but please check them all.\nThe parent class already has defined this with EventSelectorUtil so you should depend on that instead of overriding it with the same", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444129044", "createdAt": "2020-06-23T10:39:17Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/event/DatalakeDatabaseBackupCouldNotStartEvent.java", "diffHunk": "@@ -0,0 +1,33 @@\n+package com.sequenceiq.datalake.flow.dr.event;\n+\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+\n+public class DatalakeDatabaseBackupCouldNotStartEvent extends SdxEvent {\n+    private final Exception exception;\n+\n+    public DatalakeDatabaseBackupCouldNotStartEvent(Long sdxId, String userId, Exception exception) {\n+        super(sdxId, userId);\n+        this.exception = exception;\n+    }\n+\n+    public static DatalakeDatabaseBackupCouldNotStartEvent from(SdxEvent event, Exception exception) {\n+        return new DatalakeDatabaseBackupCouldNotStartEvent(event.getResourceId(), event.getUserId(), exception);\n+    }\n+\n+    @Override\n+    public String selector() {\n+        return \"DatalakeDatabaseBackupCouldNotStartEvent\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI3MzYzOA==", "bodyText": "will do", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444273638", "createdAt": "2020-06-23T14:35:42Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/event/DatalakeDatabaseBackupCouldNotStartEvent.java", "diffHunk": "@@ -0,0 +1,33 @@\n+package com.sequenceiq.datalake.flow.dr.event;\n+\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+\n+public class DatalakeDatabaseBackupCouldNotStartEvent extends SdxEvent {\n+    private final Exception exception;\n+\n+    public DatalakeDatabaseBackupCouldNotStartEvent(Long sdxId, String userId, Exception exception) {\n+        super(sdxId, userId);\n+        this.exception = exception;\n+    }\n+\n+    public static DatalakeDatabaseBackupCouldNotStartEvent from(SdxEvent event, Exception exception) {\n+        return new DatalakeDatabaseBackupCouldNotStartEvent(event.getResourceId(), event.getUserId(), exception);\n+    }\n+\n+    @Override\n+    public String selector() {\n+        return \"DatalakeDatabaseBackupCouldNotStartEvent\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEyOTA0NA=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NzMzMDg5OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseDrEvent.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxMDo0MTowM1rOGnjisQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwODo1MDoxM1rOGoI9jA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEyOTk2OQ==", "bodyText": "please check my comment on one of your event, and use there and here also EventSelectorUtil instead of duplicating string everywhere", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444129969", "createdAt": "2020-06-23T10:41:03Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseDrEvent.java", "diffHunk": "@@ -0,0 +1,32 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import com.sequenceiq.flow.core.FlowEvent;\n+\n+public enum DatalakeDatabaseDrEvent implements FlowEvent {\n+\n+    DATALAKE_DATABASE_BACKUP_EVENT(\"DATALAKE_DATABASE_BACKUP_EVENT\"),\n+    DATALAKE_DATABASE_BACKUP_COULD_NOT_START_EVENT(\"DATALAKE_DATABASE_BACKUP_COULD_NOT_START_EVENT\"),\n+    DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT(\"DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT\"),\n+    DATALAKE_DATABASE_BACKUP_SUCCESS_EVENT(\"DATALAKE_DATABASE_BACKUP_SUCCESS_EVENT\"),\n+    DATALAKE_DATABASE_BACKUP_FAILED_EVENT(\"DATALAKE_DATABASE_BACKUP_FAILED_EVENT\"),\n+    DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT(\"DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT\"),\n+    DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT(\"DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT\"),\n+    DATALAKE_DATABASE_RESTORE_EVENT(\"DATALAKE_DATABASE_RESTORE_EVENT\"),\n+    DATALAKE_DATABASE_RESTORE_COULD_NOT_START_EVENT(\"DATALAKE_DATABASE_RESTORE_COULD_NOT_START_EVENT\"),\n+    DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT(\"DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT\"),\n+    DATALAKE_DATABASE_RESTORE_SUCCESS_EVENT(\"DATALAKE_DATABASE_RESTORE_SUCCESS_EVENT\"),\n+    DATALAKE_DATABASE_RESTORE_FAILED_EVENT(\"DATALAKE_DATABASE_RESTORE_FAILED_EVENT\"),\n+    DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT(\"DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT\"),\n+    DATALAKE_DATABASE_RESTORE_FAILURE_HANDLED_EVENT(\"DATALAKE_DATABASE_RESTORE_FAILURE_HANDLED_EVENT\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDMzNDIyNw==", "bodyText": "I do not have classes defined for all the events. For the events for which classes are defined, i will make changes following your suggestion.", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444334227", "createdAt": "2020-06-23T15:58:45Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseDrEvent.java", "diffHunk": "@@ -0,0 +1,32 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import com.sequenceiq.flow.core.FlowEvent;\n+\n+public enum DatalakeDatabaseDrEvent implements FlowEvent {\n+\n+    DATALAKE_DATABASE_BACKUP_EVENT(\"DATALAKE_DATABASE_BACKUP_EVENT\"),\n+    DATALAKE_DATABASE_BACKUP_COULD_NOT_START_EVENT(\"DATALAKE_DATABASE_BACKUP_COULD_NOT_START_EVENT\"),\n+    DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT(\"DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT\"),\n+    DATALAKE_DATABASE_BACKUP_SUCCESS_EVENT(\"DATALAKE_DATABASE_BACKUP_SUCCESS_EVENT\"),\n+    DATALAKE_DATABASE_BACKUP_FAILED_EVENT(\"DATALAKE_DATABASE_BACKUP_FAILED_EVENT\"),\n+    DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT(\"DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT\"),\n+    DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT(\"DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT\"),\n+    DATALAKE_DATABASE_RESTORE_EVENT(\"DATALAKE_DATABASE_RESTORE_EVENT\"),\n+    DATALAKE_DATABASE_RESTORE_COULD_NOT_START_EVENT(\"DATALAKE_DATABASE_RESTORE_COULD_NOT_START_EVENT\"),\n+    DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT(\"DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT\"),\n+    DATALAKE_DATABASE_RESTORE_SUCCESS_EVENT(\"DATALAKE_DATABASE_RESTORE_SUCCESS_EVENT\"),\n+    DATALAKE_DATABASE_RESTORE_FAILED_EVENT(\"DATALAKE_DATABASE_RESTORE_FAILED_EVENT\"),\n+    DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT(\"DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT\"),\n+    DATALAKE_DATABASE_RESTORE_FAILURE_HANDLED_EVENT(\"DATALAKE_DATABASE_RESTORE_FAILURE_HANDLED_EVENT\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEyOTk2OQ=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDc0MzA1Mg==", "bodyText": "you don't have to create classes for all events. I didn't want to go through all of them to tag which should be replaced. Leaving those untouched which don't have class is fine", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444743052", "createdAt": "2020-06-24T08:50:13Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseDrEvent.java", "diffHunk": "@@ -0,0 +1,32 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import com.sequenceiq.flow.core.FlowEvent;\n+\n+public enum DatalakeDatabaseDrEvent implements FlowEvent {\n+\n+    DATALAKE_DATABASE_BACKUP_EVENT(\"DATALAKE_DATABASE_BACKUP_EVENT\"),\n+    DATALAKE_DATABASE_BACKUP_COULD_NOT_START_EVENT(\"DATALAKE_DATABASE_BACKUP_COULD_NOT_START_EVENT\"),\n+    DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT(\"DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT\"),\n+    DATALAKE_DATABASE_BACKUP_SUCCESS_EVENT(\"DATALAKE_DATABASE_BACKUP_SUCCESS_EVENT\"),\n+    DATALAKE_DATABASE_BACKUP_FAILED_EVENT(\"DATALAKE_DATABASE_BACKUP_FAILED_EVENT\"),\n+    DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT(\"DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT\"),\n+    DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT(\"DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT\"),\n+    DATALAKE_DATABASE_RESTORE_EVENT(\"DATALAKE_DATABASE_RESTORE_EVENT\"),\n+    DATALAKE_DATABASE_RESTORE_COULD_NOT_START_EVENT(\"DATALAKE_DATABASE_RESTORE_COULD_NOT_START_EVENT\"),\n+    DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT(\"DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT\"),\n+    DATALAKE_DATABASE_RESTORE_SUCCESS_EVENT(\"DATALAKE_DATABASE_RESTORE_SUCCESS_EVENT\"),\n+    DATALAKE_DATABASE_RESTORE_FAILED_EVENT(\"DATALAKE_DATABASE_RESTORE_FAILED_EVENT\"),\n+    DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT(\"DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT\"),\n+    DATALAKE_DATABASE_RESTORE_FAILURE_HANDLED_EVENT(\"DATALAKE_DATABASE_RESTORE_FAILURE_HANDLED_EVENT\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEyOTk2OQ=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NzM0MDc4OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/event/DatalakeDatabaseRestoreCouldNotStartEvent.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxMDo0NDoxOVrOGnjpBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxNToyNDo0NVrOGoXUfw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzMTU4OA==", "bodyText": "you should use either plain String with concatenation or StringBuilder.\nalso final is unnecessary", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444131588", "createdAt": "2020-06-23T10:44:19Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/event/DatalakeDatabaseRestoreCouldNotStartEvent.java", "diffHunk": "@@ -0,0 +1,33 @@\n+package com.sequenceiq.datalake.flow.dr.event;\n+\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+\n+public class DatalakeDatabaseRestoreCouldNotStartEvent extends SdxEvent {\n+    private final Exception exception;\n+\n+    public DatalakeDatabaseRestoreCouldNotStartEvent(Long sdxId, String userId, Exception exception) {\n+        super(\"DatalakeDatabaseRestoreCouldNotStartEvent\", sdxId, userId);\n+        this.exception = exception;\n+    }\n+\n+    public static DatalakeDatabaseRestoreCouldNotStartEvent from(SdxEvent event, Exception exception) {\n+        return new DatalakeDatabaseRestoreCouldNotStartEvent(event.getResourceId(), event.getUserId(), exception);\n+    }\n+\n+    @Override\n+    public String selector() {\n+        return \"DatalakeDatabaseRestoreCouldNotStartEvent\";\n+    }\n+\n+    public Exception getException() {\n+        return exception;\n+    }\n+\n+    @Override\n+    public String toString() {\n+        final StringBuffer sb = new StringBuffer(\"DatalakeDatabaseRestoreCouldNotStartEvent{\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDMzMzU5Mg==", "bodyText": "I will remove the final.\nStringBuilder is not thread-safe by StringBuffer is. That is why I used StringBuffer.\nIs there a specific reason to use StringBuilder?", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444333592", "createdAt": "2020-06-23T15:57:53Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/event/DatalakeDatabaseRestoreCouldNotStartEvent.java", "diffHunk": "@@ -0,0 +1,33 @@\n+package com.sequenceiq.datalake.flow.dr.event;\n+\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+\n+public class DatalakeDatabaseRestoreCouldNotStartEvent extends SdxEvent {\n+    private final Exception exception;\n+\n+    public DatalakeDatabaseRestoreCouldNotStartEvent(Long sdxId, String userId, Exception exception) {\n+        super(\"DatalakeDatabaseRestoreCouldNotStartEvent\", sdxId, userId);\n+        this.exception = exception;\n+    }\n+\n+    public static DatalakeDatabaseRestoreCouldNotStartEvent from(SdxEvent event, Exception exception) {\n+        return new DatalakeDatabaseRestoreCouldNotStartEvent(event.getResourceId(), event.getUserId(), exception);\n+    }\n+\n+    @Override\n+    public String selector() {\n+        return \"DatalakeDatabaseRestoreCouldNotStartEvent\";\n+    }\n+\n+    public Exception getException() {\n+        return exception;\n+    }\n+\n+    @Override\n+    public String toString() {\n+        final StringBuffer sb = new StringBuffer(\"DatalakeDatabaseRestoreCouldNotStartEvent{\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzMTU4OA=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDc0MTkwNQ==", "bodyText": "why would you need thread-safe solution here? it has a performance drawback\nalso simple string concatenation would do, I usually use that for toString", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444741905", "createdAt": "2020-06-24T08:48:21Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/event/DatalakeDatabaseRestoreCouldNotStartEvent.java", "diffHunk": "@@ -0,0 +1,33 @@\n+package com.sequenceiq.datalake.flow.dr.event;\n+\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+\n+public class DatalakeDatabaseRestoreCouldNotStartEvent extends SdxEvent {\n+    private final Exception exception;\n+\n+    public DatalakeDatabaseRestoreCouldNotStartEvent(Long sdxId, String userId, Exception exception) {\n+        super(\"DatalakeDatabaseRestoreCouldNotStartEvent\", sdxId, userId);\n+        this.exception = exception;\n+    }\n+\n+    public static DatalakeDatabaseRestoreCouldNotStartEvent from(SdxEvent event, Exception exception) {\n+        return new DatalakeDatabaseRestoreCouldNotStartEvent(event.getResourceId(), event.getUserId(), exception);\n+    }\n+\n+    @Override\n+    public String selector() {\n+        return \"DatalakeDatabaseRestoreCouldNotStartEvent\";\n+    }\n+\n+    public Exception getException() {\n+        return exception;\n+    }\n+\n+    @Override\n+    public String toString() {\n+        final StringBuffer sb = new StringBuffer(\"DatalakeDatabaseRestoreCouldNotStartEvent{\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzMTU4OA=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDk3ODMwMw==", "bodyText": "will do", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444978303", "createdAt": "2020-06-24T15:24:45Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/event/DatalakeDatabaseRestoreCouldNotStartEvent.java", "diffHunk": "@@ -0,0 +1,33 @@\n+package com.sequenceiq.datalake.flow.dr.event;\n+\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+\n+public class DatalakeDatabaseRestoreCouldNotStartEvent extends SdxEvent {\n+    private final Exception exception;\n+\n+    public DatalakeDatabaseRestoreCouldNotStartEvent(Long sdxId, String userId, Exception exception) {\n+        super(\"DatalakeDatabaseRestoreCouldNotStartEvent\", sdxId, userId);\n+        this.exception = exception;\n+    }\n+\n+    public static DatalakeDatabaseRestoreCouldNotStartEvent from(SdxEvent event, Exception exception) {\n+        return new DatalakeDatabaseRestoreCouldNotStartEvent(event.getResourceId(), event.getUserId(), exception);\n+    }\n+\n+    @Override\n+    public String selector() {\n+        return \"DatalakeDatabaseRestoreCouldNotStartEvent\";\n+    }\n+\n+    public Exception getException() {\n+        return exception;\n+    }\n+\n+    @Override\n+    public String toString() {\n+        final StringBuffer sb = new StringBuffer(\"DatalakeDatabaseRestoreCouldNotStartEvent{\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzMTU4OA=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NzM1MjA1OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxMDo0Nzo0NVrOGnjwKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxNTozOToxOVrOGoX81A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzMzQxOQ==", "bodyText": "this class needs some identation fix, idea fixed 8 lines for me", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444133419", "createdAt": "2020-06-23T10:47:45Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzNTQxMw==", "bodyText": "I think this class should be broken into 3\n\nbackup\nrestore\ncommon part\nwhat do you think?", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444135413", "createdAt": "2020-06-23T10:51:38Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzMzQxOQ=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDMwMTk4Mw==", "bodyText": "Fixed the indentation. What is the rationale to split the service into multiple parts?", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444301983", "createdAt": "2020-06-23T15:13:05Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzMzQxOQ=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDczODc3NA==", "bodyText": "ideally one class does one thing, see: single-responsibility principle\nalso it would be easier to read as there would be less code in that class. so if you are looking for backup related stuff, you don't have to go through the restore part", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444738774", "createdAt": "2020-06-24T08:43:04Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzMzQxOQ=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDk4ODYyOA==", "bodyText": "Will create a sperate patch for it.", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444988628", "createdAt": "2020-06-24T15:39:19Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzMzQxOQ=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 50}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NzM1MzMxOnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxMDo0ODowOVrOGnjw-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNToxMzoxM1rOGnuC-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzMzYyNg==", "bodyText": "if service is the name I think the annotation should be service", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444133626", "createdAt": "2020-06-23T10:48:09Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDMwMjA3Mw==", "bodyText": "will update", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444302073", "createdAt": "2020-06-23T15:13:13Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzMzYyNg=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 49}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NzM1NTI0OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxMDo0ODozNVrOGnjyFw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNToxNDo0NFrOGnuHIw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzMzkxMQ==", "bodyText": "if we need this comment then the name of the class is wrong", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444133911", "createdAt": "2020-06-23T10:48:35Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDMwMzEzOQ==", "bodyText": "Will rename the class to SdxDatabaseDrService.", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444303139", "createdAt": "2020-06-23T15:14:44Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzMzkxMQ=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NzM1Njc1OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxMDo0OTowNVrOGnjzAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxMDo0OTowNVrOGnjzAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzNDE0NQ==", "bodyText": "unused", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444134145", "createdAt": "2020-06-23T10:49:05Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NzM1NzExOnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxMDo0OToxMlrOGnjzOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNToxNTowMlrOGnuIDg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzNDIwMg==", "bodyText": "unused", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444134202", "createdAt": "2020-06-23T10:49:12Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDMwMzM3NA==", "bodyText": "will update.", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444303374", "createdAt": "2020-06-23T15:15:02Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzNDIwMg=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 67}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NzM2ODU2OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxMDo1MzowNVrOGnj6ig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNTowMToxMVrOGnth7Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzNjA3NA==", "bodyText": "this condition is a bit hard to read and also almost the same as in getDatabaseRestoreStatus\nit would worth to refactor them into a method", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444136074", "createdAt": "2020-06-23T10:53:05Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    private String triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, backupV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, restoreV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id %d not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void waitCloudbreakFlow(Long id, PollingConfig pollingConfig, String pollingMessage) {\n+        SdxCluster sdxCluster = sdxClusterRepository.findById(id).orElseThrow(notFound(\"SDX cluster\", id));\n+        Polling.waitPeriodly(pollingConfig.getSleepTime(), pollingConfig.getSleepTimeUnit())\n+                .stopIfException(pollingConfig.getStopPollingIfExceptionOccured())\n+                .stopAfterDelay(pollingConfig.getDuration(), pollingConfig.getDurationTimeUnit())\n+                .run(() -> checkDatabaseDrStatus(sdxCluster, pollingMessage));\n+    }\n+\n+    private AttemptResult<StackV4Response> checkDatabaseDrStatus(SdxCluster sdxCluster, String pollingMessage) throws JsonProcessingException {\n+        LOGGER.info(\"{} polling cloudbreak for stack status: '{}' in '{}' env\", pollingMessage, sdxCluster.getClusterName(), sdxCluster.getEnvName());\n+        try {\n+            if (PollGroup.CANCELLED.equals(DatalakeInMemoryStateStore.get(sdxCluster.getId()))) {\n+                LOGGER.info(\"{} polling cancelled in inmemory store, id: {}\", pollingMessage, sdxCluster.getId());\n+                return AttemptResults.breakFor(pollingMessage + \" polling cancelled in inmemory store, id: \" + sdxCluster.getId());\n+            } else {\n+                CloudbreakFlowService.FlowState flowState = cloudbreakFlowService.getLastKnownFlowState(sdxCluster);\n+                if (RUNNING.equals(flowState)) {\n+                    LOGGER.info(\"{} polling will continue, cluster has an active flow in Cloudbreak, id: {}\", pollingMessage, sdxCluster.getId());\n+                    return AttemptResults.justContinue();\n+                } else {\n+                    return getStackResponseAttemptResult(sdxCluster, pollingMessage, flowState);\n+                }\n+            }\n+        } catch (javax.ws.rs.NotFoundException e) {\n+            LOGGER.debug(\"Stack not found on CB side \" + sdxCluster.getClusterName(), e);\n+            return AttemptResults.breakFor(\"Stack not found on CB side \" + sdxCluster.getClusterName());\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> getStackResponseAttemptResult(SdxCluster sdxCluster, String pollingMessage, CloudbreakFlowService.FlowState flowState)\n+            throws JsonProcessingException {\n+        StackV4Response stackV4Response = stackV4Endpoint.get(0L, sdxCluster.getClusterName(), Collections.emptySet());\n+        LOGGER.info(\"Response from cloudbreak: {}\", JsonUtil.writeValueAsString(stackV4Response));\n+        ClusterV4Response cluster = stackV4Response.getCluster();\n+        if (stackAndClusterAvailable(stackV4Response, cluster)) {\n+            return sdxDrSucceeded(stackV4Response);\n+        } else {\n+            if (Status.BACKUP_FAILED.equals(stackV4Response.getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getStatus())) {\n+                LOGGER.info(\"{} failed for Stack {} with status {}\", pollingMessage, stackV4Response.getName(), stackV4Response.getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getStatusReason(), pollingMessage);\n+            } else if (Status.BACKUP_FAILED.equals(stackV4Response.getCluster().getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getCluster().getStatus())) {\n+                LOGGER.info(\"{} failed for Cluster {} status {}\", pollingMessage, stackV4Response.getCluster().getName(),\n+                        stackV4Response.getCluster().getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getCluster().getStatusReason(), pollingMessage);\n+            } else {\n+                if (FINISHED.equals(flowState)) {\n+                    LOGGER.info(\"Flow finished, but Backup/Restore is not complete: {}\", sdxCluster.getClusterName());\n+                    return sdxDrFailed(sdxCluster, \"stack is in improper state\", pollingMessage);\n+                } else {\n+                    return AttemptResults.justContinue();\n+                }\n+            }\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrFailed(SdxCluster sdxCluster, String statusReason, String pollingMessage) {\n+        LOGGER.info(\"{} failed, statusReason: {}\", pollingMessage, statusReason);\n+        return AttemptResults.breakFor(\"SDX \" + pollingMessage + \" failed '\" + sdxCluster.getClusterName() + \"', \" + statusReason);\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrSucceeded(StackV4Response stackV4Response) {\n+        LOGGER.info(\"Database DR operation doe SDX cluster {} is successfull\", stackV4Response.getCluster().getName());\n+        return AttemptResults.finishWith(stackV4Response);\n+    }\n+\n+    private boolean stackAndClusterAvailable(StackV4Response stackV4Response, ClusterV4Response cluster) {\n+        return stackV4Response.getStatus().isAvailable()\n+                && cluster != null\n+                && cluster.getStatus() != null\n+                && cluster.getStatus().isAvailable();\n+    }\n+\n+    /**\n+     * Updates the status of the database backup/restore operation.\n+     * @param operationId Operation Id\n+     * @param status Status of the operation\n+     * @param failedReason Failure reason, if any.\n+     */\n+    public void updateDatabaseStatusEntry(String operationId, SdxDatabaseDrStatus.Status status, String failedReason) {\n+        if (Strings.isNullOrEmpty(operationId)) {\n+            return;\n+        }\n+        SdxDatabaseDrStatus drStatus = sdxDatabaseDrStatusRepository.findSdxDatabaseDrStatusByOperationId(operationId);\n+        drStatus.setStatus(status);\n+        if (!Strings.isNullOrEmpty(failedReason)) {\n+            drStatus.setStatusReason(failedReason);\n+        }\n+        sdxDatabaseDrStatusRepository.save(drStatus);\n+    }\n+\n+    /**\n+     * Gets the status of the database backup operation.\n+     * @param sdxCluster Sdx cluster on which the backup operation is performed.\n+     * @param operationId Operation Id\n+     * @return Backup status\n+     */\n+    public SdxDatabaseBackupStatusResponse getDatabaseBackupStatus(SdxCluster sdxCluster, String operationId) {\n+        SdxDatabaseDrStatus drStatus = sdxDatabaseDrStatusRepository.findSdxDatabaseDrStatusByOperationId(operationId);\n+        if ((drStatus == null) || (!drStatus.getSdxClusterId().equals(sdxCluster.getId()))\n+                || (!drStatus.getOperationType().equals(SdxDatabaseDrStatus.SdxDatabaseDrStatusTypeEnum.BACKUP))) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 234}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI5MzYxMw==", "bodyText": "will do", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444293613", "createdAt": "2020-06-23T15:01:11Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    private String triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, backupV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, restoreV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id %d not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void waitCloudbreakFlow(Long id, PollingConfig pollingConfig, String pollingMessage) {\n+        SdxCluster sdxCluster = sdxClusterRepository.findById(id).orElseThrow(notFound(\"SDX cluster\", id));\n+        Polling.waitPeriodly(pollingConfig.getSleepTime(), pollingConfig.getSleepTimeUnit())\n+                .stopIfException(pollingConfig.getStopPollingIfExceptionOccured())\n+                .stopAfterDelay(pollingConfig.getDuration(), pollingConfig.getDurationTimeUnit())\n+                .run(() -> checkDatabaseDrStatus(sdxCluster, pollingMessage));\n+    }\n+\n+    private AttemptResult<StackV4Response> checkDatabaseDrStatus(SdxCluster sdxCluster, String pollingMessage) throws JsonProcessingException {\n+        LOGGER.info(\"{} polling cloudbreak for stack status: '{}' in '{}' env\", pollingMessage, sdxCluster.getClusterName(), sdxCluster.getEnvName());\n+        try {\n+            if (PollGroup.CANCELLED.equals(DatalakeInMemoryStateStore.get(sdxCluster.getId()))) {\n+                LOGGER.info(\"{} polling cancelled in inmemory store, id: {}\", pollingMessage, sdxCluster.getId());\n+                return AttemptResults.breakFor(pollingMessage + \" polling cancelled in inmemory store, id: \" + sdxCluster.getId());\n+            } else {\n+                CloudbreakFlowService.FlowState flowState = cloudbreakFlowService.getLastKnownFlowState(sdxCluster);\n+                if (RUNNING.equals(flowState)) {\n+                    LOGGER.info(\"{} polling will continue, cluster has an active flow in Cloudbreak, id: {}\", pollingMessage, sdxCluster.getId());\n+                    return AttemptResults.justContinue();\n+                } else {\n+                    return getStackResponseAttemptResult(sdxCluster, pollingMessage, flowState);\n+                }\n+            }\n+        } catch (javax.ws.rs.NotFoundException e) {\n+            LOGGER.debug(\"Stack not found on CB side \" + sdxCluster.getClusterName(), e);\n+            return AttemptResults.breakFor(\"Stack not found on CB side \" + sdxCluster.getClusterName());\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> getStackResponseAttemptResult(SdxCluster sdxCluster, String pollingMessage, CloudbreakFlowService.FlowState flowState)\n+            throws JsonProcessingException {\n+        StackV4Response stackV4Response = stackV4Endpoint.get(0L, sdxCluster.getClusterName(), Collections.emptySet());\n+        LOGGER.info(\"Response from cloudbreak: {}\", JsonUtil.writeValueAsString(stackV4Response));\n+        ClusterV4Response cluster = stackV4Response.getCluster();\n+        if (stackAndClusterAvailable(stackV4Response, cluster)) {\n+            return sdxDrSucceeded(stackV4Response);\n+        } else {\n+            if (Status.BACKUP_FAILED.equals(stackV4Response.getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getStatus())) {\n+                LOGGER.info(\"{} failed for Stack {} with status {}\", pollingMessage, stackV4Response.getName(), stackV4Response.getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getStatusReason(), pollingMessage);\n+            } else if (Status.BACKUP_FAILED.equals(stackV4Response.getCluster().getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getCluster().getStatus())) {\n+                LOGGER.info(\"{} failed for Cluster {} status {}\", pollingMessage, stackV4Response.getCluster().getName(),\n+                        stackV4Response.getCluster().getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getCluster().getStatusReason(), pollingMessage);\n+            } else {\n+                if (FINISHED.equals(flowState)) {\n+                    LOGGER.info(\"Flow finished, but Backup/Restore is not complete: {}\", sdxCluster.getClusterName());\n+                    return sdxDrFailed(sdxCluster, \"stack is in improper state\", pollingMessage);\n+                } else {\n+                    return AttemptResults.justContinue();\n+                }\n+            }\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrFailed(SdxCluster sdxCluster, String statusReason, String pollingMessage) {\n+        LOGGER.info(\"{} failed, statusReason: {}\", pollingMessage, statusReason);\n+        return AttemptResults.breakFor(\"SDX \" + pollingMessage + \" failed '\" + sdxCluster.getClusterName() + \"', \" + statusReason);\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrSucceeded(StackV4Response stackV4Response) {\n+        LOGGER.info(\"Database DR operation doe SDX cluster {} is successfull\", stackV4Response.getCluster().getName());\n+        return AttemptResults.finishWith(stackV4Response);\n+    }\n+\n+    private boolean stackAndClusterAvailable(StackV4Response stackV4Response, ClusterV4Response cluster) {\n+        return stackV4Response.getStatus().isAvailable()\n+                && cluster != null\n+                && cluster.getStatus() != null\n+                && cluster.getStatus().isAvailable();\n+    }\n+\n+    /**\n+     * Updates the status of the database backup/restore operation.\n+     * @param operationId Operation Id\n+     * @param status Status of the operation\n+     * @param failedReason Failure reason, if any.\n+     */\n+    public void updateDatabaseStatusEntry(String operationId, SdxDatabaseDrStatus.Status status, String failedReason) {\n+        if (Strings.isNullOrEmpty(operationId)) {\n+            return;\n+        }\n+        SdxDatabaseDrStatus drStatus = sdxDatabaseDrStatusRepository.findSdxDatabaseDrStatusByOperationId(operationId);\n+        drStatus.setStatus(status);\n+        if (!Strings.isNullOrEmpty(failedReason)) {\n+            drStatus.setStatusReason(failedReason);\n+        }\n+        sdxDatabaseDrStatusRepository.save(drStatus);\n+    }\n+\n+    /**\n+     * Gets the status of the database backup operation.\n+     * @param sdxCluster Sdx cluster on which the backup operation is performed.\n+     * @param operationId Operation Id\n+     * @return Backup status\n+     */\n+    public SdxDatabaseBackupStatusResponse getDatabaseBackupStatus(SdxCluster sdxCluster, String operationId) {\n+        SdxDatabaseDrStatus drStatus = sdxDatabaseDrStatusRepository.findSdxDatabaseDrStatusByOperationId(operationId);\n+        if ((drStatus == null) || (!drStatus.getSdxClusterId().equals(sdxCluster.getId()))\n+                || (!drStatus.getOperationType().equals(SdxDatabaseDrStatus.SdxDatabaseDrStatusTypeEnum.BACKUP))) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzNjA3NA=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 234}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NzM3NzcyOnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxMDo1NTo1M1rOGnj_7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNTo0OToxNFrOGnvmPg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzNzQ1NA==", "bodyText": "this if-else thing got out of hand. please refactor parts of into separate method with meaningful names so it would be easier to follow what happens", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444137454", "createdAt": "2020-06-23T10:55:53Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    private String triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, backupV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, restoreV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id %d not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void waitCloudbreakFlow(Long id, PollingConfig pollingConfig, String pollingMessage) {\n+        SdxCluster sdxCluster = sdxClusterRepository.findById(id).orElseThrow(notFound(\"SDX cluster\", id));\n+        Polling.waitPeriodly(pollingConfig.getSleepTime(), pollingConfig.getSleepTimeUnit())\n+                .stopIfException(pollingConfig.getStopPollingIfExceptionOccured())\n+                .stopAfterDelay(pollingConfig.getDuration(), pollingConfig.getDurationTimeUnit())\n+                .run(() -> checkDatabaseDrStatus(sdxCluster, pollingMessage));\n+    }\n+\n+    private AttemptResult<StackV4Response> checkDatabaseDrStatus(SdxCluster sdxCluster, String pollingMessage) throws JsonProcessingException {\n+        LOGGER.info(\"{} polling cloudbreak for stack status: '{}' in '{}' env\", pollingMessage, sdxCluster.getClusterName(), sdxCluster.getEnvName());\n+        try {\n+            if (PollGroup.CANCELLED.equals(DatalakeInMemoryStateStore.get(sdxCluster.getId()))) {\n+                LOGGER.info(\"{} polling cancelled in inmemory store, id: {}\", pollingMessage, sdxCluster.getId());\n+                return AttemptResults.breakFor(pollingMessage + \" polling cancelled in inmemory store, id: \" + sdxCluster.getId());\n+            } else {\n+                CloudbreakFlowService.FlowState flowState = cloudbreakFlowService.getLastKnownFlowState(sdxCluster);\n+                if (RUNNING.equals(flowState)) {\n+                    LOGGER.info(\"{} polling will continue, cluster has an active flow in Cloudbreak, id: {}\", pollingMessage, sdxCluster.getId());\n+                    return AttemptResults.justContinue();\n+                } else {\n+                    return getStackResponseAttemptResult(sdxCluster, pollingMessage, flowState);\n+                }\n+            }\n+        } catch (javax.ws.rs.NotFoundException e) {\n+            LOGGER.debug(\"Stack not found on CB side \" + sdxCluster.getClusterName(), e);\n+            return AttemptResults.breakFor(\"Stack not found on CB side \" + sdxCluster.getClusterName());\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> getStackResponseAttemptResult(SdxCluster sdxCluster, String pollingMessage, CloudbreakFlowService.FlowState flowState)\n+            throws JsonProcessingException {\n+        StackV4Response stackV4Response = stackV4Endpoint.get(0L, sdxCluster.getClusterName(), Collections.emptySet());\n+        LOGGER.info(\"Response from cloudbreak: {}\", JsonUtil.writeValueAsString(stackV4Response));\n+        ClusterV4Response cluster = stackV4Response.getCluster();\n+        if (stackAndClusterAvailable(stackV4Response, cluster)) {\n+            return sdxDrSucceeded(stackV4Response);\n+        } else {\n+            if (Status.BACKUP_FAILED.equals(stackV4Response.getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getStatus())) {\n+                LOGGER.info(\"{} failed for Stack {} with status {}\", pollingMessage, stackV4Response.getName(), stackV4Response.getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getStatusReason(), pollingMessage);\n+            } else if (Status.BACKUP_FAILED.equals(stackV4Response.getCluster().getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getCluster().getStatus())) {\n+                LOGGER.info(\"{} failed for Cluster {} status {}\", pollingMessage, stackV4Response.getCluster().getName(),\n+                        stackV4Response.getCluster().getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getCluster().getStatusReason(), pollingMessage);\n+            } else {\n+                if (FINISHED.equals(flowState)) {\n+                    LOGGER.info(\"Flow finished, but Backup/Restore is not complete: {}\", sdxCluster.getClusterName());\n+                    return sdxDrFailed(sdxCluster, \"stack is in improper state\", pollingMessage);\n+                } else {\n+                    return AttemptResults.justContinue();\n+                }\n+            }\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 187}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDMyNzQ4Ng==", "bodyText": "will refactor it.", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444327486", "createdAt": "2020-06-23T15:49:14Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    private String triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, backupV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, restoreV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id %d not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void waitCloudbreakFlow(Long id, PollingConfig pollingConfig, String pollingMessage) {\n+        SdxCluster sdxCluster = sdxClusterRepository.findById(id).orElseThrow(notFound(\"SDX cluster\", id));\n+        Polling.waitPeriodly(pollingConfig.getSleepTime(), pollingConfig.getSleepTimeUnit())\n+                .stopIfException(pollingConfig.getStopPollingIfExceptionOccured())\n+                .stopAfterDelay(pollingConfig.getDuration(), pollingConfig.getDurationTimeUnit())\n+                .run(() -> checkDatabaseDrStatus(sdxCluster, pollingMessage));\n+    }\n+\n+    private AttemptResult<StackV4Response> checkDatabaseDrStatus(SdxCluster sdxCluster, String pollingMessage) throws JsonProcessingException {\n+        LOGGER.info(\"{} polling cloudbreak for stack status: '{}' in '{}' env\", pollingMessage, sdxCluster.getClusterName(), sdxCluster.getEnvName());\n+        try {\n+            if (PollGroup.CANCELLED.equals(DatalakeInMemoryStateStore.get(sdxCluster.getId()))) {\n+                LOGGER.info(\"{} polling cancelled in inmemory store, id: {}\", pollingMessage, sdxCluster.getId());\n+                return AttemptResults.breakFor(pollingMessage + \" polling cancelled in inmemory store, id: \" + sdxCluster.getId());\n+            } else {\n+                CloudbreakFlowService.FlowState flowState = cloudbreakFlowService.getLastKnownFlowState(sdxCluster);\n+                if (RUNNING.equals(flowState)) {\n+                    LOGGER.info(\"{} polling will continue, cluster has an active flow in Cloudbreak, id: {}\", pollingMessage, sdxCluster.getId());\n+                    return AttemptResults.justContinue();\n+                } else {\n+                    return getStackResponseAttemptResult(sdxCluster, pollingMessage, flowState);\n+                }\n+            }\n+        } catch (javax.ws.rs.NotFoundException e) {\n+            LOGGER.debug(\"Stack not found on CB side \" + sdxCluster.getClusterName(), e);\n+            return AttemptResults.breakFor(\"Stack not found on CB side \" + sdxCluster.getClusterName());\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> getStackResponseAttemptResult(SdxCluster sdxCluster, String pollingMessage, CloudbreakFlowService.FlowState flowState)\n+            throws JsonProcessingException {\n+        StackV4Response stackV4Response = stackV4Endpoint.get(0L, sdxCluster.getClusterName(), Collections.emptySet());\n+        LOGGER.info(\"Response from cloudbreak: {}\", JsonUtil.writeValueAsString(stackV4Response));\n+        ClusterV4Response cluster = stackV4Response.getCluster();\n+        if (stackAndClusterAvailable(stackV4Response, cluster)) {\n+            return sdxDrSucceeded(stackV4Response);\n+        } else {\n+            if (Status.BACKUP_FAILED.equals(stackV4Response.getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getStatus())) {\n+                LOGGER.info(\"{} failed for Stack {} with status {}\", pollingMessage, stackV4Response.getName(), stackV4Response.getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getStatusReason(), pollingMessage);\n+            } else if (Status.BACKUP_FAILED.equals(stackV4Response.getCluster().getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getCluster().getStatus())) {\n+                LOGGER.info(\"{} failed for Cluster {} status {}\", pollingMessage, stackV4Response.getCluster().getName(),\n+                        stackV4Response.getCluster().getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getCluster().getStatusReason(), pollingMessage);\n+            } else {\n+                if (FINISHED.equals(flowState)) {\n+                    LOGGER.info(\"Flow finished, but Backup/Restore is not complete: {}\", sdxCluster.getClusterName());\n+                    return sdxDrFailed(sdxCluster, \"stack is in improper state\", pollingMessage);\n+                } else {\n+                    return AttemptResults.justContinue();\n+                }\n+            }\n+        }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzNzQ1NA=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 187}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NzM4MDY3OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxMDo1Njo1NVrOGnkBwA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNTo1MjoxMFrOGnvuRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzNzkyMA==", "bodyText": "you might want to use WebApplicationExceptionMessageExtractor here", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444137920", "createdAt": "2020-06-23T10:56:55Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    private String triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, backupV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, restoreV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id %d not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 130}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDMyOTU0Mw==", "bodyText": "Will use it", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444329543", "createdAt": "2020-06-23T15:52:10Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    private String triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, backupV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, restoreV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id %d not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzNzkyMA=="}, "originalCommit": {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511"}, "originalPosition": 130}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3OTg0MDU1OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/com/sequenceiq/cloudbreak/job/StackStatusCheckerJob.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMTo1ODowNFrOGpeMfw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxNToxNzo0OFrOGpkzQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEzOTUxOQ==", "bodyText": "why would you skip check in case of finished state?\nalso it looks like is changed in the other PR too", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446139519", "createdAt": "2020-06-26T11:58:04Z", "author": {"login": "lacikaaa"}, "path": "core/src/main/java/com/sequenceiq/cloudbreak/job/StackStatusCheckerJob.java", "diffHunk": "@@ -142,8 +142,10 @@ protected void executeInternal(JobExecutionContext context) throws JobExecutionE\n                 Status.EXTERNAL_DATABASE_CREATION_IN_PROGRESS,\n                 Status.BACKUP_IN_PROGRESS,\n                 Status.BACKUP_FAILED,\n+                Status.BACKUP_FINISHED,\n                 Status.RESTORE_IN_PROGRESS,\n-                Status.RESTORE_FAILED\n+                Status.RESTORE_FAILED,\n+                Status.RESTORE_FINISHED", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI0Nzc0Ng==", "bodyText": "will move the finished states to synchable states", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446247746", "createdAt": "2020-06-26T15:17:48Z", "author": {"login": "kkalvagadda1"}, "path": "core/src/main/java/com/sequenceiq/cloudbreak/job/StackStatusCheckerJob.java", "diffHunk": "@@ -142,8 +142,10 @@ protected void executeInternal(JobExecutionContext context) throws JobExecutionE\n                 Status.EXTERNAL_DATABASE_CREATION_IN_PROGRESS,\n                 Status.BACKUP_IN_PROGRESS,\n                 Status.BACKUP_FAILED,\n+                Status.BACKUP_FINISHED,\n                 Status.RESTORE_IN_PROGRESS,\n-                Status.RESTORE_FAILED\n+                Status.RESTORE_FAILED,\n+                Status.RESTORE_FINISHED", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEzOTUxOQ=="}, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3OTkxODk3OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/SdxDatabaseDrStatus.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMjoyNTo0NFrOGpe8wA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxNDo0ODozN1rOGpjvvQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE1MTg3Mg==", "bodyText": "after having another glimpse at your pr, I'm confused why would you need this altogether.\nI mean we have flowid, with that you should be able to create a status on the fly, as this is also generated from the state of the flow.\nSo I think you should drop all the operationid stuff, return with the flowid, and the query endpoint should expect flowid instead of operationid. (right now your response also have flowid in it)\nI know in freeipa we have this, but back then we didn't have this for flow, and also usersync doesn't run in flow. But if we would like to go this way, then we should move from returning flowid on all endpoint and make operation general, but I'm not sure this would worth it right now", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446151872", "createdAt": "2020-06-26T12:25:44Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/SdxDatabaseDrStatus.java", "diffHunk": "@@ -0,0 +1,92 @@\n+package com.sequenceiq.datalake.entity;\n+\n+import java.util.UUID;\n+\n+import javax.persistence.Convert;\n+import javax.persistence.Entity;\n+import javax.persistence.GeneratedValue;\n+import javax.persistence.GenerationType;\n+import javax.persistence.Id;\n+import javax.persistence.SequenceGenerator;\n+import javax.persistence.Table;\n+import javax.persistence.UniqueConstraint;\n+import javax.validation.constraints.NotNull;\n+\n+import com.sequenceiq.datalake.converter.SdxDatabaseDrOperationTypeEnumConverter;\n+import com.sequenceiq.datalake.converter.SdxDatabaseDrStatusTypeEnumConverter;\n+\n+@Entity\n+@Table(uniqueConstraints = @UniqueConstraint(columnNames = {\"operationId\"}))\n+public class SdxDatabaseDrStatus {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIzMDQ2MQ==", "bodyText": "As per our conversation, I will generalize the classes and move them to a new package,", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446230461", "createdAt": "2020-06-26T14:48:37Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/SdxDatabaseDrStatus.java", "diffHunk": "@@ -0,0 +1,92 @@\n+package com.sequenceiq.datalake.entity;\n+\n+import java.util.UUID;\n+\n+import javax.persistence.Convert;\n+import javax.persistence.Entity;\n+import javax.persistence.GeneratedValue;\n+import javax.persistence.GenerationType;\n+import javax.persistence.Id;\n+import javax.persistence.SequenceGenerator;\n+import javax.persistence.Table;\n+import javax.persistence.UniqueConstraint;\n+import javax.validation.constraints.NotNull;\n+\n+import com.sequenceiq.datalake.converter.SdxDatabaseDrOperationTypeEnumConverter;\n+import com.sequenceiq.datalake.converter.SdxDatabaseDrStatusTypeEnumConverter;\n+\n+@Entity\n+@Table(uniqueConstraints = @UniqueConstraint(columnNames = {\"operationId\"}))\n+public class SdxDatabaseDrStatus {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE1MTg3Mg=="}, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3OTk1MjQyOnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDatabaseDrService.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMjozNjo1OVrOGpfRCw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxNDo0ODo1N1rOGpjwgA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE1NzA2Nw==", "bodyText": "before the return line you should build the MDCContext using MdcBuilder", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446157067", "createdAt": "2020-06-26T12:36:59Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDatabaseDrService.java", "diffHunk": "@@ -0,0 +1,309 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_EVENT;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Service;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.auth.ThreadBasedUserCrnProvider;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.exception.WebApplicationExceptionMessageExtractor;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrOperationTypeEnum;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatusTypeEnum;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreStartEvent;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.flow.api.model.FlowIdentifier;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Service\n+public class SdxDatabaseDrService {\n+\n+    private static final int MAX_SIZE_OF_FAILURE_REASON = 254;\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDatabaseDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private WebApplicationExceptionMessageExtractor webApplicationExceptionMessageExtractor;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        return triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        return triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIzMDY1Ng==", "bodyText": "will do", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446230656", "createdAt": "2020-06-26T14:48:57Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDatabaseDrService.java", "diffHunk": "@@ -0,0 +1,309 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_EVENT;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Service;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.auth.ThreadBasedUserCrnProvider;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.exception.WebApplicationExceptionMessageExtractor;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrOperationTypeEnum;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatusTypeEnum;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreStartEvent;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.flow.api.model.FlowIdentifier;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Service\n+public class SdxDatabaseDrService {\n+\n+    private static final int MAX_SIZE_OF_FAILURE_REASON = 254;\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDatabaseDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private WebApplicationExceptionMessageExtractor webApplicationExceptionMessageExtractor;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        return triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        return triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE1NzA2Nw=="}, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 88}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3OTk5MTQ0OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseDrState.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMjo0OToxNFrOGpfovw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxNTowODo1OFrOGpkfBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2MzEzNQ==", "bodyText": "as backup and restore 2 separate flow, it should have 2 separate state and event enum", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446163135", "createdAt": "2020-06-26T12:49:14Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseDrState.java", "diffHunk": "@@ -0,0 +1,35 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import com.sequenceiq.flow.core.FlowState;\n+import com.sequenceiq.flow.core.RestartAction;\n+import com.sequenceiq.flow.core.restart.DefaultRestartAction;\n+\n+public enum DatalakeDatabaseDrState implements FlowState {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI0MjU2Nw==", "bodyText": "will do.", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446242567", "createdAt": "2020-06-26T15:08:58Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseDrState.java", "diffHunk": "@@ -0,0 +1,35 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import com.sequenceiq.flow.core.FlowState;\n+import com.sequenceiq.flow.core.RestartAction;\n+import com.sequenceiq.flow.core.restart.DefaultRestartAction;\n+\n+public enum DatalakeDatabaseDrState implements FlowState {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2MzEzNQ=="}, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4MDAwMzA2OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/event/DatalakeDatabaseBackupStartEvent.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMjo1MzowN1rOGpfwAg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxNToxMDoyM1rOGpkiOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2NDk5NA==", "bodyText": "could be final", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446164994", "createdAt": "2020-06-26T12:53:07Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/event/DatalakeDatabaseBackupStartEvent.java", "diffHunk": "@@ -0,0 +1,25 @@\n+package com.sequenceiq.datalake.flow.dr.event;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrOperationTypeEnum;\n+\n+public class DatalakeDatabaseBackupStartEvent extends DatalakeDatabaseDrStartBaseEvent {\n+\n+    private String backupId;\n+\n+    private String backupLocation;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI0MzM4NA==", "bodyText": "will fix", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446243384", "createdAt": "2020-06-26T15:10:23Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/event/DatalakeDatabaseBackupStartEvent.java", "diffHunk": "@@ -0,0 +1,25 @@\n+package com.sequenceiq.datalake.flow.dr.event;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrOperationTypeEnum;\n+\n+public class DatalakeDatabaseBackupStartEvent extends DatalakeDatabaseDrStartBaseEvent {\n+\n+    private String backupId;\n+\n+    private String backupLocation;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2NDk5NA=="}, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4MDAwMzgwOnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/event/DatalakeDatabaseBackupSuccessEvent.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMjo1MzoyMVrOGpfwfg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxNToxMDozMlrOGpkiig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2NTExOA==", "bodyText": "could be final", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446165118", "createdAt": "2020-06-26T12:53:21Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/event/DatalakeDatabaseBackupSuccessEvent.java", "diffHunk": "@@ -0,0 +1,17 @@\n+package com.sequenceiq.datalake.flow.dr.event;\n+\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+\n+public class DatalakeDatabaseBackupSuccessEvent extends SdxEvent {\n+    private String operationId;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI0MzQ2Ng==", "bodyText": "will fix", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446243466", "createdAt": "2020-06-26T15:10:32Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/event/DatalakeDatabaseBackupSuccessEvent.java", "diffHunk": "@@ -0,0 +1,17 @@\n+package com.sequenceiq.datalake.flow.dr.event;\n+\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+\n+public class DatalakeDatabaseBackupSuccessEvent extends SdxEvent {\n+    private String operationId;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2NTExOA=="}, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4MDAwNTY3OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/event/DatalakeDatabaseDrStartBaseEvent.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMjo1Mzo1OFrOGpfxkw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxNToxMDozNlrOGpkisg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2NTM5NQ==", "bodyText": "coudl be final", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446165395", "createdAt": "2020-06-26T12:53:58Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/event/DatalakeDatabaseDrStartBaseEvent.java", "diffHunk": "@@ -0,0 +1,19 @@\n+package com.sequenceiq.datalake.flow.dr.event;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrOperationTypeEnum;\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+\n+public class DatalakeDatabaseDrStartBaseEvent extends SdxEvent  {\n+    private SdxDatabaseDrStatus drStatus;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI0MzUwNg==", "bodyText": "will fix", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446243506", "createdAt": "2020-06-26T15:10:36Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/event/DatalakeDatabaseDrStartBaseEvent.java", "diffHunk": "@@ -0,0 +1,19 @@\n+package com.sequenceiq.datalake.flow.dr.event;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrOperationTypeEnum;\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+\n+public class DatalakeDatabaseDrStartBaseEvent extends SdxEvent  {\n+    private SdxDatabaseDrStatus drStatus;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2NTM5NQ=="}, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4MDAwNjk0OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/event/DatalakeDatabaseRestoreStartEvent.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMjo1NDoyM1rOGpfyYA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxNToxMDo0MVrOGpki4w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2NTYwMA==", "bodyText": "final", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446165600", "createdAt": "2020-06-26T12:54:23Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/event/DatalakeDatabaseRestoreStartEvent.java", "diffHunk": "@@ -0,0 +1,25 @@\n+package com.sequenceiq.datalake.flow.dr.event;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrOperationTypeEnum;\n+\n+public class DatalakeDatabaseRestoreStartEvent extends DatalakeDatabaseDrStartBaseEvent {\n+    private String backupId;\n+\n+    private String backupLocation;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI0MzU1NQ==", "bodyText": "will fix", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446243555", "createdAt": "2020-06-26T15:10:41Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/event/DatalakeDatabaseRestoreStartEvent.java", "diffHunk": "@@ -0,0 +1,25 @@\n+package com.sequenceiq.datalake.flow.dr.event;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrOperationTypeEnum;\n+\n+public class DatalakeDatabaseRestoreStartEvent extends DatalakeDatabaseDrStartBaseEvent {\n+    private String backupId;\n+\n+    private String backupLocation;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2NTYwMA=="}, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4MDAwOTAwOnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/handler/DatalakeDatabaseBackupWaitHandler.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMjo1NTowMlrOGpfzwA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxNTo0MDoyNFrOGplk0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2NTk1Mg==", "bodyText": "should be configurable", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446165952", "createdAt": "2020-06-26T12:55:02Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/handler/DatalakeDatabaseBackupWaitHandler.java", "diffHunk": "@@ -0,0 +1,69 @@\n+package com.sequenceiq.datalake.flow.dr.handler;\n+\n+import java.util.concurrent.TimeUnit;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.exception.PollerException;\n+import com.dyngr.exception.PollerStoppedException;\n+import com.dyngr.exception.UserBreakException;\n+import com.sequenceiq.cloudbreak.common.event.Selectable;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupFailedEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupSuccessEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupWaitRequest;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.dr.SdxDatabaseDrService;\n+import com.sequenceiq.flow.event.EventSelectorUtil;\n+import com.sequenceiq.flow.reactor.api.handler.ExceptionCatcherEventHandler;\n+\n+@Component\n+public class DatalakeDatabaseBackupWaitHandler extends ExceptionCatcherEventHandler<DatalakeDatabaseBackupWaitRequest> {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(DatalakeDatabaseBackupWaitHandler.class);\n+\n+    private static final int SLEEP_TIME_IN_SEC = 20;\n+\n+    private static final int DURATION_IN_MINUTES = 90;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI0NDg5NA==", "bodyText": "I did find a place where these values are configurable. Can you provide a reference?", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446244894", "createdAt": "2020-06-26T15:12:51Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/handler/DatalakeDatabaseBackupWaitHandler.java", "diffHunk": "@@ -0,0 +1,69 @@\n+package com.sequenceiq.datalake.flow.dr.handler;\n+\n+import java.util.concurrent.TimeUnit;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.exception.PollerException;\n+import com.dyngr.exception.PollerStoppedException;\n+import com.dyngr.exception.UserBreakException;\n+import com.sequenceiq.cloudbreak.common.event.Selectable;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupFailedEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupSuccessEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupWaitRequest;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.dr.SdxDatabaseDrService;\n+import com.sequenceiq.flow.event.EventSelectorUtil;\n+import com.sequenceiq.flow.reactor.api.handler.ExceptionCatcherEventHandler;\n+\n+@Component\n+public class DatalakeDatabaseBackupWaitHandler extends ExceptionCatcherEventHandler<DatalakeDatabaseBackupWaitRequest> {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(DatalakeDatabaseBackupWaitHandler.class);\n+\n+    private static final int SLEEP_TIME_IN_SEC = 20;\n+\n+    private static final int DURATION_IN_MINUTES = 90;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2NTk1Mg=="}, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI2MDQzMg==", "bodyText": "look for @Value anywhere in the code", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446260432", "createdAt": "2020-06-26T15:40:24Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/handler/DatalakeDatabaseBackupWaitHandler.java", "diffHunk": "@@ -0,0 +1,69 @@\n+package com.sequenceiq.datalake.flow.dr.handler;\n+\n+import java.util.concurrent.TimeUnit;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.exception.PollerException;\n+import com.dyngr.exception.PollerStoppedException;\n+import com.dyngr.exception.UserBreakException;\n+import com.sequenceiq.cloudbreak.common.event.Selectable;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupFailedEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupSuccessEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupWaitRequest;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.dr.SdxDatabaseDrService;\n+import com.sequenceiq.flow.event.EventSelectorUtil;\n+import com.sequenceiq.flow.reactor.api.handler.ExceptionCatcherEventHandler;\n+\n+@Component\n+public class DatalakeDatabaseBackupWaitHandler extends ExceptionCatcherEventHandler<DatalakeDatabaseBackupWaitRequest> {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(DatalakeDatabaseBackupWaitHandler.class);\n+\n+    private static final int SLEEP_TIME_IN_SEC = 20;\n+\n+    private static final int DURATION_IN_MINUTES = 90;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2NTk1Mg=="}, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4MDAxNzU4OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/handler/DatalakeDatabaseRestoreWaitHandler.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMjo1Nzo0MVrOGpf5OA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxNToxNDowOFrOGpkq8g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2NzM1Mg==", "bodyText": "the 2 wait handler is almost the same, might be worth to refactor them into one", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446167352", "createdAt": "2020-06-26T12:57:41Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/handler/DatalakeDatabaseRestoreWaitHandler.java", "diffHunk": "@@ -0,0 +1,69 @@\n+package com.sequenceiq.datalake.flow.dr.handler;\n+\n+import java.util.concurrent.TimeUnit;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.exception.PollerException;\n+import com.dyngr.exception.PollerStoppedException;\n+import com.dyngr.exception.UserBreakException;\n+import com.sequenceiq.cloudbreak.common.event.Selectable;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreFailedEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreSuccessEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreWaitRequest;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.dr.SdxDatabaseDrService;\n+import com.sequenceiq.flow.event.EventSelectorUtil;\n+import com.sequenceiq.flow.reactor.api.handler.ExceptionCatcherEventHandler;\n+\n+@Component\n+public class DatalakeDatabaseRestoreWaitHandler extends ExceptionCatcherEventHandler<DatalakeDatabaseRestoreWaitRequest> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI0NTYxOA==", "bodyText": "They send different events on success and failure scenarios. I think it will be clean to have them separate.", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446245618", "createdAt": "2020-06-26T15:14:08Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/handler/DatalakeDatabaseRestoreWaitHandler.java", "diffHunk": "@@ -0,0 +1,69 @@\n+package com.sequenceiq.datalake.flow.dr.handler;\n+\n+import java.util.concurrent.TimeUnit;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.exception.PollerException;\n+import com.dyngr.exception.PollerStoppedException;\n+import com.dyngr.exception.UserBreakException;\n+import com.sequenceiq.cloudbreak.common.event.Selectable;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreFailedEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreSuccessEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreWaitRequest;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.dr.SdxDatabaseDrService;\n+import com.sequenceiq.flow.event.EventSelectorUtil;\n+import com.sequenceiq.flow.reactor.api.handler.ExceptionCatcherEventHandler;\n+\n+@Component\n+public class DatalakeDatabaseRestoreWaitHandler extends ExceptionCatcherEventHandler<DatalakeDatabaseRestoreWaitRequest> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2NzM1Mg=="}, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4MDAzMTQ5OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDatabaseDrService.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMzowMTo1OVrOGpgCEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxNToxNToxOVrOGpktog==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2OTYxOQ==", "bodyText": "return Status.BACKUP_FINISHED.equals(status) ||\n                Status.RESTORE_FINISHED.equals(status);", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446169619", "createdAt": "2020-06-26T13:01:59Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDatabaseDrService.java", "diffHunk": "@@ -0,0 +1,309 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_EVENT;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Service;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.auth.ThreadBasedUserCrnProvider;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.exception.WebApplicationExceptionMessageExtractor;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrOperationTypeEnum;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatusTypeEnum;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreStartEvent;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.flow.api.model.FlowIdentifier;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Service\n+public class SdxDatabaseDrService {\n+\n+    private static final int MAX_SIZE_OF_FAILURE_REASON = 254;\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDatabaseDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private WebApplicationExceptionMessageExtractor webApplicationExceptionMessageExtractor;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        return triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        return triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }\n+\n+    private SdxDatabaseBackupResponse triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        String selector = DATALAKE_DATABASE_BACKUP_EVENT.event();\n+        String userId = ThreadBasedUserCrnProvider.getUserCrn();\n+        DatalakeDatabaseBackupStartEvent startEvent = new DatalakeDatabaseBackupStartEvent(selector, clusterId, userId, backupId, backupLocation);\n+        FlowIdentifier flowIdentifier = sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(startEvent);\n+        return new SdxDatabaseBackupResponse(startEvent.getDrStatus().getOperationId(), flowIdentifier);\n+    }\n+\n+    private SdxDatabaseRestoreResponse triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        String selector = DATALAKE_DATABASE_RESTORE_EVENT.event();\n+        String userId = ThreadBasedUserCrnProvider.getUserCrn();\n+        DatalakeDatabaseRestoreStartEvent startEvent = new DatalakeDatabaseRestoreStartEvent(selector, clusterId, userId, backupId, backupLocation);\n+        FlowIdentifier flowIdentifier = sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(startEvent);\n+        return new SdxDatabaseRestoreResponse(startEvent.getDrStatus().getOperationId(), flowIdentifier);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                updateSuccessStatus(drStatus.getOperationId(), sdxCluster, backupV4Response.getFlowIdentifier(),\n+                        SdxDatabaseDrStatusTypeEnum.TRIGGERRED);\n+            }, () -> {\n+                updateFailureStatus(drStatus.getOperationId(), clusterId, String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+            });\n+        } catch (WebApplicationException e) {\n+            String errorMessage = webApplicationExceptionMessageExtractor.getErrorMessage(e);\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, errorMessage);\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                updateSuccessStatus(drStatus.getOperationId(), sdxCluster, restoreV4Response.getFlowIdentifier(),\n+                        SdxDatabaseDrStatusTypeEnum.TRIGGERRED);\n+            }, () -> {\n+                updateFailureStatus(drStatus.getOperationId(), clusterId, String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+            });\n+        } catch (WebApplicationException e) {\n+            String errorMessage = webApplicationExceptionMessageExtractor.getErrorMessage(e);\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, errorMessage);\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void waitCloudbreakFlow(Long id, PollingConfig pollingConfig, String pollingMessage) {\n+        SdxCluster sdxCluster = sdxClusterRepository.findById(id).orElseThrow(notFound(\"SDX cluster\", id));\n+        Polling.waitPeriodly(pollingConfig.getSleepTime(), pollingConfig.getSleepTimeUnit())\n+                .stopIfException(pollingConfig.getStopPollingIfExceptionOccured())\n+                .stopAfterDelay(pollingConfig.getDuration(), pollingConfig.getDurationTimeUnit())\n+                .run(() -> checkDatabaseDrStatus(sdxCluster, pollingMessage));\n+    }\n+\n+    private AttemptResult<StackV4Response> checkDatabaseDrStatus(SdxCluster sdxCluster, String pollingMessage) throws JsonProcessingException {\n+        LOGGER.info(\"{} polling cloudbreak for stack status: '{}' in '{}' env\", pollingMessage, sdxCluster.getClusterName(), sdxCluster.getEnvName());\n+        try {\n+            if (PollGroup.CANCELLED.equals(DatalakeInMemoryStateStore.get(sdxCluster.getId()))) {\n+                LOGGER.info(\"{} polling cancelled in in-memory store, id: {}\", pollingMessage, sdxCluster.getId());\n+                return AttemptResults.breakFor(pollingMessage + \" polling cancelled in inmemory store, id: \" + sdxCluster.getId());\n+            } else {\n+                CloudbreakFlowService.FlowState flowState = cloudbreakFlowService.getLastKnownFlowState(sdxCluster);\n+                if (RUNNING.equals(flowState)) {\n+                    LOGGER.info(\"{} polling will continue, cluster has an active flow in Cloudbreak, id: {}\", pollingMessage, sdxCluster.getId());\n+                    return AttemptResults.justContinue();\n+                } else {\n+                    return getStackResponseAttemptResult(sdxCluster, pollingMessage, flowState);\n+                }\n+            }\n+        } catch (javax.ws.rs.NotFoundException e) {\n+            LOGGER.debug(\"Stack not found on CB side \" + sdxCluster.getClusterName(), e);\n+            return AttemptResults.breakFor(\"Stack not found on CB side \" + sdxCluster.getClusterName());\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> getStackResponseAttemptResult(SdxCluster sdxCluster, String pollingMessage, CloudbreakFlowService.FlowState flowState)\n+            throws JsonProcessingException {\n+        StackV4Response stackV4Response = stackV4Endpoint.get(0L, sdxCluster.getClusterName(), Collections.emptySet());\n+        LOGGER.info(\"Response from cloudbreak: {}\", JsonUtil.writeValueAsString(stackV4Response));\n+        ClusterV4Response cluster = stackV4Response.getCluster();\n+        if (isStackOrClusterDrStatusComplete(stackV4Response.getStatus()) && isStackOrClusterDrStatusComplete(cluster.getStatus())) {\n+            return sdxDrSucceeded(stackV4Response);\n+        } else if (isStackOrClusterStatusFailed(stackV4Response.getStatus())) {\n+            LOGGER.info(\"{} failed for Stack {} with status {}\", pollingMessage, stackV4Response.getName(), stackV4Response.getStatus());\n+            return sdxDrFailed(sdxCluster, stackV4Response.getStatusReason(), pollingMessage);\n+        } else if (isStackOrClusterStatusFailed(stackV4Response.getCluster().getStatus())) {\n+            LOGGER.info(\"{} failed for Cluster {} status {}\", pollingMessage, stackV4Response.getCluster().getName(),\n+                    stackV4Response.getCluster().getStatus());\n+            return sdxDrFailed(sdxCluster, stackV4Response.getCluster().getStatusReason(), pollingMessage);\n+        } else if (FINISHED.equals(flowState)) {\n+            LOGGER.info(\"Flow finished, but Backup/Restore is not complete: {}\", sdxCluster.getClusterName());\n+            return sdxDrFailed(sdxCluster, \"stack is in improper state\", pollingMessage);\n+        } else {\n+            LOGGER.info(\"Flow is unknown state\");\n+            return sdxDrFailed(sdxCluster, \"Flow is unknown state\", pollingMessage);\n+        }\n+    }\n+\n+    /**\n+     * Checks the status if the backup/restore operation failed.\n+     * @param status It could be stack/cluster status\n+     * @return true if status is failed, false otherwise.\n+     */\n+    private boolean isStackOrClusterStatusFailed(Status status) {\n+        return (Status.BACKUP_FAILED.equals(status) ||\n+                Status.RESTORE_FAILED.equals(status)) ? true : false;\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrFailed(SdxCluster sdxCluster, String statusReason, String pollingMessage) {\n+        LOGGER.info(\"{} failed, statusReason: {}\", pollingMessage, statusReason);\n+        return AttemptResults.breakFor(\"SDX \" + pollingMessage + \" failed '\" + sdxCluster.getClusterName() + \"', \" + statusReason);\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrSucceeded(StackV4Response stackV4Response) {\n+        LOGGER.info(\"Database DR operation does SDX cluster {} is successful\", stackV4Response.getCluster().getName());\n+        return AttemptResults.finishWith(stackV4Response);\n+    }\n+\n+    /**\n+     * Checks the status if the backup/restore operation is successful.\n+     * @param status It could be stack/cluster status\n+     * @return true if status is successful, false otherwise.\n+     */\n+    private boolean isStackOrClusterDrStatusComplete(Status status) {\n+        return (Status.BACKUP_FINISHED.equals(status) ||\n+                Status.RESTORE_FINISHED.equals(status)) ? true : false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 221}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI0NjMwNg==", "bodyText": "will do", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446246306", "createdAt": "2020-06-26T15:15:19Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDatabaseDrService.java", "diffHunk": "@@ -0,0 +1,309 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_EVENT;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Service;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.auth.ThreadBasedUserCrnProvider;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.exception.WebApplicationExceptionMessageExtractor;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrOperationTypeEnum;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatusTypeEnum;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreStartEvent;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.flow.api.model.FlowIdentifier;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Service\n+public class SdxDatabaseDrService {\n+\n+    private static final int MAX_SIZE_OF_FAILURE_REASON = 254;\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDatabaseDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private WebApplicationExceptionMessageExtractor webApplicationExceptionMessageExtractor;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        return triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        return triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }\n+\n+    private SdxDatabaseBackupResponse triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        String selector = DATALAKE_DATABASE_BACKUP_EVENT.event();\n+        String userId = ThreadBasedUserCrnProvider.getUserCrn();\n+        DatalakeDatabaseBackupStartEvent startEvent = new DatalakeDatabaseBackupStartEvent(selector, clusterId, userId, backupId, backupLocation);\n+        FlowIdentifier flowIdentifier = sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(startEvent);\n+        return new SdxDatabaseBackupResponse(startEvent.getDrStatus().getOperationId(), flowIdentifier);\n+    }\n+\n+    private SdxDatabaseRestoreResponse triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        String selector = DATALAKE_DATABASE_RESTORE_EVENT.event();\n+        String userId = ThreadBasedUserCrnProvider.getUserCrn();\n+        DatalakeDatabaseRestoreStartEvent startEvent = new DatalakeDatabaseRestoreStartEvent(selector, clusterId, userId, backupId, backupLocation);\n+        FlowIdentifier flowIdentifier = sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(startEvent);\n+        return new SdxDatabaseRestoreResponse(startEvent.getDrStatus().getOperationId(), flowIdentifier);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                updateSuccessStatus(drStatus.getOperationId(), sdxCluster, backupV4Response.getFlowIdentifier(),\n+                        SdxDatabaseDrStatusTypeEnum.TRIGGERRED);\n+            }, () -> {\n+                updateFailureStatus(drStatus.getOperationId(), clusterId, String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+            });\n+        } catch (WebApplicationException e) {\n+            String errorMessage = webApplicationExceptionMessageExtractor.getErrorMessage(e);\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, errorMessage);\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                updateSuccessStatus(drStatus.getOperationId(), sdxCluster, restoreV4Response.getFlowIdentifier(),\n+                        SdxDatabaseDrStatusTypeEnum.TRIGGERRED);\n+            }, () -> {\n+                updateFailureStatus(drStatus.getOperationId(), clusterId, String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+            });\n+        } catch (WebApplicationException e) {\n+            String errorMessage = webApplicationExceptionMessageExtractor.getErrorMessage(e);\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, errorMessage);\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void waitCloudbreakFlow(Long id, PollingConfig pollingConfig, String pollingMessage) {\n+        SdxCluster sdxCluster = sdxClusterRepository.findById(id).orElseThrow(notFound(\"SDX cluster\", id));\n+        Polling.waitPeriodly(pollingConfig.getSleepTime(), pollingConfig.getSleepTimeUnit())\n+                .stopIfException(pollingConfig.getStopPollingIfExceptionOccured())\n+                .stopAfterDelay(pollingConfig.getDuration(), pollingConfig.getDurationTimeUnit())\n+                .run(() -> checkDatabaseDrStatus(sdxCluster, pollingMessage));\n+    }\n+\n+    private AttemptResult<StackV4Response> checkDatabaseDrStatus(SdxCluster sdxCluster, String pollingMessage) throws JsonProcessingException {\n+        LOGGER.info(\"{} polling cloudbreak for stack status: '{}' in '{}' env\", pollingMessage, sdxCluster.getClusterName(), sdxCluster.getEnvName());\n+        try {\n+            if (PollGroup.CANCELLED.equals(DatalakeInMemoryStateStore.get(sdxCluster.getId()))) {\n+                LOGGER.info(\"{} polling cancelled in in-memory store, id: {}\", pollingMessage, sdxCluster.getId());\n+                return AttemptResults.breakFor(pollingMessage + \" polling cancelled in inmemory store, id: \" + sdxCluster.getId());\n+            } else {\n+                CloudbreakFlowService.FlowState flowState = cloudbreakFlowService.getLastKnownFlowState(sdxCluster);\n+                if (RUNNING.equals(flowState)) {\n+                    LOGGER.info(\"{} polling will continue, cluster has an active flow in Cloudbreak, id: {}\", pollingMessage, sdxCluster.getId());\n+                    return AttemptResults.justContinue();\n+                } else {\n+                    return getStackResponseAttemptResult(sdxCluster, pollingMessage, flowState);\n+                }\n+            }\n+        } catch (javax.ws.rs.NotFoundException e) {\n+            LOGGER.debug(\"Stack not found on CB side \" + sdxCluster.getClusterName(), e);\n+            return AttemptResults.breakFor(\"Stack not found on CB side \" + sdxCluster.getClusterName());\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> getStackResponseAttemptResult(SdxCluster sdxCluster, String pollingMessage, CloudbreakFlowService.FlowState flowState)\n+            throws JsonProcessingException {\n+        StackV4Response stackV4Response = stackV4Endpoint.get(0L, sdxCluster.getClusterName(), Collections.emptySet());\n+        LOGGER.info(\"Response from cloudbreak: {}\", JsonUtil.writeValueAsString(stackV4Response));\n+        ClusterV4Response cluster = stackV4Response.getCluster();\n+        if (isStackOrClusterDrStatusComplete(stackV4Response.getStatus()) && isStackOrClusterDrStatusComplete(cluster.getStatus())) {\n+            return sdxDrSucceeded(stackV4Response);\n+        } else if (isStackOrClusterStatusFailed(stackV4Response.getStatus())) {\n+            LOGGER.info(\"{} failed for Stack {} with status {}\", pollingMessage, stackV4Response.getName(), stackV4Response.getStatus());\n+            return sdxDrFailed(sdxCluster, stackV4Response.getStatusReason(), pollingMessage);\n+        } else if (isStackOrClusterStatusFailed(stackV4Response.getCluster().getStatus())) {\n+            LOGGER.info(\"{} failed for Cluster {} status {}\", pollingMessage, stackV4Response.getCluster().getName(),\n+                    stackV4Response.getCluster().getStatus());\n+            return sdxDrFailed(sdxCluster, stackV4Response.getCluster().getStatusReason(), pollingMessage);\n+        } else if (FINISHED.equals(flowState)) {\n+            LOGGER.info(\"Flow finished, but Backup/Restore is not complete: {}\", sdxCluster.getClusterName());\n+            return sdxDrFailed(sdxCluster, \"stack is in improper state\", pollingMessage);\n+        } else {\n+            LOGGER.info(\"Flow is unknown state\");\n+            return sdxDrFailed(sdxCluster, \"Flow is unknown state\", pollingMessage);\n+        }\n+    }\n+\n+    /**\n+     * Checks the status if the backup/restore operation failed.\n+     * @param status It could be stack/cluster status\n+     * @return true if status is failed, false otherwise.\n+     */\n+    private boolean isStackOrClusterStatusFailed(Status status) {\n+        return (Status.BACKUP_FAILED.equals(status) ||\n+                Status.RESTORE_FAILED.equals(status)) ? true : false;\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrFailed(SdxCluster sdxCluster, String statusReason, String pollingMessage) {\n+        LOGGER.info(\"{} failed, statusReason: {}\", pollingMessage, statusReason);\n+        return AttemptResults.breakFor(\"SDX \" + pollingMessage + \" failed '\" + sdxCluster.getClusterName() + \"', \" + statusReason);\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrSucceeded(StackV4Response stackV4Response) {\n+        LOGGER.info(\"Database DR operation does SDX cluster {} is successful\", stackV4Response.getCluster().getName());\n+        return AttemptResults.finishWith(stackV4Response);\n+    }\n+\n+    /**\n+     * Checks the status if the backup/restore operation is successful.\n+     * @param status It could be stack/cluster status\n+     * @return true if status is successful, false otherwise.\n+     */\n+    private boolean isStackOrClusterDrStatusComplete(Status status) {\n+        return (Status.BACKUP_FINISHED.equals(status) ||\n+                Status.RESTORE_FINISHED.equals(status)) ? true : false;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2OTYxOQ=="}, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 221}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4MDAzNDA3OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDatabaseDrService.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMzowMjo1N1rOGpgDzg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxNToxNToyNVrOGpkt4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE3MDA2Mg==", "bodyText": "return Status.BACKUP_FAILED.equals(status) ||\n                Status.RESTORE_FAILED.equals(status);", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446170062", "createdAt": "2020-06-26T13:02:57Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDatabaseDrService.java", "diffHunk": "@@ -0,0 +1,309 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_EVENT;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Service;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.auth.ThreadBasedUserCrnProvider;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.exception.WebApplicationExceptionMessageExtractor;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrOperationTypeEnum;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatusTypeEnum;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreStartEvent;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.flow.api.model.FlowIdentifier;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Service\n+public class SdxDatabaseDrService {\n+\n+    private static final int MAX_SIZE_OF_FAILURE_REASON = 254;\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDatabaseDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private WebApplicationExceptionMessageExtractor webApplicationExceptionMessageExtractor;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        return triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        return triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }\n+\n+    private SdxDatabaseBackupResponse triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        String selector = DATALAKE_DATABASE_BACKUP_EVENT.event();\n+        String userId = ThreadBasedUserCrnProvider.getUserCrn();\n+        DatalakeDatabaseBackupStartEvent startEvent = new DatalakeDatabaseBackupStartEvent(selector, clusterId, userId, backupId, backupLocation);\n+        FlowIdentifier flowIdentifier = sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(startEvent);\n+        return new SdxDatabaseBackupResponse(startEvent.getDrStatus().getOperationId(), flowIdentifier);\n+    }\n+\n+    private SdxDatabaseRestoreResponse triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        String selector = DATALAKE_DATABASE_RESTORE_EVENT.event();\n+        String userId = ThreadBasedUserCrnProvider.getUserCrn();\n+        DatalakeDatabaseRestoreStartEvent startEvent = new DatalakeDatabaseRestoreStartEvent(selector, clusterId, userId, backupId, backupLocation);\n+        FlowIdentifier flowIdentifier = sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(startEvent);\n+        return new SdxDatabaseRestoreResponse(startEvent.getDrStatus().getOperationId(), flowIdentifier);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                updateSuccessStatus(drStatus.getOperationId(), sdxCluster, backupV4Response.getFlowIdentifier(),\n+                        SdxDatabaseDrStatusTypeEnum.TRIGGERRED);\n+            }, () -> {\n+                updateFailureStatus(drStatus.getOperationId(), clusterId, String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+            });\n+        } catch (WebApplicationException e) {\n+            String errorMessage = webApplicationExceptionMessageExtractor.getErrorMessage(e);\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, errorMessage);\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                updateSuccessStatus(drStatus.getOperationId(), sdxCluster, restoreV4Response.getFlowIdentifier(),\n+                        SdxDatabaseDrStatusTypeEnum.TRIGGERRED);\n+            }, () -> {\n+                updateFailureStatus(drStatus.getOperationId(), clusterId, String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+            });\n+        } catch (WebApplicationException e) {\n+            String errorMessage = webApplicationExceptionMessageExtractor.getErrorMessage(e);\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, errorMessage);\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void waitCloudbreakFlow(Long id, PollingConfig pollingConfig, String pollingMessage) {\n+        SdxCluster sdxCluster = sdxClusterRepository.findById(id).orElseThrow(notFound(\"SDX cluster\", id));\n+        Polling.waitPeriodly(pollingConfig.getSleepTime(), pollingConfig.getSleepTimeUnit())\n+                .stopIfException(pollingConfig.getStopPollingIfExceptionOccured())\n+                .stopAfterDelay(pollingConfig.getDuration(), pollingConfig.getDurationTimeUnit())\n+                .run(() -> checkDatabaseDrStatus(sdxCluster, pollingMessage));\n+    }\n+\n+    private AttemptResult<StackV4Response> checkDatabaseDrStatus(SdxCluster sdxCluster, String pollingMessage) throws JsonProcessingException {\n+        LOGGER.info(\"{} polling cloudbreak for stack status: '{}' in '{}' env\", pollingMessage, sdxCluster.getClusterName(), sdxCluster.getEnvName());\n+        try {\n+            if (PollGroup.CANCELLED.equals(DatalakeInMemoryStateStore.get(sdxCluster.getId()))) {\n+                LOGGER.info(\"{} polling cancelled in in-memory store, id: {}\", pollingMessage, sdxCluster.getId());\n+                return AttemptResults.breakFor(pollingMessage + \" polling cancelled in inmemory store, id: \" + sdxCluster.getId());\n+            } else {\n+                CloudbreakFlowService.FlowState flowState = cloudbreakFlowService.getLastKnownFlowState(sdxCluster);\n+                if (RUNNING.equals(flowState)) {\n+                    LOGGER.info(\"{} polling will continue, cluster has an active flow in Cloudbreak, id: {}\", pollingMessage, sdxCluster.getId());\n+                    return AttemptResults.justContinue();\n+                } else {\n+                    return getStackResponseAttemptResult(sdxCluster, pollingMessage, flowState);\n+                }\n+            }\n+        } catch (javax.ws.rs.NotFoundException e) {\n+            LOGGER.debug(\"Stack not found on CB side \" + sdxCluster.getClusterName(), e);\n+            return AttemptResults.breakFor(\"Stack not found on CB side \" + sdxCluster.getClusterName());\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> getStackResponseAttemptResult(SdxCluster sdxCluster, String pollingMessage, CloudbreakFlowService.FlowState flowState)\n+            throws JsonProcessingException {\n+        StackV4Response stackV4Response = stackV4Endpoint.get(0L, sdxCluster.getClusterName(), Collections.emptySet());\n+        LOGGER.info(\"Response from cloudbreak: {}\", JsonUtil.writeValueAsString(stackV4Response));\n+        ClusterV4Response cluster = stackV4Response.getCluster();\n+        if (isStackOrClusterDrStatusComplete(stackV4Response.getStatus()) && isStackOrClusterDrStatusComplete(cluster.getStatus())) {\n+            return sdxDrSucceeded(stackV4Response);\n+        } else if (isStackOrClusterStatusFailed(stackV4Response.getStatus())) {\n+            LOGGER.info(\"{} failed for Stack {} with status {}\", pollingMessage, stackV4Response.getName(), stackV4Response.getStatus());\n+            return sdxDrFailed(sdxCluster, stackV4Response.getStatusReason(), pollingMessage);\n+        } else if (isStackOrClusterStatusFailed(stackV4Response.getCluster().getStatus())) {\n+            LOGGER.info(\"{} failed for Cluster {} status {}\", pollingMessage, stackV4Response.getCluster().getName(),\n+                    stackV4Response.getCluster().getStatus());\n+            return sdxDrFailed(sdxCluster, stackV4Response.getCluster().getStatusReason(), pollingMessage);\n+        } else if (FINISHED.equals(flowState)) {\n+            LOGGER.info(\"Flow finished, but Backup/Restore is not complete: {}\", sdxCluster.getClusterName());\n+            return sdxDrFailed(sdxCluster, \"stack is in improper state\", pollingMessage);\n+        } else {\n+            LOGGER.info(\"Flow is unknown state\");\n+            return sdxDrFailed(sdxCluster, \"Flow is unknown state\", pollingMessage);\n+        }\n+    }\n+\n+    /**\n+     * Checks the status if the backup/restore operation failed.\n+     * @param status It could be stack/cluster status\n+     * @return true if status is failed, false otherwise.\n+     */\n+    private boolean isStackOrClusterStatusFailed(Status status) {\n+        return (Status.BACKUP_FAILED.equals(status) ||\n+                Status.RESTORE_FAILED.equals(status)) ? true : false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 201}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI0NjM2OQ==", "bodyText": "will do", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446246369", "createdAt": "2020-06-26T15:15:25Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDatabaseDrService.java", "diffHunk": "@@ -0,0 +1,309 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_EVENT;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Service;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.auth.ThreadBasedUserCrnProvider;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.exception.WebApplicationExceptionMessageExtractor;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrOperationTypeEnum;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatusTypeEnum;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreStartEvent;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.flow.api.model.FlowIdentifier;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Service\n+public class SdxDatabaseDrService {\n+\n+    private static final int MAX_SIZE_OF_FAILURE_REASON = 254;\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDatabaseDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private WebApplicationExceptionMessageExtractor webApplicationExceptionMessageExtractor;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        return triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        return triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }\n+\n+    private SdxDatabaseBackupResponse triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        String selector = DATALAKE_DATABASE_BACKUP_EVENT.event();\n+        String userId = ThreadBasedUserCrnProvider.getUserCrn();\n+        DatalakeDatabaseBackupStartEvent startEvent = new DatalakeDatabaseBackupStartEvent(selector, clusterId, userId, backupId, backupLocation);\n+        FlowIdentifier flowIdentifier = sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(startEvent);\n+        return new SdxDatabaseBackupResponse(startEvent.getDrStatus().getOperationId(), flowIdentifier);\n+    }\n+\n+    private SdxDatabaseRestoreResponse triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        String selector = DATALAKE_DATABASE_RESTORE_EVENT.event();\n+        String userId = ThreadBasedUserCrnProvider.getUserCrn();\n+        DatalakeDatabaseRestoreStartEvent startEvent = new DatalakeDatabaseRestoreStartEvent(selector, clusterId, userId, backupId, backupLocation);\n+        FlowIdentifier flowIdentifier = sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(startEvent);\n+        return new SdxDatabaseRestoreResponse(startEvent.getDrStatus().getOperationId(), flowIdentifier);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                updateSuccessStatus(drStatus.getOperationId(), sdxCluster, backupV4Response.getFlowIdentifier(),\n+                        SdxDatabaseDrStatusTypeEnum.TRIGGERRED);\n+            }, () -> {\n+                updateFailureStatus(drStatus.getOperationId(), clusterId, String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+            });\n+        } catch (WebApplicationException e) {\n+            String errorMessage = webApplicationExceptionMessageExtractor.getErrorMessage(e);\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, errorMessage);\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                updateSuccessStatus(drStatus.getOperationId(), sdxCluster, restoreV4Response.getFlowIdentifier(),\n+                        SdxDatabaseDrStatusTypeEnum.TRIGGERRED);\n+            }, () -> {\n+                updateFailureStatus(drStatus.getOperationId(), clusterId, String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+            });\n+        } catch (WebApplicationException e) {\n+            String errorMessage = webApplicationExceptionMessageExtractor.getErrorMessage(e);\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, errorMessage);\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void waitCloudbreakFlow(Long id, PollingConfig pollingConfig, String pollingMessage) {\n+        SdxCluster sdxCluster = sdxClusterRepository.findById(id).orElseThrow(notFound(\"SDX cluster\", id));\n+        Polling.waitPeriodly(pollingConfig.getSleepTime(), pollingConfig.getSleepTimeUnit())\n+                .stopIfException(pollingConfig.getStopPollingIfExceptionOccured())\n+                .stopAfterDelay(pollingConfig.getDuration(), pollingConfig.getDurationTimeUnit())\n+                .run(() -> checkDatabaseDrStatus(sdxCluster, pollingMessage));\n+    }\n+\n+    private AttemptResult<StackV4Response> checkDatabaseDrStatus(SdxCluster sdxCluster, String pollingMessage) throws JsonProcessingException {\n+        LOGGER.info(\"{} polling cloudbreak for stack status: '{}' in '{}' env\", pollingMessage, sdxCluster.getClusterName(), sdxCluster.getEnvName());\n+        try {\n+            if (PollGroup.CANCELLED.equals(DatalakeInMemoryStateStore.get(sdxCluster.getId()))) {\n+                LOGGER.info(\"{} polling cancelled in in-memory store, id: {}\", pollingMessage, sdxCluster.getId());\n+                return AttemptResults.breakFor(pollingMessage + \" polling cancelled in inmemory store, id: \" + sdxCluster.getId());\n+            } else {\n+                CloudbreakFlowService.FlowState flowState = cloudbreakFlowService.getLastKnownFlowState(sdxCluster);\n+                if (RUNNING.equals(flowState)) {\n+                    LOGGER.info(\"{} polling will continue, cluster has an active flow in Cloudbreak, id: {}\", pollingMessage, sdxCluster.getId());\n+                    return AttemptResults.justContinue();\n+                } else {\n+                    return getStackResponseAttemptResult(sdxCluster, pollingMessage, flowState);\n+                }\n+            }\n+        } catch (javax.ws.rs.NotFoundException e) {\n+            LOGGER.debug(\"Stack not found on CB side \" + sdxCluster.getClusterName(), e);\n+            return AttemptResults.breakFor(\"Stack not found on CB side \" + sdxCluster.getClusterName());\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> getStackResponseAttemptResult(SdxCluster sdxCluster, String pollingMessage, CloudbreakFlowService.FlowState flowState)\n+            throws JsonProcessingException {\n+        StackV4Response stackV4Response = stackV4Endpoint.get(0L, sdxCluster.getClusterName(), Collections.emptySet());\n+        LOGGER.info(\"Response from cloudbreak: {}\", JsonUtil.writeValueAsString(stackV4Response));\n+        ClusterV4Response cluster = stackV4Response.getCluster();\n+        if (isStackOrClusterDrStatusComplete(stackV4Response.getStatus()) && isStackOrClusterDrStatusComplete(cluster.getStatus())) {\n+            return sdxDrSucceeded(stackV4Response);\n+        } else if (isStackOrClusterStatusFailed(stackV4Response.getStatus())) {\n+            LOGGER.info(\"{} failed for Stack {} with status {}\", pollingMessage, stackV4Response.getName(), stackV4Response.getStatus());\n+            return sdxDrFailed(sdxCluster, stackV4Response.getStatusReason(), pollingMessage);\n+        } else if (isStackOrClusterStatusFailed(stackV4Response.getCluster().getStatus())) {\n+            LOGGER.info(\"{} failed for Cluster {} status {}\", pollingMessage, stackV4Response.getCluster().getName(),\n+                    stackV4Response.getCluster().getStatus());\n+            return sdxDrFailed(sdxCluster, stackV4Response.getCluster().getStatusReason(), pollingMessage);\n+        } else if (FINISHED.equals(flowState)) {\n+            LOGGER.info(\"Flow finished, but Backup/Restore is not complete: {}\", sdxCluster.getClusterName());\n+            return sdxDrFailed(sdxCluster, \"stack is in improper state\", pollingMessage);\n+        } else {\n+            LOGGER.info(\"Flow is unknown state\");\n+            return sdxDrFailed(sdxCluster, \"Flow is unknown state\", pollingMessage);\n+        }\n+    }\n+\n+    /**\n+     * Checks the status if the backup/restore operation failed.\n+     * @param status It could be stack/cluster status\n+     * @return true if status is failed, false otherwise.\n+     */\n+    private boolean isStackOrClusterStatusFailed(Status status) {\n+        return (Status.BACKUP_FAILED.equals(status) ||\n+                Status.RESTORE_FAILED.equals(status)) ? true : false;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE3MDA2Mg=="}, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 201}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4MDAzNjE2OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDatabaseDrService.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMzowMzozOVrOGpgFEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxNToxNjowOVrOGpkvpw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE3MDM4Nw==", "bodyText": "drop the comment, method name is self explaining", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446170387", "createdAt": "2020-06-26T13:03:39Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDatabaseDrService.java", "diffHunk": "@@ -0,0 +1,309 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_EVENT;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Service;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.auth.ThreadBasedUserCrnProvider;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.exception.WebApplicationExceptionMessageExtractor;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrOperationTypeEnum;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatusTypeEnum;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreStartEvent;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.flow.api.model.FlowIdentifier;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Service\n+public class SdxDatabaseDrService {\n+\n+    private static final int MAX_SIZE_OF_FAILURE_REASON = 254;\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDatabaseDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private WebApplicationExceptionMessageExtractor webApplicationExceptionMessageExtractor;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        return triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        return triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }\n+\n+    private SdxDatabaseBackupResponse triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        String selector = DATALAKE_DATABASE_BACKUP_EVENT.event();\n+        String userId = ThreadBasedUserCrnProvider.getUserCrn();\n+        DatalakeDatabaseBackupStartEvent startEvent = new DatalakeDatabaseBackupStartEvent(selector, clusterId, userId, backupId, backupLocation);\n+        FlowIdentifier flowIdentifier = sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(startEvent);\n+        return new SdxDatabaseBackupResponse(startEvent.getDrStatus().getOperationId(), flowIdentifier);\n+    }\n+\n+    private SdxDatabaseRestoreResponse triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        String selector = DATALAKE_DATABASE_RESTORE_EVENT.event();\n+        String userId = ThreadBasedUserCrnProvider.getUserCrn();\n+        DatalakeDatabaseRestoreStartEvent startEvent = new DatalakeDatabaseRestoreStartEvent(selector, clusterId, userId, backupId, backupLocation);\n+        FlowIdentifier flowIdentifier = sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(startEvent);\n+        return new SdxDatabaseRestoreResponse(startEvent.getDrStatus().getOperationId(), flowIdentifier);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                updateSuccessStatus(drStatus.getOperationId(), sdxCluster, backupV4Response.getFlowIdentifier(),\n+                        SdxDatabaseDrStatusTypeEnum.TRIGGERRED);\n+            }, () -> {\n+                updateFailureStatus(drStatus.getOperationId(), clusterId, String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+            });\n+        } catch (WebApplicationException e) {\n+            String errorMessage = webApplicationExceptionMessageExtractor.getErrorMessage(e);\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, errorMessage);\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                updateSuccessStatus(drStatus.getOperationId(), sdxCluster, restoreV4Response.getFlowIdentifier(),\n+                        SdxDatabaseDrStatusTypeEnum.TRIGGERRED);\n+            }, () -> {\n+                updateFailureStatus(drStatus.getOperationId(), clusterId, String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+            });\n+        } catch (WebApplicationException e) {\n+            String errorMessage = webApplicationExceptionMessageExtractor.getErrorMessage(e);\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, errorMessage);\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void waitCloudbreakFlow(Long id, PollingConfig pollingConfig, String pollingMessage) {\n+        SdxCluster sdxCluster = sdxClusterRepository.findById(id).orElseThrow(notFound(\"SDX cluster\", id));\n+        Polling.waitPeriodly(pollingConfig.getSleepTime(), pollingConfig.getSleepTimeUnit())\n+                .stopIfException(pollingConfig.getStopPollingIfExceptionOccured())\n+                .stopAfterDelay(pollingConfig.getDuration(), pollingConfig.getDurationTimeUnit())\n+                .run(() -> checkDatabaseDrStatus(sdxCluster, pollingMessage));\n+    }\n+\n+    private AttemptResult<StackV4Response> checkDatabaseDrStatus(SdxCluster sdxCluster, String pollingMessage) throws JsonProcessingException {\n+        LOGGER.info(\"{} polling cloudbreak for stack status: '{}' in '{}' env\", pollingMessage, sdxCluster.getClusterName(), sdxCluster.getEnvName());\n+        try {\n+            if (PollGroup.CANCELLED.equals(DatalakeInMemoryStateStore.get(sdxCluster.getId()))) {\n+                LOGGER.info(\"{} polling cancelled in in-memory store, id: {}\", pollingMessage, sdxCluster.getId());\n+                return AttemptResults.breakFor(pollingMessage + \" polling cancelled in inmemory store, id: \" + sdxCluster.getId());\n+            } else {\n+                CloudbreakFlowService.FlowState flowState = cloudbreakFlowService.getLastKnownFlowState(sdxCluster);\n+                if (RUNNING.equals(flowState)) {\n+                    LOGGER.info(\"{} polling will continue, cluster has an active flow in Cloudbreak, id: {}\", pollingMessage, sdxCluster.getId());\n+                    return AttemptResults.justContinue();\n+                } else {\n+                    return getStackResponseAttemptResult(sdxCluster, pollingMessage, flowState);\n+                }\n+            }\n+        } catch (javax.ws.rs.NotFoundException e) {\n+            LOGGER.debug(\"Stack not found on CB side \" + sdxCluster.getClusterName(), e);\n+            return AttemptResults.breakFor(\"Stack not found on CB side \" + sdxCluster.getClusterName());\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> getStackResponseAttemptResult(SdxCluster sdxCluster, String pollingMessage, CloudbreakFlowService.FlowState flowState)\n+            throws JsonProcessingException {\n+        StackV4Response stackV4Response = stackV4Endpoint.get(0L, sdxCluster.getClusterName(), Collections.emptySet());\n+        LOGGER.info(\"Response from cloudbreak: {}\", JsonUtil.writeValueAsString(stackV4Response));\n+        ClusterV4Response cluster = stackV4Response.getCluster();\n+        if (isStackOrClusterDrStatusComplete(stackV4Response.getStatus()) && isStackOrClusterDrStatusComplete(cluster.getStatus())) {\n+            return sdxDrSucceeded(stackV4Response);\n+        } else if (isStackOrClusterStatusFailed(stackV4Response.getStatus())) {\n+            LOGGER.info(\"{} failed for Stack {} with status {}\", pollingMessage, stackV4Response.getName(), stackV4Response.getStatus());\n+            return sdxDrFailed(sdxCluster, stackV4Response.getStatusReason(), pollingMessage);\n+        } else if (isStackOrClusterStatusFailed(stackV4Response.getCluster().getStatus())) {\n+            LOGGER.info(\"{} failed for Cluster {} status {}\", pollingMessage, stackV4Response.getCluster().getName(),\n+                    stackV4Response.getCluster().getStatus());\n+            return sdxDrFailed(sdxCluster, stackV4Response.getCluster().getStatusReason(), pollingMessage);\n+        } else if (FINISHED.equals(flowState)) {\n+            LOGGER.info(\"Flow finished, but Backup/Restore is not complete: {}\", sdxCluster.getClusterName());\n+            return sdxDrFailed(sdxCluster, \"stack is in improper state\", pollingMessage);\n+        } else {\n+            LOGGER.info(\"Flow is unknown state\");\n+            return sdxDrFailed(sdxCluster, \"Flow is unknown state\", pollingMessage);\n+        }\n+    }\n+\n+    /**\n+     * Checks the status if the backup/restore operation failed.\n+     * @param status It could be stack/cluster status\n+     * @return true if status is failed, false otherwise.\n+     */\n+    private boolean isStackOrClusterStatusFailed(Status status) {\n+        return (Status.BACKUP_FAILED.equals(status) ||\n+                Status.RESTORE_FAILED.equals(status)) ? true : false;\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrFailed(SdxCluster sdxCluster, String statusReason, String pollingMessage) {\n+        LOGGER.info(\"{} failed, statusReason: {}\", pollingMessage, statusReason);\n+        return AttemptResults.breakFor(\"SDX \" + pollingMessage + \" failed '\" + sdxCluster.getClusterName() + \"', \" + statusReason);\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrSucceeded(StackV4Response stackV4Response) {\n+        LOGGER.info(\"Database DR operation does SDX cluster {} is successful\", stackV4Response.getCluster().getName());\n+        return AttemptResults.finishWith(stackV4Response);\n+    }\n+\n+    /**\n+     * Checks the status if the backup/restore operation is successful.\n+     * @param status It could be stack/cluster status\n+     * @return true if status is successful, false otherwise.\n+     */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 218}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI0NjgyMw==", "bodyText": "will do", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446246823", "createdAt": "2020-06-26T15:16:09Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDatabaseDrService.java", "diffHunk": "@@ -0,0 +1,309 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_EVENT;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Service;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.auth.ThreadBasedUserCrnProvider;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.exception.WebApplicationExceptionMessageExtractor;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrOperationTypeEnum;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatusTypeEnum;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreStartEvent;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.flow.api.model.FlowIdentifier;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Service\n+public class SdxDatabaseDrService {\n+\n+    private static final int MAX_SIZE_OF_FAILURE_REASON = 254;\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDatabaseDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private WebApplicationExceptionMessageExtractor webApplicationExceptionMessageExtractor;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        return triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        return triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }\n+\n+    private SdxDatabaseBackupResponse triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        String selector = DATALAKE_DATABASE_BACKUP_EVENT.event();\n+        String userId = ThreadBasedUserCrnProvider.getUserCrn();\n+        DatalakeDatabaseBackupStartEvent startEvent = new DatalakeDatabaseBackupStartEvent(selector, clusterId, userId, backupId, backupLocation);\n+        FlowIdentifier flowIdentifier = sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(startEvent);\n+        return new SdxDatabaseBackupResponse(startEvent.getDrStatus().getOperationId(), flowIdentifier);\n+    }\n+\n+    private SdxDatabaseRestoreResponse triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        String selector = DATALAKE_DATABASE_RESTORE_EVENT.event();\n+        String userId = ThreadBasedUserCrnProvider.getUserCrn();\n+        DatalakeDatabaseRestoreStartEvent startEvent = new DatalakeDatabaseRestoreStartEvent(selector, clusterId, userId, backupId, backupLocation);\n+        FlowIdentifier flowIdentifier = sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(startEvent);\n+        return new SdxDatabaseRestoreResponse(startEvent.getDrStatus().getOperationId(), flowIdentifier);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                updateSuccessStatus(drStatus.getOperationId(), sdxCluster, backupV4Response.getFlowIdentifier(),\n+                        SdxDatabaseDrStatusTypeEnum.TRIGGERRED);\n+            }, () -> {\n+                updateFailureStatus(drStatus.getOperationId(), clusterId, String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+            });\n+        } catch (WebApplicationException e) {\n+            String errorMessage = webApplicationExceptionMessageExtractor.getErrorMessage(e);\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, errorMessage);\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                updateSuccessStatus(drStatus.getOperationId(), sdxCluster, restoreV4Response.getFlowIdentifier(),\n+                        SdxDatabaseDrStatusTypeEnum.TRIGGERRED);\n+            }, () -> {\n+                updateFailureStatus(drStatus.getOperationId(), clusterId, String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+            });\n+        } catch (WebApplicationException e) {\n+            String errorMessage = webApplicationExceptionMessageExtractor.getErrorMessage(e);\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, errorMessage);\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void waitCloudbreakFlow(Long id, PollingConfig pollingConfig, String pollingMessage) {\n+        SdxCluster sdxCluster = sdxClusterRepository.findById(id).orElseThrow(notFound(\"SDX cluster\", id));\n+        Polling.waitPeriodly(pollingConfig.getSleepTime(), pollingConfig.getSleepTimeUnit())\n+                .stopIfException(pollingConfig.getStopPollingIfExceptionOccured())\n+                .stopAfterDelay(pollingConfig.getDuration(), pollingConfig.getDurationTimeUnit())\n+                .run(() -> checkDatabaseDrStatus(sdxCluster, pollingMessage));\n+    }\n+\n+    private AttemptResult<StackV4Response> checkDatabaseDrStatus(SdxCluster sdxCluster, String pollingMessage) throws JsonProcessingException {\n+        LOGGER.info(\"{} polling cloudbreak for stack status: '{}' in '{}' env\", pollingMessage, sdxCluster.getClusterName(), sdxCluster.getEnvName());\n+        try {\n+            if (PollGroup.CANCELLED.equals(DatalakeInMemoryStateStore.get(sdxCluster.getId()))) {\n+                LOGGER.info(\"{} polling cancelled in in-memory store, id: {}\", pollingMessage, sdxCluster.getId());\n+                return AttemptResults.breakFor(pollingMessage + \" polling cancelled in inmemory store, id: \" + sdxCluster.getId());\n+            } else {\n+                CloudbreakFlowService.FlowState flowState = cloudbreakFlowService.getLastKnownFlowState(sdxCluster);\n+                if (RUNNING.equals(flowState)) {\n+                    LOGGER.info(\"{} polling will continue, cluster has an active flow in Cloudbreak, id: {}\", pollingMessage, sdxCluster.getId());\n+                    return AttemptResults.justContinue();\n+                } else {\n+                    return getStackResponseAttemptResult(sdxCluster, pollingMessage, flowState);\n+                }\n+            }\n+        } catch (javax.ws.rs.NotFoundException e) {\n+            LOGGER.debug(\"Stack not found on CB side \" + sdxCluster.getClusterName(), e);\n+            return AttemptResults.breakFor(\"Stack not found on CB side \" + sdxCluster.getClusterName());\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> getStackResponseAttemptResult(SdxCluster sdxCluster, String pollingMessage, CloudbreakFlowService.FlowState flowState)\n+            throws JsonProcessingException {\n+        StackV4Response stackV4Response = stackV4Endpoint.get(0L, sdxCluster.getClusterName(), Collections.emptySet());\n+        LOGGER.info(\"Response from cloudbreak: {}\", JsonUtil.writeValueAsString(stackV4Response));\n+        ClusterV4Response cluster = stackV4Response.getCluster();\n+        if (isStackOrClusterDrStatusComplete(stackV4Response.getStatus()) && isStackOrClusterDrStatusComplete(cluster.getStatus())) {\n+            return sdxDrSucceeded(stackV4Response);\n+        } else if (isStackOrClusterStatusFailed(stackV4Response.getStatus())) {\n+            LOGGER.info(\"{} failed for Stack {} with status {}\", pollingMessage, stackV4Response.getName(), stackV4Response.getStatus());\n+            return sdxDrFailed(sdxCluster, stackV4Response.getStatusReason(), pollingMessage);\n+        } else if (isStackOrClusterStatusFailed(stackV4Response.getCluster().getStatus())) {\n+            LOGGER.info(\"{} failed for Cluster {} status {}\", pollingMessage, stackV4Response.getCluster().getName(),\n+                    stackV4Response.getCluster().getStatus());\n+            return sdxDrFailed(sdxCluster, stackV4Response.getCluster().getStatusReason(), pollingMessage);\n+        } else if (FINISHED.equals(flowState)) {\n+            LOGGER.info(\"Flow finished, but Backup/Restore is not complete: {}\", sdxCluster.getClusterName());\n+            return sdxDrFailed(sdxCluster, \"stack is in improper state\", pollingMessage);\n+        } else {\n+            LOGGER.info(\"Flow is unknown state\");\n+            return sdxDrFailed(sdxCluster, \"Flow is unknown state\", pollingMessage);\n+        }\n+    }\n+\n+    /**\n+     * Checks the status if the backup/restore operation failed.\n+     * @param status It could be stack/cluster status\n+     * @return true if status is failed, false otherwise.\n+     */\n+    private boolean isStackOrClusterStatusFailed(Status status) {\n+        return (Status.BACKUP_FAILED.equals(status) ||\n+                Status.RESTORE_FAILED.equals(status)) ? true : false;\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrFailed(SdxCluster sdxCluster, String statusReason, String pollingMessage) {\n+        LOGGER.info(\"{} failed, statusReason: {}\", pollingMessage, statusReason);\n+        return AttemptResults.breakFor(\"SDX \" + pollingMessage + \" failed '\" + sdxCluster.getClusterName() + \"', \" + statusReason);\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrSucceeded(StackV4Response stackV4Response) {\n+        LOGGER.info(\"Database DR operation does SDX cluster {} is successful\", stackV4Response.getCluster().getName());\n+        return AttemptResults.finishWith(stackV4Response);\n+    }\n+\n+    /**\n+     * Checks the status if the backup/restore operation is successful.\n+     * @param status It could be stack/cluster status\n+     * @return true if status is successful, false otherwise.\n+     */", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE3MDM4Nw=="}, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 218}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4MDAzNjQ3OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDatabaseDrService.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMzowMzo0N1rOGpgFTg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxNToxNjoxN1rOGpkv8Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE3MDQ0Ng==", "bodyText": "drop the comment", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446170446", "createdAt": "2020-06-26T13:03:47Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDatabaseDrService.java", "diffHunk": "@@ -0,0 +1,309 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_EVENT;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Service;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.auth.ThreadBasedUserCrnProvider;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.exception.WebApplicationExceptionMessageExtractor;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrOperationTypeEnum;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatusTypeEnum;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreStartEvent;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.flow.api.model.FlowIdentifier;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Service\n+public class SdxDatabaseDrService {\n+\n+    private static final int MAX_SIZE_OF_FAILURE_REASON = 254;\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDatabaseDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private WebApplicationExceptionMessageExtractor webApplicationExceptionMessageExtractor;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        return triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        return triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }\n+\n+    private SdxDatabaseBackupResponse triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        String selector = DATALAKE_DATABASE_BACKUP_EVENT.event();\n+        String userId = ThreadBasedUserCrnProvider.getUserCrn();\n+        DatalakeDatabaseBackupStartEvent startEvent = new DatalakeDatabaseBackupStartEvent(selector, clusterId, userId, backupId, backupLocation);\n+        FlowIdentifier flowIdentifier = sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(startEvent);\n+        return new SdxDatabaseBackupResponse(startEvent.getDrStatus().getOperationId(), flowIdentifier);\n+    }\n+\n+    private SdxDatabaseRestoreResponse triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        String selector = DATALAKE_DATABASE_RESTORE_EVENT.event();\n+        String userId = ThreadBasedUserCrnProvider.getUserCrn();\n+        DatalakeDatabaseRestoreStartEvent startEvent = new DatalakeDatabaseRestoreStartEvent(selector, clusterId, userId, backupId, backupLocation);\n+        FlowIdentifier flowIdentifier = sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(startEvent);\n+        return new SdxDatabaseRestoreResponse(startEvent.getDrStatus().getOperationId(), flowIdentifier);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                updateSuccessStatus(drStatus.getOperationId(), sdxCluster, backupV4Response.getFlowIdentifier(),\n+                        SdxDatabaseDrStatusTypeEnum.TRIGGERRED);\n+            }, () -> {\n+                updateFailureStatus(drStatus.getOperationId(), clusterId, String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+            });\n+        } catch (WebApplicationException e) {\n+            String errorMessage = webApplicationExceptionMessageExtractor.getErrorMessage(e);\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, errorMessage);\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                updateSuccessStatus(drStatus.getOperationId(), sdxCluster, restoreV4Response.getFlowIdentifier(),\n+                        SdxDatabaseDrStatusTypeEnum.TRIGGERRED);\n+            }, () -> {\n+                updateFailureStatus(drStatus.getOperationId(), clusterId, String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+            });\n+        } catch (WebApplicationException e) {\n+            String errorMessage = webApplicationExceptionMessageExtractor.getErrorMessage(e);\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, errorMessage);\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void waitCloudbreakFlow(Long id, PollingConfig pollingConfig, String pollingMessage) {\n+        SdxCluster sdxCluster = sdxClusterRepository.findById(id).orElseThrow(notFound(\"SDX cluster\", id));\n+        Polling.waitPeriodly(pollingConfig.getSleepTime(), pollingConfig.getSleepTimeUnit())\n+                .stopIfException(pollingConfig.getStopPollingIfExceptionOccured())\n+                .stopAfterDelay(pollingConfig.getDuration(), pollingConfig.getDurationTimeUnit())\n+                .run(() -> checkDatabaseDrStatus(sdxCluster, pollingMessage));\n+    }\n+\n+    private AttemptResult<StackV4Response> checkDatabaseDrStatus(SdxCluster sdxCluster, String pollingMessage) throws JsonProcessingException {\n+        LOGGER.info(\"{} polling cloudbreak for stack status: '{}' in '{}' env\", pollingMessage, sdxCluster.getClusterName(), sdxCluster.getEnvName());\n+        try {\n+            if (PollGroup.CANCELLED.equals(DatalakeInMemoryStateStore.get(sdxCluster.getId()))) {\n+                LOGGER.info(\"{} polling cancelled in in-memory store, id: {}\", pollingMessage, sdxCluster.getId());\n+                return AttemptResults.breakFor(pollingMessage + \" polling cancelled in inmemory store, id: \" + sdxCluster.getId());\n+            } else {\n+                CloudbreakFlowService.FlowState flowState = cloudbreakFlowService.getLastKnownFlowState(sdxCluster);\n+                if (RUNNING.equals(flowState)) {\n+                    LOGGER.info(\"{} polling will continue, cluster has an active flow in Cloudbreak, id: {}\", pollingMessage, sdxCluster.getId());\n+                    return AttemptResults.justContinue();\n+                } else {\n+                    return getStackResponseAttemptResult(sdxCluster, pollingMessage, flowState);\n+                }\n+            }\n+        } catch (javax.ws.rs.NotFoundException e) {\n+            LOGGER.debug(\"Stack not found on CB side \" + sdxCluster.getClusterName(), e);\n+            return AttemptResults.breakFor(\"Stack not found on CB side \" + sdxCluster.getClusterName());\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> getStackResponseAttemptResult(SdxCluster sdxCluster, String pollingMessage, CloudbreakFlowService.FlowState flowState)\n+            throws JsonProcessingException {\n+        StackV4Response stackV4Response = stackV4Endpoint.get(0L, sdxCluster.getClusterName(), Collections.emptySet());\n+        LOGGER.info(\"Response from cloudbreak: {}\", JsonUtil.writeValueAsString(stackV4Response));\n+        ClusterV4Response cluster = stackV4Response.getCluster();\n+        if (isStackOrClusterDrStatusComplete(stackV4Response.getStatus()) && isStackOrClusterDrStatusComplete(cluster.getStatus())) {\n+            return sdxDrSucceeded(stackV4Response);\n+        } else if (isStackOrClusterStatusFailed(stackV4Response.getStatus())) {\n+            LOGGER.info(\"{} failed for Stack {} with status {}\", pollingMessage, stackV4Response.getName(), stackV4Response.getStatus());\n+            return sdxDrFailed(sdxCluster, stackV4Response.getStatusReason(), pollingMessage);\n+        } else if (isStackOrClusterStatusFailed(stackV4Response.getCluster().getStatus())) {\n+            LOGGER.info(\"{} failed for Cluster {} status {}\", pollingMessage, stackV4Response.getCluster().getName(),\n+                    stackV4Response.getCluster().getStatus());\n+            return sdxDrFailed(sdxCluster, stackV4Response.getCluster().getStatusReason(), pollingMessage);\n+        } else if (FINISHED.equals(flowState)) {\n+            LOGGER.info(\"Flow finished, but Backup/Restore is not complete: {}\", sdxCluster.getClusterName());\n+            return sdxDrFailed(sdxCluster, \"stack is in improper state\", pollingMessage);\n+        } else {\n+            LOGGER.info(\"Flow is unknown state\");\n+            return sdxDrFailed(sdxCluster, \"Flow is unknown state\", pollingMessage);\n+        }\n+    }\n+\n+    /**\n+     * Checks the status if the backup/restore operation failed.\n+     * @param status It could be stack/cluster status\n+     * @return true if status is failed, false otherwise.\n+     */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 198}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI0Njg5Nw==", "bodyText": "will do", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446246897", "createdAt": "2020-06-26T15:16:17Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDatabaseDrService.java", "diffHunk": "@@ -0,0 +1,309 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_EVENT;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Service;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.auth.ThreadBasedUserCrnProvider;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.exception.WebApplicationExceptionMessageExtractor;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrOperationTypeEnum;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatusTypeEnum;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreStartEvent;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.flow.api.model.FlowIdentifier;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Service\n+public class SdxDatabaseDrService {\n+\n+    private static final int MAX_SIZE_OF_FAILURE_REASON = 254;\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDatabaseDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private WebApplicationExceptionMessageExtractor webApplicationExceptionMessageExtractor;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        return triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        return triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }\n+\n+    private SdxDatabaseBackupResponse triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        String selector = DATALAKE_DATABASE_BACKUP_EVENT.event();\n+        String userId = ThreadBasedUserCrnProvider.getUserCrn();\n+        DatalakeDatabaseBackupStartEvent startEvent = new DatalakeDatabaseBackupStartEvent(selector, clusterId, userId, backupId, backupLocation);\n+        FlowIdentifier flowIdentifier = sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(startEvent);\n+        return new SdxDatabaseBackupResponse(startEvent.getDrStatus().getOperationId(), flowIdentifier);\n+    }\n+\n+    private SdxDatabaseRestoreResponse triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        String selector = DATALAKE_DATABASE_RESTORE_EVENT.event();\n+        String userId = ThreadBasedUserCrnProvider.getUserCrn();\n+        DatalakeDatabaseRestoreStartEvent startEvent = new DatalakeDatabaseRestoreStartEvent(selector, clusterId, userId, backupId, backupLocation);\n+        FlowIdentifier flowIdentifier = sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(startEvent);\n+        return new SdxDatabaseRestoreResponse(startEvent.getDrStatus().getOperationId(), flowIdentifier);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                updateSuccessStatus(drStatus.getOperationId(), sdxCluster, backupV4Response.getFlowIdentifier(),\n+                        SdxDatabaseDrStatusTypeEnum.TRIGGERRED);\n+            }, () -> {\n+                updateFailureStatus(drStatus.getOperationId(), clusterId, String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+            });\n+        } catch (WebApplicationException e) {\n+            String errorMessage = webApplicationExceptionMessageExtractor.getErrorMessage(e);\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, errorMessage);\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                updateSuccessStatus(drStatus.getOperationId(), sdxCluster, restoreV4Response.getFlowIdentifier(),\n+                        SdxDatabaseDrStatusTypeEnum.TRIGGERRED);\n+            }, () -> {\n+                updateFailureStatus(drStatus.getOperationId(), clusterId, String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+            });\n+        } catch (WebApplicationException e) {\n+            String errorMessage = webApplicationExceptionMessageExtractor.getErrorMessage(e);\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, errorMessage);\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void waitCloudbreakFlow(Long id, PollingConfig pollingConfig, String pollingMessage) {\n+        SdxCluster sdxCluster = sdxClusterRepository.findById(id).orElseThrow(notFound(\"SDX cluster\", id));\n+        Polling.waitPeriodly(pollingConfig.getSleepTime(), pollingConfig.getSleepTimeUnit())\n+                .stopIfException(pollingConfig.getStopPollingIfExceptionOccured())\n+                .stopAfterDelay(pollingConfig.getDuration(), pollingConfig.getDurationTimeUnit())\n+                .run(() -> checkDatabaseDrStatus(sdxCluster, pollingMessage));\n+    }\n+\n+    private AttemptResult<StackV4Response> checkDatabaseDrStatus(SdxCluster sdxCluster, String pollingMessage) throws JsonProcessingException {\n+        LOGGER.info(\"{} polling cloudbreak for stack status: '{}' in '{}' env\", pollingMessage, sdxCluster.getClusterName(), sdxCluster.getEnvName());\n+        try {\n+            if (PollGroup.CANCELLED.equals(DatalakeInMemoryStateStore.get(sdxCluster.getId()))) {\n+                LOGGER.info(\"{} polling cancelled in in-memory store, id: {}\", pollingMessage, sdxCluster.getId());\n+                return AttemptResults.breakFor(pollingMessage + \" polling cancelled in inmemory store, id: \" + sdxCluster.getId());\n+            } else {\n+                CloudbreakFlowService.FlowState flowState = cloudbreakFlowService.getLastKnownFlowState(sdxCluster);\n+                if (RUNNING.equals(flowState)) {\n+                    LOGGER.info(\"{} polling will continue, cluster has an active flow in Cloudbreak, id: {}\", pollingMessage, sdxCluster.getId());\n+                    return AttemptResults.justContinue();\n+                } else {\n+                    return getStackResponseAttemptResult(sdxCluster, pollingMessage, flowState);\n+                }\n+            }\n+        } catch (javax.ws.rs.NotFoundException e) {\n+            LOGGER.debug(\"Stack not found on CB side \" + sdxCluster.getClusterName(), e);\n+            return AttemptResults.breakFor(\"Stack not found on CB side \" + sdxCluster.getClusterName());\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> getStackResponseAttemptResult(SdxCluster sdxCluster, String pollingMessage, CloudbreakFlowService.FlowState flowState)\n+            throws JsonProcessingException {\n+        StackV4Response stackV4Response = stackV4Endpoint.get(0L, sdxCluster.getClusterName(), Collections.emptySet());\n+        LOGGER.info(\"Response from cloudbreak: {}\", JsonUtil.writeValueAsString(stackV4Response));\n+        ClusterV4Response cluster = stackV4Response.getCluster();\n+        if (isStackOrClusterDrStatusComplete(stackV4Response.getStatus()) && isStackOrClusterDrStatusComplete(cluster.getStatus())) {\n+            return sdxDrSucceeded(stackV4Response);\n+        } else if (isStackOrClusterStatusFailed(stackV4Response.getStatus())) {\n+            LOGGER.info(\"{} failed for Stack {} with status {}\", pollingMessage, stackV4Response.getName(), stackV4Response.getStatus());\n+            return sdxDrFailed(sdxCluster, stackV4Response.getStatusReason(), pollingMessage);\n+        } else if (isStackOrClusterStatusFailed(stackV4Response.getCluster().getStatus())) {\n+            LOGGER.info(\"{} failed for Cluster {} status {}\", pollingMessage, stackV4Response.getCluster().getName(),\n+                    stackV4Response.getCluster().getStatus());\n+            return sdxDrFailed(sdxCluster, stackV4Response.getCluster().getStatusReason(), pollingMessage);\n+        } else if (FINISHED.equals(flowState)) {\n+            LOGGER.info(\"Flow finished, but Backup/Restore is not complete: {}\", sdxCluster.getClusterName());\n+            return sdxDrFailed(sdxCluster, \"stack is in improper state\", pollingMessage);\n+        } else {\n+            LOGGER.info(\"Flow is unknown state\");\n+            return sdxDrFailed(sdxCluster, \"Flow is unknown state\", pollingMessage);\n+        }\n+    }\n+\n+    /**\n+     * Checks the status if the backup/restore operation failed.\n+     * @param status It could be stack/cluster status\n+     * @return true if status is failed, false otherwise.\n+     */", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE3MDQ0Ng=="}, "originalCommit": {"oid": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a"}, "originalPosition": 198}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4NTE0MjkxOnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/converter/SdxDatabaseDrStatusTypeEnumConverter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOVQxMToyMjo0N1rOGqMT7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOVQxMzo1ODoyM1rOGqSQbQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njg5NTA4Ng==", "bodyText": "I think this class should be renamed also", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446895086", "createdAt": "2020-06-29T11:22:47Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/converter/SdxDatabaseDrStatusTypeEnumConverter.java", "diffHunk": "@@ -0,0 +1,12 @@\n+package com.sequenceiq.datalake.converter;\n+\n+import com.sequenceiq.cloudbreak.converter.DefaultEnumConverter;\n+import com.sequenceiq.datalake.entity.operation.SdxOperationStatusTypeEnum;\n+\n+public class SdxDatabaseDrStatusTypeEnumConverter extends DefaultEnumConverter<SdxOperationStatusTypeEnum> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4592edaca637c73aebd18e9c87b2e62a1143aa11"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njk5MjQ5Mw==", "bodyText": "ok", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446992493", "createdAt": "2020-06-29T13:58:23Z", "author": {"login": "kkalvagadda1"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/converter/SdxDatabaseDrStatusTypeEnumConverter.java", "diffHunk": "@@ -0,0 +1,12 @@\n+package com.sequenceiq.datalake.converter;\n+\n+import com.sequenceiq.cloudbreak.converter.DefaultEnumConverter;\n+import com.sequenceiq.datalake.entity.operation.SdxOperationStatusTypeEnum;\n+\n+public class SdxDatabaseDrStatusTypeEnumConverter extends DefaultEnumConverter<SdxOperationStatusTypeEnum> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njg5NTA4Ng=="}, "originalCommit": {"oid": "4592edaca637c73aebd18e9c87b2e62a1143aa11"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4NTE1MzMxOnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/operation/SdxOperationStatus.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOVQxMToyNTo1NFrOGqMaHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOVQxMToyNTo1NFrOGqMaHg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njg5NjY3MA==", "bodyText": "I think SdxOperation would fit better, there is too much status around this class", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446896670", "createdAt": "2020-06-29T11:25:54Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/operation/SdxOperationStatus.java", "diffHunk": "@@ -0,0 +1,92 @@\n+package com.sequenceiq.datalake.entity.operation;\n+\n+import java.util.UUID;\n+\n+import javax.persistence.Convert;\n+import javax.persistence.Entity;\n+import javax.persistence.GeneratedValue;\n+import javax.persistence.GenerationType;\n+import javax.persistence.Id;\n+import javax.persistence.SequenceGenerator;\n+import javax.persistence.Table;\n+import javax.persistence.UniqueConstraint;\n+import javax.validation.constraints.NotNull;\n+\n+import com.sequenceiq.datalake.converter.SdxOperationTypeEnumConverter;\n+import com.sequenceiq.datalake.converter.SdxDatabaseDrStatusTypeEnumConverter;\n+\n+@Entity\n+@Table(uniqueConstraints = @UniqueConstraint(columnNames = {\"operationId\"}))\n+public class SdxOperationStatus {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4592edaca637c73aebd18e9c87b2e62a1143aa11"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4NTE1ODg4OnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/operation/SdxOperationStatusTypeEnum.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOVQxMToyNzozMVrOGqMdgw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOVQxMToyNzozMVrOGqMdgw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njg5NzUzOQ==", "bodyText": "rename to SdxOperationStatus", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446897539", "createdAt": "2020-06-29T11:27:31Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/operation/SdxOperationStatusTypeEnum.java", "diffHunk": "@@ -0,0 +1,19 @@\n+package com.sequenceiq.datalake.entity.operation;\n+\n+public enum SdxOperationStatusTypeEnum {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4592edaca637c73aebd18e9c87b2e62a1143aa11"}, "originalPosition": 3}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4NTE2MzIyOnYy", "diffSide": "RIGHT", "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/operation/SdxOperationTypeEnum.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOVQxMToyODo1N1rOGqMgOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOVQxMToyODo1N1rOGqMgOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njg5ODIzNA==", "bodyText": "drop the Enum from the end", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446898234", "createdAt": "2020-06-29T11:28:57Z", "author": {"login": "lacikaaa"}, "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/operation/SdxOperationTypeEnum.java", "diffHunk": "@@ -0,0 +1,17 @@\n+package com.sequenceiq.datalake.entity.operation;\n+\n+public enum SdxOperationTypeEnum {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4592edaca637c73aebd18e9c87b2e62a1143aa11"}, "originalPosition": 3}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4NTE4Mzc4OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/com/sequenceiq/cloudbreak/job/StackStatusCheckerJob.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOVQxMTozNDo0OVrOGqMsKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOVQxNDowNjozM1rOGqSnWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjkwMTI5MQ==", "bodyText": "I don't know which pr will contain the final setting for this, but the 2 FAILED statuses should be here also.\ncc @hreeve-cloudera", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446901291", "createdAt": "2020-06-29T11:34:49Z", "author": {"login": "lacikaaa"}, "path": "core/src/main/java/com/sequenceiq/cloudbreak/job/StackStatusCheckerJob.java", "diffHunk": "@@ -156,7 +156,9 @@ protected void executeInternal(JobExecutionContext context) throws JobExecutionE\n                 Status.START_FAILED,\n                 Status.STOPPED,\n                 Status.STOP_FAILED,\n-                Status.AMBIGUOUS\n+                Status.AMBIGUOUS,\n+                Status.BACKUP_FINISHED,\n+                Status.RESTORE_FINISHED", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4592edaca637c73aebd18e9c87b2e62a1143aa11"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njk5ODM2Mg==", "bodyText": "will take care of it.", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446998362", "createdAt": "2020-06-29T14:06:33Z", "author": {"login": "kkalvagadda1"}, "path": "core/src/main/java/com/sequenceiq/cloudbreak/job/StackStatusCheckerJob.java", "diffHunk": "@@ -156,7 +156,9 @@ protected void executeInternal(JobExecutionContext context) throws JobExecutionE\n                 Status.START_FAILED,\n                 Status.STOPPED,\n                 Status.STOP_FAILED,\n-                Status.AMBIGUOUS\n+                Status.AMBIGUOUS,\n+                Status.BACKUP_FINISHED,\n+                Status.RESTORE_FINISHED", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjkwMTI5MQ=="}, "originalCommit": {"oid": "4592edaca637c73aebd18e9c87b2e62a1143aa11"}, "originalPosition": 7}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3268, "cost": 1, "resetAt": "2021-11-12T20:44:06Z"}}}