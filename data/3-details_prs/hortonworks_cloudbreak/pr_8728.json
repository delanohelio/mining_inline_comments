{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDYzNjM2ODQz", "number": 8728, "title": "CB-8278: Backup using hdfs cli", "bodyText": "Closes CB-8278\n\nRestore db using HDFS cli to transfer backups. Fix styling.\nHack together a fix for embedded database backups.\nAdjust url construction and region passing to account for HDFS cli usage.\nFix style issues.\n\nTested by triggering backup and restore using HTTP requests and against external and embedded database.\nRanger admin group changes were merged in as well after which I retested by triggering via HTTP requests.", "createdAt": "2020-08-05T20:53:02Z", "url": "https://github.com/hortonworks/cloudbreak/pull/8728", "merged": true, "mergeCommit": {"oid": "c6ec70db7d9bcde04b1207e879210b23aa6e8854"}, "closed": true, "closedAt": "2020-08-13T15:56:07Z", "author": {"login": "brycederriso"}, "timelineItems": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc8KeQNAFqTQ2MjI0NDAwMg==", "endCursor": "Y3Vyc29yOnYyOpPPAAABc-SSxfgFqTQ2NjI5MDE3MQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYyMjQ0MDAy", "url": "https://github.com/hortonworks/cloudbreak/pull/8728#pullrequestreview-462244002", "createdAt": "2020-08-06T07:13:38Z", "commit": {"oid": "6e43ad33e768122070f5c6c40dc0950b02664db3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwNzoxMzozOFrOG8mKXw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwNzoxMzozOFrOG8mKXw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjE5Mjk5MQ==", "bodyText": "Why is the region removed?", "url": "https://github.com/hortonworks/cloudbreak/pull/8728#discussion_r466192991", "createdAt": "2020-08-06T07:13:38Z", "author": {"login": "keyki"}, "path": "core/src/main/java/com/sequenceiq/cloudbreak/reactor/handler/cluster/dr/BackupRestoreSaltConfigGenerator.java", "diffHunk": "@@ -34,7 +34,6 @@ public SaltConfig createSaltConfig(String location, String backupId, Stack stack\n \n         Map<String, String> disasterRecoveryValues = new HashMap<>();\n         disasterRecoveryValues.put(OBJECT_STORAGE_URL_KEY, fullLocation);\n-        disasterRecoveryValues.put(AWS_REGION_KEY, stack.getRegion());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6e43ad33e768122070f5c6c40dc0950b02664db3"}, "originalPosition": 13}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYyMzI1OTM0", "url": "https://github.com/hortonworks/cloudbreak/pull/8728#pullrequestreview-462325934", "createdAt": "2020-08-06T09:02:17Z", "commit": {"oid": "6e43ad33e768122070f5c6c40dc0950b02664db3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwOTowMjoxN1rOG8p4iQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwOTowMjoxN1rOG8p4iQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjI1Mzk2MQ==", "bodyText": "we can not really remove pillar property because then the older clusters will not work .", "url": "https://github.com/hortonworks/cloudbreak/pull/8728#discussion_r466253961", "createdAt": "2020-08-06T09:02:17Z", "author": {"login": "doktoric"}, "path": "orchestrator-salt/src/main/resources/salt/pillar/postgresql/disaster_recovery.sls", "diffHunk": "@@ -1,4 +1,2 @@\n disaster_recovery:\n   object_storage_url:\n-  aws_region: '' # empty string to prevent salt from passing the literal 'None'", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6e43ad33e768122070f5c6c40dc0950b02664db3"}, "originalPosition": 3}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYzNTg4MjA0", "url": "https://github.com/hortonworks/cloudbreak/pull/8728#pullrequestreview-463588204", "createdAt": "2020-08-07T20:20:40Z", "commit": {"oid": "f3dbea01996700c4e2361762a9bc8a516e4a232c"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYzNjU4NDMx", "url": "https://github.com/hortonworks/cloudbreak/pull/8728#pullrequestreview-463658431", "createdAt": "2020-08-07T21:50:42Z", "commit": {"oid": "f3dbea01996700c4e2361762a9bc8a516e4a232c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QyMTo1MDo0MlrOG9pfIQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QyMTo1MDo0MlrOG9pfIQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzI5NjAzMw==", "bodyText": "It's not a HDFS location. It is the location of the object-store. Please rename it accordingly.", "url": "https://github.com/hortonworks/cloudbreak/pull/8728#discussion_r467296033", "createdAt": "2020-08-07T21:50:42Z", "author": {"login": "kkalvagadda1"}, "path": "orchestrator-salt/src/main/resources/salt/salt/postgresql/disaster_recovery/scripts/backup_db.sh", "diffHunk": "@@ -54,69 +42,50 @@ errorExit() {\n   exit 1\n }\n \n-dump_to_azure() {\n+kinit_as_hdfs() {\n+  # Use a new Kerberos credential cache, to keep from clobbering the default.\n+  export KRB5CCNAME=/tmp/krb5cc_cloudbreak_$EUID\n+  HDFS_KEYTAB=/run/cloudera-scm-agent/process/$(ls -t /run/cloudera-scm-agent/process/ | grep hdfs-NAMENODE$ | head -n 1)/hdfs.keytab\n+  doLog \"kinit as hdfs using Keytab: $HDFS_KEYTAB\"\n+  kinit -kt \"$HDFS_KEYTAB\" \"hdfs/$(hostname -f)\"\n+  if [[ $? -ne 0 ]]; then\n+    errorExit \"Couldn't kinit as hdfs\"\n+  fi\n+}\n+\n+move_backup_to_cloud () {\n+  LOCAL_BACKUP=\"$1\"\n+  kinit_as_hdfs\n+  doLog \"INFO Uploading to ${BACKUP_LOCATION}\"\n+\n+  hdfs dfs -mkdir -p \"$BACKUP_LOCATION\"\n+  HDFS_FILE_LOCATION=\"${BACKUP_LOCATION}/${SERVICE}_backup\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f3dbea01996700c4e2361762a9bc8a516e4a232c"}, "originalPosition": 77}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYzNjYwNTM4", "url": "https://github.com/hortonworks/cloudbreak/pull/8728#pullrequestreview-463660538", "createdAt": "2020-08-07T21:56:21Z", "commit": {"oid": "f3dbea01996700c4e2361762a9bc8a516e4a232c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QyMTo1NjoyMVrOG9pv0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QyMTo1NjoyMVrOG9pv0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzMwMDMwNw==", "bodyText": "here are were are creating a directory and accessing directly. S3 provides an eventual guarantee.\nIn short, if you create a directory and try to write to it immediately, it might fail.\nwe use S3 guard to handle this issue with S3. S3 guard tracks all the operations and changes the metadata.\nI'm not sure if S3 guard is enabled by default. Is there anything that has to be done for it. Can you check with @risdenk  on this?", "url": "https://github.com/hortonworks/cloudbreak/pull/8728#discussion_r467300307", "createdAt": "2020-08-07T21:56:21Z", "author": {"login": "kkalvagadda1"}, "path": "orchestrator-salt/src/main/resources/salt/salt/postgresql/disaster_recovery/scripts/backup_db.sh", "diffHunk": "@@ -54,69 +42,50 @@ errorExit() {\n   exit 1\n }\n \n-dump_to_azure() {\n+kinit_as_hdfs() {\n+  # Use a new Kerberos credential cache, to keep from clobbering the default.\n+  export KRB5CCNAME=/tmp/krb5cc_cloudbreak_$EUID\n+  HDFS_KEYTAB=/run/cloudera-scm-agent/process/$(ls -t /run/cloudera-scm-agent/process/ | grep hdfs-NAMENODE$ | head -n 1)/hdfs.keytab\n+  doLog \"kinit as hdfs using Keytab: $HDFS_KEYTAB\"\n+  kinit -kt \"$HDFS_KEYTAB\" \"hdfs/$(hostname -f)\"\n+  if [[ $? -ne 0 ]]; then\n+    errorExit \"Couldn't kinit as hdfs\"\n+  fi\n+}\n+\n+move_backup_to_cloud () {\n+  LOCAL_BACKUP=\"$1\"\n+  kinit_as_hdfs\n+  doLog \"INFO Uploading to ${BACKUP_LOCATION}\"\n+\n+  hdfs dfs -mkdir -p \"$BACKUP_LOCATION\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f3dbea01996700c4e2361762a9bc8a516e4a232c"}, "originalPosition": 76}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f3dbea01996700c4e2361762a9bc8a516e4a232c", "author": {"user": {"login": "brycederriso", "name": "Bryce"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/f3dbea01996700c4e2361762a9bc8a516e4a232c", "committedDate": "2020-08-06T20:39:02Z", "message": "Remove unused region pillar value again."}, "afterCommit": {"oid": "c33ebfc599ac204d10ff6b28d621d12691197804", "author": {"user": {"login": "brycederriso", "name": "Bryce"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/c33ebfc599ac204d10ff6b28d621d12691197804", "committedDate": "2020-08-10T20:53:37Z", "message": "Backup and restore using HDFS cli agains object store.\n\n* Restore db using HDFS cli to transfer backups.\n* Hack together a fix for embedded database backups.\n* Adjust url construction and region passing to account for HDFS cli usage.\n* Use a new kerberos credential cache when kiniting.\n* Remove unused `region` pillar value."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c33ebfc599ac204d10ff6b28d621d12691197804", "author": {"user": {"login": "brycederriso", "name": "Bryce"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/c33ebfc599ac204d10ff6b28d621d12691197804", "committedDate": "2020-08-10T20:53:37Z", "message": "Backup and restore using HDFS cli agains object store.\n\n* Restore db using HDFS cli to transfer backups.\n* Hack together a fix for embedded database backups.\n* Adjust url construction and region passing to account for HDFS cli usage.\n* Use a new kerberos credential cache when kiniting.\n* Remove unused `region` pillar value."}, "afterCommit": {"oid": "1bd741d2e5e079087950a42dc2d977b15b3cf04a", "author": {"user": {"login": "brycederriso", "name": "Bryce"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/1bd741d2e5e079087950a42dc2d977b15b3cf04a", "committedDate": "2020-08-11T14:57:02Z", "message": "Backup and restore using HDFS cli agains object store.\n\n* Restore db using HDFS cli to transfer backups.\n* Hack together a fix for embedded database backups.\n* Adjust url construction and region passing to account for HDFS cli usage.\n* Use a new kerberos credential cache when kiniting.\n* Remove unused `region` pillar value."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "1bd741d2e5e079087950a42dc2d977b15b3cf04a", "author": {"user": {"login": "brycederriso", "name": "Bryce"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/1bd741d2e5e079087950a42dc2d977b15b3cf04a", "committedDate": "2020-08-11T14:57:02Z", "message": "Backup and restore using HDFS cli agains object store.\n\n* Restore db using HDFS cli to transfer backups.\n* Hack together a fix for embedded database backups.\n* Adjust url construction and region passing to account for HDFS cli usage.\n* Use a new kerberos credential cache when kiniting.\n* Remove unused `region` pillar value."}, "afterCommit": {"oid": "57f5ee7564cbcfacb75db6210665f0d561e05dd9", "author": {"user": {"login": "brycederriso", "name": "Bryce"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/57f5ee7564cbcfacb75db6210665f0d561e05dd9", "committedDate": "2020-08-11T19:49:14Z", "message": "Backup and restore using HDFS cli agains object store.\n\n* Restore db using HDFS cli to transfer backups.\n* Hack together a fix for embedded database backups.\n* Adjust url construction and region passing to account for HDFS cli usage.\n* Use a new kerberos credential cache when kiniting.\n* Remove unused `region` pillar value.\n* Clean up after successful Ranger admin group substitution."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY2MjcwODcx", "url": "https://github.com/hortonworks/cloudbreak/pull/8728#pullrequestreview-466270871", "createdAt": "2020-08-12T20:56:08Z", "commit": {"oid": "57f5ee7564cbcfacb75db6210665f0d561e05dd9"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMDo1NjowOFrOG_yjUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMDo1NjowOFrOG_yjUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU0MTcxMw==", "bodyText": "URL is not an HDFS URL . You can just say \" We retrieve the SQL files from a valid URL, ...\"", "url": "https://github.com/hortonworks/cloudbreak/pull/8728#discussion_r469541713", "createdAt": "2020-08-12T20:56:08Z", "author": {"login": "kkalvagadda1"}, "path": "orchestrator-salt/src/main/resources/salt/salt/postgresql/disaster_recovery/scripts/restore_db.sh", "diffHunk": "@@ -1,47 +1,37 @@\n #!/bin/bash\n # restore_db.sh\n # This script uses the 'psql' cli to drop hive and ranger databases, create them, then read in a plain SQL file to restore data.\n-# We use the `azcopy copy` and `aws s3 cp` commands to copy backups from Azure or AWS respectively.\n-# When using either cloud provider, the plaintext SQL files are first copied to the local filesystem, then fed into psql.\n+# We retrieve the SQL files from a valid HDFS URL, which should be an `s3a://` or `abfs://` for AWS or Azure clouds respectively.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "57f5ee7564cbcfacb75db6210665f0d561e05dd9"}, "originalPosition": 6}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY2MjczMzMz", "url": "https://github.com/hortonworks/cloudbreak/pull/8728#pullrequestreview-466273333", "createdAt": "2020-08-12T20:59:43Z", "commit": {"oid": "57f5ee7564cbcfacb75db6210665f0d561e05dd9"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMDo1OTo0M1rOG_yqzA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMDo1OTo0M1rOG_yqzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU0MzYyOA==", "bodyText": "URL is not an HDFS URL . You can just say \" .... the SQL file is uploaded to the cloud storage using the hdfs cli.\"", "url": "https://github.com/hortonworks/cloudbreak/pull/8728#discussion_r469543628", "createdAt": "2020-08-12T20:59:43Z", "author": {"login": "kkalvagadda1"}, "path": "orchestrator-salt/src/main/resources/salt/salt/postgresql/disaster_recovery/scripts/backup_db.sh", "diffHunk": "@@ -1,47 +1,36 @@\n #!/bin/bash\n # backup_db.sh\n # This script uses the 'pg_dump' utility to dump the contents of hive and ranger PostgreSQL databases as plain SQL.\n-# After PostgreSQL contents are dumped, the SQL is uploaded to AWS or Azure using their CLI clients, 'aws s3 cp' and 'azcopy copy' respectively.\n-# For the Azure upload, we must write the SQL commands to a local disk before running the `azcopy copy` command.\n-# For AWS, the cli supports piping file contents to stdin, so we do that and avoid writing to a local file.\n+# After PostgreSQL contents are dumped, the SQL file is uploaded to an HDFS URL using the hdfs cli.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "57f5ee7564cbcfacb75db6210665f0d561e05dd9"}, "originalPosition": 7}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY2MjczNjgz", "url": "https://github.com/hortonworks/cloudbreak/pull/8728#pullrequestreview-466273683", "createdAt": "2020-08-12T21:00:14Z", "commit": {"oid": "57f5ee7564cbcfacb75db6210665f0d561e05dd9"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "628aa1910dd13125e40108de5852e23a355f8a5b", "author": {"user": {"login": "brycederriso", "name": "Bryce"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/628aa1910dd13125e40108de5852e23a355f8a5b", "committedDate": "2020-08-12T21:27:32Z", "message": "Backup and restore using HDFS cli agains object store.\n\n* Restore db using HDFS cli to transfer backups.\n* Hack together a fix for embedded database backups.\n* Adjust url construction and region passing to account for HDFS cli usage.\n* Use a new kerberos credential cache when kiniting.\n* Remove unused `region` pillar value.\n* Clean up after successful Ranger admin group substitution."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "0a5214be0435bd8b20df1d6ea3ff998a19da6545", "author": {"user": {"login": "brycederriso", "name": "Bryce"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/0a5214be0435bd8b20df1d6ea3ff998a19da6545", "committedDate": "2020-08-12T21:08:29Z", "message": "Fix header comments for backup and restore salt scripts."}, "afterCommit": {"oid": "628aa1910dd13125e40108de5852e23a355f8a5b", "author": {"user": {"login": "brycederriso", "name": "Bryce"}}, "url": "https://github.com/hortonworks/cloudbreak/commit/628aa1910dd13125e40108de5852e23a355f8a5b", "committedDate": "2020-08-12T21:27:32Z", "message": "Backup and restore using HDFS cli agains object store.\n\n* Restore db using HDFS cli to transfer backups.\n* Hack together a fix for embedded database backups.\n* Adjust url construction and region passing to account for HDFS cli usage.\n* Use a new kerberos credential cache when kiniting.\n* Remove unused `region` pillar value.\n* Clean up after successful Ranger admin group substitution."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY2MjkwMTcx", "url": "https://github.com/hortonworks/cloudbreak/pull/8728#pullrequestreview-466290171", "createdAt": "2020-08-12T21:28:11Z", "commit": {"oid": "0a5214be0435bd8b20df1d6ea3ff998a19da6545"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2559, "cost": 1, "resetAt": "2021-11-01T14:51:55Z"}}}