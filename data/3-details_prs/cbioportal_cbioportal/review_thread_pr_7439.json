{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDA4NTU3NzAz", "number": 7439, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxNToyMTo0MVrOD2vhng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxNTozMjoxMlrOD2v5Bw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4NzI4MzUwOnYy", "diffSide": "RIGHT", "path": "docs/Deployment-Procedure.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxNToyMTo0MVrOGMo6jA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxNToyMTo0MVrOGMo6jA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTkwNjQ0NA==", "bodyText": "add space between pipelines,log", "url": "https://github.com/cBioPortal/cbioportal/pull/7439#discussion_r415906444", "createdAt": "2020-04-27T15:21:41Z", "author": {"login": "sheridancbio"}, "path": "docs/Deployment-Procedure.md", "diffHunk": "@@ -170,3 +170,64 @@ The last step is to modify the frontend url file for the triage portal. Log in t\n ```\n echo 'https://cbioportal-frontend.netlify.com' > /srv/www/triage-tomcat/frontend_url_version_2_1_0.txt\n ```\n+\n+## Upgrading Related Backend Components\n+Backend upgrades involving the database schema, DAO classes, etc. require updates to databases and importers. CBioPortal has multiple databases (located both internally on pipelines and in AWS) backing different portals. Similarly there are multiple importers responsible for loading portal-specific data. Every database must be manually migrated on an individual basis; all importers/data fetchers can be updated simultaenously through an existing deployment script.\n+\n+Before upgrading, make sure to turn off import jobs in the crontab and alert the backend pipelines team (Avery, Angelica, Rob, Manda). \n+\n+To access the crontab, log in to pipelines,log in as cbioportal_importer: `sudo su - cbioportal_importer`, and run `crontab -e`. Comment out any lines that run import jobs, save, and exit. Make sure to uncomment these lines once the upgrade (database and importers) is complete. Lines that need to be commented out will be under the `Import Jobs` section, shown [here](https://github.com/knowledgesystems/cmo-pipelines/blob/942de83c0f9a731e301151d10dad73744cd9c9a0/import-scripts/mycrontab#L4).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e80ff07306b78f7dfe1299c3bd6c9d6003954cdd"}, "originalPosition": 10}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4NzMxOTM4OnYy", "diffSide": "RIGHT", "path": "docs/Deployment-Procedure.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxNToyODowNFrOGMpPbg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxNToyODowNFrOGMpPbg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTkxMTc5MA==", "bodyText": "typo : cbiportal -> cbioportal (or cBioPortal)", "url": "https://github.com/cBioPortal/cbioportal/pull/7439#discussion_r415911790", "createdAt": "2020-04-27T15:28:04Z", "author": {"login": "sheridancbio"}, "path": "docs/Deployment-Procedure.md", "diffHunk": "@@ -170,3 +170,64 @@ The last step is to modify the frontend url file for the triage portal. Log in t\n ```\n echo 'https://cbioportal-frontend.netlify.com' > /srv/www/triage-tomcat/frontend_url_version_2_1_0.txt\n ```\n+\n+## Upgrading Related Backend Components\n+Backend upgrades involving the database schema, DAO classes, etc. require updates to databases and importers. CBioPortal has multiple databases (located both internally on pipelines and in AWS) backing different portals. Similarly there are multiple importers responsible for loading portal-specific data. Every database must be manually migrated on an individual basis; all importers/data fetchers can be updated simultaenously through an existing deployment script.\n+\n+Before upgrading, make sure to turn off import jobs in the crontab and alert the backend pipelines team (Avery, Angelica, Rob, Manda). \n+\n+To access the crontab, log in to pipelines,log in as cbioportal_importer: `sudo su - cbioportal_importer`, and run `crontab -e`. Comment out any lines that run import jobs, save, and exit. Make sure to uncomment these lines once the upgrade (database and importers) is complete. Lines that need to be commented out will be under the `Import Jobs` section, shown [here](https://github.com/knowledgesystems/cmo-pipelines/blob/942de83c0f9a731e301151d10dad73744cd9c9a0/import-scripts/mycrontab#L4).\n+\n+## Updating Databases\n+First, make sure there is a backup of the database being migrated. \n+If there is not a weekly dump, backup the database being migrated using mysqldump. This process may take awhile depending on the size of the database. \n+\n+```\n+mysqldump -u <user> -h <host> -p <database name> | gzip > <database_name>_`date +%Y%m%d_%H%M`.sql.gz \n+```\n+    \n+The second step is to migrate the database. Make sure that the migration script is the same version as the deployed cBioPortal website. It is recommended to first test the migration script manually line-by-line in a copy of the existing database. This will catch any data-related bugs that might not be captured by the python migration script. After testing is successful, migrate the production databases following these steps [here](Updating-your-cBioPortal-installation.md#Running-the-migration-script). \n+\n+These are all cBioPortal databases and their locations:\n+| Website  | Database | Location |\n+| ------------- | ------------- | ------------- |\n+| cbioportal.mskcc.org  | cgds_gdac  | pipelines |\n+| cbioportal.org  | cgds_public  | AWS |\n+| genie.cbioportal.org | cgds_genie | AWS | \n+| triage.cbioportal.org | cgds_triage | pipelines |\n+\n+To obtain information such as usernames, passwords, hostnames - ask Avery, Angelica, Rob, Manda, and Ino. \n+\n+## Updating Importers/Data Fetchers\n+Importers (code found [here](https://github.com/knowledgesystems/pipelines)) and data fetchers (code found [here](https://github.com/knowledgesystems/cmo-pipelines)) use code from the cBioPortal codebase. The cbioportal dependency is packaged with the genome-nexus-annotation-pipeline and specified in the pipelines importer pom.\n+\n+The following steps are used during releases/updates to build new importers with the **most-up-to-date** cBioPortal and genome-nexus-annotation-pipeline code. All steps should be performed on the pipelines machine. \n+\n+1. Set the jitpack hash [here](https://github.com/genome-nexus/genome-nexus-annotation-pipeline/blob/9510299395986653d9e9b672a38d472e52e7625b/pom.xml#L71) in the genome-nexus-annotation-pipeline codebase to the most recent cbioportal/cbioportal commit hash in master.\n+\n+2. Merge this change into genome-nexus-annotation-pipeline/master.\n+\n+3. Set the commit hash [here](https://github.com/knowledgesystems/pipelines/blob/f6c52bbda86b3929222d42c9bc84581fd6333fb4/pom.xml#L76) in the pipelines codebase to the most most recent genome-nexus/genome-nexus-annotation-pipeline commit hash **(after merge specfied in step 2)**. Also ensure the db version in the pom [here](https://github.com/knowledgesystems/pipelines/blob/f6c52bbda86b3929222d42c9bc84581fd6333fb4/pom.xml#L76) matches the db schema version in the cbiportal codebase. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e80ff07306b78f7dfe1299c3bd6c9d6003954cdd"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4NzMzNjE5OnYy", "diffSide": "RIGHT", "path": "docs/Deployment-Procedure.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxNTozMTowMVrOGMpZXw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxNTozMTowMVrOGMpZXw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTkxNDMzNQ==", "bodyText": "add a reminder to run git pull in the pipelines-configuration repo in order to make sure you have the latest updates to the build script.  maybe git status is a good idea too .. to see whether anybody left the build script in an edited / modified state from the previous run.", "url": "https://github.com/cBioPortal/cbioportal/pull/7439#discussion_r415914335", "createdAt": "2020-04-27T15:31:01Z", "author": {"login": "sheridancbio"}, "path": "docs/Deployment-Procedure.md", "diffHunk": "@@ -170,3 +170,64 @@ The last step is to modify the frontend url file for the triage portal. Log in t\n ```\n echo 'https://cbioportal-frontend.netlify.com' > /srv/www/triage-tomcat/frontend_url_version_2_1_0.txt\n ```\n+\n+## Upgrading Related Backend Components\n+Backend upgrades involving the database schema, DAO classes, etc. require updates to databases and importers. CBioPortal has multiple databases (located both internally on pipelines and in AWS) backing different portals. Similarly there are multiple importers responsible for loading portal-specific data. Every database must be manually migrated on an individual basis; all importers/data fetchers can be updated simultaenously through an existing deployment script.\n+\n+Before upgrading, make sure to turn off import jobs in the crontab and alert the backend pipelines team (Avery, Angelica, Rob, Manda). \n+\n+To access the crontab, log in to pipelines,log in as cbioportal_importer: `sudo su - cbioportal_importer`, and run `crontab -e`. Comment out any lines that run import jobs, save, and exit. Make sure to uncomment these lines once the upgrade (database and importers) is complete. Lines that need to be commented out will be under the `Import Jobs` section, shown [here](https://github.com/knowledgesystems/cmo-pipelines/blob/942de83c0f9a731e301151d10dad73744cd9c9a0/import-scripts/mycrontab#L4).\n+\n+## Updating Databases\n+First, make sure there is a backup of the database being migrated. \n+If there is not a weekly dump, backup the database being migrated using mysqldump. This process may take awhile depending on the size of the database. \n+\n+```\n+mysqldump -u <user> -h <host> -p <database name> | gzip > <database_name>_`date +%Y%m%d_%H%M`.sql.gz \n+```\n+    \n+The second step is to migrate the database. Make sure that the migration script is the same version as the deployed cBioPortal website. It is recommended to first test the migration script manually line-by-line in a copy of the existing database. This will catch any data-related bugs that might not be captured by the python migration script. After testing is successful, migrate the production databases following these steps [here](Updating-your-cBioPortal-installation.md#Running-the-migration-script). \n+\n+These are all cBioPortal databases and their locations:\n+| Website  | Database | Location |\n+| ------------- | ------------- | ------------- |\n+| cbioportal.mskcc.org  | cgds_gdac  | pipelines |\n+| cbioportal.org  | cgds_public  | AWS |\n+| genie.cbioportal.org | cgds_genie | AWS | \n+| triage.cbioportal.org | cgds_triage | pipelines |\n+\n+To obtain information such as usernames, passwords, hostnames - ask Avery, Angelica, Rob, Manda, and Ino. \n+\n+## Updating Importers/Data Fetchers\n+Importers (code found [here](https://github.com/knowledgesystems/pipelines)) and data fetchers (code found [here](https://github.com/knowledgesystems/cmo-pipelines)) use code from the cBioPortal codebase. The cbioportal dependency is packaged with the genome-nexus-annotation-pipeline and specified in the pipelines importer pom.\n+\n+The following steps are used during releases/updates to build new importers with the **most-up-to-date** cBioPortal and genome-nexus-annotation-pipeline code. All steps should be performed on the pipelines machine. \n+\n+1. Set the jitpack hash [here](https://github.com/genome-nexus/genome-nexus-annotation-pipeline/blob/9510299395986653d9e9b672a38d472e52e7625b/pom.xml#L71) in the genome-nexus-annotation-pipeline codebase to the most recent cbioportal/cbioportal commit hash in master.\n+\n+2. Merge this change into genome-nexus-annotation-pipeline/master.\n+\n+3. Set the commit hash [here](https://github.com/knowledgesystems/pipelines/blob/f6c52bbda86b3929222d42c9bc84581fd6333fb4/pom.xml#L76) in the pipelines codebase to the most most recent genome-nexus/genome-nexus-annotation-pipeline commit hash **(after merge specfied in step 2)**. Also ensure the db version in the pom [here](https://github.com/knowledgesystems/pipelines/blob/f6c52bbda86b3929222d42c9bc84581fd6333fb4/pom.xml#L76) matches the db schema version in the cbiportal codebase. \n+\n+4. Merge this change into pipelines/master.\n+\n+5. Set the commit hash [here](https://github.com/knowledgesystems/cmo-pipelines/blob/e740c9fa3d409ab75988e7a157682733e261fca5/cvr/pom.xml#L70) in the cmo-pipelines codebase to the most recent genome-nexus/genome-nexus-annotation-pipeline commit hash **(after merge specified in step 2)**\n+\n+6. Merge this change into cmo-pipelines/master\n+\n+7. Run the deployment wrapper script. See details [here](Deployment-Procedure.md#Deployment-Script).  \n+\n+8. Verify new importers/data fetchers have been placed in `/data/portal-cron/lib` by checking timestamps.\n+```\n+ls -tlra /data/portal-cron/lib\n+```\n+\n+## Deployment Script\n+The wrapper script is found on pipelines here:\n+`/data/portal-cron/git-repos/pipelines-configuration/build-importer-jars/buildproductionjars.sh`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e80ff07306b78f7dfe1299c3bd6c9d6003954cdd"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4NzM0MzQzOnYy", "diffSide": "RIGHT", "path": "docs/Deployment-Procedure.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxNTozMjoxMlrOGMpdeA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxNTozMjoxMlrOGMpdeA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTkxNTM4NA==", "bodyText": "I would boldface \"step 1\" .. to emphasize that the deployer should use the cbioportal codebase hash from the start of the procedure .. no the genome-nexus-annotation-pipeline hash (which they were working with most recently)", "url": "https://github.com/cBioPortal/cbioportal/pull/7439#discussion_r415915384", "createdAt": "2020-04-27T15:32:12Z", "author": {"login": "sheridancbio"}, "path": "docs/Deployment-Procedure.md", "diffHunk": "@@ -170,3 +170,64 @@ The last step is to modify the frontend url file for the triage portal. Log in t\n ```\n echo 'https://cbioportal-frontend.netlify.com' > /srv/www/triage-tomcat/frontend_url_version_2_1_0.txt\n ```\n+\n+## Upgrading Related Backend Components\n+Backend upgrades involving the database schema, DAO classes, etc. require updates to databases and importers. CBioPortal has multiple databases (located both internally on pipelines and in AWS) backing different portals. Similarly there are multiple importers responsible for loading portal-specific data. Every database must be manually migrated on an individual basis; all importers/data fetchers can be updated simultaenously through an existing deployment script.\n+\n+Before upgrading, make sure to turn off import jobs in the crontab and alert the backend pipelines team (Avery, Angelica, Rob, Manda). \n+\n+To access the crontab, log in to pipelines,log in as cbioportal_importer: `sudo su - cbioportal_importer`, and run `crontab -e`. Comment out any lines that run import jobs, save, and exit. Make sure to uncomment these lines once the upgrade (database and importers) is complete. Lines that need to be commented out will be under the `Import Jobs` section, shown [here](https://github.com/knowledgesystems/cmo-pipelines/blob/942de83c0f9a731e301151d10dad73744cd9c9a0/import-scripts/mycrontab#L4).\n+\n+## Updating Databases\n+First, make sure there is a backup of the database being migrated. \n+If there is not a weekly dump, backup the database being migrated using mysqldump. This process may take awhile depending on the size of the database. \n+\n+```\n+mysqldump -u <user> -h <host> -p <database name> | gzip > <database_name>_`date +%Y%m%d_%H%M`.sql.gz \n+```\n+    \n+The second step is to migrate the database. Make sure that the migration script is the same version as the deployed cBioPortal website. It is recommended to first test the migration script manually line-by-line in a copy of the existing database. This will catch any data-related bugs that might not be captured by the python migration script. After testing is successful, migrate the production databases following these steps [here](Updating-your-cBioPortal-installation.md#Running-the-migration-script). \n+\n+These are all cBioPortal databases and their locations:\n+| Website  | Database | Location |\n+| ------------- | ------------- | ------------- |\n+| cbioportal.mskcc.org  | cgds_gdac  | pipelines |\n+| cbioportal.org  | cgds_public  | AWS |\n+| genie.cbioportal.org | cgds_genie | AWS | \n+| triage.cbioportal.org | cgds_triage | pipelines |\n+\n+To obtain information such as usernames, passwords, hostnames - ask Avery, Angelica, Rob, Manda, and Ino. \n+\n+## Updating Importers/Data Fetchers\n+Importers (code found [here](https://github.com/knowledgesystems/pipelines)) and data fetchers (code found [here](https://github.com/knowledgesystems/cmo-pipelines)) use code from the cBioPortal codebase. The cbioportal dependency is packaged with the genome-nexus-annotation-pipeline and specified in the pipelines importer pom.\n+\n+The following steps are used during releases/updates to build new importers with the **most-up-to-date** cBioPortal and genome-nexus-annotation-pipeline code. All steps should be performed on the pipelines machine. \n+\n+1. Set the jitpack hash [here](https://github.com/genome-nexus/genome-nexus-annotation-pipeline/blob/9510299395986653d9e9b672a38d472e52e7625b/pom.xml#L71) in the genome-nexus-annotation-pipeline codebase to the most recent cbioportal/cbioportal commit hash in master.\n+\n+2. Merge this change into genome-nexus-annotation-pipeline/master.\n+\n+3. Set the commit hash [here](https://github.com/knowledgesystems/pipelines/blob/f6c52bbda86b3929222d42c9bc84581fd6333fb4/pom.xml#L76) in the pipelines codebase to the most most recent genome-nexus/genome-nexus-annotation-pipeline commit hash **(after merge specfied in step 2)**. Also ensure the db version in the pom [here](https://github.com/knowledgesystems/pipelines/blob/f6c52bbda86b3929222d42c9bc84581fd6333fb4/pom.xml#L76) matches the db schema version in the cbiportal codebase. \n+\n+4. Merge this change into pipelines/master.\n+\n+5. Set the commit hash [here](https://github.com/knowledgesystems/cmo-pipelines/blob/e740c9fa3d409ab75988e7a157682733e261fca5/cvr/pom.xml#L70) in the cmo-pipelines codebase to the most recent genome-nexus/genome-nexus-annotation-pipeline commit hash **(after merge specified in step 2)**\n+\n+6. Merge this change into cmo-pipelines/master\n+\n+7. Run the deployment wrapper script. See details [here](Deployment-Procedure.md#Deployment-Script).  \n+\n+8. Verify new importers/data fetchers have been placed in `/data/portal-cron/lib` by checking timestamps.\n+```\n+ls -tlra /data/portal-cron/lib\n+```\n+\n+## Deployment Script\n+The wrapper script is found on pipelines here:\n+`/data/portal-cron/git-repos/pipelines-configuration/build-importer-jars/buildproductionjars.sh`.\n+\n+The wrapper script takes two arguments:\n+1. --cbioportal-git-hash (required): Set to the cBioPortal commit hash being used in the pipelines build (hash specified in step 1 of [updating importers](#Updating-Importers). This must match because the build copies out resource files (e.g application-context-business.xml) from the cbioportal codebase. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e80ff07306b78f7dfe1299c3bd6c9d6003954cdd"}, "originalPosition": 61}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3634, "cost": 1, "resetAt": "2021-11-12T18:49:56Z"}}}