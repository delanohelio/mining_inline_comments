{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzYwMTE3MDMx", "number": 415, "title": "HDDS-2840. Implement ofs://: mkdir", "bodyText": "What changes were proposed in this pull request?\nImplement mkdir in ofs://\nWhat is the link to the Apache JIRA\nhttps://issues.apache.org/jira/browse/HDDS-2840\nHow was this patch tested?\nTestRootedOzoneFilesystem all passed. This test class is copied and modified from TestOzoneFilesystem.", "createdAt": "2020-01-07T18:32:06Z", "url": "https://github.com/apache/ozone/pull/415", "merged": true, "mergeCommit": {"oid": "20bb5b56d6b15ecd47126559374c3391b3661d99"}, "closed": true, "closedAt": "2020-01-30T20:45:07Z", "author": {"login": "smengcl"}, "timelineItems": {"totalCount": 101, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABb_ff4JAH2gAyMzYwMTE3MDMxOjBkNzEwMDYwNDMzNGYwNmZlMzA0YjE0M2EzYjkwNmM5Y2JmYzg4ZGQ=", "endCursor": "Y3Vyc29yOnYyOpPPAAABb_ff4JAH2gAyMzYwMTE3MDMxOjBkNzEwMDYwNDMzNGYwNmZlMzA0YjE0M2EzYjkwNmM5Y2JmYzg4ZGQ=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "0d7100604334f06fe304b143a3b906c9cbfc88dd", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/0d7100604334f06fe304b143a3b906c9cbfc88dd", "committedDate": "2020-01-30T19:14:02Z", "message": "testListStatusOnLargeDirectory: numDirs reduced from 5111 to 1024+512."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "343f2cdd8a0ca2a6168b935e865a6dcb201f8233", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/343f2cdd8a0ca2a6168b935e865a6dcb201f8233", "committedDate": "2020-01-10T21:48:32Z", "message": "Copy OzoneFileSystem -> OFileSystem, BasicOzoneFileSystem -> BasicOFileSystem."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0707c68c8c0d2aae4a5dd51e0c4bae7c3821f262", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/0707c68c8c0d2aae4a5dd51e0c4bae7c3821f262", "committedDate": "2020-01-10T21:48:32Z", "message": "Copy TestOzoneFileSystemWithMocks -> TestOFileSystemWithMocks."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c9d22b34fae03688c5de9965caa0df3cda2d8a0d", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/c9d22b34fae03688c5de9965caa0df3cda2d8a0d", "committedDate": "2020-01-10T21:48:32Z", "message": "Copy TestOzoneFileSystem -> TestOFileSystem."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "08cee598a397ac10911240640b7c595060532413", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/08cee598a397ac10911240640b7c595060532413", "committedDate": "2020-01-10T21:48:32Z", "message": "Copy BasicOzoneClientAdapterImpl -> BasicOzoneClientOFSAdapterImpl, OzoneClientAdapterImpl -> OzoneClientOFSAdapterImpl."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0bee28acadfb2c358c6f008173e7eca6ed7fa23f", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/0bee28acadfb2c358c6f008173e7eca6ed7fa23f", "committedDate": "2020-01-10T22:11:05Z", "message": "Copy OzoneClientAdapter -> RootedOzoneClientAdapter."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5252f459276bb26bea254ab29a9a94e61af601dd", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/5252f459276bb26bea254ab29a9a94e61af601dd", "committedDate": "2020-01-10T22:11:05Z", "message": "Change to new scheme name \"ofs\"."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7e66a20d8b50971aa25755e7a235d0468e560cf4", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/7e66a20d8b50971aa25755e7a235d0468e560cf4", "committedDate": "2020-01-10T22:11:05Z", "message": "Fix typo. Tested that ofs:// scheme is registered:\n\nmvn clean install -Pdist -DskipTests -e -Dmaven.javadoc.skip=true\ncd hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozone\nalias docc=docker-compose\ndocc up -d --scale datanode=3\ndocc exec om /bin/bash\n\nozone sh volume create vol1\nozone sh bucket create vol1/buc1\n\nozone fs -put README.md o3fs://buc1.vol1/\nozone fs -ls o3fs://buc1.vol1/\n\n# Currently tested\nozone fs -ls ofs://buc1.vol1/\n\n# Goal\nozone fs -ls ofs://vol1/buc1/"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "24c38187e7d423a143db19be0e137abc34013332", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/24c38187e7d423a143db19be0e137abc34013332", "committedDate": "2020-01-10T22:11:05Z", "message": "Mod TestOFileSystemWithMocks#testFSUriWithHostPortOverrides."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "407093077ea18b487be02e19ec00c4395433f673", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/407093077ea18b487be02e19ec00c4395433f673", "committedDate": "2020-01-10T22:11:05Z", "message": "Fix TestOFileSystemWithMocks#testFSUriWithHostPortOverrides."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "34ce7e87e519402ae4db21afd8ac8c06897ccff5", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/34ce7e87e519402ae4db21afd8ac8c06897ccff5", "committedDate": "2020-01-10T22:11:05Z", "message": "Mod TestOFileSystem."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1eb19366cdc2e20335b33f1fe11784bfde8f6da2", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/1eb19366cdc2e20335b33f1fe11784bfde8f6da2", "committedDate": "2020-01-10T22:11:05Z", "message": "Implement parsing. Next: Test with TestOFileSystem."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bc581f1cfc9670f1601fd7de92cc98e669e51c94", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/bc581f1cfc9670f1601fd7de92cc98e669e51c94", "committedDate": "2020-01-10T22:11:05Z", "message": "Remove unused regex imports."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a958afc749af5ba9b838c35a88f6fcf8e934fcf5", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/a958afc749af5ba9b838c35a88f6fcf8e934fcf5", "committedDate": "2020-01-10T22:11:05Z", "message": "Mod pathToKey. Successfully tested with -put and -ls with ofs://\n\n```\nozone sh volume create vol1\nozone sh bucket create vol1/buc1\n\nozone fs -put README.md ofs://om/vol1/buc1/2.md\nozone fs -ls ofs://om/vol1/buc1/\n```"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "61dd45a60ff0e4b55e0f39ae89d450eedc6d99c1", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/61dd45a60ff0e4b55e0f39ae89d450eedc6d99c1", "committedDate": "2020-01-10T22:11:05Z", "message": "Mod BasicOFileSystem#toString"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1464bcb406e9b74d33b1f0e58dd9d58de7a59456", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/1464bcb406e9b74d33b1f0e58dd9d58de7a59456", "committedDate": "2020-01-10T22:11:05Z", "message": "Mod TestOFileSystem. Note: during \"fs = FileSystem.get(conf)\", protobuf isn't decoding the getServiceInfo request correctly:\n\n2019-12-11 09:10:46,622 [Thread-0] WARN  ha.OMFailoverProxyProvider (OzoneManagerProtocolClientSideTranslatorPB.java:submitRequest(391)) - OMRequest payload =\ncmdType: ServiceList\ntraceID: \"15dc425ffcc948a2:15dc425ffcc948a2:0:1\"\nclientId: \"client-EFF2A03AAB22\"\nserviceListRequest {\n}\n\n2019-12-11 09:10:46,699 [qtp799408188-143] WARN  http.HttpParser (HttpParser.java:<init>(1859)) - Illegal character 0x9 in state=METHOD for buffer HeapByteBuffer@7d1994d5[p=5,l=280,c=8192,r=275]={hrpc\\t<<<\\x00\\x00\\x00\\x00\\x00_\\x1a\\x08\\x02\\x10\\x00\\x18\\x05\"\\x10>x...EFF2A03AAB22\\x9a\\x03\\x00>>>\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00...\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00}\n2019-12-11 09:10:46,700 [qtp799408188-143] WARN  http.HttpParser (HttpParser.java:parseNext(1454)) - bad HTTP parsed: 400 Illegal character 0x9 for HttpChannelOverHttp@9e9d902{r=0,c=false,a=IDLE,uri=null}\n2019-12-11 09:10:46,705 [Thread-0] WARN  net.NetUtils (NetUtils.java:wrapWithMessage(834)) - Unable to wrap exception of type class org.apache.hadoop.ipc.RpcException: it has no (String) constructor\njava.lang.NoSuchMethodException: org.apache.hadoop.ipc.RpcException.<init>(java.lang.String)\n\tat java.lang.Class.getConstructor0(Class.java:3082)\n\tat java.lang.Class.getConstructor(Class.java:1825)\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:830)\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:806)\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1457)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1367)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy38.submitRequest(Unknown Source)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy38.submitRequest(Unknown Source)\n\tat org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.submitRequest(OzoneManagerProtocolClientSideTranslatorPB.java:393)\n\tat org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.getServiceInfo(OzoneManagerProtocolClientSideTranslatorPB.java:1282)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:71)\n\tat com.sun.proxy.$Proxy39.getServiceInfo(Unknown Source)\n\tat org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:158)\n\tat org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:256)\n\tat org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:239)\n\tat org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:203)\n\tat org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:165)\n\tat org.apache.hadoop.fs.ozone.BasicOzoneClientAdapterImpl.<init>(BasicOzoneClientAdapterImpl.java:161)\n\tat org.apache.hadoop.fs.ozone.OzoneClientAdapterImpl.<init>(OzoneClientAdapterImpl.java:50)\n\tat org.apache.hadoop.fs.ozone.OFileSystem.createAdapter(OFileSystem.java:102)\n\tat org.apache.hadoop.fs.ozone.BasicOFileSystem.initialize(BasicOFileSystem.java:156)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:227)\n\tat org.apache.hadoop.fs.ozone.TestOFileSystem.init(TestOFileSystem.java:103)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)\n\tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\n\tat org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d12a0c632b450488f52edf61719b954734d9ee41", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/d12a0c632b450488f52edf61719b954734d9ee41", "committedDate": "2020-01-10T22:11:05Z", "message": "Basic listStatus works now: TestOFileSystem#testListStatus"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6794e8c5c2572f300f9cc62989da672358bdcf7b", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/6794e8c5c2572f300f9cc62989da672358bdcf7b", "committedDate": "2020-01-10T22:11:06Z", "message": "Impl class OFSPath."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a7720ad272ab78e20de1a27ae63aee8fb6c7f8dc", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/a7720ad272ab78e20de1a27ae63aee8fb6c7f8dc", "committedDate": "2020-01-10T22:11:06Z", "message": "Overload createAdapter for OFSPath. Use it in all FileSystem ops."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7e145f4b8a4b97e69b399dacec1ff487aa7cf207", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/7e145f4b8a4b97e69b399dacec1ff487aa7cf207", "committedDate": "2020-01-10T22:11:06Z", "message": "Test testListStatus fully passed."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "07097bd0a367ba4033500b32f556d6422491e262", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/07097bd0a367ba4033500b32f556d6422491e262", "committedDate": "2020-01-10T22:11:06Z", "message": "Start working on other TestOFileSystem tests."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2561a58c814bddf227a6b132a155bcd6558eae84", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/2561a58c814bddf227a6b132a155bcd6558eae84", "committedDate": "2020-01-10T22:11:06Z", "message": "Close previous adapter before every new adapter initialization. TODO: 1. Can be more concise; 2. Can reuse the previous adapter if volume and bucket don't change."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "83c0e9f10f41b40c3dec223c25bcd49506b7c376", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/83c0e9f10f41b40c3dec223c25bcd49506b7c376", "committedDate": "2020-01-10T22:11:06Z", "message": "Test testListStatusOnRoot now passes (hacky)."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3f0e2e8c91cb9d4e791afd7d0a491a8889cded83", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/3f0e2e8c91cb9d4e791afd7d0a491a8889cded83", "committedDate": "2020-01-10T22:11:06Z", "message": "Fix test. testListStatusOnSubDirs, testCreateDoesNotAddParentDirKeys, testDeleteCreatesFakeParentDir now pass."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "530d26e1dffa0b12193674766d6a77c355e7ea59", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/530d26e1dffa0b12193674766d6a77c355e7ea59", "committedDate": "2020-01-10T22:11:06Z", "message": "Fix rename operation. testNonExplicitlyCreatedPathExistsAfterItsLeafsWereRemoved now passes.\nAll existing TestOFileSystem test cases now pass. Next:\n1. Rename some existing test cases to better names;\n2. Add new test cases (e.g. renaming across buckets shall fail);\n3. Clean up existing code."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b7072fe493c9afcc7043361514bf9fd85d150b3c", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/b7072fe493c9afcc7043361514bf9fd85d150b3c", "committedDate": "2020-01-10T22:11:06Z", "message": "Fix checkstyle."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d2f3771629c72e783c90b0821bd7c1594dd58ab1", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/d2f3771629c72e783c90b0821bd7c1594dd58ab1", "committedDate": "2020-01-10T22:11:06Z", "message": "Remove debug warn in OzoneManagerProtocolClientSideTranslatorPB."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b075046b3dd8fde2310c2d420f100500a3b85e33", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/b075046b3dd8fde2310c2d420f100500a3b85e33", "committedDate": "2020-01-10T22:11:06Z", "message": "Clean up code."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1e6d333e6d71df9d6f87ffda15baae113dfe3fd8", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/1e6d333e6d71df9d6f87ffda15baae113dfe3fd8", "committedDate": "2020-01-10T22:11:06Z", "message": "Rename overloaded createAdapter to checkAndCreateAdapter. Refactor a bit to make it cleaner."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bbd0e9c3420130189962545fc87141f839fa6bdf", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/bbd0e9c3420130189962545fc87141f839fa6bdf", "committedDate": "2020-01-10T22:11:06Z", "message": "Improve URI_EXCEPTION_TEXT. https://github.com/apache/hadoop-ozone/pull/367#discussion_r359004295"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "952474cc9b22f2d7d8cf04edcd7c559ea2eb0c33", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/952474cc9b22f2d7d8cf04edcd7c559ea2eb0c33", "committedDate": "2020-01-10T22:11:06Z", "message": "Mkdir: Will now (try to) create volume and bucket if they don't exist."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e6d11054e5ec9f23d67353777f62a2004393fc98", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/e6d11054e5ec9f23d67353777f62a2004393fc98", "committedDate": "2020-01-10T22:11:06Z", "message": "Manually applied HDDS-2777 b5008d04b896f17d4abfa5c476d79115581fcae6 into BasicOFileSystem in order to compile due to FSDataInputStream constructor change."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ba3a21ecc2e27690e8b823bd72fa8d8976ffdb43", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/ba3a21ecc2e27690e8b823bd72fa8d8976ffdb43", "committedDate": "2020-01-10T22:11:06Z", "message": "Update getFileStatus adapter impl. since mkdir under shell invokes getFileStatus first; moved TestOFileSystem under integration-test as in HDDS-2785 28cefc6000c198726d6437db5efe4a87b766fc21.\n\nTested under shell;\nmvn clean install -Pdist -DskipTests -e -Dmaven.javadoc.skip=true -DskipShade -DskipRecon -Dmaven.test.skip=true\ncd hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozone\ndocker-compose up -d --scale datanode=3\ndocker-compose exec om /bin/bash\nozone fs -mkdir -p ofs://om/vol1/buc1/dir1/\n\nThe above mkdir -p command will create volume vol1, bucket buc1 then dir1.\n\nNote that mkdir without -p will fail as expected."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6903f5bf6c59f2430ff42ef6cda2d986d6107518", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/6903f5bf6c59f2430ff42ef6cda2d986d6107518", "committedDate": "2020-01-10T22:11:06Z", "message": "Clean up code."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f6772d760df91b639b2bd6e5ebf4f5056708b5e3", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/f6772d760df91b639b2bd6e5ebf4f5056708b5e3", "committedDate": "2020-01-10T22:11:06Z", "message": "Improve getVolumeAndBucket(): Check exception result."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "129aa9ceb6122e19f19945976bcf4029c4371d64", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/129aa9ceb6122e19f19945976bcf4029c4371d64", "committedDate": "2020-01-10T22:11:06Z", "message": "Refactor BasicOzoneClientOFSAdapterImpl to have no volume/bucket input in the constructor. All public functions in Adapter impl should take a path (includes volume & bucket information or pseudo bucket info) instead of a key."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7c348f2aae9d48cc7d35d911e939333507e4d7ad", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/7c348f2aae9d48cc7d35d911e939333507e4d7ad", "committedDate": "2020-01-10T22:11:06Z", "message": "(1) Rename OFS class files with \"Rooted\" keyword in them;\n(2) Copy and mod OzoneClientAdapterFactory -> RootedOzoneClientAdapterFactory;\n(3) Finish refactoring AdapterImpl and RootedOzoneFileSystem.\n\nThere are failing TestRootedOzoneFileSystem tests after the refactoring, working on them now."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "33ec9d63c8796b7a7773b07d553e588b0cf065c6", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/33ec9d63c8796b7a7773b07d553e588b0cf065c6", "committedDate": "2020-01-10T22:11:06Z", "message": "Fix all failing TestRootedOzoneFileSystem tests:\n(1) listStatus and getFileStatus should be returning correct path with volume and bucket now, previously it was missing volume and bucket name in the result;\n(2) rename should be working now (fixed a typo and processKey), but delete doesn't work for now (solution known, to be fixed);\n(3) Renamed renameKey to renamePath for semantics;\n(4) OFSPath class is moved to a new independent file. It is used in tests.\n(5) Renamed listStatus param startKey -> startPath for correct semantics, and fixed the continuation."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b3d2003117900dac4c66b12808edb13d58812883", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/b3d2003117900dac4c66b12808edb13d58812883", "committedDate": "2020-01-10T22:11:06Z", "message": "Add Apache license header to OFSPath."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3c7b634f2a4cdd12ca36dfef84a7e849286f1603", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/3c7b634f2a4cdd12ca36dfef84a7e849286f1603", "committedDate": "2020-01-10T22:11:06Z", "message": "Clean up code. Fix checkstyle and findbugs."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9f1539e3c99bf02aa99670f12b3e220bc33f85bc", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/9f1539e3c99bf02aa99670f12b3e220bc33f85bc", "committedDate": "2020-01-10T22:11:07Z", "message": "Fix checkstyle."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8405b2d754a0f559ad07815379b04db6292407f5", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/8405b2d754a0f559ad07815379b04db6292407f5", "committedDate": "2020-01-10T22:11:07Z", "message": "Fix checkstyle."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "068863984077dd29d2228c6f39bbf0feb17f79ed", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/068863984077dd29d2228c6f39bbf0feb17f79ed", "committedDate": "2020-01-10T22:11:07Z", "message": "Fix recursive delete by adjusting OzoneListingIterator#processKey -> processKeyPath."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "82d39526903771e59363e13a7417505326b63fa6", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/82d39526903771e59363e13a7417505326b63fa6", "committedDate": "2020-01-10T22:11:07Z", "message": "Finish testMkdirOnNonExistentVolumeBucket."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f24bcf4cef7c5a73e760fc299f0d23d20150917d", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/f24bcf4cef7c5a73e760fc299f0d23d20150917d", "committedDate": "2020-01-10T22:11:07Z", "message": "Merge HDDS-2188 changes to BasicRootedOzoneFileSystem and BasicRootedOzoneClientAdapterImpl due to other API changes."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2c0c879ccd447ade27dbd0a2d0673d8d09d34d33", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/2c0c879ccd447ade27dbd0a2d0673d8d09d34d33", "committedDate": "2020-01-10T22:11:07Z", "message": "Clean up code."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6dbe6623a87938184250cb81d52c361210d89aac", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/6dbe6623a87938184250cb81d52c361210d89aac", "committedDate": "2020-01-09T23:27:18Z", "message": "Finish testMkdirOnNonExistentVolumeBucket."}, "afterCommit": {"oid": "2c0c879ccd447ade27dbd0a2d0673d8d09d34d33", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/2c0c879ccd447ade27dbd0a2d0673d8d09d34d33", "committedDate": "2020-01-10T22:11:07Z", "message": "Clean up code."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQzNjMzMDc5", "url": "https://github.com/apache/ozone/pull/415#pullrequestreview-343633079", "createdAt": "2020-01-16T01:52:24Z", "commit": {"oid": "2c0c879ccd447ade27dbd0a2d0673d8d09d34d33"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQwMTo1MjoyNFrOFeL5cg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQwMTo1MjoyNFrOFeL5cg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzE5NjUzMA==", "bodyText": "Can we rename this method to setVolumeAndBucket as we are setting the values here.", "url": "https://github.com/apache/ozone/pull/415#discussion_r367196530", "createdAt": "2020-01-16T01:52:24Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneClientAdapterImpl.java", "diffHunk": "@@ -0,0 +1,602 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import org.apache.commons.collections.CollectionUtils;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.crypto.key.KeyProvider;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdds.client.ReplicationFactor;\n+import org.apache.hadoop.hdds.client.ReplicationType;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.security.x509.SecurityConfig;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.ozone.OmUtils;\n+import org.apache.hadoop.ozone.OzoneConfigKeys;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClient;\n+import org.apache.hadoop.ozone.client.OzoneClientFactory;\n+import org.apache.hadoop.ozone.client.OzoneKey;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.ozone.client.io.OzoneOutputStream;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfoGroup;\n+import org.apache.hadoop.ozone.om.helpers.OzoneFileStatus;\n+import org.apache.hadoop.ozone.security.OzoneTokenIdentifier;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.security.token.TokenRenewer;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Basic Implementation of the OzoneFileSystem calls.\n+ * <p>\n+ * This is the minimal version which doesn't include any statistics.\n+ * <p>\n+ * For full featured version use OzoneClientAdapterImpl.\n+ */\n+public class BasicRootedOzoneClientAdapterImpl\n+    implements RootedOzoneClientAdapter {\n+\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneClientAdapterImpl.class);\n+\n+  private OzoneClient ozoneClient;\n+  private ObjectStore objectStore;\n+  private OzoneVolume volume;\n+  private OzoneBucket bucket;\n+  private ReplicationType replicationType;\n+  private ReplicationFactor replicationFactor;\n+  private boolean securityEnabled;\n+  private int configuredDnPort;\n+\n+  /**\n+   * Create new OzoneClientAdapter implementation.\n+   *\n+   * @throws IOException In case of a problem.\n+   */\n+  public BasicRootedOzoneClientAdapterImpl() throws IOException {\n+    this(createConf());\n+  }\n+\n+  private static OzoneConfiguration createConf() {\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+    try {\n+      return new OzoneConfiguration();\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(OzoneConfiguration conf)\n+      throws IOException {\n+    this(null, -1, conf);\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(String omHost, int omPort,\n+      Configuration hadoopConf) throws IOException {\n+\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+\n+    try {\n+      OzoneConfiguration conf = OzoneConfiguration.of(hadoopConf);\n+\n+      if (omHost == null && OmUtils.isServiceIdsDefined(conf)) {\n+        // When the host name or service id isn't given\n+        // but ozone.om.service.ids is defined, declare failure.\n+\n+        // This is a safety precaution that prevents the client from\n+        // accidentally failing over to an unintended OM.\n+        throw new IllegalArgumentException(\"Service ID or host name must not\"\n+            + \" be omitted when ozone.om.service.ids is defined.\");\n+      }\n+\n+      if (omPort != -1) {\n+        // When the port number is specified, perform the following check\n+        if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+          // If omHost is a service id, it shouldn't use a port\n+          throw new IllegalArgumentException(\"Port \" + omPort +\n+              \" specified in URI but host '\" + omHost + \"' is a \"\n+              + \"logical (HA) OzoneManager and does not use port information.\");\n+        }\n+      } else {\n+        // When port number is not specified, read it from config\n+        omPort = OmUtils.getOmRpcPort(conf);\n+      }\n+\n+      SecurityConfig secConfig = new SecurityConfig(conf);\n+\n+      if (secConfig.isSecurityEnabled()) {\n+        this.securityEnabled = true;\n+      }\n+\n+      String replicationTypeConf =\n+          conf.get(OzoneConfigKeys.OZONE_REPLICATION_TYPE,\n+              OzoneConfigKeys.OZONE_REPLICATION_TYPE_DEFAULT);\n+\n+      int replicationCountConf = conf.getInt(OzoneConfigKeys.OZONE_REPLICATION,\n+          OzoneConfigKeys.OZONE_REPLICATION_DEFAULT);\n+\n+      if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+        // omHost is listed as one of the service ids in the config,\n+        // thus we should treat omHost as omServiceId\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, conf);\n+      } else if (StringUtils.isNotEmpty(omHost) && omPort != -1) {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, omPort, conf);\n+      } else {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(conf);\n+      }\n+      objectStore = ozoneClient.getObjectStore();\n+      this.replicationType = ReplicationType.valueOf(replicationTypeConf);\n+      this.replicationFactor = ReplicationFactor.valueOf(replicationCountConf);\n+      this.configuredDnPort = conf.getInt(\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT,\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT_DEFAULT);\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  public void setVolume(String volumeString) throws IOException {\n+    this.volume = objectStore.getVolume(volumeString);\n+  }\n+\n+  public void setBucket(String bucketString) throws IOException {\n+    this.bucket = volume.getBucket(bucketString);\n+  }\n+\n+  private void getVolumeAndBucket(OFSPath ofsPath,\n+      boolean createIfNotExist) throws IOException {\n+    getVolumeAndBucket(ofsPath.getVolumeName(), ofsPath.getBucketName(),\n+        createIfNotExist);\n+  }\n+\n+  /**\n+   * Apply volumeStr and bucketStr stored in the object instance before\n+   * executing a FileSystem operation.\n+   *\n+   * @param createIfNotExist Set this to true if the caller is a write operation\n+   *                         in order to create the volume and bucket.\n+   * @throws IOException\n+   */\n+  private void getVolumeAndBucket(String volumeStr, String bucketStr,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c0c879ccd447ade27dbd0a2d0673d8d09d34d33"}, "originalPosition": 202}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b932fa8c131e9821ef4dc1d41be84a23d78eb28b", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/b932fa8c131e9821ef4dc1d41be84a23d78eb28b", "committedDate": "2020-01-16T02:34:19Z", "message": "Rename getVolumeAndBucket -> setVolumeAndBucket."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9fb74809a662efe38bba30cb280abd4df2b22b64", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/9fb74809a662efe38bba30cb280abd4df2b22b64", "committedDate": "2020-01-16T19:45:28Z", "message": "Fix checkstyle."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f409519797b5fcc230516c25590f1d23d6074629", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/f409519797b5fcc230516c25590f1d23d6074629", "committedDate": "2020-01-16T22:28:03Z", "message": "OFSPath: Use defined OFS_MOUNT_NAME_TMP instead of hard-coded \"tmp\"."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d2a01d19f0cfa2cdce5e252be2dd33fcb0d4fb6a", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/d2a01d19f0cfa2cdce5e252be2dd33fcb0d4fb6a", "committedDate": "2020-01-16T22:32:35Z", "message": "Each operation will now get a separate OzoneBucket object to operate in.\nThis should make the implementation thread-safe and faster."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7efa5504e836b8a822089cadc60f94239634ba41", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/7efa5504e836b8a822089cadc60f94239634ba41", "committedDate": "2020-01-16T23:26:02Z", "message": "Clean up code."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e86728221247038f7e266afe3e971a98bf4dd1e6", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/e86728221247038f7e266afe3e971a98bf4dd1e6", "committedDate": "2020-01-16T23:27:26Z", "message": "Rename TestOFileSystemWithMocks -> TestRootedOzoneFileSystemWithMocks."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "47e9f8786b9be9bc5b96cf9df5f27105a491a6df", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/47e9f8786b9be9bc5b96cf9df5f27105a491a6df", "committedDate": "2020-01-16T23:34:18Z", "message": "Address checkstyle/findbugs. Clean up code."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bfe5660e2c19de6944eba6220e51b4bcc27cb259", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/bfe5660e2c19de6944eba6220e51b4bcc27cb259", "committedDate": "2020-01-16T23:37:17Z", "message": "listKeys() now throws IOException."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bdffa6ab5c1f59cedc968bf36c2ccd286cc6288f", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/bdffa6ab5c1f59cedc968bf36c2ccd286cc6288f", "committedDate": "2020-01-17T18:22:46Z", "message": "Rename check (same bucket policy) is now performed in both BROFS and BROCAdapterImpl. Test added."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ2MjYxOTE2", "url": "https://github.com/apache/ozone/pull/415#pullrequestreview-346261916", "createdAt": "2020-01-21T23:27:47Z", "commit": {"oid": "d2a01d19f0cfa2cdce5e252be2dd33fcb0d4fb6a"}, "state": "COMMENTED", "comments": {"totalCount": 19, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQyMzoyNzo0N1rOFgMOOA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMlQyMjoyNDozMVrOFgtI1g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTI5OTAwMA==", "bodyText": "OFileSystem -> RootedOzoneFileSystem", "url": "https://github.com/apache/ozone/pull/415#discussion_r369299000", "createdAt": "2020-01-21T23:27:47Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/fs/ozone/TestRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,395 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.ozone;\n+\n+import org.apache.commons.io.IOUtils;\n+import org.apache.commons.lang3.RandomStringUtils;\n+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.contract.ContractTestUtils;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.ozone.MiniOzoneCluster;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+import org.apache.hadoop.ozone.TestDataUtil;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClientException;\n+import org.apache.hadoop.ozone.client.OzoneKeyDetails;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Set;\n+import java.util.TreeSet;\n+\n+import static org.apache.hadoop.ozone.om.OMConfigKeys.OZONE_OM_ADDRESS_KEY;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Ozone file system tests that are not covered by contract tests.\n+ */\n+public class TestRootedOzoneFileSystem {\n+\n+  @Rule\n+  public Timeout globalTimeout = new Timeout(300_000);\n+\n+  private static MiniOzoneCluster cluster = null;\n+\n+  private static FileSystem fs;\n+  private static RootedOzoneFileSystem ofs;\n+\n+  private static ObjectStore objectStore;\n+\n+  private String volumeName;\n+  private String bucketName;\n+\n+  private String rootPath;\n+\n+  @Before\n+  public void init() throws Exception {\n+    OzoneConfiguration conf = new OzoneConfiguration();\n+    cluster = MiniOzoneCluster.newBuilder(conf)\n+        .setNumDatanodes(3)\n+        .build();\n+    cluster.waitForClusterToBeReady();\n+    objectStore = cluster.getClient().getObjectStore();\n+\n+    // create a volume and a bucket to be used by OFileSystem", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2a01d19f0cfa2cdce5e252be2dd33fcb0d4fb6a"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTI5OTkyOQ==", "bodyText": "If this is going to change, can we please add a TODO here to keep track.", "url": "https://github.com/apache/ozone/pull/415#discussion_r369299929", "createdAt": "2020-01-21T23:30:55Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/fs/ozone/TestRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,395 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.ozone;\n+\n+import org.apache.commons.io.IOUtils;\n+import org.apache.commons.lang3.RandomStringUtils;\n+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.contract.ContractTestUtils;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.ozone.MiniOzoneCluster;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+import org.apache.hadoop.ozone.TestDataUtil;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClientException;\n+import org.apache.hadoop.ozone.client.OzoneKeyDetails;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Set;\n+import java.util.TreeSet;\n+\n+import static org.apache.hadoop.ozone.om.OMConfigKeys.OZONE_OM_ADDRESS_KEY;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Ozone file system tests that are not covered by contract tests.\n+ */\n+public class TestRootedOzoneFileSystem {\n+\n+  @Rule\n+  public Timeout globalTimeout = new Timeout(300_000);\n+\n+  private static MiniOzoneCluster cluster = null;\n+\n+  private static FileSystem fs;\n+  private static RootedOzoneFileSystem ofs;\n+\n+  private static ObjectStore objectStore;\n+\n+  private String volumeName;\n+  private String bucketName;\n+\n+  private String rootPath;\n+\n+  @Before\n+  public void init() throws Exception {\n+    OzoneConfiguration conf = new OzoneConfiguration();\n+    cluster = MiniOzoneCluster.newBuilder(conf)\n+        .setNumDatanodes(3)\n+        .build();\n+    cluster.waitForClusterToBeReady();\n+    objectStore = cluster.getClient().getObjectStore();\n+\n+    // create a volume and a bucket to be used by OFileSystem\n+    OzoneBucket bucket = TestDataUtil.createVolumeAndBucket(cluster);\n+    volumeName = bucket.getVolumeName();\n+    bucketName = bucket.getName();\n+\n+    // For now:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2a01d19f0cfa2cdce5e252be2dd33fcb0d4fb6a"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTMwMjkxOQ==", "bodyText": "rootBucket is used globally in this Test suit. Can we make it a global variable?\nAlso, rootBucket gives the impression that it's the bucket at the root of the FS. Could we rename it to testBucket or volumeAndBucketPath or something else?", "url": "https://github.com/apache/ozone/pull/415#discussion_r369302919", "createdAt": "2020-01-21T23:41:29Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/fs/ozone/TestRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,395 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.ozone;\n+\n+import org.apache.commons.io.IOUtils;\n+import org.apache.commons.lang3.RandomStringUtils;\n+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.contract.ContractTestUtils;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.ozone.MiniOzoneCluster;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+import org.apache.hadoop.ozone.TestDataUtil;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClientException;\n+import org.apache.hadoop.ozone.client.OzoneKeyDetails;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Set;\n+import java.util.TreeSet;\n+\n+import static org.apache.hadoop.ozone.om.OMConfigKeys.OZONE_OM_ADDRESS_KEY;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Ozone file system tests that are not covered by contract tests.\n+ */\n+public class TestRootedOzoneFileSystem {\n+\n+  @Rule\n+  public Timeout globalTimeout = new Timeout(300_000);\n+\n+  private static MiniOzoneCluster cluster = null;\n+\n+  private static FileSystem fs;\n+  private static RootedOzoneFileSystem ofs;\n+\n+  private static ObjectStore objectStore;\n+\n+  private String volumeName;\n+  private String bucketName;\n+\n+  private String rootPath;\n+\n+  @Before\n+  public void init() throws Exception {\n+    OzoneConfiguration conf = new OzoneConfiguration();\n+    cluster = MiniOzoneCluster.newBuilder(conf)\n+        .setNumDatanodes(3)\n+        .build();\n+    cluster.waitForClusterToBeReady();\n+    objectStore = cluster.getClient().getObjectStore();\n+\n+    // create a volume and a bucket to be used by OFileSystem\n+    OzoneBucket bucket = TestDataUtil.createVolumeAndBucket(cluster);\n+    volumeName = bucket.getVolumeName();\n+    bucketName = bucket.getName();\n+\n+    // For now:\n+    rootPath = String.format(\"%s://%s/\", OzoneConsts.OZONE_OFS_URI_SCHEME,\n+        conf.get(OZONE_OM_ADDRESS_KEY));\n+\n+    // Set the fs.defaultFS and start the filesystem\n+    conf.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, rootPath);\n+    // TODO: FileSystem#loadFileSystems is not loading ofs:// class by default\n+    //  hence this workaround. Might need to add some config in hadoop source.\n+    conf.set(\"fs.ofs.impl\", \"org.apache.hadoop.fs.ozone.RootedOzoneFileSystem\");\n+    fs = FileSystem.get(conf);\n+    ofs = (RootedOzoneFileSystem) fs;\n+  }\n+\n+  @After\n+  public void teardown() throws IOException {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n+    IOUtils.closeQuietly(fs);\n+  }\n+\n+  @Test\n+  public void testOzoneFsServiceLoader() throws IOException {\n+    OzoneConfiguration conf = new OzoneConfiguration();\n+    // TODO: Address this properly.\n+    conf.set(\"fs.ofs.impl\", \"org.apache.hadoop.fs.ozone.RootedOzoneFileSystem\");\n+    assertEquals(\n+        FileSystem.getFileSystemClass(OzoneConsts.OZONE_OFS_URI_SCHEME,\n+            conf), RootedOzoneFileSystem.class);\n+  }\n+\n+  @Test\n+  public void testCreateDoesNotAddParentDirKeys() throws Exception {\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2a01d19f0cfa2cdce5e252be2dd33fcb0d4fb6a"}, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTMwNDA5NQ==", "bodyText": "We can add one more check later that listStatus(rootBucket) returns 1 fileStatus object - parent.", "url": "https://github.com/apache/ozone/pull/415#discussion_r369304095", "createdAt": "2020-01-21T23:45:33Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/fs/ozone/TestRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,395 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.ozone;\n+\n+import org.apache.commons.io.IOUtils;\n+import org.apache.commons.lang3.RandomStringUtils;\n+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.contract.ContractTestUtils;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.ozone.MiniOzoneCluster;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+import org.apache.hadoop.ozone.TestDataUtil;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClientException;\n+import org.apache.hadoop.ozone.client.OzoneKeyDetails;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Set;\n+import java.util.TreeSet;\n+\n+import static org.apache.hadoop.ozone.om.OMConfigKeys.OZONE_OM_ADDRESS_KEY;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Ozone file system tests that are not covered by contract tests.\n+ */\n+public class TestRootedOzoneFileSystem {\n+\n+  @Rule\n+  public Timeout globalTimeout = new Timeout(300_000);\n+\n+  private static MiniOzoneCluster cluster = null;\n+\n+  private static FileSystem fs;\n+  private static RootedOzoneFileSystem ofs;\n+\n+  private static ObjectStore objectStore;\n+\n+  private String volumeName;\n+  private String bucketName;\n+\n+  private String rootPath;\n+\n+  @Before\n+  public void init() throws Exception {\n+    OzoneConfiguration conf = new OzoneConfiguration();\n+    cluster = MiniOzoneCluster.newBuilder(conf)\n+        .setNumDatanodes(3)\n+        .build();\n+    cluster.waitForClusterToBeReady();\n+    objectStore = cluster.getClient().getObjectStore();\n+\n+    // create a volume and a bucket to be used by OFileSystem\n+    OzoneBucket bucket = TestDataUtil.createVolumeAndBucket(cluster);\n+    volumeName = bucket.getVolumeName();\n+    bucketName = bucket.getName();\n+\n+    // For now:\n+    rootPath = String.format(\"%s://%s/\", OzoneConsts.OZONE_OFS_URI_SCHEME,\n+        conf.get(OZONE_OM_ADDRESS_KEY));\n+\n+    // Set the fs.defaultFS and start the filesystem\n+    conf.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, rootPath);\n+    // TODO: FileSystem#loadFileSystems is not loading ofs:// class by default\n+    //  hence this workaround. Might need to add some config in hadoop source.\n+    conf.set(\"fs.ofs.impl\", \"org.apache.hadoop.fs.ozone.RootedOzoneFileSystem\");\n+    fs = FileSystem.get(conf);\n+    ofs = (RootedOzoneFileSystem) fs;\n+  }\n+\n+  @After\n+  public void teardown() throws IOException {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n+    IOUtils.closeQuietly(fs);\n+  }\n+\n+  @Test\n+  public void testOzoneFsServiceLoader() throws IOException {\n+    OzoneConfiguration conf = new OzoneConfiguration();\n+    // TODO: Address this properly.\n+    conf.set(\"fs.ofs.impl\", \"org.apache.hadoop.fs.ozone.RootedOzoneFileSystem\");\n+    assertEquals(\n+        FileSystem.getFileSystemClass(OzoneConsts.OZONE_OFS_URI_SCHEME,\n+            conf), RootedOzoneFileSystem.class);\n+  }\n+\n+  @Test\n+  public void testCreateDoesNotAddParentDirKeys() throws Exception {\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path grandparent = new Path(rootBucket,\n+        \"testCreateDoesNotAddParentDirKeys\");\n+    Path parent = new Path(grandparent, \"parent\");\n+    Path child = new Path(parent, \"child\");\n+    ContractTestUtils.touch(fs, child);\n+\n+    OzoneKeyDetails key = getKeyInBucket(child, false);\n+    OFSPath childOFSPath = new OFSPath(child);\n+    assertEquals(key.getName(), childOFSPath.getKeyName());\n+\n+    // Creating a child should not add parent keys to the bucket\n+    try {\n+      getKeyInBucket(parent, true);\n+    } catch (IOException ex) {\n+      assertKeyNotFoundException(ex);\n+    }\n+\n+    // List status on the parent should show the child file\n+    assertEquals(\"List status of parent should include the 1 child file\", 1L,\n+        (long)fs.listStatus(parent).length);\n+    assertTrue(\"Parent directory does not appear to be a directory\",\n+        fs.getFileStatus(parent).isDirectory());\n+  }\n+\n+  @Test\n+  public void testDeleteCreatesFakeParentDir() throws Exception {\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path grandparent = new Path(rootBucket, \"testDeleteCreatesFakeParentDir\");\n+    Path parent = new Path(grandparent, \"parent\");\n+    Path child = new Path(parent, \"child\");\n+    ContractTestUtils.touch(fs, child);\n+\n+    // Verify that parent dir key does not exist\n+    // Creating a child should not add parent keys to the bucket\n+    try {\n+      getKeyInBucket(parent, true);\n+    } catch (IOException ex) {\n+      assertKeyNotFoundException(ex);\n+    }\n+\n+    // Delete the child key\n+    assertTrue(fs.delete(child, false));\n+\n+    // Deleting the only child should create the parent dir key if it does\n+    // not exist\n+    OFSPath parentOFSPath = new OFSPath(parent);\n+    String parentKey = parentOFSPath.getKeyName() + \"/\";\n+    OzoneKeyDetails parentKeyInfo = getKeyInBucket(parent, true);\n+    assertEquals(parentKey, parentKeyInfo.getName());\n+\n+    // Recursive delete with DeleteIterator\n+    assertTrue(fs.delete(grandparent, true));\n+  }\n+\n+  @Test\n+  public void testListStatus() throws Exception {\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path parent = new Path(rootBucket, \"testListStatus\");\n+    Path file1 = new Path(parent, \"key1\");\n+    Path file2 = new Path(parent, \"key2\");\n+\n+    FileStatus[] fileStatuses = ofs.listStatus(rootBucket);\n+    assertEquals(\"Should be empty\", 0, fileStatuses.length);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2a01d19f0cfa2cdce5e252be2dd33fcb0d4fb6a"}, "originalPosition": 185}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTMwODU1Ng==", "bodyText": "Let's test \"rename to different bucket\" in a separate method.", "url": "https://github.com/apache/ozone/pull/415#discussion_r369308556", "createdAt": "2020-01-22T00:00:55Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/fs/ozone/TestRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,408 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.ozone;\n+\n+import org.apache.commons.io.IOUtils;\n+import org.apache.commons.lang3.RandomStringUtils;\n+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.contract.ContractTestUtils;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.ozone.MiniOzoneCluster;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+import org.apache.hadoop.ozone.TestDataUtil;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClientException;\n+import org.apache.hadoop.ozone.client.OzoneKeyDetails;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Set;\n+import java.util.TreeSet;\n+\n+import static org.apache.hadoop.ozone.om.OMConfigKeys.OZONE_OM_ADDRESS_KEY;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+/**\n+ * Ozone file system tests that are not covered by contract tests.\n+ */\n+public class TestRootedOzoneFileSystem {\n+\n+  @Rule\n+  public Timeout globalTimeout = new Timeout(300_000);\n+\n+  private static MiniOzoneCluster cluster = null;\n+\n+  private static FileSystem fs;\n+  private static RootedOzoneFileSystem ofs;\n+\n+  private static ObjectStore objectStore;\n+\n+  private String volumeName;\n+  private String bucketName;\n+\n+  private String rootPath;\n+\n+  @Before\n+  public void init() throws Exception {\n+    OzoneConfiguration conf = new OzoneConfiguration();\n+    cluster = MiniOzoneCluster.newBuilder(conf)\n+        .setNumDatanodes(3)\n+        .build();\n+    cluster.waitForClusterToBeReady();\n+    objectStore = cluster.getClient().getObjectStore();\n+\n+    // create a volume and a bucket to be used by OFileSystem\n+    OzoneBucket bucket = TestDataUtil.createVolumeAndBucket(cluster);\n+    volumeName = bucket.getVolumeName();\n+    bucketName = bucket.getName();\n+\n+    // For now:\n+    rootPath = String.format(\"%s://%s/\", OzoneConsts.OZONE_OFS_URI_SCHEME,\n+        conf.get(OZONE_OM_ADDRESS_KEY));\n+\n+    // Set the fs.defaultFS and start the filesystem\n+    conf.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, rootPath);\n+    // Note: FileSystem#loadFileSystems won't load OFS class due to META-INF\n+    //  hence this workaround.\n+    conf.set(\"fs.ofs.impl\", \"org.apache.hadoop.fs.ozone.RootedOzoneFileSystem\");\n+    fs = FileSystem.get(conf);\n+    ofs = (RootedOzoneFileSystem) fs;\n+  }\n+\n+  @After\n+  public void teardown() throws IOException {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n+    IOUtils.closeQuietly(fs);\n+  }\n+\n+  @Test\n+  public void testOzoneFsServiceLoader() throws IOException {\n+    OzoneConfiguration conf = new OzoneConfiguration();\n+    // Note: FileSystem#loadFileSystems won't load OFS class due to META-INF\n+    //  hence this workaround.\n+    conf.set(\"fs.ofs.impl\", \"org.apache.hadoop.fs.ozone.RootedOzoneFileSystem\");\n+    assertEquals(\n+        FileSystem.getFileSystemClass(OzoneConsts.OZONE_OFS_URI_SCHEME, conf),\n+        RootedOzoneFileSystem.class);\n+  }\n+\n+  @Test\n+  public void testCreateDoesNotAddParentDirKeys() throws Exception {\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path grandparent = new Path(rootBucket,\n+        \"testCreateDoesNotAddParentDirKeys\");\n+    Path parent = new Path(grandparent, \"parent\");\n+    Path child = new Path(parent, \"child\");\n+    ContractTestUtils.touch(fs, child);\n+\n+    OzoneKeyDetails key = getKeyInBucket(child, false);\n+    OFSPath childOFSPath = new OFSPath(child);\n+    assertEquals(key.getName(), childOFSPath.getKeyName());\n+\n+    // Creating a child should not add parent keys to the bucket\n+    try {\n+      getKeyInBucket(parent, true);\n+    } catch (IOException ex) {\n+      assertKeyNotFoundException(ex);\n+    }\n+\n+    // List status on the parent should show the child file\n+    assertEquals(\"List status of parent should include the 1 child file\", 1L,\n+        (long)fs.listStatus(parent).length);\n+    assertTrue(\"Parent directory does not appear to be a directory\",\n+        fs.getFileStatus(parent).isDirectory());\n+  }\n+\n+  @Test\n+  public void testDeleteCreatesFakeParentDir() throws Exception {\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path grandparent = new Path(rootBucket, \"testDeleteCreatesFakeParentDir\");\n+    Path parent = new Path(grandparent, \"parent\");\n+    Path child = new Path(parent, \"child\");\n+    ContractTestUtils.touch(fs, child);\n+\n+    // Verify that parent dir key does not exist\n+    // Creating a child should not add parent keys to the bucket\n+    try {\n+      getKeyInBucket(parent, true);\n+    } catch (IOException ex) {\n+      assertKeyNotFoundException(ex);\n+    }\n+\n+    // Delete the child key\n+    assertTrue(fs.delete(child, false));\n+\n+    // Deleting the only child should create the parent dir key if it does\n+    // not exist\n+    OFSPath parentOFSPath = new OFSPath(parent);\n+    String parentKey = parentOFSPath.getKeyName() + \"/\";\n+    OzoneKeyDetails parentKeyInfo = getKeyInBucket(parent, true);\n+    assertEquals(parentKey, parentKeyInfo.getName());\n+\n+    // Recursive delete with DeleteIterator\n+    assertTrue(fs.delete(grandparent, true));\n+  }\n+\n+  @Test\n+  public void testListStatus() throws Exception {\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path parent = new Path(rootBucket, \"testListStatus\");\n+    Path file1 = new Path(parent, \"key1\");\n+    Path file2 = new Path(parent, \"key2\");\n+\n+    FileStatus[] fileStatuses = ofs.listStatus(rootBucket);\n+    assertEquals(\"Should be empty\", 0, fileStatuses.length);\n+\n+    ContractTestUtils.touch(fs, file1);\n+    ContractTestUtils.touch(fs, file2);\n+\n+    // ListStatus on a directory should return all subdirs along with\n+    // files, even if there exists a file and sub-dir with the same name.\n+    fileStatuses = ofs.listStatus(parent);\n+    assertEquals(\"FileStatus did not return all children of the directory\",\n+        2, fileStatuses.length);\n+\n+    // ListStatus should return only the immediate children of a directory.\n+    Path file3 = new Path(parent, \"dir1/key3\");\n+    Path file4 = new Path(parent, \"dir1/key4\");\n+    ContractTestUtils.touch(fs, file3);\n+    ContractTestUtils.touch(fs, file4);\n+    fileStatuses = ofs.listStatus(parent);\n+    assertEquals(\"FileStatus did not return all children of the directory\",\n+        3, fileStatuses.length);\n+  }\n+\n+  private String getRandomNonExistVolumeName() throws Exception {\n+    final int numDigit = 5;\n+    long retriesLeft = Math.round(Math.pow(10, 5));\n+    String name = null;\n+    while (name == null && retriesLeft-- > 0) {\n+      name = \"volume-\" + RandomStringUtils.randomNumeric(numDigit);\n+      // Check volume existence.\n+      Iterator<? extends OzoneVolume> iter =\n+          objectStore.listVolumesByUser(null, name, null);\n+      if (iter.hasNext()) {\n+        // If there is a match, try again.\n+        // Note that volume name prefix match doesn't equal volume existence\n+        //  but the check is sufficient for this test.\n+        name = null;\n+      }\n+    }\n+    if (retriesLeft <= 0) {\n+      throw new Exception(\n+          \"Failed to generate random volume name that doesn't exist already.\");\n+    }\n+    return name;\n+  }\n+\n+  /**\n+   * Tests Mkdir operation on a volume that doesn't exist.\n+   * Expect Mkdir to create the volume and bucket.\n+   */\n+  @Test\n+  public void testMkdirOnNonExistentVolumeBucket() throws Exception {\n+    String volumeNameLocal = getRandomNonExistVolumeName();\n+    String bucketNameLocal = \"bucket-\" + RandomStringUtils.randomNumeric(5);\n+    Path root = new Path(\"/\" + volumeNameLocal + \"/\" + bucketNameLocal);\n+    Path dir1 = new Path(root, \"dir1\");\n+    Path dir12 = new Path(dir1, \"dir12\");\n+    Path dir2 = new Path(root, \"dir2\");\n+    fs.mkdirs(dir12);\n+    fs.mkdirs(dir2);\n+\n+    // Check volume and bucket existence, they should both be created.\n+    OzoneVolume ozoneVolume = objectStore.getVolume(volumeNameLocal);\n+    OzoneBucket ozoneBucket = ozoneVolume.getBucket(bucketNameLocal);\n+    OFSPath ofsPathDir1 = new OFSPath(dir12);\n+    String key = ofsPathDir1.getKeyName() + \"/\";\n+    OzoneKeyDetails ozoneKeyDetails = ozoneBucket.getKey(key);\n+    assertEquals(key, ozoneKeyDetails.getName());\n+\n+    // Verify that directories are created.\n+    FileStatus[] fileStatuses = ofs.listStatus(root);\n+    assertEquals(fileStatuses[0].getPath().toUri().getPath(), dir1.toString());\n+    assertEquals(fileStatuses[1].getPath().toUri().getPath(), dir2.toString());\n+\n+    fileStatuses = ofs.listStatus(dir1);\n+    assertEquals(fileStatuses[0].getPath().toUri().getPath(), dir12.toString());\n+    fileStatuses = ofs.listStatus(dir12);\n+    assertEquals(fileStatuses.length, 0);\n+    fileStatuses = ofs.listStatus(dir2);\n+    assertEquals(fileStatuses.length, 0);\n+  }\n+\n+  /**\n+   * Tests listStatus operation on root directory.\n+   */\n+  @Test\n+  public void testListStatusOnRoot() throws Exception {\n+    Path root = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path dir1 = new Path(root, \"dir1\");\n+    Path dir12 = new Path(dir1, \"dir12\");\n+    Path dir2 = new Path(root, \"dir2\");\n+    fs.mkdirs(dir12);\n+    fs.mkdirs(dir2);\n+\n+    // ListStatus on root should return dir1 (even though /dir1 key does not\n+    // exist) and dir2 only. dir12 is not an immediate child of root and\n+    // hence should not be listed.\n+    FileStatus[] fileStatuses = ofs.listStatus(root);\n+    assertEquals(\"FileStatus should return only the immediate children\", 2,\n+        fileStatuses.length);\n+\n+    // Verify that dir12 is not included in the result of the listStatus on root\n+    String fileStatus1 = fileStatuses[0].getPath().toUri().getPath();\n+    String fileStatus2 = fileStatuses[1].getPath().toUri().getPath();\n+    assertFalse(fileStatus1.equals(dir12.toString()));\n+    assertFalse(fileStatus2.equals(dir12.toString()));\n+  }\n+\n+  /**\n+   * Tests listStatus operation on root directory.\n+   */\n+  @Test\n+  public void testListStatusOnLargeDirectory() throws Exception {\n+    Path root = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Set<String> paths = new TreeSet<>();\n+    int numDirs = 5111;\n+    for(int i = 0; i < numDirs; i++) {\n+      Path p = new Path(root, String.valueOf(i));\n+      fs.mkdirs(p);\n+      paths.add(p.getName());\n+    }\n+\n+    FileStatus[] fileStatuses = ofs.listStatus(root);\n+    assertEquals(\n+        \"Total directories listed do not match the existing directories\",\n+        numDirs, fileStatuses.length);\n+\n+    for (int i=0; i < numDirs; i++) {\n+      assertTrue(paths.contains(fileStatuses[i].getPath().getName()));\n+    }\n+  }\n+\n+  /**\n+   * Tests listStatus on a path with subdirs.\n+   */\n+  @Test\n+  public void testListStatusOnSubDirs() throws Exception {\n+    // Create the following key structure\n+    //      /dir1/dir11/dir111\n+    //      /dir1/dir12\n+    //      /dir1/dir12/file121\n+    //      /dir2\n+    // ListStatus on /dir1 should return all its immediated subdirs only\n+    // which are /dir1/dir11 and /dir1/dir12. Super child files/dirs\n+    // (/dir1/dir12/file121 and /dir1/dir11/dir111) should not be returned by\n+    // listStatus.\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path dir1 = new Path(rootBucket, \"dir1\");\n+    Path dir11 = new Path(dir1, \"dir11\");\n+    Path dir111 = new Path(dir11, \"dir111\");\n+    Path dir12 = new Path(dir1, \"dir12\");\n+    Path file121 = new Path(dir12, \"file121\");\n+    Path dir2 = new Path(rootBucket, \"dir2\");\n+    fs.mkdirs(dir111);\n+    fs.mkdirs(dir12);\n+    ContractTestUtils.touch(fs, file121);\n+    fs.mkdirs(dir2);\n+\n+    FileStatus[] fileStatuses = ofs.listStatus(dir1);\n+    assertEquals(\"FileStatus should return only the immediate children\", 2,\n+        fileStatuses.length);\n+\n+    // Verify that the two children of /dir1 returned by listStatus operation\n+    // are /dir1/dir11 and /dir1/dir12.\n+    String fileStatus1 = fileStatuses[0].getPath().toUri().getPath();\n+    String fileStatus2 = fileStatuses[1].getPath().toUri().getPath();\n+    assertTrue(fileStatus1.equals(dir11.toString()) ||\n+        fileStatus1.equals(dir12.toString()));\n+    assertTrue(fileStatus2.equals(dir11.toString()) ||\n+        fileStatus2.equals(dir12.toString()));\n+  }\n+\n+  @Test\n+  public void testNonExplicitlyCreatedPathExistsAfterItsLeafsWereRemoved()\n+      throws Exception {\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path source = new Path(rootBucket, \"source\");\n+    Path interimPath = new Path(source, \"interimPath\");\n+    Path leafInsideInterimPath = new Path(interimPath, \"leaf\");\n+    Path target = new Path(rootBucket, \"target\");\n+    Path leafInTarget = new Path(target, \"leaf\");\n+\n+    fs.mkdirs(source);\n+    fs.mkdirs(target);\n+    fs.mkdirs(leafInsideInterimPath);\n+\n+    // Attempt to rename the key to a different bucket\n+    Path bucket2 = new Path(\"/\" + volumeName + \"/\" + bucketName + \"test\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdffa6ab5c1f59cedc968bf36c2ccd286cc6288f"}, "originalPosition": 372}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTMwODgzMA==", "bodyText": "I think we can avoid printing this here. Just a comment would suffice.", "url": "https://github.com/apache/ozone/pull/415#discussion_r369308830", "createdAt": "2020-01-22T00:01:51Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/fs/ozone/TestRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,408 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.ozone;\n+\n+import org.apache.commons.io.IOUtils;\n+import org.apache.commons.lang3.RandomStringUtils;\n+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.contract.ContractTestUtils;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.ozone.MiniOzoneCluster;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+import org.apache.hadoop.ozone.TestDataUtil;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClientException;\n+import org.apache.hadoop.ozone.client.OzoneKeyDetails;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Set;\n+import java.util.TreeSet;\n+\n+import static org.apache.hadoop.ozone.om.OMConfigKeys.OZONE_OM_ADDRESS_KEY;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+/**\n+ * Ozone file system tests that are not covered by contract tests.\n+ */\n+public class TestRootedOzoneFileSystem {\n+\n+  @Rule\n+  public Timeout globalTimeout = new Timeout(300_000);\n+\n+  private static MiniOzoneCluster cluster = null;\n+\n+  private static FileSystem fs;\n+  private static RootedOzoneFileSystem ofs;\n+\n+  private static ObjectStore objectStore;\n+\n+  private String volumeName;\n+  private String bucketName;\n+\n+  private String rootPath;\n+\n+  @Before\n+  public void init() throws Exception {\n+    OzoneConfiguration conf = new OzoneConfiguration();\n+    cluster = MiniOzoneCluster.newBuilder(conf)\n+        .setNumDatanodes(3)\n+        .build();\n+    cluster.waitForClusterToBeReady();\n+    objectStore = cluster.getClient().getObjectStore();\n+\n+    // create a volume and a bucket to be used by OFileSystem\n+    OzoneBucket bucket = TestDataUtil.createVolumeAndBucket(cluster);\n+    volumeName = bucket.getVolumeName();\n+    bucketName = bucket.getName();\n+\n+    // For now:\n+    rootPath = String.format(\"%s://%s/\", OzoneConsts.OZONE_OFS_URI_SCHEME,\n+        conf.get(OZONE_OM_ADDRESS_KEY));\n+\n+    // Set the fs.defaultFS and start the filesystem\n+    conf.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, rootPath);\n+    // Note: FileSystem#loadFileSystems won't load OFS class due to META-INF\n+    //  hence this workaround.\n+    conf.set(\"fs.ofs.impl\", \"org.apache.hadoop.fs.ozone.RootedOzoneFileSystem\");\n+    fs = FileSystem.get(conf);\n+    ofs = (RootedOzoneFileSystem) fs;\n+  }\n+\n+  @After\n+  public void teardown() throws IOException {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n+    IOUtils.closeQuietly(fs);\n+  }\n+\n+  @Test\n+  public void testOzoneFsServiceLoader() throws IOException {\n+    OzoneConfiguration conf = new OzoneConfiguration();\n+    // Note: FileSystem#loadFileSystems won't load OFS class due to META-INF\n+    //  hence this workaround.\n+    conf.set(\"fs.ofs.impl\", \"org.apache.hadoop.fs.ozone.RootedOzoneFileSystem\");\n+    assertEquals(\n+        FileSystem.getFileSystemClass(OzoneConsts.OZONE_OFS_URI_SCHEME, conf),\n+        RootedOzoneFileSystem.class);\n+  }\n+\n+  @Test\n+  public void testCreateDoesNotAddParentDirKeys() throws Exception {\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path grandparent = new Path(rootBucket,\n+        \"testCreateDoesNotAddParentDirKeys\");\n+    Path parent = new Path(grandparent, \"parent\");\n+    Path child = new Path(parent, \"child\");\n+    ContractTestUtils.touch(fs, child);\n+\n+    OzoneKeyDetails key = getKeyInBucket(child, false);\n+    OFSPath childOFSPath = new OFSPath(child);\n+    assertEquals(key.getName(), childOFSPath.getKeyName());\n+\n+    // Creating a child should not add parent keys to the bucket\n+    try {\n+      getKeyInBucket(parent, true);\n+    } catch (IOException ex) {\n+      assertKeyNotFoundException(ex);\n+    }\n+\n+    // List status on the parent should show the child file\n+    assertEquals(\"List status of parent should include the 1 child file\", 1L,\n+        (long)fs.listStatus(parent).length);\n+    assertTrue(\"Parent directory does not appear to be a directory\",\n+        fs.getFileStatus(parent).isDirectory());\n+  }\n+\n+  @Test\n+  public void testDeleteCreatesFakeParentDir() throws Exception {\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path grandparent = new Path(rootBucket, \"testDeleteCreatesFakeParentDir\");\n+    Path parent = new Path(grandparent, \"parent\");\n+    Path child = new Path(parent, \"child\");\n+    ContractTestUtils.touch(fs, child);\n+\n+    // Verify that parent dir key does not exist\n+    // Creating a child should not add parent keys to the bucket\n+    try {\n+      getKeyInBucket(parent, true);\n+    } catch (IOException ex) {\n+      assertKeyNotFoundException(ex);\n+    }\n+\n+    // Delete the child key\n+    assertTrue(fs.delete(child, false));\n+\n+    // Deleting the only child should create the parent dir key if it does\n+    // not exist\n+    OFSPath parentOFSPath = new OFSPath(parent);\n+    String parentKey = parentOFSPath.getKeyName() + \"/\";\n+    OzoneKeyDetails parentKeyInfo = getKeyInBucket(parent, true);\n+    assertEquals(parentKey, parentKeyInfo.getName());\n+\n+    // Recursive delete with DeleteIterator\n+    assertTrue(fs.delete(grandparent, true));\n+  }\n+\n+  @Test\n+  public void testListStatus() throws Exception {\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path parent = new Path(rootBucket, \"testListStatus\");\n+    Path file1 = new Path(parent, \"key1\");\n+    Path file2 = new Path(parent, \"key2\");\n+\n+    FileStatus[] fileStatuses = ofs.listStatus(rootBucket);\n+    assertEquals(\"Should be empty\", 0, fileStatuses.length);\n+\n+    ContractTestUtils.touch(fs, file1);\n+    ContractTestUtils.touch(fs, file2);\n+\n+    // ListStatus on a directory should return all subdirs along with\n+    // files, even if there exists a file and sub-dir with the same name.\n+    fileStatuses = ofs.listStatus(parent);\n+    assertEquals(\"FileStatus did not return all children of the directory\",\n+        2, fileStatuses.length);\n+\n+    // ListStatus should return only the immediate children of a directory.\n+    Path file3 = new Path(parent, \"dir1/key3\");\n+    Path file4 = new Path(parent, \"dir1/key4\");\n+    ContractTestUtils.touch(fs, file3);\n+    ContractTestUtils.touch(fs, file4);\n+    fileStatuses = ofs.listStatus(parent);\n+    assertEquals(\"FileStatus did not return all children of the directory\",\n+        3, fileStatuses.length);\n+  }\n+\n+  private String getRandomNonExistVolumeName() throws Exception {\n+    final int numDigit = 5;\n+    long retriesLeft = Math.round(Math.pow(10, 5));\n+    String name = null;\n+    while (name == null && retriesLeft-- > 0) {\n+      name = \"volume-\" + RandomStringUtils.randomNumeric(numDigit);\n+      // Check volume existence.\n+      Iterator<? extends OzoneVolume> iter =\n+          objectStore.listVolumesByUser(null, name, null);\n+      if (iter.hasNext()) {\n+        // If there is a match, try again.\n+        // Note that volume name prefix match doesn't equal volume existence\n+        //  but the check is sufficient for this test.\n+        name = null;\n+      }\n+    }\n+    if (retriesLeft <= 0) {\n+      throw new Exception(\n+          \"Failed to generate random volume name that doesn't exist already.\");\n+    }\n+    return name;\n+  }\n+\n+  /**\n+   * Tests Mkdir operation on a volume that doesn't exist.\n+   * Expect Mkdir to create the volume and bucket.\n+   */\n+  @Test\n+  public void testMkdirOnNonExistentVolumeBucket() throws Exception {\n+    String volumeNameLocal = getRandomNonExistVolumeName();\n+    String bucketNameLocal = \"bucket-\" + RandomStringUtils.randomNumeric(5);\n+    Path root = new Path(\"/\" + volumeNameLocal + \"/\" + bucketNameLocal);\n+    Path dir1 = new Path(root, \"dir1\");\n+    Path dir12 = new Path(dir1, \"dir12\");\n+    Path dir2 = new Path(root, \"dir2\");\n+    fs.mkdirs(dir12);\n+    fs.mkdirs(dir2);\n+\n+    // Check volume and bucket existence, they should both be created.\n+    OzoneVolume ozoneVolume = objectStore.getVolume(volumeNameLocal);\n+    OzoneBucket ozoneBucket = ozoneVolume.getBucket(bucketNameLocal);\n+    OFSPath ofsPathDir1 = new OFSPath(dir12);\n+    String key = ofsPathDir1.getKeyName() + \"/\";\n+    OzoneKeyDetails ozoneKeyDetails = ozoneBucket.getKey(key);\n+    assertEquals(key, ozoneKeyDetails.getName());\n+\n+    // Verify that directories are created.\n+    FileStatus[] fileStatuses = ofs.listStatus(root);\n+    assertEquals(fileStatuses[0].getPath().toUri().getPath(), dir1.toString());\n+    assertEquals(fileStatuses[1].getPath().toUri().getPath(), dir2.toString());\n+\n+    fileStatuses = ofs.listStatus(dir1);\n+    assertEquals(fileStatuses[0].getPath().toUri().getPath(), dir12.toString());\n+    fileStatuses = ofs.listStatus(dir12);\n+    assertEquals(fileStatuses.length, 0);\n+    fileStatuses = ofs.listStatus(dir2);\n+    assertEquals(fileStatuses.length, 0);\n+  }\n+\n+  /**\n+   * Tests listStatus operation on root directory.\n+   */\n+  @Test\n+  public void testListStatusOnRoot() throws Exception {\n+    Path root = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path dir1 = new Path(root, \"dir1\");\n+    Path dir12 = new Path(dir1, \"dir12\");\n+    Path dir2 = new Path(root, \"dir2\");\n+    fs.mkdirs(dir12);\n+    fs.mkdirs(dir2);\n+\n+    // ListStatus on root should return dir1 (even though /dir1 key does not\n+    // exist) and dir2 only. dir12 is not an immediate child of root and\n+    // hence should not be listed.\n+    FileStatus[] fileStatuses = ofs.listStatus(root);\n+    assertEquals(\"FileStatus should return only the immediate children\", 2,\n+        fileStatuses.length);\n+\n+    // Verify that dir12 is not included in the result of the listStatus on root\n+    String fileStatus1 = fileStatuses[0].getPath().toUri().getPath();\n+    String fileStatus2 = fileStatuses[1].getPath().toUri().getPath();\n+    assertFalse(fileStatus1.equals(dir12.toString()));\n+    assertFalse(fileStatus2.equals(dir12.toString()));\n+  }\n+\n+  /**\n+   * Tests listStatus operation on root directory.\n+   */\n+  @Test\n+  public void testListStatusOnLargeDirectory() throws Exception {\n+    Path root = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Set<String> paths = new TreeSet<>();\n+    int numDirs = 5111;\n+    for(int i = 0; i < numDirs; i++) {\n+      Path p = new Path(root, String.valueOf(i));\n+      fs.mkdirs(p);\n+      paths.add(p.getName());\n+    }\n+\n+    FileStatus[] fileStatuses = ofs.listStatus(root);\n+    assertEquals(\n+        \"Total directories listed do not match the existing directories\",\n+        numDirs, fileStatuses.length);\n+\n+    for (int i=0; i < numDirs; i++) {\n+      assertTrue(paths.contains(fileStatuses[i].getPath().getName()));\n+    }\n+  }\n+\n+  /**\n+   * Tests listStatus on a path with subdirs.\n+   */\n+  @Test\n+  public void testListStatusOnSubDirs() throws Exception {\n+    // Create the following key structure\n+    //      /dir1/dir11/dir111\n+    //      /dir1/dir12\n+    //      /dir1/dir12/file121\n+    //      /dir2\n+    // ListStatus on /dir1 should return all its immediated subdirs only\n+    // which are /dir1/dir11 and /dir1/dir12. Super child files/dirs\n+    // (/dir1/dir12/file121 and /dir1/dir11/dir111) should not be returned by\n+    // listStatus.\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path dir1 = new Path(rootBucket, \"dir1\");\n+    Path dir11 = new Path(dir1, \"dir11\");\n+    Path dir111 = new Path(dir11, \"dir111\");\n+    Path dir12 = new Path(dir1, \"dir12\");\n+    Path file121 = new Path(dir12, \"file121\");\n+    Path dir2 = new Path(rootBucket, \"dir2\");\n+    fs.mkdirs(dir111);\n+    fs.mkdirs(dir12);\n+    ContractTestUtils.touch(fs, file121);\n+    fs.mkdirs(dir2);\n+\n+    FileStatus[] fileStatuses = ofs.listStatus(dir1);\n+    assertEquals(\"FileStatus should return only the immediate children\", 2,\n+        fileStatuses.length);\n+\n+    // Verify that the two children of /dir1 returned by listStatus operation\n+    // are /dir1/dir11 and /dir1/dir12.\n+    String fileStatus1 = fileStatuses[0].getPath().toUri().getPath();\n+    String fileStatus2 = fileStatuses[1].getPath().toUri().getPath();\n+    assertTrue(fileStatus1.equals(dir11.toString()) ||\n+        fileStatus1.equals(dir12.toString()));\n+    assertTrue(fileStatus2.equals(dir11.toString()) ||\n+        fileStatus2.equals(dir12.toString()));\n+  }\n+\n+  @Test\n+  public void testNonExplicitlyCreatedPathExistsAfterItsLeafsWereRemoved()\n+      throws Exception {\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path source = new Path(rootBucket, \"source\");\n+    Path interimPath = new Path(source, \"interimPath\");\n+    Path leafInsideInterimPath = new Path(interimPath, \"leaf\");\n+    Path target = new Path(rootBucket, \"target\");\n+    Path leafInTarget = new Path(target, \"leaf\");\n+\n+    fs.mkdirs(source);\n+    fs.mkdirs(target);\n+    fs.mkdirs(leafInsideInterimPath);\n+\n+    // Attempt to rename the key to a different bucket\n+    Path bucket2 = new Path(\"/\" + volumeName + \"/\" + bucketName + \"test\");\n+    Path leafInTargetInAnotherBucket = new Path(bucket2, \"leaf\");\n+    try {\n+      fs.rename(leafInsideInterimPath, leafInTargetInAnotherBucket);\n+      fail(\"Should have thrown exception when renaming to a different bucket\");\n+    } catch (IOException ex) {\n+      System.out.println(\"Exception thrown as expected\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdffa6ab5c1f59cedc968bf36c2ccd286cc6288f"}, "originalPosition": 378}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTMwOTE3Mw==", "bodyText": "getKeyInBucket looks like we are getting key from a specific bucket and not the global bucket. Can we rename this back to getKey?", "url": "https://github.com/apache/ozone/pull/415#discussion_r369309173", "createdAt": "2020-01-22T00:03:09Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/fs/ozone/TestRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,408 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.ozone;\n+\n+import org.apache.commons.io.IOUtils;\n+import org.apache.commons.lang3.RandomStringUtils;\n+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.contract.ContractTestUtils;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.ozone.MiniOzoneCluster;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+import org.apache.hadoop.ozone.TestDataUtil;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClientException;\n+import org.apache.hadoop.ozone.client.OzoneKeyDetails;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Set;\n+import java.util.TreeSet;\n+\n+import static org.apache.hadoop.ozone.om.OMConfigKeys.OZONE_OM_ADDRESS_KEY;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+/**\n+ * Ozone file system tests that are not covered by contract tests.\n+ */\n+public class TestRootedOzoneFileSystem {\n+\n+  @Rule\n+  public Timeout globalTimeout = new Timeout(300_000);\n+\n+  private static MiniOzoneCluster cluster = null;\n+\n+  private static FileSystem fs;\n+  private static RootedOzoneFileSystem ofs;\n+\n+  private static ObjectStore objectStore;\n+\n+  private String volumeName;\n+  private String bucketName;\n+\n+  private String rootPath;\n+\n+  @Before\n+  public void init() throws Exception {\n+    OzoneConfiguration conf = new OzoneConfiguration();\n+    cluster = MiniOzoneCluster.newBuilder(conf)\n+        .setNumDatanodes(3)\n+        .build();\n+    cluster.waitForClusterToBeReady();\n+    objectStore = cluster.getClient().getObjectStore();\n+\n+    // create a volume and a bucket to be used by OFileSystem\n+    OzoneBucket bucket = TestDataUtil.createVolumeAndBucket(cluster);\n+    volumeName = bucket.getVolumeName();\n+    bucketName = bucket.getName();\n+\n+    // For now:\n+    rootPath = String.format(\"%s://%s/\", OzoneConsts.OZONE_OFS_URI_SCHEME,\n+        conf.get(OZONE_OM_ADDRESS_KEY));\n+\n+    // Set the fs.defaultFS and start the filesystem\n+    conf.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, rootPath);\n+    // Note: FileSystem#loadFileSystems won't load OFS class due to META-INF\n+    //  hence this workaround.\n+    conf.set(\"fs.ofs.impl\", \"org.apache.hadoop.fs.ozone.RootedOzoneFileSystem\");\n+    fs = FileSystem.get(conf);\n+    ofs = (RootedOzoneFileSystem) fs;\n+  }\n+\n+  @After\n+  public void teardown() throws IOException {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n+    IOUtils.closeQuietly(fs);\n+  }\n+\n+  @Test\n+  public void testOzoneFsServiceLoader() throws IOException {\n+    OzoneConfiguration conf = new OzoneConfiguration();\n+    // Note: FileSystem#loadFileSystems won't load OFS class due to META-INF\n+    //  hence this workaround.\n+    conf.set(\"fs.ofs.impl\", \"org.apache.hadoop.fs.ozone.RootedOzoneFileSystem\");\n+    assertEquals(\n+        FileSystem.getFileSystemClass(OzoneConsts.OZONE_OFS_URI_SCHEME, conf),\n+        RootedOzoneFileSystem.class);\n+  }\n+\n+  @Test\n+  public void testCreateDoesNotAddParentDirKeys() throws Exception {\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path grandparent = new Path(rootBucket,\n+        \"testCreateDoesNotAddParentDirKeys\");\n+    Path parent = new Path(grandparent, \"parent\");\n+    Path child = new Path(parent, \"child\");\n+    ContractTestUtils.touch(fs, child);\n+\n+    OzoneKeyDetails key = getKeyInBucket(child, false);\n+    OFSPath childOFSPath = new OFSPath(child);\n+    assertEquals(key.getName(), childOFSPath.getKeyName());\n+\n+    // Creating a child should not add parent keys to the bucket\n+    try {\n+      getKeyInBucket(parent, true);\n+    } catch (IOException ex) {\n+      assertKeyNotFoundException(ex);\n+    }\n+\n+    // List status on the parent should show the child file\n+    assertEquals(\"List status of parent should include the 1 child file\", 1L,\n+        (long)fs.listStatus(parent).length);\n+    assertTrue(\"Parent directory does not appear to be a directory\",\n+        fs.getFileStatus(parent).isDirectory());\n+  }\n+\n+  @Test\n+  public void testDeleteCreatesFakeParentDir() throws Exception {\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path grandparent = new Path(rootBucket, \"testDeleteCreatesFakeParentDir\");\n+    Path parent = new Path(grandparent, \"parent\");\n+    Path child = new Path(parent, \"child\");\n+    ContractTestUtils.touch(fs, child);\n+\n+    // Verify that parent dir key does not exist\n+    // Creating a child should not add parent keys to the bucket\n+    try {\n+      getKeyInBucket(parent, true);\n+    } catch (IOException ex) {\n+      assertKeyNotFoundException(ex);\n+    }\n+\n+    // Delete the child key\n+    assertTrue(fs.delete(child, false));\n+\n+    // Deleting the only child should create the parent dir key if it does\n+    // not exist\n+    OFSPath parentOFSPath = new OFSPath(parent);\n+    String parentKey = parentOFSPath.getKeyName() + \"/\";\n+    OzoneKeyDetails parentKeyInfo = getKeyInBucket(parent, true);\n+    assertEquals(parentKey, parentKeyInfo.getName());\n+\n+    // Recursive delete with DeleteIterator\n+    assertTrue(fs.delete(grandparent, true));\n+  }\n+\n+  @Test\n+  public void testListStatus() throws Exception {\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path parent = new Path(rootBucket, \"testListStatus\");\n+    Path file1 = new Path(parent, \"key1\");\n+    Path file2 = new Path(parent, \"key2\");\n+\n+    FileStatus[] fileStatuses = ofs.listStatus(rootBucket);\n+    assertEquals(\"Should be empty\", 0, fileStatuses.length);\n+\n+    ContractTestUtils.touch(fs, file1);\n+    ContractTestUtils.touch(fs, file2);\n+\n+    // ListStatus on a directory should return all subdirs along with\n+    // files, even if there exists a file and sub-dir with the same name.\n+    fileStatuses = ofs.listStatus(parent);\n+    assertEquals(\"FileStatus did not return all children of the directory\",\n+        2, fileStatuses.length);\n+\n+    // ListStatus should return only the immediate children of a directory.\n+    Path file3 = new Path(parent, \"dir1/key3\");\n+    Path file4 = new Path(parent, \"dir1/key4\");\n+    ContractTestUtils.touch(fs, file3);\n+    ContractTestUtils.touch(fs, file4);\n+    fileStatuses = ofs.listStatus(parent);\n+    assertEquals(\"FileStatus did not return all children of the directory\",\n+        3, fileStatuses.length);\n+  }\n+\n+  private String getRandomNonExistVolumeName() throws Exception {\n+    final int numDigit = 5;\n+    long retriesLeft = Math.round(Math.pow(10, 5));\n+    String name = null;\n+    while (name == null && retriesLeft-- > 0) {\n+      name = \"volume-\" + RandomStringUtils.randomNumeric(numDigit);\n+      // Check volume existence.\n+      Iterator<? extends OzoneVolume> iter =\n+          objectStore.listVolumesByUser(null, name, null);\n+      if (iter.hasNext()) {\n+        // If there is a match, try again.\n+        // Note that volume name prefix match doesn't equal volume existence\n+        //  but the check is sufficient for this test.\n+        name = null;\n+      }\n+    }\n+    if (retriesLeft <= 0) {\n+      throw new Exception(\n+          \"Failed to generate random volume name that doesn't exist already.\");\n+    }\n+    return name;\n+  }\n+\n+  /**\n+   * Tests Mkdir operation on a volume that doesn't exist.\n+   * Expect Mkdir to create the volume and bucket.\n+   */\n+  @Test\n+  public void testMkdirOnNonExistentVolumeBucket() throws Exception {\n+    String volumeNameLocal = getRandomNonExistVolumeName();\n+    String bucketNameLocal = \"bucket-\" + RandomStringUtils.randomNumeric(5);\n+    Path root = new Path(\"/\" + volumeNameLocal + \"/\" + bucketNameLocal);\n+    Path dir1 = new Path(root, \"dir1\");\n+    Path dir12 = new Path(dir1, \"dir12\");\n+    Path dir2 = new Path(root, \"dir2\");\n+    fs.mkdirs(dir12);\n+    fs.mkdirs(dir2);\n+\n+    // Check volume and bucket existence, they should both be created.\n+    OzoneVolume ozoneVolume = objectStore.getVolume(volumeNameLocal);\n+    OzoneBucket ozoneBucket = ozoneVolume.getBucket(bucketNameLocal);\n+    OFSPath ofsPathDir1 = new OFSPath(dir12);\n+    String key = ofsPathDir1.getKeyName() + \"/\";\n+    OzoneKeyDetails ozoneKeyDetails = ozoneBucket.getKey(key);\n+    assertEquals(key, ozoneKeyDetails.getName());\n+\n+    // Verify that directories are created.\n+    FileStatus[] fileStatuses = ofs.listStatus(root);\n+    assertEquals(fileStatuses[0].getPath().toUri().getPath(), dir1.toString());\n+    assertEquals(fileStatuses[1].getPath().toUri().getPath(), dir2.toString());\n+\n+    fileStatuses = ofs.listStatus(dir1);\n+    assertEquals(fileStatuses[0].getPath().toUri().getPath(), dir12.toString());\n+    fileStatuses = ofs.listStatus(dir12);\n+    assertEquals(fileStatuses.length, 0);\n+    fileStatuses = ofs.listStatus(dir2);\n+    assertEquals(fileStatuses.length, 0);\n+  }\n+\n+  /**\n+   * Tests listStatus operation on root directory.\n+   */\n+  @Test\n+  public void testListStatusOnRoot() throws Exception {\n+    Path root = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path dir1 = new Path(root, \"dir1\");\n+    Path dir12 = new Path(dir1, \"dir12\");\n+    Path dir2 = new Path(root, \"dir2\");\n+    fs.mkdirs(dir12);\n+    fs.mkdirs(dir2);\n+\n+    // ListStatus on root should return dir1 (even though /dir1 key does not\n+    // exist) and dir2 only. dir12 is not an immediate child of root and\n+    // hence should not be listed.\n+    FileStatus[] fileStatuses = ofs.listStatus(root);\n+    assertEquals(\"FileStatus should return only the immediate children\", 2,\n+        fileStatuses.length);\n+\n+    // Verify that dir12 is not included in the result of the listStatus on root\n+    String fileStatus1 = fileStatuses[0].getPath().toUri().getPath();\n+    String fileStatus2 = fileStatuses[1].getPath().toUri().getPath();\n+    assertFalse(fileStatus1.equals(dir12.toString()));\n+    assertFalse(fileStatus2.equals(dir12.toString()));\n+  }\n+\n+  /**\n+   * Tests listStatus operation on root directory.\n+   */\n+  @Test\n+  public void testListStatusOnLargeDirectory() throws Exception {\n+    Path root = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Set<String> paths = new TreeSet<>();\n+    int numDirs = 5111;\n+    for(int i = 0; i < numDirs; i++) {\n+      Path p = new Path(root, String.valueOf(i));\n+      fs.mkdirs(p);\n+      paths.add(p.getName());\n+    }\n+\n+    FileStatus[] fileStatuses = ofs.listStatus(root);\n+    assertEquals(\n+        \"Total directories listed do not match the existing directories\",\n+        numDirs, fileStatuses.length);\n+\n+    for (int i=0; i < numDirs; i++) {\n+      assertTrue(paths.contains(fileStatuses[i].getPath().getName()));\n+    }\n+  }\n+\n+  /**\n+   * Tests listStatus on a path with subdirs.\n+   */\n+  @Test\n+  public void testListStatusOnSubDirs() throws Exception {\n+    // Create the following key structure\n+    //      /dir1/dir11/dir111\n+    //      /dir1/dir12\n+    //      /dir1/dir12/file121\n+    //      /dir2\n+    // ListStatus on /dir1 should return all its immediated subdirs only\n+    // which are /dir1/dir11 and /dir1/dir12. Super child files/dirs\n+    // (/dir1/dir12/file121 and /dir1/dir11/dir111) should not be returned by\n+    // listStatus.\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path dir1 = new Path(rootBucket, \"dir1\");\n+    Path dir11 = new Path(dir1, \"dir11\");\n+    Path dir111 = new Path(dir11, \"dir111\");\n+    Path dir12 = new Path(dir1, \"dir12\");\n+    Path file121 = new Path(dir12, \"file121\");\n+    Path dir2 = new Path(rootBucket, \"dir2\");\n+    fs.mkdirs(dir111);\n+    fs.mkdirs(dir12);\n+    ContractTestUtils.touch(fs, file121);\n+    fs.mkdirs(dir2);\n+\n+    FileStatus[] fileStatuses = ofs.listStatus(dir1);\n+    assertEquals(\"FileStatus should return only the immediate children\", 2,\n+        fileStatuses.length);\n+\n+    // Verify that the two children of /dir1 returned by listStatus operation\n+    // are /dir1/dir11 and /dir1/dir12.\n+    String fileStatus1 = fileStatuses[0].getPath().toUri().getPath();\n+    String fileStatus2 = fileStatuses[1].getPath().toUri().getPath();\n+    assertTrue(fileStatus1.equals(dir11.toString()) ||\n+        fileStatus1.equals(dir12.toString()));\n+    assertTrue(fileStatus2.equals(dir11.toString()) ||\n+        fileStatus2.equals(dir12.toString()));\n+  }\n+\n+  @Test\n+  public void testNonExplicitlyCreatedPathExistsAfterItsLeafsWereRemoved()\n+      throws Exception {\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path source = new Path(rootBucket, \"source\");\n+    Path interimPath = new Path(source, \"interimPath\");\n+    Path leafInsideInterimPath = new Path(interimPath, \"leaf\");\n+    Path target = new Path(rootBucket, \"target\");\n+    Path leafInTarget = new Path(target, \"leaf\");\n+\n+    fs.mkdirs(source);\n+    fs.mkdirs(target);\n+    fs.mkdirs(leafInsideInterimPath);\n+\n+    // Attempt to rename the key to a different bucket\n+    Path bucket2 = new Path(\"/\" + volumeName + \"/\" + bucketName + \"test\");\n+    Path leafInTargetInAnotherBucket = new Path(bucket2, \"leaf\");\n+    try {\n+      fs.rename(leafInsideInterimPath, leafInTargetInAnotherBucket);\n+      fail(\"Should have thrown exception when renaming to a different bucket\");\n+    } catch (IOException ex) {\n+      System.out.println(\"Exception thrown as expected\");\n+    }\n+\n+    assertTrue(fs.rename(leafInsideInterimPath, leafInTarget));\n+\n+    // after rename listStatus for interimPath should succeed and\n+    // interimPath should have no children\n+    FileStatus[] statuses = fs.listStatus(interimPath);\n+    assertNotNull(\"liststatus returns a null array\", statuses);\n+    assertEquals(\"Statuses array is not empty\", 0, statuses.length);\n+    FileStatus fileStatus = fs.getFileStatus(interimPath);\n+    assertEquals(\"FileStatus does not point to interimPath\",\n+        interimPath.getName(), fileStatus.getPath().getName());\n+  }\n+\n+  private OzoneKeyDetails getKeyInBucket(Path keyPath, boolean isDirectory)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdffa6ab5c1f59cedc968bf36c2ccd286cc6288f"}, "originalPosition": 393}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTMyNTQ4NA==", "bodyText": "Do we want to convert startPath also to ofsPath here? Most likely, the startPath would only have the keyName prefix as the full path with volume and bucket is already given in pathStr.\nI am ok either way but we should add JavaDocs to explain this. Otherwise, user may assume it to be keyName prefix but we take it as volume/bucket/key or vice versa.\nIt would be good to add JavaDoc to all the methods. Atleast all the methods in OzoneClientAdapter should have a JavaDoc as they are client facing.", "url": "https://github.com/apache/ozone/pull/415#discussion_r369325484", "createdAt": "2020-01-22T01:06:25Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneClientAdapterImpl.java", "diffHunk": "@@ -0,0 +1,604 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import org.apache.commons.collections.CollectionUtils;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.crypto.key.KeyProvider;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdds.client.ReplicationFactor;\n+import org.apache.hadoop.hdds.client.ReplicationType;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.security.x509.SecurityConfig;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.ozone.OmUtils;\n+import org.apache.hadoop.ozone.OzoneConfigKeys;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClient;\n+import org.apache.hadoop.ozone.client.OzoneClientFactory;\n+import org.apache.hadoop.ozone.client.OzoneKey;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.ozone.client.io.OzoneOutputStream;\n+import org.apache.hadoop.ozone.client.protocol.ClientProtocol;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfoGroup;\n+import org.apache.hadoop.ozone.om.helpers.OzoneFileStatus;\n+import org.apache.hadoop.ozone.security.OzoneTokenIdentifier;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.security.token.TokenRenewer;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .VOLUME_NOT_FOUND;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .BUCKET_NOT_FOUND;\n+\n+/**\n+ * Basic Implementation of the OzoneFileSystem calls.\n+ * <p>\n+ * This is the minimal version which doesn't include any statistics.\n+ * <p>\n+ * For full featured version use OzoneClientAdapterImpl.\n+ */\n+public class BasicRootedOzoneClientAdapterImpl\n+    implements RootedOzoneClientAdapter {\n+\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneClientAdapterImpl.class);\n+\n+  private OzoneClient ozoneClient;\n+  private ClientProtocol proxy;\n+  private ObjectStore objectStore;\n+  private ReplicationType replicationType;\n+  private ReplicationFactor replicationFactor;\n+  private boolean securityEnabled;\n+  private int configuredDnPort;\n+\n+  /**\n+   * Create new OzoneClientAdapter implementation.\n+   *\n+   * @throws IOException In case of a problem.\n+   */\n+  public BasicRootedOzoneClientAdapterImpl() throws IOException {\n+    this(createConf());\n+  }\n+\n+  private static OzoneConfiguration createConf() {\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+    try {\n+      return new OzoneConfiguration();\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(OzoneConfiguration conf)\n+      throws IOException {\n+    this(null, -1, conf);\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(String omHost, int omPort,\n+      Configuration hadoopConf) throws IOException {\n+\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+\n+    try {\n+      OzoneConfiguration conf = OzoneConfiguration.of(hadoopConf);\n+\n+      if (omHost == null && OmUtils.isServiceIdsDefined(conf)) {\n+        // When the host name or service id isn't given\n+        // but ozone.om.service.ids is defined, declare failure.\n+\n+        // This is a safety precaution that prevents the client from\n+        // accidentally failing over to an unintended OM.\n+        throw new IllegalArgumentException(\"Service ID or host name must not\"\n+            + \" be omitted when ozone.om.service.ids is defined.\");\n+      }\n+\n+      if (omPort != -1) {\n+        // When the port number is specified, perform the following check\n+        if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+          // If omHost is a service id, it shouldn't use a port\n+          throw new IllegalArgumentException(\"Port \" + omPort +\n+              \" specified in URI but host '\" + omHost + \"' is a \"\n+              + \"logical (HA) OzoneManager and does not use port information.\");\n+        }\n+      } else {\n+        // When port number is not specified, read it from config\n+        omPort = OmUtils.getOmRpcPort(conf);\n+      }\n+\n+      SecurityConfig secConfig = new SecurityConfig(conf);\n+\n+      if (secConfig.isSecurityEnabled()) {\n+        this.securityEnabled = true;\n+      }\n+\n+      String replicationTypeConf =\n+          conf.get(OzoneConfigKeys.OZONE_REPLICATION_TYPE,\n+              OzoneConfigKeys.OZONE_REPLICATION_TYPE_DEFAULT);\n+\n+      int replicationCountConf = conf.getInt(OzoneConfigKeys.OZONE_REPLICATION,\n+          OzoneConfigKeys.OZONE_REPLICATION_DEFAULT);\n+\n+      if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+        // omHost is listed as one of the service ids in the config,\n+        // thus we should treat omHost as omServiceId\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, conf);\n+      } else if (StringUtils.isNotEmpty(omHost) && omPort != -1) {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, omPort, conf);\n+      } else {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(conf);\n+      }\n+      objectStore = ozoneClient.getObjectStore();\n+      proxy = objectStore.getClientProxy();\n+      this.replicationType = ReplicationType.valueOf(replicationTypeConf);\n+      this.replicationFactor = ReplicationFactor.valueOf(replicationCountConf);\n+      this.configuredDnPort = conf.getInt(\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT,\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT_DEFAULT);\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  private OzoneBucket getBucket(OFSPath ofsPath,\n+      boolean createIfNotExist) throws IOException {\n+\n+    return getBucket(ofsPath.getVolumeName(), ofsPath.getBucketName(),\n+        createIfNotExist);\n+  }\n+\n+  /**\n+   * Get OzoneBucket object to operate in.\n+   * Optionally create volume and bucket if not found.\n+   *\n+   * @param createIfNotExist Set this to true if the caller is a write operation\n+   *                         in order to create the volume and bucket.\n+   * @throws IOException Exceptions other than OMException with result code\n+   *                     VOLUME_NOT_FOUND or BUCKET_NOT_FOUND.\n+   */\n+  private OzoneBucket getBucket(String volumeStr, String bucketStr,\n+      boolean createIfNotExist) throws IOException {\n+\n+    OzoneBucket bucket;\n+    try {\n+      bucket = proxy.getBucketDetails(volumeStr, bucketStr);\n+    } catch (OMException ex) {\n+      if (createIfNotExist) {\n+        // Note: getBucketDetails always throws BUCKET_NOT_FOUND, even if\n+        // the volume doesn't exist.\n+        if (ex.getResult().equals(BUCKET_NOT_FOUND)) {\n+          OzoneVolume volume;\n+          try {\n+            volume = proxy.getVolumeDetails(volumeStr);\n+          } catch (OMException volEx) {\n+            if (volEx.getResult().equals(VOLUME_NOT_FOUND)) {\n+              // Volume doesn't exist. Create it\n+              objectStore.createVolume(volumeStr);\n+            } else {\n+              throw volEx;\n+            }\n+            // Try get volume again\n+            volume = proxy.getVolumeDetails(volumeStr);\n+          }\n+          // Create the bucket\n+          volume.createBucket(bucketStr);\n+        }\n+        // Try get bucket again\n+        bucket = proxy.getBucketDetails(volumeStr, bucketStr);\n+      } else {\n+        throw ex;\n+      }\n+    }\n+\n+    return bucket;\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    ozoneClient.close();\n+  }\n+\n+  @Override\n+  public InputStream readFile(String pathStr) throws IOException {\n+    incrementCounter(Statistic.OBJECTS_READ);\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    String key = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, false);\n+      return bucket.readFile(key).getInputStream();\n+    } catch (OMException ex) {\n+      if (ex.getResult() == OMException.ResultCodes.FILE_NOT_FOUND\n+          || ex.getResult() == OMException.ResultCodes.NOT_A_FILE) {\n+        throw new FileNotFoundException(\n+            ex.getResult().name() + \": \" + ex.getMessage());\n+      } else {\n+        throw ex;\n+      }\n+    }\n+  }\n+\n+  protected void incrementCounter(Statistic objectsRead) {\n+    //noop: Use OzoneClientAdapterImpl which supports statistics.\n+  }\n+\n+  @Override\n+  public OzoneFSOutputStream createFile(String pathStr, boolean overWrite,\n+      boolean recursive) throws IOException {\n+    incrementCounter(Statistic.OBJECTS_CREATED);\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    String key = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, false);\n+      OzoneOutputStream ozoneOutputStream = bucket.createFile(\n+          key, 0, replicationType, replicationFactor, overWrite, recursive);\n+      return new OzoneFSOutputStream(ozoneOutputStream.getOutputStream());\n+    } catch (OMException ex) {\n+      if (ex.getResult() == OMException.ResultCodes.FILE_ALREADY_EXISTS\n+          || ex.getResult() == OMException.ResultCodes.NOT_A_FILE) {\n+        throw new FileAlreadyExistsException(\n+            ex.getResult().name() + \": \" + ex.getMessage());\n+      } else {\n+        throw ex;\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void renamePath(String path, String newPath) throws IOException {\n+    incrementCounter(Statistic.OBJECTS_RENAMED);\n+    OFSPath ofsPath = new OFSPath(path);\n+    OFSPath ofsNewPath = new OFSPath(newPath);\n+\n+    // Check path and newPathName are in the same volume and same bucket.\n+    // This should have been checked in BasicRootedOzoneFileSystem#rename\n+    // already via regular call path unless bypassed.\n+    if (!ofsPath.isInSameBucketAs(ofsNewPath)) {\n+      throw new IOException(\"Cannot rename a key to a different bucket\");\n+    }\n+\n+    OzoneBucket bucket = getBucket(ofsPath, false);\n+    String key = ofsPath.getKeyName();\n+    String newKey = ofsNewPath.getKeyName();\n+    bucket.renameKey(key, newKey);\n+  }\n+\n+  /**\n+   * Helper method to create an directory specified by key name in bucket.\n+   *\n+   * @param pathStr path to be created as directory\n+   * @return true if the key is created, false otherwise\n+   */\n+  @Override\n+  public boolean createDirectory(String pathStr) throws IOException {\n+    LOG.trace(\"creating dir for path: {}\", pathStr);\n+    incrementCounter(Statistic.OBJECTS_CREATED);\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    String keyStr = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, true);\n+      bucket.createDirectory(keyStr);\n+    } catch (OMException e) {\n+      if (e.getResult() == OMException.ResultCodes.FILE_ALREADY_EXISTS) {\n+        throw new FileAlreadyExistsException(e.getMessage());\n+      }\n+      throw e;\n+    }\n+    return true;\n+  }\n+\n+  /**\n+   * Helper method to delete an object specified by key name in bucket.\n+   *\n+   * @param path path to a key to be deleted\n+   * @return true if the key is deleted, false otherwise\n+   */\n+  @Override\n+  public boolean deleteObject(String path) {\n+    LOG.trace(\"issuing delete for path to key: {}\", path);\n+    OFSPath ofsPath = new OFSPath(path);\n+    String keyName = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, false);\n+      incrementCounter(Statistic.OBJECTS_DELETED);\n+      bucket.deleteKey(keyName);\n+      return true;\n+    } catch (IOException ioe) {\n+      LOG.error(\"delete key failed \" + ioe.getMessage());\n+      return false;\n+    }\n+  }\n+\n+  public FileStatusAdapter getFileStatus(String path, URI uri,\n+      Path qualifiedPath, String userName)\n+      throws IOException {\n+    OFSPath ofsPath = new OFSPath(path);\n+    String key = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, false);\n+      incrementCounter(Statistic.OBJECTS_QUERY);\n+      OzoneFileStatus status = bucket.getFileStatus(key);\n+      // Note: qualifiedPath passed in is good from\n+      //  BasicRootedOzoneFileSystem#getFileStatus. No need to prepend here.\n+      makeQualified(status, uri, qualifiedPath, userName);\n+      return toFileStatusAdapter(status);\n+\n+    } catch (OMException e) {\n+      if (e.getResult() == OMException.ResultCodes.FILE_NOT_FOUND) {\n+        throw new\n+            FileNotFoundException(key + \": No such file or directory!\");\n+      }\n+      throw e;\n+    }\n+  }\n+\n+  public void makeQualified(FileStatus status, URI uri, Path path,\n+      String username) {\n+    if (status instanceof OzoneFileStatus) {\n+      ((OzoneFileStatus) status)\n+          .makeQualified(uri, path,\n+              username, username);\n+    }\n+  }\n+\n+  @Override\n+  public Iterator<BasicKeyInfo> listKeys(String pathStr) throws IOException {\n+    incrementCounter(Statistic.OBJECTS_LIST);\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    String key = ofsPath.getKeyName();\n+    OzoneBucket bucket = getBucket(ofsPath, false);\n+    return new IteratorAdapter(bucket.listKeys(key));\n+  }\n+\n+  public List<FileStatusAdapter> listStatus(String pathStr, boolean recursive,\n+      String startPath, long numEntries, URI uri,\n+      Path workingDir, String username) throws IOException {\n+\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    String keyName = ofsPath.getKeyName();\n+    OFSPath ofsStartPath = new OFSPath(startPath);\n+    String startKey = ofsStartPath.getKeyName();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdffa6ab5c1f59cedc968bf36c2ccd286cc6288f"}, "originalPosition": 402}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTMyODE5NA==", "bodyText": "Commented variables", "url": "https://github.com/apache/ozone/pull/415#discussion_r369328194", "createdAt": "2020-01-22T01:17:48Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,805 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import com.google.common.base.Preconditions;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.CreateFlag;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocatedFileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PathIsNotEmptyDirectoryException;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.util.Progressable;\n+import org.apache.http.client.utils.URIBuilder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.EnumSet;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hadoop.fs.ozone.Constants.LISTING_PAGE_SIZE;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_DEFAULT_USER;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_USER_DIR;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_URI_DELIMITER;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_OFS_URI_SCHEME;\n+\n+/**\n+ * The minimal Ozone Filesystem implementation.\n+ * <p>\n+ * This is a basic version which doesn't extend\n+ * KeyProviderTokenIssuer and doesn't include statistics. It can be used\n+ * from older hadoop version. For newer hadoop version use the full featured\n+ * OFileSystem.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Evolving\n+public class BasicRootedOzoneFileSystem extends FileSystem {\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneFileSystem.class);\n+\n+  /**\n+   * The Ozone client for connecting to Ozone server.\n+   */\n+\n+  private URI uri;\n+  private String userName;\n+  private Path workingDir;\n+//  private String gOmHost;\n+//  private int gOmPort;\n+//  private Configuration gConf;\n+//  private boolean gIsolatedClassloader;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdffa6ab5c1f59cedc968bf36c2ccd286cc6288f"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTMyODMxMA==", "bodyText": "OFileSystem -> RootedOzoneFileSystem", "url": "https://github.com/apache/ozone/pull/415#discussion_r369328310", "createdAt": "2020-01-22T01:18:18Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,805 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import com.google.common.base.Preconditions;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.CreateFlag;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocatedFileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PathIsNotEmptyDirectoryException;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.util.Progressable;\n+import org.apache.http.client.utils.URIBuilder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.EnumSet;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hadoop.fs.ozone.Constants.LISTING_PAGE_SIZE;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_DEFAULT_USER;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_USER_DIR;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_URI_DELIMITER;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_OFS_URI_SCHEME;\n+\n+/**\n+ * The minimal Ozone Filesystem implementation.\n+ * <p>\n+ * This is a basic version which doesn't extend\n+ * KeyProviderTokenIssuer and doesn't include statistics. It can be used\n+ * from older hadoop version. For newer hadoop version use the full featured\n+ * OFileSystem.\n+ */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdffa6ab5c1f59cedc968bf36c2ccd286cc6288f"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTMyODczNw==", "bodyText": "The URL can also have the path, right?", "url": "https://github.com/apache/ozone/pull/415#discussion_r369328737", "createdAt": "2020-01-22T01:20:05Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,805 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import com.google.common.base.Preconditions;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.CreateFlag;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocatedFileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PathIsNotEmptyDirectoryException;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.util.Progressable;\n+import org.apache.http.client.utils.URIBuilder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.EnumSet;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hadoop.fs.ozone.Constants.LISTING_PAGE_SIZE;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_DEFAULT_USER;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_USER_DIR;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_URI_DELIMITER;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_OFS_URI_SCHEME;\n+\n+/**\n+ * The minimal Ozone Filesystem implementation.\n+ * <p>\n+ * This is a basic version which doesn't extend\n+ * KeyProviderTokenIssuer and doesn't include statistics. It can be used\n+ * from older hadoop version. For newer hadoop version use the full featured\n+ * OFileSystem.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Evolving\n+public class BasicRootedOzoneFileSystem extends FileSystem {\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneFileSystem.class);\n+\n+  /**\n+   * The Ozone client for connecting to Ozone server.\n+   */\n+\n+  private URI uri;\n+  private String userName;\n+  private Path workingDir;\n+//  private String gOmHost;\n+//  private int gOmPort;\n+//  private Configuration gConf;\n+//  private boolean gIsolatedClassloader;\n+\n+  private RootedOzoneClientAdapter adapter;\n+//  private String adapterPath;\n+\n+  private static final String URI_EXCEPTION_TEXT =\n+      \"URL should be one of the following formats: \" +\n+      \"ofs://om-service-id/  OR \" +\n+      \"ofs://om-host.example.com/  OR \" +\n+      \"ofs://om-host.example.com:5678/\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdffa6ab5c1f59cedc968bf36c2ccd286cc6288f"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTMzMDIwMQ==", "bodyText": "We are assigning serviceID also to omHost variable. Can we either change the variable name to reflect this or add a comment explaining this.", "url": "https://github.com/apache/ozone/pull/415#discussion_r369330201", "createdAt": "2020-01-22T01:26:32Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,805 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import com.google.common.base.Preconditions;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.CreateFlag;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocatedFileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PathIsNotEmptyDirectoryException;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.util.Progressable;\n+import org.apache.http.client.utils.URIBuilder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.EnumSet;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hadoop.fs.ozone.Constants.LISTING_PAGE_SIZE;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_DEFAULT_USER;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_USER_DIR;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_URI_DELIMITER;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_OFS_URI_SCHEME;\n+\n+/**\n+ * The minimal Ozone Filesystem implementation.\n+ * <p>\n+ * This is a basic version which doesn't extend\n+ * KeyProviderTokenIssuer and doesn't include statistics. It can be used\n+ * from older hadoop version. For newer hadoop version use the full featured\n+ * OFileSystem.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Evolving\n+public class BasicRootedOzoneFileSystem extends FileSystem {\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneFileSystem.class);\n+\n+  /**\n+   * The Ozone client for connecting to Ozone server.\n+   */\n+\n+  private URI uri;\n+  private String userName;\n+  private Path workingDir;\n+//  private String gOmHost;\n+//  private int gOmPort;\n+//  private Configuration gConf;\n+//  private boolean gIsolatedClassloader;\n+\n+  private RootedOzoneClientAdapter adapter;\n+//  private String adapterPath;\n+\n+  private static final String URI_EXCEPTION_TEXT =\n+      \"URL should be one of the following formats: \" +\n+      \"ofs://om-service-id/  OR \" +\n+      \"ofs://om-host.example.com/  OR \" +\n+      \"ofs://om-host.example.com:5678/\";\n+\n+  @Override\n+  public void initialize(URI name, Configuration conf) throws IOException {\n+    super.initialize(name, conf);\n+    setConf(conf);\n+    Objects.requireNonNull(name.getScheme(), \"No scheme provided in \" + name);\n+    Preconditions.checkArgument(getScheme().equals(name.getScheme()),\n+        \"Invalid scheme provided in \" + name);\n+\n+    String authority = name.getAuthority();\n+    if (authority == null) {\n+      // authority is null when fs.defaultFS is not a qualified ofs URI and\n+      // ofs:/// is passed to the client. matcher will NPE if authority is null\n+      throw new IllegalArgumentException(URI_EXCEPTION_TEXT);\n+    }\n+\n+    String omHost;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdffa6ab5c1f59cedc968bf36c2ccd286cc6288f"}, "originalPosition": 110}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc5NTM5MA==", "bodyText": "I am confused about pathToKey() functionality. If the input path is not an absolute path, it prepends the working dir (/user/username/) to the path. Please correct me if my understanding is wrong.\nFor o3fs, if the input key is lets say \"dir1/dir2/key1\", then pathToKey would be /user/username/dir1/dir2/key1. The volume and bucket would have been set already while creating the FS object.\nThe same example with ofs would result in volume being set as \"user\" and bucket as \"username\" and key as \"dir1/dir2/key1\". Here, we are making an assumption of the volume and bucket. Would it not be better to not allow non-absolute paths in ofs?", "url": "https://github.com/apache/ozone/pull/415#discussion_r369795390", "createdAt": "2020-01-22T20:50:29Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,805 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import com.google.common.base.Preconditions;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.CreateFlag;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocatedFileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PathIsNotEmptyDirectoryException;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.util.Progressable;\n+import org.apache.http.client.utils.URIBuilder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.EnumSet;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hadoop.fs.ozone.Constants.LISTING_PAGE_SIZE;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_DEFAULT_USER;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_USER_DIR;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_URI_DELIMITER;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_OFS_URI_SCHEME;\n+\n+/**\n+ * The minimal Ozone Filesystem implementation.\n+ * <p>\n+ * This is a basic version which doesn't extend\n+ * KeyProviderTokenIssuer and doesn't include statistics. It can be used\n+ * from older hadoop version. For newer hadoop version use the full featured\n+ * OFileSystem.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Evolving\n+public class BasicRootedOzoneFileSystem extends FileSystem {\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneFileSystem.class);\n+\n+  /**\n+   * The Ozone client for connecting to Ozone server.\n+   */\n+\n+  private URI uri;\n+  private String userName;\n+  private Path workingDir;\n+//  private String gOmHost;\n+//  private int gOmPort;\n+//  private Configuration gConf;\n+//  private boolean gIsolatedClassloader;\n+\n+  private RootedOzoneClientAdapter adapter;\n+//  private String adapterPath;\n+\n+  private static final String URI_EXCEPTION_TEXT =\n+      \"URL should be one of the following formats: \" +\n+      \"ofs://om-service-id/  OR \" +\n+      \"ofs://om-host.example.com/  OR \" +\n+      \"ofs://om-host.example.com:5678/\";\n+\n+  @Override\n+  public void initialize(URI name, Configuration conf) throws IOException {\n+    super.initialize(name, conf);\n+    setConf(conf);\n+    Objects.requireNonNull(name.getScheme(), \"No scheme provided in \" + name);\n+    Preconditions.checkArgument(getScheme().equals(name.getScheme()),\n+        \"Invalid scheme provided in \" + name);\n+\n+    String authority = name.getAuthority();\n+    if (authority == null) {\n+      // authority is null when fs.defaultFS is not a qualified ofs URI and\n+      // ofs:/// is passed to the client. matcher will NPE if authority is null\n+      throw new IllegalArgumentException(URI_EXCEPTION_TEXT);\n+    }\n+\n+    String omHost;\n+    int omPort = -1;\n+    // Parse hostname and port\n+    String[] parts = authority.split(\":\");\n+    if (parts.length > 2) {\n+      throw new IllegalArgumentException(URI_EXCEPTION_TEXT);\n+    }\n+    omHost = parts[0];\n+    if (parts.length == 2) {\n+      try {\n+        omPort = Integer.parseInt(parts[1]);\n+      } catch (NumberFormatException e) {\n+        throw new IllegalArgumentException(URI_EXCEPTION_TEXT);\n+      }\n+    }\n+\n+    try {\n+      uri = new URIBuilder().setScheme(OZONE_OFS_URI_SCHEME)\n+          .setHost(authority)\n+          .build();\n+      LOG.trace(\"Ozone URI for OFS initialization is \" + uri);\n+\n+      //isolated is the default for ozonefs-lib-legacy which includes the\n+      // /ozonefs.txt, otherwise the default is false. It could be overridden.\n+      boolean defaultValue =\n+          BasicRootedOzoneFileSystem.class.getClassLoader()\n+              .getResource(\"ozonefs.txt\") != null;\n+\n+      //Use string here instead of the constant as constant may not be available\n+      //on the classpath of a hadoop 2.7\n+      boolean isolatedClassloader =\n+          conf.getBoolean(\"ozone.fs.isolated-classloader\", defaultValue);\n+\n+      // adapter should be initialized in operations.\n+      this.adapter = createAdapter(conf, omHost, omPort, isolatedClassloader);\n+\n+      try {\n+        this.userName =\n+            UserGroupInformation.getCurrentUser().getShortUserName();\n+      } catch (IOException e) {\n+        this.userName = OZONE_DEFAULT_USER;\n+      }\n+      this.workingDir = new Path(OZONE_USER_DIR, this.userName)\n+          .makeQualified(this.uri, this.workingDir);\n+    } catch (URISyntaxException ue) {\n+      final String msg = \"Invalid Ozone endpoint \" + name;\n+      LOG.error(msg, ue);\n+      throw new IOException(msg, ue);\n+    }\n+  }\n+\n+  protected RootedOzoneClientAdapter createAdapter(Configuration conf,\n+      String omHost, int omPort, boolean isolatedClassloader)\n+      throws IOException {\n+\n+    if (isolatedClassloader) {\n+      // TODO: Check how this code path need to be changed, for legacy Hadoop?\n+      return RootedOzoneClientAdapterFactory.createAdapter();\n+    } else {\n+      return new BasicRootedOzoneClientAdapterImpl(omHost, omPort, conf);\n+    }\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    try {\n+      adapter.close();\n+    } finally {\n+      super.close();\n+    }\n+  }\n+\n+  @Override\n+  public URI getUri() {\n+    return uri;\n+  }\n+\n+  @Override\n+  public String getScheme() {\n+    return OZONE_OFS_URI_SCHEME;\n+  }\n+\n+  @Override\n+  public FSDataInputStream open(Path path, int bufferSize) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_OPEN);\n+    statistics.incrementReadOps(1);\n+    LOG.trace(\"open() path: {}\", path);\n+    final String key = pathToKey(path);\n+    return new FSDataInputStream(\n+        new OzoneFSInputStream(adapter.readFile(key), statistics));\n+  }\n+\n+  protected void incrementCounter(Statistic statistic) {\n+    //don't do anything in this default implementation.\n+  }\n+\n+  @Override\n+  public FSDataOutputStream create(Path f, FsPermission permission,\n+      boolean overwrite, int bufferSize,\n+      short replication, long blockSize,\n+      Progressable progress) throws IOException {\n+    LOG.trace(\"create() path:{}\", f);\n+    incrementCounter(Statistic.INVOCATION_CREATE);\n+    statistics.incrementWriteOps(1);\n+    final String key = pathToKey(f);\n+    return createOutputStream(key, overwrite, true);\n+  }\n+\n+  @Override\n+  public FSDataOutputStream createNonRecursive(Path path,\n+      FsPermission permission,\n+      EnumSet<CreateFlag> flags,\n+      int bufferSize,\n+      short replication,\n+      long blockSize,\n+      Progressable progress) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_CREATE_NON_RECURSIVE);\n+    statistics.incrementWriteOps(1);\n+    final String key = pathToKey(path);\n+    return createOutputStream(key, flags.contains(CreateFlag.OVERWRITE), false);\n+  }\n+\n+  private FSDataOutputStream createOutputStream(String key, boolean overwrite,\n+      boolean recursive) throws IOException {\n+    return new FSDataOutputStream(adapter.createFile(key, overwrite, recursive),\n+        statistics);\n+  }\n+\n+  @Override\n+  public FSDataOutputStream append(Path f, int bufferSize,\n+      Progressable progress) throws IOException {\n+    throw new UnsupportedOperationException(\"append() Not implemented by the \"\n+        + getClass().getSimpleName() + \" FileSystem implementation\");\n+  }\n+\n+  private class RenameIterator extends OzoneListingIterator {\n+    private final String srcPath;\n+    private final String dstPath;\n+\n+    RenameIterator(Path srcPath, Path dstPath)\n+        throws IOException {\n+      super(srcPath);\n+      this.srcPath = pathToKey(srcPath);\n+      this.dstPath = pathToKey(dstPath);\n+      LOG.trace(\"rename from:{} to:{}\", this.srcPath, this.dstPath);\n+    }\n+\n+    @Override\n+    boolean processKeyPath(String keyPath) throws IOException {\n+      String newPath = dstPath.concat(keyPath.substring(srcPath.length()));\n+      adapter.renamePath(keyPath, newPath);\n+      return true;\n+    }\n+  }\n+\n+  /**\n+   * Check whether the source and destination path are valid and then perform\n+   * rename from source path to destination path.\n+   * <p>\n+   * The rename operation is performed by renaming the keys with src as prefix.\n+   * For such keys the prefix is changed from src to dst.\n+   *\n+   * @param src source path for rename\n+   * @param dst destination path for rename\n+   * @return true if rename operation succeeded or\n+   * if the src and dst have the same path and are of the same type\n+   * @throws IOException on I/O errors or if the src/dst paths are invalid.\n+   */\n+  @Override\n+  public boolean rename(Path src, Path dst) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_RENAME);\n+    statistics.incrementWriteOps(1);\n+    if (src.equals(dst)) {\n+      return true;\n+    }\n+\n+    LOG.trace(\"rename() from: {} to: {}\", src, dst);\n+    if (src.isRoot()) {\n+      // Cannot rename root of file system\n+      LOG.trace(\"Cannot rename the root of a filesystem\");\n+      return false;\n+    }\n+\n+    // src and dst should be in the same bucket\n+    OFSPath ofsSrc = new OFSPath(src);\n+    OFSPath ofsDst = new OFSPath(dst);\n+    if (!ofsSrc.isInSameBucketAs(ofsDst)) {\n+      throw new IOException(\"Cannot rename a key to a different bucket\");\n+    }\n+\n+    // Cannot rename a directory to its own subdirectory\n+    Path dstParent = dst.getParent();\n+    while (dstParent != null && !src.equals(dstParent)) {\n+      dstParent = dstParent.getParent();\n+    }\n+    Preconditions.checkArgument(dstParent == null,\n+        \"Cannot rename a directory to its own subdirectory\");\n+    // Check if the source exists\n+    FileStatus srcStatus;\n+    try {\n+      srcStatus = getFileStatus(src);\n+    } catch (FileNotFoundException fnfe) {\n+      // source doesn't exist, return\n+      return false;\n+    }\n+\n+    // Check if the destination exists\n+    FileStatus dstStatus;\n+    try {\n+      dstStatus = getFileStatus(dst);\n+    } catch (FileNotFoundException fnde) {\n+      dstStatus = null;\n+    }\n+\n+    if (dstStatus == null) {\n+      // If dst doesn't exist, check whether dst parent dir exists or not\n+      // if the parent exists, the source can still be renamed to dst path\n+      dstStatus = getFileStatus(dst.getParent());\n+      if (!dstStatus.isDirectory()) {\n+        throw new IOException(String.format(\n+            \"Failed to rename %s to %s, %s is a file\", src, dst,\n+            dst.getParent()));\n+      }\n+    } else {\n+      // if dst exists and source and destination are same,\n+      // check both the src and dst are of same type\n+      if (srcStatus.getPath().equals(dstStatus.getPath())) {\n+        return !srcStatus.isDirectory();\n+      } else if (dstStatus.isDirectory()) {\n+        // If dst is a directory, rename source as subpath of it.\n+        // for example rename /source to /dst will lead to /dst/source\n+        dst = new Path(dst, src.getName());\n+        FileStatus[] statuses;\n+        try {\n+          statuses = listStatus(dst);\n+        } catch (FileNotFoundException fnde) {\n+          statuses = null;\n+        }\n+\n+        if (statuses != null && statuses.length > 0) {\n+          // If dst exists and not a directory not empty\n+          throw new FileAlreadyExistsException(String.format(\n+              \"Failed to rename %s to %s, file already exists or not empty!\",\n+              src, dst));\n+        }\n+      } else {\n+        // If dst is not a directory\n+        throw new FileAlreadyExistsException(String.format(\n+            \"Failed to rename %s to %s, file already exists!\", src, dst));\n+      }\n+    }\n+\n+    if (srcStatus.isDirectory()) {\n+      if (dst.toString().startsWith(src.toString() + OZONE_URI_DELIMITER)) {\n+        LOG.trace(\"Cannot rename a directory to a subdirectory of self\");\n+        return false;\n+      }\n+    }\n+    RenameIterator iterator = new RenameIterator(src, dst);\n+    boolean result = iterator.iterate();\n+    if (result) {\n+      createFakeParentDirectory(src);\n+    }\n+    return result;\n+  }\n+\n+  private class DeleteIterator extends OzoneListingIterator {\n+    private boolean recursive;\n+\n+    DeleteIterator(Path f, boolean recursive)\n+        throws IOException {\n+      super(f);\n+      this.recursive = recursive;\n+      if (getStatus().isDirectory()\n+          && !this.recursive\n+          && listStatus(f).length != 0) {\n+        throw new PathIsNotEmptyDirectoryException(f.toString());\n+      }\n+    }\n+\n+    @Override\n+    boolean processKeyPath(String keyPath) throws IOException {\n+      if (keyPath.equals(\"\")) {\n+        LOG.trace(\"Skipping deleting root directory\");\n+        return true;\n+      } else {\n+        LOG.trace(\"deleting key path:\" + keyPath);\n+        boolean succeed = adapter.deleteObject(keyPath);\n+        // if recursive delete is requested ignore the return value of\n+        // deleteObject and issue deletes for other keys.\n+        return recursive || succeed;\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Deletes the children of the input dir path by iterating though the\n+   * DeleteIterator.\n+   *\n+   * @param f directory path to be deleted\n+   * @return true if successfully deletes all required keys, false otherwise\n+   * @throws IOException\n+   */\n+  private boolean innerDelete(Path f, boolean recursive) throws IOException {\n+    LOG.trace(\"delete() path:{} recursive:{}\", f, recursive);\n+    try {\n+      DeleteIterator iterator = new DeleteIterator(f, recursive);\n+      return iterator.iterate();\n+    } catch (FileNotFoundException e) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Couldn't delete {} - does not exist\", f);\n+      }\n+      return false;\n+    }\n+  }\n+\n+  @Override\n+  public boolean delete(Path f, boolean recursive) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_DELETE);\n+    statistics.incrementWriteOps(1);\n+    LOG.debug(\"Delete path {} - recursive {}\", f, recursive);\n+    FileStatus status;\n+    try {\n+      status = getFileStatus(f);\n+    } catch (FileNotFoundException ex) {\n+      LOG.warn(\"delete: Path does not exist: {}\", f);\n+      return false;\n+    }\n+\n+    String key = pathToKey(f);\n+    boolean result;\n+\n+    if (status.isDirectory()) {\n+      LOG.debug(\"delete: Path is a directory: {}\", f);\n+      key = addTrailingSlashIfNeeded(key);\n+\n+      if (key.equals(\"/\")) {\n+        LOG.warn(\"Cannot delete root directory.\");\n+        return false;\n+      }\n+\n+      result = innerDelete(f, recursive);\n+    } else {\n+      LOG.debug(\"delete: Path is a file: {}\", f);\n+      result = adapter.deleteObject(key);\n+    }\n+\n+    if (result) {\n+      // If this delete operation removes all files/directories from the\n+      // parent direcotry, then an empty parent directory must be created.\n+      createFakeParentDirectory(f);\n+    }\n+\n+    return result;\n+  }\n+\n+  /**\n+   * Create a fake parent directory key if it does not already exist and no\n+   * other child of this parent directory exists.\n+   *\n+   * @param f path to the fake parent directory\n+   * @throws IOException\n+   */\n+  private void createFakeParentDirectory(Path f) throws IOException {\n+    Path parent = f.getParent();\n+    if (parent != null && !parent.isRoot()) {\n+      createFakeDirectoryIfNecessary(parent);\n+    }\n+  }\n+\n+  /**\n+   * Create a fake directory key if it does not already exist.\n+   *\n+   * @param f path to the fake directory\n+   * @throws IOException\n+   */\n+  private void createFakeDirectoryIfNecessary(Path f) throws IOException {\n+    String key = pathToKey(f);\n+    if (!key.isEmpty() && !o3Exists(f)) {\n+      LOG.debug(\"Creating new fake directory at {}\", f);\n+      String dirKey = addTrailingSlashIfNeeded(key);\n+      adapter.createDirectory(dirKey);\n+    }\n+  }\n+\n+  /**\n+   * Check if a file or directory exists corresponding to given path.\n+   *\n+   * @param f path to file/directory.\n+   * @return true if it exists, false otherwise.\n+   * @throws IOException\n+   */\n+  private boolean o3Exists(final Path f) throws IOException {\n+    Path path = makeQualified(f);\n+    try {\n+      getFileStatus(path);\n+      return true;\n+    } catch (FileNotFoundException ex) {\n+      return false;\n+    }\n+  }\n+\n+  @Override\n+  public FileStatus[] listStatus(Path f) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_LIST_STATUS);\n+    statistics.incrementReadOps(1);\n+    LOG.trace(\"listStatus() path:{}\", f);\n+    int numEntries = LISTING_PAGE_SIZE;\n+    LinkedList<FileStatus> statuses = new LinkedList<>();\n+    List<FileStatus> tmpStatusList;\n+    String startPath = \"\";\n+\n+    do {\n+      tmpStatusList =\n+          adapter.listStatus(pathToKey(f), false, startPath,\n+              numEntries, uri, workingDir, getUsername())\n+              .stream()\n+              .map(this::convertFileStatus)\n+              .collect(Collectors.toList());\n+\n+      if (!tmpStatusList.isEmpty()) {\n+        if (startPath.isEmpty()) {\n+          statuses.addAll(tmpStatusList);\n+        } else {\n+          statuses.addAll(tmpStatusList.subList(1, tmpStatusList.size()));\n+        }\n+        startPath = pathToKey(statuses.getLast().getPath());\n+      }\n+      // listStatus returns entries numEntries in size if available.\n+      // Any lesser number of entries indicate that the required entries have\n+      // exhausted.\n+    } while (tmpStatusList.size() == numEntries);\n+\n+    return statuses.toArray(new FileStatus[0]);\n+  }\n+\n+  @Override\n+  public void setWorkingDirectory(Path newDir) {\n+    workingDir = newDir;\n+  }\n+\n+  @Override\n+  public Path getWorkingDirectory() {\n+    return workingDir;\n+  }\n+\n+  @Override\n+  public Token<?> getDelegationToken(String renewer) throws IOException {\n+    return adapter.getDelegationToken(renewer);\n+  }\n+\n+  /**\n+   * Get a canonical service name for this file system. If the URI is logical,\n+   * the hostname part of the URI will be returned.\n+   *\n+   * @return a service string that uniquely identifies this file system.\n+   */\n+  @Override\n+  public String getCanonicalServiceName() {\n+    return adapter.getCanonicalServiceName();\n+  }\n+\n+  /**\n+   * Get the username of the FS.\n+   *\n+   * @return the short name of the user who instantiated the FS\n+   */\n+  public String getUsername() {\n+    return userName;\n+  }\n+\n+  /**\n+   * Creates a directory. Directory is represented using a key with no value.\n+   *\n+   * @param path directory path to be created\n+   * @return true if directory exists or created successfully.\n+   * @throws IOException\n+   */\n+  private boolean mkdir(Path path) throws IOException {\n+    return adapter.createDirectory(pathToKey(path));\n+  }\n+\n+  @Override\n+  public boolean mkdirs(Path f, FsPermission permission) throws IOException {\n+    LOG.trace(\"mkdir() path:{} \", f);\n+    String key = pathToKey(f);\n+    if (isEmpty(key)) {\n+      return false;\n+    }\n+    return mkdir(f);\n+  }\n+\n+  @Override\n+  public FileStatus getFileStatus(Path f) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_GET_FILE_STATUS);\n+    statistics.incrementReadOps(1);\n+    LOG.trace(\"getFileStatus() path:{}\", f);\n+    Path qualifiedPath = f.makeQualified(uri, workingDir);\n+    String key = pathToKey(qualifiedPath);\n+    FileStatus fileStatus = null;\n+    try {\n+      fileStatus = convertFileStatus(\n+          adapter.getFileStatus(key, uri, qualifiedPath, getUsername()));\n+    } catch (OMException ex) {\n+      if (ex.getResult().equals(OMException.ResultCodes.KEY_NOT_FOUND)) {\n+        throw new FileNotFoundException(\"File not found. path:\" + f);\n+      }\n+    }\n+    return fileStatus;\n+  }\n+\n+  @Override\n+  public BlockLocation[] getFileBlockLocations(FileStatus fileStatus,\n+      long start, long len)\n+      throws IOException {\n+    if (fileStatus instanceof LocatedFileStatus) {\n+      return ((LocatedFileStatus) fileStatus).getBlockLocations();\n+    } else {\n+      return super.getFileBlockLocations(fileStatus, start, len);\n+    }\n+  }\n+\n+  /**\n+   * Turn a path (relative or otherwise) into an Ozone key.\n+   *\n+   * @param path the path of the file.\n+   * @return the key of the object that represents the file.\n+   */\n+  public String pathToKey(Path path) {\n+    Objects.requireNonNull(path, \"Path can't be null!\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdffa6ab5c1f59cedc968bf36c2ccd286cc6288f"}, "originalPosition": 639}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTgwMTUwNw==", "bodyText": "When renaming a directory, we iterate through all the keys in that directory. For each key, we are going to call adapter.renamePath(). This function would in turn get BucketInfo from OM each time. We should optimize this to avoid the extra getBucketInfo calls.", "url": "https://github.com/apache/ozone/pull/415#discussion_r369801507", "createdAt": "2020-01-22T21:03:25Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,805 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import com.google.common.base.Preconditions;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.CreateFlag;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocatedFileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PathIsNotEmptyDirectoryException;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.util.Progressable;\n+import org.apache.http.client.utils.URIBuilder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.EnumSet;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hadoop.fs.ozone.Constants.LISTING_PAGE_SIZE;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_DEFAULT_USER;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_USER_DIR;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_URI_DELIMITER;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_OFS_URI_SCHEME;\n+\n+/**\n+ * The minimal Ozone Filesystem implementation.\n+ * <p>\n+ * This is a basic version which doesn't extend\n+ * KeyProviderTokenIssuer and doesn't include statistics. It can be used\n+ * from older hadoop version. For newer hadoop version use the full featured\n+ * OFileSystem.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Evolving\n+public class BasicRootedOzoneFileSystem extends FileSystem {\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneFileSystem.class);\n+\n+  /**\n+   * The Ozone client for connecting to Ozone server.\n+   */\n+\n+  private URI uri;\n+  private String userName;\n+  private Path workingDir;\n+//  private String gOmHost;\n+//  private int gOmPort;\n+//  private Configuration gConf;\n+//  private boolean gIsolatedClassloader;\n+\n+  private RootedOzoneClientAdapter adapter;\n+//  private String adapterPath;\n+\n+  private static final String URI_EXCEPTION_TEXT =\n+      \"URL should be one of the following formats: \" +\n+      \"ofs://om-service-id/  OR \" +\n+      \"ofs://om-host.example.com/  OR \" +\n+      \"ofs://om-host.example.com:5678/\";\n+\n+  @Override\n+  public void initialize(URI name, Configuration conf) throws IOException {\n+    super.initialize(name, conf);\n+    setConf(conf);\n+    Objects.requireNonNull(name.getScheme(), \"No scheme provided in \" + name);\n+    Preconditions.checkArgument(getScheme().equals(name.getScheme()),\n+        \"Invalid scheme provided in \" + name);\n+\n+    String authority = name.getAuthority();\n+    if (authority == null) {\n+      // authority is null when fs.defaultFS is not a qualified ofs URI and\n+      // ofs:/// is passed to the client. matcher will NPE if authority is null\n+      throw new IllegalArgumentException(URI_EXCEPTION_TEXT);\n+    }\n+\n+    String omHost;\n+    int omPort = -1;\n+    // Parse hostname and port\n+    String[] parts = authority.split(\":\");\n+    if (parts.length > 2) {\n+      throw new IllegalArgumentException(URI_EXCEPTION_TEXT);\n+    }\n+    omHost = parts[0];\n+    if (parts.length == 2) {\n+      try {\n+        omPort = Integer.parseInt(parts[1]);\n+      } catch (NumberFormatException e) {\n+        throw new IllegalArgumentException(URI_EXCEPTION_TEXT);\n+      }\n+    }\n+\n+    try {\n+      uri = new URIBuilder().setScheme(OZONE_OFS_URI_SCHEME)\n+          .setHost(authority)\n+          .build();\n+      LOG.trace(\"Ozone URI for OFS initialization is \" + uri);\n+\n+      //isolated is the default for ozonefs-lib-legacy which includes the\n+      // /ozonefs.txt, otherwise the default is false. It could be overridden.\n+      boolean defaultValue =\n+          BasicRootedOzoneFileSystem.class.getClassLoader()\n+              .getResource(\"ozonefs.txt\") != null;\n+\n+      //Use string here instead of the constant as constant may not be available\n+      //on the classpath of a hadoop 2.7\n+      boolean isolatedClassloader =\n+          conf.getBoolean(\"ozone.fs.isolated-classloader\", defaultValue);\n+\n+      // adapter should be initialized in operations.\n+      this.adapter = createAdapter(conf, omHost, omPort, isolatedClassloader);\n+\n+      try {\n+        this.userName =\n+            UserGroupInformation.getCurrentUser().getShortUserName();\n+      } catch (IOException e) {\n+        this.userName = OZONE_DEFAULT_USER;\n+      }\n+      this.workingDir = new Path(OZONE_USER_DIR, this.userName)\n+          .makeQualified(this.uri, this.workingDir);\n+    } catch (URISyntaxException ue) {\n+      final String msg = \"Invalid Ozone endpoint \" + name;\n+      LOG.error(msg, ue);\n+      throw new IOException(msg, ue);\n+    }\n+  }\n+\n+  protected RootedOzoneClientAdapter createAdapter(Configuration conf,\n+      String omHost, int omPort, boolean isolatedClassloader)\n+      throws IOException {\n+\n+    if (isolatedClassloader) {\n+      // TODO: Check how this code path need to be changed, for legacy Hadoop?\n+      return RootedOzoneClientAdapterFactory.createAdapter();\n+    } else {\n+      return new BasicRootedOzoneClientAdapterImpl(omHost, omPort, conf);\n+    }\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    try {\n+      adapter.close();\n+    } finally {\n+      super.close();\n+    }\n+  }\n+\n+  @Override\n+  public URI getUri() {\n+    return uri;\n+  }\n+\n+  @Override\n+  public String getScheme() {\n+    return OZONE_OFS_URI_SCHEME;\n+  }\n+\n+  @Override\n+  public FSDataInputStream open(Path path, int bufferSize) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_OPEN);\n+    statistics.incrementReadOps(1);\n+    LOG.trace(\"open() path: {}\", path);\n+    final String key = pathToKey(path);\n+    return new FSDataInputStream(\n+        new OzoneFSInputStream(adapter.readFile(key), statistics));\n+  }\n+\n+  protected void incrementCounter(Statistic statistic) {\n+    //don't do anything in this default implementation.\n+  }\n+\n+  @Override\n+  public FSDataOutputStream create(Path f, FsPermission permission,\n+      boolean overwrite, int bufferSize,\n+      short replication, long blockSize,\n+      Progressable progress) throws IOException {\n+    LOG.trace(\"create() path:{}\", f);\n+    incrementCounter(Statistic.INVOCATION_CREATE);\n+    statistics.incrementWriteOps(1);\n+    final String key = pathToKey(f);\n+    return createOutputStream(key, overwrite, true);\n+  }\n+\n+  @Override\n+  public FSDataOutputStream createNonRecursive(Path path,\n+      FsPermission permission,\n+      EnumSet<CreateFlag> flags,\n+      int bufferSize,\n+      short replication,\n+      long blockSize,\n+      Progressable progress) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_CREATE_NON_RECURSIVE);\n+    statistics.incrementWriteOps(1);\n+    final String key = pathToKey(path);\n+    return createOutputStream(key, flags.contains(CreateFlag.OVERWRITE), false);\n+  }\n+\n+  private FSDataOutputStream createOutputStream(String key, boolean overwrite,\n+      boolean recursive) throws IOException {\n+    return new FSDataOutputStream(adapter.createFile(key, overwrite, recursive),\n+        statistics);\n+  }\n+\n+  @Override\n+  public FSDataOutputStream append(Path f, int bufferSize,\n+      Progressable progress) throws IOException {\n+    throw new UnsupportedOperationException(\"append() Not implemented by the \"\n+        + getClass().getSimpleName() + \" FileSystem implementation\");\n+  }\n+\n+  private class RenameIterator extends OzoneListingIterator {\n+    private final String srcPath;\n+    private final String dstPath;\n+\n+    RenameIterator(Path srcPath, Path dstPath)\n+        throws IOException {\n+      super(srcPath);\n+      this.srcPath = pathToKey(srcPath);\n+      this.dstPath = pathToKey(dstPath);\n+      LOG.trace(\"rename from:{} to:{}\", this.srcPath, this.dstPath);\n+    }\n+\n+    @Override\n+    boolean processKeyPath(String keyPath) throws IOException {\n+      String newPath = dstPath.concat(keyPath.substring(srcPath.length()));\n+      adapter.renamePath(keyPath, newPath);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdffa6ab5c1f59cedc968bf36c2ccd286cc6288f"}, "originalPosition": 260}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTgyMTgwNA==", "bodyText": "When deleting a directory, we iterate through all the keys in that directory. For each key, we are going to call adapter.deleteObject(). This function would in turn get BucketInfo from OM each time. We should optimize this to avoid the extra getBucketInfo calls.", "url": "https://github.com/apache/ozone/pull/415#discussion_r369821804", "createdAt": "2020-01-22T21:47:40Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,805 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import com.google.common.base.Preconditions;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.CreateFlag;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocatedFileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PathIsNotEmptyDirectoryException;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.util.Progressable;\n+import org.apache.http.client.utils.URIBuilder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.EnumSet;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hadoop.fs.ozone.Constants.LISTING_PAGE_SIZE;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_DEFAULT_USER;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_USER_DIR;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_URI_DELIMITER;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_OFS_URI_SCHEME;\n+\n+/**\n+ * The minimal Ozone Filesystem implementation.\n+ * <p>\n+ * This is a basic version which doesn't extend\n+ * KeyProviderTokenIssuer and doesn't include statistics. It can be used\n+ * from older hadoop version. For newer hadoop version use the full featured\n+ * OFileSystem.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Evolving\n+public class BasicRootedOzoneFileSystem extends FileSystem {\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneFileSystem.class);\n+\n+  /**\n+   * The Ozone client for connecting to Ozone server.\n+   */\n+\n+  private URI uri;\n+  private String userName;\n+  private Path workingDir;\n+//  private String gOmHost;\n+//  private int gOmPort;\n+//  private Configuration gConf;\n+//  private boolean gIsolatedClassloader;\n+\n+  private RootedOzoneClientAdapter adapter;\n+//  private String adapterPath;\n+\n+  private static final String URI_EXCEPTION_TEXT =\n+      \"URL should be one of the following formats: \" +\n+      \"ofs://om-service-id/  OR \" +\n+      \"ofs://om-host.example.com/  OR \" +\n+      \"ofs://om-host.example.com:5678/\";\n+\n+  @Override\n+  public void initialize(URI name, Configuration conf) throws IOException {\n+    super.initialize(name, conf);\n+    setConf(conf);\n+    Objects.requireNonNull(name.getScheme(), \"No scheme provided in \" + name);\n+    Preconditions.checkArgument(getScheme().equals(name.getScheme()),\n+        \"Invalid scheme provided in \" + name);\n+\n+    String authority = name.getAuthority();\n+    if (authority == null) {\n+      // authority is null when fs.defaultFS is not a qualified ofs URI and\n+      // ofs:/// is passed to the client. matcher will NPE if authority is null\n+      throw new IllegalArgumentException(URI_EXCEPTION_TEXT);\n+    }\n+\n+    String omHost;\n+    int omPort = -1;\n+    // Parse hostname and port\n+    String[] parts = authority.split(\":\");\n+    if (parts.length > 2) {\n+      throw new IllegalArgumentException(URI_EXCEPTION_TEXT);\n+    }\n+    omHost = parts[0];\n+    if (parts.length == 2) {\n+      try {\n+        omPort = Integer.parseInt(parts[1]);\n+      } catch (NumberFormatException e) {\n+        throw new IllegalArgumentException(URI_EXCEPTION_TEXT);\n+      }\n+    }\n+\n+    try {\n+      uri = new URIBuilder().setScheme(OZONE_OFS_URI_SCHEME)\n+          .setHost(authority)\n+          .build();\n+      LOG.trace(\"Ozone URI for OFS initialization is \" + uri);\n+\n+      //isolated is the default for ozonefs-lib-legacy which includes the\n+      // /ozonefs.txt, otherwise the default is false. It could be overridden.\n+      boolean defaultValue =\n+          BasicRootedOzoneFileSystem.class.getClassLoader()\n+              .getResource(\"ozonefs.txt\") != null;\n+\n+      //Use string here instead of the constant as constant may not be available\n+      //on the classpath of a hadoop 2.7\n+      boolean isolatedClassloader =\n+          conf.getBoolean(\"ozone.fs.isolated-classloader\", defaultValue);\n+\n+      // adapter should be initialized in operations.\n+      this.adapter = createAdapter(conf, omHost, omPort, isolatedClassloader);\n+\n+      try {\n+        this.userName =\n+            UserGroupInformation.getCurrentUser().getShortUserName();\n+      } catch (IOException e) {\n+        this.userName = OZONE_DEFAULT_USER;\n+      }\n+      this.workingDir = new Path(OZONE_USER_DIR, this.userName)\n+          .makeQualified(this.uri, this.workingDir);\n+    } catch (URISyntaxException ue) {\n+      final String msg = \"Invalid Ozone endpoint \" + name;\n+      LOG.error(msg, ue);\n+      throw new IOException(msg, ue);\n+    }\n+  }\n+\n+  protected RootedOzoneClientAdapter createAdapter(Configuration conf,\n+      String omHost, int omPort, boolean isolatedClassloader)\n+      throws IOException {\n+\n+    if (isolatedClassloader) {\n+      // TODO: Check how this code path need to be changed, for legacy Hadoop?\n+      return RootedOzoneClientAdapterFactory.createAdapter();\n+    } else {\n+      return new BasicRootedOzoneClientAdapterImpl(omHost, omPort, conf);\n+    }\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    try {\n+      adapter.close();\n+    } finally {\n+      super.close();\n+    }\n+  }\n+\n+  @Override\n+  public URI getUri() {\n+    return uri;\n+  }\n+\n+  @Override\n+  public String getScheme() {\n+    return OZONE_OFS_URI_SCHEME;\n+  }\n+\n+  @Override\n+  public FSDataInputStream open(Path path, int bufferSize) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_OPEN);\n+    statistics.incrementReadOps(1);\n+    LOG.trace(\"open() path: {}\", path);\n+    final String key = pathToKey(path);\n+    return new FSDataInputStream(\n+        new OzoneFSInputStream(adapter.readFile(key), statistics));\n+  }\n+\n+  protected void incrementCounter(Statistic statistic) {\n+    //don't do anything in this default implementation.\n+  }\n+\n+  @Override\n+  public FSDataOutputStream create(Path f, FsPermission permission,\n+      boolean overwrite, int bufferSize,\n+      short replication, long blockSize,\n+      Progressable progress) throws IOException {\n+    LOG.trace(\"create() path:{}\", f);\n+    incrementCounter(Statistic.INVOCATION_CREATE);\n+    statistics.incrementWriteOps(1);\n+    final String key = pathToKey(f);\n+    return createOutputStream(key, overwrite, true);\n+  }\n+\n+  @Override\n+  public FSDataOutputStream createNonRecursive(Path path,\n+      FsPermission permission,\n+      EnumSet<CreateFlag> flags,\n+      int bufferSize,\n+      short replication,\n+      long blockSize,\n+      Progressable progress) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_CREATE_NON_RECURSIVE);\n+    statistics.incrementWriteOps(1);\n+    final String key = pathToKey(path);\n+    return createOutputStream(key, flags.contains(CreateFlag.OVERWRITE), false);\n+  }\n+\n+  private FSDataOutputStream createOutputStream(String key, boolean overwrite,\n+      boolean recursive) throws IOException {\n+    return new FSDataOutputStream(adapter.createFile(key, overwrite, recursive),\n+        statistics);\n+  }\n+\n+  @Override\n+  public FSDataOutputStream append(Path f, int bufferSize,\n+      Progressable progress) throws IOException {\n+    throw new UnsupportedOperationException(\"append() Not implemented by the \"\n+        + getClass().getSimpleName() + \" FileSystem implementation\");\n+  }\n+\n+  private class RenameIterator extends OzoneListingIterator {\n+    private final String srcPath;\n+    private final String dstPath;\n+\n+    RenameIterator(Path srcPath, Path dstPath)\n+        throws IOException {\n+      super(srcPath);\n+      this.srcPath = pathToKey(srcPath);\n+      this.dstPath = pathToKey(dstPath);\n+      LOG.trace(\"rename from:{} to:{}\", this.srcPath, this.dstPath);\n+    }\n+\n+    @Override\n+    boolean processKeyPath(String keyPath) throws IOException {\n+      String newPath = dstPath.concat(keyPath.substring(srcPath.length()));\n+      adapter.renamePath(keyPath, newPath);\n+      return true;\n+    }\n+  }\n+\n+  /**\n+   * Check whether the source and destination path are valid and then perform\n+   * rename from source path to destination path.\n+   * <p>\n+   * The rename operation is performed by renaming the keys with src as prefix.\n+   * For such keys the prefix is changed from src to dst.\n+   *\n+   * @param src source path for rename\n+   * @param dst destination path for rename\n+   * @return true if rename operation succeeded or\n+   * if the src and dst have the same path and are of the same type\n+   * @throws IOException on I/O errors or if the src/dst paths are invalid.\n+   */\n+  @Override\n+  public boolean rename(Path src, Path dst) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_RENAME);\n+    statistics.incrementWriteOps(1);\n+    if (src.equals(dst)) {\n+      return true;\n+    }\n+\n+    LOG.trace(\"rename() from: {} to: {}\", src, dst);\n+    if (src.isRoot()) {\n+      // Cannot rename root of file system\n+      LOG.trace(\"Cannot rename the root of a filesystem\");\n+      return false;\n+    }\n+\n+    // src and dst should be in the same bucket\n+    OFSPath ofsSrc = new OFSPath(src);\n+    OFSPath ofsDst = new OFSPath(dst);\n+    if (!ofsSrc.isInSameBucketAs(ofsDst)) {\n+      throw new IOException(\"Cannot rename a key to a different bucket\");\n+    }\n+\n+    // Cannot rename a directory to its own subdirectory\n+    Path dstParent = dst.getParent();\n+    while (dstParent != null && !src.equals(dstParent)) {\n+      dstParent = dstParent.getParent();\n+    }\n+    Preconditions.checkArgument(dstParent == null,\n+        \"Cannot rename a directory to its own subdirectory\");\n+    // Check if the source exists\n+    FileStatus srcStatus;\n+    try {\n+      srcStatus = getFileStatus(src);\n+    } catch (FileNotFoundException fnfe) {\n+      // source doesn't exist, return\n+      return false;\n+    }\n+\n+    // Check if the destination exists\n+    FileStatus dstStatus;\n+    try {\n+      dstStatus = getFileStatus(dst);\n+    } catch (FileNotFoundException fnde) {\n+      dstStatus = null;\n+    }\n+\n+    if (dstStatus == null) {\n+      // If dst doesn't exist, check whether dst parent dir exists or not\n+      // if the parent exists, the source can still be renamed to dst path\n+      dstStatus = getFileStatus(dst.getParent());\n+      if (!dstStatus.isDirectory()) {\n+        throw new IOException(String.format(\n+            \"Failed to rename %s to %s, %s is a file\", src, dst,\n+            dst.getParent()));\n+      }\n+    } else {\n+      // if dst exists and source and destination are same,\n+      // check both the src and dst are of same type\n+      if (srcStatus.getPath().equals(dstStatus.getPath())) {\n+        return !srcStatus.isDirectory();\n+      } else if (dstStatus.isDirectory()) {\n+        // If dst is a directory, rename source as subpath of it.\n+        // for example rename /source to /dst will lead to /dst/source\n+        dst = new Path(dst, src.getName());\n+        FileStatus[] statuses;\n+        try {\n+          statuses = listStatus(dst);\n+        } catch (FileNotFoundException fnde) {\n+          statuses = null;\n+        }\n+\n+        if (statuses != null && statuses.length > 0) {\n+          // If dst exists and not a directory not empty\n+          throw new FileAlreadyExistsException(String.format(\n+              \"Failed to rename %s to %s, file already exists or not empty!\",\n+              src, dst));\n+        }\n+      } else {\n+        // If dst is not a directory\n+        throw new FileAlreadyExistsException(String.format(\n+            \"Failed to rename %s to %s, file already exists!\", src, dst));\n+      }\n+    }\n+\n+    if (srcStatus.isDirectory()) {\n+      if (dst.toString().startsWith(src.toString() + OZONE_URI_DELIMITER)) {\n+        LOG.trace(\"Cannot rename a directory to a subdirectory of self\");\n+        return false;\n+      }\n+    }\n+    RenameIterator iterator = new RenameIterator(src, dst);\n+    boolean result = iterator.iterate();\n+    if (result) {\n+      createFakeParentDirectory(src);\n+    }\n+    return result;\n+  }\n+\n+  private class DeleteIterator extends OzoneListingIterator {\n+    private boolean recursive;\n+\n+    DeleteIterator(Path f, boolean recursive)\n+        throws IOException {\n+      super(f);\n+      this.recursive = recursive;\n+      if (getStatus().isDirectory()\n+          && !this.recursive\n+          && listStatus(f).length != 0) {\n+        throw new PathIsNotEmptyDirectoryException(f.toString());\n+      }\n+    }\n+\n+    @Override\n+    boolean processKeyPath(String keyPath) throws IOException {\n+      if (keyPath.equals(\"\")) {\n+        LOG.trace(\"Skipping deleting root directory\");\n+        return true;\n+      } else {\n+        LOG.trace(\"deleting key path:\" + keyPath);\n+        boolean succeed = adapter.deleteObject(keyPath);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdffa6ab5c1f59cedc968bf36c2ccd286cc6288f"}, "originalPosition": 397}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTgyNzkyMw==", "bodyText": "Do we need to convert pathKey to ofsPath here? We make this conversion when corresponding function (rename/delete) is called in processKeyPath anyway.", "url": "https://github.com/apache/ozone/pull/415#discussion_r369827923", "createdAt": "2020-01-22T22:01:14Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,805 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import com.google.common.base.Preconditions;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.CreateFlag;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocatedFileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PathIsNotEmptyDirectoryException;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.util.Progressable;\n+import org.apache.http.client.utils.URIBuilder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.EnumSet;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hadoop.fs.ozone.Constants.LISTING_PAGE_SIZE;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_DEFAULT_USER;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_USER_DIR;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_URI_DELIMITER;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_OFS_URI_SCHEME;\n+\n+/**\n+ * The minimal Ozone Filesystem implementation.\n+ * <p>\n+ * This is a basic version which doesn't extend\n+ * KeyProviderTokenIssuer and doesn't include statistics. It can be used\n+ * from older hadoop version. For newer hadoop version use the full featured\n+ * OFileSystem.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Evolving\n+public class BasicRootedOzoneFileSystem extends FileSystem {\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneFileSystem.class);\n+\n+  /**\n+   * The Ozone client for connecting to Ozone server.\n+   */\n+\n+  private URI uri;\n+  private String userName;\n+  private Path workingDir;\n+//  private String gOmHost;\n+//  private int gOmPort;\n+//  private Configuration gConf;\n+//  private boolean gIsolatedClassloader;\n+\n+  private RootedOzoneClientAdapter adapter;\n+//  private String adapterPath;\n+\n+  private static final String URI_EXCEPTION_TEXT =\n+      \"URL should be one of the following formats: \" +\n+      \"ofs://om-service-id/  OR \" +\n+      \"ofs://om-host.example.com/  OR \" +\n+      \"ofs://om-host.example.com:5678/\";\n+\n+  @Override\n+  public void initialize(URI name, Configuration conf) throws IOException {\n+    super.initialize(name, conf);\n+    setConf(conf);\n+    Objects.requireNonNull(name.getScheme(), \"No scheme provided in \" + name);\n+    Preconditions.checkArgument(getScheme().equals(name.getScheme()),\n+        \"Invalid scheme provided in \" + name);\n+\n+    String authority = name.getAuthority();\n+    if (authority == null) {\n+      // authority is null when fs.defaultFS is not a qualified ofs URI and\n+      // ofs:/// is passed to the client. matcher will NPE if authority is null\n+      throw new IllegalArgumentException(URI_EXCEPTION_TEXT);\n+    }\n+\n+    String omHost;\n+    int omPort = -1;\n+    // Parse hostname and port\n+    String[] parts = authority.split(\":\");\n+    if (parts.length > 2) {\n+      throw new IllegalArgumentException(URI_EXCEPTION_TEXT);\n+    }\n+    omHost = parts[0];\n+    if (parts.length == 2) {\n+      try {\n+        omPort = Integer.parseInt(parts[1]);\n+      } catch (NumberFormatException e) {\n+        throw new IllegalArgumentException(URI_EXCEPTION_TEXT);\n+      }\n+    }\n+\n+    try {\n+      uri = new URIBuilder().setScheme(OZONE_OFS_URI_SCHEME)\n+          .setHost(authority)\n+          .build();\n+      LOG.trace(\"Ozone URI for OFS initialization is \" + uri);\n+\n+      //isolated is the default for ozonefs-lib-legacy which includes the\n+      // /ozonefs.txt, otherwise the default is false. It could be overridden.\n+      boolean defaultValue =\n+          BasicRootedOzoneFileSystem.class.getClassLoader()\n+              .getResource(\"ozonefs.txt\") != null;\n+\n+      //Use string here instead of the constant as constant may not be available\n+      //on the classpath of a hadoop 2.7\n+      boolean isolatedClassloader =\n+          conf.getBoolean(\"ozone.fs.isolated-classloader\", defaultValue);\n+\n+      // adapter should be initialized in operations.\n+      this.adapter = createAdapter(conf, omHost, omPort, isolatedClassloader);\n+\n+      try {\n+        this.userName =\n+            UserGroupInformation.getCurrentUser().getShortUserName();\n+      } catch (IOException e) {\n+        this.userName = OZONE_DEFAULT_USER;\n+      }\n+      this.workingDir = new Path(OZONE_USER_DIR, this.userName)\n+          .makeQualified(this.uri, this.workingDir);\n+    } catch (URISyntaxException ue) {\n+      final String msg = \"Invalid Ozone endpoint \" + name;\n+      LOG.error(msg, ue);\n+      throw new IOException(msg, ue);\n+    }\n+  }\n+\n+  protected RootedOzoneClientAdapter createAdapter(Configuration conf,\n+      String omHost, int omPort, boolean isolatedClassloader)\n+      throws IOException {\n+\n+    if (isolatedClassloader) {\n+      // TODO: Check how this code path need to be changed, for legacy Hadoop?\n+      return RootedOzoneClientAdapterFactory.createAdapter();\n+    } else {\n+      return new BasicRootedOzoneClientAdapterImpl(omHost, omPort, conf);\n+    }\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    try {\n+      adapter.close();\n+    } finally {\n+      super.close();\n+    }\n+  }\n+\n+  @Override\n+  public URI getUri() {\n+    return uri;\n+  }\n+\n+  @Override\n+  public String getScheme() {\n+    return OZONE_OFS_URI_SCHEME;\n+  }\n+\n+  @Override\n+  public FSDataInputStream open(Path path, int bufferSize) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_OPEN);\n+    statistics.incrementReadOps(1);\n+    LOG.trace(\"open() path: {}\", path);\n+    final String key = pathToKey(path);\n+    return new FSDataInputStream(\n+        new OzoneFSInputStream(adapter.readFile(key), statistics));\n+  }\n+\n+  protected void incrementCounter(Statistic statistic) {\n+    //don't do anything in this default implementation.\n+  }\n+\n+  @Override\n+  public FSDataOutputStream create(Path f, FsPermission permission,\n+      boolean overwrite, int bufferSize,\n+      short replication, long blockSize,\n+      Progressable progress) throws IOException {\n+    LOG.trace(\"create() path:{}\", f);\n+    incrementCounter(Statistic.INVOCATION_CREATE);\n+    statistics.incrementWriteOps(1);\n+    final String key = pathToKey(f);\n+    return createOutputStream(key, overwrite, true);\n+  }\n+\n+  @Override\n+  public FSDataOutputStream createNonRecursive(Path path,\n+      FsPermission permission,\n+      EnumSet<CreateFlag> flags,\n+      int bufferSize,\n+      short replication,\n+      long blockSize,\n+      Progressable progress) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_CREATE_NON_RECURSIVE);\n+    statistics.incrementWriteOps(1);\n+    final String key = pathToKey(path);\n+    return createOutputStream(key, flags.contains(CreateFlag.OVERWRITE), false);\n+  }\n+\n+  private FSDataOutputStream createOutputStream(String key, boolean overwrite,\n+      boolean recursive) throws IOException {\n+    return new FSDataOutputStream(adapter.createFile(key, overwrite, recursive),\n+        statistics);\n+  }\n+\n+  @Override\n+  public FSDataOutputStream append(Path f, int bufferSize,\n+      Progressable progress) throws IOException {\n+    throw new UnsupportedOperationException(\"append() Not implemented by the \"\n+        + getClass().getSimpleName() + \" FileSystem implementation\");\n+  }\n+\n+  private class RenameIterator extends OzoneListingIterator {\n+    private final String srcPath;\n+    private final String dstPath;\n+\n+    RenameIterator(Path srcPath, Path dstPath)\n+        throws IOException {\n+      super(srcPath);\n+      this.srcPath = pathToKey(srcPath);\n+      this.dstPath = pathToKey(dstPath);\n+      LOG.trace(\"rename from:{} to:{}\", this.srcPath, this.dstPath);\n+    }\n+\n+    @Override\n+    boolean processKeyPath(String keyPath) throws IOException {\n+      String newPath = dstPath.concat(keyPath.substring(srcPath.length()));\n+      adapter.renamePath(keyPath, newPath);\n+      return true;\n+    }\n+  }\n+\n+  /**\n+   * Check whether the source and destination path are valid and then perform\n+   * rename from source path to destination path.\n+   * <p>\n+   * The rename operation is performed by renaming the keys with src as prefix.\n+   * For such keys the prefix is changed from src to dst.\n+   *\n+   * @param src source path for rename\n+   * @param dst destination path for rename\n+   * @return true if rename operation succeeded or\n+   * if the src and dst have the same path and are of the same type\n+   * @throws IOException on I/O errors or if the src/dst paths are invalid.\n+   */\n+  @Override\n+  public boolean rename(Path src, Path dst) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_RENAME);\n+    statistics.incrementWriteOps(1);\n+    if (src.equals(dst)) {\n+      return true;\n+    }\n+\n+    LOG.trace(\"rename() from: {} to: {}\", src, dst);\n+    if (src.isRoot()) {\n+      // Cannot rename root of file system\n+      LOG.trace(\"Cannot rename the root of a filesystem\");\n+      return false;\n+    }\n+\n+    // src and dst should be in the same bucket\n+    OFSPath ofsSrc = new OFSPath(src);\n+    OFSPath ofsDst = new OFSPath(dst);\n+    if (!ofsSrc.isInSameBucketAs(ofsDst)) {\n+      throw new IOException(\"Cannot rename a key to a different bucket\");\n+    }\n+\n+    // Cannot rename a directory to its own subdirectory\n+    Path dstParent = dst.getParent();\n+    while (dstParent != null && !src.equals(dstParent)) {\n+      dstParent = dstParent.getParent();\n+    }\n+    Preconditions.checkArgument(dstParent == null,\n+        \"Cannot rename a directory to its own subdirectory\");\n+    // Check if the source exists\n+    FileStatus srcStatus;\n+    try {\n+      srcStatus = getFileStatus(src);\n+    } catch (FileNotFoundException fnfe) {\n+      // source doesn't exist, return\n+      return false;\n+    }\n+\n+    // Check if the destination exists\n+    FileStatus dstStatus;\n+    try {\n+      dstStatus = getFileStatus(dst);\n+    } catch (FileNotFoundException fnde) {\n+      dstStatus = null;\n+    }\n+\n+    if (dstStatus == null) {\n+      // If dst doesn't exist, check whether dst parent dir exists or not\n+      // if the parent exists, the source can still be renamed to dst path\n+      dstStatus = getFileStatus(dst.getParent());\n+      if (!dstStatus.isDirectory()) {\n+        throw new IOException(String.format(\n+            \"Failed to rename %s to %s, %s is a file\", src, dst,\n+            dst.getParent()));\n+      }\n+    } else {\n+      // if dst exists and source and destination are same,\n+      // check both the src and dst are of same type\n+      if (srcStatus.getPath().equals(dstStatus.getPath())) {\n+        return !srcStatus.isDirectory();\n+      } else if (dstStatus.isDirectory()) {\n+        // If dst is a directory, rename source as subpath of it.\n+        // for example rename /source to /dst will lead to /dst/source\n+        dst = new Path(dst, src.getName());\n+        FileStatus[] statuses;\n+        try {\n+          statuses = listStatus(dst);\n+        } catch (FileNotFoundException fnde) {\n+          statuses = null;\n+        }\n+\n+        if (statuses != null && statuses.length > 0) {\n+          // If dst exists and not a directory not empty\n+          throw new FileAlreadyExistsException(String.format(\n+              \"Failed to rename %s to %s, file already exists or not empty!\",\n+              src, dst));\n+        }\n+      } else {\n+        // If dst is not a directory\n+        throw new FileAlreadyExistsException(String.format(\n+            \"Failed to rename %s to %s, file already exists!\", src, dst));\n+      }\n+    }\n+\n+    if (srcStatus.isDirectory()) {\n+      if (dst.toString().startsWith(src.toString() + OZONE_URI_DELIMITER)) {\n+        LOG.trace(\"Cannot rename a directory to a subdirectory of self\");\n+        return false;\n+      }\n+    }\n+    RenameIterator iterator = new RenameIterator(src, dst);\n+    boolean result = iterator.iterate();\n+    if (result) {\n+      createFakeParentDirectory(src);\n+    }\n+    return result;\n+  }\n+\n+  private class DeleteIterator extends OzoneListingIterator {\n+    private boolean recursive;\n+\n+    DeleteIterator(Path f, boolean recursive)\n+        throws IOException {\n+      super(f);\n+      this.recursive = recursive;\n+      if (getStatus().isDirectory()\n+          && !this.recursive\n+          && listStatus(f).length != 0) {\n+        throw new PathIsNotEmptyDirectoryException(f.toString());\n+      }\n+    }\n+\n+    @Override\n+    boolean processKeyPath(String keyPath) throws IOException {\n+      if (keyPath.equals(\"\")) {\n+        LOG.trace(\"Skipping deleting root directory\");\n+        return true;\n+      } else {\n+        LOG.trace(\"deleting key path:\" + keyPath);\n+        boolean succeed = adapter.deleteObject(keyPath);\n+        // if recursive delete is requested ignore the return value of\n+        // deleteObject and issue deletes for other keys.\n+        return recursive || succeed;\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Deletes the children of the input dir path by iterating though the\n+   * DeleteIterator.\n+   *\n+   * @param f directory path to be deleted\n+   * @return true if successfully deletes all required keys, false otherwise\n+   * @throws IOException\n+   */\n+  private boolean innerDelete(Path f, boolean recursive) throws IOException {\n+    LOG.trace(\"delete() path:{} recursive:{}\", f, recursive);\n+    try {\n+      DeleteIterator iterator = new DeleteIterator(f, recursive);\n+      return iterator.iterate();\n+    } catch (FileNotFoundException e) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Couldn't delete {} - does not exist\", f);\n+      }\n+      return false;\n+    }\n+  }\n+\n+  @Override\n+  public boolean delete(Path f, boolean recursive) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_DELETE);\n+    statistics.incrementWriteOps(1);\n+    LOG.debug(\"Delete path {} - recursive {}\", f, recursive);\n+    FileStatus status;\n+    try {\n+      status = getFileStatus(f);\n+    } catch (FileNotFoundException ex) {\n+      LOG.warn(\"delete: Path does not exist: {}\", f);\n+      return false;\n+    }\n+\n+    String key = pathToKey(f);\n+    boolean result;\n+\n+    if (status.isDirectory()) {\n+      LOG.debug(\"delete: Path is a directory: {}\", f);\n+      key = addTrailingSlashIfNeeded(key);\n+\n+      if (key.equals(\"/\")) {\n+        LOG.warn(\"Cannot delete root directory.\");\n+        return false;\n+      }\n+\n+      result = innerDelete(f, recursive);\n+    } else {\n+      LOG.debug(\"delete: Path is a file: {}\", f);\n+      result = adapter.deleteObject(key);\n+    }\n+\n+    if (result) {\n+      // If this delete operation removes all files/directories from the\n+      // parent direcotry, then an empty parent directory must be created.\n+      createFakeParentDirectory(f);\n+    }\n+\n+    return result;\n+  }\n+\n+  /**\n+   * Create a fake parent directory key if it does not already exist and no\n+   * other child of this parent directory exists.\n+   *\n+   * @param f path to the fake parent directory\n+   * @throws IOException\n+   */\n+  private void createFakeParentDirectory(Path f) throws IOException {\n+    Path parent = f.getParent();\n+    if (parent != null && !parent.isRoot()) {\n+      createFakeDirectoryIfNecessary(parent);\n+    }\n+  }\n+\n+  /**\n+   * Create a fake directory key if it does not already exist.\n+   *\n+   * @param f path to the fake directory\n+   * @throws IOException\n+   */\n+  private void createFakeDirectoryIfNecessary(Path f) throws IOException {\n+    String key = pathToKey(f);\n+    if (!key.isEmpty() && !o3Exists(f)) {\n+      LOG.debug(\"Creating new fake directory at {}\", f);\n+      String dirKey = addTrailingSlashIfNeeded(key);\n+      adapter.createDirectory(dirKey);\n+    }\n+  }\n+\n+  /**\n+   * Check if a file or directory exists corresponding to given path.\n+   *\n+   * @param f path to file/directory.\n+   * @return true if it exists, false otherwise.\n+   * @throws IOException\n+   */\n+  private boolean o3Exists(final Path f) throws IOException {\n+    Path path = makeQualified(f);\n+    try {\n+      getFileStatus(path);\n+      return true;\n+    } catch (FileNotFoundException ex) {\n+      return false;\n+    }\n+  }\n+\n+  @Override\n+  public FileStatus[] listStatus(Path f) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_LIST_STATUS);\n+    statistics.incrementReadOps(1);\n+    LOG.trace(\"listStatus() path:{}\", f);\n+    int numEntries = LISTING_PAGE_SIZE;\n+    LinkedList<FileStatus> statuses = new LinkedList<>();\n+    List<FileStatus> tmpStatusList;\n+    String startPath = \"\";\n+\n+    do {\n+      tmpStatusList =\n+          adapter.listStatus(pathToKey(f), false, startPath,\n+              numEntries, uri, workingDir, getUsername())\n+              .stream()\n+              .map(this::convertFileStatus)\n+              .collect(Collectors.toList());\n+\n+      if (!tmpStatusList.isEmpty()) {\n+        if (startPath.isEmpty()) {\n+          statuses.addAll(tmpStatusList);\n+        } else {\n+          statuses.addAll(tmpStatusList.subList(1, tmpStatusList.size()));\n+        }\n+        startPath = pathToKey(statuses.getLast().getPath());\n+      }\n+      // listStatus returns entries numEntries in size if available.\n+      // Any lesser number of entries indicate that the required entries have\n+      // exhausted.\n+    } while (tmpStatusList.size() == numEntries);\n+\n+    return statuses.toArray(new FileStatus[0]);\n+  }\n+\n+  @Override\n+  public void setWorkingDirectory(Path newDir) {\n+    workingDir = newDir;\n+  }\n+\n+  @Override\n+  public Path getWorkingDirectory() {\n+    return workingDir;\n+  }\n+\n+  @Override\n+  public Token<?> getDelegationToken(String renewer) throws IOException {\n+    return adapter.getDelegationToken(renewer);\n+  }\n+\n+  /**\n+   * Get a canonical service name for this file system. If the URI is logical,\n+   * the hostname part of the URI will be returned.\n+   *\n+   * @return a service string that uniquely identifies this file system.\n+   */\n+  @Override\n+  public String getCanonicalServiceName() {\n+    return adapter.getCanonicalServiceName();\n+  }\n+\n+  /**\n+   * Get the username of the FS.\n+   *\n+   * @return the short name of the user who instantiated the FS\n+   */\n+  public String getUsername() {\n+    return userName;\n+  }\n+\n+  /**\n+   * Creates a directory. Directory is represented using a key with no value.\n+   *\n+   * @param path directory path to be created\n+   * @return true if directory exists or created successfully.\n+   * @throws IOException\n+   */\n+  private boolean mkdir(Path path) throws IOException {\n+    return adapter.createDirectory(pathToKey(path));\n+  }\n+\n+  @Override\n+  public boolean mkdirs(Path f, FsPermission permission) throws IOException {\n+    LOG.trace(\"mkdir() path:{} \", f);\n+    String key = pathToKey(f);\n+    if (isEmpty(key)) {\n+      return false;\n+    }\n+    return mkdir(f);\n+  }\n+\n+  @Override\n+  public FileStatus getFileStatus(Path f) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_GET_FILE_STATUS);\n+    statistics.incrementReadOps(1);\n+    LOG.trace(\"getFileStatus() path:{}\", f);\n+    Path qualifiedPath = f.makeQualified(uri, workingDir);\n+    String key = pathToKey(qualifiedPath);\n+    FileStatus fileStatus = null;\n+    try {\n+      fileStatus = convertFileStatus(\n+          adapter.getFileStatus(key, uri, qualifiedPath, getUsername()));\n+    } catch (OMException ex) {\n+      if (ex.getResult().equals(OMException.ResultCodes.KEY_NOT_FOUND)) {\n+        throw new FileNotFoundException(\"File not found. path:\" + f);\n+      }\n+    }\n+    return fileStatus;\n+  }\n+\n+  @Override\n+  public BlockLocation[] getFileBlockLocations(FileStatus fileStatus,\n+      long start, long len)\n+      throws IOException {\n+    if (fileStatus instanceof LocatedFileStatus) {\n+      return ((LocatedFileStatus) fileStatus).getBlockLocations();\n+    } else {\n+      return super.getFileBlockLocations(fileStatus, start, len);\n+    }\n+  }\n+\n+  /**\n+   * Turn a path (relative or otherwise) into an Ozone key.\n+   *\n+   * @param path the path of the file.\n+   * @return the key of the object that represents the file.\n+   */\n+  public String pathToKey(Path path) {\n+    Objects.requireNonNull(path, \"Path can't be null!\");\n+    if (!path.isAbsolute()) {\n+      path = new Path(workingDir, path);\n+    }\n+    // removing leading '/' char\n+    String key = path.toUri().getPath().substring(1);\n+    LOG.trace(\"path for key: {} is: {}\", key, path);\n+    return key;\n+  }\n+\n+  /**\n+   * Add trailing delimiter to path if it is already not present.\n+   *\n+   * @param key the ozone Key which needs to be appended\n+   * @return delimiter appended key\n+   */\n+  private String addTrailingSlashIfNeeded(String key) {\n+    if (!isEmpty(key) && !key.endsWith(OZONE_URI_DELIMITER)) {\n+      return key + OZONE_URI_DELIMITER;\n+    } else {\n+      return key;\n+    }\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return \"RootedOzoneFileSystem{URI=\" + uri + \", \"\n+        + \"workingDir=\" + workingDir + \", \"\n+        + \"userName=\" + userName + \", \"\n+        + \"statistics=\" + statistics\n+        + \"}\";\n+  }\n+\n+  /**\n+   * This class provides an interface to iterate through all the keys in the\n+   * bucket prefixed with the input path key and process them.\n+   * <p>\n+   * Each implementing class should define how the keys should be processed\n+   * through the processKeyPath() function.\n+   */\n+  private abstract class OzoneListingIterator {\n+    private final Path path;\n+    private final FileStatus status;\n+    private String pathKey;\n+    private Iterator<BasicKeyInfo> keyIterator;\n+\n+    OzoneListingIterator(Path path)\n+        throws IOException {\n+      this.path = path;\n+      this.status = getFileStatus(path);\n+      this.pathKey = pathToKey(path);\n+      if (status.isDirectory()) {\n+        this.pathKey = addTrailingSlashIfNeeded(pathKey);\n+      }\n+      keyIterator = adapter.listKeys(pathKey);\n+    }\n+\n+    /**\n+     * The output of processKey determines if further iteration through the\n+     * keys should be done or not.\n+     *\n+     * @return true if we should continue iteration of keys, false otherwise.\n+     * @throws IOException\n+     */\n+    abstract boolean processKeyPath(String keyPath) throws IOException;\n+\n+    /**\n+     * Iterates through all the keys prefixed with the input path's key and\n+     * processes the key though processKey().\n+     * If for any key, the processKey() returns false, then the iteration is\n+     * stopped and returned with false indicating that all the keys could not\n+     * be processed successfully.\n+     *\n+     * @return true if all keys are processed successfully, false otherwise.\n+     * @throws IOException\n+     */\n+    boolean iterate() throws IOException {\n+      LOG.trace(\"Iterating path: {}\", path);\n+      if (status.isDirectory()) {\n+        LOG.trace(\"Iterating directory: {}\", pathKey);\n+        OFSPath ofsPath = new OFSPath(pathKey);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdffa6ab5c1f59cedc968bf36c2ccd286cc6288f"}, "originalPosition": 719}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTgzMDU5OA==", "bodyText": "I did not understand what we are doing here. Could you please give an example.", "url": "https://github.com/apache/ozone/pull/415#discussion_r369830598", "createdAt": "2020-01-22T22:07:27Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,805 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import com.google.common.base.Preconditions;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.CreateFlag;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocatedFileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PathIsNotEmptyDirectoryException;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.util.Progressable;\n+import org.apache.http.client.utils.URIBuilder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.EnumSet;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hadoop.fs.ozone.Constants.LISTING_PAGE_SIZE;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_DEFAULT_USER;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_USER_DIR;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_URI_DELIMITER;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_OFS_URI_SCHEME;\n+\n+/**\n+ * The minimal Ozone Filesystem implementation.\n+ * <p>\n+ * This is a basic version which doesn't extend\n+ * KeyProviderTokenIssuer and doesn't include statistics. It can be used\n+ * from older hadoop version. For newer hadoop version use the full featured\n+ * OFileSystem.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Evolving\n+public class BasicRootedOzoneFileSystem extends FileSystem {\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneFileSystem.class);\n+\n+  /**\n+   * The Ozone client for connecting to Ozone server.\n+   */\n+\n+  private URI uri;\n+  private String userName;\n+  private Path workingDir;\n+//  private String gOmHost;\n+//  private int gOmPort;\n+//  private Configuration gConf;\n+//  private boolean gIsolatedClassloader;\n+\n+  private RootedOzoneClientAdapter adapter;\n+//  private String adapterPath;\n+\n+  private static final String URI_EXCEPTION_TEXT =\n+      \"URL should be one of the following formats: \" +\n+      \"ofs://om-service-id/  OR \" +\n+      \"ofs://om-host.example.com/  OR \" +\n+      \"ofs://om-host.example.com:5678/\";\n+\n+  @Override\n+  public void initialize(URI name, Configuration conf) throws IOException {\n+    super.initialize(name, conf);\n+    setConf(conf);\n+    Objects.requireNonNull(name.getScheme(), \"No scheme provided in \" + name);\n+    Preconditions.checkArgument(getScheme().equals(name.getScheme()),\n+        \"Invalid scheme provided in \" + name);\n+\n+    String authority = name.getAuthority();\n+    if (authority == null) {\n+      // authority is null when fs.defaultFS is not a qualified ofs URI and\n+      // ofs:/// is passed to the client. matcher will NPE if authority is null\n+      throw new IllegalArgumentException(URI_EXCEPTION_TEXT);\n+    }\n+\n+    String omHost;\n+    int omPort = -1;\n+    // Parse hostname and port\n+    String[] parts = authority.split(\":\");\n+    if (parts.length > 2) {\n+      throw new IllegalArgumentException(URI_EXCEPTION_TEXT);\n+    }\n+    omHost = parts[0];\n+    if (parts.length == 2) {\n+      try {\n+        omPort = Integer.parseInt(parts[1]);\n+      } catch (NumberFormatException e) {\n+        throw new IllegalArgumentException(URI_EXCEPTION_TEXT);\n+      }\n+    }\n+\n+    try {\n+      uri = new URIBuilder().setScheme(OZONE_OFS_URI_SCHEME)\n+          .setHost(authority)\n+          .build();\n+      LOG.trace(\"Ozone URI for OFS initialization is \" + uri);\n+\n+      //isolated is the default for ozonefs-lib-legacy which includes the\n+      // /ozonefs.txt, otherwise the default is false. It could be overridden.\n+      boolean defaultValue =\n+          BasicRootedOzoneFileSystem.class.getClassLoader()\n+              .getResource(\"ozonefs.txt\") != null;\n+\n+      //Use string here instead of the constant as constant may not be available\n+      //on the classpath of a hadoop 2.7\n+      boolean isolatedClassloader =\n+          conf.getBoolean(\"ozone.fs.isolated-classloader\", defaultValue);\n+\n+      // adapter should be initialized in operations.\n+      this.adapter = createAdapter(conf, omHost, omPort, isolatedClassloader);\n+\n+      try {\n+        this.userName =\n+            UserGroupInformation.getCurrentUser().getShortUserName();\n+      } catch (IOException e) {\n+        this.userName = OZONE_DEFAULT_USER;\n+      }\n+      this.workingDir = new Path(OZONE_USER_DIR, this.userName)\n+          .makeQualified(this.uri, this.workingDir);\n+    } catch (URISyntaxException ue) {\n+      final String msg = \"Invalid Ozone endpoint \" + name;\n+      LOG.error(msg, ue);\n+      throw new IOException(msg, ue);\n+    }\n+  }\n+\n+  protected RootedOzoneClientAdapter createAdapter(Configuration conf,\n+      String omHost, int omPort, boolean isolatedClassloader)\n+      throws IOException {\n+\n+    if (isolatedClassloader) {\n+      // TODO: Check how this code path need to be changed, for legacy Hadoop?\n+      return RootedOzoneClientAdapterFactory.createAdapter();\n+    } else {\n+      return new BasicRootedOzoneClientAdapterImpl(omHost, omPort, conf);\n+    }\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    try {\n+      adapter.close();\n+    } finally {\n+      super.close();\n+    }\n+  }\n+\n+  @Override\n+  public URI getUri() {\n+    return uri;\n+  }\n+\n+  @Override\n+  public String getScheme() {\n+    return OZONE_OFS_URI_SCHEME;\n+  }\n+\n+  @Override\n+  public FSDataInputStream open(Path path, int bufferSize) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_OPEN);\n+    statistics.incrementReadOps(1);\n+    LOG.trace(\"open() path: {}\", path);\n+    final String key = pathToKey(path);\n+    return new FSDataInputStream(\n+        new OzoneFSInputStream(adapter.readFile(key), statistics));\n+  }\n+\n+  protected void incrementCounter(Statistic statistic) {\n+    //don't do anything in this default implementation.\n+  }\n+\n+  @Override\n+  public FSDataOutputStream create(Path f, FsPermission permission,\n+      boolean overwrite, int bufferSize,\n+      short replication, long blockSize,\n+      Progressable progress) throws IOException {\n+    LOG.trace(\"create() path:{}\", f);\n+    incrementCounter(Statistic.INVOCATION_CREATE);\n+    statistics.incrementWriteOps(1);\n+    final String key = pathToKey(f);\n+    return createOutputStream(key, overwrite, true);\n+  }\n+\n+  @Override\n+  public FSDataOutputStream createNonRecursive(Path path,\n+      FsPermission permission,\n+      EnumSet<CreateFlag> flags,\n+      int bufferSize,\n+      short replication,\n+      long blockSize,\n+      Progressable progress) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_CREATE_NON_RECURSIVE);\n+    statistics.incrementWriteOps(1);\n+    final String key = pathToKey(path);\n+    return createOutputStream(key, flags.contains(CreateFlag.OVERWRITE), false);\n+  }\n+\n+  private FSDataOutputStream createOutputStream(String key, boolean overwrite,\n+      boolean recursive) throws IOException {\n+    return new FSDataOutputStream(adapter.createFile(key, overwrite, recursive),\n+        statistics);\n+  }\n+\n+  @Override\n+  public FSDataOutputStream append(Path f, int bufferSize,\n+      Progressable progress) throws IOException {\n+    throw new UnsupportedOperationException(\"append() Not implemented by the \"\n+        + getClass().getSimpleName() + \" FileSystem implementation\");\n+  }\n+\n+  private class RenameIterator extends OzoneListingIterator {\n+    private final String srcPath;\n+    private final String dstPath;\n+\n+    RenameIterator(Path srcPath, Path dstPath)\n+        throws IOException {\n+      super(srcPath);\n+      this.srcPath = pathToKey(srcPath);\n+      this.dstPath = pathToKey(dstPath);\n+      LOG.trace(\"rename from:{} to:{}\", this.srcPath, this.dstPath);\n+    }\n+\n+    @Override\n+    boolean processKeyPath(String keyPath) throws IOException {\n+      String newPath = dstPath.concat(keyPath.substring(srcPath.length()));\n+      adapter.renamePath(keyPath, newPath);\n+      return true;\n+    }\n+  }\n+\n+  /**\n+   * Check whether the source and destination path are valid and then perform\n+   * rename from source path to destination path.\n+   * <p>\n+   * The rename operation is performed by renaming the keys with src as prefix.\n+   * For such keys the prefix is changed from src to dst.\n+   *\n+   * @param src source path for rename\n+   * @param dst destination path for rename\n+   * @return true if rename operation succeeded or\n+   * if the src and dst have the same path and are of the same type\n+   * @throws IOException on I/O errors or if the src/dst paths are invalid.\n+   */\n+  @Override\n+  public boolean rename(Path src, Path dst) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_RENAME);\n+    statistics.incrementWriteOps(1);\n+    if (src.equals(dst)) {\n+      return true;\n+    }\n+\n+    LOG.trace(\"rename() from: {} to: {}\", src, dst);\n+    if (src.isRoot()) {\n+      // Cannot rename root of file system\n+      LOG.trace(\"Cannot rename the root of a filesystem\");\n+      return false;\n+    }\n+\n+    // src and dst should be in the same bucket\n+    OFSPath ofsSrc = new OFSPath(src);\n+    OFSPath ofsDst = new OFSPath(dst);\n+    if (!ofsSrc.isInSameBucketAs(ofsDst)) {\n+      throw new IOException(\"Cannot rename a key to a different bucket\");\n+    }\n+\n+    // Cannot rename a directory to its own subdirectory\n+    Path dstParent = dst.getParent();\n+    while (dstParent != null && !src.equals(dstParent)) {\n+      dstParent = dstParent.getParent();\n+    }\n+    Preconditions.checkArgument(dstParent == null,\n+        \"Cannot rename a directory to its own subdirectory\");\n+    // Check if the source exists\n+    FileStatus srcStatus;\n+    try {\n+      srcStatus = getFileStatus(src);\n+    } catch (FileNotFoundException fnfe) {\n+      // source doesn't exist, return\n+      return false;\n+    }\n+\n+    // Check if the destination exists\n+    FileStatus dstStatus;\n+    try {\n+      dstStatus = getFileStatus(dst);\n+    } catch (FileNotFoundException fnde) {\n+      dstStatus = null;\n+    }\n+\n+    if (dstStatus == null) {\n+      // If dst doesn't exist, check whether dst parent dir exists or not\n+      // if the parent exists, the source can still be renamed to dst path\n+      dstStatus = getFileStatus(dst.getParent());\n+      if (!dstStatus.isDirectory()) {\n+        throw new IOException(String.format(\n+            \"Failed to rename %s to %s, %s is a file\", src, dst,\n+            dst.getParent()));\n+      }\n+    } else {\n+      // if dst exists and source and destination are same,\n+      // check both the src and dst are of same type\n+      if (srcStatus.getPath().equals(dstStatus.getPath())) {\n+        return !srcStatus.isDirectory();\n+      } else if (dstStatus.isDirectory()) {\n+        // If dst is a directory, rename source as subpath of it.\n+        // for example rename /source to /dst will lead to /dst/source\n+        dst = new Path(dst, src.getName());\n+        FileStatus[] statuses;\n+        try {\n+          statuses = listStatus(dst);\n+        } catch (FileNotFoundException fnde) {\n+          statuses = null;\n+        }\n+\n+        if (statuses != null && statuses.length > 0) {\n+          // If dst exists and not a directory not empty\n+          throw new FileAlreadyExistsException(String.format(\n+              \"Failed to rename %s to %s, file already exists or not empty!\",\n+              src, dst));\n+        }\n+      } else {\n+        // If dst is not a directory\n+        throw new FileAlreadyExistsException(String.format(\n+            \"Failed to rename %s to %s, file already exists!\", src, dst));\n+      }\n+    }\n+\n+    if (srcStatus.isDirectory()) {\n+      if (dst.toString().startsWith(src.toString() + OZONE_URI_DELIMITER)) {\n+        LOG.trace(\"Cannot rename a directory to a subdirectory of self\");\n+        return false;\n+      }\n+    }\n+    RenameIterator iterator = new RenameIterator(src, dst);\n+    boolean result = iterator.iterate();\n+    if (result) {\n+      createFakeParentDirectory(src);\n+    }\n+    return result;\n+  }\n+\n+  private class DeleteIterator extends OzoneListingIterator {\n+    private boolean recursive;\n+\n+    DeleteIterator(Path f, boolean recursive)\n+        throws IOException {\n+      super(f);\n+      this.recursive = recursive;\n+      if (getStatus().isDirectory()\n+          && !this.recursive\n+          && listStatus(f).length != 0) {\n+        throw new PathIsNotEmptyDirectoryException(f.toString());\n+      }\n+    }\n+\n+    @Override\n+    boolean processKeyPath(String keyPath) throws IOException {\n+      if (keyPath.equals(\"\")) {\n+        LOG.trace(\"Skipping deleting root directory\");\n+        return true;\n+      } else {\n+        LOG.trace(\"deleting key path:\" + keyPath);\n+        boolean succeed = adapter.deleteObject(keyPath);\n+        // if recursive delete is requested ignore the return value of\n+        // deleteObject and issue deletes for other keys.\n+        return recursive || succeed;\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Deletes the children of the input dir path by iterating though the\n+   * DeleteIterator.\n+   *\n+   * @param f directory path to be deleted\n+   * @return true if successfully deletes all required keys, false otherwise\n+   * @throws IOException\n+   */\n+  private boolean innerDelete(Path f, boolean recursive) throws IOException {\n+    LOG.trace(\"delete() path:{} recursive:{}\", f, recursive);\n+    try {\n+      DeleteIterator iterator = new DeleteIterator(f, recursive);\n+      return iterator.iterate();\n+    } catch (FileNotFoundException e) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Couldn't delete {} - does not exist\", f);\n+      }\n+      return false;\n+    }\n+  }\n+\n+  @Override\n+  public boolean delete(Path f, boolean recursive) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_DELETE);\n+    statistics.incrementWriteOps(1);\n+    LOG.debug(\"Delete path {} - recursive {}\", f, recursive);\n+    FileStatus status;\n+    try {\n+      status = getFileStatus(f);\n+    } catch (FileNotFoundException ex) {\n+      LOG.warn(\"delete: Path does not exist: {}\", f);\n+      return false;\n+    }\n+\n+    String key = pathToKey(f);\n+    boolean result;\n+\n+    if (status.isDirectory()) {\n+      LOG.debug(\"delete: Path is a directory: {}\", f);\n+      key = addTrailingSlashIfNeeded(key);\n+\n+      if (key.equals(\"/\")) {\n+        LOG.warn(\"Cannot delete root directory.\");\n+        return false;\n+      }\n+\n+      result = innerDelete(f, recursive);\n+    } else {\n+      LOG.debug(\"delete: Path is a file: {}\", f);\n+      result = adapter.deleteObject(key);\n+    }\n+\n+    if (result) {\n+      // If this delete operation removes all files/directories from the\n+      // parent direcotry, then an empty parent directory must be created.\n+      createFakeParentDirectory(f);\n+    }\n+\n+    return result;\n+  }\n+\n+  /**\n+   * Create a fake parent directory key if it does not already exist and no\n+   * other child of this parent directory exists.\n+   *\n+   * @param f path to the fake parent directory\n+   * @throws IOException\n+   */\n+  private void createFakeParentDirectory(Path f) throws IOException {\n+    Path parent = f.getParent();\n+    if (parent != null && !parent.isRoot()) {\n+      createFakeDirectoryIfNecessary(parent);\n+    }\n+  }\n+\n+  /**\n+   * Create a fake directory key if it does not already exist.\n+   *\n+   * @param f path to the fake directory\n+   * @throws IOException\n+   */\n+  private void createFakeDirectoryIfNecessary(Path f) throws IOException {\n+    String key = pathToKey(f);\n+    if (!key.isEmpty() && !o3Exists(f)) {\n+      LOG.debug(\"Creating new fake directory at {}\", f);\n+      String dirKey = addTrailingSlashIfNeeded(key);\n+      adapter.createDirectory(dirKey);\n+    }\n+  }\n+\n+  /**\n+   * Check if a file or directory exists corresponding to given path.\n+   *\n+   * @param f path to file/directory.\n+   * @return true if it exists, false otherwise.\n+   * @throws IOException\n+   */\n+  private boolean o3Exists(final Path f) throws IOException {\n+    Path path = makeQualified(f);\n+    try {\n+      getFileStatus(path);\n+      return true;\n+    } catch (FileNotFoundException ex) {\n+      return false;\n+    }\n+  }\n+\n+  @Override\n+  public FileStatus[] listStatus(Path f) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_LIST_STATUS);\n+    statistics.incrementReadOps(1);\n+    LOG.trace(\"listStatus() path:{}\", f);\n+    int numEntries = LISTING_PAGE_SIZE;\n+    LinkedList<FileStatus> statuses = new LinkedList<>();\n+    List<FileStatus> tmpStatusList;\n+    String startPath = \"\";\n+\n+    do {\n+      tmpStatusList =\n+          adapter.listStatus(pathToKey(f), false, startPath,\n+              numEntries, uri, workingDir, getUsername())\n+              .stream()\n+              .map(this::convertFileStatus)\n+              .collect(Collectors.toList());\n+\n+      if (!tmpStatusList.isEmpty()) {\n+        if (startPath.isEmpty()) {\n+          statuses.addAll(tmpStatusList);\n+        } else {\n+          statuses.addAll(tmpStatusList.subList(1, tmpStatusList.size()));\n+        }\n+        startPath = pathToKey(statuses.getLast().getPath());\n+      }\n+      // listStatus returns entries numEntries in size if available.\n+      // Any lesser number of entries indicate that the required entries have\n+      // exhausted.\n+    } while (tmpStatusList.size() == numEntries);\n+\n+    return statuses.toArray(new FileStatus[0]);\n+  }\n+\n+  @Override\n+  public void setWorkingDirectory(Path newDir) {\n+    workingDir = newDir;\n+  }\n+\n+  @Override\n+  public Path getWorkingDirectory() {\n+    return workingDir;\n+  }\n+\n+  @Override\n+  public Token<?> getDelegationToken(String renewer) throws IOException {\n+    return adapter.getDelegationToken(renewer);\n+  }\n+\n+  /**\n+   * Get a canonical service name for this file system. If the URI is logical,\n+   * the hostname part of the URI will be returned.\n+   *\n+   * @return a service string that uniquely identifies this file system.\n+   */\n+  @Override\n+  public String getCanonicalServiceName() {\n+    return adapter.getCanonicalServiceName();\n+  }\n+\n+  /**\n+   * Get the username of the FS.\n+   *\n+   * @return the short name of the user who instantiated the FS\n+   */\n+  public String getUsername() {\n+    return userName;\n+  }\n+\n+  /**\n+   * Creates a directory. Directory is represented using a key with no value.\n+   *\n+   * @param path directory path to be created\n+   * @return true if directory exists or created successfully.\n+   * @throws IOException\n+   */\n+  private boolean mkdir(Path path) throws IOException {\n+    return adapter.createDirectory(pathToKey(path));\n+  }\n+\n+  @Override\n+  public boolean mkdirs(Path f, FsPermission permission) throws IOException {\n+    LOG.trace(\"mkdir() path:{} \", f);\n+    String key = pathToKey(f);\n+    if (isEmpty(key)) {\n+      return false;\n+    }\n+    return mkdir(f);\n+  }\n+\n+  @Override\n+  public FileStatus getFileStatus(Path f) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_GET_FILE_STATUS);\n+    statistics.incrementReadOps(1);\n+    LOG.trace(\"getFileStatus() path:{}\", f);\n+    Path qualifiedPath = f.makeQualified(uri, workingDir);\n+    String key = pathToKey(qualifiedPath);\n+    FileStatus fileStatus = null;\n+    try {\n+      fileStatus = convertFileStatus(\n+          adapter.getFileStatus(key, uri, qualifiedPath, getUsername()));\n+    } catch (OMException ex) {\n+      if (ex.getResult().equals(OMException.ResultCodes.KEY_NOT_FOUND)) {\n+        throw new FileNotFoundException(\"File not found. path:\" + f);\n+      }\n+    }\n+    return fileStatus;\n+  }\n+\n+  @Override\n+  public BlockLocation[] getFileBlockLocations(FileStatus fileStatus,\n+      long start, long len)\n+      throws IOException {\n+    if (fileStatus instanceof LocatedFileStatus) {\n+      return ((LocatedFileStatus) fileStatus).getBlockLocations();\n+    } else {\n+      return super.getFileBlockLocations(fileStatus, start, len);\n+    }\n+  }\n+\n+  /**\n+   * Turn a path (relative or otherwise) into an Ozone key.\n+   *\n+   * @param path the path of the file.\n+   * @return the key of the object that represents the file.\n+   */\n+  public String pathToKey(Path path) {\n+    Objects.requireNonNull(path, \"Path can't be null!\");\n+    if (!path.isAbsolute()) {\n+      path = new Path(workingDir, path);\n+    }\n+    // removing leading '/' char\n+    String key = path.toUri().getPath().substring(1);\n+    LOG.trace(\"path for key: {} is: {}\", key, path);\n+    return key;\n+  }\n+\n+  /**\n+   * Add trailing delimiter to path if it is already not present.\n+   *\n+   * @param key the ozone Key which needs to be appended\n+   * @return delimiter appended key\n+   */\n+  private String addTrailingSlashIfNeeded(String key) {\n+    if (!isEmpty(key) && !key.endsWith(OZONE_URI_DELIMITER)) {\n+      return key + OZONE_URI_DELIMITER;\n+    } else {\n+      return key;\n+    }\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return \"RootedOzoneFileSystem{URI=\" + uri + \", \"\n+        + \"workingDir=\" + workingDir + \", \"\n+        + \"userName=\" + userName + \", \"\n+        + \"statistics=\" + statistics\n+        + \"}\";\n+  }\n+\n+  /**\n+   * This class provides an interface to iterate through all the keys in the\n+   * bucket prefixed with the input path key and process them.\n+   * <p>\n+   * Each implementing class should define how the keys should be processed\n+   * through the processKeyPath() function.\n+   */\n+  private abstract class OzoneListingIterator {\n+    private final Path path;\n+    private final FileStatus status;\n+    private String pathKey;\n+    private Iterator<BasicKeyInfo> keyIterator;\n+\n+    OzoneListingIterator(Path path)\n+        throws IOException {\n+      this.path = path;\n+      this.status = getFileStatus(path);\n+      this.pathKey = pathToKey(path);\n+      if (status.isDirectory()) {\n+        this.pathKey = addTrailingSlashIfNeeded(pathKey);\n+      }\n+      keyIterator = adapter.listKeys(pathKey);\n+    }\n+\n+    /**\n+     * The output of processKey determines if further iteration through the\n+     * keys should be done or not.\n+     *\n+     * @return true if we should continue iteration of keys, false otherwise.\n+     * @throws IOException\n+     */\n+    abstract boolean processKeyPath(String keyPath) throws IOException;\n+\n+    /**\n+     * Iterates through all the keys prefixed with the input path's key and\n+     * processes the key though processKey().\n+     * If for any key, the processKey() returns false, then the iteration is\n+     * stopped and returned with false indicating that all the keys could not\n+     * be processed successfully.\n+     *\n+     * @return true if all keys are processed successfully, false otherwise.\n+     * @throws IOException\n+     */\n+    boolean iterate() throws IOException {\n+      LOG.trace(\"Iterating path: {}\", path);\n+      if (status.isDirectory()) {\n+        LOG.trace(\"Iterating directory: {}\", pathKey);\n+        OFSPath ofsPath = new OFSPath(pathKey);\n+        while (keyIterator.hasNext()) {\n+          BasicKeyInfo key = keyIterator.next();\n+          String keyPath = ofsPath.getNonKeyPathNoPrefixDelim() +\n+              OZONE_URI_DELIMITER + key.getName();\n+          LOG.trace(\"iterating key path: {}\", keyPath);\n+          if (!processKeyPath(keyPath)) {\n+            return false;\n+          }\n+        }\n+        return true;\n+      } else {\n+        LOG.trace(\"iterating file: {}\", path);\n+        return processKeyPath(pathKey);\n+      }\n+    }\n+\n+    String getPathKey() {\n+      return pathKey;\n+    }\n+\n+    boolean pathIsDirectory() {\n+      return status.isDirectory();\n+    }\n+\n+    FileStatus getStatus() {\n+      return status;\n+    }\n+  }\n+\n+  public RootedOzoneClientAdapter getAdapter() {\n+    return adapter;\n+  }\n+\n+  public boolean isEmpty(CharSequence cs) {\n+    return cs == null || cs.length() == 0;\n+  }\n+\n+  public boolean isNumber(String number) {\n+    try {\n+      Integer.parseInt(number);\n+    } catch (NumberFormatException ex) {\n+      return false;\n+    }\n+    return true;\n+  }\n+\n+  private FileStatus convertFileStatus(FileStatusAdapter fileStatusAdapter) {\n+    Path symLink = null;\n+    try {\n+      fileStatusAdapter.getSymlink();\n+    } catch (Exception ex) {\n+      //NOOP: If not symlink symlink remains null.\n+    }\n+\n+    // Process path.\n+    URI newUri = fileStatusAdapter.getPath().toUri();\n+    try {\n+      newUri = new URIBuilder().setScheme(newUri.getScheme())\n+          .setHost(newUri.getAuthority())\n+          .setPath(newUri.getPath())\n+          .build();\n+    } catch (URISyntaxException e) {\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdffa6ab5c1f59cedc968bf36c2ccd286cc6288f"}, "originalPosition": 782}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTgzMjMzNg==", "bodyText": "Can you please add comments explaining how vol, bucket and mounts are used.", "url": "https://github.com/apache/ozone/pull/415#discussion_r369832336", "createdAt": "2020-01-22T22:11:16Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OFSPath.java", "diffHunk": "@@ -0,0 +1,137 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.yetus.audience.InterfaceStability;\n+import java.util.StringTokenizer;\n+\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_URI_DELIMITER;\n+\n+/**\n+ * Utility class for Rooted Ozone Filesystem (OFS) path processing.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Unstable\n+class OFSPath {\n+  private String volumeName;\n+  private String bucketName;\n+  private String mountName;\n+  private String keyName;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdffa6ab5c1f59cedc968bf36c2ccd286cc6288f"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTgzODI5NA==", "bodyText": "testFSUriHostVersionDefault() and testFSUriWithHostPortUnspecified() need to be modified to test ofs.", "url": "https://github.com/apache/ozone/pull/415#discussion_r369838294", "createdAt": "2020-01-22T22:24:31Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/TestRootedOzoneFileSystemWithMocks.java", "diffHunk": "@@ -0,0 +1,152 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.ozone.OmUtils;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClient;\n+import org.apache.hadoop.ozone.client.OzoneClientFactory;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.powermock.api.mockito.PowerMockito;\n+import org.powermock.core.classloader.annotations.PowerMockIgnore;\n+import org.powermock.core.classloader.annotations.PrepareForTest;\n+import org.powermock.modules.junit4.PowerMockRunner;\n+\n+import java.net.URI;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.mockito.Matchers.eq;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+/**\n+ * Ozone File system tests that are light weight and use mocks.\n+ */\n+@RunWith(PowerMockRunner.class)\n+@PrepareForTest({ OzoneClientFactory.class, UserGroupInformation.class })\n+@PowerMockIgnore(\"javax.management.*\")\n+public class TestRootedOzoneFileSystemWithMocks {\n+\n+  @Test\n+  public void testFSUriWithHostPortOverrides() throws Exception {\n+    Configuration conf = new OzoneConfiguration();\n+    OzoneClient ozoneClient = mock(OzoneClient.class);\n+    ObjectStore objectStore = mock(ObjectStore.class);\n+    OzoneVolume volume = mock(OzoneVolume.class);\n+    OzoneBucket bucket = mock(OzoneBucket.class);\n+\n+    when(ozoneClient.getObjectStore()).thenReturn(objectStore);\n+    when(objectStore.getVolume(eq(\"volume1\"))).thenReturn(volume);\n+    when(volume.getBucket(\"bucket1\")).thenReturn(bucket);\n+\n+    PowerMockito.mockStatic(OzoneClientFactory.class);\n+    PowerMockito.when(OzoneClientFactory.getRpcClient(eq(\"local.host\"),\n+        eq(5899), eq(conf))).thenReturn(ozoneClient);\n+\n+    UserGroupInformation ugi = mock(UserGroupInformation.class);\n+    PowerMockito.mockStatic(UserGroupInformation.class);\n+    PowerMockito.when(UserGroupInformation.getCurrentUser()).thenReturn(ugi);\n+    when(ugi.getShortUserName()).thenReturn(\"user1\");\n+\n+    // Note: FileSystem#loadFileSystems doesn't load OFS class because\n+    //  META-INF still points to org.apache.hadoop.fs.ozone.OzoneFileSystem\n+    conf.set(\"fs.ofs.impl\", \"org.apache.hadoop.fs.ozone.RootedOzoneFileSystem\");\n+    URI uri = new URI(\"ofs://local.host:5899/volume1/bucket1\");\n+\n+    FileSystem fileSystem = FileSystem.get(uri, conf);\n+    RootedOzoneFileSystem ofs = (RootedOzoneFileSystem) fileSystem;\n+\n+    assertEquals(ofs.getUri().getAuthority(), \"local.host:5899\");\n+    PowerMockito.verifyStatic();\n+    OzoneClientFactory.getRpcClient(\"local.host\", 5899, conf);\n+  }\n+\n+  @Test\n+  public void testFSUriWithHostPortUnspecified() throws Exception {\n+    Configuration conf = new OzoneConfiguration();\n+    final int omPort = OmUtils.getOmRpcPort(conf);\n+\n+    OzoneClient ozoneClient = mock(OzoneClient.class);\n+    ObjectStore objectStore = mock(ObjectStore.class);\n+    OzoneVolume volume = mock(OzoneVolume.class);\n+    OzoneBucket bucket = mock(OzoneBucket.class);\n+\n+    when(ozoneClient.getObjectStore()).thenReturn(objectStore);\n+    when(objectStore.getVolume(eq(\"volume1\"))).thenReturn(volume);\n+    when(volume.getBucket(\"bucket1\")).thenReturn(bucket);\n+\n+    PowerMockito.mockStatic(OzoneClientFactory.class);\n+    PowerMockito.when(OzoneClientFactory.getRpcClient(eq(\"local.host\"),\n+        eq(omPort), eq(conf))).thenReturn(ozoneClient);\n+\n+    UserGroupInformation ugi = mock(UserGroupInformation.class);\n+    PowerMockito.mockStatic(UserGroupInformation.class);\n+    PowerMockito.when(UserGroupInformation.getCurrentUser()).thenReturn(ugi);\n+    when(ugi.getShortUserName()).thenReturn(\"user1\");\n+\n+    URI uri = new URI(\"o3fs://bucket1.volume1.local.host\");\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdffa6ab5c1f59cedc968bf36c2ccd286cc6288f"}, "originalPosition": 110}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ab64a98fadd52b483136f99031d8ed9d401b36c9", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/ab64a98fadd52b483136f99031d8ed9d401b36c9", "committedDate": "2020-01-23T23:56:36Z", "message": "Ignore potential VOLUME_ALREADY_EXISTS and BUCKET_ALREADY_EXISTS in getBucket(), addressing comment https://github.com/apache/hadoop-ozone/pull/415#commitcomment-36906753"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7a3de51e6e23abba3e91971f80e8666c1fc6c108", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/7a3de51e6e23abba3e91971f80e8666c1fc6c108", "committedDate": "2020-01-24T00:18:01Z", "message": "Make testBucketPath class global, addressing https://github.com/apache/hadoop-ozone/pull/415#discussion_r369302919"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f3f6d459b8bfe0435cfee20cab3e7e25263db1ee", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/f3f6d459b8bfe0435cfee20cab3e7e25263db1ee", "committedDate": "2020-01-27T08:12:46Z", "message": "Clean up OFSPath."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c323b6cb8051907bf6e2c456191eb07aafc4d860", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/c323b6cb8051907bf6e2c456191eb07aafc4d860", "committedDate": "2020-01-27T11:22:13Z", "message": "Improved testListStatus(), addressing https://github.com/apache/hadoop-ozone/pull/415#discussion_r369304095"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "11ac3a9c4c4118005017dd5c1c1cbe59ffbfd4e3", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/11ac3a9c4c4118005017dd5c1c1cbe59ffbfd4e3", "committedDate": "2020-01-27T11:26:31Z", "message": "Move rename to different bucket test case to a new unit test `testRenameToDifferentBucket()`, addressing https://github.com/apache/hadoop-ozone/pull/415#discussion_r369308556"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ae95babbefcb67acd9e33a736600086348fa3598", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/ae95babbefcb67acd9e33a736600086348fa3598", "committedDate": "2020-01-27T11:28:05Z", "message": "Remove print from testRenameToDifferentBucket success, addressing https://github.com/apache/hadoop-ozone/pull/415#discussion_r369308830"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f6001f660dc7884dc0aa14efaf4aa5a84d41784a", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/f6001f660dc7884dc0aa14efaf4aa5a84d41784a", "committedDate": "2020-01-27T11:29:12Z", "message": "TROFS: Rename getKeyInBucket -> getKey. https://github.com/apache/hadoop-ozone/pull/415#discussion_r369309173"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b21005ebacf25f5a3a6bfdb97348a2e2e567423d", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/b21005ebacf25f5a3a6bfdb97348a2e2e567423d", "committedDate": "2020-01-27T11:44:51Z", "message": "Add comment for BROCAI#listStatus. https://github.com/apache/hadoop-ozone/pull/415#discussion_r369325484"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "da12d487fe5c5bb05f013c33cb2d02aba96b63c3", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/da12d487fe5c5bb05f013c33cb2d02aba96b63c3", "committedDate": "2020-01-27T11:47:29Z", "message": "Clean up code. https://github.com/apache/hadoop-ozone/pull/415#discussion_r369328737"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8e34c5f6718680c35b683ca4b0969286e975e318", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/8e34c5f6718680c35b683ca4b0969286e975e318", "committedDate": "2020-01-27T11:50:20Z", "message": "omHost -> omHostOrServiceId. https://github.com/apache/hadoop-ozone/pull/415#discussion_r369330201"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "881e9a49d7af59821dea1161a7db1da91d38231b", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/881e9a49d7af59821dea1161a7db1da91d38231b", "committedDate": "2020-01-27T13:42:24Z", "message": "Reduce # of RPC calls in rename and delete iterator. https://github.com/apache/hadoop-ozone/pull/415#discussion_r369801507 and https://github.com/apache/hadoop-ozone/pull/415#discussion_r369821804"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "10fe834c08cf72911b6d0409fb1138ff7d23294e", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/10fe834c08cf72911b6d0409fb1138ff7d23294e", "committedDate": "2020-01-27T13:49:24Z", "message": "Add comment&TODO for OzoneListingIterator key to full path conversion. https://github.com/apache/hadoop-ozone/pull/415#discussion_r369827923"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fa73d519e68e1dff5b98ad40bb7a69d9eb697efb", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/fa73d519e68e1dff5b98ad40bb7a69d9eb697efb", "committedDate": "2020-01-27T13:58:29Z", "message": "Remove useless code in convertFileStatus. https://github.com/apache/hadoop-ozone/pull/415#discussion_r369830598"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4bb7ed19674dcb959703fe633e940ab4bf17873e", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/4bb7ed19674dcb959703fe633e940ab4bf17873e", "committedDate": "2020-01-27T14:14:21Z", "message": "Add a table for OFSPath. https://github.com/apache/hadoop-ozone/pull/415#discussion_r369832336"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5a20f9da30a25fd40fa1a73e6b0e742f40647a94", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/5a20f9da30a25fd40fa1a73e6b0e742f40647a94", "committedDate": "2020-01-27T14:35:22Z", "message": "Remove vol/buc path from testFSUriWithHostPortOverrides.\nMod testFSUriWithHostPortUnspecified to test OFS.\nRemove testFSUriHostVersionDefault as omitting host name is not allowed in OFS.\nhttps://github.com/apache/hadoop-ozone/pull/415#discussion_r369838294"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3c9727dbfc6221b8e44decef56f836af78eeef4e", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/3c9727dbfc6221b8e44decef56f836af78eeef4e", "committedDate": "2020-01-27T14:39:45Z", "message": "Unused imports - address checkstyle."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "60b4bf8e6008cbd53d95c3e0f249f33227209c1e", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/60b4bf8e6008cbd53d95c3e0f249f33227209c1e", "committedDate": "2020-01-28T02:34:28Z", "message": "Fix a case where `mkdir -p /volume1` or `mkdir -p /volume1/bucket2` doesn't work: https://github.com/apache/hadoop-ozone/pull/415#issuecomment-578954095\nAdd a few more mkdir test cases for above.\nAdd TestOFSPath."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ5NjMxNzI4", "url": "https://github.com/apache/ozone/pull/415#pullrequestreview-349631728", "createdAt": "2020-01-28T19:12:42Z", "commit": {"oid": "60b4bf8e6008cbd53d95c3e0f249f33227209c1e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOFQxOToxMjo0MlrOFixLKg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOFQxOToxMjo0MlrOFixLKg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAwMTU3OA==", "bodyText": "The test cases are very similar to TestOzoneFileSystem, most of the difference are in setup.\nCan we refactor to have a base class that contains all the common test methods and leave only the setup in the subclass for different file system? (This is the approach taken in the contract test to avoid duplicate code)", "url": "https://github.com/apache/ozone/pull/415#discussion_r372001578", "createdAt": "2020-01-28T19:12:42Z", "author": {"login": "xiaoyuyao"}, "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/fs/ozone/TestRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,477 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.ozone;\n+\n+import org.apache.commons.io.IOUtils;\n+import org.apache.commons.lang3.RandomStringUtils;\n+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.contract.ContractTestUtils;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.ozone.MiniOzoneCluster;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+import org.apache.hadoop.ozone.TestDataUtil;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClientException;\n+import org.apache.hadoop.ozone.client.OzoneKeyDetails;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Set;\n+import java.util.TreeSet;\n+\n+import static org.apache.hadoop.ozone.om.OMConfigKeys.OZONE_OM_ADDRESS_KEY;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+/**\n+ * Ozone file system tests that are not covered by contract tests.\n+ */\n+public class TestRootedOzoneFileSystem {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60b4bf8e6008cbd53d95c3e0f249f33227209c1e"}, "originalPosition": 59}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ5NjQ5NjEw", "url": "https://github.com/apache/ozone/pull/415#pullrequestreview-349649610", "createdAt": "2020-01-28T19:40:12Z", "commit": {"oid": "60b4bf8e6008cbd53d95c3e0f249f33227209c1e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOFQxOTo0MDoxMlrOFiyBpw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOFQxOTo0MDoxMlrOFiyBpw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAxNTUyNw==", "bodyText": "should we do getVolumeDetails/create first and if that fails, there is no need to create bucket.\nAs createVolume is an admin only operation, I would expect most of the client driven operation will fail here.", "url": "https://github.com/apache/ozone/pull/415#discussion_r372015527", "createdAt": "2020-01-28T19:40:12Z", "author": {"login": "xiaoyuyao"}, "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneClientAdapterImpl.java", "diffHunk": "@@ -0,0 +1,694 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import org.apache.commons.collections.CollectionUtils;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.crypto.key.KeyProvider;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdds.client.ReplicationFactor;\n+import org.apache.hadoop.hdds.client.ReplicationType;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.security.x509.SecurityConfig;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.ozone.OmUtils;\n+import org.apache.hadoop.ozone.OzoneConfigKeys;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClient;\n+import org.apache.hadoop.ozone.client.OzoneClientFactory;\n+import org.apache.hadoop.ozone.client.OzoneKey;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.ozone.client.io.OzoneOutputStream;\n+import org.apache.hadoop.ozone.client.protocol.ClientProtocol;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfoGroup;\n+import org.apache.hadoop.ozone.om.helpers.OzoneFileStatus;\n+import org.apache.hadoop.ozone.security.OzoneTokenIdentifier;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.security.token.TokenRenewer;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .BUCKET_ALREADY_EXISTS;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .VOLUME_ALREADY_EXISTS;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .VOLUME_NOT_FOUND;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .BUCKET_NOT_FOUND;\n+\n+/**\n+ * Basic Implementation of the OzoneFileSystem calls.\n+ * <p>\n+ * This is the minimal version which doesn't include any statistics.\n+ * <p>\n+ * For full featured version use OzoneClientAdapterImpl.\n+ */\n+public class BasicRootedOzoneClientAdapterImpl\n+    implements RootedOzoneClientAdapter {\n+\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneClientAdapterImpl.class);\n+\n+  private OzoneClient ozoneClient;\n+  private ClientProtocol proxy;\n+  private ObjectStore objectStore;\n+  private ReplicationType replicationType;\n+  private ReplicationFactor replicationFactor;\n+  private boolean securityEnabled;\n+  private int configuredDnPort;\n+\n+  /**\n+   * Create new OzoneClientAdapter implementation.\n+   *\n+   * @throws IOException In case of a problem.\n+   */\n+  public BasicRootedOzoneClientAdapterImpl() throws IOException {\n+    this(createConf());\n+  }\n+\n+  private static OzoneConfiguration createConf() {\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+    try {\n+      return new OzoneConfiguration();\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(OzoneConfiguration conf)\n+      throws IOException {\n+    this(null, -1, conf);\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(String omHost, int omPort,\n+      Configuration hadoopConf) throws IOException {\n+\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+\n+    try {\n+      OzoneConfiguration conf = OzoneConfiguration.of(hadoopConf);\n+\n+      if (omHost == null && OmUtils.isServiceIdsDefined(conf)) {\n+        // When the host name or service id isn't given\n+        // but ozone.om.service.ids is defined, declare failure.\n+\n+        // This is a safety precaution that prevents the client from\n+        // accidentally failing over to an unintended OM.\n+        throw new IllegalArgumentException(\"Service ID or host name must not\"\n+            + \" be omitted when ozone.om.service.ids is defined.\");\n+      }\n+\n+      if (omPort != -1) {\n+        // When the port number is specified, perform the following check\n+        if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+          // If omHost is a service id, it shouldn't use a port\n+          throw new IllegalArgumentException(\"Port \" + omPort +\n+              \" specified in URI but host '\" + omHost + \"' is a \"\n+              + \"logical (HA) OzoneManager and does not use port information.\");\n+        }\n+      } else {\n+        // When port number is not specified, read it from config\n+        omPort = OmUtils.getOmRpcPort(conf);\n+      }\n+\n+      SecurityConfig secConfig = new SecurityConfig(conf);\n+\n+      if (secConfig.isSecurityEnabled()) {\n+        this.securityEnabled = true;\n+      }\n+\n+      String replicationTypeConf =\n+          conf.get(OzoneConfigKeys.OZONE_REPLICATION_TYPE,\n+              OzoneConfigKeys.OZONE_REPLICATION_TYPE_DEFAULT);\n+\n+      int replicationCountConf = conf.getInt(OzoneConfigKeys.OZONE_REPLICATION,\n+          OzoneConfigKeys.OZONE_REPLICATION_DEFAULT);\n+\n+      if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+        // omHost is listed as one of the service ids in the config,\n+        // thus we should treat omHost as omServiceId\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, conf);\n+      } else if (StringUtils.isNotEmpty(omHost) && omPort != -1) {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, omPort, conf);\n+      } else {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(conf);\n+      }\n+      objectStore = ozoneClient.getObjectStore();\n+      proxy = objectStore.getClientProxy();\n+      this.replicationType = ReplicationType.valueOf(replicationTypeConf);\n+      this.replicationFactor = ReplicationFactor.valueOf(replicationCountConf);\n+      this.configuredDnPort = conf.getInt(\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT,\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT_DEFAULT);\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  OzoneBucket getBucket(OFSPath ofsPath, boolean createIfNotExist)\n+      throws IOException {\n+\n+    return getBucket(ofsPath.getVolumeName(), ofsPath.getBucketName(),\n+        createIfNotExist);\n+  }\n+\n+  /**\n+   * Get OzoneBucket object to operate in.\n+   * Optionally create volume and bucket if not found.\n+   *\n+   * @param createIfNotExist Set this to true if the caller is a write operation\n+   *                         in order to create the volume and bucket.\n+   * @throws IOException Exceptions other than OMException with result code\n+   *                     VOLUME_NOT_FOUND or BUCKET_NOT_FOUND.\n+   */\n+  private OzoneBucket getBucket(String volumeStr, String bucketStr,\n+      boolean createIfNotExist) throws IOException {\n+\n+    OzoneBucket bucket;\n+    try {\n+      bucket = proxy.getBucketDetails(volumeStr, bucketStr);\n+    } catch (OMException ex) {\n+      if (createIfNotExist) {\n+        // Note: getBucketDetails always throws BUCKET_NOT_FOUND, even if\n+        // the volume doesn't exist.\n+        if (ex.getResult().equals(BUCKET_NOT_FOUND)) {\n+          OzoneVolume volume;\n+          try {\n+            volume = proxy.getVolumeDetails(volumeStr);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60b4bf8e6008cbd53d95c3e0f249f33227209c1e"}, "originalPosition": 219}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ5NjUxNTk4", "url": "https://github.com/apache/ozone/pull/415#pullrequestreview-349651598", "createdAt": "2020-01-28T19:43:20Z", "commit": {"oid": "60b4bf8e6008cbd53d95c3e0f249f33227209c1e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOFQxOTo0MzoyMFrOFiyH5w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOFQxOTo0MzoyMFrOFiyH5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAxNzEyNw==", "bodyText": "Even the volume creation succeed, the bucket creation might fail if the ACLs are not set properly when security and acls are enabled. With o3fs, this is OK because the volume/bucket are provisioned and acls setup before it is mounted. With ofs, this may not work without ACL changes.", "url": "https://github.com/apache/ozone/pull/415#discussion_r372017127", "createdAt": "2020-01-28T19:43:20Z", "author": {"login": "xiaoyuyao"}, "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneClientAdapterImpl.java", "diffHunk": "@@ -0,0 +1,694 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import org.apache.commons.collections.CollectionUtils;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.crypto.key.KeyProvider;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdds.client.ReplicationFactor;\n+import org.apache.hadoop.hdds.client.ReplicationType;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.security.x509.SecurityConfig;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.ozone.OmUtils;\n+import org.apache.hadoop.ozone.OzoneConfigKeys;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClient;\n+import org.apache.hadoop.ozone.client.OzoneClientFactory;\n+import org.apache.hadoop.ozone.client.OzoneKey;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.ozone.client.io.OzoneOutputStream;\n+import org.apache.hadoop.ozone.client.protocol.ClientProtocol;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfoGroup;\n+import org.apache.hadoop.ozone.om.helpers.OzoneFileStatus;\n+import org.apache.hadoop.ozone.security.OzoneTokenIdentifier;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.security.token.TokenRenewer;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .BUCKET_ALREADY_EXISTS;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .VOLUME_ALREADY_EXISTS;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .VOLUME_NOT_FOUND;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .BUCKET_NOT_FOUND;\n+\n+/**\n+ * Basic Implementation of the OzoneFileSystem calls.\n+ * <p>\n+ * This is the minimal version which doesn't include any statistics.\n+ * <p>\n+ * For full featured version use OzoneClientAdapterImpl.\n+ */\n+public class BasicRootedOzoneClientAdapterImpl\n+    implements RootedOzoneClientAdapter {\n+\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneClientAdapterImpl.class);\n+\n+  private OzoneClient ozoneClient;\n+  private ClientProtocol proxy;\n+  private ObjectStore objectStore;\n+  private ReplicationType replicationType;\n+  private ReplicationFactor replicationFactor;\n+  private boolean securityEnabled;\n+  private int configuredDnPort;\n+\n+  /**\n+   * Create new OzoneClientAdapter implementation.\n+   *\n+   * @throws IOException In case of a problem.\n+   */\n+  public BasicRootedOzoneClientAdapterImpl() throws IOException {\n+    this(createConf());\n+  }\n+\n+  private static OzoneConfiguration createConf() {\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+    try {\n+      return new OzoneConfiguration();\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(OzoneConfiguration conf)\n+      throws IOException {\n+    this(null, -1, conf);\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(String omHost, int omPort,\n+      Configuration hadoopConf) throws IOException {\n+\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+\n+    try {\n+      OzoneConfiguration conf = OzoneConfiguration.of(hadoopConf);\n+\n+      if (omHost == null && OmUtils.isServiceIdsDefined(conf)) {\n+        // When the host name or service id isn't given\n+        // but ozone.om.service.ids is defined, declare failure.\n+\n+        // This is a safety precaution that prevents the client from\n+        // accidentally failing over to an unintended OM.\n+        throw new IllegalArgumentException(\"Service ID or host name must not\"\n+            + \" be omitted when ozone.om.service.ids is defined.\");\n+      }\n+\n+      if (omPort != -1) {\n+        // When the port number is specified, perform the following check\n+        if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+          // If omHost is a service id, it shouldn't use a port\n+          throw new IllegalArgumentException(\"Port \" + omPort +\n+              \" specified in URI but host '\" + omHost + \"' is a \"\n+              + \"logical (HA) OzoneManager and does not use port information.\");\n+        }\n+      } else {\n+        // When port number is not specified, read it from config\n+        omPort = OmUtils.getOmRpcPort(conf);\n+      }\n+\n+      SecurityConfig secConfig = new SecurityConfig(conf);\n+\n+      if (secConfig.isSecurityEnabled()) {\n+        this.securityEnabled = true;\n+      }\n+\n+      String replicationTypeConf =\n+          conf.get(OzoneConfigKeys.OZONE_REPLICATION_TYPE,\n+              OzoneConfigKeys.OZONE_REPLICATION_TYPE_DEFAULT);\n+\n+      int replicationCountConf = conf.getInt(OzoneConfigKeys.OZONE_REPLICATION,\n+          OzoneConfigKeys.OZONE_REPLICATION_DEFAULT);\n+\n+      if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+        // omHost is listed as one of the service ids in the config,\n+        // thus we should treat omHost as omServiceId\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, conf);\n+      } else if (StringUtils.isNotEmpty(omHost) && omPort != -1) {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, omPort, conf);\n+      } else {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(conf);\n+      }\n+      objectStore = ozoneClient.getObjectStore();\n+      proxy = objectStore.getClientProxy();\n+      this.replicationType = ReplicationType.valueOf(replicationTypeConf);\n+      this.replicationFactor = ReplicationFactor.valueOf(replicationCountConf);\n+      this.configuredDnPort = conf.getInt(\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT,\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT_DEFAULT);\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  OzoneBucket getBucket(OFSPath ofsPath, boolean createIfNotExist)\n+      throws IOException {\n+\n+    return getBucket(ofsPath.getVolumeName(), ofsPath.getBucketName(),\n+        createIfNotExist);\n+  }\n+\n+  /**\n+   * Get OzoneBucket object to operate in.\n+   * Optionally create volume and bucket if not found.\n+   *\n+   * @param createIfNotExist Set this to true if the caller is a write operation\n+   *                         in order to create the volume and bucket.\n+   * @throws IOException Exceptions other than OMException with result code\n+   *                     VOLUME_NOT_FOUND or BUCKET_NOT_FOUND.\n+   */\n+  private OzoneBucket getBucket(String volumeStr, String bucketStr,\n+      boolean createIfNotExist) throws IOException {\n+\n+    OzoneBucket bucket;\n+    try {\n+      bucket = proxy.getBucketDetails(volumeStr, bucketStr);\n+    } catch (OMException ex) {\n+      if (createIfNotExist) {\n+        // Note: getBucketDetails always throws BUCKET_NOT_FOUND, even if\n+        // the volume doesn't exist.\n+        if (ex.getResult().equals(BUCKET_NOT_FOUND)) {\n+          OzoneVolume volume;\n+          try {\n+            volume = proxy.getVolumeDetails(volumeStr);\n+          } catch (OMException getVolEx) {\n+            if (getVolEx.getResult().equals(VOLUME_NOT_FOUND)) {\n+              // Volume doesn't exist. Create it\n+              try {\n+                objectStore.createVolume(volumeStr);\n+              } catch (OMException newVolEx) {\n+                // Ignore the case where another client created the volume\n+                if (!newVolEx.getResult().equals(VOLUME_ALREADY_EXISTS)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60b4bf8e6008cbd53d95c3e0f249f33227209c1e"}, "originalPosition": 227}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ5NjUyMzI4", "url": "https://github.com/apache/ozone/pull/415#pullrequestreview-349652328", "createdAt": "2020-01-28T19:44:29Z", "commit": {"oid": "60b4bf8e6008cbd53d95c3e0f249f33227209c1e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOFQxOTo0NDoyOVrOFiyKOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOFQxOTo0NDoyOVrOFiyKOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAxNzcyMQ==", "bodyText": "what is permission denied, do we recover the environment by delete the volume?", "url": "https://github.com/apache/ozone/pull/415#discussion_r372017721", "createdAt": "2020-01-28T19:44:29Z", "author": {"login": "xiaoyuyao"}, "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneClientAdapterImpl.java", "diffHunk": "@@ -0,0 +1,694 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import org.apache.commons.collections.CollectionUtils;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.crypto.key.KeyProvider;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdds.client.ReplicationFactor;\n+import org.apache.hadoop.hdds.client.ReplicationType;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.security.x509.SecurityConfig;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.ozone.OmUtils;\n+import org.apache.hadoop.ozone.OzoneConfigKeys;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClient;\n+import org.apache.hadoop.ozone.client.OzoneClientFactory;\n+import org.apache.hadoop.ozone.client.OzoneKey;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.ozone.client.io.OzoneOutputStream;\n+import org.apache.hadoop.ozone.client.protocol.ClientProtocol;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfoGroup;\n+import org.apache.hadoop.ozone.om.helpers.OzoneFileStatus;\n+import org.apache.hadoop.ozone.security.OzoneTokenIdentifier;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.security.token.TokenRenewer;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .BUCKET_ALREADY_EXISTS;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .VOLUME_ALREADY_EXISTS;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .VOLUME_NOT_FOUND;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .BUCKET_NOT_FOUND;\n+\n+/**\n+ * Basic Implementation of the OzoneFileSystem calls.\n+ * <p>\n+ * This is the minimal version which doesn't include any statistics.\n+ * <p>\n+ * For full featured version use OzoneClientAdapterImpl.\n+ */\n+public class BasicRootedOzoneClientAdapterImpl\n+    implements RootedOzoneClientAdapter {\n+\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneClientAdapterImpl.class);\n+\n+  private OzoneClient ozoneClient;\n+  private ClientProtocol proxy;\n+  private ObjectStore objectStore;\n+  private ReplicationType replicationType;\n+  private ReplicationFactor replicationFactor;\n+  private boolean securityEnabled;\n+  private int configuredDnPort;\n+\n+  /**\n+   * Create new OzoneClientAdapter implementation.\n+   *\n+   * @throws IOException In case of a problem.\n+   */\n+  public BasicRootedOzoneClientAdapterImpl() throws IOException {\n+    this(createConf());\n+  }\n+\n+  private static OzoneConfiguration createConf() {\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+    try {\n+      return new OzoneConfiguration();\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(OzoneConfiguration conf)\n+      throws IOException {\n+    this(null, -1, conf);\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(String omHost, int omPort,\n+      Configuration hadoopConf) throws IOException {\n+\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+\n+    try {\n+      OzoneConfiguration conf = OzoneConfiguration.of(hadoopConf);\n+\n+      if (omHost == null && OmUtils.isServiceIdsDefined(conf)) {\n+        // When the host name or service id isn't given\n+        // but ozone.om.service.ids is defined, declare failure.\n+\n+        // This is a safety precaution that prevents the client from\n+        // accidentally failing over to an unintended OM.\n+        throw new IllegalArgumentException(\"Service ID or host name must not\"\n+            + \" be omitted when ozone.om.service.ids is defined.\");\n+      }\n+\n+      if (omPort != -1) {\n+        // When the port number is specified, perform the following check\n+        if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+          // If omHost is a service id, it shouldn't use a port\n+          throw new IllegalArgumentException(\"Port \" + omPort +\n+              \" specified in URI but host '\" + omHost + \"' is a \"\n+              + \"logical (HA) OzoneManager and does not use port information.\");\n+        }\n+      } else {\n+        // When port number is not specified, read it from config\n+        omPort = OmUtils.getOmRpcPort(conf);\n+      }\n+\n+      SecurityConfig secConfig = new SecurityConfig(conf);\n+\n+      if (secConfig.isSecurityEnabled()) {\n+        this.securityEnabled = true;\n+      }\n+\n+      String replicationTypeConf =\n+          conf.get(OzoneConfigKeys.OZONE_REPLICATION_TYPE,\n+              OzoneConfigKeys.OZONE_REPLICATION_TYPE_DEFAULT);\n+\n+      int replicationCountConf = conf.getInt(OzoneConfigKeys.OZONE_REPLICATION,\n+          OzoneConfigKeys.OZONE_REPLICATION_DEFAULT);\n+\n+      if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+        // omHost is listed as one of the service ids in the config,\n+        // thus we should treat omHost as omServiceId\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, conf);\n+      } else if (StringUtils.isNotEmpty(omHost) && omPort != -1) {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, omPort, conf);\n+      } else {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(conf);\n+      }\n+      objectStore = ozoneClient.getObjectStore();\n+      proxy = objectStore.getClientProxy();\n+      this.replicationType = ReplicationType.valueOf(replicationTypeConf);\n+      this.replicationFactor = ReplicationFactor.valueOf(replicationCountConf);\n+      this.configuredDnPort = conf.getInt(\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT,\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT_DEFAULT);\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  OzoneBucket getBucket(OFSPath ofsPath, boolean createIfNotExist)\n+      throws IOException {\n+\n+    return getBucket(ofsPath.getVolumeName(), ofsPath.getBucketName(),\n+        createIfNotExist);\n+  }\n+\n+  /**\n+   * Get OzoneBucket object to operate in.\n+   * Optionally create volume and bucket if not found.\n+   *\n+   * @param createIfNotExist Set this to true if the caller is a write operation\n+   *                         in order to create the volume and bucket.\n+   * @throws IOException Exceptions other than OMException with result code\n+   *                     VOLUME_NOT_FOUND or BUCKET_NOT_FOUND.\n+   */\n+  private OzoneBucket getBucket(String volumeStr, String bucketStr,\n+      boolean createIfNotExist) throws IOException {\n+\n+    OzoneBucket bucket;\n+    try {\n+      bucket = proxy.getBucketDetails(volumeStr, bucketStr);\n+    } catch (OMException ex) {\n+      if (createIfNotExist) {\n+        // Note: getBucketDetails always throws BUCKET_NOT_FOUND, even if\n+        // the volume doesn't exist.\n+        if (ex.getResult().equals(BUCKET_NOT_FOUND)) {\n+          OzoneVolume volume;\n+          try {\n+            volume = proxy.getVolumeDetails(volumeStr);\n+          } catch (OMException getVolEx) {\n+            if (getVolEx.getResult().equals(VOLUME_NOT_FOUND)) {\n+              // Volume doesn't exist. Create it\n+              try {\n+                objectStore.createVolume(volumeStr);\n+              } catch (OMException newVolEx) {\n+                // Ignore the case where another client created the volume\n+                if (!newVolEx.getResult().equals(VOLUME_ALREADY_EXISTS)) {\n+                  throw newVolEx;\n+                }\n+              }\n+            } else {\n+              throw getVolEx;\n+            }\n+            // Try get volume again\n+            volume = proxy.getVolumeDetails(volumeStr);\n+          }\n+          // Create the bucket\n+          try {\n+            volume.createBucket(bucketStr);\n+          } catch (OMException newBucEx) {\n+            // Ignore the case where another client created the bucket\n+            if (!newBucEx.getResult().equals(BUCKET_ALREADY_EXISTS)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60b4bf8e6008cbd53d95c3e0f249f33227209c1e"}, "originalPosition": 242}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ5NjY0MDAz", "url": "https://github.com/apache/ozone/pull/415#pullrequestreview-349664003", "createdAt": "2020-01-28T20:02:39Z", "commit": {"oid": "60b4bf8e6008cbd53d95c3e0f249f33227209c1e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOFQyMDowMjozOVrOFiytyw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOFQyMDowMjozOVrOFiytyw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAyNjgyNw==", "bodyText": "In the /tmp case, the bucketName length is also 0? do we want to create a /tmp volume here?", "url": "https://github.com/apache/ozone/pull/415#discussion_r372026827", "createdAt": "2020-01-28T20:02:39Z", "author": {"login": "xiaoyuyao"}, "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneClientAdapterImpl.java", "diffHunk": "@@ -0,0 +1,694 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import org.apache.commons.collections.CollectionUtils;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.crypto.key.KeyProvider;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdds.client.ReplicationFactor;\n+import org.apache.hadoop.hdds.client.ReplicationType;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.security.x509.SecurityConfig;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.ozone.OmUtils;\n+import org.apache.hadoop.ozone.OzoneConfigKeys;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClient;\n+import org.apache.hadoop.ozone.client.OzoneClientFactory;\n+import org.apache.hadoop.ozone.client.OzoneKey;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.ozone.client.io.OzoneOutputStream;\n+import org.apache.hadoop.ozone.client.protocol.ClientProtocol;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfoGroup;\n+import org.apache.hadoop.ozone.om.helpers.OzoneFileStatus;\n+import org.apache.hadoop.ozone.security.OzoneTokenIdentifier;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.security.token.TokenRenewer;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .BUCKET_ALREADY_EXISTS;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .VOLUME_ALREADY_EXISTS;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .VOLUME_NOT_FOUND;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .BUCKET_NOT_FOUND;\n+\n+/**\n+ * Basic Implementation of the OzoneFileSystem calls.\n+ * <p>\n+ * This is the minimal version which doesn't include any statistics.\n+ * <p>\n+ * For full featured version use OzoneClientAdapterImpl.\n+ */\n+public class BasicRootedOzoneClientAdapterImpl\n+    implements RootedOzoneClientAdapter {\n+\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneClientAdapterImpl.class);\n+\n+  private OzoneClient ozoneClient;\n+  private ClientProtocol proxy;\n+  private ObjectStore objectStore;\n+  private ReplicationType replicationType;\n+  private ReplicationFactor replicationFactor;\n+  private boolean securityEnabled;\n+  private int configuredDnPort;\n+\n+  /**\n+   * Create new OzoneClientAdapter implementation.\n+   *\n+   * @throws IOException In case of a problem.\n+   */\n+  public BasicRootedOzoneClientAdapterImpl() throws IOException {\n+    this(createConf());\n+  }\n+\n+  private static OzoneConfiguration createConf() {\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+    try {\n+      return new OzoneConfiguration();\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(OzoneConfiguration conf)\n+      throws IOException {\n+    this(null, -1, conf);\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(String omHost, int omPort,\n+      Configuration hadoopConf) throws IOException {\n+\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+\n+    try {\n+      OzoneConfiguration conf = OzoneConfiguration.of(hadoopConf);\n+\n+      if (omHost == null && OmUtils.isServiceIdsDefined(conf)) {\n+        // When the host name or service id isn't given\n+        // but ozone.om.service.ids is defined, declare failure.\n+\n+        // This is a safety precaution that prevents the client from\n+        // accidentally failing over to an unintended OM.\n+        throw new IllegalArgumentException(\"Service ID or host name must not\"\n+            + \" be omitted when ozone.om.service.ids is defined.\");\n+      }\n+\n+      if (omPort != -1) {\n+        // When the port number is specified, perform the following check\n+        if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+          // If omHost is a service id, it shouldn't use a port\n+          throw new IllegalArgumentException(\"Port \" + omPort +\n+              \" specified in URI but host '\" + omHost + \"' is a \"\n+              + \"logical (HA) OzoneManager and does not use port information.\");\n+        }\n+      } else {\n+        // When port number is not specified, read it from config\n+        omPort = OmUtils.getOmRpcPort(conf);\n+      }\n+\n+      SecurityConfig secConfig = new SecurityConfig(conf);\n+\n+      if (secConfig.isSecurityEnabled()) {\n+        this.securityEnabled = true;\n+      }\n+\n+      String replicationTypeConf =\n+          conf.get(OzoneConfigKeys.OZONE_REPLICATION_TYPE,\n+              OzoneConfigKeys.OZONE_REPLICATION_TYPE_DEFAULT);\n+\n+      int replicationCountConf = conf.getInt(OzoneConfigKeys.OZONE_REPLICATION,\n+          OzoneConfigKeys.OZONE_REPLICATION_DEFAULT);\n+\n+      if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+        // omHost is listed as one of the service ids in the config,\n+        // thus we should treat omHost as omServiceId\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, conf);\n+      } else if (StringUtils.isNotEmpty(omHost) && omPort != -1) {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, omPort, conf);\n+      } else {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(conf);\n+      }\n+      objectStore = ozoneClient.getObjectStore();\n+      proxy = objectStore.getClientProxy();\n+      this.replicationType = ReplicationType.valueOf(replicationTypeConf);\n+      this.replicationFactor = ReplicationFactor.valueOf(replicationCountConf);\n+      this.configuredDnPort = conf.getInt(\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT,\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT_DEFAULT);\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  OzoneBucket getBucket(OFSPath ofsPath, boolean createIfNotExist)\n+      throws IOException {\n+\n+    return getBucket(ofsPath.getVolumeName(), ofsPath.getBucketName(),\n+        createIfNotExist);\n+  }\n+\n+  /**\n+   * Get OzoneBucket object to operate in.\n+   * Optionally create volume and bucket if not found.\n+   *\n+   * @param createIfNotExist Set this to true if the caller is a write operation\n+   *                         in order to create the volume and bucket.\n+   * @throws IOException Exceptions other than OMException with result code\n+   *                     VOLUME_NOT_FOUND or BUCKET_NOT_FOUND.\n+   */\n+  private OzoneBucket getBucket(String volumeStr, String bucketStr,\n+      boolean createIfNotExist) throws IOException {\n+\n+    OzoneBucket bucket;\n+    try {\n+      bucket = proxy.getBucketDetails(volumeStr, bucketStr);\n+    } catch (OMException ex) {\n+      if (createIfNotExist) {\n+        // Note: getBucketDetails always throws BUCKET_NOT_FOUND, even if\n+        // the volume doesn't exist.\n+        if (ex.getResult().equals(BUCKET_NOT_FOUND)) {\n+          OzoneVolume volume;\n+          try {\n+            volume = proxy.getVolumeDetails(volumeStr);\n+          } catch (OMException getVolEx) {\n+            if (getVolEx.getResult().equals(VOLUME_NOT_FOUND)) {\n+              // Volume doesn't exist. Create it\n+              try {\n+                objectStore.createVolume(volumeStr);\n+              } catch (OMException newVolEx) {\n+                // Ignore the case where another client created the volume\n+                if (!newVolEx.getResult().equals(VOLUME_ALREADY_EXISTS)) {\n+                  throw newVolEx;\n+                }\n+              }\n+            } else {\n+              throw getVolEx;\n+            }\n+            // Try get volume again\n+            volume = proxy.getVolumeDetails(volumeStr);\n+          }\n+          // Create the bucket\n+          try {\n+            volume.createBucket(bucketStr);\n+          } catch (OMException newBucEx) {\n+            // Ignore the case where another client created the bucket\n+            if (!newBucEx.getResult().equals(BUCKET_ALREADY_EXISTS)) {\n+              throw newBucEx;\n+            }\n+          }\n+        }\n+        // Try get bucket again\n+        bucket = proxy.getBucketDetails(volumeStr, bucketStr);\n+      } else {\n+        throw ex;\n+      }\n+    }\n+\n+    return bucket;\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    ozoneClient.close();\n+  }\n+\n+  @Override\n+  public InputStream readFile(String pathStr) throws IOException {\n+    incrementCounter(Statistic.OBJECTS_READ);\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    String key = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, false);\n+      return bucket.readFile(key).getInputStream();\n+    } catch (OMException ex) {\n+      if (ex.getResult() == OMException.ResultCodes.FILE_NOT_FOUND\n+          || ex.getResult() == OMException.ResultCodes.NOT_A_FILE) {\n+        throw new FileNotFoundException(\n+            ex.getResult().name() + \": \" + ex.getMessage());\n+      } else {\n+        throw ex;\n+      }\n+    }\n+  }\n+\n+  protected void incrementCounter(Statistic objectsRead) {\n+    //noop: Use OzoneClientAdapterImpl which supports statistics.\n+  }\n+\n+  @Override\n+  public OzoneFSOutputStream createFile(String pathStr, boolean overWrite,\n+      boolean recursive) throws IOException {\n+    incrementCounter(Statistic.OBJECTS_CREATED);\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    String key = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, false);\n+      OzoneOutputStream ozoneOutputStream = bucket.createFile(\n+          key, 0, replicationType, replicationFactor, overWrite, recursive);\n+      return new OzoneFSOutputStream(ozoneOutputStream.getOutputStream());\n+    } catch (OMException ex) {\n+      if (ex.getResult() == OMException.ResultCodes.FILE_ALREADY_EXISTS\n+          || ex.getResult() == OMException.ResultCodes.NOT_A_FILE) {\n+        throw new FileAlreadyExistsException(\n+            ex.getResult().name() + \": \" + ex.getMessage());\n+      } else {\n+        throw ex;\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void renamePath(String path, String newPath) throws IOException {\n+    incrementCounter(Statistic.OBJECTS_RENAMED);\n+    OFSPath ofsPath = new OFSPath(path);\n+    OFSPath ofsNewPath = new OFSPath(newPath);\n+\n+    // Check path and newPathName are in the same volume and same bucket.\n+    // This should have been checked in BasicRootedOzoneFileSystem#rename\n+    // already via regular call path unless bypassed.\n+    if (!ofsPath.isInSameBucketAs(ofsNewPath)) {\n+      throw new IOException(\"Cannot rename a key to a different bucket\");\n+    }\n+\n+    OzoneBucket bucket = getBucket(ofsPath, false);\n+    String key = ofsPath.getKeyName();\n+    String newKey = ofsNewPath.getKeyName();\n+    bucket.renameKey(key, newKey);\n+  }\n+\n+  /**\n+   * Package-private helper function to reduce calls to getBucket().\n+   * @param bucket Bucket to operate in.\n+   * @param path Existing key path.\n+   * @param newPath New key path.\n+   * @throws IOException IOException from bucket.renameKey().\n+   */\n+  void renamePath(OzoneBucket bucket, String path, String newPath)\n+      throws IOException {\n+    incrementCounter(Statistic.OBJECTS_RENAMED);\n+    OFSPath ofsPath = new OFSPath(path);\n+    OFSPath ofsNewPath = new OFSPath(newPath);\n+    // No same-bucket policy check here since this call path is controlled\n+    String key = ofsPath.getKeyName();\n+    String newKey = ofsNewPath.getKeyName();\n+    bucket.renameKey(key, newKey);\n+  }\n+\n+  /**\n+   * Helper method to create an directory specified by key name in bucket.\n+   *\n+   * @param pathStr path to be created as directory\n+   * @return true if the key is created, false otherwise\n+   */\n+  @Override\n+  public boolean createDirectory(String pathStr) throws IOException {\n+    LOG.trace(\"creating dir for path: {}\", pathStr);\n+    incrementCounter(Statistic.OBJECTS_CREATED);\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    // Volume name unspecified, return failure\n+    if (ofsPath.getVolumeName().length() == 0) {\n+      return false;\n+    }\n+    // Handle where only volume is specified in pathStr\n+    if (ofsPath.getBucketName().length() == 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60b4bf8e6008cbd53d95c3e0f249f33227209c1e"}, "originalPosition": 360}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ5NjY0MzMy", "url": "https://github.com/apache/ozone/pull/415#pullrequestreview-349664332", "createdAt": "2020-01-28T20:03:10Z", "commit": {"oid": "60b4bf8e6008cbd53d95c3e0f249f33227209c1e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOFQyMDowMzoxMFrOFiyuxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOFQyMDowMzoxMFrOFiyuxw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAyNzA3OQ==", "bodyText": "what if keyStr is empty(length 0) should we check that before getBucket?", "url": "https://github.com/apache/ozone/pull/415#discussion_r372027079", "createdAt": "2020-01-28T20:03:10Z", "author": {"login": "xiaoyuyao"}, "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneClientAdapterImpl.java", "diffHunk": "@@ -0,0 +1,694 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import org.apache.commons.collections.CollectionUtils;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.crypto.key.KeyProvider;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdds.client.ReplicationFactor;\n+import org.apache.hadoop.hdds.client.ReplicationType;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.security.x509.SecurityConfig;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.ozone.OmUtils;\n+import org.apache.hadoop.ozone.OzoneConfigKeys;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClient;\n+import org.apache.hadoop.ozone.client.OzoneClientFactory;\n+import org.apache.hadoop.ozone.client.OzoneKey;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.ozone.client.io.OzoneOutputStream;\n+import org.apache.hadoop.ozone.client.protocol.ClientProtocol;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfoGroup;\n+import org.apache.hadoop.ozone.om.helpers.OzoneFileStatus;\n+import org.apache.hadoop.ozone.security.OzoneTokenIdentifier;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.security.token.TokenRenewer;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .BUCKET_ALREADY_EXISTS;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .VOLUME_ALREADY_EXISTS;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .VOLUME_NOT_FOUND;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .BUCKET_NOT_FOUND;\n+\n+/**\n+ * Basic Implementation of the OzoneFileSystem calls.\n+ * <p>\n+ * This is the minimal version which doesn't include any statistics.\n+ * <p>\n+ * For full featured version use OzoneClientAdapterImpl.\n+ */\n+public class BasicRootedOzoneClientAdapterImpl\n+    implements RootedOzoneClientAdapter {\n+\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneClientAdapterImpl.class);\n+\n+  private OzoneClient ozoneClient;\n+  private ClientProtocol proxy;\n+  private ObjectStore objectStore;\n+  private ReplicationType replicationType;\n+  private ReplicationFactor replicationFactor;\n+  private boolean securityEnabled;\n+  private int configuredDnPort;\n+\n+  /**\n+   * Create new OzoneClientAdapter implementation.\n+   *\n+   * @throws IOException In case of a problem.\n+   */\n+  public BasicRootedOzoneClientAdapterImpl() throws IOException {\n+    this(createConf());\n+  }\n+\n+  private static OzoneConfiguration createConf() {\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+    try {\n+      return new OzoneConfiguration();\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(OzoneConfiguration conf)\n+      throws IOException {\n+    this(null, -1, conf);\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(String omHost, int omPort,\n+      Configuration hadoopConf) throws IOException {\n+\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+\n+    try {\n+      OzoneConfiguration conf = OzoneConfiguration.of(hadoopConf);\n+\n+      if (omHost == null && OmUtils.isServiceIdsDefined(conf)) {\n+        // When the host name or service id isn't given\n+        // but ozone.om.service.ids is defined, declare failure.\n+\n+        // This is a safety precaution that prevents the client from\n+        // accidentally failing over to an unintended OM.\n+        throw new IllegalArgumentException(\"Service ID or host name must not\"\n+            + \" be omitted when ozone.om.service.ids is defined.\");\n+      }\n+\n+      if (omPort != -1) {\n+        // When the port number is specified, perform the following check\n+        if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+          // If omHost is a service id, it shouldn't use a port\n+          throw new IllegalArgumentException(\"Port \" + omPort +\n+              \" specified in URI but host '\" + omHost + \"' is a \"\n+              + \"logical (HA) OzoneManager and does not use port information.\");\n+        }\n+      } else {\n+        // When port number is not specified, read it from config\n+        omPort = OmUtils.getOmRpcPort(conf);\n+      }\n+\n+      SecurityConfig secConfig = new SecurityConfig(conf);\n+\n+      if (secConfig.isSecurityEnabled()) {\n+        this.securityEnabled = true;\n+      }\n+\n+      String replicationTypeConf =\n+          conf.get(OzoneConfigKeys.OZONE_REPLICATION_TYPE,\n+              OzoneConfigKeys.OZONE_REPLICATION_TYPE_DEFAULT);\n+\n+      int replicationCountConf = conf.getInt(OzoneConfigKeys.OZONE_REPLICATION,\n+          OzoneConfigKeys.OZONE_REPLICATION_DEFAULT);\n+\n+      if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+        // omHost is listed as one of the service ids in the config,\n+        // thus we should treat omHost as omServiceId\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, conf);\n+      } else if (StringUtils.isNotEmpty(omHost) && omPort != -1) {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, omPort, conf);\n+      } else {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(conf);\n+      }\n+      objectStore = ozoneClient.getObjectStore();\n+      proxy = objectStore.getClientProxy();\n+      this.replicationType = ReplicationType.valueOf(replicationTypeConf);\n+      this.replicationFactor = ReplicationFactor.valueOf(replicationCountConf);\n+      this.configuredDnPort = conf.getInt(\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT,\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT_DEFAULT);\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  OzoneBucket getBucket(OFSPath ofsPath, boolean createIfNotExist)\n+      throws IOException {\n+\n+    return getBucket(ofsPath.getVolumeName(), ofsPath.getBucketName(),\n+        createIfNotExist);\n+  }\n+\n+  /**\n+   * Get OzoneBucket object to operate in.\n+   * Optionally create volume and bucket if not found.\n+   *\n+   * @param createIfNotExist Set this to true if the caller is a write operation\n+   *                         in order to create the volume and bucket.\n+   * @throws IOException Exceptions other than OMException with result code\n+   *                     VOLUME_NOT_FOUND or BUCKET_NOT_FOUND.\n+   */\n+  private OzoneBucket getBucket(String volumeStr, String bucketStr,\n+      boolean createIfNotExist) throws IOException {\n+\n+    OzoneBucket bucket;\n+    try {\n+      bucket = proxy.getBucketDetails(volumeStr, bucketStr);\n+    } catch (OMException ex) {\n+      if (createIfNotExist) {\n+        // Note: getBucketDetails always throws BUCKET_NOT_FOUND, even if\n+        // the volume doesn't exist.\n+        if (ex.getResult().equals(BUCKET_NOT_FOUND)) {\n+          OzoneVolume volume;\n+          try {\n+            volume = proxy.getVolumeDetails(volumeStr);\n+          } catch (OMException getVolEx) {\n+            if (getVolEx.getResult().equals(VOLUME_NOT_FOUND)) {\n+              // Volume doesn't exist. Create it\n+              try {\n+                objectStore.createVolume(volumeStr);\n+              } catch (OMException newVolEx) {\n+                // Ignore the case where another client created the volume\n+                if (!newVolEx.getResult().equals(VOLUME_ALREADY_EXISTS)) {\n+                  throw newVolEx;\n+                }\n+              }\n+            } else {\n+              throw getVolEx;\n+            }\n+            // Try get volume again\n+            volume = proxy.getVolumeDetails(volumeStr);\n+          }\n+          // Create the bucket\n+          try {\n+            volume.createBucket(bucketStr);\n+          } catch (OMException newBucEx) {\n+            // Ignore the case where another client created the bucket\n+            if (!newBucEx.getResult().equals(BUCKET_ALREADY_EXISTS)) {\n+              throw newBucEx;\n+            }\n+          }\n+        }\n+        // Try get bucket again\n+        bucket = proxy.getBucketDetails(volumeStr, bucketStr);\n+      } else {\n+        throw ex;\n+      }\n+    }\n+\n+    return bucket;\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    ozoneClient.close();\n+  }\n+\n+  @Override\n+  public InputStream readFile(String pathStr) throws IOException {\n+    incrementCounter(Statistic.OBJECTS_READ);\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    String key = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, false);\n+      return bucket.readFile(key).getInputStream();\n+    } catch (OMException ex) {\n+      if (ex.getResult() == OMException.ResultCodes.FILE_NOT_FOUND\n+          || ex.getResult() == OMException.ResultCodes.NOT_A_FILE) {\n+        throw new FileNotFoundException(\n+            ex.getResult().name() + \": \" + ex.getMessage());\n+      } else {\n+        throw ex;\n+      }\n+    }\n+  }\n+\n+  protected void incrementCounter(Statistic objectsRead) {\n+    //noop: Use OzoneClientAdapterImpl which supports statistics.\n+  }\n+\n+  @Override\n+  public OzoneFSOutputStream createFile(String pathStr, boolean overWrite,\n+      boolean recursive) throws IOException {\n+    incrementCounter(Statistic.OBJECTS_CREATED);\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    String key = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, false);\n+      OzoneOutputStream ozoneOutputStream = bucket.createFile(\n+          key, 0, replicationType, replicationFactor, overWrite, recursive);\n+      return new OzoneFSOutputStream(ozoneOutputStream.getOutputStream());\n+    } catch (OMException ex) {\n+      if (ex.getResult() == OMException.ResultCodes.FILE_ALREADY_EXISTS\n+          || ex.getResult() == OMException.ResultCodes.NOT_A_FILE) {\n+        throw new FileAlreadyExistsException(\n+            ex.getResult().name() + \": \" + ex.getMessage());\n+      } else {\n+        throw ex;\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void renamePath(String path, String newPath) throws IOException {\n+    incrementCounter(Statistic.OBJECTS_RENAMED);\n+    OFSPath ofsPath = new OFSPath(path);\n+    OFSPath ofsNewPath = new OFSPath(newPath);\n+\n+    // Check path and newPathName are in the same volume and same bucket.\n+    // This should have been checked in BasicRootedOzoneFileSystem#rename\n+    // already via regular call path unless bypassed.\n+    if (!ofsPath.isInSameBucketAs(ofsNewPath)) {\n+      throw new IOException(\"Cannot rename a key to a different bucket\");\n+    }\n+\n+    OzoneBucket bucket = getBucket(ofsPath, false);\n+    String key = ofsPath.getKeyName();\n+    String newKey = ofsNewPath.getKeyName();\n+    bucket.renameKey(key, newKey);\n+  }\n+\n+  /**\n+   * Package-private helper function to reduce calls to getBucket().\n+   * @param bucket Bucket to operate in.\n+   * @param path Existing key path.\n+   * @param newPath New key path.\n+   * @throws IOException IOException from bucket.renameKey().\n+   */\n+  void renamePath(OzoneBucket bucket, String path, String newPath)\n+      throws IOException {\n+    incrementCounter(Statistic.OBJECTS_RENAMED);\n+    OFSPath ofsPath = new OFSPath(path);\n+    OFSPath ofsNewPath = new OFSPath(newPath);\n+    // No same-bucket policy check here since this call path is controlled\n+    String key = ofsPath.getKeyName();\n+    String newKey = ofsNewPath.getKeyName();\n+    bucket.renameKey(key, newKey);\n+  }\n+\n+  /**\n+   * Helper method to create an directory specified by key name in bucket.\n+   *\n+   * @param pathStr path to be created as directory\n+   * @return true if the key is created, false otherwise\n+   */\n+  @Override\n+  public boolean createDirectory(String pathStr) throws IOException {\n+    LOG.trace(\"creating dir for path: {}\", pathStr);\n+    incrementCounter(Statistic.OBJECTS_CREATED);\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    // Volume name unspecified, return failure\n+    if (ofsPath.getVolumeName().length() == 0) {\n+      return false;\n+    }\n+    // Handle where only volume is specified in pathStr\n+    if (ofsPath.getBucketName().length() == 0) {\n+      objectStore.createVolume(ofsPath.getVolumeName());\n+      return true;\n+    }\n+    String keyStr = ofsPath.getKeyName();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60b4bf8e6008cbd53d95c3e0f249f33227209c1e"}, "originalPosition": 364}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ5NjY1ODkw", "url": "https://github.com/apache/ozone/pull/415#pullrequestreview-349665890", "createdAt": "2020-01-28T20:05:48Z", "commit": {"oid": "60b4bf8e6008cbd53d95c3e0f249f33227209c1e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOFQyMDowNTo0OVrOFiyzXg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOFQyMDowNTo0OVrOFiyzXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAyODI1NA==", "bodyText": "check keyName is empty?", "url": "https://github.com/apache/ozone/pull/415#discussion_r372028254", "createdAt": "2020-01-28T20:05:49Z", "author": {"login": "xiaoyuyao"}, "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneClientAdapterImpl.java", "diffHunk": "@@ -0,0 +1,694 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import org.apache.commons.collections.CollectionUtils;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.crypto.key.KeyProvider;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdds.client.ReplicationFactor;\n+import org.apache.hadoop.hdds.client.ReplicationType;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.security.x509.SecurityConfig;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.ozone.OmUtils;\n+import org.apache.hadoop.ozone.OzoneConfigKeys;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClient;\n+import org.apache.hadoop.ozone.client.OzoneClientFactory;\n+import org.apache.hadoop.ozone.client.OzoneKey;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.ozone.client.io.OzoneOutputStream;\n+import org.apache.hadoop.ozone.client.protocol.ClientProtocol;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfoGroup;\n+import org.apache.hadoop.ozone.om.helpers.OzoneFileStatus;\n+import org.apache.hadoop.ozone.security.OzoneTokenIdentifier;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.security.token.TokenRenewer;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .BUCKET_ALREADY_EXISTS;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .VOLUME_ALREADY_EXISTS;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .VOLUME_NOT_FOUND;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .BUCKET_NOT_FOUND;\n+\n+/**\n+ * Basic Implementation of the OzoneFileSystem calls.\n+ * <p>\n+ * This is the minimal version which doesn't include any statistics.\n+ * <p>\n+ * For full featured version use OzoneClientAdapterImpl.\n+ */\n+public class BasicRootedOzoneClientAdapterImpl\n+    implements RootedOzoneClientAdapter {\n+\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneClientAdapterImpl.class);\n+\n+  private OzoneClient ozoneClient;\n+  private ClientProtocol proxy;\n+  private ObjectStore objectStore;\n+  private ReplicationType replicationType;\n+  private ReplicationFactor replicationFactor;\n+  private boolean securityEnabled;\n+  private int configuredDnPort;\n+\n+  /**\n+   * Create new OzoneClientAdapter implementation.\n+   *\n+   * @throws IOException In case of a problem.\n+   */\n+  public BasicRootedOzoneClientAdapterImpl() throws IOException {\n+    this(createConf());\n+  }\n+\n+  private static OzoneConfiguration createConf() {\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+    try {\n+      return new OzoneConfiguration();\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(OzoneConfiguration conf)\n+      throws IOException {\n+    this(null, -1, conf);\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(String omHost, int omPort,\n+      Configuration hadoopConf) throws IOException {\n+\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+\n+    try {\n+      OzoneConfiguration conf = OzoneConfiguration.of(hadoopConf);\n+\n+      if (omHost == null && OmUtils.isServiceIdsDefined(conf)) {\n+        // When the host name or service id isn't given\n+        // but ozone.om.service.ids is defined, declare failure.\n+\n+        // This is a safety precaution that prevents the client from\n+        // accidentally failing over to an unintended OM.\n+        throw new IllegalArgumentException(\"Service ID or host name must not\"\n+            + \" be omitted when ozone.om.service.ids is defined.\");\n+      }\n+\n+      if (omPort != -1) {\n+        // When the port number is specified, perform the following check\n+        if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+          // If omHost is a service id, it shouldn't use a port\n+          throw new IllegalArgumentException(\"Port \" + omPort +\n+              \" specified in URI but host '\" + omHost + \"' is a \"\n+              + \"logical (HA) OzoneManager and does not use port information.\");\n+        }\n+      } else {\n+        // When port number is not specified, read it from config\n+        omPort = OmUtils.getOmRpcPort(conf);\n+      }\n+\n+      SecurityConfig secConfig = new SecurityConfig(conf);\n+\n+      if (secConfig.isSecurityEnabled()) {\n+        this.securityEnabled = true;\n+      }\n+\n+      String replicationTypeConf =\n+          conf.get(OzoneConfigKeys.OZONE_REPLICATION_TYPE,\n+              OzoneConfigKeys.OZONE_REPLICATION_TYPE_DEFAULT);\n+\n+      int replicationCountConf = conf.getInt(OzoneConfigKeys.OZONE_REPLICATION,\n+          OzoneConfigKeys.OZONE_REPLICATION_DEFAULT);\n+\n+      if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+        // omHost is listed as one of the service ids in the config,\n+        // thus we should treat omHost as omServiceId\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, conf);\n+      } else if (StringUtils.isNotEmpty(omHost) && omPort != -1) {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, omPort, conf);\n+      } else {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(conf);\n+      }\n+      objectStore = ozoneClient.getObjectStore();\n+      proxy = objectStore.getClientProxy();\n+      this.replicationType = ReplicationType.valueOf(replicationTypeConf);\n+      this.replicationFactor = ReplicationFactor.valueOf(replicationCountConf);\n+      this.configuredDnPort = conf.getInt(\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT,\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT_DEFAULT);\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  OzoneBucket getBucket(OFSPath ofsPath, boolean createIfNotExist)\n+      throws IOException {\n+\n+    return getBucket(ofsPath.getVolumeName(), ofsPath.getBucketName(),\n+        createIfNotExist);\n+  }\n+\n+  /**\n+   * Get OzoneBucket object to operate in.\n+   * Optionally create volume and bucket if not found.\n+   *\n+   * @param createIfNotExist Set this to true if the caller is a write operation\n+   *                         in order to create the volume and bucket.\n+   * @throws IOException Exceptions other than OMException with result code\n+   *                     VOLUME_NOT_FOUND or BUCKET_NOT_FOUND.\n+   */\n+  private OzoneBucket getBucket(String volumeStr, String bucketStr,\n+      boolean createIfNotExist) throws IOException {\n+\n+    OzoneBucket bucket;\n+    try {\n+      bucket = proxy.getBucketDetails(volumeStr, bucketStr);\n+    } catch (OMException ex) {\n+      if (createIfNotExist) {\n+        // Note: getBucketDetails always throws BUCKET_NOT_FOUND, even if\n+        // the volume doesn't exist.\n+        if (ex.getResult().equals(BUCKET_NOT_FOUND)) {\n+          OzoneVolume volume;\n+          try {\n+            volume = proxy.getVolumeDetails(volumeStr);\n+          } catch (OMException getVolEx) {\n+            if (getVolEx.getResult().equals(VOLUME_NOT_FOUND)) {\n+              // Volume doesn't exist. Create it\n+              try {\n+                objectStore.createVolume(volumeStr);\n+              } catch (OMException newVolEx) {\n+                // Ignore the case where another client created the volume\n+                if (!newVolEx.getResult().equals(VOLUME_ALREADY_EXISTS)) {\n+                  throw newVolEx;\n+                }\n+              }\n+            } else {\n+              throw getVolEx;\n+            }\n+            // Try get volume again\n+            volume = proxy.getVolumeDetails(volumeStr);\n+          }\n+          // Create the bucket\n+          try {\n+            volume.createBucket(bucketStr);\n+          } catch (OMException newBucEx) {\n+            // Ignore the case where another client created the bucket\n+            if (!newBucEx.getResult().equals(BUCKET_ALREADY_EXISTS)) {\n+              throw newBucEx;\n+            }\n+          }\n+        }\n+        // Try get bucket again\n+        bucket = proxy.getBucketDetails(volumeStr, bucketStr);\n+      } else {\n+        throw ex;\n+      }\n+    }\n+\n+    return bucket;\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    ozoneClient.close();\n+  }\n+\n+  @Override\n+  public InputStream readFile(String pathStr) throws IOException {\n+    incrementCounter(Statistic.OBJECTS_READ);\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    String key = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, false);\n+      return bucket.readFile(key).getInputStream();\n+    } catch (OMException ex) {\n+      if (ex.getResult() == OMException.ResultCodes.FILE_NOT_FOUND\n+          || ex.getResult() == OMException.ResultCodes.NOT_A_FILE) {\n+        throw new FileNotFoundException(\n+            ex.getResult().name() + \": \" + ex.getMessage());\n+      } else {\n+        throw ex;\n+      }\n+    }\n+  }\n+\n+  protected void incrementCounter(Statistic objectsRead) {\n+    //noop: Use OzoneClientAdapterImpl which supports statistics.\n+  }\n+\n+  @Override\n+  public OzoneFSOutputStream createFile(String pathStr, boolean overWrite,\n+      boolean recursive) throws IOException {\n+    incrementCounter(Statistic.OBJECTS_CREATED);\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    String key = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, false);\n+      OzoneOutputStream ozoneOutputStream = bucket.createFile(\n+          key, 0, replicationType, replicationFactor, overWrite, recursive);\n+      return new OzoneFSOutputStream(ozoneOutputStream.getOutputStream());\n+    } catch (OMException ex) {\n+      if (ex.getResult() == OMException.ResultCodes.FILE_ALREADY_EXISTS\n+          || ex.getResult() == OMException.ResultCodes.NOT_A_FILE) {\n+        throw new FileAlreadyExistsException(\n+            ex.getResult().name() + \": \" + ex.getMessage());\n+      } else {\n+        throw ex;\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void renamePath(String path, String newPath) throws IOException {\n+    incrementCounter(Statistic.OBJECTS_RENAMED);\n+    OFSPath ofsPath = new OFSPath(path);\n+    OFSPath ofsNewPath = new OFSPath(newPath);\n+\n+    // Check path and newPathName are in the same volume and same bucket.\n+    // This should have been checked in BasicRootedOzoneFileSystem#rename\n+    // already via regular call path unless bypassed.\n+    if (!ofsPath.isInSameBucketAs(ofsNewPath)) {\n+      throw new IOException(\"Cannot rename a key to a different bucket\");\n+    }\n+\n+    OzoneBucket bucket = getBucket(ofsPath, false);\n+    String key = ofsPath.getKeyName();\n+    String newKey = ofsNewPath.getKeyName();\n+    bucket.renameKey(key, newKey);\n+  }\n+\n+  /**\n+   * Package-private helper function to reduce calls to getBucket().\n+   * @param bucket Bucket to operate in.\n+   * @param path Existing key path.\n+   * @param newPath New key path.\n+   * @throws IOException IOException from bucket.renameKey().\n+   */\n+  void renamePath(OzoneBucket bucket, String path, String newPath)\n+      throws IOException {\n+    incrementCounter(Statistic.OBJECTS_RENAMED);\n+    OFSPath ofsPath = new OFSPath(path);\n+    OFSPath ofsNewPath = new OFSPath(newPath);\n+    // No same-bucket policy check here since this call path is controlled\n+    String key = ofsPath.getKeyName();\n+    String newKey = ofsNewPath.getKeyName();\n+    bucket.renameKey(key, newKey);\n+  }\n+\n+  /**\n+   * Helper method to create an directory specified by key name in bucket.\n+   *\n+   * @param pathStr path to be created as directory\n+   * @return true if the key is created, false otherwise\n+   */\n+  @Override\n+  public boolean createDirectory(String pathStr) throws IOException {\n+    LOG.trace(\"creating dir for path: {}\", pathStr);\n+    incrementCounter(Statistic.OBJECTS_CREATED);\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    // Volume name unspecified, return failure\n+    if (ofsPath.getVolumeName().length() == 0) {\n+      return false;\n+    }\n+    // Handle where only volume is specified in pathStr\n+    if (ofsPath.getBucketName().length() == 0) {\n+      objectStore.createVolume(ofsPath.getVolumeName());\n+      return true;\n+    }\n+    String keyStr = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, true);\n+      // if keyStr is empty, it indicates that only volume or volume+bucket is\n+      // given in pathStr, so getBucket() above should've handled the creation\n+      // of volume/bucket already.\n+      if (keyStr != null && keyStr.length() > 0) {\n+        bucket.createDirectory(keyStr);\n+      }\n+    } catch (OMException e) {\n+      if (e.getResult() == OMException.ResultCodes.FILE_ALREADY_EXISTS) {\n+        throw new FileAlreadyExistsException(e.getMessage());\n+      }\n+      throw e;\n+    }\n+    return true;\n+  }\n+\n+  /**\n+   * Helper method to delete an object specified by key name in bucket.\n+   *\n+   * @param path path to a key to be deleted\n+   * @return true if the key is deleted, false otherwise\n+   */\n+  @Override\n+  public boolean deleteObject(String path) {\n+    LOG.trace(\"issuing delete for path to key: {}\", path);\n+    OFSPath ofsPath = new OFSPath(path);\n+    String keyName = ofsPath.getKeyName();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60b4bf8e6008cbd53d95c3e0f249f33227209c1e"}, "originalPosition": 392}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ5NjY3NTYz", "url": "https://github.com/apache/ozone/pull/415#pullrequestreview-349667563", "createdAt": "2020-01-28T20:08:41Z", "commit": {"oid": "60b4bf8e6008cbd53d95c3e0f249f33227209c1e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOFQyMDowODo0MlrOFiy4Kw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOFQyMDowODo0MlrOFiy4Kw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAyOTQ4Mw==", "bodyText": "should we move the counter incr before getBucket so that we can accurately count the ops even though getBucket may fail?", "url": "https://github.com/apache/ozone/pull/415#discussion_r372029483", "createdAt": "2020-01-28T20:08:42Z", "author": {"login": "xiaoyuyao"}, "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneClientAdapterImpl.java", "diffHunk": "@@ -0,0 +1,694 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import org.apache.commons.collections.CollectionUtils;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.crypto.key.KeyProvider;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdds.client.ReplicationFactor;\n+import org.apache.hadoop.hdds.client.ReplicationType;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.security.x509.SecurityConfig;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.ozone.OmUtils;\n+import org.apache.hadoop.ozone.OzoneConfigKeys;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClient;\n+import org.apache.hadoop.ozone.client.OzoneClientFactory;\n+import org.apache.hadoop.ozone.client.OzoneKey;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.ozone.client.io.OzoneOutputStream;\n+import org.apache.hadoop.ozone.client.protocol.ClientProtocol;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfoGroup;\n+import org.apache.hadoop.ozone.om.helpers.OzoneFileStatus;\n+import org.apache.hadoop.ozone.security.OzoneTokenIdentifier;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.security.token.TokenRenewer;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .BUCKET_ALREADY_EXISTS;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .VOLUME_ALREADY_EXISTS;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .VOLUME_NOT_FOUND;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .BUCKET_NOT_FOUND;\n+\n+/**\n+ * Basic Implementation of the OzoneFileSystem calls.\n+ * <p>\n+ * This is the minimal version which doesn't include any statistics.\n+ * <p>\n+ * For full featured version use OzoneClientAdapterImpl.\n+ */\n+public class BasicRootedOzoneClientAdapterImpl\n+    implements RootedOzoneClientAdapter {\n+\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneClientAdapterImpl.class);\n+\n+  private OzoneClient ozoneClient;\n+  private ClientProtocol proxy;\n+  private ObjectStore objectStore;\n+  private ReplicationType replicationType;\n+  private ReplicationFactor replicationFactor;\n+  private boolean securityEnabled;\n+  private int configuredDnPort;\n+\n+  /**\n+   * Create new OzoneClientAdapter implementation.\n+   *\n+   * @throws IOException In case of a problem.\n+   */\n+  public BasicRootedOzoneClientAdapterImpl() throws IOException {\n+    this(createConf());\n+  }\n+\n+  private static OzoneConfiguration createConf() {\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+    try {\n+      return new OzoneConfiguration();\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(OzoneConfiguration conf)\n+      throws IOException {\n+    this(null, -1, conf);\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(String omHost, int omPort,\n+      Configuration hadoopConf) throws IOException {\n+\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+\n+    try {\n+      OzoneConfiguration conf = OzoneConfiguration.of(hadoopConf);\n+\n+      if (omHost == null && OmUtils.isServiceIdsDefined(conf)) {\n+        // When the host name or service id isn't given\n+        // but ozone.om.service.ids is defined, declare failure.\n+\n+        // This is a safety precaution that prevents the client from\n+        // accidentally failing over to an unintended OM.\n+        throw new IllegalArgumentException(\"Service ID or host name must not\"\n+            + \" be omitted when ozone.om.service.ids is defined.\");\n+      }\n+\n+      if (omPort != -1) {\n+        // When the port number is specified, perform the following check\n+        if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+          // If omHost is a service id, it shouldn't use a port\n+          throw new IllegalArgumentException(\"Port \" + omPort +\n+              \" specified in URI but host '\" + omHost + \"' is a \"\n+              + \"logical (HA) OzoneManager and does not use port information.\");\n+        }\n+      } else {\n+        // When port number is not specified, read it from config\n+        omPort = OmUtils.getOmRpcPort(conf);\n+      }\n+\n+      SecurityConfig secConfig = new SecurityConfig(conf);\n+\n+      if (secConfig.isSecurityEnabled()) {\n+        this.securityEnabled = true;\n+      }\n+\n+      String replicationTypeConf =\n+          conf.get(OzoneConfigKeys.OZONE_REPLICATION_TYPE,\n+              OzoneConfigKeys.OZONE_REPLICATION_TYPE_DEFAULT);\n+\n+      int replicationCountConf = conf.getInt(OzoneConfigKeys.OZONE_REPLICATION,\n+          OzoneConfigKeys.OZONE_REPLICATION_DEFAULT);\n+\n+      if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+        // omHost is listed as one of the service ids in the config,\n+        // thus we should treat omHost as omServiceId\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, conf);\n+      } else if (StringUtils.isNotEmpty(omHost) && omPort != -1) {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, omPort, conf);\n+      } else {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(conf);\n+      }\n+      objectStore = ozoneClient.getObjectStore();\n+      proxy = objectStore.getClientProxy();\n+      this.replicationType = ReplicationType.valueOf(replicationTypeConf);\n+      this.replicationFactor = ReplicationFactor.valueOf(replicationCountConf);\n+      this.configuredDnPort = conf.getInt(\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT,\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT_DEFAULT);\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  OzoneBucket getBucket(OFSPath ofsPath, boolean createIfNotExist)\n+      throws IOException {\n+\n+    return getBucket(ofsPath.getVolumeName(), ofsPath.getBucketName(),\n+        createIfNotExist);\n+  }\n+\n+  /**\n+   * Get OzoneBucket object to operate in.\n+   * Optionally create volume and bucket if not found.\n+   *\n+   * @param createIfNotExist Set this to true if the caller is a write operation\n+   *                         in order to create the volume and bucket.\n+   * @throws IOException Exceptions other than OMException with result code\n+   *                     VOLUME_NOT_FOUND or BUCKET_NOT_FOUND.\n+   */\n+  private OzoneBucket getBucket(String volumeStr, String bucketStr,\n+      boolean createIfNotExist) throws IOException {\n+\n+    OzoneBucket bucket;\n+    try {\n+      bucket = proxy.getBucketDetails(volumeStr, bucketStr);\n+    } catch (OMException ex) {\n+      if (createIfNotExist) {\n+        // Note: getBucketDetails always throws BUCKET_NOT_FOUND, even if\n+        // the volume doesn't exist.\n+        if (ex.getResult().equals(BUCKET_NOT_FOUND)) {\n+          OzoneVolume volume;\n+          try {\n+            volume = proxy.getVolumeDetails(volumeStr);\n+          } catch (OMException getVolEx) {\n+            if (getVolEx.getResult().equals(VOLUME_NOT_FOUND)) {\n+              // Volume doesn't exist. Create it\n+              try {\n+                objectStore.createVolume(volumeStr);\n+              } catch (OMException newVolEx) {\n+                // Ignore the case where another client created the volume\n+                if (!newVolEx.getResult().equals(VOLUME_ALREADY_EXISTS)) {\n+                  throw newVolEx;\n+                }\n+              }\n+            } else {\n+              throw getVolEx;\n+            }\n+            // Try get volume again\n+            volume = proxy.getVolumeDetails(volumeStr);\n+          }\n+          // Create the bucket\n+          try {\n+            volume.createBucket(bucketStr);\n+          } catch (OMException newBucEx) {\n+            // Ignore the case where another client created the bucket\n+            if (!newBucEx.getResult().equals(BUCKET_ALREADY_EXISTS)) {\n+              throw newBucEx;\n+            }\n+          }\n+        }\n+        // Try get bucket again\n+        bucket = proxy.getBucketDetails(volumeStr, bucketStr);\n+      } else {\n+        throw ex;\n+      }\n+    }\n+\n+    return bucket;\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    ozoneClient.close();\n+  }\n+\n+  @Override\n+  public InputStream readFile(String pathStr) throws IOException {\n+    incrementCounter(Statistic.OBJECTS_READ);\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    String key = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, false);\n+      return bucket.readFile(key).getInputStream();\n+    } catch (OMException ex) {\n+      if (ex.getResult() == OMException.ResultCodes.FILE_NOT_FOUND\n+          || ex.getResult() == OMException.ResultCodes.NOT_A_FILE) {\n+        throw new FileNotFoundException(\n+            ex.getResult().name() + \": \" + ex.getMessage());\n+      } else {\n+        throw ex;\n+      }\n+    }\n+  }\n+\n+  protected void incrementCounter(Statistic objectsRead) {\n+    //noop: Use OzoneClientAdapterImpl which supports statistics.\n+  }\n+\n+  @Override\n+  public OzoneFSOutputStream createFile(String pathStr, boolean overWrite,\n+      boolean recursive) throws IOException {\n+    incrementCounter(Statistic.OBJECTS_CREATED);\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    String key = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, false);\n+      OzoneOutputStream ozoneOutputStream = bucket.createFile(\n+          key, 0, replicationType, replicationFactor, overWrite, recursive);\n+      return new OzoneFSOutputStream(ozoneOutputStream.getOutputStream());\n+    } catch (OMException ex) {\n+      if (ex.getResult() == OMException.ResultCodes.FILE_ALREADY_EXISTS\n+          || ex.getResult() == OMException.ResultCodes.NOT_A_FILE) {\n+        throw new FileAlreadyExistsException(\n+            ex.getResult().name() + \": \" + ex.getMessage());\n+      } else {\n+        throw ex;\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void renamePath(String path, String newPath) throws IOException {\n+    incrementCounter(Statistic.OBJECTS_RENAMED);\n+    OFSPath ofsPath = new OFSPath(path);\n+    OFSPath ofsNewPath = new OFSPath(newPath);\n+\n+    // Check path and newPathName are in the same volume and same bucket.\n+    // This should have been checked in BasicRootedOzoneFileSystem#rename\n+    // already via regular call path unless bypassed.\n+    if (!ofsPath.isInSameBucketAs(ofsNewPath)) {\n+      throw new IOException(\"Cannot rename a key to a different bucket\");\n+    }\n+\n+    OzoneBucket bucket = getBucket(ofsPath, false);\n+    String key = ofsPath.getKeyName();\n+    String newKey = ofsNewPath.getKeyName();\n+    bucket.renameKey(key, newKey);\n+  }\n+\n+  /**\n+   * Package-private helper function to reduce calls to getBucket().\n+   * @param bucket Bucket to operate in.\n+   * @param path Existing key path.\n+   * @param newPath New key path.\n+   * @throws IOException IOException from bucket.renameKey().\n+   */\n+  void renamePath(OzoneBucket bucket, String path, String newPath)\n+      throws IOException {\n+    incrementCounter(Statistic.OBJECTS_RENAMED);\n+    OFSPath ofsPath = new OFSPath(path);\n+    OFSPath ofsNewPath = new OFSPath(newPath);\n+    // No same-bucket policy check here since this call path is controlled\n+    String key = ofsPath.getKeyName();\n+    String newKey = ofsNewPath.getKeyName();\n+    bucket.renameKey(key, newKey);\n+  }\n+\n+  /**\n+   * Helper method to create an directory specified by key name in bucket.\n+   *\n+   * @param pathStr path to be created as directory\n+   * @return true if the key is created, false otherwise\n+   */\n+  @Override\n+  public boolean createDirectory(String pathStr) throws IOException {\n+    LOG.trace(\"creating dir for path: {}\", pathStr);\n+    incrementCounter(Statistic.OBJECTS_CREATED);\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    // Volume name unspecified, return failure\n+    if (ofsPath.getVolumeName().length() == 0) {\n+      return false;\n+    }\n+    // Handle where only volume is specified in pathStr\n+    if (ofsPath.getBucketName().length() == 0) {\n+      objectStore.createVolume(ofsPath.getVolumeName());\n+      return true;\n+    }\n+    String keyStr = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, true);\n+      // if keyStr is empty, it indicates that only volume or volume+bucket is\n+      // given in pathStr, so getBucket() above should've handled the creation\n+      // of volume/bucket already.\n+      if (keyStr != null && keyStr.length() > 0) {\n+        bucket.createDirectory(keyStr);\n+      }\n+    } catch (OMException e) {\n+      if (e.getResult() == OMException.ResultCodes.FILE_ALREADY_EXISTS) {\n+        throw new FileAlreadyExistsException(e.getMessage());\n+      }\n+      throw e;\n+    }\n+    return true;\n+  }\n+\n+  /**\n+   * Helper method to delete an object specified by key name in bucket.\n+   *\n+   * @param path path to a key to be deleted\n+   * @return true if the key is deleted, false otherwise\n+   */\n+  @Override\n+  public boolean deleteObject(String path) {\n+    LOG.trace(\"issuing delete for path to key: {}\", path);\n+    OFSPath ofsPath = new OFSPath(path);\n+    String keyName = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, false);\n+      incrementCounter(Statistic.OBJECTS_DELETED);\n+      bucket.deleteKey(keyName);\n+      return true;\n+    } catch (IOException ioe) {\n+      LOG.error(\"delete key failed \" + ioe.getMessage());\n+      return false;\n+    }\n+  }\n+\n+  /**\n+   * Package-private helper function to reduce calls to getBucket().\n+   * @param bucket Bucket to operate in.\n+   * @param path Path to delete.\n+   * @return true if operation succeeded, false upon IOException.\n+   */\n+  boolean deleteObject(OzoneBucket bucket, String path) {\n+    LOG.trace(\"issuing delete for path to key: {}\", path);\n+    OFSPath ofsPath = new OFSPath(path);\n+    String keyName = ofsPath.getKeyName();\n+    try {\n+      incrementCounter(Statistic.OBJECTS_DELETED);\n+      bucket.deleteKey(keyName);\n+      return true;\n+    } catch (IOException ioe) {\n+      LOG.error(\"delete key failed \" + ioe.getMessage());\n+      return false;\n+    }\n+  }\n+\n+  public FileStatusAdapter getFileStatus(String path, URI uri,\n+      Path qualifiedPath, String userName)\n+      throws IOException {\n+    OFSPath ofsPath = new OFSPath(path);\n+    String key = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, false);\n+      incrementCounter(Statistic.OBJECTS_QUERY);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60b4bf8e6008cbd53d95c3e0f249f33227209c1e"}, "originalPosition": 431}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ5NzI1Mzkw", "url": "https://github.com/apache/ozone/pull/415#pullrequestreview-349725390", "createdAt": "2020-01-28T21:44:12Z", "commit": {"oid": "60b4bf8e6008cbd53d95c3e0f249f33227209c1e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOFQyMTo0NDoxMlrOFi1p8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOFQyMTo0NDoxMlrOFi1p8w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjA3NDk5NQ==", "bodyText": "How is the /tempVol/tempBucket created? Do we provide a CLI to admin?", "url": "https://github.com/apache/ozone/pull/415#discussion_r372074995", "createdAt": "2020-01-28T21:44:12Z", "author": {"login": "xiaoyuyao"}, "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OFSPath.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.yetus.audience.InterfaceStability;\n+import java.util.StringTokenizer;\n+\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_URI_DELIMITER;\n+\n+/**\n+ * Utility class for Rooted Ozone Filesystem (OFS) path processing.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Unstable\n+class OFSPath {\n+  /**\n+   * Here is a table illustrating what each name variable is given an input path\n+   * Assuming /tmp is mounted to /tempVol/tempBucket", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60b4bf8e6008cbd53d95c3e0f249f33227209c1e"}, "originalPosition": 35}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ5NzYzMzIx", "url": "https://github.com/apache/ozone/pull/415#pullrequestreview-349763321", "createdAt": "2020-01-28T22:55:35Z", "commit": {"oid": "60b4bf8e6008cbd53d95c3e0f249f33227209c1e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOFQyMjo1NTozNVrOFi3fgw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOFQyMjo1NTozNVrOFi3fgw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjEwNTA5MQ==", "bodyText": "Let's change renamePath to rename", "url": "https://github.com/apache/ozone/pull/415#discussion_r372105091", "createdAt": "2020-01-28T22:55:35Z", "author": {"login": "xiaoyuyao"}, "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneClientAdapterImpl.java", "diffHunk": "@@ -0,0 +1,694 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import org.apache.commons.collections.CollectionUtils;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.crypto.key.KeyProvider;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdds.client.ReplicationFactor;\n+import org.apache.hadoop.hdds.client.ReplicationType;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.security.x509.SecurityConfig;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.ozone.OmUtils;\n+import org.apache.hadoop.ozone.OzoneConfigKeys;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClient;\n+import org.apache.hadoop.ozone.client.OzoneClientFactory;\n+import org.apache.hadoop.ozone.client.OzoneKey;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.ozone.client.io.OzoneOutputStream;\n+import org.apache.hadoop.ozone.client.protocol.ClientProtocol;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfoGroup;\n+import org.apache.hadoop.ozone.om.helpers.OzoneFileStatus;\n+import org.apache.hadoop.ozone.security.OzoneTokenIdentifier;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.security.token.TokenRenewer;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .BUCKET_ALREADY_EXISTS;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .VOLUME_ALREADY_EXISTS;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .VOLUME_NOT_FOUND;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .BUCKET_NOT_FOUND;\n+\n+/**\n+ * Basic Implementation of the OzoneFileSystem calls.\n+ * <p>\n+ * This is the minimal version which doesn't include any statistics.\n+ * <p>\n+ * For full featured version use OzoneClientAdapterImpl.\n+ */\n+public class BasicRootedOzoneClientAdapterImpl\n+    implements RootedOzoneClientAdapter {\n+\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneClientAdapterImpl.class);\n+\n+  private OzoneClient ozoneClient;\n+  private ClientProtocol proxy;\n+  private ObjectStore objectStore;\n+  private ReplicationType replicationType;\n+  private ReplicationFactor replicationFactor;\n+  private boolean securityEnabled;\n+  private int configuredDnPort;\n+\n+  /**\n+   * Create new OzoneClientAdapter implementation.\n+   *\n+   * @throws IOException In case of a problem.\n+   */\n+  public BasicRootedOzoneClientAdapterImpl() throws IOException {\n+    this(createConf());\n+  }\n+\n+  private static OzoneConfiguration createConf() {\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+    try {\n+      return new OzoneConfiguration();\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(OzoneConfiguration conf)\n+      throws IOException {\n+    this(null, -1, conf);\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(String omHost, int omPort,\n+      Configuration hadoopConf) throws IOException {\n+\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+\n+    try {\n+      OzoneConfiguration conf = OzoneConfiguration.of(hadoopConf);\n+\n+      if (omHost == null && OmUtils.isServiceIdsDefined(conf)) {\n+        // When the host name or service id isn't given\n+        // but ozone.om.service.ids is defined, declare failure.\n+\n+        // This is a safety precaution that prevents the client from\n+        // accidentally failing over to an unintended OM.\n+        throw new IllegalArgumentException(\"Service ID or host name must not\"\n+            + \" be omitted when ozone.om.service.ids is defined.\");\n+      }\n+\n+      if (omPort != -1) {\n+        // When the port number is specified, perform the following check\n+        if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+          // If omHost is a service id, it shouldn't use a port\n+          throw new IllegalArgumentException(\"Port \" + omPort +\n+              \" specified in URI but host '\" + omHost + \"' is a \"\n+              + \"logical (HA) OzoneManager and does not use port information.\");\n+        }\n+      } else {\n+        // When port number is not specified, read it from config\n+        omPort = OmUtils.getOmRpcPort(conf);\n+      }\n+\n+      SecurityConfig secConfig = new SecurityConfig(conf);\n+\n+      if (secConfig.isSecurityEnabled()) {\n+        this.securityEnabled = true;\n+      }\n+\n+      String replicationTypeConf =\n+          conf.get(OzoneConfigKeys.OZONE_REPLICATION_TYPE,\n+              OzoneConfigKeys.OZONE_REPLICATION_TYPE_DEFAULT);\n+\n+      int replicationCountConf = conf.getInt(OzoneConfigKeys.OZONE_REPLICATION,\n+          OzoneConfigKeys.OZONE_REPLICATION_DEFAULT);\n+\n+      if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+        // omHost is listed as one of the service ids in the config,\n+        // thus we should treat omHost as omServiceId\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, conf);\n+      } else if (StringUtils.isNotEmpty(omHost) && omPort != -1) {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, omPort, conf);\n+      } else {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(conf);\n+      }\n+      objectStore = ozoneClient.getObjectStore();\n+      proxy = objectStore.getClientProxy();\n+      this.replicationType = ReplicationType.valueOf(replicationTypeConf);\n+      this.replicationFactor = ReplicationFactor.valueOf(replicationCountConf);\n+      this.configuredDnPort = conf.getInt(\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT,\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT_DEFAULT);\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  OzoneBucket getBucket(OFSPath ofsPath, boolean createIfNotExist)\n+      throws IOException {\n+\n+    return getBucket(ofsPath.getVolumeName(), ofsPath.getBucketName(),\n+        createIfNotExist);\n+  }\n+\n+  /**\n+   * Get OzoneBucket object to operate in.\n+   * Optionally create volume and bucket if not found.\n+   *\n+   * @param createIfNotExist Set this to true if the caller is a write operation\n+   *                         in order to create the volume and bucket.\n+   * @throws IOException Exceptions other than OMException with result code\n+   *                     VOLUME_NOT_FOUND or BUCKET_NOT_FOUND.\n+   */\n+  private OzoneBucket getBucket(String volumeStr, String bucketStr,\n+      boolean createIfNotExist) throws IOException {\n+\n+    OzoneBucket bucket;\n+    try {\n+      bucket = proxy.getBucketDetails(volumeStr, bucketStr);\n+    } catch (OMException ex) {\n+      if (createIfNotExist) {\n+        // Note: getBucketDetails always throws BUCKET_NOT_FOUND, even if\n+        // the volume doesn't exist.\n+        if (ex.getResult().equals(BUCKET_NOT_FOUND)) {\n+          OzoneVolume volume;\n+          try {\n+            volume = proxy.getVolumeDetails(volumeStr);\n+          } catch (OMException getVolEx) {\n+            if (getVolEx.getResult().equals(VOLUME_NOT_FOUND)) {\n+              // Volume doesn't exist. Create it\n+              try {\n+                objectStore.createVolume(volumeStr);\n+              } catch (OMException newVolEx) {\n+                // Ignore the case where another client created the volume\n+                if (!newVolEx.getResult().equals(VOLUME_ALREADY_EXISTS)) {\n+                  throw newVolEx;\n+                }\n+              }\n+            } else {\n+              throw getVolEx;\n+            }\n+            // Try get volume again\n+            volume = proxy.getVolumeDetails(volumeStr);\n+          }\n+          // Create the bucket\n+          try {\n+            volume.createBucket(bucketStr);\n+          } catch (OMException newBucEx) {\n+            // Ignore the case where another client created the bucket\n+            if (!newBucEx.getResult().equals(BUCKET_ALREADY_EXISTS)) {\n+              throw newBucEx;\n+            }\n+          }\n+        }\n+        // Try get bucket again\n+        bucket = proxy.getBucketDetails(volumeStr, bucketStr);\n+      } else {\n+        throw ex;\n+      }\n+    }\n+\n+    return bucket;\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    ozoneClient.close();\n+  }\n+\n+  @Override\n+  public InputStream readFile(String pathStr) throws IOException {\n+    incrementCounter(Statistic.OBJECTS_READ);\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    String key = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, false);\n+      return bucket.readFile(key).getInputStream();\n+    } catch (OMException ex) {\n+      if (ex.getResult() == OMException.ResultCodes.FILE_NOT_FOUND\n+          || ex.getResult() == OMException.ResultCodes.NOT_A_FILE) {\n+        throw new FileNotFoundException(\n+            ex.getResult().name() + \": \" + ex.getMessage());\n+      } else {\n+        throw ex;\n+      }\n+    }\n+  }\n+\n+  protected void incrementCounter(Statistic objectsRead) {\n+    //noop: Use OzoneClientAdapterImpl which supports statistics.\n+  }\n+\n+  @Override\n+  public OzoneFSOutputStream createFile(String pathStr, boolean overWrite,\n+      boolean recursive) throws IOException {\n+    incrementCounter(Statistic.OBJECTS_CREATED);\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    String key = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, false);\n+      OzoneOutputStream ozoneOutputStream = bucket.createFile(\n+          key, 0, replicationType, replicationFactor, overWrite, recursive);\n+      return new OzoneFSOutputStream(ozoneOutputStream.getOutputStream());\n+    } catch (OMException ex) {\n+      if (ex.getResult() == OMException.ResultCodes.FILE_ALREADY_EXISTS\n+          || ex.getResult() == OMException.ResultCodes.NOT_A_FILE) {\n+        throw new FileAlreadyExistsException(\n+            ex.getResult().name() + \": \" + ex.getMessage());\n+      } else {\n+        throw ex;\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void renamePath(String path, String newPath) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60b4bf8e6008cbd53d95c3e0f249f33227209c1e"}, "originalPosition": 308}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "90c57d18c2e7bb8b5c59db333c1d6dd92392a6be", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/90c57d18c2e7bb8b5c59db333c1d6dd92392a6be", "committedDate": "2020-01-29T18:16:29Z", "message": "Add TODO for test refactoring; new OFS tests are prefixed with \"OFS: \" in javadoc comment."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e16a52ee7629ce89a185be00ab71742f56f1b7ce", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/e16a52ee7629ce89a185be00ab71742f56f1b7ce", "committedDate": "2020-01-29T18:17:36Z", "message": "OFSPath: hard-code volume and bucket name for now so that the rename logic isn't compromised. Addressed comment https://github.com/apache/hadoop-ozone/pull/415#discussion_r372026827"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c0e2ed7f4cf08490b151613fdb3415b097ee5ac0", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/c0e2ed7f4cf08490b151613fdb3415b097ee5ac0", "committedDate": "2020-01-29T18:24:20Z", "message": "Update comment for createDirectory."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a9ef567103a13fd57d2c63197e7e356b447cafd7", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/a9ef567103a13fd57d2c63197e7e356b447cafd7", "committedDate": "2020-01-29T18:26:20Z", "message": "Place incrementCounter at the beginning of operations."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ac0cd77f9c22c19c7442f7e248ab401bbb9ffb73", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/ac0cd77f9c22c19c7442f7e248ab401bbb9ffb73", "committedDate": "2020-01-29T18:29:07Z", "message": "Check keyName empty in deleteObject."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5c1c08178a4d230ded059c8f11e82f19a375a862", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/5c1c08178a4d230ded059c8f11e82f19a375a862", "committedDate": "2020-01-29T18:34:05Z", "message": "renamePath -> rename. https://github.com/apache/hadoop-ozone/pull/415#discussion_r372105091"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2d365d2a355daef244319038be7896ec2266f7a1", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/2d365d2a355daef244319038be7896ec2266f7a1", "committedDate": "2020-01-29T22:38:06Z", "message": "OFSPath: Fix typo; fix test case."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "98887e710d799d345b3ee106299a4d24a9d154fc", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/98887e710d799d345b3ee106299a4d24a9d154fc", "committedDate": "2020-01-29T22:59:38Z", "message": "Move ofsPath.getNonKeyPath*() call out of the loop to speed up / lower memory pressure."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "741c6aa4b21baddcc69cd337acf22a871e36c3f0", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/741c6aa4b21baddcc69cd337acf22a871e36c3f0", "committedDate": "2020-01-29T23:14:41Z", "message": "Fix incompatibility for listKeys in RootedOzoneClientAdapter."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5e12395a0ee8257d135fd4205363fc46d340bd87", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/5e12395a0ee8257d135fd4205363fc46d340bd87", "committedDate": "2020-01-29T23:21:59Z", "message": "Clean up comments."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "09a85c2da5e5270c2e6576682c8b9e77c669b29c", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/09a85c2da5e5270c2e6576682c8b9e77c669b29c", "committedDate": "2020-01-29T23:26:55Z", "message": "De-duplicate RootedOzoneClientAdapter by extending OzoneClientAdapter."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0e58f83a722c17d995f5afc5591f4bd63c89329c", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/0e58f83a722c17d995f5afc5591f4bd63c89329c", "committedDate": "2020-01-29T23:34:47Z", "message": " new ArrayList<OzoneKey>().iterator() -> Collections.emptyIterator() to eliminate warning."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "364e4142abdde20ea18b86525e70eda340fc8145", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/364e4142abdde20ea18b86525e70eda340fc8145", "committedDate": "2020-01-29T23:45:20Z", "message": "Remove RootedOzoneClientAdapterFactory;\nPlace new createAdapter() inside OzoneClientAdapterFactory."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "40d146fc6edbc01dbb5d6b7315ed6d93a71073d4", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/40d146fc6edbc01dbb5d6b7315ed6d93a71073d4", "committedDate": "2020-01-29T23:48:38Z", "message": "Localize adapterImpl into RenameIterator/DeleteIterator for now."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "780ebaf5b5279e921a9eed5a43db34027c9b1aa4", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/780ebaf5b5279e921a9eed5a43db34027c9b1aa4", "committedDate": "2020-01-30T00:01:27Z", "message": "Refine comments."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUxMDQ4NTUx", "url": "https://github.com/apache/ozone/pull/415#pullrequestreview-351048551", "createdAt": "2020-01-30T18:34:07Z", "commit": {"oid": "780ebaf5b5279e921a9eed5a43db34027c9b1aa4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQxODozNDowOFrOFj1dqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQxODozNDowOFrOFj1dqg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzEyMDQyNg==", "bodyText": "Consider reducing the numDirs as it will unnecessarily slow down the PR.", "url": "https://github.com/apache/ozone/pull/415#discussion_r373120426", "createdAt": "2020-01-30T18:34:08Z", "author": {"login": "xiaoyuyao"}, "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/fs/ozone/TestRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,482 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.ozone;\n+\n+import org.apache.commons.io.IOUtils;\n+import org.apache.commons.lang3.RandomStringUtils;\n+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.contract.ContractTestUtils;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.ozone.MiniOzoneCluster;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+import org.apache.hadoop.ozone.TestDataUtil;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClientException;\n+import org.apache.hadoop.ozone.client.OzoneKeyDetails;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Set;\n+import java.util.TreeSet;\n+\n+import static org.apache.hadoop.ozone.om.OMConfigKeys.OZONE_OM_ADDRESS_KEY;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+/**\n+ * Ozone file system tests that are not covered by contract tests.\n+ *\n+ * TODO: Refactor this and TestOzoneFileSystem to eliminate most\n+ *  code duplication.\n+ */\n+public class TestRootedOzoneFileSystem {\n+\n+  @Rule\n+  public Timeout globalTimeout = new Timeout(300_000);\n+\n+  private static MiniOzoneCluster cluster = null;\n+\n+  private static FileSystem fs;\n+  private static RootedOzoneFileSystem ofs;\n+\n+  private static ObjectStore objectStore;\n+\n+  private String volumeName;\n+  private String bucketName;\n+\n+  private String rootPath;\n+\n+  // Store path commonly used by tests that test functionality within a bucket\n+  private String testBucketStr;\n+  private Path testBucketPath;\n+\n+  @Before\n+  public void init() throws Exception {\n+    OzoneConfiguration conf = new OzoneConfiguration();\n+    cluster = MiniOzoneCluster.newBuilder(conf)\n+        .setNumDatanodes(3)\n+        .build();\n+    cluster.waitForClusterToBeReady();\n+    objectStore = cluster.getClient().getObjectStore();\n+\n+    // create a volume and a bucket to be used by RootedOzoneFileSystem (OFS)\n+    OzoneBucket bucket = TestDataUtil.createVolumeAndBucket(cluster);\n+    volumeName = bucket.getVolumeName();\n+    bucketName = bucket.getName();\n+    testBucketStr = \"/\" + volumeName + \"/\" + bucketName;\n+    testBucketPath = new Path(testBucketStr);\n+\n+    rootPath = String.format(\"%s://%s/\", OzoneConsts.OZONE_OFS_URI_SCHEME,\n+        conf.get(OZONE_OM_ADDRESS_KEY));\n+\n+    // Set the fs.defaultFS and start the filesystem\n+    conf.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, rootPath);\n+    // Note: FileSystem#loadFileSystems won't load OFS class due to META-INF\n+    //  hence this workaround.\n+    conf.set(\"fs.ofs.impl\", \"org.apache.hadoop.fs.ozone.RootedOzoneFileSystem\");\n+    fs = FileSystem.get(conf);\n+    ofs = (RootedOzoneFileSystem) fs;\n+  }\n+\n+  @After\n+  public void teardown() {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n+    IOUtils.closeQuietly(fs);\n+  }\n+\n+  @Test\n+  public void testOzoneFsServiceLoader() throws IOException {\n+    OzoneConfiguration conf = new OzoneConfiguration();\n+    // Note: FileSystem#loadFileSystems won't load OFS class due to META-INF\n+    //  hence this workaround.\n+    conf.set(\"fs.ofs.impl\", \"org.apache.hadoop.fs.ozone.RootedOzoneFileSystem\");\n+    assertEquals(\n+        FileSystem.getFileSystemClass(OzoneConsts.OZONE_OFS_URI_SCHEME, conf),\n+        RootedOzoneFileSystem.class);\n+  }\n+\n+  @Test\n+  public void testCreateDoesNotAddParentDirKeys() throws Exception {\n+    Path grandparent = new Path(testBucketPath,\n+        \"testCreateDoesNotAddParentDirKeys\");\n+    Path parent = new Path(grandparent, \"parent\");\n+    Path child = new Path(parent, \"child\");\n+    ContractTestUtils.touch(fs, child);\n+\n+    OzoneKeyDetails key = getKey(child, false);\n+    OFSPath childOFSPath = new OFSPath(child);\n+    assertEquals(key.getName(), childOFSPath.getKeyName());\n+\n+    // Creating a child should not add parent keys to the bucket\n+    try {\n+      getKey(parent, true);\n+    } catch (IOException ex) {\n+      assertKeyNotFoundException(ex);\n+    }\n+\n+    // List status on the parent should show the child file\n+    assertEquals(\"List status of parent should include the 1 child file\", 1L,\n+        (long)fs.listStatus(parent).length);\n+    assertTrue(\"Parent directory does not appear to be a directory\",\n+        fs.getFileStatus(parent).isDirectory());\n+  }\n+\n+  @Test\n+  public void testDeleteCreatesFakeParentDir() throws Exception {\n+    Path grandparent = new Path(testBucketPath,\n+        \"testDeleteCreatesFakeParentDir\");\n+    Path parent = new Path(grandparent, \"parent\");\n+    Path child = new Path(parent, \"child\");\n+    ContractTestUtils.touch(fs, child);\n+\n+    // Verify that parent dir key does not exist\n+    // Creating a child should not add parent keys to the bucket\n+    try {\n+      getKey(parent, true);\n+    } catch (IOException ex) {\n+      assertKeyNotFoundException(ex);\n+    }\n+\n+    // Delete the child key\n+    assertTrue(fs.delete(child, false));\n+\n+    // Deleting the only child should create the parent dir key if it does\n+    // not exist\n+    OFSPath parentOFSPath = new OFSPath(parent);\n+    String parentKey = parentOFSPath.getKeyName() + \"/\";\n+    OzoneKeyDetails parentKeyInfo = getKey(parent, true);\n+    assertEquals(parentKey, parentKeyInfo.getName());\n+\n+    // Recursive delete with DeleteIterator\n+    assertTrue(fs.delete(grandparent, true));\n+  }\n+\n+  @Test\n+  public void testListStatus() throws Exception {\n+    Path parent = new Path(testBucketPath, \"testListStatus\");\n+    Path file1 = new Path(parent, \"key1\");\n+    Path file2 = new Path(parent, \"key2\");\n+\n+    FileStatus[] fileStatuses = ofs.listStatus(testBucketPath);\n+    assertEquals(\"Should be empty\", 0, fileStatuses.length);\n+\n+    ContractTestUtils.touch(fs, file1);\n+    ContractTestUtils.touch(fs, file2);\n+\n+    fileStatuses = ofs.listStatus(testBucketPath);\n+    assertEquals(\"Should have created parent\",\n+        1, fileStatuses.length);\n+    assertEquals(\"Parent path doesn't match\",\n+        fileStatuses[0].getPath().toUri().getPath(), parent.toString());\n+\n+    // ListStatus on a directory should return all subdirs along with\n+    // files, even if there exists a file and sub-dir with the same name.\n+    fileStatuses = ofs.listStatus(parent);\n+    assertEquals(\"FileStatus did not return all children of the directory\",\n+        2, fileStatuses.length);\n+\n+    // ListStatus should return only the immediate children of a directory.\n+    Path file3 = new Path(parent, \"dir1/key3\");\n+    Path file4 = new Path(parent, \"dir1/key4\");\n+    ContractTestUtils.touch(fs, file3);\n+    ContractTestUtils.touch(fs, file4);\n+    fileStatuses = ofs.listStatus(parent);\n+    assertEquals(\"FileStatus did not return all children of the directory\",\n+        3, fileStatuses.length);\n+  }\n+\n+  /**\n+   * OFS: Helper function for tests. Return a volume name that doesn't exist.\n+   */\n+  private String getRandomNonExistVolumeName() throws Exception {\n+    final int numDigit = 5;\n+    long retriesLeft = Math.round(Math.pow(10, 5));\n+    String name = null;\n+    while (name == null && retriesLeft-- > 0) {\n+      name = \"volume-\" + RandomStringUtils.randomNumeric(numDigit);\n+      // Check volume existence.\n+      Iterator<? extends OzoneVolume> iter =\n+          objectStore.listVolumesByUser(null, name, null);\n+      if (iter.hasNext()) {\n+        // If there is a match, try again.\n+        // Note that volume name prefix match doesn't equal volume existence\n+        //  but the check is sufficient for this test.\n+        name = null;\n+      }\n+    }\n+    if (retriesLeft <= 0) {\n+      throw new Exception(\n+          \"Failed to generate random volume name that doesn't exist already.\");\n+    }\n+    return name;\n+  }\n+\n+  /**\n+   * OFS: Test mkdir on volume, bucket and dir that doesn't exist.\n+   */\n+  @Test\n+  public void testMkdirOnNonExistentVolumeBucketDir() throws Exception {\n+    String volumeNameLocal = getRandomNonExistVolumeName();\n+    String bucketNameLocal = \"bucket-\" + RandomStringUtils.randomNumeric(5);\n+    Path root = new Path(\"/\" + volumeNameLocal + \"/\" + bucketNameLocal);\n+    Path dir1 = new Path(root, \"dir1\");\n+    Path dir12 = new Path(dir1, \"dir12\");\n+    Path dir2 = new Path(root, \"dir2\");\n+    fs.mkdirs(dir12);\n+    fs.mkdirs(dir2);\n+\n+    // Check volume and bucket existence, they should both be created.\n+    OzoneVolume ozoneVolume = objectStore.getVolume(volumeNameLocal);\n+    OzoneBucket ozoneBucket = ozoneVolume.getBucket(bucketNameLocal);\n+    OFSPath ofsPathDir1 = new OFSPath(dir12);\n+    String key = ofsPathDir1.getKeyName() + \"/\";\n+    OzoneKeyDetails ozoneKeyDetails = ozoneBucket.getKey(key);\n+    assertEquals(key, ozoneKeyDetails.getName());\n+\n+    // Verify that directories are created.\n+    FileStatus[] fileStatuses = ofs.listStatus(root);\n+    assertEquals(fileStatuses[0].getPath().toUri().getPath(), dir1.toString());\n+    assertEquals(fileStatuses[1].getPath().toUri().getPath(), dir2.toString());\n+\n+    fileStatuses = ofs.listStatus(dir1);\n+    assertEquals(fileStatuses[0].getPath().toUri().getPath(), dir12.toString());\n+    fileStatuses = ofs.listStatus(dir12);\n+    assertEquals(fileStatuses.length, 0);\n+    fileStatuses = ofs.listStatus(dir2);\n+    assertEquals(fileStatuses.length, 0);\n+  }\n+\n+  /**\n+   * OFS: Tests mkdir on a volume and bucket that doesn't exist.\n+   */\n+  @Test\n+  public void testMkdirNonExistentVolumeBucket() throws Exception {\n+    String volumeNameLocal = getRandomNonExistVolumeName();\n+    String bucketNameLocal = \"bucket-\" + RandomStringUtils.randomNumeric(5);\n+    Path newVolBucket = new Path(\n+        \"/\" + volumeNameLocal + \"/\" + bucketNameLocal);\n+    fs.mkdirs(newVolBucket);\n+\n+    // Verify with listVolumes and listBuckets\n+    Iterator<? extends OzoneVolume> iterVol =\n+        objectStore.listVolumesByUser(null, volumeNameLocal, null);\n+    OzoneVolume ozoneVolume = iterVol.next();\n+    assertNotNull(ozoneVolume);\n+    assertEquals(volumeNameLocal, ozoneVolume.getName());\n+\n+    Iterator<? extends OzoneBucket> iterBuc =\n+        ozoneVolume.listBuckets(\"bucket-\");\n+    OzoneBucket ozoneBucket = iterBuc.next();\n+    assertNotNull(ozoneBucket);\n+    assertEquals(bucketNameLocal, ozoneBucket.getName());\n+\n+    // TODO: Use listStatus to check volume and bucket creation in HDDS-2928.\n+  }\n+\n+  /**\n+   * OFS: Tests mkdir on a volume that doesn't exist.\n+   */\n+  @Test\n+  public void testMkdirNonExistentVolume() throws Exception {\n+    String volumeNameLocal = getRandomNonExistVolumeName();\n+    Path newVolume = new Path(\"/\" + volumeNameLocal);\n+    fs.mkdirs(newVolume);\n+\n+    // Verify with listVolumes and listBuckets\n+    Iterator<? extends OzoneVolume> iterVol =\n+        objectStore.listVolumesByUser(null, volumeNameLocal, null);\n+    OzoneVolume ozoneVolume = iterVol.next();\n+    assertNotNull(ozoneVolume);\n+    assertEquals(volumeNameLocal, ozoneVolume.getName());\n+\n+    // TODO: Use listStatus to check volume and bucket creation in HDDS-2928.\n+  }\n+\n+  /**\n+   * Tests listStatus operation in a bucket.\n+   */\n+  @Test\n+  public void testListStatusOnRoot() throws Exception {\n+    Path root = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path dir1 = new Path(root, \"dir1\");\n+    Path dir12 = new Path(dir1, \"dir12\");\n+    Path dir2 = new Path(root, \"dir2\");\n+    fs.mkdirs(dir12);\n+    fs.mkdirs(dir2);\n+\n+    // ListStatus on root should return dir1 (even though /dir1 key does not\n+    // exist) and dir2 only. dir12 is not an immediate child of root and\n+    // hence should not be listed.\n+    FileStatus[] fileStatuses = ofs.listStatus(root);\n+    assertEquals(\"FileStatus should return only the immediate children\", 2,\n+        fileStatuses.length);\n+\n+    // Verify that dir12 is not included in the result of the listStatus on root\n+    String fileStatus1 = fileStatuses[0].getPath().toUri().getPath();\n+    String fileStatus2 = fileStatuses[1].getPath().toUri().getPath();\n+    assertFalse(fileStatus1.equals(dir12.toString()));\n+    assertFalse(fileStatus2.equals(dir12.toString()));\n+  }\n+\n+  /**\n+   * Tests listStatus operation on root directory.\n+   */\n+  @Test\n+  public void testListStatusOnLargeDirectory() throws Exception {\n+    Path root = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Set<String> paths = new TreeSet<>();\n+    int numDirs = 5111;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "780ebaf5b5279e921a9eed5a43db34027c9b1aa4"}, "originalPosition": 360}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3889, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}