{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzY5MTEzMzE1", "number": 508, "title": "HDDS-2950. Upgrade jetty to the latest 9.4 release", "bodyText": "What changes were proposed in this pull request?\nThe jetty which is used by web interfaces of Ozone is from the September of 2018.\nSince then many bug and security fixes added to the Jetty project. I would suggest to use the latest jetty (from January of 2020).\nAs HttpServer2 (hadoop 3.2 class) has a strong dependency on the older version of Jetty (it uses SessionManager which is removed), it seems to be easier to clone HttpServer2 and move it to the Ozone project.\nIt provides us the flexibility:\n\nTo upgrade jetty independent from the Hadoop releases\nTo remove unused features and make our HTTP stack more reliable (eg. Yarn, WebHDFS part of the code)\nTo customize our HTTP server usage (eg. SPNEGO should be ignore for S3 REST)\nTo support ozone style configuration and maintain all our configuration est\nTo add additional insights/metrics to our own HttpServer\nTo simplify the current server initialization (current BaseHttpServer class of hdds/ozone is a wrapper around the Hadoop class, but we can combine the two functionalities)\n\nWhat is included in this patch\n\nHttpServer2 and lightweight dependencies are cloned from Hadoop 3.2\nHADOOP-16152 (Jetty upgrade since 3.2 release)\nCheckstyle / findbugs (to follow our coding standards)\nAs we have >5 http related classes I moved them to a dedicated package in the framework project.\n**The main logic has not been modified by this patch ** This is mainly code organization and code import.\n\nWhat is the link to the Apache JIRA\nhttps://issues.apache.org/jira/browse/HDDS-2950\nHow was this patch tested?\nThere are related unit tests, but also checked with docker-compose based cluster + checking the ui pages from browser.", "createdAt": "2020-01-30T14:45:03Z", "url": "https://github.com/apache/ozone/pull/508", "merged": true, "mergeCommit": {"oid": "1ac8263405e5e4d3a12ceea1efcba40209547705"}, "closed": true, "closedAt": "2020-02-07T21:22:30Z", "author": {"login": "elek"}, "timelineItems": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABb-wLtaAH2gAyMzY5MTEzMzE1OjAwOTA2NmY3YzA4MWY3ZmE3N2EwOGEwMTc3ZmQ3ZDc2ZDcxZjE1NDA=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcB8Fg6AH2gAyMzY5MTEzMzE1OmFlYmUwMjgzNDMxODFlZDlkYWZhNGYyYWNiZGJjZDBkMmVlNjgyMWI=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "009066f7c081f7fa77a08a0177fd7d76d71f1540", "author": {"user": {"login": "elek", "name": "Elek, M\u00e1rton"}}, "url": "https://github.com/apache/ozone/commit/009066f7c081f7fa77a08a0177fd7d76d71f1540", "committedDate": "2020-01-28T12:06:28Z", "message": "HDDS-2950. Upgrade jetty to the latest 9.4 release"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "39b27423bc4eed088ad760d0a0a6180fc591b4de", "author": {"user": {"login": "elek", "name": "Elek, M\u00e1rton"}}, "url": "https://github.com/apache/ozone/commit/39b27423bc4eed088ad760d0a0a6180fc591b4de", "committedDate": "2020-01-28T13:05:43Z", "message": "fix findbugs error"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "48175b715194d47cbb435b0b40879e1a9e84d5d2", "author": {"user": {"login": "elek", "name": "Elek, M\u00e1rton"}}, "url": "https://github.com/apache/ozone/commit/48175b715194d47cbb435b0b40879e1a9e84d5d2", "committedDate": "2020-01-28T13:44:05Z", "message": "disable spnego filter for s3g"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ba3b4ea8bd8ee76625859bbfc5ca9f2d0e07f749", "author": {"user": {"login": "elek", "name": "Elek, M\u00e1rton"}}, "url": "https://github.com/apache/ozone/commit/ba3b4ea8bd8ee76625859bbfc5ca9f2d0e07f749", "committedDate": "2020-01-29T09:46:58Z", "message": "fix checkstyle + server filter initialization"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ef0d720e5b97f2a54c639ef2fddb976f137cd456", "author": {"user": {"login": "elek", "name": "Elek, M\u00e1rton"}}, "url": "https://github.com/apache/ozone/commit/ef0d720e5b97f2a54c639ef2fddb976f137cd456", "committedDate": "2020-01-30T13:10:56Z", "message": "Import proper 3.2 version from HttpServer2 and dependencies + fix all the checkstyle / findbug errors"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUxMTIxMTc1", "url": "https://github.com/apache/ozone/pull/508#pullrequestreview-351121175", "createdAt": "2020-01-30T20:31:03Z", "commit": {"oid": "ef0d720e5b97f2a54c639ef2fddb976f137cd456"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQyMDozMTowNFrOFj4-Vg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQyMDozNzoxMlrOFj5Ijw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzE3Nzk0Mg==", "bodyText": "This can be changed to private as well?", "url": "https://github.com/apache/ozone/pull/508#discussion_r373177942", "createdAt": "2020-01-30T20:31:04Z", "author": {"login": "smengcl"}, "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/http/HttpServer2.java", "diffHunk": "@@ -0,0 +1,1701 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdds.server.http;\n+\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InterruptedIOException;\n+import java.io.PrintStream;\n+import java.net.BindException;\n+import java.net.InetSocketAddress;\n+import java.net.MalformedURLException;\n+import java.net.URI;\n+import java.net.URL;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import javax.servlet.Filter;\n+import javax.servlet.FilterChain;\n+import javax.servlet.FilterConfig;\n+import javax.servlet.ServletContext;\n+import javax.servlet.ServletException;\n+import javax.servlet.ServletRequest;\n+import javax.servlet.ServletResponse;\n+import javax.servlet.http.HttpServlet;\n+import javax.servlet.http.HttpServletRequest;\n+import javax.servlet.http.HttpServletRequestWrapper;\n+import javax.servlet.http.HttpServletResponse;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import com.sun.jersey.spi.container.servlet.ServletContainer;\n+import org.apache.hadoop.HadoopIllegalArgumentException;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.ConfServlet;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.conf.Configuration.IntegerRanges;\n+import org.apache.hadoop.fs.CommonConfigurationKeys;\n+import org.apache.hadoop.jmx.JMXJsonServlet;\n+import org.apache.hadoop.log.LogLevel;\n+import org.apache.hadoop.security.AuthenticationFilterInitializer;\n+import org.apache.hadoop.security.SecurityUtil;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.security.authentication.server.AuthenticationFilter;\n+import org.apache.hadoop.security.authentication.util.SignerSecretProvider;\n+import org.apache.hadoop.security.authorize.AccessControlList;\n+import org.apache.hadoop.security.ssl.SSLFactory;\n+import org.apache.hadoop.util.ReflectionUtils;\n+import org.apache.hadoop.util.Shell;\n+import org.apache.hadoop.util.StringUtils;\n+import org.eclipse.jetty.http.HttpVersion;\n+import org.eclipse.jetty.server.ConnectionFactory;\n+import org.eclipse.jetty.server.Connector;\n+import org.eclipse.jetty.server.Handler;\n+import org.eclipse.jetty.server.HttpConfiguration;\n+import org.eclipse.jetty.server.HttpConnectionFactory;\n+import org.eclipse.jetty.server.RequestLog;\n+import org.eclipse.jetty.server.SecureRequestCustomizer;\n+import org.eclipse.jetty.server.Server;\n+import org.eclipse.jetty.server.ServerConnector;\n+import org.eclipse.jetty.server.SslConnectionFactory;\n+import org.eclipse.jetty.server.handler.ContextHandlerCollection;\n+import org.eclipse.jetty.server.handler.HandlerCollection;\n+import org.eclipse.jetty.server.handler.RequestLogHandler;\n+import org.eclipse.jetty.server.session.SessionHandler;\n+import org.eclipse.jetty.servlet.DefaultServlet;\n+import org.eclipse.jetty.servlet.FilterHolder;\n+import org.eclipse.jetty.servlet.FilterMapping;\n+import org.eclipse.jetty.servlet.ServletContextHandler;\n+import org.eclipse.jetty.servlet.ServletHandler;\n+import org.eclipse.jetty.servlet.ServletHolder;\n+import org.eclipse.jetty.servlet.ServletMapping;\n+import org.eclipse.jetty.util.ArrayUtil;\n+import org.eclipse.jetty.util.MultiException;\n+import org.eclipse.jetty.util.ssl.SslContextFactory;\n+import org.eclipse.jetty.util.thread.QueuedThreadPool;\n+import org.eclipse.jetty.webapp.WebAppContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Create a Jetty embedded server to answer http requests. The primary goal is\n+ * to serve up status information for the server. There are three contexts:\n+ * \"/logs/\" -> points to the log directory \"/static/\" -> points to common static\n+ * files (src/webapps/static) \"/\" -> the jsp server code from\n+ * (src/webapps/<name>)\n+ *\n+ * This class is a fork of the old HttpServer. HttpServer exists for\n+ * compatibility reasons. See HBASE-10336 for more details.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Evolving\n+public final class HttpServer2 implements FilterContainer {\n+  public static final Logger LOG = LoggerFactory.getLogger(HttpServer2.class);\n+\n+  private static final String HTTP_SCHEME = \"http\";\n+  public static final String HTTPS_SCHEME = \"https\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0d720e5b97f2a54c639ef2fddb976f137cd456"}, "originalPosition": 121}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzE4MDA2Nw==", "bodyText": "hadoop.http.sni.host.check.enabled is added in HADOOP-16718 in trunk. We might want it here as well?", "url": "https://github.com/apache/ozone/pull/508#discussion_r373180067", "createdAt": "2020-01-30T20:36:01Z", "author": {"login": "smengcl"}, "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/http/HttpServer2.java", "diffHunk": "@@ -0,0 +1,1701 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdds.server.http;\n+\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InterruptedIOException;\n+import java.io.PrintStream;\n+import java.net.BindException;\n+import java.net.InetSocketAddress;\n+import java.net.MalformedURLException;\n+import java.net.URI;\n+import java.net.URL;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import javax.servlet.Filter;\n+import javax.servlet.FilterChain;\n+import javax.servlet.FilterConfig;\n+import javax.servlet.ServletContext;\n+import javax.servlet.ServletException;\n+import javax.servlet.ServletRequest;\n+import javax.servlet.ServletResponse;\n+import javax.servlet.http.HttpServlet;\n+import javax.servlet.http.HttpServletRequest;\n+import javax.servlet.http.HttpServletRequestWrapper;\n+import javax.servlet.http.HttpServletResponse;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import com.sun.jersey.spi.container.servlet.ServletContainer;\n+import org.apache.hadoop.HadoopIllegalArgumentException;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.ConfServlet;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.conf.Configuration.IntegerRanges;\n+import org.apache.hadoop.fs.CommonConfigurationKeys;\n+import org.apache.hadoop.jmx.JMXJsonServlet;\n+import org.apache.hadoop.log.LogLevel;\n+import org.apache.hadoop.security.AuthenticationFilterInitializer;\n+import org.apache.hadoop.security.SecurityUtil;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.security.authentication.server.AuthenticationFilter;\n+import org.apache.hadoop.security.authentication.util.SignerSecretProvider;\n+import org.apache.hadoop.security.authorize.AccessControlList;\n+import org.apache.hadoop.security.ssl.SSLFactory;\n+import org.apache.hadoop.util.ReflectionUtils;\n+import org.apache.hadoop.util.Shell;\n+import org.apache.hadoop.util.StringUtils;\n+import org.eclipse.jetty.http.HttpVersion;\n+import org.eclipse.jetty.server.ConnectionFactory;\n+import org.eclipse.jetty.server.Connector;\n+import org.eclipse.jetty.server.Handler;\n+import org.eclipse.jetty.server.HttpConfiguration;\n+import org.eclipse.jetty.server.HttpConnectionFactory;\n+import org.eclipse.jetty.server.RequestLog;\n+import org.eclipse.jetty.server.SecureRequestCustomizer;\n+import org.eclipse.jetty.server.Server;\n+import org.eclipse.jetty.server.ServerConnector;\n+import org.eclipse.jetty.server.SslConnectionFactory;\n+import org.eclipse.jetty.server.handler.ContextHandlerCollection;\n+import org.eclipse.jetty.server.handler.HandlerCollection;\n+import org.eclipse.jetty.server.handler.RequestLogHandler;\n+import org.eclipse.jetty.server.session.SessionHandler;\n+import org.eclipse.jetty.servlet.DefaultServlet;\n+import org.eclipse.jetty.servlet.FilterHolder;\n+import org.eclipse.jetty.servlet.FilterMapping;\n+import org.eclipse.jetty.servlet.ServletContextHandler;\n+import org.eclipse.jetty.servlet.ServletHandler;\n+import org.eclipse.jetty.servlet.ServletHolder;\n+import org.eclipse.jetty.servlet.ServletMapping;\n+import org.eclipse.jetty.util.ArrayUtil;\n+import org.eclipse.jetty.util.MultiException;\n+import org.eclipse.jetty.util.ssl.SslContextFactory;\n+import org.eclipse.jetty.util.thread.QueuedThreadPool;\n+import org.eclipse.jetty.webapp.WebAppContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Create a Jetty embedded server to answer http requests. The primary goal is\n+ * to serve up status information for the server. There are three contexts:\n+ * \"/logs/\" -> points to the log directory \"/static/\" -> points to common static\n+ * files (src/webapps/static) \"/\" -> the jsp server code from\n+ * (src/webapps/<name>)\n+ *\n+ * This class is a fork of the old HttpServer. HttpServer exists for\n+ * compatibility reasons. See HBASE-10336 for more details.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Evolving\n+public final class HttpServer2 implements FilterContainer {\n+  public static final Logger LOG = LoggerFactory.getLogger(HttpServer2.class);\n+\n+  private static final String HTTP_SCHEME = \"http\";\n+  public static final String HTTPS_SCHEME = \"https\";\n+\n+  public static final String HTTP_MAX_REQUEST_HEADER_SIZE_KEY =\n+      \"hadoop.http.max.request.header.size\";\n+  public static final int HTTP_MAX_REQUEST_HEADER_SIZE_DEFAULT = 65536;\n+  public static final String HTTP_MAX_RESPONSE_HEADER_SIZE_KEY =\n+      \"hadoop.http.max.response.header.size\";\n+  public static final int HTTP_MAX_RESPONSE_HEADER_SIZE_DEFAULT = 65536;\n+\n+  public static final String HTTP_SOCKET_BACKLOG_SIZE_KEY =\n+      \"hadoop.http.socket.backlog.size\";\n+  public static final int HTTP_SOCKET_BACKLOG_SIZE_DEFAULT = 128;\n+  public static final String HTTP_MAX_THREADS_KEY = \"hadoop.http.max.threads\";\n+  public static final String HTTP_ACCEPTOR_COUNT_KEY =\n+      \"hadoop.http.acceptor.count\";\n+  // -1 to use default behavior of setting count based on CPU core count\n+  public static final int HTTP_ACCEPTOR_COUNT_DEFAULT = -1;\n+  public static final String HTTP_SELECTOR_COUNT_KEY =\n+      \"hadoop.http.selector.count\";\n+  // -1 to use default behavior of setting count based on CPU core count\n+  public static final int HTTP_SELECTOR_COUNT_DEFAULT = -1;\n+  // idle timeout in milliseconds\n+  public static final String HTTP_IDLE_TIMEOUT_MS_KEY =\n+      \"hadoop.http.idle_timeout.ms\";\n+  public static final int HTTP_IDLE_TIMEOUT_MS_DEFAULT = 10000;\n+  public static final String HTTP_TEMP_DIR_KEY = \"hadoop.http.temp.dir\";\n+\n+  public static final String FILTER_INITIALIZER_PROPERTY\n+      = \"ozone.http.filter.initializers\";\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0d720e5b97f2a54c639ef2fddb976f137cd456"}, "originalPosition": 150}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzE4MDU1OQ==", "bodyText": "I see .clone() is added here. For safe practice?", "url": "https://github.com/apache/ozone/pull/508#discussion_r373180559", "createdAt": "2020-01-30T20:37:12Z", "author": {"login": "smengcl"}, "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/http/HttpServer2.java", "diffHunk": "@@ -0,0 +1,1701 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdds.server.http;\n+\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InterruptedIOException;\n+import java.io.PrintStream;\n+import java.net.BindException;\n+import java.net.InetSocketAddress;\n+import java.net.MalformedURLException;\n+import java.net.URI;\n+import java.net.URL;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import javax.servlet.Filter;\n+import javax.servlet.FilterChain;\n+import javax.servlet.FilterConfig;\n+import javax.servlet.ServletContext;\n+import javax.servlet.ServletException;\n+import javax.servlet.ServletRequest;\n+import javax.servlet.ServletResponse;\n+import javax.servlet.http.HttpServlet;\n+import javax.servlet.http.HttpServletRequest;\n+import javax.servlet.http.HttpServletRequestWrapper;\n+import javax.servlet.http.HttpServletResponse;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import com.sun.jersey.spi.container.servlet.ServletContainer;\n+import org.apache.hadoop.HadoopIllegalArgumentException;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.ConfServlet;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.conf.Configuration.IntegerRanges;\n+import org.apache.hadoop.fs.CommonConfigurationKeys;\n+import org.apache.hadoop.jmx.JMXJsonServlet;\n+import org.apache.hadoop.log.LogLevel;\n+import org.apache.hadoop.security.AuthenticationFilterInitializer;\n+import org.apache.hadoop.security.SecurityUtil;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.security.authentication.server.AuthenticationFilter;\n+import org.apache.hadoop.security.authentication.util.SignerSecretProvider;\n+import org.apache.hadoop.security.authorize.AccessControlList;\n+import org.apache.hadoop.security.ssl.SSLFactory;\n+import org.apache.hadoop.util.ReflectionUtils;\n+import org.apache.hadoop.util.Shell;\n+import org.apache.hadoop.util.StringUtils;\n+import org.eclipse.jetty.http.HttpVersion;\n+import org.eclipse.jetty.server.ConnectionFactory;\n+import org.eclipse.jetty.server.Connector;\n+import org.eclipse.jetty.server.Handler;\n+import org.eclipse.jetty.server.HttpConfiguration;\n+import org.eclipse.jetty.server.HttpConnectionFactory;\n+import org.eclipse.jetty.server.RequestLog;\n+import org.eclipse.jetty.server.SecureRequestCustomizer;\n+import org.eclipse.jetty.server.Server;\n+import org.eclipse.jetty.server.ServerConnector;\n+import org.eclipse.jetty.server.SslConnectionFactory;\n+import org.eclipse.jetty.server.handler.ContextHandlerCollection;\n+import org.eclipse.jetty.server.handler.HandlerCollection;\n+import org.eclipse.jetty.server.handler.RequestLogHandler;\n+import org.eclipse.jetty.server.session.SessionHandler;\n+import org.eclipse.jetty.servlet.DefaultServlet;\n+import org.eclipse.jetty.servlet.FilterHolder;\n+import org.eclipse.jetty.servlet.FilterMapping;\n+import org.eclipse.jetty.servlet.ServletContextHandler;\n+import org.eclipse.jetty.servlet.ServletHandler;\n+import org.eclipse.jetty.servlet.ServletHolder;\n+import org.eclipse.jetty.servlet.ServletMapping;\n+import org.eclipse.jetty.util.ArrayUtil;\n+import org.eclipse.jetty.util.MultiException;\n+import org.eclipse.jetty.util.ssl.SslContextFactory;\n+import org.eclipse.jetty.util.thread.QueuedThreadPool;\n+import org.eclipse.jetty.webapp.WebAppContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Create a Jetty embedded server to answer http requests. The primary goal is\n+ * to serve up status information for the server. There are three contexts:\n+ * \"/logs/\" -> points to the log directory \"/static/\" -> points to common static\n+ * files (src/webapps/static) \"/\" -> the jsp server code from\n+ * (src/webapps/<name>)\n+ *\n+ * This class is a fork of the old HttpServer. HttpServer exists for\n+ * compatibility reasons. See HBASE-10336 for more details.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Evolving\n+public final class HttpServer2 implements FilterContainer {\n+  public static final Logger LOG = LoggerFactory.getLogger(HttpServer2.class);\n+\n+  private static final String HTTP_SCHEME = \"http\";\n+  public static final String HTTPS_SCHEME = \"https\";\n+\n+  public static final String HTTP_MAX_REQUEST_HEADER_SIZE_KEY =\n+      \"hadoop.http.max.request.header.size\";\n+  public static final int HTTP_MAX_REQUEST_HEADER_SIZE_DEFAULT = 65536;\n+  public static final String HTTP_MAX_RESPONSE_HEADER_SIZE_KEY =\n+      \"hadoop.http.max.response.header.size\";\n+  public static final int HTTP_MAX_RESPONSE_HEADER_SIZE_DEFAULT = 65536;\n+\n+  public static final String HTTP_SOCKET_BACKLOG_SIZE_KEY =\n+      \"hadoop.http.socket.backlog.size\";\n+  public static final int HTTP_SOCKET_BACKLOG_SIZE_DEFAULT = 128;\n+  public static final String HTTP_MAX_THREADS_KEY = \"hadoop.http.max.threads\";\n+  public static final String HTTP_ACCEPTOR_COUNT_KEY =\n+      \"hadoop.http.acceptor.count\";\n+  // -1 to use default behavior of setting count based on CPU core count\n+  public static final int HTTP_ACCEPTOR_COUNT_DEFAULT = -1;\n+  public static final String HTTP_SELECTOR_COUNT_KEY =\n+      \"hadoop.http.selector.count\";\n+  // -1 to use default behavior of setting count based on CPU core count\n+  public static final int HTTP_SELECTOR_COUNT_DEFAULT = -1;\n+  // idle timeout in milliseconds\n+  public static final String HTTP_IDLE_TIMEOUT_MS_KEY =\n+      \"hadoop.http.idle_timeout.ms\";\n+  public static final int HTTP_IDLE_TIMEOUT_MS_DEFAULT = 10000;\n+  public static final String HTTP_TEMP_DIR_KEY = \"hadoop.http.temp.dir\";\n+\n+  public static final String FILTER_INITIALIZER_PROPERTY\n+      = \"ozone.http.filter.initializers\";\n+\n+  // The ServletContext attribute where the daemon Configuration\n+  // gets stored.\n+  public static final String CONF_CONTEXT_ATTRIBUTE = \"hadoop.conf\";\n+  public static final String ADMINS_ACL = \"admins.acl\";\n+  public static final String SPNEGO_FILTER = \"SpnegoFilter\";\n+  public static final String NO_CACHE_FILTER = \"NoCacheFilter\";\n+\n+  public static final String BIND_ADDRESS = \"bind.address\";\n+\n+  private final AccessControlList adminsAcl;\n+\n+  private final Server webServer;\n+\n+  private final HandlerCollection handlers;\n+\n+  private final List<ServerConnector> listeners = Lists.newArrayList();\n+\n+  private final WebAppContext webAppContext;\n+  private final boolean findPort;\n+  private final IntegerRanges portRanges;\n+  private final Map<ServletContextHandler, Boolean> defaultContexts =\n+      new HashMap<>();\n+  private final List<String> filterNames = new ArrayList<>();\n+  static final String STATE_DESCRIPTION_ALIVE = \" - alive\";\n+  static final String STATE_DESCRIPTION_NOT_LIVE = \" - not live\";\n+  private final SignerSecretProvider secretProvider;\n+  private XFrameOption xFrameOption;\n+  private boolean xFrameOptionIsEnabled;\n+  public static final String HTTP_HEADER_PREFIX = \"hadoop.http.header.\";\n+  private static final String HTTP_HEADER_REGEX =\n+      \"hadoop\\\\.http\\\\.header\\\\.([a-zA-Z\\\\-_]+)\";\n+  static final String X_XSS_PROTECTION =\n+      \"X-XSS-Protection:1; mode=block\";\n+  static final String X_CONTENT_TYPE_OPTIONS =\n+      \"X-Content-Type-Options:nosniff\";\n+  private static final String X_FRAME_OPTIONS = \"X-FRAME-OPTIONS\";\n+  private static final Pattern PATTERN_HTTP_HEADER_REGEX =\n+      Pattern.compile(HTTP_HEADER_REGEX);\n+  /**\n+   * Class to construct instances of HTTP server with specific options.\n+   */\n+  public static class Builder {\n+    private ArrayList<URI> endpoints = Lists.newArrayList();\n+    private String name;\n+    private Configuration conf;\n+    private Configuration sslConf;\n+    private String[] pathSpecs;\n+    private AccessControlList adminsAcl;\n+    private boolean securityEnabled = false;\n+    private String usernameConfKey;\n+    private String keytabConfKey;\n+    private boolean needsClientAuth;\n+    private String trustStore;\n+    private String trustStorePassword;\n+    private String trustStoreType;\n+\n+    private String keyStore;\n+    private String keyStorePassword;\n+    private String keyStoreType;\n+\n+    // The -keypass option in keytool\n+    private String keyPassword;\n+\n+    private boolean findPort;\n+    private IntegerRanges portRanges = null;\n+\n+    private String hostName;\n+    private boolean disallowFallbackToRandomSignerSecretProvider;\n+    private String authFilterConfigurationPrefix =\n+        \"hadoop.http.authentication.\";\n+    private String excludeCiphers;\n+\n+    private boolean xFrameEnabled;\n+    private XFrameOption xFrameOption = XFrameOption.SAMEORIGIN;\n+\n+    public Builder setName(String serverName) {\n+      this.name = serverName;\n+      return this;\n+    }\n+\n+    /**\n+     * Add an endpoint that the HTTP server should listen to.\n+     *\n+     * @param endpoint\n+     *          the endpoint of that the HTTP server should listen to. The\n+     *          scheme specifies the protocol (i.e. HTTP / HTTPS), the host\n+     *          specifies the binding address, and the port specifies the\n+     *          listening port. Unspecified or zero port means that the server\n+     *          can listen to any port.\n+     */\n+    public Builder addEndpoint(URI endpoint) {\n+      endpoints.add(endpoint);\n+      return this;\n+    }\n+\n+    /**\n+     * Set the hostname of the http server. The host name is used to resolve the\n+     * _HOST field in Kerberos principals. The hostname of the first listener\n+     * will be used if the name is unspecified.\n+     */\n+    public Builder hostName(String host) {\n+      this.hostName = host;\n+      return this;\n+    }\n+\n+    public Builder trustStore(String location, String password, String type) {\n+      this.trustStore = location;\n+      this.trustStorePassword = password;\n+      this.trustStoreType = type;\n+      return this;\n+    }\n+\n+    public Builder keyStore(String location, String password, String type) {\n+      this.keyStore = location;\n+      this.keyStorePassword = password;\n+      this.keyStoreType = type;\n+      return this;\n+    }\n+\n+    public Builder keyPassword(String password) {\n+      this.keyPassword = password;\n+      return this;\n+    }\n+\n+    /**\n+     * Specify whether the server should authorize the client in SSL\n+     * connections.\n+     */\n+    public Builder needsClientAuth(boolean value) {\n+      this.needsClientAuth = value;\n+      return this;\n+    }\n+\n+    public Builder setFindPort(boolean portFind) {\n+      this.findPort = portFind;\n+      return this;\n+    }\n+\n+    public Builder setPortRanges(IntegerRanges ranges) {\n+      this.portRanges = ranges;\n+      return this;\n+    }\n+\n+    public Builder setConf(Configuration configuration) {\n+      this.conf = configuration;\n+      return this;\n+    }\n+\n+    /**\n+     * Specify the SSL configuration to load. This API provides an alternative\n+     * to keyStore/keyPassword/trustStore.\n+     */\n+    public Builder setSSLConf(Configuration sslCnf) {\n+      this.sslConf = sslCnf;\n+      return this;\n+    }\n+\n+    public Builder setPathSpec(String[] pathSpec) {\n+      this.pathSpecs = pathSpec.clone();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0d720e5b97f2a54c639ef2fddb976f137cd456"}, "originalPosition": 309}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8e9db69e2230cd89743344fc30cf30fe17309a32", "author": {"user": {"login": "elek", "name": "Elek, M\u00e1rton"}}, "url": "https://github.com/apache/ozone/commit/8e9db69e2230cd89743344fc30cf30fe17309a32", "committedDate": "2020-02-03T09:26:24Z", "message": "importing null checks from HADOOP-16727."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1f98b9bcf29868924ed4b550a58807217d43c05b", "author": {"user": {"login": "elek", "name": "Elek, M\u00e1rton"}}, "url": "https://github.com/apache/ozone/commit/1f98b9bcf29868924ed4b550a58807217d43c05b", "committedDate": "2020-02-03T09:27:32Z", "message": "make private constants private"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU0ODU0NDA3", "url": "https://github.com/apache/ozone/pull/508#pullrequestreview-354854407", "createdAt": "2020-02-06T23:55:17Z", "commit": {"oid": "1f98b9bcf29868924ed4b550a58807217d43c05b"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "aebe028343181ed9dafa4f2acbdbcd0d2ee6821b", "author": {"user": {"login": "elek", "name": "Elek, M\u00e1rton"}}, "url": "https://github.com/apache/ozone/commit/aebe028343181ed9dafa4f2acbdbcd0d2ee6821b", "committedDate": "2020-02-07T09:40:20Z", "message": "Merge remote-tracking branch 'origin/master' into HDDS-2950"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3795, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}