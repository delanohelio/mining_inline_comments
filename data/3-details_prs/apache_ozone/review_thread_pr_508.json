{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzY5MTEzMzE1", "number": 508, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQyMDozMTowNFrODcDQ8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQyMDozNzoxMlrODcDXeA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwNzQwMjExOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/http/HttpServer2.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQyMDozMTowNFrOFj4-Vg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QwOToyNzozOVrOFkq48A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzE3Nzk0Mg==", "bodyText": "This can be changed to private as well?", "url": "https://github.com/apache/ozone/pull/508#discussion_r373177942", "createdAt": "2020-01-30T20:31:04Z", "author": {"login": "smengcl"}, "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/http/HttpServer2.java", "diffHunk": "@@ -0,0 +1,1701 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdds.server.http;\n+\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InterruptedIOException;\n+import java.io.PrintStream;\n+import java.net.BindException;\n+import java.net.InetSocketAddress;\n+import java.net.MalformedURLException;\n+import java.net.URI;\n+import java.net.URL;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import javax.servlet.Filter;\n+import javax.servlet.FilterChain;\n+import javax.servlet.FilterConfig;\n+import javax.servlet.ServletContext;\n+import javax.servlet.ServletException;\n+import javax.servlet.ServletRequest;\n+import javax.servlet.ServletResponse;\n+import javax.servlet.http.HttpServlet;\n+import javax.servlet.http.HttpServletRequest;\n+import javax.servlet.http.HttpServletRequestWrapper;\n+import javax.servlet.http.HttpServletResponse;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import com.sun.jersey.spi.container.servlet.ServletContainer;\n+import org.apache.hadoop.HadoopIllegalArgumentException;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.ConfServlet;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.conf.Configuration.IntegerRanges;\n+import org.apache.hadoop.fs.CommonConfigurationKeys;\n+import org.apache.hadoop.jmx.JMXJsonServlet;\n+import org.apache.hadoop.log.LogLevel;\n+import org.apache.hadoop.security.AuthenticationFilterInitializer;\n+import org.apache.hadoop.security.SecurityUtil;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.security.authentication.server.AuthenticationFilter;\n+import org.apache.hadoop.security.authentication.util.SignerSecretProvider;\n+import org.apache.hadoop.security.authorize.AccessControlList;\n+import org.apache.hadoop.security.ssl.SSLFactory;\n+import org.apache.hadoop.util.ReflectionUtils;\n+import org.apache.hadoop.util.Shell;\n+import org.apache.hadoop.util.StringUtils;\n+import org.eclipse.jetty.http.HttpVersion;\n+import org.eclipse.jetty.server.ConnectionFactory;\n+import org.eclipse.jetty.server.Connector;\n+import org.eclipse.jetty.server.Handler;\n+import org.eclipse.jetty.server.HttpConfiguration;\n+import org.eclipse.jetty.server.HttpConnectionFactory;\n+import org.eclipse.jetty.server.RequestLog;\n+import org.eclipse.jetty.server.SecureRequestCustomizer;\n+import org.eclipse.jetty.server.Server;\n+import org.eclipse.jetty.server.ServerConnector;\n+import org.eclipse.jetty.server.SslConnectionFactory;\n+import org.eclipse.jetty.server.handler.ContextHandlerCollection;\n+import org.eclipse.jetty.server.handler.HandlerCollection;\n+import org.eclipse.jetty.server.handler.RequestLogHandler;\n+import org.eclipse.jetty.server.session.SessionHandler;\n+import org.eclipse.jetty.servlet.DefaultServlet;\n+import org.eclipse.jetty.servlet.FilterHolder;\n+import org.eclipse.jetty.servlet.FilterMapping;\n+import org.eclipse.jetty.servlet.ServletContextHandler;\n+import org.eclipse.jetty.servlet.ServletHandler;\n+import org.eclipse.jetty.servlet.ServletHolder;\n+import org.eclipse.jetty.servlet.ServletMapping;\n+import org.eclipse.jetty.util.ArrayUtil;\n+import org.eclipse.jetty.util.MultiException;\n+import org.eclipse.jetty.util.ssl.SslContextFactory;\n+import org.eclipse.jetty.util.thread.QueuedThreadPool;\n+import org.eclipse.jetty.webapp.WebAppContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Create a Jetty embedded server to answer http requests. The primary goal is\n+ * to serve up status information for the server. There are three contexts:\n+ * \"/logs/\" -> points to the log directory \"/static/\" -> points to common static\n+ * files (src/webapps/static) \"/\" -> the jsp server code from\n+ * (src/webapps/<name>)\n+ *\n+ * This class is a fork of the old HttpServer. HttpServer exists for\n+ * compatibility reasons. See HBASE-10336 for more details.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Evolving\n+public final class HttpServer2 implements FilterContainer {\n+  public static final Logger LOG = LoggerFactory.getLogger(HttpServer2.class);\n+\n+  private static final String HTTP_SCHEME = \"http\";\n+  public static final String HTTPS_SCHEME = \"https\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0d720e5b97f2a54c639ef2fddb976f137cd456"}, "originalPosition": 121}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzk5NTc2MA==", "bodyText": "sure, done.", "url": "https://github.com/apache/ozone/pull/508#discussion_r373995760", "createdAt": "2020-02-03T09:27:39Z", "author": {"login": "elek"}, "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/http/HttpServer2.java", "diffHunk": "@@ -0,0 +1,1701 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdds.server.http;\n+\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InterruptedIOException;\n+import java.io.PrintStream;\n+import java.net.BindException;\n+import java.net.InetSocketAddress;\n+import java.net.MalformedURLException;\n+import java.net.URI;\n+import java.net.URL;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import javax.servlet.Filter;\n+import javax.servlet.FilterChain;\n+import javax.servlet.FilterConfig;\n+import javax.servlet.ServletContext;\n+import javax.servlet.ServletException;\n+import javax.servlet.ServletRequest;\n+import javax.servlet.ServletResponse;\n+import javax.servlet.http.HttpServlet;\n+import javax.servlet.http.HttpServletRequest;\n+import javax.servlet.http.HttpServletRequestWrapper;\n+import javax.servlet.http.HttpServletResponse;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import com.sun.jersey.spi.container.servlet.ServletContainer;\n+import org.apache.hadoop.HadoopIllegalArgumentException;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.ConfServlet;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.conf.Configuration.IntegerRanges;\n+import org.apache.hadoop.fs.CommonConfigurationKeys;\n+import org.apache.hadoop.jmx.JMXJsonServlet;\n+import org.apache.hadoop.log.LogLevel;\n+import org.apache.hadoop.security.AuthenticationFilterInitializer;\n+import org.apache.hadoop.security.SecurityUtil;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.security.authentication.server.AuthenticationFilter;\n+import org.apache.hadoop.security.authentication.util.SignerSecretProvider;\n+import org.apache.hadoop.security.authorize.AccessControlList;\n+import org.apache.hadoop.security.ssl.SSLFactory;\n+import org.apache.hadoop.util.ReflectionUtils;\n+import org.apache.hadoop.util.Shell;\n+import org.apache.hadoop.util.StringUtils;\n+import org.eclipse.jetty.http.HttpVersion;\n+import org.eclipse.jetty.server.ConnectionFactory;\n+import org.eclipse.jetty.server.Connector;\n+import org.eclipse.jetty.server.Handler;\n+import org.eclipse.jetty.server.HttpConfiguration;\n+import org.eclipse.jetty.server.HttpConnectionFactory;\n+import org.eclipse.jetty.server.RequestLog;\n+import org.eclipse.jetty.server.SecureRequestCustomizer;\n+import org.eclipse.jetty.server.Server;\n+import org.eclipse.jetty.server.ServerConnector;\n+import org.eclipse.jetty.server.SslConnectionFactory;\n+import org.eclipse.jetty.server.handler.ContextHandlerCollection;\n+import org.eclipse.jetty.server.handler.HandlerCollection;\n+import org.eclipse.jetty.server.handler.RequestLogHandler;\n+import org.eclipse.jetty.server.session.SessionHandler;\n+import org.eclipse.jetty.servlet.DefaultServlet;\n+import org.eclipse.jetty.servlet.FilterHolder;\n+import org.eclipse.jetty.servlet.FilterMapping;\n+import org.eclipse.jetty.servlet.ServletContextHandler;\n+import org.eclipse.jetty.servlet.ServletHandler;\n+import org.eclipse.jetty.servlet.ServletHolder;\n+import org.eclipse.jetty.servlet.ServletMapping;\n+import org.eclipse.jetty.util.ArrayUtil;\n+import org.eclipse.jetty.util.MultiException;\n+import org.eclipse.jetty.util.ssl.SslContextFactory;\n+import org.eclipse.jetty.util.thread.QueuedThreadPool;\n+import org.eclipse.jetty.webapp.WebAppContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Create a Jetty embedded server to answer http requests. The primary goal is\n+ * to serve up status information for the server. There are three contexts:\n+ * \"/logs/\" -> points to the log directory \"/static/\" -> points to common static\n+ * files (src/webapps/static) \"/\" -> the jsp server code from\n+ * (src/webapps/<name>)\n+ *\n+ * This class is a fork of the old HttpServer. HttpServer exists for\n+ * compatibility reasons. See HBASE-10336 for more details.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Evolving\n+public final class HttpServer2 implements FilterContainer {\n+  public static final Logger LOG = LoggerFactory.getLogger(HttpServer2.class);\n+\n+  private static final String HTTP_SCHEME = \"http\";\n+  public static final String HTTPS_SCHEME = \"https\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzE3Nzk0Mg=="}, "originalCommit": {"oid": "ef0d720e5b97f2a54c639ef2fddb976f137cd456"}, "originalPosition": 121}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwNzQxNTgyOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/http/HttpServer2.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQyMDozNjowMVrOFj5Gow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQyMDozNjowMVrOFj5Gow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzE4MDA2Nw==", "bodyText": "hadoop.http.sni.host.check.enabled is added in HADOOP-16718 in trunk. We might want it here as well?", "url": "https://github.com/apache/ozone/pull/508#discussion_r373180067", "createdAt": "2020-01-30T20:36:01Z", "author": {"login": "smengcl"}, "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/http/HttpServer2.java", "diffHunk": "@@ -0,0 +1,1701 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdds.server.http;\n+\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InterruptedIOException;\n+import java.io.PrintStream;\n+import java.net.BindException;\n+import java.net.InetSocketAddress;\n+import java.net.MalformedURLException;\n+import java.net.URI;\n+import java.net.URL;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import javax.servlet.Filter;\n+import javax.servlet.FilterChain;\n+import javax.servlet.FilterConfig;\n+import javax.servlet.ServletContext;\n+import javax.servlet.ServletException;\n+import javax.servlet.ServletRequest;\n+import javax.servlet.ServletResponse;\n+import javax.servlet.http.HttpServlet;\n+import javax.servlet.http.HttpServletRequest;\n+import javax.servlet.http.HttpServletRequestWrapper;\n+import javax.servlet.http.HttpServletResponse;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import com.sun.jersey.spi.container.servlet.ServletContainer;\n+import org.apache.hadoop.HadoopIllegalArgumentException;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.ConfServlet;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.conf.Configuration.IntegerRanges;\n+import org.apache.hadoop.fs.CommonConfigurationKeys;\n+import org.apache.hadoop.jmx.JMXJsonServlet;\n+import org.apache.hadoop.log.LogLevel;\n+import org.apache.hadoop.security.AuthenticationFilterInitializer;\n+import org.apache.hadoop.security.SecurityUtil;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.security.authentication.server.AuthenticationFilter;\n+import org.apache.hadoop.security.authentication.util.SignerSecretProvider;\n+import org.apache.hadoop.security.authorize.AccessControlList;\n+import org.apache.hadoop.security.ssl.SSLFactory;\n+import org.apache.hadoop.util.ReflectionUtils;\n+import org.apache.hadoop.util.Shell;\n+import org.apache.hadoop.util.StringUtils;\n+import org.eclipse.jetty.http.HttpVersion;\n+import org.eclipse.jetty.server.ConnectionFactory;\n+import org.eclipse.jetty.server.Connector;\n+import org.eclipse.jetty.server.Handler;\n+import org.eclipse.jetty.server.HttpConfiguration;\n+import org.eclipse.jetty.server.HttpConnectionFactory;\n+import org.eclipse.jetty.server.RequestLog;\n+import org.eclipse.jetty.server.SecureRequestCustomizer;\n+import org.eclipse.jetty.server.Server;\n+import org.eclipse.jetty.server.ServerConnector;\n+import org.eclipse.jetty.server.SslConnectionFactory;\n+import org.eclipse.jetty.server.handler.ContextHandlerCollection;\n+import org.eclipse.jetty.server.handler.HandlerCollection;\n+import org.eclipse.jetty.server.handler.RequestLogHandler;\n+import org.eclipse.jetty.server.session.SessionHandler;\n+import org.eclipse.jetty.servlet.DefaultServlet;\n+import org.eclipse.jetty.servlet.FilterHolder;\n+import org.eclipse.jetty.servlet.FilterMapping;\n+import org.eclipse.jetty.servlet.ServletContextHandler;\n+import org.eclipse.jetty.servlet.ServletHandler;\n+import org.eclipse.jetty.servlet.ServletHolder;\n+import org.eclipse.jetty.servlet.ServletMapping;\n+import org.eclipse.jetty.util.ArrayUtil;\n+import org.eclipse.jetty.util.MultiException;\n+import org.eclipse.jetty.util.ssl.SslContextFactory;\n+import org.eclipse.jetty.util.thread.QueuedThreadPool;\n+import org.eclipse.jetty.webapp.WebAppContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Create a Jetty embedded server to answer http requests. The primary goal is\n+ * to serve up status information for the server. There are three contexts:\n+ * \"/logs/\" -> points to the log directory \"/static/\" -> points to common static\n+ * files (src/webapps/static) \"/\" -> the jsp server code from\n+ * (src/webapps/<name>)\n+ *\n+ * This class is a fork of the old HttpServer. HttpServer exists for\n+ * compatibility reasons. See HBASE-10336 for more details.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Evolving\n+public final class HttpServer2 implements FilterContainer {\n+  public static final Logger LOG = LoggerFactory.getLogger(HttpServer2.class);\n+\n+  private static final String HTTP_SCHEME = \"http\";\n+  public static final String HTTPS_SCHEME = \"https\";\n+\n+  public static final String HTTP_MAX_REQUEST_HEADER_SIZE_KEY =\n+      \"hadoop.http.max.request.header.size\";\n+  public static final int HTTP_MAX_REQUEST_HEADER_SIZE_DEFAULT = 65536;\n+  public static final String HTTP_MAX_RESPONSE_HEADER_SIZE_KEY =\n+      \"hadoop.http.max.response.header.size\";\n+  public static final int HTTP_MAX_RESPONSE_HEADER_SIZE_DEFAULT = 65536;\n+\n+  public static final String HTTP_SOCKET_BACKLOG_SIZE_KEY =\n+      \"hadoop.http.socket.backlog.size\";\n+  public static final int HTTP_SOCKET_BACKLOG_SIZE_DEFAULT = 128;\n+  public static final String HTTP_MAX_THREADS_KEY = \"hadoop.http.max.threads\";\n+  public static final String HTTP_ACCEPTOR_COUNT_KEY =\n+      \"hadoop.http.acceptor.count\";\n+  // -1 to use default behavior of setting count based on CPU core count\n+  public static final int HTTP_ACCEPTOR_COUNT_DEFAULT = -1;\n+  public static final String HTTP_SELECTOR_COUNT_KEY =\n+      \"hadoop.http.selector.count\";\n+  // -1 to use default behavior of setting count based on CPU core count\n+  public static final int HTTP_SELECTOR_COUNT_DEFAULT = -1;\n+  // idle timeout in milliseconds\n+  public static final String HTTP_IDLE_TIMEOUT_MS_KEY =\n+      \"hadoop.http.idle_timeout.ms\";\n+  public static final int HTTP_IDLE_TIMEOUT_MS_DEFAULT = 10000;\n+  public static final String HTTP_TEMP_DIR_KEY = \"hadoop.http.temp.dir\";\n+\n+  public static final String FILTER_INITIALIZER_PROPERTY\n+      = \"ozone.http.filter.initializers\";\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0d720e5b97f2a54c639ef2fddb976f137cd456"}, "originalPosition": 150}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwNzQxODgwOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/http/HttpServer2.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQyMDozNzoxMlrOFj5Ijw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QwOToyODoxNFrOFkq6Ow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzE4MDU1OQ==", "bodyText": "I see .clone() is added here. For safe practice?", "url": "https://github.com/apache/ozone/pull/508#discussion_r373180559", "createdAt": "2020-01-30T20:37:12Z", "author": {"login": "smengcl"}, "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/http/HttpServer2.java", "diffHunk": "@@ -0,0 +1,1701 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdds.server.http;\n+\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InterruptedIOException;\n+import java.io.PrintStream;\n+import java.net.BindException;\n+import java.net.InetSocketAddress;\n+import java.net.MalformedURLException;\n+import java.net.URI;\n+import java.net.URL;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import javax.servlet.Filter;\n+import javax.servlet.FilterChain;\n+import javax.servlet.FilterConfig;\n+import javax.servlet.ServletContext;\n+import javax.servlet.ServletException;\n+import javax.servlet.ServletRequest;\n+import javax.servlet.ServletResponse;\n+import javax.servlet.http.HttpServlet;\n+import javax.servlet.http.HttpServletRequest;\n+import javax.servlet.http.HttpServletRequestWrapper;\n+import javax.servlet.http.HttpServletResponse;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import com.sun.jersey.spi.container.servlet.ServletContainer;\n+import org.apache.hadoop.HadoopIllegalArgumentException;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.ConfServlet;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.conf.Configuration.IntegerRanges;\n+import org.apache.hadoop.fs.CommonConfigurationKeys;\n+import org.apache.hadoop.jmx.JMXJsonServlet;\n+import org.apache.hadoop.log.LogLevel;\n+import org.apache.hadoop.security.AuthenticationFilterInitializer;\n+import org.apache.hadoop.security.SecurityUtil;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.security.authentication.server.AuthenticationFilter;\n+import org.apache.hadoop.security.authentication.util.SignerSecretProvider;\n+import org.apache.hadoop.security.authorize.AccessControlList;\n+import org.apache.hadoop.security.ssl.SSLFactory;\n+import org.apache.hadoop.util.ReflectionUtils;\n+import org.apache.hadoop.util.Shell;\n+import org.apache.hadoop.util.StringUtils;\n+import org.eclipse.jetty.http.HttpVersion;\n+import org.eclipse.jetty.server.ConnectionFactory;\n+import org.eclipse.jetty.server.Connector;\n+import org.eclipse.jetty.server.Handler;\n+import org.eclipse.jetty.server.HttpConfiguration;\n+import org.eclipse.jetty.server.HttpConnectionFactory;\n+import org.eclipse.jetty.server.RequestLog;\n+import org.eclipse.jetty.server.SecureRequestCustomizer;\n+import org.eclipse.jetty.server.Server;\n+import org.eclipse.jetty.server.ServerConnector;\n+import org.eclipse.jetty.server.SslConnectionFactory;\n+import org.eclipse.jetty.server.handler.ContextHandlerCollection;\n+import org.eclipse.jetty.server.handler.HandlerCollection;\n+import org.eclipse.jetty.server.handler.RequestLogHandler;\n+import org.eclipse.jetty.server.session.SessionHandler;\n+import org.eclipse.jetty.servlet.DefaultServlet;\n+import org.eclipse.jetty.servlet.FilterHolder;\n+import org.eclipse.jetty.servlet.FilterMapping;\n+import org.eclipse.jetty.servlet.ServletContextHandler;\n+import org.eclipse.jetty.servlet.ServletHandler;\n+import org.eclipse.jetty.servlet.ServletHolder;\n+import org.eclipse.jetty.servlet.ServletMapping;\n+import org.eclipse.jetty.util.ArrayUtil;\n+import org.eclipse.jetty.util.MultiException;\n+import org.eclipse.jetty.util.ssl.SslContextFactory;\n+import org.eclipse.jetty.util.thread.QueuedThreadPool;\n+import org.eclipse.jetty.webapp.WebAppContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Create a Jetty embedded server to answer http requests. The primary goal is\n+ * to serve up status information for the server. There are three contexts:\n+ * \"/logs/\" -> points to the log directory \"/static/\" -> points to common static\n+ * files (src/webapps/static) \"/\" -> the jsp server code from\n+ * (src/webapps/<name>)\n+ *\n+ * This class is a fork of the old HttpServer. HttpServer exists for\n+ * compatibility reasons. See HBASE-10336 for more details.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Evolving\n+public final class HttpServer2 implements FilterContainer {\n+  public static final Logger LOG = LoggerFactory.getLogger(HttpServer2.class);\n+\n+  private static final String HTTP_SCHEME = \"http\";\n+  public static final String HTTPS_SCHEME = \"https\";\n+\n+  public static final String HTTP_MAX_REQUEST_HEADER_SIZE_KEY =\n+      \"hadoop.http.max.request.header.size\";\n+  public static final int HTTP_MAX_REQUEST_HEADER_SIZE_DEFAULT = 65536;\n+  public static final String HTTP_MAX_RESPONSE_HEADER_SIZE_KEY =\n+      \"hadoop.http.max.response.header.size\";\n+  public static final int HTTP_MAX_RESPONSE_HEADER_SIZE_DEFAULT = 65536;\n+\n+  public static final String HTTP_SOCKET_BACKLOG_SIZE_KEY =\n+      \"hadoop.http.socket.backlog.size\";\n+  public static final int HTTP_SOCKET_BACKLOG_SIZE_DEFAULT = 128;\n+  public static final String HTTP_MAX_THREADS_KEY = \"hadoop.http.max.threads\";\n+  public static final String HTTP_ACCEPTOR_COUNT_KEY =\n+      \"hadoop.http.acceptor.count\";\n+  // -1 to use default behavior of setting count based on CPU core count\n+  public static final int HTTP_ACCEPTOR_COUNT_DEFAULT = -1;\n+  public static final String HTTP_SELECTOR_COUNT_KEY =\n+      \"hadoop.http.selector.count\";\n+  // -1 to use default behavior of setting count based on CPU core count\n+  public static final int HTTP_SELECTOR_COUNT_DEFAULT = -1;\n+  // idle timeout in milliseconds\n+  public static final String HTTP_IDLE_TIMEOUT_MS_KEY =\n+      \"hadoop.http.idle_timeout.ms\";\n+  public static final int HTTP_IDLE_TIMEOUT_MS_DEFAULT = 10000;\n+  public static final String HTTP_TEMP_DIR_KEY = \"hadoop.http.temp.dir\";\n+\n+  public static final String FILTER_INITIALIZER_PROPERTY\n+      = \"ozone.http.filter.initializers\";\n+\n+  // The ServletContext attribute where the daemon Configuration\n+  // gets stored.\n+  public static final String CONF_CONTEXT_ATTRIBUTE = \"hadoop.conf\";\n+  public static final String ADMINS_ACL = \"admins.acl\";\n+  public static final String SPNEGO_FILTER = \"SpnegoFilter\";\n+  public static final String NO_CACHE_FILTER = \"NoCacheFilter\";\n+\n+  public static final String BIND_ADDRESS = \"bind.address\";\n+\n+  private final AccessControlList adminsAcl;\n+\n+  private final Server webServer;\n+\n+  private final HandlerCollection handlers;\n+\n+  private final List<ServerConnector> listeners = Lists.newArrayList();\n+\n+  private final WebAppContext webAppContext;\n+  private final boolean findPort;\n+  private final IntegerRanges portRanges;\n+  private final Map<ServletContextHandler, Boolean> defaultContexts =\n+      new HashMap<>();\n+  private final List<String> filterNames = new ArrayList<>();\n+  static final String STATE_DESCRIPTION_ALIVE = \" - alive\";\n+  static final String STATE_DESCRIPTION_NOT_LIVE = \" - not live\";\n+  private final SignerSecretProvider secretProvider;\n+  private XFrameOption xFrameOption;\n+  private boolean xFrameOptionIsEnabled;\n+  public static final String HTTP_HEADER_PREFIX = \"hadoop.http.header.\";\n+  private static final String HTTP_HEADER_REGEX =\n+      \"hadoop\\\\.http\\\\.header\\\\.([a-zA-Z\\\\-_]+)\";\n+  static final String X_XSS_PROTECTION =\n+      \"X-XSS-Protection:1; mode=block\";\n+  static final String X_CONTENT_TYPE_OPTIONS =\n+      \"X-Content-Type-Options:nosniff\";\n+  private static final String X_FRAME_OPTIONS = \"X-FRAME-OPTIONS\";\n+  private static final Pattern PATTERN_HTTP_HEADER_REGEX =\n+      Pattern.compile(HTTP_HEADER_REGEX);\n+  /**\n+   * Class to construct instances of HTTP server with specific options.\n+   */\n+  public static class Builder {\n+    private ArrayList<URI> endpoints = Lists.newArrayList();\n+    private String name;\n+    private Configuration conf;\n+    private Configuration sslConf;\n+    private String[] pathSpecs;\n+    private AccessControlList adminsAcl;\n+    private boolean securityEnabled = false;\n+    private String usernameConfKey;\n+    private String keytabConfKey;\n+    private boolean needsClientAuth;\n+    private String trustStore;\n+    private String trustStorePassword;\n+    private String trustStoreType;\n+\n+    private String keyStore;\n+    private String keyStorePassword;\n+    private String keyStoreType;\n+\n+    // The -keypass option in keytool\n+    private String keyPassword;\n+\n+    private boolean findPort;\n+    private IntegerRanges portRanges = null;\n+\n+    private String hostName;\n+    private boolean disallowFallbackToRandomSignerSecretProvider;\n+    private String authFilterConfigurationPrefix =\n+        \"hadoop.http.authentication.\";\n+    private String excludeCiphers;\n+\n+    private boolean xFrameEnabled;\n+    private XFrameOption xFrameOption = XFrameOption.SAMEORIGIN;\n+\n+    public Builder setName(String serverName) {\n+      this.name = serverName;\n+      return this;\n+    }\n+\n+    /**\n+     * Add an endpoint that the HTTP server should listen to.\n+     *\n+     * @param endpoint\n+     *          the endpoint of that the HTTP server should listen to. The\n+     *          scheme specifies the protocol (i.e. HTTP / HTTPS), the host\n+     *          specifies the binding address, and the port specifies the\n+     *          listening port. Unspecified or zero port means that the server\n+     *          can listen to any port.\n+     */\n+    public Builder addEndpoint(URI endpoint) {\n+      endpoints.add(endpoint);\n+      return this;\n+    }\n+\n+    /**\n+     * Set the hostname of the http server. The host name is used to resolve the\n+     * _HOST field in Kerberos principals. The hostname of the first listener\n+     * will be used if the name is unspecified.\n+     */\n+    public Builder hostName(String host) {\n+      this.hostName = host;\n+      return this;\n+    }\n+\n+    public Builder trustStore(String location, String password, String type) {\n+      this.trustStore = location;\n+      this.trustStorePassword = password;\n+      this.trustStoreType = type;\n+      return this;\n+    }\n+\n+    public Builder keyStore(String location, String password, String type) {\n+      this.keyStore = location;\n+      this.keyStorePassword = password;\n+      this.keyStoreType = type;\n+      return this;\n+    }\n+\n+    public Builder keyPassword(String password) {\n+      this.keyPassword = password;\n+      return this;\n+    }\n+\n+    /**\n+     * Specify whether the server should authorize the client in SSL\n+     * connections.\n+     */\n+    public Builder needsClientAuth(boolean value) {\n+      this.needsClientAuth = value;\n+      return this;\n+    }\n+\n+    public Builder setFindPort(boolean portFind) {\n+      this.findPort = portFind;\n+      return this;\n+    }\n+\n+    public Builder setPortRanges(IntegerRanges ranges) {\n+      this.portRanges = ranges;\n+      return this;\n+    }\n+\n+    public Builder setConf(Configuration configuration) {\n+      this.conf = configuration;\n+      return this;\n+    }\n+\n+    /**\n+     * Specify the SSL configuration to load. This API provides an alternative\n+     * to keyStore/keyPassword/trustStore.\n+     */\n+    public Builder setSSLConf(Configuration sslCnf) {\n+      this.sslConf = sslCnf;\n+      return this;\n+    }\n+\n+    public Builder setPathSpec(String[] pathSpec) {\n+      this.pathSpecs = pathSpec.clone();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0d720e5b97f2a54c639ef2fddb976f137cd456"}, "originalPosition": 309}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzk5NjA5MQ==", "bodyText": "Yes, it was a findbugs violation (internal representation is leaked...)", "url": "https://github.com/apache/ozone/pull/508#discussion_r373996091", "createdAt": "2020-02-03T09:28:14Z", "author": {"login": "elek"}, "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/http/HttpServer2.java", "diffHunk": "@@ -0,0 +1,1701 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdds.server.http;\n+\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InterruptedIOException;\n+import java.io.PrintStream;\n+import java.net.BindException;\n+import java.net.InetSocketAddress;\n+import java.net.MalformedURLException;\n+import java.net.URI;\n+import java.net.URL;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import javax.servlet.Filter;\n+import javax.servlet.FilterChain;\n+import javax.servlet.FilterConfig;\n+import javax.servlet.ServletContext;\n+import javax.servlet.ServletException;\n+import javax.servlet.ServletRequest;\n+import javax.servlet.ServletResponse;\n+import javax.servlet.http.HttpServlet;\n+import javax.servlet.http.HttpServletRequest;\n+import javax.servlet.http.HttpServletRequestWrapper;\n+import javax.servlet.http.HttpServletResponse;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import com.sun.jersey.spi.container.servlet.ServletContainer;\n+import org.apache.hadoop.HadoopIllegalArgumentException;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.ConfServlet;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.conf.Configuration.IntegerRanges;\n+import org.apache.hadoop.fs.CommonConfigurationKeys;\n+import org.apache.hadoop.jmx.JMXJsonServlet;\n+import org.apache.hadoop.log.LogLevel;\n+import org.apache.hadoop.security.AuthenticationFilterInitializer;\n+import org.apache.hadoop.security.SecurityUtil;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.security.authentication.server.AuthenticationFilter;\n+import org.apache.hadoop.security.authentication.util.SignerSecretProvider;\n+import org.apache.hadoop.security.authorize.AccessControlList;\n+import org.apache.hadoop.security.ssl.SSLFactory;\n+import org.apache.hadoop.util.ReflectionUtils;\n+import org.apache.hadoop.util.Shell;\n+import org.apache.hadoop.util.StringUtils;\n+import org.eclipse.jetty.http.HttpVersion;\n+import org.eclipse.jetty.server.ConnectionFactory;\n+import org.eclipse.jetty.server.Connector;\n+import org.eclipse.jetty.server.Handler;\n+import org.eclipse.jetty.server.HttpConfiguration;\n+import org.eclipse.jetty.server.HttpConnectionFactory;\n+import org.eclipse.jetty.server.RequestLog;\n+import org.eclipse.jetty.server.SecureRequestCustomizer;\n+import org.eclipse.jetty.server.Server;\n+import org.eclipse.jetty.server.ServerConnector;\n+import org.eclipse.jetty.server.SslConnectionFactory;\n+import org.eclipse.jetty.server.handler.ContextHandlerCollection;\n+import org.eclipse.jetty.server.handler.HandlerCollection;\n+import org.eclipse.jetty.server.handler.RequestLogHandler;\n+import org.eclipse.jetty.server.session.SessionHandler;\n+import org.eclipse.jetty.servlet.DefaultServlet;\n+import org.eclipse.jetty.servlet.FilterHolder;\n+import org.eclipse.jetty.servlet.FilterMapping;\n+import org.eclipse.jetty.servlet.ServletContextHandler;\n+import org.eclipse.jetty.servlet.ServletHandler;\n+import org.eclipse.jetty.servlet.ServletHolder;\n+import org.eclipse.jetty.servlet.ServletMapping;\n+import org.eclipse.jetty.util.ArrayUtil;\n+import org.eclipse.jetty.util.MultiException;\n+import org.eclipse.jetty.util.ssl.SslContextFactory;\n+import org.eclipse.jetty.util.thread.QueuedThreadPool;\n+import org.eclipse.jetty.webapp.WebAppContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Create a Jetty embedded server to answer http requests. The primary goal is\n+ * to serve up status information for the server. There are three contexts:\n+ * \"/logs/\" -> points to the log directory \"/static/\" -> points to common static\n+ * files (src/webapps/static) \"/\" -> the jsp server code from\n+ * (src/webapps/<name>)\n+ *\n+ * This class is a fork of the old HttpServer. HttpServer exists for\n+ * compatibility reasons. See HBASE-10336 for more details.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Evolving\n+public final class HttpServer2 implements FilterContainer {\n+  public static final Logger LOG = LoggerFactory.getLogger(HttpServer2.class);\n+\n+  private static final String HTTP_SCHEME = \"http\";\n+  public static final String HTTPS_SCHEME = \"https\";\n+\n+  public static final String HTTP_MAX_REQUEST_HEADER_SIZE_KEY =\n+      \"hadoop.http.max.request.header.size\";\n+  public static final int HTTP_MAX_REQUEST_HEADER_SIZE_DEFAULT = 65536;\n+  public static final String HTTP_MAX_RESPONSE_HEADER_SIZE_KEY =\n+      \"hadoop.http.max.response.header.size\";\n+  public static final int HTTP_MAX_RESPONSE_HEADER_SIZE_DEFAULT = 65536;\n+\n+  public static final String HTTP_SOCKET_BACKLOG_SIZE_KEY =\n+      \"hadoop.http.socket.backlog.size\";\n+  public static final int HTTP_SOCKET_BACKLOG_SIZE_DEFAULT = 128;\n+  public static final String HTTP_MAX_THREADS_KEY = \"hadoop.http.max.threads\";\n+  public static final String HTTP_ACCEPTOR_COUNT_KEY =\n+      \"hadoop.http.acceptor.count\";\n+  // -1 to use default behavior of setting count based on CPU core count\n+  public static final int HTTP_ACCEPTOR_COUNT_DEFAULT = -1;\n+  public static final String HTTP_SELECTOR_COUNT_KEY =\n+      \"hadoop.http.selector.count\";\n+  // -1 to use default behavior of setting count based on CPU core count\n+  public static final int HTTP_SELECTOR_COUNT_DEFAULT = -1;\n+  // idle timeout in milliseconds\n+  public static final String HTTP_IDLE_TIMEOUT_MS_KEY =\n+      \"hadoop.http.idle_timeout.ms\";\n+  public static final int HTTP_IDLE_TIMEOUT_MS_DEFAULT = 10000;\n+  public static final String HTTP_TEMP_DIR_KEY = \"hadoop.http.temp.dir\";\n+\n+  public static final String FILTER_INITIALIZER_PROPERTY\n+      = \"ozone.http.filter.initializers\";\n+\n+  // The ServletContext attribute where the daemon Configuration\n+  // gets stored.\n+  public static final String CONF_CONTEXT_ATTRIBUTE = \"hadoop.conf\";\n+  public static final String ADMINS_ACL = \"admins.acl\";\n+  public static final String SPNEGO_FILTER = \"SpnegoFilter\";\n+  public static final String NO_CACHE_FILTER = \"NoCacheFilter\";\n+\n+  public static final String BIND_ADDRESS = \"bind.address\";\n+\n+  private final AccessControlList adminsAcl;\n+\n+  private final Server webServer;\n+\n+  private final HandlerCollection handlers;\n+\n+  private final List<ServerConnector> listeners = Lists.newArrayList();\n+\n+  private final WebAppContext webAppContext;\n+  private final boolean findPort;\n+  private final IntegerRanges portRanges;\n+  private final Map<ServletContextHandler, Boolean> defaultContexts =\n+      new HashMap<>();\n+  private final List<String> filterNames = new ArrayList<>();\n+  static final String STATE_DESCRIPTION_ALIVE = \" - alive\";\n+  static final String STATE_DESCRIPTION_NOT_LIVE = \" - not live\";\n+  private final SignerSecretProvider secretProvider;\n+  private XFrameOption xFrameOption;\n+  private boolean xFrameOptionIsEnabled;\n+  public static final String HTTP_HEADER_PREFIX = \"hadoop.http.header.\";\n+  private static final String HTTP_HEADER_REGEX =\n+      \"hadoop\\\\.http\\\\.header\\\\.([a-zA-Z\\\\-_]+)\";\n+  static final String X_XSS_PROTECTION =\n+      \"X-XSS-Protection:1; mode=block\";\n+  static final String X_CONTENT_TYPE_OPTIONS =\n+      \"X-Content-Type-Options:nosniff\";\n+  private static final String X_FRAME_OPTIONS = \"X-FRAME-OPTIONS\";\n+  private static final Pattern PATTERN_HTTP_HEADER_REGEX =\n+      Pattern.compile(HTTP_HEADER_REGEX);\n+  /**\n+   * Class to construct instances of HTTP server with specific options.\n+   */\n+  public static class Builder {\n+    private ArrayList<URI> endpoints = Lists.newArrayList();\n+    private String name;\n+    private Configuration conf;\n+    private Configuration sslConf;\n+    private String[] pathSpecs;\n+    private AccessControlList adminsAcl;\n+    private boolean securityEnabled = false;\n+    private String usernameConfKey;\n+    private String keytabConfKey;\n+    private boolean needsClientAuth;\n+    private String trustStore;\n+    private String trustStorePassword;\n+    private String trustStoreType;\n+\n+    private String keyStore;\n+    private String keyStorePassword;\n+    private String keyStoreType;\n+\n+    // The -keypass option in keytool\n+    private String keyPassword;\n+\n+    private boolean findPort;\n+    private IntegerRanges portRanges = null;\n+\n+    private String hostName;\n+    private boolean disallowFallbackToRandomSignerSecretProvider;\n+    private String authFilterConfigurationPrefix =\n+        \"hadoop.http.authentication.\";\n+    private String excludeCiphers;\n+\n+    private boolean xFrameEnabled;\n+    private XFrameOption xFrameOption = XFrameOption.SAMEORIGIN;\n+\n+    public Builder setName(String serverName) {\n+      this.name = serverName;\n+      return this;\n+    }\n+\n+    /**\n+     * Add an endpoint that the HTTP server should listen to.\n+     *\n+     * @param endpoint\n+     *          the endpoint of that the HTTP server should listen to. The\n+     *          scheme specifies the protocol (i.e. HTTP / HTTPS), the host\n+     *          specifies the binding address, and the port specifies the\n+     *          listening port. Unspecified or zero port means that the server\n+     *          can listen to any port.\n+     */\n+    public Builder addEndpoint(URI endpoint) {\n+      endpoints.add(endpoint);\n+      return this;\n+    }\n+\n+    /**\n+     * Set the hostname of the http server. The host name is used to resolve the\n+     * _HOST field in Kerberos principals. The hostname of the first listener\n+     * will be used if the name is unspecified.\n+     */\n+    public Builder hostName(String host) {\n+      this.hostName = host;\n+      return this;\n+    }\n+\n+    public Builder trustStore(String location, String password, String type) {\n+      this.trustStore = location;\n+      this.trustStorePassword = password;\n+      this.trustStoreType = type;\n+      return this;\n+    }\n+\n+    public Builder keyStore(String location, String password, String type) {\n+      this.keyStore = location;\n+      this.keyStorePassword = password;\n+      this.keyStoreType = type;\n+      return this;\n+    }\n+\n+    public Builder keyPassword(String password) {\n+      this.keyPassword = password;\n+      return this;\n+    }\n+\n+    /**\n+     * Specify whether the server should authorize the client in SSL\n+     * connections.\n+     */\n+    public Builder needsClientAuth(boolean value) {\n+      this.needsClientAuth = value;\n+      return this;\n+    }\n+\n+    public Builder setFindPort(boolean portFind) {\n+      this.findPort = portFind;\n+      return this;\n+    }\n+\n+    public Builder setPortRanges(IntegerRanges ranges) {\n+      this.portRanges = ranges;\n+      return this;\n+    }\n+\n+    public Builder setConf(Configuration configuration) {\n+      this.conf = configuration;\n+      return this;\n+    }\n+\n+    /**\n+     * Specify the SSL configuration to load. This API provides an alternative\n+     * to keyStore/keyPassword/trustStore.\n+     */\n+    public Builder setSSLConf(Configuration sslCnf) {\n+      this.sslConf = sslCnf;\n+      return this;\n+    }\n+\n+    public Builder setPathSpec(String[] pathSpec) {\n+      this.pathSpecs = pathSpec.clone();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzE4MDU1OQ=="}, "originalCommit": {"oid": "ef0d720e5b97f2a54c639ef2fddb976f137cd456"}, "originalPosition": 309}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 34, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}