{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDI0NDUwMTk4", "number": 980, "reviewThreads": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQwMzo1ODoyOVrOEAv2ng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0zMFQxNDowNjoyMlrOEBHClw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MjE5NDg2OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelineManagerV2Impl.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQwMzo1ODoyOVrOGcOAaw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQwNTowNDowMVrOGcO4LQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI0Mjc5NQ==", "bodyText": "NIT: we can just pass RatisServer instead of SCMHAManager here.", "url": "https://github.com/apache/ozone/pull/980#discussion_r432242795", "createdAt": "2020-05-29T03:58:29Z", "author": {"login": "xiaoyuyao"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelineManagerV2Impl.java", "diffHunk": "@@ -0,0 +1,637 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdds.scm.pipeline;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.hdds.HddsConfigKeys;\n+import org.apache.hadoop.hdds.conf.ConfigurationSource;\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.protocol.proto.HddsProtos;\n+import org.apache.hadoop.hdds.protocol.proto.HddsProtos.ReplicationType;\n+import org.apache.hadoop.hdds.protocol.proto.HddsProtos.ReplicationFactor;\n+import org.apache.hadoop.hdds.scm.ScmConfigKeys;\n+import org.apache.hadoop.hdds.scm.container.ContainerID;\n+import org.apache.hadoop.hdds.scm.ha.SCMHAManager;\n+import org.apache.hadoop.hdds.scm.node.NodeManager;\n+import org.apache.hadoop.hdds.scm.safemode.SCMSafeModeManager;\n+import org.apache.hadoop.hdds.server.events.EventPublisher;\n+import org.apache.hadoop.hdds.utils.Scheduler;\n+import org.apache.hadoop.hdds.utils.db.Table;\n+import org.apache.hadoop.hdds.utils.db.TableIterator;\n+import org.apache.hadoop.metrics2.util.MBeans;\n+import org.apache.hadoop.util.Time;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import javax.management.ObjectName;\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableSet;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.locks.ReadWriteLock;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * SCM Pipeline Manager implementation.\n+ * All the write operations for pipelines must come via PipelineManager.\n+ * It synchronises all write and read operations via a ReadWriteLock.\n+ */\n+public class PipelineManagerV2Impl implements PipelineManager {\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(SCMPipelineManager.class);\n+\n+  private final ReadWriteLock lock;\n+  private PipelineFactory pipelineFactory;\n+  private PipelineStateManagerV2 stateManager;\n+  private Scheduler scheduler;\n+  private BackgroundPipelineCreator backgroundPipelineCreator;\n+  private final NodeManager nodeManager;\n+  private final ConfigurationSource conf;\n+  // Pipeline Manager MXBean\n+  private ObjectName pmInfoBean;\n+  private final SCMPipelineMetrics metrics;\n+  private long pipelineWaitDefaultTimeout;\n+  private final AtomicBoolean isInSafeMode;\n+  // Used to track if the safemode pre-checks have completed. This is designed\n+  // to prevent pipelines being created until sufficient nodes have registered.\n+  private final AtomicBoolean pipelineCreationAllowed;\n+\n+  public PipelineManagerV2Impl(ConfigurationSource conf,\n+                               NodeManager nodeManager,\n+                               PipelineStateManagerV2 pipelineStateManager,\n+                               PipelineFactory pipelineFactory)\n+      throws IOException {\n+    this.lock = new ReentrantReadWriteLock();\n+    this.pipelineFactory = pipelineFactory;\n+    this.stateManager = pipelineStateManager;\n+    this.nodeManager = nodeManager;\n+    this.conf = conf;\n+    this.pmInfoBean = MBeans.register(\"SCMPipelineManager\",\n+        \"SCMPipelineManagerInfo\", this);\n+    this.metrics = SCMPipelineMetrics.create();\n+    this.pipelineWaitDefaultTimeout = conf.getTimeDuration(\n+        HddsConfigKeys.HDDS_PIPELINE_REPORT_INTERVAL,\n+        HddsConfigKeys.HDDS_PIPELINE_REPORT_INTERVAL_DEFAULT,\n+        TimeUnit.MILLISECONDS);\n+    this.isInSafeMode = new AtomicBoolean(conf.getBoolean(\n+        HddsConfigKeys.HDDS_SCM_SAFEMODE_ENABLED,\n+        HddsConfigKeys.HDDS_SCM_SAFEMODE_ENABLED_DEFAULT));\n+    // Pipeline creation is only allowed after the safemode prechecks have\n+    // passed, eg sufficient nodes have registered.\n+    this.pipelineCreationAllowed = new AtomicBoolean(!this.isInSafeMode.get());\n+    initializePipelineState();\n+  }\n+\n+  public static PipelineManager newPipelineManager(\n+      ConfigurationSource conf, SCMHAManager scmhaManager,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a81e213dddf1983e0dde796de4fb2383cec0802f"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI1MjY0NQ==", "bodyText": "We may have more stuff in SCMHAManager than just SCMRatisServer for PipelineManager use cases. The idea here is to have a manager interface for SCM HA so that we won't worry about passing more things into PipelineManager like configs or other things. @xiaoyuyao", "url": "https://github.com/apache/ozone/pull/980#discussion_r432252645", "createdAt": "2020-05-29T04:44:15Z", "author": {"login": "timmylicheng"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelineManagerV2Impl.java", "diffHunk": "@@ -0,0 +1,637 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdds.scm.pipeline;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.hdds.HddsConfigKeys;\n+import org.apache.hadoop.hdds.conf.ConfigurationSource;\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.protocol.proto.HddsProtos;\n+import org.apache.hadoop.hdds.protocol.proto.HddsProtos.ReplicationType;\n+import org.apache.hadoop.hdds.protocol.proto.HddsProtos.ReplicationFactor;\n+import org.apache.hadoop.hdds.scm.ScmConfigKeys;\n+import org.apache.hadoop.hdds.scm.container.ContainerID;\n+import org.apache.hadoop.hdds.scm.ha.SCMHAManager;\n+import org.apache.hadoop.hdds.scm.node.NodeManager;\n+import org.apache.hadoop.hdds.scm.safemode.SCMSafeModeManager;\n+import org.apache.hadoop.hdds.server.events.EventPublisher;\n+import org.apache.hadoop.hdds.utils.Scheduler;\n+import org.apache.hadoop.hdds.utils.db.Table;\n+import org.apache.hadoop.hdds.utils.db.TableIterator;\n+import org.apache.hadoop.metrics2.util.MBeans;\n+import org.apache.hadoop.util.Time;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import javax.management.ObjectName;\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableSet;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.locks.ReadWriteLock;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * SCM Pipeline Manager implementation.\n+ * All the write operations for pipelines must come via PipelineManager.\n+ * It synchronises all write and read operations via a ReadWriteLock.\n+ */\n+public class PipelineManagerV2Impl implements PipelineManager {\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(SCMPipelineManager.class);\n+\n+  private final ReadWriteLock lock;\n+  private PipelineFactory pipelineFactory;\n+  private PipelineStateManagerV2 stateManager;\n+  private Scheduler scheduler;\n+  private BackgroundPipelineCreator backgroundPipelineCreator;\n+  private final NodeManager nodeManager;\n+  private final ConfigurationSource conf;\n+  // Pipeline Manager MXBean\n+  private ObjectName pmInfoBean;\n+  private final SCMPipelineMetrics metrics;\n+  private long pipelineWaitDefaultTimeout;\n+  private final AtomicBoolean isInSafeMode;\n+  // Used to track if the safemode pre-checks have completed. This is designed\n+  // to prevent pipelines being created until sufficient nodes have registered.\n+  private final AtomicBoolean pipelineCreationAllowed;\n+\n+  public PipelineManagerV2Impl(ConfigurationSource conf,\n+                               NodeManager nodeManager,\n+                               PipelineStateManagerV2 pipelineStateManager,\n+                               PipelineFactory pipelineFactory)\n+      throws IOException {\n+    this.lock = new ReentrantReadWriteLock();\n+    this.pipelineFactory = pipelineFactory;\n+    this.stateManager = pipelineStateManager;\n+    this.nodeManager = nodeManager;\n+    this.conf = conf;\n+    this.pmInfoBean = MBeans.register(\"SCMPipelineManager\",\n+        \"SCMPipelineManagerInfo\", this);\n+    this.metrics = SCMPipelineMetrics.create();\n+    this.pipelineWaitDefaultTimeout = conf.getTimeDuration(\n+        HddsConfigKeys.HDDS_PIPELINE_REPORT_INTERVAL,\n+        HddsConfigKeys.HDDS_PIPELINE_REPORT_INTERVAL_DEFAULT,\n+        TimeUnit.MILLISECONDS);\n+    this.isInSafeMode = new AtomicBoolean(conf.getBoolean(\n+        HddsConfigKeys.HDDS_SCM_SAFEMODE_ENABLED,\n+        HddsConfigKeys.HDDS_SCM_SAFEMODE_ENABLED_DEFAULT));\n+    // Pipeline creation is only allowed after the safemode prechecks have\n+    // passed, eg sufficient nodes have registered.\n+    this.pipelineCreationAllowed = new AtomicBoolean(!this.isInSafeMode.get());\n+    initializePipelineState();\n+  }\n+\n+  public static PipelineManager newPipelineManager(\n+      ConfigurationSource conf, SCMHAManager scmhaManager,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI0Mjc5NQ=="}, "originalCommit": {"oid": "a81e213dddf1983e0dde796de4fb2383cec0802f"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI1NzA2OQ==", "bodyText": "In the future, we have to do isLeader check inside PipelineManager for which we might need SCMHAManager instance.", "url": "https://github.com/apache/ozone/pull/980#discussion_r432257069", "createdAt": "2020-05-29T05:04:01Z", "author": {"login": "nandakumar131"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelineManagerV2Impl.java", "diffHunk": "@@ -0,0 +1,637 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdds.scm.pipeline;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.hdds.HddsConfigKeys;\n+import org.apache.hadoop.hdds.conf.ConfigurationSource;\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.protocol.proto.HddsProtos;\n+import org.apache.hadoop.hdds.protocol.proto.HddsProtos.ReplicationType;\n+import org.apache.hadoop.hdds.protocol.proto.HddsProtos.ReplicationFactor;\n+import org.apache.hadoop.hdds.scm.ScmConfigKeys;\n+import org.apache.hadoop.hdds.scm.container.ContainerID;\n+import org.apache.hadoop.hdds.scm.ha.SCMHAManager;\n+import org.apache.hadoop.hdds.scm.node.NodeManager;\n+import org.apache.hadoop.hdds.scm.safemode.SCMSafeModeManager;\n+import org.apache.hadoop.hdds.server.events.EventPublisher;\n+import org.apache.hadoop.hdds.utils.Scheduler;\n+import org.apache.hadoop.hdds.utils.db.Table;\n+import org.apache.hadoop.hdds.utils.db.TableIterator;\n+import org.apache.hadoop.metrics2.util.MBeans;\n+import org.apache.hadoop.util.Time;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import javax.management.ObjectName;\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableSet;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.locks.ReadWriteLock;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * SCM Pipeline Manager implementation.\n+ * All the write operations for pipelines must come via PipelineManager.\n+ * It synchronises all write and read operations via a ReadWriteLock.\n+ */\n+public class PipelineManagerV2Impl implements PipelineManager {\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(SCMPipelineManager.class);\n+\n+  private final ReadWriteLock lock;\n+  private PipelineFactory pipelineFactory;\n+  private PipelineStateManagerV2 stateManager;\n+  private Scheduler scheduler;\n+  private BackgroundPipelineCreator backgroundPipelineCreator;\n+  private final NodeManager nodeManager;\n+  private final ConfigurationSource conf;\n+  // Pipeline Manager MXBean\n+  private ObjectName pmInfoBean;\n+  private final SCMPipelineMetrics metrics;\n+  private long pipelineWaitDefaultTimeout;\n+  private final AtomicBoolean isInSafeMode;\n+  // Used to track if the safemode pre-checks have completed. This is designed\n+  // to prevent pipelines being created until sufficient nodes have registered.\n+  private final AtomicBoolean pipelineCreationAllowed;\n+\n+  public PipelineManagerV2Impl(ConfigurationSource conf,\n+                               NodeManager nodeManager,\n+                               PipelineStateManagerV2 pipelineStateManager,\n+                               PipelineFactory pipelineFactory)\n+      throws IOException {\n+    this.lock = new ReentrantReadWriteLock();\n+    this.pipelineFactory = pipelineFactory;\n+    this.stateManager = pipelineStateManager;\n+    this.nodeManager = nodeManager;\n+    this.conf = conf;\n+    this.pmInfoBean = MBeans.register(\"SCMPipelineManager\",\n+        \"SCMPipelineManagerInfo\", this);\n+    this.metrics = SCMPipelineMetrics.create();\n+    this.pipelineWaitDefaultTimeout = conf.getTimeDuration(\n+        HddsConfigKeys.HDDS_PIPELINE_REPORT_INTERVAL,\n+        HddsConfigKeys.HDDS_PIPELINE_REPORT_INTERVAL_DEFAULT,\n+        TimeUnit.MILLISECONDS);\n+    this.isInSafeMode = new AtomicBoolean(conf.getBoolean(\n+        HddsConfigKeys.HDDS_SCM_SAFEMODE_ENABLED,\n+        HddsConfigKeys.HDDS_SCM_SAFEMODE_ENABLED_DEFAULT));\n+    // Pipeline creation is only allowed after the safemode prechecks have\n+    // passed, eg sufficient nodes have registered.\n+    this.pipelineCreationAllowed = new AtomicBoolean(!this.isInSafeMode.get());\n+    initializePipelineState();\n+  }\n+\n+  public static PipelineManager newPipelineManager(\n+      ConfigurationSource conf, SCMHAManager scmhaManager,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI0Mjc5NQ=="}, "originalCommit": {"oid": "a81e213dddf1983e0dde796de4fb2383cec0802f"}, "originalPosition": 109}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MjIxMjIzOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelineStateManagerV2.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQwNDoxMDo1NVrOGcOLPA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQwNjoyMjo0MFrOGcQLFA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI0NTU2NA==", "bodyText": "NIT: can we group all the method with @replicate annotation together either at the begining or end with some java doc?", "url": "https://github.com/apache/ozone/pull/980#discussion_r432245564", "createdAt": "2020-05-29T04:10:55Z", "author": {"login": "xiaoyuyao"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelineStateManagerV2.java", "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with this\n+ * work for additional information regarding copyright ownership. The ASF\n+ * licenses this file to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ * <p>\n+ * <p>http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * <p>Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package org.apache.hadoop.hdds.scm.pipeline;\n+\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.protocol.proto.HddsProtos;\n+import org.apache.hadoop.hdds.scm.container.ContainerID;\n+import org.apache.hadoop.hdds.scm.metadata.Replicate;\n+import org.apache.hadoop.hdds.utils.db.Table;\n+import org.apache.hadoop.hdds.utils.db.TableIterator;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.NavigableSet;\n+\n+/**\n+ * Manages the state of pipelines in SCM.\n+ */\n+public interface PipelineStateManagerV2 {\n+\n+  @Replicate", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a81e213dddf1983e0dde796de4fb2383cec0802f"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI3ODI5Mg==", "bodyText": "Sure.", "url": "https://github.com/apache/ozone/pull/980#discussion_r432278292", "createdAt": "2020-05-29T06:22:40Z", "author": {"login": "timmylicheng"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelineStateManagerV2.java", "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with this\n+ * work for additional information regarding copyright ownership. The ASF\n+ * licenses this file to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ * <p>\n+ * <p>http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * <p>Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package org.apache.hadoop.hdds.scm.pipeline;\n+\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.protocol.proto.HddsProtos;\n+import org.apache.hadoop.hdds.scm.container.ContainerID;\n+import org.apache.hadoop.hdds.scm.metadata.Replicate;\n+import org.apache.hadoop.hdds.utils.db.Table;\n+import org.apache.hadoop.hdds.utils.db.TableIterator;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.NavigableSet;\n+\n+/**\n+ * Manages the state of pipelines in SCM.\n+ */\n+public interface PipelineStateManagerV2 {\n+\n+  @Replicate", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI0NTU2NA=="}, "originalCommit": {"oid": "a81e213dddf1983e0dde796de4fb2383cec0802f"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MjMwNzcwOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelineStateManagerV2.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQwNToxODozMlrOGcPE0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQwNzozOTozOVrOGcSCmw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI2MDMwNQ==", "bodyText": "We should not expose TableIterator from PipelineStateManager", "url": "https://github.com/apache/ozone/pull/980#discussion_r432260305", "createdAt": "2020-05-29T05:18:32Z", "author": {"login": "nandakumar131"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelineStateManagerV2.java", "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with this\n+ * work for additional information regarding copyright ownership. The ASF\n+ * licenses this file to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ * <p>\n+ * <p>http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * <p>Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package org.apache.hadoop.hdds.scm.pipeline;\n+\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.protocol.proto.HddsProtos;\n+import org.apache.hadoop.hdds.scm.container.ContainerID;\n+import org.apache.hadoop.hdds.scm.metadata.Replicate;\n+import org.apache.hadoop.hdds.utils.db.Table;\n+import org.apache.hadoop.hdds.utils.db.TableIterator;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.NavigableSet;\n+\n+/**\n+ * Manages the state of pipelines in SCM.\n+ */\n+public interface PipelineStateManagerV2 {\n+\n+  @Replicate\n+  void addPipeline(HddsProtos.Pipeline pipelineProto) throws IOException;\n+\n+  void addContainerToPipeline(PipelineID pipelineID,\n+                              ContainerID containerID) throws IOException;\n+\n+  Pipeline getPipeline(PipelineID pipelineID) throws PipelineNotFoundException;\n+\n+  List<Pipeline> getPipelines();\n+\n+  List<Pipeline> getPipelines(HddsProtos.ReplicationType type);\n+\n+  List<Pipeline> getPipelines(HddsProtos.ReplicationType type,\n+                              HddsProtos.ReplicationFactor factor);\n+\n+  List<Pipeline> getPipelines(HddsProtos.ReplicationType type,\n+                              HddsProtos.ReplicationFactor factor,\n+                              Pipeline.PipelineState state);\n+\n+  List<Pipeline> getPipelines(HddsProtos.ReplicationType type,\n+                              HddsProtos.ReplicationFactor factor,\n+                              Pipeline.PipelineState state,\n+                              Collection<DatanodeDetails> excludeDns,\n+                              Collection<PipelineID> excludePipelines);\n+\n+  List<Pipeline> getPipelines(HddsProtos.ReplicationType type,\n+                              Pipeline.PipelineState... states);\n+\n+  NavigableSet<ContainerID> getContainers(PipelineID pipelineID)\n+      throws IOException;\n+\n+  int getNumberOfContainers(PipelineID pipelineID) throws IOException;\n+\n+  @Replicate\n+  Pipeline removePipeline(HddsProtos.PipelineID pipelineIDProto)\n+      throws IOException;\n+\n+  void removeContainerFromPipeline(PipelineID pipelineID,\n+                                   ContainerID containerID) throws IOException;\n+\n+  @Replicate\n+  void updatePipelineState(HddsProtos.PipelineID pipelineIDProto,\n+                           HddsProtos.PipelineState newState)\n+      throws IOException;\n+\n+  boolean isPipelineStoreEmpty() throws IOException;\n+\n+  TableIterator<PipelineID, ? extends Table.KeyValue<PipelineID, Pipeline>>\n+      getPipelineStoreIterator() throws IOException;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a81e213dddf1983e0dde796de4fb2383cec0802f"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjMwODg5MQ==", "bodyText": "Updated.", "url": "https://github.com/apache/ozone/pull/980#discussion_r432308891", "createdAt": "2020-05-29T07:39:39Z", "author": {"login": "timmylicheng"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelineStateManagerV2.java", "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with this\n+ * work for additional information regarding copyright ownership. The ASF\n+ * licenses this file to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ * <p>\n+ * <p>http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * <p>Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package org.apache.hadoop.hdds.scm.pipeline;\n+\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.protocol.proto.HddsProtos;\n+import org.apache.hadoop.hdds.scm.container.ContainerID;\n+import org.apache.hadoop.hdds.scm.metadata.Replicate;\n+import org.apache.hadoop.hdds.utils.db.Table;\n+import org.apache.hadoop.hdds.utils.db.TableIterator;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.NavigableSet;\n+\n+/**\n+ * Manages the state of pipelines in SCM.\n+ */\n+public interface PipelineStateManagerV2 {\n+\n+  @Replicate\n+  void addPipeline(HddsProtos.Pipeline pipelineProto) throws IOException;\n+\n+  void addContainerToPipeline(PipelineID pipelineID,\n+                              ContainerID containerID) throws IOException;\n+\n+  Pipeline getPipeline(PipelineID pipelineID) throws PipelineNotFoundException;\n+\n+  List<Pipeline> getPipelines();\n+\n+  List<Pipeline> getPipelines(HddsProtos.ReplicationType type);\n+\n+  List<Pipeline> getPipelines(HddsProtos.ReplicationType type,\n+                              HddsProtos.ReplicationFactor factor);\n+\n+  List<Pipeline> getPipelines(HddsProtos.ReplicationType type,\n+                              HddsProtos.ReplicationFactor factor,\n+                              Pipeline.PipelineState state);\n+\n+  List<Pipeline> getPipelines(HddsProtos.ReplicationType type,\n+                              HddsProtos.ReplicationFactor factor,\n+                              Pipeline.PipelineState state,\n+                              Collection<DatanodeDetails> excludeDns,\n+                              Collection<PipelineID> excludePipelines);\n+\n+  List<Pipeline> getPipelines(HddsProtos.ReplicationType type,\n+                              Pipeline.PipelineState... states);\n+\n+  NavigableSet<ContainerID> getContainers(PipelineID pipelineID)\n+      throws IOException;\n+\n+  int getNumberOfContainers(PipelineID pipelineID) throws IOException;\n+\n+  @Replicate\n+  Pipeline removePipeline(HddsProtos.PipelineID pipelineIDProto)\n+      throws IOException;\n+\n+  void removeContainerFromPipeline(PipelineID pipelineID,\n+                                   ContainerID containerID) throws IOException;\n+\n+  @Replicate\n+  void updatePipelineState(HddsProtos.PipelineID pipelineIDProto,\n+                           HddsProtos.PipelineState newState)\n+      throws IOException;\n+\n+  boolean isPipelineStoreEmpty() throws IOException;\n+\n+  TableIterator<PipelineID, ? extends Table.KeyValue<PipelineID, Pipeline>>\n+      getPipelineStoreIterator() throws IOException;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI2MDMwNQ=="}, "originalCommit": {"oid": "a81e213dddf1983e0dde796de4fb2383cec0802f"}, "originalPosition": 85}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MjMxNTkzOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelineManagerV2Impl.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQwNToyMzo1NFrOGcPJ-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQwNjo1NzowOVrOGcQ6hA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI2MTYyNQ==", "bodyText": "stateManager#addPipeline is replicated via Ratis, which will make the pipeline added to in-memory and to DB on all the SCMs.\nnodeManager#addPipeline, since executed outside of PipelineStateManager will only be executed on Leader SCM. This will be a problem as the followers will not have this information.\nMove nodeManager#addPipeline call to stateManager#addPipeline.", "url": "https://github.com/apache/ozone/pull/980#discussion_r432261625", "createdAt": "2020-05-29T05:23:54Z", "author": {"login": "nandakumar131"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelineManagerV2Impl.java", "diffHunk": "@@ -0,0 +1,637 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdds.scm.pipeline;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.hdds.HddsConfigKeys;\n+import org.apache.hadoop.hdds.conf.ConfigurationSource;\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.protocol.proto.HddsProtos;\n+import org.apache.hadoop.hdds.protocol.proto.HddsProtos.ReplicationType;\n+import org.apache.hadoop.hdds.protocol.proto.HddsProtos.ReplicationFactor;\n+import org.apache.hadoop.hdds.scm.ScmConfigKeys;\n+import org.apache.hadoop.hdds.scm.container.ContainerID;\n+import org.apache.hadoop.hdds.scm.ha.SCMHAManager;\n+import org.apache.hadoop.hdds.scm.node.NodeManager;\n+import org.apache.hadoop.hdds.scm.safemode.SCMSafeModeManager;\n+import org.apache.hadoop.hdds.server.events.EventPublisher;\n+import org.apache.hadoop.hdds.utils.Scheduler;\n+import org.apache.hadoop.hdds.utils.db.Table;\n+import org.apache.hadoop.hdds.utils.db.TableIterator;\n+import org.apache.hadoop.metrics2.util.MBeans;\n+import org.apache.hadoop.util.Time;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import javax.management.ObjectName;\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableSet;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.locks.ReadWriteLock;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * SCM Pipeline Manager implementation.\n+ * All the write operations for pipelines must come via PipelineManager.\n+ * It synchronises all write and read operations via a ReadWriteLock.\n+ */\n+public class PipelineManagerV2Impl implements PipelineManager {\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(SCMPipelineManager.class);\n+\n+  private final ReadWriteLock lock;\n+  private PipelineFactory pipelineFactory;\n+  private PipelineStateManagerV2 stateManager;\n+  private Scheduler scheduler;\n+  private BackgroundPipelineCreator backgroundPipelineCreator;\n+  private final NodeManager nodeManager;\n+  private final ConfigurationSource conf;\n+  // Pipeline Manager MXBean\n+  private ObjectName pmInfoBean;\n+  private final SCMPipelineMetrics metrics;\n+  private long pipelineWaitDefaultTimeout;\n+  private final AtomicBoolean isInSafeMode;\n+  // Used to track if the safemode pre-checks have completed. This is designed\n+  // to prevent pipelines being created until sufficient nodes have registered.\n+  private final AtomicBoolean pipelineCreationAllowed;\n+\n+  public PipelineManagerV2Impl(ConfigurationSource conf,\n+                               NodeManager nodeManager,\n+                               PipelineStateManagerV2 pipelineStateManager,\n+                               PipelineFactory pipelineFactory)\n+      throws IOException {\n+    this.lock = new ReentrantReadWriteLock();\n+    this.pipelineFactory = pipelineFactory;\n+    this.stateManager = pipelineStateManager;\n+    this.nodeManager = nodeManager;\n+    this.conf = conf;\n+    this.pmInfoBean = MBeans.register(\"SCMPipelineManager\",\n+        \"SCMPipelineManagerInfo\", this);\n+    this.metrics = SCMPipelineMetrics.create();\n+    this.pipelineWaitDefaultTimeout = conf.getTimeDuration(\n+        HddsConfigKeys.HDDS_PIPELINE_REPORT_INTERVAL,\n+        HddsConfigKeys.HDDS_PIPELINE_REPORT_INTERVAL_DEFAULT,\n+        TimeUnit.MILLISECONDS);\n+    this.isInSafeMode = new AtomicBoolean(conf.getBoolean(\n+        HddsConfigKeys.HDDS_SCM_SAFEMODE_ENABLED,\n+        HddsConfigKeys.HDDS_SCM_SAFEMODE_ENABLED_DEFAULT));\n+    // Pipeline creation is only allowed after the safemode prechecks have\n+    // passed, eg sufficient nodes have registered.\n+    this.pipelineCreationAllowed = new AtomicBoolean(!this.isInSafeMode.get());\n+    initializePipelineState();\n+  }\n+\n+  public static PipelineManager newPipelineManager(\n+      ConfigurationSource conf, SCMHAManager scmhaManager,\n+      NodeManager nodeManager, Table<PipelineID, Pipeline> pipelineStore,\n+      PipelineFactory pipelineFactory) throws IOException {\n+    // Create PipelineStateManager\n+    PipelineStateManagerV2 stateManager = PipelineStateManagerV2Impl\n+        .newBuilder().setPipelineStore(pipelineStore)\n+        .setRatisServer(scmhaManager.getRatisServer()).build();\n+\n+    // Create PipelineManager\n+    PipelineManagerV2Impl pipelineManager = new PipelineManagerV2Impl(conf,\n+        nodeManager, stateManager, pipelineFactory);\n+\n+    // Create background thread.\n+    Scheduler scheduler = new Scheduler(\n+        \"RatisPipelineUtilsThread\", false, 1);\n+    BackgroundPipelineCreator backgroundPipelineCreator =\n+        new BackgroundPipelineCreator(pipelineManager, scheduler, conf);\n+    pipelineManager.setBackgroundPipelineCreator(backgroundPipelineCreator);\n+    pipelineManager.setScheduler(scheduler);\n+\n+    return pipelineManager;\n+  }\n+\n+  protected void initializePipelineState() throws IOException {\n+    if (stateManager.isPipelineStoreEmpty()) {\n+      LOG.info(\"No pipeline exists in current db\");\n+      return;\n+    }\n+    TableIterator<PipelineID, ? extends Table.KeyValue<PipelineID, Pipeline>>\n+        iterator = stateManager.getPipelineStoreIterator();\n+    while (iterator.hasNext()) {\n+      Pipeline pipeline = iterator.next().getValue();\n+      stateManager.addPipeline(pipeline.getProtobufMessage());\n+      nodeManager.addPipeline(pipeline);\n+    }\n+  }\n+\n+  @Override\n+  public Pipeline createPipeline(ReplicationType type,\n+                                 ReplicationFactor factor) throws IOException {\n+    if (!isPipelineCreationAllowed() && factor != ReplicationFactor.ONE) {\n+      LOG.debug(\"Pipeline creation is not allowed until safe mode prechecks \" +\n+          \"complete\");\n+      throw new IOException(\"Pipeline creation is not allowed as safe mode \" +\n+          \"prechecks have not yet passed\");\n+    }\n+    lock.writeLock().lock();\n+    try {\n+      Pipeline pipeline = pipelineFactory.create(type, factor);\n+      stateManager.addPipeline(pipeline.getProtobufMessage());\n+      nodeManager.addPipeline(pipeline);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a81e213dddf1983e0dde796de4fb2383cec0802f"}, "originalPosition": 159}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI5MDQzNg==", "bodyText": "OK Will do", "url": "https://github.com/apache/ozone/pull/980#discussion_r432290436", "createdAt": "2020-05-29T06:57:09Z", "author": {"login": "timmylicheng"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelineManagerV2Impl.java", "diffHunk": "@@ -0,0 +1,637 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdds.scm.pipeline;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.hdds.HddsConfigKeys;\n+import org.apache.hadoop.hdds.conf.ConfigurationSource;\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.protocol.proto.HddsProtos;\n+import org.apache.hadoop.hdds.protocol.proto.HddsProtos.ReplicationType;\n+import org.apache.hadoop.hdds.protocol.proto.HddsProtos.ReplicationFactor;\n+import org.apache.hadoop.hdds.scm.ScmConfigKeys;\n+import org.apache.hadoop.hdds.scm.container.ContainerID;\n+import org.apache.hadoop.hdds.scm.ha.SCMHAManager;\n+import org.apache.hadoop.hdds.scm.node.NodeManager;\n+import org.apache.hadoop.hdds.scm.safemode.SCMSafeModeManager;\n+import org.apache.hadoop.hdds.server.events.EventPublisher;\n+import org.apache.hadoop.hdds.utils.Scheduler;\n+import org.apache.hadoop.hdds.utils.db.Table;\n+import org.apache.hadoop.hdds.utils.db.TableIterator;\n+import org.apache.hadoop.metrics2.util.MBeans;\n+import org.apache.hadoop.util.Time;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import javax.management.ObjectName;\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableSet;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.locks.ReadWriteLock;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * SCM Pipeline Manager implementation.\n+ * All the write operations for pipelines must come via PipelineManager.\n+ * It synchronises all write and read operations via a ReadWriteLock.\n+ */\n+public class PipelineManagerV2Impl implements PipelineManager {\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(SCMPipelineManager.class);\n+\n+  private final ReadWriteLock lock;\n+  private PipelineFactory pipelineFactory;\n+  private PipelineStateManagerV2 stateManager;\n+  private Scheduler scheduler;\n+  private BackgroundPipelineCreator backgroundPipelineCreator;\n+  private final NodeManager nodeManager;\n+  private final ConfigurationSource conf;\n+  // Pipeline Manager MXBean\n+  private ObjectName pmInfoBean;\n+  private final SCMPipelineMetrics metrics;\n+  private long pipelineWaitDefaultTimeout;\n+  private final AtomicBoolean isInSafeMode;\n+  // Used to track if the safemode pre-checks have completed. This is designed\n+  // to prevent pipelines being created until sufficient nodes have registered.\n+  private final AtomicBoolean pipelineCreationAllowed;\n+\n+  public PipelineManagerV2Impl(ConfigurationSource conf,\n+                               NodeManager nodeManager,\n+                               PipelineStateManagerV2 pipelineStateManager,\n+                               PipelineFactory pipelineFactory)\n+      throws IOException {\n+    this.lock = new ReentrantReadWriteLock();\n+    this.pipelineFactory = pipelineFactory;\n+    this.stateManager = pipelineStateManager;\n+    this.nodeManager = nodeManager;\n+    this.conf = conf;\n+    this.pmInfoBean = MBeans.register(\"SCMPipelineManager\",\n+        \"SCMPipelineManagerInfo\", this);\n+    this.metrics = SCMPipelineMetrics.create();\n+    this.pipelineWaitDefaultTimeout = conf.getTimeDuration(\n+        HddsConfigKeys.HDDS_PIPELINE_REPORT_INTERVAL,\n+        HddsConfigKeys.HDDS_PIPELINE_REPORT_INTERVAL_DEFAULT,\n+        TimeUnit.MILLISECONDS);\n+    this.isInSafeMode = new AtomicBoolean(conf.getBoolean(\n+        HddsConfigKeys.HDDS_SCM_SAFEMODE_ENABLED,\n+        HddsConfigKeys.HDDS_SCM_SAFEMODE_ENABLED_DEFAULT));\n+    // Pipeline creation is only allowed after the safemode prechecks have\n+    // passed, eg sufficient nodes have registered.\n+    this.pipelineCreationAllowed = new AtomicBoolean(!this.isInSafeMode.get());\n+    initializePipelineState();\n+  }\n+\n+  public static PipelineManager newPipelineManager(\n+      ConfigurationSource conf, SCMHAManager scmhaManager,\n+      NodeManager nodeManager, Table<PipelineID, Pipeline> pipelineStore,\n+      PipelineFactory pipelineFactory) throws IOException {\n+    // Create PipelineStateManager\n+    PipelineStateManagerV2 stateManager = PipelineStateManagerV2Impl\n+        .newBuilder().setPipelineStore(pipelineStore)\n+        .setRatisServer(scmhaManager.getRatisServer()).build();\n+\n+    // Create PipelineManager\n+    PipelineManagerV2Impl pipelineManager = new PipelineManagerV2Impl(conf,\n+        nodeManager, stateManager, pipelineFactory);\n+\n+    // Create background thread.\n+    Scheduler scheduler = new Scheduler(\n+        \"RatisPipelineUtilsThread\", false, 1);\n+    BackgroundPipelineCreator backgroundPipelineCreator =\n+        new BackgroundPipelineCreator(pipelineManager, scheduler, conf);\n+    pipelineManager.setBackgroundPipelineCreator(backgroundPipelineCreator);\n+    pipelineManager.setScheduler(scheduler);\n+\n+    return pipelineManager;\n+  }\n+\n+  protected void initializePipelineState() throws IOException {\n+    if (stateManager.isPipelineStoreEmpty()) {\n+      LOG.info(\"No pipeline exists in current db\");\n+      return;\n+    }\n+    TableIterator<PipelineID, ? extends Table.KeyValue<PipelineID, Pipeline>>\n+        iterator = stateManager.getPipelineStoreIterator();\n+    while (iterator.hasNext()) {\n+      Pipeline pipeline = iterator.next().getValue();\n+      stateManager.addPipeline(pipeline.getProtobufMessage());\n+      nodeManager.addPipeline(pipeline);\n+    }\n+  }\n+\n+  @Override\n+  public Pipeline createPipeline(ReplicationType type,\n+                                 ReplicationFactor factor) throws IOException {\n+    if (!isPipelineCreationAllowed() && factor != ReplicationFactor.ONE) {\n+      LOG.debug(\"Pipeline creation is not allowed until safe mode prechecks \" +\n+          \"complete\");\n+      throw new IOException(\"Pipeline creation is not allowed as safe mode \" +\n+          \"prechecks have not yet passed\");\n+    }\n+    lock.writeLock().lock();\n+    try {\n+      Pipeline pipeline = pipelineFactory.create(type, factor);\n+      stateManager.addPipeline(pipeline.getProtobufMessage());\n+      nodeManager.addPipeline(pipeline);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI2MTYyNQ=="}, "originalCommit": {"oid": "a81e213dddf1983e0dde796de4fb2383cec0802f"}, "originalPosition": 159}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5NTk5MjY5OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelineStateManagerV2.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0zMFQxNDowNDoxNlrOGcytEQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMVQwMzoyMjoxM1rOGc90-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjg0NDA0OQ==", "bodyText": "We don't need a initialize method exposed, this can. be done called inside PipelineStateManagerImpl constructor.", "url": "https://github.com/apache/ozone/pull/980#discussion_r432844049", "createdAt": "2020-05-30T14:04:16Z", "author": {"login": "nandakumar131"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelineStateManagerV2.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with this\n+ * work for additional information regarding copyright ownership. The ASF\n+ * licenses this file to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ * <p>\n+ * <p>http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * <p>Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package org.apache.hadoop.hdds.scm.pipeline;\n+\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.protocol.proto.HddsProtos;\n+import org.apache.hadoop.hdds.scm.container.ContainerID;\n+import org.apache.hadoop.hdds.scm.metadata.Replicate;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.NavigableSet;\n+\n+/**\n+ * Manages the state of pipelines in SCM.\n+ */\n+public interface PipelineStateManagerV2 {\n+\n+  /**\n+   * Adding pipeline would be replicated to Ratis.\n+   * @param pipelineProto\n+   * @throws IOException\n+   */\n+  @Replicate\n+  void addPipeline(HddsProtos.Pipeline pipelineProto) throws IOException;\n+\n+  /**\n+   * Removing pipeline would be replicated to Ratis.\n+   * @param pipelineIDProto\n+   * @return Pipeline removed\n+   * @throws IOException\n+   */\n+  @Replicate\n+  void removePipeline(HddsProtos.PipelineID pipelineIDProto)\n+      throws IOException;\n+\n+  /**\n+   * Updating pipeline state would be replicated to Ratis.\n+   * @param pipelineIDProto\n+   * @param newState\n+   * @throws IOException\n+   */\n+  @Replicate\n+  void updatePipelineState(HddsProtos.PipelineID pipelineIDProto,\n+                           HddsProtos.PipelineState newState)\n+      throws IOException;\n+\n+  void addContainerToPipeline(PipelineID pipelineID,\n+                              ContainerID containerID) throws IOException;\n+\n+  Pipeline getPipeline(PipelineID pipelineID) throws PipelineNotFoundException;\n+\n+  List<Pipeline> getPipelines();\n+\n+  List<Pipeline> getPipelines(HddsProtos.ReplicationType type);\n+\n+  List<Pipeline> getPipelines(HddsProtos.ReplicationType type,\n+                              HddsProtos.ReplicationFactor factor);\n+\n+  List<Pipeline> getPipelines(HddsProtos.ReplicationType type,\n+                              HddsProtos.ReplicationFactor factor,\n+                              Pipeline.PipelineState state);\n+\n+  List<Pipeline> getPipelines(HddsProtos.ReplicationType type,\n+                              HddsProtos.ReplicationFactor factor,\n+                              Pipeline.PipelineState state,\n+                              Collection<DatanodeDetails> excludeDns,\n+                              Collection<PipelineID> excludePipelines);\n+\n+  List<Pipeline> getPipelines(HddsProtos.ReplicationType type,\n+                              Pipeline.PipelineState... states);\n+\n+  NavigableSet<ContainerID> getContainers(PipelineID pipelineID)\n+      throws IOException;\n+\n+  int getNumberOfContainers(PipelineID pipelineID) throws IOException;\n+\n+\n+  void removeContainerFromPipeline(PipelineID pipelineID,\n+                                   ContainerID containerID) throws IOException;\n+\n+  void initialize() throws IOException;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aac141d704996265a8523231e3f2af2b1f5e20a9"}, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzAyNjI5OA==", "bodyText": "Updated.", "url": "https://github.com/apache/ozone/pull/980#discussion_r433026298", "createdAt": "2020-06-01T03:22:13Z", "author": {"login": "timmylicheng"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelineStateManagerV2.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with this\n+ * work for additional information regarding copyright ownership. The ASF\n+ * licenses this file to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ * <p>\n+ * <p>http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * <p>Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package org.apache.hadoop.hdds.scm.pipeline;\n+\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.protocol.proto.HddsProtos;\n+import org.apache.hadoop.hdds.scm.container.ContainerID;\n+import org.apache.hadoop.hdds.scm.metadata.Replicate;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.NavigableSet;\n+\n+/**\n+ * Manages the state of pipelines in SCM.\n+ */\n+public interface PipelineStateManagerV2 {\n+\n+  /**\n+   * Adding pipeline would be replicated to Ratis.\n+   * @param pipelineProto\n+   * @throws IOException\n+   */\n+  @Replicate\n+  void addPipeline(HddsProtos.Pipeline pipelineProto) throws IOException;\n+\n+  /**\n+   * Removing pipeline would be replicated to Ratis.\n+   * @param pipelineIDProto\n+   * @return Pipeline removed\n+   * @throws IOException\n+   */\n+  @Replicate\n+  void removePipeline(HddsProtos.PipelineID pipelineIDProto)\n+      throws IOException;\n+\n+  /**\n+   * Updating pipeline state would be replicated to Ratis.\n+   * @param pipelineIDProto\n+   * @param newState\n+   * @throws IOException\n+   */\n+  @Replicate\n+  void updatePipelineState(HddsProtos.PipelineID pipelineIDProto,\n+                           HddsProtos.PipelineState newState)\n+      throws IOException;\n+\n+  void addContainerToPipeline(PipelineID pipelineID,\n+                              ContainerID containerID) throws IOException;\n+\n+  Pipeline getPipeline(PipelineID pipelineID) throws PipelineNotFoundException;\n+\n+  List<Pipeline> getPipelines();\n+\n+  List<Pipeline> getPipelines(HddsProtos.ReplicationType type);\n+\n+  List<Pipeline> getPipelines(HddsProtos.ReplicationType type,\n+                              HddsProtos.ReplicationFactor factor);\n+\n+  List<Pipeline> getPipelines(HddsProtos.ReplicationType type,\n+                              HddsProtos.ReplicationFactor factor,\n+                              Pipeline.PipelineState state);\n+\n+  List<Pipeline> getPipelines(HddsProtos.ReplicationType type,\n+                              HddsProtos.ReplicationFactor factor,\n+                              Pipeline.PipelineState state,\n+                              Collection<DatanodeDetails> excludeDns,\n+                              Collection<PipelineID> excludePipelines);\n+\n+  List<Pipeline> getPipelines(HddsProtos.ReplicationType type,\n+                              Pipeline.PipelineState... states);\n+\n+  NavigableSet<ContainerID> getContainers(PipelineID pipelineID)\n+      throws IOException;\n+\n+  int getNumberOfContainers(PipelineID pipelineID) throws IOException;\n+\n+\n+  void removeContainerFromPipeline(PipelineID pipelineID,\n+                                   ContainerID containerID) throws IOException;\n+\n+  void initialize() throws IOException;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjg0NDA0OQ=="}, "originalCommit": {"oid": "aac141d704996265a8523231e3f2af2b1f5e20a9"}, "originalPosition": 98}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5NTk5Mzc1OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelineStateManagerV2Impl.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0zMFQxNDowNjoxMFrOGcytpg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMVQwMzoyMjoyMVrOGc91Gg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjg0NDE5OA==", "bodyText": "We should not do null check here. If pipelineStore and nodeManager are null there is a problem/bug.", "url": "https://github.com/apache/ozone/pull/980#discussion_r432844198", "createdAt": "2020-05-30T14:06:10Z", "author": {"login": "nandakumar131"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelineStateManagerV2Impl.java", "diffHunk": "@@ -0,0 +1,233 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with this\n+ * work for additional information regarding copyright ownership. The ASF\n+ * licenses this file to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ * <p>\n+ * <p>http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * <p>Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package org.apache.hadoop.hdds.scm.pipeline;\n+\n+import com.google.common.base.Preconditions;\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.protocol.proto.HddsProtos;\n+import org.apache.hadoop.hdds.protocol.proto.SCMRatisProtocol;\n+import org.apache.hadoop.hdds.scm.container.ContainerID;\n+import org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler;\n+import org.apache.hadoop.hdds.scm.ha.SCMRatisServer;\n+import org.apache.hadoop.hdds.scm.node.NodeManager;\n+import org.apache.hadoop.hdds.utils.db.Table;\n+import org.apache.hadoop.hdds.utils.db.TableIterator;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Proxy;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.NavigableSet;\n+\n+/**\n+ * Implementation of pipeline state manager.\n+ * PipelineStateMap class holds the data structures related to pipeline and its\n+ * state. All the read and write operations in PipelineStateMap are protected\n+ * by a read write lock.\n+ */\n+public class PipelineStateManagerV2Impl implements PipelineStateManagerV2 {\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(PipelineStateManager.class);\n+\n+  private final PipelineStateMap pipelineStateMap;\n+  private final NodeManager nodeManager;\n+  private Table<PipelineID, Pipeline> pipelineStore;\n+\n+  public PipelineStateManagerV2Impl(Table<PipelineID, Pipeline> pipelineStore,\n+                                    NodeManager nodeManager) {\n+    this.pipelineStateMap = new PipelineStateMap();\n+    this.nodeManager = nodeManager;\n+    this.pipelineStore = pipelineStore;\n+  }\n+\n+  @Override\n+  public void initialize() throws IOException {\n+    if (pipelineStore == null || nodeManager == null) {\n+      throw new IOException(\"PipelineStore cannot be null\");\n+    }\n+    if (pipelineStore.isEmpty()) {\n+      LOG.info(\"No pipeline exists in current db\");\n+      return;\n+    }\n+    TableIterator<PipelineID, ? extends Table.KeyValue<PipelineID, Pipeline>>\n+        iterator = pipelineStore.iterator();\n+    while (iterator.hasNext()) {\n+      Pipeline pipeline = iterator.next().getValue();\n+      addPipeline(pipeline.getProtobufMessage());\n+    }\n+  }\n+\n+  @Override\n+  public void addPipeline(HddsProtos.Pipeline pipelineProto)\n+      throws IOException {\n+    Pipeline pipeline = Pipeline.getFromProtobuf(pipelineProto);\n+    if (pipelineStore != null) {\n+      pipelineStore.put(pipeline.getId(), pipeline);\n+    }\n+    pipelineStateMap.addPipeline(pipeline);\n+    if (nodeManager != null) {\n+      nodeManager.addPipeline(pipeline);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aac141d704996265a8523231e3f2af2b1f5e20a9"}, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzAyNjMzMA==", "bodyText": "Update.", "url": "https://github.com/apache/ozone/pull/980#discussion_r433026330", "createdAt": "2020-06-01T03:22:21Z", "author": {"login": "timmylicheng"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelineStateManagerV2Impl.java", "diffHunk": "@@ -0,0 +1,233 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with this\n+ * work for additional information regarding copyright ownership. The ASF\n+ * licenses this file to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ * <p>\n+ * <p>http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * <p>Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package org.apache.hadoop.hdds.scm.pipeline;\n+\n+import com.google.common.base.Preconditions;\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.protocol.proto.HddsProtos;\n+import org.apache.hadoop.hdds.protocol.proto.SCMRatisProtocol;\n+import org.apache.hadoop.hdds.scm.container.ContainerID;\n+import org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler;\n+import org.apache.hadoop.hdds.scm.ha.SCMRatisServer;\n+import org.apache.hadoop.hdds.scm.node.NodeManager;\n+import org.apache.hadoop.hdds.utils.db.Table;\n+import org.apache.hadoop.hdds.utils.db.TableIterator;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Proxy;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.NavigableSet;\n+\n+/**\n+ * Implementation of pipeline state manager.\n+ * PipelineStateMap class holds the data structures related to pipeline and its\n+ * state. All the read and write operations in PipelineStateMap are protected\n+ * by a read write lock.\n+ */\n+public class PipelineStateManagerV2Impl implements PipelineStateManagerV2 {\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(PipelineStateManager.class);\n+\n+  private final PipelineStateMap pipelineStateMap;\n+  private final NodeManager nodeManager;\n+  private Table<PipelineID, Pipeline> pipelineStore;\n+\n+  public PipelineStateManagerV2Impl(Table<PipelineID, Pipeline> pipelineStore,\n+                                    NodeManager nodeManager) {\n+    this.pipelineStateMap = new PipelineStateMap();\n+    this.nodeManager = nodeManager;\n+    this.pipelineStore = pipelineStore;\n+  }\n+\n+  @Override\n+  public void initialize() throws IOException {\n+    if (pipelineStore == null || nodeManager == null) {\n+      throw new IOException(\"PipelineStore cannot be null\");\n+    }\n+    if (pipelineStore.isEmpty()) {\n+      LOG.info(\"No pipeline exists in current db\");\n+      return;\n+    }\n+    TableIterator<PipelineID, ? extends Table.KeyValue<PipelineID, Pipeline>>\n+        iterator = pipelineStore.iterator();\n+    while (iterator.hasNext()) {\n+      Pipeline pipeline = iterator.next().getValue();\n+      addPipeline(pipeline.getProtobufMessage());\n+    }\n+  }\n+\n+  @Override\n+  public void addPipeline(HddsProtos.Pipeline pipelineProto)\n+      throws IOException {\n+    Pipeline pipeline = Pipeline.getFromProtobuf(pipelineProto);\n+    if (pipelineStore != null) {\n+      pipelineStore.put(pipeline.getId(), pipeline);\n+    }\n+    pipelineStateMap.addPipeline(pipeline);\n+    if (nodeManager != null) {\n+      nodeManager.addPipeline(pipeline);\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjg0NDE5OA=="}, "originalCommit": {"oid": "aac141d704996265a8523231e3f2af2b1f5e20a9"}, "originalPosition": 88}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5NTk5MzgzOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelineStateManagerV2Impl.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0zMFQxNDowNjoyMlrOGcytsw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0zMFQxNDowNjoyMlrOGcytsw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjg0NDIxMQ==", "bodyText": "We should not do null check here. If pipelineStore and nodeManager are null there is a problem/bug.", "url": "https://github.com/apache/ozone/pull/980#discussion_r432844211", "createdAt": "2020-05-30T14:06:22Z", "author": {"login": "nandakumar131"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelineStateManagerV2Impl.java", "diffHunk": "@@ -0,0 +1,233 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with this\n+ * work for additional information regarding copyright ownership. The ASF\n+ * licenses this file to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ * <p>\n+ * <p>http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * <p>Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package org.apache.hadoop.hdds.scm.pipeline;\n+\n+import com.google.common.base.Preconditions;\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.protocol.proto.HddsProtos;\n+import org.apache.hadoop.hdds.protocol.proto.SCMRatisProtocol;\n+import org.apache.hadoop.hdds.scm.container.ContainerID;\n+import org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler;\n+import org.apache.hadoop.hdds.scm.ha.SCMRatisServer;\n+import org.apache.hadoop.hdds.scm.node.NodeManager;\n+import org.apache.hadoop.hdds.utils.db.Table;\n+import org.apache.hadoop.hdds.utils.db.TableIterator;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Proxy;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.NavigableSet;\n+\n+/**\n+ * Implementation of pipeline state manager.\n+ * PipelineStateMap class holds the data structures related to pipeline and its\n+ * state. All the read and write operations in PipelineStateMap are protected\n+ * by a read write lock.\n+ */\n+public class PipelineStateManagerV2Impl implements PipelineStateManagerV2 {\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(PipelineStateManager.class);\n+\n+  private final PipelineStateMap pipelineStateMap;\n+  private final NodeManager nodeManager;\n+  private Table<PipelineID, Pipeline> pipelineStore;\n+\n+  public PipelineStateManagerV2Impl(Table<PipelineID, Pipeline> pipelineStore,\n+                                    NodeManager nodeManager) {\n+    this.pipelineStateMap = new PipelineStateMap();\n+    this.nodeManager = nodeManager;\n+    this.pipelineStore = pipelineStore;\n+  }\n+\n+  @Override\n+  public void initialize() throws IOException {\n+    if (pipelineStore == null || nodeManager == null) {\n+      throw new IOException(\"PipelineStore cannot be null\");\n+    }\n+    if (pipelineStore.isEmpty()) {\n+      LOG.info(\"No pipeline exists in current db\");\n+      return;\n+    }\n+    TableIterator<PipelineID, ? extends Table.KeyValue<PipelineID, Pipeline>>\n+        iterator = pipelineStore.iterator();\n+    while (iterator.hasNext()) {\n+      Pipeline pipeline = iterator.next().getValue();\n+      addPipeline(pipeline.getProtobufMessage());\n+    }\n+  }\n+\n+  @Override\n+  public void addPipeline(HddsProtos.Pipeline pipelineProto)\n+      throws IOException {\n+    Pipeline pipeline = Pipeline.getFromProtobuf(pipelineProto);\n+    if (pipelineStore != null) {\n+      pipelineStore.put(pipeline.getId(), pipeline);\n+    }\n+    pipelineStateMap.addPipeline(pipeline);\n+    if (nodeManager != null) {\n+      nodeManager.addPipeline(pipeline);\n+    }\n+    LOG.info(\"Created pipeline {}.\", pipeline);\n+  }\n+\n+  @Override\n+  public void addContainerToPipeline(\n+      PipelineID pipelineId, ContainerID containerID)\n+      throws IOException {\n+    pipelineStateMap.addContainerToPipeline(pipelineId, containerID);\n+  }\n+\n+  @Override\n+  public Pipeline getPipeline(PipelineID pipelineID)\n+      throws PipelineNotFoundException {\n+    return pipelineStateMap.getPipeline(pipelineID);\n+  }\n+\n+  @Override\n+  public List<Pipeline> getPipelines() {\n+    return pipelineStateMap.getPipelines();\n+  }\n+\n+  @Override\n+  public List<Pipeline> getPipelines(HddsProtos.ReplicationType type) {\n+    return pipelineStateMap.getPipelines(type);\n+  }\n+\n+  @Override\n+  public List<Pipeline> getPipelines(\n+      HddsProtos.ReplicationType type, HddsProtos.ReplicationFactor factor) {\n+    return pipelineStateMap.getPipelines(type, factor);\n+  }\n+\n+  @Override\n+  public List<Pipeline> getPipelines(\n+      HddsProtos.ReplicationType type, HddsProtos.ReplicationFactor factor,\n+                              Pipeline.PipelineState state) {\n+    return pipelineStateMap.getPipelines(type, factor, state);\n+  }\n+\n+  @Override\n+  public List<Pipeline> getPipelines(\n+      HddsProtos.ReplicationType type, HddsProtos.ReplicationFactor factor,\n+      Pipeline.PipelineState state, Collection<DatanodeDetails> excludeDns,\n+      Collection<PipelineID> excludePipelines) {\n+    return pipelineStateMap\n+        .getPipelines(type, factor, state, excludeDns, excludePipelines);\n+  }\n+\n+  @Override\n+  public List<Pipeline> getPipelines(HddsProtos.ReplicationType type,\n+                                     Pipeline.PipelineState... states) {\n+    return pipelineStateMap.getPipelines(type, states);\n+  }\n+\n+  @Override\n+  public NavigableSet<ContainerID> getContainers(PipelineID pipelineID)\n+      throws IOException {\n+    return pipelineStateMap.getContainers(pipelineID);\n+  }\n+\n+  @Override\n+  public int getNumberOfContainers(PipelineID pipelineID) throws IOException {\n+    return pipelineStateMap.getNumberOfContainers(pipelineID);\n+  }\n+\n+  @Override\n+  public void removePipeline(HddsProtos.PipelineID pipelineIDProto)\n+      throws IOException {\n+    PipelineID pipelineID = PipelineID.getFromProtobuf(pipelineIDProto);\n+    if (pipelineStore != null) {\n+      pipelineStore.delete(pipelineID);\n+    }\n+    Pipeline pipeline = pipelineStateMap.removePipeline(pipelineID);\n+    if (nodeManager != null) {\n+      nodeManager.removePipeline(pipeline);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aac141d704996265a8523231e3f2af2b1f5e20a9"}, "originalPosition": 164}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4181, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}