{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTM5NzQyMDM4", "number": 1699, "reviewThreads": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQyMjoxNTozOVrOFH80gQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQyMToxMzoyOFrOFITZEQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzODgwODMzOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/DBStore.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQyMjoxNTozOVrOIJnlHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQyMjoxNTozOVrOIJnlHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njk1NjU3Mw==", "bodyText": "Can we add new JavaDoc here.", "url": "https://github.com/apache/ozone/pull/1699#discussion_r546956573", "createdAt": "2020-12-21T22:15:39Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/DBStore.java", "diffHunk": "@@ -61,14 +60,9 @@\n   <KEY, VALUE> Table<KEY, VALUE> getTable(String name,\n       Class<KEY> keyType, Class<VALUE> valueType) throws IOException;\n \n-  /**\n-   * Gets an existing TableStore with implicit key/value conversion and\n-   * with specified cleanup policy for cache.\n-   * @throws IOException\n-   */\n   <KEY, VALUE> Table<KEY, VALUE> getTable(String name,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1df7910e8294d0c7b7258444087ebaf385cb04ff"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzODgxMzg1OnYy", "diffSide": "LEFT", "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/RDBStore.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQyMjoxNzo1NVrOIJnoUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQyMjoxNzo1NVrOIJnoUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njk1NzM5Mg==", "bodyText": "Any reason for removing the Override flag?", "url": "https://github.com/apache/ozone/pull/1699#discussion_r546957392", "createdAt": "2020-12-21T22:17:55Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/RDBStore.java", "diffHunk": "@@ -307,12 +307,11 @@ protected ObjectName getStatMBeanName() {\n         valueType);\n   }\n \n-  @Override", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1df7910e8294d0c7b7258444087ebaf385cb04ff"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzODgyMTE5OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/TypedTable.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQyMjoyMDo1NFrOIJnseA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQyMjoyMDo1NFrOIJnseA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njk1ODQ1Ng==", "bodyText": "JavaDoc says default cache type is FullCache but the calling parameter is set to PartialCache.", "url": "https://github.com/apache/ozone/pull/1699#discussion_r546958456", "createdAt": "2020-12-21T22:20:54Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/TypedTable.java", "diffHunk": "@@ -61,8 +62,7 @@\n \n   /**\n    * Create an TypedTable from the raw table.\n-   * Default cleanup policy used for the table is\n-   * {@link CacheCleanupPolicy#MANUAL}.\n+   * Default cache type for the table is {@link CacheType#FullCache}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1df7910e8294d0c7b7258444087ebaf385cb04ff"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzOTA4NDU2OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/FullTableCache.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQwMDoxODo0NVrOIJqERA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQwMDoxODo0NVrOIJqERA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njk5NzMxNg==", "bodyText": "NIT: look ups", "url": "https://github.com/apache/ozone/pull/1699#discussion_r546997316", "createdAt": "2020-12-22T00:18:45Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/FullTableCache.java", "diffHunk": "@@ -40,64 +41,74 @@\n import org.slf4j.LoggerFactory;\n \n /**\n- * Cache implementation for the table. Depending on the cache clean up policy\n- * this cache will be full cache or partial cache.\n- *\n- * If cache cleanup policy is set as {@link CacheCleanupPolicy#MANUAL},\n- * this will be a partial cache.\n- *\n- * If cache cleanup policy is set as {@link CacheCleanupPolicy#NEVER},\n- * this will be a full cache.\n+ * Cache implementation for the table. Full Table cache, where the DB state\n+ * and cache state will be same for these tables.\n  */\n @Private\n @Evolving\n-public class TableCacheImpl<CACHEKEY extends CacheKey,\n+public class FullTableCache<CACHEKEY extends CacheKey,\n     CACHEVALUE extends CacheValue> implements TableCache<CACHEKEY, CACHEVALUE> {\n \n   public static final Logger LOG =\n-      LoggerFactory.getLogger(TableCacheImpl.class);\n+      LoggerFactory.getLogger(FullTableCache.class);\n \n   private final Map<CACHEKEY, CACHEVALUE> cache;\n   private final NavigableSet<EpochEntry<CACHEKEY>> epochEntries;\n   private ExecutorService executorService;\n-  private CacheCleanupPolicy cleanupPolicy;\n \n+  private final ReadWriteLock lock;\n \n \n-  public TableCacheImpl(CacheCleanupPolicy cleanupPolicy) {\n-\n+  public FullTableCache() {\n     // As for full table cache only we need elements to be inserted in sorted\n-    // manner, so that list will be easy. For other we can go with Hash map.\n-    if (cleanupPolicy == CacheCleanupPolicy.NEVER) {\n-      cache = new ConcurrentSkipListMap<>();\n-    } else {\n-      cache = new ConcurrentHashMap<>();\n-    }\n+    // manner, so that list will be easy. But looks up have log(N) time", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1df7910e8294d0c7b7258444087ebaf385cb04ff"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzOTEwNzUxOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/FullTableCache.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQwMDozMToxOVrOIJqREQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQwMDozMToxOVrOIJqREQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzAwMDU5Mw==", "bodyText": "I think with the new log4j, we do not need this check for isDebugEnabled(). Parameters will be evaluated only if Debug is enabled.", "url": "https://github.com/apache/ozone/pull/1699#discussion_r547000593", "createdAt": "2020-12-22T00:31:19Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/FullTableCache.java", "diffHunk": "@@ -125,43 +136,44 @@ protected void evictCache(List<Long> epochs) {\n       currentEntry = iterator.next();\n       cachekey = currentEntry.getCachekey();\n       long currentEpoch = currentEntry.getEpoch();\n-      CacheValue cacheValue = cache.computeIfPresent(cachekey, ((k, v) -> {\n-        if (cleanupPolicy == CacheCleanupPolicy.MANUAL) {\n-          if (v.getEpoch() == currentEpoch && epochs.contains(v.getEpoch())) {\n-            LOG.debug(\"CacheKey {} with epoch {} is removed from cache\",\n-                k.getCacheKey(), currentEpoch);\n-            iterator.remove();\n-            removed.set(true);\n-            return null;\n-          }\n-        } else if (cleanupPolicy == CacheCleanupPolicy.NEVER) {\n-          // Remove only entries which are marked for delete.\n+\n+      // Acquire lock to avoid race between cleanup and add to cache entry by\n+      // client requests.\n+      try {\n+        lock.writeLock().lock();\n+        cache.computeIfPresent(cachekey, ((k, v) -> {\n           if (v.getEpoch() == currentEpoch && epochs.contains(v.getEpoch())\n               && v.getCacheValue() == null) {\n-            LOG.debug(\"CacheKey {} with epoch {} is removed from cache\",\n-                k.getCacheKey(), currentEpoch);\n-            removed.set(true);\n+            if (LOG.isDebugEnabled()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1df7910e8294d0c7b7258444087ebaf385cb04ff"}, "originalPosition": 150}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzOTEzMjMwOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/FullTableCache.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQwMDo0NDozMlrOIJqesA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQyMzozNDozOVrOIKMQHw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzAwNDA4MA==", "bodyText": "Can we add the comment back - // Remove only entries which are marked for delete. and elaborate more that only the epoch entry corresponding to the current CacheValue will be removed here.", "url": "https://github.com/apache/ozone/pull/1699#discussion_r547004080", "createdAt": "2020-12-22T00:44:32Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/FullTableCache.java", "diffHunk": "@@ -125,43 +136,44 @@ protected void evictCache(List<Long> epochs) {\n       currentEntry = iterator.next();\n       cachekey = currentEntry.getCachekey();\n       long currentEpoch = currentEntry.getEpoch();\n-      CacheValue cacheValue = cache.computeIfPresent(cachekey, ((k, v) -> {\n-        if (cleanupPolicy == CacheCleanupPolicy.MANUAL) {\n-          if (v.getEpoch() == currentEpoch && epochs.contains(v.getEpoch())) {\n-            LOG.debug(\"CacheKey {} with epoch {} is removed from cache\",\n-                k.getCacheKey(), currentEpoch);\n-            iterator.remove();\n-            removed.set(true);\n-            return null;\n-          }\n-        } else if (cleanupPolicy == CacheCleanupPolicy.NEVER) {\n-          // Remove only entries which are marked for delete.\n+\n+      // Acquire lock to avoid race between cleanup and add to cache entry by\n+      // client requests.\n+      try {\n+        lock.writeLock().lock();\n+        cache.computeIfPresent(cachekey, ((k, v) -> {\n           if (v.getEpoch() == currentEpoch && epochs.contains(v.getEpoch())\n               && v.getCacheValue() == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1df7910e8294d0c7b7258444087ebaf385cb04ff"}, "originalPosition": 146}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU1NzQwNw==", "bodyText": "Updated.", "url": "https://github.com/apache/ozone/pull/1699#discussion_r547557407", "createdAt": "2020-12-22T23:34:39Z", "author": {"login": "bharatviswa504"}, "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/FullTableCache.java", "diffHunk": "@@ -125,43 +136,44 @@ protected void evictCache(List<Long> epochs) {\n       currentEntry = iterator.next();\n       cachekey = currentEntry.getCachekey();\n       long currentEpoch = currentEntry.getEpoch();\n-      CacheValue cacheValue = cache.computeIfPresent(cachekey, ((k, v) -> {\n-        if (cleanupPolicy == CacheCleanupPolicy.MANUAL) {\n-          if (v.getEpoch() == currentEpoch && epochs.contains(v.getEpoch())) {\n-            LOG.debug(\"CacheKey {} with epoch {} is removed from cache\",\n-                k.getCacheKey(), currentEpoch);\n-            iterator.remove();\n-            removed.set(true);\n-            return null;\n-          }\n-        } else if (cleanupPolicy == CacheCleanupPolicy.NEVER) {\n-          // Remove only entries which are marked for delete.\n+\n+      // Acquire lock to avoid race between cleanup and add to cache entry by\n+      // client requests.\n+      try {\n+        lock.writeLock().lock();\n+        cache.computeIfPresent(cachekey, ((k, v) -> {\n           if (v.getEpoch() == currentEpoch && epochs.contains(v.getEpoch())\n               && v.getCacheValue() == null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzAwNDA4MA=="}, "originalCommit": {"oid": "1df7910e8294d0c7b7258444087ebaf385cb04ff"}, "originalPosition": 146}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0MjMyMTE1OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/FullTableCache.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQyMDowMTo1MFrOIKHpYA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQyMzozNDoxNlrOIKMPzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ4MTk1Mg==", "bodyText": "In the non-ratis OM cluster, we might get evictCache in non sorted order. Let's say we get evictCache (1,3,4). Then we might remove epoch entry 2 also even though it has not been flushed by DoubleBuffer, right?\nInstead of having this removed boolean, would it be easier if we remove the epoch entry whenever it is there in evictCache list and only then? Something like this check before the cache.computeIfPresent()...\nlock.writeLock().lock();\nif (epochs.contains(currentEpoch) {\n   iterator.remove();\n   cache.computeIfPresent(cachekey, ((k, v) -> {\n   ........\n}", "url": "https://github.com/apache/ozone/pull/1699#discussion_r547481952", "createdAt": "2020-12-22T20:01:50Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/FullTableCache.java", "diffHunk": "@@ -125,43 +136,44 @@ protected void evictCache(List<Long> epochs) {\n       currentEntry = iterator.next();\n       cachekey = currentEntry.getCachekey();\n       long currentEpoch = currentEntry.getEpoch();\n-      CacheValue cacheValue = cache.computeIfPresent(cachekey, ((k, v) -> {\n-        if (cleanupPolicy == CacheCleanupPolicy.MANUAL) {\n-          if (v.getEpoch() == currentEpoch && epochs.contains(v.getEpoch())) {\n-            LOG.debug(\"CacheKey {} with epoch {} is removed from cache\",\n-                k.getCacheKey(), currentEpoch);\n-            iterator.remove();\n-            removed.set(true);\n-            return null;\n-          }\n-        } else if (cleanupPolicy == CacheCleanupPolicy.NEVER) {\n-          // Remove only entries which are marked for delete.\n+\n+      // Acquire lock to avoid race between cleanup and add to cache entry by\n+      // client requests.\n+      try {\n+        lock.writeLock().lock();\n+        cache.computeIfPresent(cachekey, ((k, v) -> {\n           if (v.getEpoch() == currentEpoch && epochs.contains(v.getEpoch())\n               && v.getCacheValue() == null) {\n-            LOG.debug(\"CacheKey {} with epoch {} is removed from cache\",\n-                k.getCacheKey(), currentEpoch);\n-            removed.set(true);\n+            if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"CacheKey {} with epoch {} is removed from cache\",\n+                  k.getCacheKey(), currentEpoch);\n+            }\n             iterator.remove();\n+            removed.set(true);\n             return null;\n           }\n+          return v;\n+        }));\n+      } finally {\n+        lock.writeLock().unlock();\n+      }\n+\n+      // If currentEntry epoch is greater than last epoch provided, we have\n+      // deleted all entries less than specified epoch. So, we can break.\n+      if (currentEpoch > lastEpoch) {\n+        break;\n+      }\n+\n+      // When epoch entry is not removed, this might be a override entry in\n+      // cache. Clean that epoch entry.\n+      if (!removed.get()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1df7910e8294d0c7b7258444087ebaf385cb04ff"}, "originalPosition": 172}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU1NzMyNA==", "bodyText": "Good idea, updated code.", "url": "https://github.com/apache/ozone/pull/1699#discussion_r547557324", "createdAt": "2020-12-22T23:34:16Z", "author": {"login": "bharatviswa504"}, "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/FullTableCache.java", "diffHunk": "@@ -125,43 +136,44 @@ protected void evictCache(List<Long> epochs) {\n       currentEntry = iterator.next();\n       cachekey = currentEntry.getCachekey();\n       long currentEpoch = currentEntry.getEpoch();\n-      CacheValue cacheValue = cache.computeIfPresent(cachekey, ((k, v) -> {\n-        if (cleanupPolicy == CacheCleanupPolicy.MANUAL) {\n-          if (v.getEpoch() == currentEpoch && epochs.contains(v.getEpoch())) {\n-            LOG.debug(\"CacheKey {} with epoch {} is removed from cache\",\n-                k.getCacheKey(), currentEpoch);\n-            iterator.remove();\n-            removed.set(true);\n-            return null;\n-          }\n-        } else if (cleanupPolicy == CacheCleanupPolicy.NEVER) {\n-          // Remove only entries which are marked for delete.\n+\n+      // Acquire lock to avoid race between cleanup and add to cache entry by\n+      // client requests.\n+      try {\n+        lock.writeLock().lock();\n+        cache.computeIfPresent(cachekey, ((k, v) -> {\n           if (v.getEpoch() == currentEpoch && epochs.contains(v.getEpoch())\n               && v.getCacheValue() == null) {\n-            LOG.debug(\"CacheKey {} with epoch {} is removed from cache\",\n-                k.getCacheKey(), currentEpoch);\n-            removed.set(true);\n+            if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"CacheKey {} with epoch {} is removed from cache\",\n+                  k.getCacheKey(), currentEpoch);\n+            }\n             iterator.remove();\n+            removed.set(true);\n             return null;\n           }\n+          return v;\n+        }));\n+      } finally {\n+        lock.writeLock().unlock();\n+      }\n+\n+      // If currentEntry epoch is greater than last epoch provided, we have\n+      // deleted all entries less than specified epoch. So, we can break.\n+      if (currentEpoch > lastEpoch) {\n+        break;\n+      }\n+\n+      // When epoch entry is not removed, this might be a override entry in\n+      // cache. Clean that epoch entry.\n+      if (!removed.get()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ4MTk1Mg=="}, "originalCommit": {"oid": "1df7910e8294d0c7b7258444087ebaf385cb04ff"}, "originalPosition": 172}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0MjQ4NjU2OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/PartialTableCache.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQyMTowNDo0MVrOIKJK_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQyMTowNDo0MVrOIKJK_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzUwNjk0Mg==", "bodyText": "Typo: Partial table cache", "url": "https://github.com/apache/ozone/pull/1699#discussion_r547506942", "createdAt": "2020-12-22T21:04:41Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/PartialTableCache.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ *\n+ */\n+\n+package org.apache.hadoop.hdds.utils.db.cache;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableSet;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListSet;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ThreadFactory;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.hadoop.hdds.annotation.InterfaceAudience.Private;\n+import org.apache.hadoop.hdds.annotation.InterfaceStability.Evolving;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Cache implementation for the table. Partial Table cache, where the DB state\n+ * and cache state will not be same. Partial table cache holds entries until\n+ * flush to DB happens.\n+ */\n+@Private\n+@Evolving\n+public class PartialTableCache<CACHEKEY extends CacheKey,\n+    CACHEVALUE extends CacheValue> implements TableCache<CACHEKEY, CACHEVALUE> {\n+\n+  public static final Logger LOG =\n+      LoggerFactory.getLogger(PartialTableCache.class);\n+\n+  private final Map<CACHEKEY, CACHEVALUE> cache;\n+  private final NavigableSet<EpochEntry<CACHEKEY>> epochEntries;\n+  private ExecutorService executorService;\n+\n+\n+  public PartialTableCache() {\n+    // We use concurrent Hash map for O(1) lookup for get API.\n+    // During list operation for partial cache we anyway merge between DB and\n+    // cache state. So entries in cache does not need to be in sorted order.\n+\n+    // And as concurrentHashMap computeIfPresent which is used by cleanup is\n+    // atomic operation, and ozone level locks like bucket/volume locks\n+    // protect updating same key, here it is not required to hold cache\n+    // level locks during update/cleanup operation.\n+\n+    // 1. During update, it is caller responsibility to hold volume/bucket\n+    // locks.\n+    // 2. During cleanup which removes entry, while request is updating cache\n+    // that should be guarded by concurrentHashMap guaranty.\n+    cache = new ConcurrentHashMap<>();\n+\n+    epochEntries = new ConcurrentSkipListSet<>();\n+    // Created a singleThreadExecutor, so one cleanup will be running at a\n+    // time.\n+    ThreadFactory build = new ThreadFactoryBuilder().setDaemon(true)\n+        .setNameFormat(\"PartialTableCache Cleanup Thread - %d\").build();\n+    executorService = Executors.newSingleThreadExecutor(build);\n+  }\n+\n+  @Override\n+  public CACHEVALUE get(CACHEKEY cachekey) {\n+    return cache.get(cachekey);\n+  }\n+\n+  @Override\n+  public void loadInitial(CACHEKEY cacheKey, CACHEVALUE cacheValue) {\n+    // Do nothing for full table cache.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1df7910e8294d0c7b7258444087ebaf385cb04ff"}, "originalPosition": 90}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0MjQ5NTE5OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/PartialTableCache.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQyMTowODoxNFrOIKJQCQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQyMTowODoxNFrOIKJQCQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzUwODIzMw==", "bodyText": "LOG.isDebugEnabled check can be removed here.", "url": "https://github.com/apache/ozone/pull/1699#discussion_r547508233", "createdAt": "2020-12-22T21:08:14Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/PartialTableCache.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ *\n+ */\n+\n+package org.apache.hadoop.hdds.utils.db.cache;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableSet;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListSet;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ThreadFactory;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.hadoop.hdds.annotation.InterfaceAudience.Private;\n+import org.apache.hadoop.hdds.annotation.InterfaceStability.Evolving;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Cache implementation for the table. Partial Table cache, where the DB state\n+ * and cache state will not be same. Partial table cache holds entries until\n+ * flush to DB happens.\n+ */\n+@Private\n+@Evolving\n+public class PartialTableCache<CACHEKEY extends CacheKey,\n+    CACHEVALUE extends CacheValue> implements TableCache<CACHEKEY, CACHEVALUE> {\n+\n+  public static final Logger LOG =\n+      LoggerFactory.getLogger(PartialTableCache.class);\n+\n+  private final Map<CACHEKEY, CACHEVALUE> cache;\n+  private final NavigableSet<EpochEntry<CACHEKEY>> epochEntries;\n+  private ExecutorService executorService;\n+\n+\n+  public PartialTableCache() {\n+    // We use concurrent Hash map for O(1) lookup for get API.\n+    // During list operation for partial cache we anyway merge between DB and\n+    // cache state. So entries in cache does not need to be in sorted order.\n+\n+    // And as concurrentHashMap computeIfPresent which is used by cleanup is\n+    // atomic operation, and ozone level locks like bucket/volume locks\n+    // protect updating same key, here it is not required to hold cache\n+    // level locks during update/cleanup operation.\n+\n+    // 1. During update, it is caller responsibility to hold volume/bucket\n+    // locks.\n+    // 2. During cleanup which removes entry, while request is updating cache\n+    // that should be guarded by concurrentHashMap guaranty.\n+    cache = new ConcurrentHashMap<>();\n+\n+    epochEntries = new ConcurrentSkipListSet<>();\n+    // Created a singleThreadExecutor, so one cleanup will be running at a\n+    // time.\n+    ThreadFactory build = new ThreadFactoryBuilder().setDaemon(true)\n+        .setNameFormat(\"PartialTableCache Cleanup Thread - %d\").build();\n+    executorService = Executors.newSingleThreadExecutor(build);\n+  }\n+\n+  @Override\n+  public CACHEVALUE get(CACHEKEY cachekey) {\n+    return cache.get(cachekey);\n+  }\n+\n+  @Override\n+  public void loadInitial(CACHEKEY cacheKey, CACHEVALUE cacheValue) {\n+    // Do nothing for full table cache.\n+  }\n+\n+  @Override\n+  public void put(CACHEKEY cacheKey, CACHEVALUE value) {\n+    cache.put(cacheKey, value);\n+    epochEntries.add(new EpochEntry<>(value.getEpoch(), cacheKey));\n+  }\n+\n+  public void cleanup(List<Long> epochs) {\n+    executorService.execute(() -> evictCache(epochs));\n+  }\n+\n+  @Override\n+  public int size() {\n+    return cache.size();\n+  }\n+\n+  @Override\n+  public Iterator<Map.Entry<CACHEKEY, CACHEVALUE>> iterator() {\n+    return cache.entrySet().iterator();\n+  }\n+\n+  @VisibleForTesting\n+  public void evictCache(List<Long> epochs) {\n+    EpochEntry<CACHEKEY> currentEntry;\n+    final AtomicBoolean removed = new AtomicBoolean();\n+    CACHEKEY cachekey;\n+    long lastEpoch = epochs.get(epochs.size() - 1);\n+    for (Iterator<EpochEntry<CACHEKEY>> iterator = epochEntries.iterator();\n+         iterator.hasNext();) {\n+      currentEntry = iterator.next();\n+      cachekey = currentEntry.getCachekey();\n+      long currentEpoch = currentEntry.getEpoch();\n+\n+      // As ConcurrentHashMap computeIfPresent is atomic, there is no race\n+      // condition between cache cleanup and requests updating same cache entry.\n+\n+      cache.computeIfPresent(cachekey, ((k, v) -> {\n+        if (v.getEpoch() == currentEpoch && epochs.contains(v.getEpoch())) {\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"CacheKey {} with epoch {} is removed from cache\",\n+                k.getCacheKey(), currentEpoch);\n+          }\n+          iterator.remove();\n+          removed.set(true);\n+          return null;\n+        }\n+        return v;\n+      }));\n+\n+      // If currentEntry epoch is greater than last epoch provided, we have\n+      // deleted all entries less than specified epoch. So, we can break.\n+      if (currentEpoch > lastEpoch) {\n+        break;\n+      }\n+\n+      // When epoch entry is not removed, this might be a override entry in\n+      // cache. Clean that epoch entry.\n+      if (!removed.get()) {\n+        if (LOG.isDebugEnabled()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1df7910e8294d0c7b7258444087ebaf385cb04ff"}, "originalPosition": 150}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0MjQ5NjYxOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/PartialTableCache.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQyMTowODo1MFrOIKJQ5g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQyMTowODo1MFrOIKJQ5g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzUwODQ1NA==", "bodyText": "Same suggestion as in FullTableCache for removed usage.", "url": "https://github.com/apache/ozone/pull/1699#discussion_r547508454", "createdAt": "2020-12-22T21:08:50Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/PartialTableCache.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ *\n+ */\n+\n+package org.apache.hadoop.hdds.utils.db.cache;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableSet;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListSet;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ThreadFactory;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.hadoop.hdds.annotation.InterfaceAudience.Private;\n+import org.apache.hadoop.hdds.annotation.InterfaceStability.Evolving;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Cache implementation for the table. Partial Table cache, where the DB state\n+ * and cache state will not be same. Partial table cache holds entries until\n+ * flush to DB happens.\n+ */\n+@Private\n+@Evolving\n+public class PartialTableCache<CACHEKEY extends CacheKey,\n+    CACHEVALUE extends CacheValue> implements TableCache<CACHEKEY, CACHEVALUE> {\n+\n+  public static final Logger LOG =\n+      LoggerFactory.getLogger(PartialTableCache.class);\n+\n+  private final Map<CACHEKEY, CACHEVALUE> cache;\n+  private final NavigableSet<EpochEntry<CACHEKEY>> epochEntries;\n+  private ExecutorService executorService;\n+\n+\n+  public PartialTableCache() {\n+    // We use concurrent Hash map for O(1) lookup for get API.\n+    // During list operation for partial cache we anyway merge between DB and\n+    // cache state. So entries in cache does not need to be in sorted order.\n+\n+    // And as concurrentHashMap computeIfPresent which is used by cleanup is\n+    // atomic operation, and ozone level locks like bucket/volume locks\n+    // protect updating same key, here it is not required to hold cache\n+    // level locks during update/cleanup operation.\n+\n+    // 1. During update, it is caller responsibility to hold volume/bucket\n+    // locks.\n+    // 2. During cleanup which removes entry, while request is updating cache\n+    // that should be guarded by concurrentHashMap guaranty.\n+    cache = new ConcurrentHashMap<>();\n+\n+    epochEntries = new ConcurrentSkipListSet<>();\n+    // Created a singleThreadExecutor, so one cleanup will be running at a\n+    // time.\n+    ThreadFactory build = new ThreadFactoryBuilder().setDaemon(true)\n+        .setNameFormat(\"PartialTableCache Cleanup Thread - %d\").build();\n+    executorService = Executors.newSingleThreadExecutor(build);\n+  }\n+\n+  @Override\n+  public CACHEVALUE get(CACHEKEY cachekey) {\n+    return cache.get(cachekey);\n+  }\n+\n+  @Override\n+  public void loadInitial(CACHEKEY cacheKey, CACHEVALUE cacheValue) {\n+    // Do nothing for full table cache.\n+  }\n+\n+  @Override\n+  public void put(CACHEKEY cacheKey, CACHEVALUE value) {\n+    cache.put(cacheKey, value);\n+    epochEntries.add(new EpochEntry<>(value.getEpoch(), cacheKey));\n+  }\n+\n+  public void cleanup(List<Long> epochs) {\n+    executorService.execute(() -> evictCache(epochs));\n+  }\n+\n+  @Override\n+  public int size() {\n+    return cache.size();\n+  }\n+\n+  @Override\n+  public Iterator<Map.Entry<CACHEKEY, CACHEVALUE>> iterator() {\n+    return cache.entrySet().iterator();\n+  }\n+\n+  @VisibleForTesting\n+  public void evictCache(List<Long> epochs) {\n+    EpochEntry<CACHEKEY> currentEntry;\n+    final AtomicBoolean removed = new AtomicBoolean();\n+    CACHEKEY cachekey;\n+    long lastEpoch = epochs.get(epochs.size() - 1);\n+    for (Iterator<EpochEntry<CACHEKEY>> iterator = epochEntries.iterator();\n+         iterator.hasNext();) {\n+      currentEntry = iterator.next();\n+      cachekey = currentEntry.getCachekey();\n+      long currentEpoch = currentEntry.getEpoch();\n+\n+      // As ConcurrentHashMap computeIfPresent is atomic, there is no race\n+      // condition between cache cleanup and requests updating same cache entry.\n+\n+      cache.computeIfPresent(cachekey, ((k, v) -> {\n+        if (v.getEpoch() == currentEpoch && epochs.contains(v.getEpoch())) {\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"CacheKey {} with epoch {} is removed from cache\",\n+                k.getCacheKey(), currentEpoch);\n+          }\n+          iterator.remove();\n+          removed.set(true);\n+          return null;\n+        }\n+        return v;\n+      }));\n+\n+      // If currentEntry epoch is greater than last epoch provided, we have\n+      // deleted all entries less than specified epoch. So, we can break.\n+      if (currentEpoch > lastEpoch) {\n+        break;\n+      }\n+\n+      // When epoch entry is not removed, this might be a override entry in\n+      // cache. Clean that epoch entry.\n+      if (!removed.get()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1df7910e8294d0c7b7258444087ebaf385cb04ff"}, "originalPosition": 149}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0MjUwNjQxOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/framework/src/test/java/org/apache/hadoop/hdds/utils/db/cache/TestTableCache.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQyMToxMzoyOFrOIKJW4A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQyMzozNDowNFrOIKMPqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzUwOTk4NA==", "bodyText": "Can we add the case where evictCache is called with an missing entry from consecutive list of epochs.", "url": "https://github.com/apache/ozone/pull/1699#discussion_r547509984", "createdAt": "2020-12-22T21:13:28Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/framework/src/test/java/org/apache/hadoop/hdds/utils/db/cache/TestTableCache.java", "diffHunk": "@@ -380,6 +383,33 @@ public void testPartialTableCacheParallel() throws Exception {\n \n   }\n \n+  @Test\n+  public void testTableCache() {\n+\n+    // In non-HA epoch entries might be out of order.\n+    // Scenario is like create vol, set vol, set vol, delete vol\n+    tableCache.put(new CacheKey<>(Long.toString(0)),\n+        new CacheValue<>(Optional.of(Long.toString(0)), 0));\n+    tableCache.put(new CacheKey<>(Long.toString(0)),\n+        new CacheValue<>(Optional.of(Long.toString(1)), 1));\n+    tableCache.put(new CacheKey<>(Long.toString(0)),\n+        new CacheValue<>(Optional.of(Long.toString(2)), 3));\n+\n+    tableCache.put(new CacheKey<>(Long.toString(0)),\n+        new CacheValue<>(Optional.absent(), 2));\n+\n+    List<Long> epochs = new ArrayList<>();\n+    epochs.add(0L);\n+    epochs.add(1L);\n+    epochs.add(2L);\n+    epochs.add(3L);\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1df7910e8294d0c7b7258444087ebaf385cb04ff"}, "originalPosition": 176}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU1NzI4OQ==", "bodyText": "Added a new test", "url": "https://github.com/apache/ozone/pull/1699#discussion_r547557289", "createdAt": "2020-12-22T23:34:04Z", "author": {"login": "bharatviswa504"}, "path": "hadoop-hdds/framework/src/test/java/org/apache/hadoop/hdds/utils/db/cache/TestTableCache.java", "diffHunk": "@@ -380,6 +383,33 @@ public void testPartialTableCacheParallel() throws Exception {\n \n   }\n \n+  @Test\n+  public void testTableCache() {\n+\n+    // In non-HA epoch entries might be out of order.\n+    // Scenario is like create vol, set vol, set vol, delete vol\n+    tableCache.put(new CacheKey<>(Long.toString(0)),\n+        new CacheValue<>(Optional.of(Long.toString(0)), 0));\n+    tableCache.put(new CacheKey<>(Long.toString(0)),\n+        new CacheValue<>(Optional.of(Long.toString(1)), 1));\n+    tableCache.put(new CacheKey<>(Long.toString(0)),\n+        new CacheValue<>(Optional.of(Long.toString(2)), 3));\n+\n+    tableCache.put(new CacheKey<>(Long.toString(0)),\n+        new CacheValue<>(Optional.absent(), 2));\n+\n+    List<Long> epochs = new ArrayList<>();\n+    epochs.add(0L);\n+    epochs.add(1L);\n+    epochs.add(2L);\n+    epochs.add(3L);\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzUwOTk4NA=="}, "originalCommit": {"oid": "1df7910e8294d0c7b7258444087ebaf385cb04ff"}, "originalPosition": 176}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4623, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}