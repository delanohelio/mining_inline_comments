{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM0OTQzMTMy", "number": 1079, "reviewThreads": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQwODoxMzoyN1rOEF1I4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxMDowNTozNVrOEF3uUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0NTQ4OTYxOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/FilePerChunkStrategy.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQwODoxMzoyN1rOGkQNfA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQwODoxMzoyN1rOGkQNfA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDY2NzUxNg==", "bodyText": "Nit: Can we please avoid this dependency?  I think it's better to use StringUtils from Apache Commons Lang, or even simply check explicitly both for null and empty string.", "url": "https://github.com/apache/ozone/pull/1079#discussion_r440667516", "createdAt": "2020-06-16T08:13:27Z", "author": {"login": "adoroszlai"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/FilePerChunkStrategy.java", "diffHunk": "@@ -34,11 +34,14 @@\n import org.apache.hadoop.ozone.container.common.volume.HddsVolume;\n import org.apache.hadoop.ozone.container.common.volume.VolumeIOStats;\n import org.apache.hadoop.ozone.container.keyvalue.helpers.ChunkUtils;\n+import org.apache.hadoop.ozone.container.keyvalue.interfaces.BlockManager;\n import org.apache.hadoop.ozone.container.keyvalue.interfaces.ChunkManager;\n import org.apache.hadoop.ozone.container.common.interfaces.Container;\n \n import static org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.Result.IO_EXCEPTION;\n import static org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.Result.UNABLE_TO_FIND_CHUNK;\n+\n+import org.eclipse.jetty.util.StringUtil;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a929967b379971a06903d0e1a10dc04b3b347c9"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0NTQ5NTU4OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/FilePerChunkStrategy.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQwODoxNTowMFrOGkQRLg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQwODoxNTowMFrOGkQRLg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDY2ODQ2Mg==", "bodyText": "Please store file.length() in local variable to avoid duplicate call.\nAlso, should we consider moving these checks to ChunkUtils.readData?", "url": "https://github.com/apache/ozone/pull/1079#discussion_r440668462", "createdAt": "2020-06-16T08:15:00Z", "author": {"login": "adoroszlai"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/FilePerChunkStrategy.java", "diffHunk": "@@ -218,17 +244,36 @@ public ChunkBuffer readChunk(Container container, BlockID blockID,\n     long len = info.getLen();\n     ByteBuffer data = ByteBuffer.allocate((int) len);\n \n+    int index = extractChunkIndex(info.getChunkName());\n+    if (index == -1) {\n+      throw new StorageContainerException(\n+          \"Chunk file name can't be parsed \" + possibleFiles.toString(),\n+          UNABLE_TO_FIND_CHUNK);\n+    }\n+    // Chunk index start from 1\n+    Preconditions.checkState(index > 0);\n+\n+    long chunkFileOffset;\n+    try {\n+      BlockData blockData = blockManager.getBlock(kvContainer, blockID);\n+      List<ContainerProtos.ChunkInfo> chunks = blockData.getChunks();\n+      Preconditions.checkState(index <= chunks.size());\n+      chunkFileOffset = chunks.get(index - 1).getOffset();\n+    } catch (IOException e) {\n+      throw new StorageContainerException(\n+          \"Cannot find block \" + blockID.toString() + \" for chunk \" +\n+              info.getChunkName(), UNABLE_TO_FIND_CHUNK);\n+    }\n+\n     for (File file : possibleFiles) {\n       try {\n-        // use offset only if file written by old datanode\n-        long offset;\n-        if (file.exists() && file.length() == info.getOffset() + len) {\n-          offset = info.getOffset();\n-        } else {\n-          offset = 0;\n+        if (file.exists()) {\n+          long offset = info.getOffset() - chunkFileOffset;\n+          Preconditions.checkState(offset < file.length());\n+          Preconditions.checkState((offset + len) <= file.length());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a929967b379971a06903d0e1a10dc04b3b347c9"}, "originalPosition": 92}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0NTUwNDU3OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/impl/AbstractTestChunkManager.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQwODoxNzoyNlrOGkQXDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQwODoxNzoyNlrOGkQXDA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDY2OTk2NA==", "bodyText": "Nit: I think it can be a public static constant.  It's not instance-specific, and it also makes the accessor method unnecessary.", "url": "https://github.com/apache/ozone/pull/1079#discussion_r440669964", "createdAt": "2020-06-16T08:17:26Z", "author": {"login": "adoroszlai"}, "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/impl/AbstractTestChunkManager.java", "diffHunk": "@@ -62,14 +63,17 @@\n   private ChunkInfo chunkInfo;\n   private ByteBuffer data;\n   private byte[] header;\n+  private BlockManager blockManager;\n+  private final String chunkFileNamePattern = \"%d_data_%d\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a929967b379971a06903d0e1a10dc04b3b347c9"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0NTYzMDIzOnYy", "diffSide": "RIGHT", "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestKeyInputStream.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQwODo0OTo0NlrOGkRmmw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QwNzowNzowOFrOGk4aLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDY5MDMzMQ==", "bodyText": "It seems the combination of increasing chunk size, creating new cluster for each test method, and running for both chunk layouts makes this integration test flaky: https://github.com/apache/hadoop-ozone/pull/1079/checks?check_run_id=775565603\nCan we use KB instead of MB for each size setting?\nadoroszlai@0cc9272", "url": "https://github.com/apache/ozone/pull/1079#discussion_r440690331", "createdAt": "2020-06-16T08:49:46Z", "author": {"login": "adoroszlai"}, "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestKeyInputStream.java", "diffHunk": "@@ -64,25 +71,38 @@\n   private static String volumeName;\n   private static String bucketName;\n   private static String keyString;\n+  private static ChunkLayoutTestInfo chunkLayout;\n+\n+  @Parameterized.Parameters\n+  public static Collection<Object[]> layouts() {\n+    return Arrays.asList(new Object[][] {\n+        {ChunkLayoutTestInfo.FILE_PER_CHUNK},\n+        {ChunkLayoutTestInfo.FILE_PER_BLOCK}\n+    });\n+  }\n \n+  public TestKeyInputStream(ChunkLayoutTestInfo layout) {\n+    this.chunkLayout = layout;\n+  }\n   /**\n    * Create a MiniDFSCluster for testing.\n    * <p>\n    * Ozone is made active by setting OZONE_ENABLED = true\n    *\n    * @throws IOException\n    */\n-  @BeforeClass\n-  public static void init() throws Exception {\n-    chunkSize = 100;\n+  @Before\n+  public void init() throws Exception {\n+    chunkSize = 1024 * 1024 * 4;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a929967b379971a06903d0e1a10dc04b3b347c9"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMyNjEyNA==", "bodyText": "I will try to reduce the chunkSize. One limitation here is chunkSize cann't be less than the mininum bytes-per-checksume value which is 256KB, otherwise the TestKeyInputStream will always succeed even without this patch.", "url": "https://github.com/apache/ozone/pull/1079#discussion_r441326124", "createdAt": "2020-06-17T07:07:08Z", "author": {"login": "ChenSammi"}, "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestKeyInputStream.java", "diffHunk": "@@ -64,25 +71,38 @@\n   private static String volumeName;\n   private static String bucketName;\n   private static String keyString;\n+  private static ChunkLayoutTestInfo chunkLayout;\n+\n+  @Parameterized.Parameters\n+  public static Collection<Object[]> layouts() {\n+    return Arrays.asList(new Object[][] {\n+        {ChunkLayoutTestInfo.FILE_PER_CHUNK},\n+        {ChunkLayoutTestInfo.FILE_PER_BLOCK}\n+    });\n+  }\n \n+  public TestKeyInputStream(ChunkLayoutTestInfo layout) {\n+    this.chunkLayout = layout;\n+  }\n   /**\n    * Create a MiniDFSCluster for testing.\n    * <p>\n    * Ozone is made active by setting OZONE_ENABLED = true\n    *\n    * @throws IOException\n    */\n-  @BeforeClass\n-  public static void init() throws Exception {\n-    chunkSize = 100;\n+  @Before\n+  public void init() throws Exception {\n+    chunkSize = 1024 * 1024 * 4;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDY5MDMzMQ=="}, "originalCommit": {"oid": "3a929967b379971a06903d0e1a10dc04b3b347c9"}, "originalPosition": 68}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0NTg0MjY1OnYy", "diffSide": "RIGHT", "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestKeyInputStream.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQwOTo0NjowNVrOGkTtyw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QwODo1MTozOVrOGk8KVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDcyNDkzOQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    Arrays.equals(tmp1, tmp2);\n          \n          \n            \n                    Assert.assertArrayEquals(tmp1, tmp2);\n          \n          \n            \n                    totalRead += numBytesRead;", "url": "https://github.com/apache/ozone/pull/1079#discussion_r440724939", "createdAt": "2020-06-16T09:46:05Z", "author": {"login": "adoroszlai"}, "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestKeyInputStream.java", "diffHunk": "@@ -331,4 +351,45 @@ public void testCopyLarge() throws Exception {\n       }\n     }\n   }\n+\n+  @Test\n+  public void testReadChunk() throws Exception {\n+    String keyName = getKeyName();\n+    OzoneOutputStream key = TestHelper.createKey(keyName,\n+        ReplicationType.RATIS, 0, objectStore, volumeName, bucketName);\n+\n+    // write data spanning multiple chunks\n+    int dataLength = (2 * chunkSize) + (chunkSize / 2);\n+    byte[] originData = new byte[dataLength];\n+    Random r = new Random();\n+    r.nextBytes(originData);\n+    key.write(originData);\n+    key.close();\n+\n+    // read chunk data\n+    KeyInputStream keyInputStream = (KeyInputStream) objectStore\n+        .getVolume(volumeName).getBucket(bucketName).readKey(keyName)\n+        .getInputStream();\n+\n+    int[] bufferSizeList = {chunkSize / 4, chunkSize / 2, chunkSize - 1,\n+        chunkSize, chunkSize + 1, blockSize - 1, blockSize, blockSize + 1,\n+        blockSize * 2};\n+    for (int bufferSize : bufferSizeList) {\n+      byte[] data = new byte[bufferSize];\n+      int totalRead = 0;\n+      while (totalRead < dataLength) {\n+        int numBytesRead = keyInputStream.read(data);\n+        if (numBytesRead == -1 || numBytesRead == 0) {\n+          break;\n+        }\n+        byte[] tmp1 =\n+            Arrays.copyOfRange(originData, totalRead, totalRead + numBytesRead);\n+        byte[] tmp2 =\n+            Arrays.copyOfRange(data, 0, numBytesRead);\n+        Arrays.equals(tmp1, tmp2);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a929967b379971a06903d0e1a10dc04b3b347c9"}, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM4NzYwNA==", "bodyText": "Thanks.", "url": "https://github.com/apache/ozone/pull/1079#discussion_r441387604", "createdAt": "2020-06-17T08:51:39Z", "author": {"login": "ChenSammi"}, "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestKeyInputStream.java", "diffHunk": "@@ -331,4 +351,45 @@ public void testCopyLarge() throws Exception {\n       }\n     }\n   }\n+\n+  @Test\n+  public void testReadChunk() throws Exception {\n+    String keyName = getKeyName();\n+    OzoneOutputStream key = TestHelper.createKey(keyName,\n+        ReplicationType.RATIS, 0, objectStore, volumeName, bucketName);\n+\n+    // write data spanning multiple chunks\n+    int dataLength = (2 * chunkSize) + (chunkSize / 2);\n+    byte[] originData = new byte[dataLength];\n+    Random r = new Random();\n+    r.nextBytes(originData);\n+    key.write(originData);\n+    key.close();\n+\n+    // read chunk data\n+    KeyInputStream keyInputStream = (KeyInputStream) objectStore\n+        .getVolume(volumeName).getBucket(bucketName).readKey(keyName)\n+        .getInputStream();\n+\n+    int[] bufferSizeList = {chunkSize / 4, chunkSize / 2, chunkSize - 1,\n+        chunkSize, chunkSize + 1, blockSize - 1, blockSize, blockSize + 1,\n+        blockSize * 2};\n+    for (int bufferSize : bufferSizeList) {\n+      byte[] data = new byte[bufferSize];\n+      int totalRead = 0;\n+      while (totalRead < dataLength) {\n+        int numBytesRead = keyInputStream.read(data);\n+        if (numBytesRead == -1 || numBytesRead == 0) {\n+          break;\n+        }\n+        byte[] tmp1 =\n+            Arrays.copyOfRange(originData, totalRead, totalRead + numBytesRead);\n+        byte[] tmp2 =\n+            Arrays.copyOfRange(data, 0, numBytesRead);\n+        Arrays.equals(tmp1, tmp2);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDcyNDkzOQ=="}, "originalCommit": {"oid": "3a929967b379971a06903d0e1a10dc04b3b347c9"}, "originalPosition": 132}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0NTkwOTA2OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/FilePerChunkStrategy.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxMDowNDoyOFrOGkUX1g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QwMzo0MDoxMVrOGk0haA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDczNTcwMg==", "bodyText": "Instead of parsing the chunk name for an index, we could find the chunk with the exact same name.  I think it's less fragile, although might be slower if the block has too many chunks.  What's your opinion?\nadoroszlai@398ea1d (note: CI is still in progress, may need tweak based on the result)", "url": "https://github.com/apache/ozone/pull/1079#discussion_r440735702", "createdAt": "2020-06-16T10:04:28Z", "author": {"login": "adoroszlai"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/FilePerChunkStrategy.java", "diffHunk": "@@ -218,17 +244,36 @@ public ChunkBuffer readChunk(Container container, BlockID blockID,\n     long len = info.getLen();\n     ByteBuffer data = ByteBuffer.allocate((int) len);\n \n+    int index = extractChunkIndex(info.getChunkName());\n+    if (index == -1) {\n+      throw new StorageContainerException(\n+          \"Chunk file name can't be parsed \" + possibleFiles.toString(),\n+          UNABLE_TO_FIND_CHUNK);\n+    }\n+    // Chunk index start from 1\n+    Preconditions.checkState(index > 0);\n+\n+    long chunkFileOffset;\n+    try {\n+      BlockData blockData = blockManager.getBlock(kvContainer, blockID);\n+      List<ContainerProtos.ChunkInfo> chunks = blockData.getChunks();\n+      Preconditions.checkState(index <= chunks.size());\n+      chunkFileOffset = chunks.get(index - 1).getOffset();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a929967b379971a06903d0e1a10dc04b3b347c9"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI2MjQ0MA==", "bodyText": "A good point. I think the performance overhead can be ignored.", "url": "https://github.com/apache/ozone/pull/1079#discussion_r441262440", "createdAt": "2020-06-17T03:40:11Z", "author": {"login": "ChenSammi"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/FilePerChunkStrategy.java", "diffHunk": "@@ -218,17 +244,36 @@ public ChunkBuffer readChunk(Container container, BlockID blockID,\n     long len = info.getLen();\n     ByteBuffer data = ByteBuffer.allocate((int) len);\n \n+    int index = extractChunkIndex(info.getChunkName());\n+    if (index == -1) {\n+      throw new StorageContainerException(\n+          \"Chunk file name can't be parsed \" + possibleFiles.toString(),\n+          UNABLE_TO_FIND_CHUNK);\n+    }\n+    // Chunk index start from 1\n+    Preconditions.checkState(index > 0);\n+\n+    long chunkFileOffset;\n+    try {\n+      BlockData blockData = blockManager.getBlock(kvContainer, blockID);\n+      List<ContainerProtos.ChunkInfo> chunks = blockData.getChunks();\n+      Preconditions.checkState(index <= chunks.size());\n+      chunkFileOffset = chunks.get(index - 1).getOffset();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDczNTcwMg=="}, "originalCommit": {"oid": "3a929967b379971a06903d0e1a10dc04b3b347c9"}, "originalPosition": 74}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0NTkxMzE0OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/FilePerChunkStrategy.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxMDowNTozNVrOGkUabQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxMDowNTozNVrOGkUabQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDczNjM2NQ==", "bodyText": "I think we can skip trying to determine chunk file offset if info.getOffset() == 0.", "url": "https://github.com/apache/ozone/pull/1079#discussion_r440736365", "createdAt": "2020-06-16T10:05:35Z", "author": {"login": "adoroszlai"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/FilePerChunkStrategy.java", "diffHunk": "@@ -218,17 +244,36 @@ public ChunkBuffer readChunk(Container container, BlockID blockID,\n     long len = info.getLen();\n     ByteBuffer data = ByteBuffer.allocate((int) len);\n \n+    int index = extractChunkIndex(info.getChunkName());\n+    if (index == -1) {\n+      throw new StorageContainerException(\n+          \"Chunk file name can't be parsed \" + possibleFiles.toString(),\n+          UNABLE_TO_FIND_CHUNK);\n+    }\n+    // Chunk index start from 1\n+    Preconditions.checkState(index > 0);\n+\n+    long chunkFileOffset;\n+    try {\n+      BlockData blockData = blockManager.getBlock(kvContainer, blockID);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a929967b379971a06903d0e1a10dc04b3b347c9"}, "originalPosition": 71}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4066, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}